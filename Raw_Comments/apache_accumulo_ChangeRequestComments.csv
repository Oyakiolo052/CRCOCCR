Pull,Path,Diff_hunk,Comment
154,core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java,"@@ -92,6 +94,20 @@ public Password(String dfault) {
     public String toString() {
       return new String(value, UTF_8);
     }
+
+    /**
+     * Prompts user for a password
+     *
+     * @return user entered Password object, null if no console exists
+     */
+    public static Password promptUser() throws IOException {
+      if (System.console() == null) {
+        return null;","[{'comment': ""I'm thinking that throwing an exception here would be more clear. This method cannot properly function as the caller expected, so they should get a clear exception in this case (not just a null return value).\n"", 'commenter': 'joshelser'}, {'comment': 'Fixed in 895591c\n', 'commenter': 'milleruntime'}]"
154,core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java,"@@ -142,11 +158,22 @@ public AuthenticationToken getToken() {
       }
     }
 
-    if (securePassword != null)
-      return new PasswordToken(securePassword.value);
-
-    if (password != null)
-      return new PasswordToken(password.value);
+    // other token types should have resolved by this point, so return PasswordToken
+    Password pass = null;
+    if (securePassword != null) {
+      pass = securePassword;
+    } else if (password != null) {
+      pass = password;
+    } else {
+      try {
+        pass = Password.promptUser();
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    if (pass != null) {","[{'comment': 'This becomes simpler too. Either we throw an RTE from the attempt to get a password, or we create a `PasswordToken` with the results. This method always gets to return a non-null value (woo!).\n', 'commenter': 'joshelser'}, {'comment': 'I had something like that before running tests. [TestClientOpts.test ](https://github.com/apache/accumulo/blob/master/core/src/test/java/org/apache/accumulo/core/cli/TestClientOpts.java#L69) expects for that method and ones that call it to return null.\n', 'commenter': 'milleruntime'}, {'comment': ""That's a weird test. I feel like the `fail()` is the expected execution path (e.g. it's asserting that `getPrincipal()` throw an exception) and the `assertNull()` is an afterthought? I wouldn't be too worried about making sure the tests pass exactly as-is. This isn't public API, so the semantics are likely not well-defined.\n"", 'commenter': 'joshelser'}, {'comment': ""OK I like removing the null return. If I get rid of the try/catch, I'd have to make getToken() throw IOException which will touch a bunch of files. \n"", 'commenter': 'milleruntime'}, {'comment': 'I will keep the try catch but remove the check for null\n', 'commenter': 'milleruntime'}]"
159,core/src/main/java/org/apache/accumulo/core/iterators/OrIterator.java,"@@ -80,6 +80,19 @@ public int compareTo(TermSource o) {
       // sorted after they have been determined to be valid.
       return this.iter.getTopKey().compareColumnQualifier(o.iter.getTopKey().getColumnQualifier());
     }
+
+    @Override
+    public void close() throws Exception {
+      this.iter.close();
+    }
+
+    public void closeSafely() {","[{'comment': 'why redifine this if its defined in interface?\n', 'commenter': 'keith-turner'}, {'comment': 'I made TermSource just implement AutoCloseable.  But I could make it implement SKVI instead\n', 'commenter': 'milleruntime'}]"
159,core/src/main/java/org/apache/accumulo/core/iterators/system/SourceSwitchingIterator.java,"@@ -213,4 +221,13 @@ public void setInterruptFlag(AtomicBoolean flag) {
       source.setInterruptFlag(flag);
     }
   }
+
+  @Override
+  public void close() throws Exception {
+    copies.forEach(ssi -> ssi.closeSafely());","[{'comment': 'this may need to sync on copies\n', 'commenter': 'keith-turner'}]"
159,core/src/main/java/org/apache/accumulo/core/iterators/user/IntersectingIterator.java,"@@ -110,6 +111,19 @@ public TermSource(SortedKeyValueIterator<Key,Value> iter, Text term, boolean not
     public String getTermString() {
       return (this.term == null) ? ""Iterator"" : this.term.toString();
     }
+
+    @Override
+    public void close() throws Exception {
+      this.iter.close();
+    }
+
+    public void closeSafely() {","[{'comment': 'why override default impl in interface?\n', 'commenter': 'keith-turner'}, {'comment': 'Same as the OrIterator.. I made TermSource just implement AutoCloseable. But I could make it implement SKVI instead\n', 'commenter': 'milleruntime'}]"
159,core/src/main/java/org/apache/accumulo/core/client/ClientSideIteratorScanner.java,"@@ -257,26 +257,19 @@ public void setSource(final Scanner scanner) {
       tm.put(iterInfo.getPriority(), iterInfo);
     }
 
-    SortedKeyValueIterator<Key,Value> skvi;
-    try {
-      skvi = IteratorUtil.loadIterators(smi, tm.values(), serverSideIteratorOptions, new ClientSideIteratorEnvironment(getSamplerConfiguration() != null,
-          getIteratorSamplerConfigurationInternal()), false, null);
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
+    try (SortedKeyValueIterator<Key,Value> skvi = IteratorUtil.loadIterators(smi, tm.values(), serverSideIteratorOptions, new ClientSideIteratorEnvironment(
+        getSamplerConfiguration() != null, getIteratorSamplerConfigurationInternal()), false, null)) {
 
-    final Set<ByteSequence> colfs = new TreeSet<>();
-    for (Column c : this.getFetchedColumns()) {
-      colfs.add(new ArrayByteSequence(c.getColumnFamily()));
-    }
+      final Set<ByteSequence> colfs = new TreeSet<>();
+      for (Column c : this.getFetchedColumns()) {
+        colfs.add(new ArrayByteSequence(c.getColumnFamily()));
+      }
 
-    try {
       skvi.seek(range, colfs, true);
-    } catch (IOException e) {
+      return new IteratorAdapter(skvi);
+    } catch (Exception e) {","[{'comment': 'This will catch exceptions that extend RuntimeException and rethrow them as a different exception. Seems very undesirable.\n\nLooks like this is because of AutoCloseable. I think we need to do something a little smarter.\n', 'commenter': 'joshelser'}, {'comment': 'Quick fix is to just put a `} catch (RuntimeException e) { throw e; }` bit above the catch-all for `Exception`.\n', 'commenter': 'ctubbsii'}]"
159,core/src/main/java/org/apache/accumulo/core/iterators/OrIterator.java,"@@ -249,4 +268,9 @@ final public boolean hasTop() {
   public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options, IteratorEnvironment env) throws IOException {
     throw new UnsupportedOperationException();
   }
+
+  @Override
+  public void close() throws Exception {
+    sources.forEach(s -> s.closeSafely());
+  }","[{'comment': ""I think it's probably rare to have multiple sources, and this is really the only value of closeSafely. I think maybe we shouldn't have closeSafely at all, because it forces implementations to think implement it safely, and also they must decide when it's appropriate to use it when closing their sources.\n\nInstead of having that, we should probably just do something special here in the case of multiple sources, to handle and aggregate exceptions from multiple sources.\n"", 'commenter': 'ctubbsii'}, {'comment': ""I agree.  My original thinking was to define closeSafely in the interface for coveinent use with Java 1.8.  But you are right, having multiple sources is a special case so it shouldn't be in the interface.\n"", 'commenter': 'milleruntime'}, {'comment': 'For exception handling of multiple sources, you thinking of not using forEach and going old school loop?\n', 'commenter': 'milleruntime'}, {'comment': 'Probably, yeah. If any one throws an exception, you can remember that, and either aggregate them into a single exception to throw at the end, or just propagate the last one seen.\n', 'commenter': 'ctubbsii'}]"
159,core/src/main/java/org/apache/accumulo/core/iterators/SortedKeyValueIterator.java,"@@ -147,4 +147,25 @@
    *              if not supported.
    */
   SortedKeyValueIterator<K,V> deepCopy(IteratorEnvironment env);
+
+  /**
+   * Closes the Iterator. This must be overridden by the implementing class that has access to <tt>SortedKeyValueIterator source</tt> provided in the
+   * <tt>init</tt> method.
+   */
+  @Override
+  default void close() throws Exception {","[{'comment': 'Is it useful to keep the Exception generic, or should we narrow the Exception type on the interface?\n', 'commenter': 'ctubbsii'}, {'comment': ""IOException? FileSKVIterator's override of close for AutoCloseable throws IOException...\n"", 'commenter': 'milleruntime'}, {'comment': ""Not sure. IOException might make the most sense, or a custom one, SKVICloseException (for example... but not that, because that's awful).\n"", 'commenter': 'ctubbsii'}, {'comment': 'After some discussion with @ctubbsii and @keith-turner, we were thinking of removing the ""throws Exception"".  We came to the conclusion that we don\'t want to assume that the close will throw an exception. This will require removing the IOException throws from FileSKVIterator.close and modifying its implementing classes to handle the IOException.  What do you think @joshelser ?\n', 'commenter': 'milleruntime'}]"
159,core/src/main/java/org/apache/accumulo/core/iterators/SortedKeyValueIterator.java,"@@ -147,4 +147,13 @@
    *              if not supported.
    */
   SortedKeyValueIterator<K,V> deepCopy(IteratorEnvironment env);
+
+  /**
+   * Closes the Iterator. This must be overridden by the implementing class that has access to <tt>SortedKeyValueIterator source</tt> provided in the","[{'comment': 'This should say ""should be overridden"". Or it should say that it ""is expected that"". Clearly it\'s not a ""must"" situation, since the default method does exist, and allows them to avoid it. This could cause problems mixing legacy iterators with new iterators, as the new ones will be expected to be closed, but the legacy ones won\'t close them.\n', 'commenter': 'ctubbsii'}, {'comment': 'fixed in ea2e2ae7f10ac950c87e7ff1cb5d7de169ef8974\n', 'commenter': 'milleruntime'}]"
159,server/base/src/main/java/org/apache/accumulo/server/util/FileUtil.java,"@@ -163,15 +163,15 @@ private static Path createTmpDir(AccumuloConfiguration acuConf, VolumeManager fs
         try {
           if (reader != null)
             reader.close();
-        } catch (IOException e) {
+        } catch (Exception e) {","[{'comment': ""Are these still needed if the reader can't throw?\n"", 'commenter': 'ctubbsii'}, {'comment': ""No they are not needed for compilation but since the code was failing quietly before, I didn't want to introduce an exception to classes that use FileUtil.\n"", 'commenter': 'milleruntime'}, {'comment': 'Should these just catch RuntimeException, then? Or does this reader still throw checked exceptions, too?\n', 'commenter': 'ctubbsii'}, {'comment': ""Changed to just catch RuntimeException and I also found places where MultiIterator wasn't being closed in 38049d4d5a655915c89eeb6a5635572967d12118\n"", 'commenter': 'milleruntime'}]"
159,core/src/main/java/org/apache/accumulo/core/client/ClientSideIteratorScanner.java,"@@ -257,26 +257,19 @@ public void setSource(final Scanner scanner) {
       tm.put(iterInfo.getPriority(), iterInfo);
     }
 
-    SortedKeyValueIterator<Key,Value> skvi;
-    try {
-      skvi = IteratorUtil.loadIterators(smi, tm.values(), serverSideIteratorOptions, new ClientSideIteratorEnvironment(getSamplerConfiguration() != null,
-          getIteratorSamplerConfigurationInternal()), false, null);
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
+    try (SortedKeyValueIterator<Key,Value> skvi = IteratorUtil.loadIterators(smi, tm.values(), serverSideIteratorOptions, new ClientSideIteratorEnvironment(
+        getSamplerConfiguration() != null, getIteratorSamplerConfigurationInternal()), false, null)) {
 
-    final Set<ByteSequence> colfs = new TreeSet<>();
-    for (Column c : this.getFetchedColumns()) {
-      colfs.add(new ArrayByteSequence(c.getColumnFamily()));
-    }
+      final Set<ByteSequence> colfs = new TreeSet<>();
+      for (Column c : this.getFetchedColumns()) {
+        colfs.add(new ArrayByteSequence(c.getColumnFamily()));
+      }
 
-    try {
       skvi.seek(range, colfs, true);
+      return new IteratorAdapter(skvi);","[{'comment': ""Won't turning this into a try-with-resources cause the new `skvi` returned in this `IteratorAdapter` to be closed?\n"", 'commenter': 'ctubbsii'}, {'comment': 'fixed in 0698836b3572359e8f28e036a853a8a3cc0f22db\n', 'commenter': 'milleruntime'}]"
159,core/src/main/java/org/apache/accumulo/core/file/rfile/RFile.java,"@@ -1164,11 +1167,7 @@ private Reader(Reader r, boolean useSample) {
 
     private void closeLocalityGroupReaders() {
       for (LocalityGroupReader lgr : currentReaders) {
-        try {
-          lgr.close();
-        } catch (IOException e) {
-          log.warn(""Errored out attempting to close LocalityGroupReader."", e);
-        }
+        lgr.close();","[{'comment': ""If `currentReaders` were changed to an ArrayList, this could use the stream `forEach`. Not sure if that would impact performance, but it'd probably improve readability in a few places.\n"", 'commenter': 'ctubbsii'}, {'comment': 'It looks like `readers` and `sampleReaders` could be changed as well. I think this change could be another ticket though, something like ""Utilize functional programming in RFile"".  \n', 'commenter': 'milleruntime'}]"
159,core/src/main/java/org/apache/accumulo/core/file/rfile/RFile.java,"@@ -1193,16 +1192,14 @@ public void close() throws IOException {
 
       if (sampleReaders != null) {
         for (LocalityGroupReader lgr : sampleReaders) {
-          try {
-            lgr.close();
-          } catch (IOException e) {
-            log.warn(""Errored out attempting to close LocalityGroupReader."", e);
-          }
+          lgr.close();
         }
       }
 
       try {
         reader.close();
+      } catch (IOException e) {
+        throw new RuntimeException(e);","[{'comment': 'Should probably use the new `UncheckedIOException` wrapper here and elsewhere when `IOException` is wrapped.\n', 'commenter': 'ctubbsii'}]"
159,core/src/main/java/org/apache/accumulo/core/iterators/ColumnFamilyCounter.java,"@@ -89,4 +89,11 @@ public Value getTopValue() {
     return null;
   }
 
+  @Override","[{'comment': 'I wonder if we could clean up some of these iterators implementations by making them use `WrappingIterator`.\n', 'commenter': 'ctubbsii'}]"
159,core/src/main/java/org/apache/accumulo/core/iterators/system/SourceSwitchingIterator.java,"@@ -162,6 +162,14 @@ private void readNext(boolean initialSeek) throws IOException {
   private boolean switchSource() throws IOException {
     if (!source.isCurrent()) {
       source = source.getNewDataSource();
+      // if our source actually changed, then attempt to close the previous iterator
+      try {
+        if (iter != null) {
+          iter.close();
+        }
+      } catch (Exception e) {
+        throw new IOException(e);","[{'comment': 'This is a private method. Do we need to wrap exceptions here, or should we handle it in the caller? It seems we might be converting them to IOExceptions here, only to have them converted back to RuntimeExceptions later.\n', 'commenter': 'ctubbsii'}]"
159,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Compactor.java,"@@ -388,9 +389,10 @@ else if (env.getIteratorScope() == IteratorScope.minc)
           }
           throw new CompactionCanceledException();
         }
-
+      } catch (Exception e) {
+        throw new IOException(e);
       } finally {
-        CompactionStats lgMajcStats = new CompactionStats(citr.getCount(), entriesCompacted);
+        CompactionStats lgMajcStats = new CompactionStats(statsCount, entriesCompacted);","[{'comment': ""Careful about suppressed exceptions here, especially when using multiple resources in a single try-with-resources, and mixing try-with-resources with a finally block. Any exception thrown in the finally block will suppress try-with-resources, and exception from closing one resource may result in others being suppressed.\n\nHow do we want to handle suppressed exceptions? Do we assume they've already been logged earlier, or do we try to handle them here? Wrapping `Exception e` with `IOException` may also make it more difficult for the caller to track the suppressed exceptions attached to `e`, which is now in the `getCause()` of the `IOException`.\n\nAlso, statsCount might be stale (or uninitialized) in the finally block if an exception occurs in the try block.\n"", 'commenter': 'ctubbsii'}]"
159,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Scanner.java,"@@ -120,16 +108,15 @@ public ScanBatch read() throws IOException, TabletClosedException {
       }
 
       sawException = true;
-      dataSource.close(true);
       throw ioe;
-    } catch (RuntimeException re) {
+    } catch (Exception e) {","[{'comment': 'Careful about catching other RuntimeExceptions and wrapping them again with yet another RuntimeException. Also, try to pick a more specific RuntimeException, if possible. Perhaps `IllegalStateException`, `IllegalArgumentException`, or `UncheckedIOException`.\n', 'commenter': 'ctubbsii'}]"
163,core/src/main/java/org/apache/accumulo/core/security/crypto/NonCachingSecretKeyEncryptionStrategy.java,"@@ -81,7 +81,10 @@ private void doKeyEncryptionOperation(int encryptionMode, CryptoModuleParameters
       Cipher cipher = DefaultCryptoModuleUtils.getCipher(params.getAllOptions().get(Property.CRYPTO_DEFAULT_KEY_STRATEGY_CIPHER_SUITE.getKey()));
 
       try {
-        cipher.init(encryptionMode, new SecretKeySpec(keyEncryptionKey, params.getAlgorithmName()));
+    	if (keyEncryptionKey.length > 0)","[{'comment': 'It looks like this change might actually change the code flow. When the keyEncryptionKey was of length `0`, would `cipher.init(...)` have thrown an `InvalidKeyException`? I would assume that when the key is empty, we would want to throw a `RuntimeException`.\n', 'commenter': 'joshelser'}, {'comment': ""This makes me wonder what the API contract is on `read()`. Will the return value from this always be equal to the length of the byte array? If it's not, should we explicitly throw an `InvalidKeyException`? Should we use some `readFully()` method to populate the read buffer instead of `read()`?\n"", 'commenter': 'ctubbsii'}, {'comment': 'The SecretKeySpec constructor would throw IllegalArgumentException\n\nhttps://docs.oracle.com/javase/7/docs/api/javax/crypto/spec/SecretKeySpec.html \n', 'commenter': 'mstair'}, {'comment': 'Code was throwing an exception even before the if check was added, adding this check probably caused the rest of the code to be executed resulting in a null exception in another part of the code.\n', 'commenter': 'mm376'}, {'comment': '> The SecretKeySpec constructor would throw IllegalArgumentException\n> \n> adding this check probably caused the rest of the code to be executed resulting in a null exception in another part of the code.\n\nSounds to me like we would want to throw our own IllegalArgumentException (or some unchecked exception) in an `else` branch to the conditional you added. Care to update the pull request with that change?\n', 'commenter': 'joshelser'}, {'comment': ""The original reported issue is that the return value of the read is not captured or checked. The conditional being checked here only verifies that the allocated array was larger than an empty array. What it actually needs to check is that the number of bytes read into the array is the same as the value of the length field, regardless of what else we want to check. The conditional in this patch is sufficient only if we switch to using `readFully` instead of `read`, which throws an EOFException if the number of bytes read from the input stream isn't enough to fill the buffer (e.g. doesn't match the expected keyEncryptionKeyLength).\n"", 'commenter': 'ctubbsii'}, {'comment': 'Ahhh...yes. We will fix the scope and get resubmitted today.\n', 'commenter': 'mstair'}, {'comment': 'Chris, Josh: I made the code change and pushed it into repository, take a look at it when you get a chance.\n', 'commenter': 'mm376'}]"
175,server/monitor/src/main/java/org/apache/accumulo/monitor/util/Table.java,"@@ -143,7 +146,24 @@ public synchronized void generate(HttpServletRequest req, StringBuilder sb) {
       showLegend = showStr != null && Boolean.parseBoolean(showStr);
     }
 
-    sb.append(""<div>\n"");
+    String redir = BasicServlet.currentPage(req);
+
+    if (namespaces != null) {
+      sb.append(""<div id=\""filters\"">\n"");
+      String namespace = BasicServlet.getCookieValue(req, ""namespaceDropdown."" + BasicServlet.encode(page) + ""."" + BasicServlet.encode(tableName) + "".""
+          + ""selected"");
+      if (namespace == null) {
+        namespace = ""*"";
+      }
+      sb.append(""<div class='left show'><dl>\n"");
+      doDropdownMenu(redir, page, tableName, sb, namespaces, namespace);","[{'comment': 'Could add header to menu like ""Table namespaces"" or ""Namespaces""\n', 'commenter': 'mikewalch'}, {'comment': 'Will add a header ""Namespaces"" that uses a different font than the rest of the list.\n', 'commenter': 'lstav'}]"
175,server/monitor/src/main/java/org/apache/accumulo/monitor/util/Table.java,"@@ -227,4 +247,25 @@ private static void row(StringBuilder sb, boolean highlight, ArrayList<TableColu
     sb.append(""</tr>\n"");
   }
 
+  private static void doDropdownMenu(String redir, String page, String tableName, StringBuilder sb, SortedMap<String,String> namespaces, String namespace) {
+
+    String namespaceUrl = String.format(""/op?action=namespace&redir=%s&page=%s&table=%s&selected="", redir, page, tableName);
+
+    sb.append(""<ul id=\""namespaces\"">\n"");
+    sb.append(""<li><a "").append(namespace.equals(""*"") ? ""class=\""active\"" "" : """").append(""href=\"""").append(namespaceUrl).append(""*\"">*</a></li>"");","[{'comment': '`*` is a little confusing on its own.  Could add something about this being all tables\n', 'commenter': 'mikewalch'}, {'comment': 'Will change it to be ""\\* (All Tables)""\n', 'commenter': 'lstav'}]"
175,server/monitor/src/main/java/org/apache/accumulo/monitor/util/Table.java,"@@ -227,4 +247,25 @@ private static void row(StringBuilder sb, boolean highlight, ArrayList<TableColu
     sb.append(""</tr>\n"");
   }
 
+  private static void doDropdownMenu(String redir, String page, String tableName, StringBuilder sb, SortedMap<String,String> namespaces, String namespace) {
+
+    String namespaceUrl = String.format(""/op?action=namespace&redir=%s&page=%s&table=%s&selected="", redir, page, tableName);
+
+    sb.append(""<ul id=\""namespaces\"">\n"");
+    sb.append(""<li><a "").append(namespace.equals(""*"") ? ""class=\""active\"" "" : """").append(""href=\"""").append(namespaceUrl).append(""*\"">*</a></li>"");
+    for (String key : namespaces.keySet()) {
+      if (key.equals("""")) {
+        sb.append(""<li><a "").append(namespace.equals(""-"") ? ""class=\""active\"" "" : """").append(""href=\"""").append(namespaceUrl).append(""-\"">-</a></li>"");","[{'comment': 'Same as above. `-` is a little confusing.  Could add text describing this option as default tables.\n', 'commenter': 'mikewalch'}, {'comment': 'Will change this to be ""- (DEFAULT)""\n', 'commenter': 'lstav'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  ","[{'comment': 'Kind of makes the reader wonder if there are existing tickets open for HDFS to improve this situation.\n', 'commenter': 'ctubbsii'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  
+
+## Configuring WAL flush/sync in Accumulo 1.6
+
+Accumulo 1.6.0 only supported `hsync` and this caused [performance
+problems][160_RN_WAL].  In order to offer better performance, the option to
+configure `hflush` was [added in 1.6.1][161_RN_WAL].  The","[{'comment': 'Was there no third option to do neither?\n', 'commenter': 'ctubbsii'}, {'comment': ""I don't think so, but not 100% sure.  Would have to go look at code.  The docs do not mention this option.  1.6 did have a per table option to turn the WAL on and off.  This was also superseded by `table.durability`.   I had forgotten about this property. \n"", 'commenter': 'keith-turner'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  
+
+## Configuring WAL flush/sync in Accumulo 1.6
+
+Accumulo 1.6.0 only supported `hsync` and this caused [performance
+problems][160_RN_WAL].  In order to offer better performance, the option to
+configure `hflush` was [added in 1.6.1][161_RN_WAL].  The
+[tserver.wal.sync.method][16_UM_SM] configuration option was added to support
+this feature.  This was a tablet server wide option that applied to everything
+written to any table.   
+
+## Group Commit
+
+Each Accumulo Tablet Server has a single WAL.  When multiple clients send
+mutations to a tablet server at around the same time, the tablet sever may group
+all of this into a single WAL operation.  It will do this instead of writing and
+syncing or flushing each client's mutations to the WAL separately.  Doing this
+increase throughput and lowers average latency for clients.
+
+## Configuring WAL flush/sync in Accumulo 1.7+
+
+Accumulo 1.7.0 introduced [table.durability][17_UM_TD], a new per table property
+for configuring durability.  It also stopped using the `tserver.wal.sync.method`
+property.  The `table.durability` property has the following four legal values.
+
+ * **none** : Do not write to WAL            
+ * **log**  : Write to WAL, but do not sync  
+ * **flush** : Write to WAL and call `hflush` 
+ * **sync** : Write to WAL and call `hsync`  
+","[{'comment': 'Which of the above 4 options are the default?\n', 'commenter': 'ctubbsii'}, {'comment': 'Knowing the default is really important to understanding later information.  I suppose I will add it.\n', 'commenter': 'keith-turner'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  
+
+## Configuring WAL flush/sync in Accumulo 1.6
+
+Accumulo 1.6.0 only supported `hsync` and this caused [performance
+problems][160_RN_WAL].  In order to offer better performance, the option to
+configure `hflush` was [added in 1.6.1][161_RN_WAL].  The
+[tserver.wal.sync.method][16_UM_SM] configuration option was added to support
+this feature.  This was a tablet server wide option that applied to everything
+written to any table.   
+
+## Group Commit","[{'comment': 'This section should be above the section for configuring 1.6, so the configuration section for 1.6 is closer to the one for 1.7+.\n', 'commenter': 'ctubbsii'}, {'comment': 'I had it like that initially.  Then I realized that the information about group commit was useful for understanding the 1.7 section, so I moved it.  Does this change your opinion?\n', 'commenter': 'keith-turner'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  
+
+## Configuring WAL flush/sync in Accumulo 1.6
+
+Accumulo 1.6.0 only supported `hsync` and this caused [performance
+problems][160_RN_WAL].  In order to offer better performance, the option to
+configure `hflush` was [added in 1.6.1][161_RN_WAL].  The
+[tserver.wal.sync.method][16_UM_SM] configuration option was added to support
+this feature.  This was a tablet server wide option that applied to everything
+written to any table.   
+
+## Group Commit
+
+Each Accumulo Tablet Server has a single WAL.  When multiple clients send
+mutations to a tablet server at around the same time, the tablet sever may group
+all of this into a single WAL operation.  It will do this instead of writing and
+syncing or flushing each client's mutations to the WAL separately.  Doing this
+increase throughput and lowers average latency for clients.
+
+## Configuring WAL flush/sync in Accumulo 1.7+
+
+Accumulo 1.7.0 introduced [table.durability][17_UM_TD], a new per table property
+for configuring durability.  It also stopped using the `tserver.wal.sync.method`
+property.  The `table.durability` property has the following four legal values.
+
+ * **none** : Do not write to WAL            
+ * **log**  : Write to WAL, but do not sync  
+ * **flush** : Write to WAL and call `hflush` 
+ * **sync** : Write to WAL and call `hsync`  
+
+If multiple writes arrive at around the same time with different durability
+settings, then the group commit code will choose the most conservative","[{'comment': 'What does ""most conservative"" mean here? Maybe ""most durable"" would be a better word choice?\n', 'commenter': 'ctubbsii'}, {'comment': 'I like the wording `most durable` so much better.\n', 'commenter': 'keith-turner'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  
+
+## Configuring WAL flush/sync in Accumulo 1.6
+
+Accumulo 1.6.0 only supported `hsync` and this caused [performance
+problems][160_RN_WAL].  In order to offer better performance, the option to
+configure `hflush` was [added in 1.6.1][161_RN_WAL].  The
+[tserver.wal.sync.method][16_UM_SM] configuration option was added to support
+this feature.  This was a tablet server wide option that applied to everything
+written to any table.   
+
+## Group Commit
+
+Each Accumulo Tablet Server has a single WAL.  When multiple clients send
+mutations to a tablet server at around the same time, the tablet sever may group
+all of this into a single WAL operation.  It will do this instead of writing and
+syncing or flushing each client's mutations to the WAL separately.  Doing this
+increase throughput and lowers average latency for clients.
+
+## Configuring WAL flush/sync in Accumulo 1.7+
+
+Accumulo 1.7.0 introduced [table.durability][17_UM_TD], a new per table property
+for configuring durability.  It also stopped using the `tserver.wal.sync.method`
+property.  The `table.durability` property has the following four legal values.
+
+ * **none** : Do not write to WAL            
+ * **log**  : Write to WAL, but do not sync  
+ * **flush** : Write to WAL and call `hflush` 
+ * **sync** : Write to WAL and call `hsync`  
+
+If multiple writes arrive at around the same time with different durability
+settings, then the group commit code will choose the most conservative
+durability.  This can cause one tables settings to slow down writes to another
+table.  
+
+In Accumulo 1.6, it was easy to make all writes use `hflush` because there was
+only one tserver setting.  Getting everything to use `flush` in 1.7 and later
+can be a little tricky because by default the Accumulo metadata table is set to
+use `sync`.  Executing the following command in the Accumulo shell will
+accomplish this (assuming no tables or namespaces have been specifically set to
+`sync`).  The first command sets a system wide table default for `flush`.  The
+second two commands override metadata table specific settings of `sync`.
+
+```
+config -s table.durability=flush
+config -t accumulo.metadata -s table.durability=flush
+config -t accumulo.root -s table.durability=flush","[{'comment': 'The two metadata tables are using flush here, not sync, as the paragraph above described.\n', 'commenter': 'ctubbsii'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  
+
+## Configuring WAL flush/sync in Accumulo 1.6
+
+Accumulo 1.6.0 only supported `hsync` and this caused [performance
+problems][160_RN_WAL].  In order to offer better performance, the option to
+configure `hflush` was [added in 1.6.1][161_RN_WAL].  The
+[tserver.wal.sync.method][16_UM_SM] configuration option was added to support
+this feature.  This was a tablet server wide option that applied to everything
+written to any table.   
+
+## Group Commit
+
+Each Accumulo Tablet Server has a single WAL.  When multiple clients send
+mutations to a tablet server at around the same time, the tablet sever may group
+all of this into a single WAL operation.  It will do this instead of writing and
+syncing or flushing each client's mutations to the WAL separately.  Doing this
+increase throughput and lowers average latency for clients.
+
+## Configuring WAL flush/sync in Accumulo 1.7+
+
+Accumulo 1.7.0 introduced [table.durability][17_UM_TD], a new per table property
+for configuring durability.  It also stopped using the `tserver.wal.sync.method`
+property.  The `table.durability` property has the following four legal values.
+
+ * **none** : Do not write to WAL            
+ * **log**  : Write to WAL, but do not sync  
+ * **flush** : Write to WAL and call `hflush` 
+ * **sync** : Write to WAL and call `hsync`  
+
+If multiple writes arrive at around the same time with different durability
+settings, then the group commit code will choose the most conservative
+durability.  This can cause one tables settings to slow down writes to another
+table.  
+
+In Accumulo 1.6, it was easy to make all writes use `hflush` because there was
+only one tserver setting.  Getting everything to use `flush` in 1.7 and later
+can be a little tricky because by default the Accumulo metadata table is set to
+use `sync`.  Executing the following command in the Accumulo shell will
+accomplish this (assuming no tables or namespaces have been specifically set to
+`sync`).  The first command sets a system wide table default for `flush`.  The
+second two commands override metadata table specific settings of `sync`.
+
+```
+config -s table.durability=flush
+config -t accumulo.metadata -s table.durability=flush
+config -t accumulo.root -s table.durability=flush
+```
+
+Even with these settings adjusted, minor compactions could still force `hsync`
+to be called in 1.7.0 and 1.7.1.  This was fixed in 1.7.2 and 1.8.0.  See the
+[1.7.2 release notes][172_RN_MCHS] and [ACCUMULO-4112] for more details.
+
+In addition to the per table durability setting, a per batch writer durability
+setting was also added in 1.7.0.  See
+[BatchWriterConfig.setDurability(...)][SD].  This means any client could
+potentially cause a `hsync` operation to occur, even if the system is
+configured to use `hflush`.
+
+## Improving the situation
+
+The more granular durability settings introduced in 1.7.0 can cause some
+unexpected problems.  [ACCUMULO-4146] suggest one possible way to solve these
+problems with Per-durability write ahead logs.
+
+[fcf]: https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileChannel.html#force-boolean-
+[ros1]: https://github.com/apache/hadoop/blob/release-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaOutputStreams.java#L78
+[ros2]: https://github.com/apache/hadoop/blob/release-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaOutputStreams.java#L87
+[fos]: https://github.com/apache/hadoop/blob/release-2.7.1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java#L358
+[ACCUMULO-4146]: https://issues.apache.org/jira/browse/ACCUMULO-4146
+[ACCUMULO-4112]: https://issues.apache.org/jira/browse/ACCUMULO-4112
+[160_RN_WAL]: /release_notes/1.6.0#slower-writes-than-previous-accumulo-versions","[{'comment': ""Jekyll note: you should prefix absolute links with `{{ site.baseurl }}` so the links don't break if running locally or on a test fork.\n"", 'commenter': 'ctubbsii'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server","[{'comment': 'Nit: Consistent capitalization of ""Tablet Server"" across the next three lines.\n', 'commenter': 'joshelser'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the","[{'comment': 'nit: it\'s also written.\n\nWrite the long-form ""write ahead log"" and then put the abbreviation in parens. Then, you can use ""WAL"" to refer to ""write ahead log""\n', 'commenter': 'joshelser'}, {'comment': 'When I wrote that I thought it looked screwy, now I know why.\n', 'commenter': 'keith-turner'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps","[{'comment': 'Nit: singular ""map"" instead of ""maps"" (I think?)\n', 'commenter': 'joshelser'}, {'comment': ""A batch of mutations may go to multiple tablets.  I'll try to make that a little more clear.\n"", 'commenter': 'keith-turner'}, {'comment': 'But isn\'t there still only one IMM, just partitioned by tablet? That was my distinction.\n\nAlso, ""Tablet Servers\' WAL"" looking at this again :)\n', 'commenter': 'joshelser'}, {'comment': 'Each tablet has its own IMM.\n', 'commenter': 'keith-turner'}, {'comment': 'My apologies! Ignore this one then.\n', 'commenter': 'joshelser'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only","[{'comment': 's/ensure/guarantee/ ? I think that better represents the ""it may or may not be on disk"" notion.\n', 'commenter': 'joshelser'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process","[{'comment': 's/reboot/fail/ ?\n', 'commenter': 'joshelser'}, {'comment': 'I was using the word `die`... then I replaced that with `reboot`...  I am looking for the shortest way to say the operating system dies... the OS can die w/o a hardware failure, thats why I settled on reboot as a short way to communicate this in a way that I think most would understand\n', 'commenter': 'keith-turner'}, {'comment': 'When I read ""reboot"", I thought of it as an ""planned"" occurrence, instead of a planned _or_ unplanned failure (such as operator error, hardware failure, etc, kernel panic, etc). Admittedly, it might be my colloquial use of the word ""reboot"" affecting my interpretation :)\n', 'commenter': 'joshelser'}, {'comment': 'I agree it does give that connotation of planned.  Still struggling with how to write this concisely.  Suppose I could just add more words.\n', 'commenter': 'keith-turner'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data","[{'comment': 's/thats ok because it flushed to OS/data loss will not happen because the data was still sitting in OS I/O buffers to be written to disk/\n', 'commenter': 'joshelser'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in","[{'comment': ""I'd avoid these two sentences about something you don't know about. Just omit it. I don't think it will have the effect you intend it to have (without knowing your mannerism/way of speaking personally).\n"", 'commenter': 'joshelser'}, {'comment': 'Its certainly not very useful.  Thinking I can do one of the following :\n- Omit it\n- Research it\n- Change it to ask for anyone how might know the answer to this mystery to let us know.\n\nResearching it would be ideal, but I am thinking of going with the last option and asking for help.  If there is a reader w/ expertise they could probably answer this much more quickly than me.\n', 'commenter': 'keith-turner'}, {'comment': ""The latter two would definitely be best, but I didn't want to impose that you _had_ to do either of them :). I'm not sure who would be most likely to know something like that off the top of their head. Maybe Todd Lipcon?\n"", 'commenter': 'joshelser'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  
+
+## Configuring WAL flush/sync in Accumulo 1.6
+
+Accumulo 1.6.0 only supported `hsync` and this caused [performance
+problems][160_RN_WAL].  In order to offer better performance, the option to
+configure `hflush` was [added in 1.6.1][161_RN_WAL].  The
+[tserver.wal.sync.method][16_UM_SM] configuration option was added to support
+this feature.  This was a tablet server wide option that applied to everything
+written to any table.   
+
+## Group Commit
+
+Each Accumulo Tablet Server has a single WAL.  When multiple clients send
+mutations to a tablet server at around the same time, the tablet sever may group
+all of this into a single WAL operation.  It will do this instead of writing and
+syncing or flushing each client's mutations to the WAL separately.  Doing this
+increase throughput and lowers average latency for clients.
+
+## Configuring WAL flush/sync in Accumulo 1.7+
+
+Accumulo 1.7.0 introduced [table.durability][17_UM_TD], a new per table property
+for configuring durability.  It also stopped using the `tserver.wal.sync.method`
+property.  The `table.durability` property has the following four legal values.
+
+ * **none** : Do not write to WAL            
+ * **log**  : Write to WAL, but do not sync  
+ * **flush** : Write to WAL and call `hflush` 
+ * **sync** : Write to WAL and call `hsync`  
+
+If multiple writes arrive at around the same time with different durability
+settings, then the group commit code will choose the most conservative
+durability.  This can cause one tables settings to slow down writes to another
+table.  ","[{'comment': ""This is very interesting. I hadn't considered this before. Sounds like this would be a very good metric to expose (since it could mutations written to one table much slower than expected).\n"", 'commenter': 'joshelser'}]"
176,_posts/blog/2016-10-28-durability-performance.md,"@@ -0,0 +1,136 @@
+---
+title: ""Durability Performance Implications""
+date: 2016-10-28 17:00:00 +0000
+author: Keith Turner
+---
+
+## Overview
+
+Accumulo stores recently written data in a sorted in memory map.  Before data is
+added to this map, it's written to an unsorted WAL (write ahead log).  In the
+case when a Tablet Server dies, the recently written data is recovered from the
+WAL.
+
+When data is written to Accumulo the following happens :
+
+ * Client sends a batch of mutations to a tablet server
+ * Tablet server does the following :
+   * Writes mutation to Tablet Servers WAL
+   * Sync or flush WAL
+   * Adds mutations to sorted in memory maps
+   * Reports success back to client.
+
+The sync/flush step above moves data written to the WAL from memory to disk.
+Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data to
+disk for an open file : `hsync` and `hflush`.  
+
+## HDFS Sync/Flush Details
+
+When `hflush` is called on a WAL, it does not ensure data is on disk.  It only
+ensure that data is in OS buffers on each datanode and on its way to disk.  As a
+result calls to `hflush` are very fast.  If a WAL is replicated to 3 data nodes
+then data may be lost if all three machines reboot.  If the datanode process
+dies, thats ok because it flushed to OS.  The machines have to reboot for data
+loss to occur.
+
+In order to avoid data loss in the event of reboot, `hsync` can be called.  This
+will ensure data is written to disk on all datanodes before returning.  When
+using `hsync` for the WAL, if Accumulo reports success to a user it means the
+data is on disk.  However `hsync` is much slower than `hflush` and the way it's
+implemented exacerbates the problem.  For example `hflush` make take 1ms and
+`hsync` may take 50ms.  This difference will impact writes to Accumulo and can
+be mitigated in some situations with larger buffers in Accumulo.
+
+HDFS keeps checksum data internally by default.  Datanodes store checksum data
+in a separate file in the local filesystem.  This means when `hsync` is called
+on a WAL, two files must be synced on each datanode.  Syncing two files doubles
+the time. To make matters even worse, when the two files are synced the local
+filesystem metadata is also synced.  Depending on the local filesystem and its
+configuration, syncing the metadata may or may not take time.  In the worst
+case, we need to wait for four sync operations at the local filesystem level on
+each datanode. One thing I am not sure about, is if these sync operations occur
+in parallel on the replicas on different datanodes.  Lets hope they occur in
+parallel.  The following pointers show where sync occurs in the datanode code.
+
+ * [BlockReceiver.flushOrSync()][fos] calls [ReplicaOutputStreams.syncDataOut()][ros1] and [ReplicaOutputStreams.syncChecksumOut()][ros2] when `isSync` is true.
+ * The methods in ReplicaOutputStreams call [FileChannel.force(true)][fcf] which
+   synchronously flushes data and filesystem metadata.
+
+If files were preallocated (this would avoid syncing local filesystem metadata)
+and checksums were stored in-line, then 1 sync could be done instead of 4.  
+
+## Configuring WAL flush/sync in Accumulo 1.6
+
+Accumulo 1.6.0 only supported `hsync` and this caused [performance
+problems][160_RN_WAL].  In order to offer better performance, the option to
+configure `hflush` was [added in 1.6.1][161_RN_WAL].  The
+[tserver.wal.sync.method][16_UM_SM] configuration option was added to support
+this feature.  This was a tablet server wide option that applied to everything
+written to any table.   
+
+## Group Commit
+
+Each Accumulo Tablet Server has a single WAL.  When multiple clients send
+mutations to a tablet server at around the same time, the tablet sever may group
+all of this into a single WAL operation.  It will do this instead of writing and
+syncing or flushing each client's mutations to the WAL separately.  Doing this
+increase throughput and lowers average latency for clients.
+
+## Configuring WAL flush/sync in Accumulo 1.7+
+
+Accumulo 1.7.0 introduced [table.durability][17_UM_TD], a new per table property
+for configuring durability.  It also stopped using the `tserver.wal.sync.method`
+property.  The `table.durability` property has the following four legal values.
+
+ * **none** : Do not write to WAL            
+ * **log**  : Write to WAL, but do not sync  
+ * **flush** : Write to WAL and call `hflush` 
+ * **sync** : Write to WAL and call `hsync`  
+
+If multiple writes arrive at around the same time with different durability
+settings, then the group commit code will choose the most conservative
+durability.  This can cause one tables settings to slow down writes to another
+table.  
+
+In Accumulo 1.6, it was easy to make all writes use `hflush` because there was
+only one tserver setting.  Getting everything to use `flush` in 1.7 and later
+can be a little tricky because by default the Accumulo metadata table is set to
+use `sync`.  Executing the following command in the Accumulo shell will
+accomplish this (assuming no tables or namespaces have been specifically set to
+`sync`).  The first command sets a system wide table default for `flush`.  The
+second two commands override metadata table specific settings of `sync`.
+
+```
+config -s table.durability=flush
+config -t accumulo.metadata -s table.durability=flush
+config -t accumulo.root -s table.durability=flush
+```
+
+Even with these settings adjusted, minor compactions could still force `hsync`
+to be called in 1.7.0 and 1.7.1.  This was fixed in 1.7.2 and 1.8.0.  See the
+[1.7.2 release notes][172_RN_MCHS] and [ACCUMULO-4112] for more details.
+
+In addition to the per table durability setting, a per batch writer durability
+setting was also added in 1.7.0.  See
+[BatchWriterConfig.setDurability(...)][SD].  This means any client could
+potentially cause a `hsync` operation to occur, even if the system is
+configured to use `hflush`.
+
+## Improving the situation
+
+The more granular durability settings introduced in 1.7.0 can cause some
+unexpected problems.  [ACCUMULO-4146] suggest one possible way to solve these","[{'comment': 's/suggest/suggests/\n', 'commenter': 'joshelser'}]"
187,images/_logo/accumulo/accumulo.pde,"@@ -0,0 +1,217 @@
+/* ","[{'comment': 'Should we add the ASF licence header to the Processing file `accumulo.pde`?', 'commenter': 'drewfarris'}, {'comment': 'Good idea..  added in 46bda7d8f4b23f', 'commenter': 'mikewalch'}]"
194,assemble/src/main/assemblies/component.xml,"@@ -40,15 +40,48 @@
         <include>org.apache.commons:commons-math3</include>
         <include>org.apache.commons:commons-vfs2</include>
         <include>org.apache.thrift:libthrift</include>
+        <include>org.eclipse.jetty:jetty-continuation</include>
         <include>org.eclipse.jetty:jetty-http</include>
         <include>org.eclipse.jetty:jetty-io</include>
         <include>org.eclipse.jetty:jetty-security</include>
         <include>org.eclipse.jetty:jetty-server</include>
+        <!-- jetty-servlet only needed by old monitor -->
         <include>org.eclipse.jetty:jetty-servlet</include>
         <include>org.eclipse.jetty:jetty-util</include>
         <include>org.apache.htrace:htrace-core</include>
         <include>org.slf4j:slf4j-api</include>
         <include>org.slf4j:slf4j-log4j12</include>
+        <!-- Jersey/Jackson-based webservice -->","[{'comment': 'Jersey brings in a lot of dependencies that will have to be distributed with Accumulo.  While I think Jersey is great for unreleased software, I think the [Spark microframework](http://sparkjava.com/) would be a better fit for the Accumulo monitor.  Spark only relies on Jetty (which is already a dependency).  You could generate JSON using GSON (which is also already a dependency). Spark also has a freemarker template engine.  If you use Spark, new dependencies should only be limited to the following:\r\n\r\n```\r\n<include>com.sparkjava:spark-core</include>\r\n<include>com.sparkjava:spark-template-freemarker</include>\r\n<include>org.freemarker:freemarker</include>\r\n```\r\nIf you are interested in using Spark, I can send you some code examples.  With Spark, you might need to have separate REST & HTML routes but you can keep your code simple by wrapping your business logic in methods.', 'commenter': 'mikewalch'}, {'comment': ""Unfortunately, I disagree with @mikewalch on this point.\r\n\r\nI'd prefer jersey. As the JAX-RS (JSR 311 & JSR 339) reference implementation, it's the standard for webapps, widely available in many Linux distributions, community-established APIs, and easily ported to various containers. While we don't absolutely *have* to stick with Jersey, using the JAX-RS standards as closely as possible ensures we have the ability to switch out implementations with minimal code changes, as needed. I think that there's a compelling case to be made for sticking with standards.\r\n\r\nJackson is useful with Jersey for automatic generation of JSON and XML, and provides a lot of standard providers for serialization in Java webapps. I wouldn't want us to switch to using Gson, as that would probably require writing a custom provider.\r\n\r\nWhile I find sparkjava compelling for quickly built applications, I'm not convinced it's the best choice for us. Keep in mind, we will probably still have to support some of the custom servlet stuff regardless (the shell servlet cannot be converted to a REST interface, and we're using the base servlet to load classpath resources). I'd rather stick to the standards to start, so we can get the work done, and then consider switching frameworks later, after we've stripped out all the servlet clutter.\r\n\r\nIf your concern is about bundling, we could stop bundling in the tarball. But, a few extra jars, especially ones that are relatively small and standard, isn't really a big deal, in my opinion."", 'commenter': 'ctubbsii'}, {'comment': ""I looked at Spark vs Jersey packaging in Linux distributions and I agree with @ctubbsii's point that Jersey is a better choice for packaging.  Jersey is in Fedora and Ubuntu while Spark hasn't been packaged there yet."", 'commenter': 'mikewalch'}, {'comment': ""For fun, I might try to package sparkjava in Fedora. For the purposes of this effort, I think I'm more concerned about straying too far from established standards and running into novel problems to solve which would have been easily addressed with the more well-traveled path."", 'commenter': 'ctubbsii'}, {'comment': ""> Jersey brings in a lot of dependencies that will have to be distributed with Accumulo\r\n\r\nThis is a good point and one that I had punted on originally. I agree with Jersey being ideal as the reference implementation for jax-rs; however, depending on the size of what we actually need to include to make it work, I could be swayed. I'm not sure what this actually looks like transitively."", 'commenter': 'joshelser'}]"
194,server/monitor/src/main/resources/templates/org/apache/accumulo/monitor/rest/view/Overview/index.ftl,"@@ -0,0 +1,83 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the ""License""); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<html>
+  <head>
+    <title>${title} - Accumulo ${version}</title>
+    <#if refresh gt 0 ><meta http-equiv='refresh' content='${refresh}' /></#if>
+    <meta http-equiv='Content-Type' content='test/html""' />
+    <meta http-equiv='Content-Script-Type' content='text/javascript' />
+    <meta http-equiv='Content-Style-Type' content='text/css' />
+    <link rel='shortcut icon' type='image/jpg' href='/web/favicon.png' />
+    <link rel='stylesheet' type='text/css' href='/web/screen.css' media='screen' />
+    <script src='/web/functions.js' type='text/javascript'></script>
+
+    <!--[if lte IE 8]><script language=""javascript"" type=""text/javascript"" src=""/web/flot/excanvas.min.js""></script><![endif]-->
+    <script language=""javascript"" type=""text/javascript"" src=""/web/flot/jquery.js""></script>
+    <script language=""javascript"" type=""text/javascript"" src=""/web/flot/jquery.flot.js""></script>
+  </head>
+
+  <body>
+    <div id='content-wrapper'>
+      <div id='content'>
+        <div id='header'>
+          <div id='headertitle'>
+            <h1>${title}</h1>
+          </div>
+          <div id='subheader'>Instance&nbsp;Name:&nbsp;${instance_name}&nbsp;&nbsp;&nbsp;Version:&nbsp;${version}
+            <br><span class='smalltext'>Instance&nbsp;ID:&nbsp;${instance_id}</span>
+            <br><span class='smalltext'>${current_date}</span>
+          </div>
+        </div>
+
+        <#include ""/templates/sidebar.ftl"">
+
+        <div id='main' style='bottom:0'>
+        <#--","[{'comment': 'Commented out HTML should be removed', 'commenter': 'mikewalch'}, {'comment': 'This whole file might be removed/modified when the UI is completed, but thanks for pointing it out!', 'commenter': 'lstav'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/GrizzlyMonitorApplication.java,"@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest;
+
+public class GrizzlyMonitorApplication { // extends MonitorApplication {","[{'comment': 'Is this used anymore?', 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/AccumuloExceptionMapper.java,"@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest;
+
+import javax.ws.rs.core.Response;
+import javax.ws.rs.ext.ExceptionMapper;
+import javax.ws.rs.ext.Provider;
+
+import org.apache.commons.lang.exception.ExceptionUtils;
+
+/**
+ *","[{'comment': 'Nit: add some javadoc or remove the empty block-comment.', 'commenter': 'joshelser'}, {'comment': 'Will do, thanks for pointing it out.', 'commenter': 'lstav'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/JettyMonitorApplication.java,"@@ -0,0 +1,89 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.Instance;
+import org.apache.accumulo.server.AccumuloServerContext;
+import org.apache.accumulo.server.client.HdfsZooInstance;
+import org.apache.accumulo.server.conf.ServerConfigurationFactory;
+import org.eclipse.jetty.server.Server;
+import org.eclipse.jetty.server.ServerConnector;
+import org.glassfish.jersey.filter.LoggingFilter;
+import org.glassfish.jersey.jackson.JacksonFeature;
+import org.glassfish.jersey.jetty.JettyHttpContainerFactory;
+import org.glassfish.jersey.server.ResourceConfig;
+import org.glassfish.jersey.server.ServerProperties;
+import org.glassfish.jersey.server.mvc.MvcFeature;
+import org.glassfish.jersey.server.mvc.freemarker.FreemarkerMvcFeature;
+import org.glassfish.jersey.servlet.ServletProperties;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class JettyMonitorApplication extends MonitorApplication {
+  private static final Logger log = LoggerFactory.getLogger(JettyMonitorApplication.class);
+
+  public JettyMonitorApplication() {}
+
+  public void run() {
+    Instance instance = HdfsZooInstance.getInstance();
+    ServerConfigurationFactory config = new ServerConfigurationFactory(instance);
+    AccumuloServerContext serverContext = new AccumuloServerContext(config);
+
+    // Set the objects on the old monitor and start daemons to regularly poll the data
+    startDataDaemons(config, instance, serverContext);
+
+    final ResourceConfig rc = new ResourceConfig().register(FreemarkerMvcFeature.class).property(MvcFeature.TEMPLATE_BASE_PATH, ""/templates"")
+        .packages(""org.apache.accumulo.monitor.rest.api"", ""org.apache.accumulo.monitor.rest.resources"", ""org.apache.accumulo.monitor.rest.view"")
+        .property(ServerProperties.TRACING, ""ALL"").property(ServletProperties.FILTER_FORWARD_ON_404, ""true"")
+        .property(ServletProperties.FILTER_STATIC_CONTENT_REGEX, ""/web/.*"")
+        .register(new LoggingFilter(java.util.logging.Logger.getLogger(""JettyMonitorApplication""), true)).register(JacksonFeature.class)
+        .registerClasses(AccumuloExceptionMapper.class);
+
+    final URI serverUri = getServerUri();
+
+    Server server = JettyHttpContainerFactory.createServer(serverUri, rc);
+
+    int port = serverUri.getPort();
+    if (0 == port) {
+      port = ((ServerConnector) server.getConnectors()[0]).getLocalPort();
+    }
+
+    String hostname = serverUri.getHost();
+
+    log.info(""Server bound to "" + hostname + "":"" + port);
+
+    try {
+      advertiseHttpAddress(serverContext.getConnector().getInstance(), hostname, port);
+    } catch (AccumuloException | AccumuloSecurityException e) {
+      throw new RuntimeException(""Failed to connect to Accumulo"", e);
+    }
+
+    try {
+      System.in.read();","[{'comment': ""We'll need to create some lifecycle management code here. The `System.in.read()` was a quick hack to start/stop the server. Stopping the monitor can be done via `kill` (and the normal JVM shutdown handler can be used). So, likely, we'd just want a `while (true) { Thread.sleep(5000); }`, breaking on `InterruptedException`."", 'commenter': 'joshelser'}, {'comment': ""I didn't do much work on this package as a whole since I think most of it will change when we substitute the current monitor with the REST one, I left this pretty much untouched since I wanted to make sure I had the REST calls first."", 'commenter': 'lstav'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/MonitorApplication.java,"@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+
+import java.net.URI;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.client.Instance;
+import org.apache.accumulo.core.util.Daemon;
+import org.apache.accumulo.core.zookeeper.ZooUtil;
+import org.apache.accumulo.fate.util.LoggingRunnable;
+import org.apache.accumulo.fate.util.UtilWaitThread;
+import org.apache.accumulo.fate.zookeeper.ZooUtil.NodeExistsPolicy;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.ZooKeeperStatus;
+import org.apache.accumulo.server.AccumuloServerContext;
+import org.apache.accumulo.server.conf.ServerConfigurationFactory;
+import org.apache.accumulo.server.zookeeper.ZooReaderWriter;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.net.HostAndPort;
+
+/**
+ *
+ */
+public abstract class MonitorApplication implements Runnable {
+  private static final Logger log = LoggerFactory.getLogger(MonitorApplication.class);
+
+  protected URI getServerUri() {
+    return URI.create(""http://localhost:50096/accumulo"");","[{'comment': 'The hostname and port to bind to needs to be pulled from configuration.', 'commenter': 'joshelser'}, {'comment': 'See comment from above diff.', 'commenter': 'lstav'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/AddlInformation.java,"@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class AddlInformation {","[{'comment': 'Nit: What is this class used for? Add some javadoc, please.', 'commenter': 'joshelser'}, {'comment': ""I'll add the Javadoc, but this and some other classes similar to this are used to specify the JSON object of an Array, or as the root name for an object (Tablet Servers for example). I didn't find a better way to do it, if you know of one, I would appreciate if you point me in the right direction."", 'commenter': 'lstav'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/BadTabletServerInformation.java,"@@ -0,0 +1,32 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+import javax.xml.bind.annotation.XmlAttribute;
+
+public class BadTabletServerInformation {
+
+  @XmlAttribute","[{'comment': 'Does the `@XmlAttribute` need to be on the other pojos?', 'commenter': 'joshelser'}, {'comment': '`@XmlAttribute` is used for the XML representation, and it is only used in places where a specific tag needs an attribute (i.e. each Tablet Server has an ID as part of the XML tag attributes), so not all the POJOs need it.', 'commenter': 'lstav'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/AllTimeTabletResults.java,"@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+public class AllTimeTabletResults {
+
+  public String operation;","[{'comment': 'Would prefer to see these be `private` with setters and getters. I believe Jackson is smart enough to use either the member fields or the appropriately named setters and getters.', 'commenter': 'joshelser'}, {'comment': ""@ctubbsii and I discussed about using the `public` variable vs `private` with setters and getters, and we decided to use `public` variables, with the plus that it decreases the number of lines it takes to write each call. I was following what you had with `private` variables at first before talking with @ctubbsii, is there a reason we shouldn't go with `public` variables?"", 'commenter': 'lstav'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/DeadLoggerInformation.java,"@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+import javax.xml.bind.annotation.XmlAttribute;
+
+public class DeadLoggerInformation {","[{'comment': ""I would guess that this is unnecessary. We haven't had loggers since Accumulo 1.4."", 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/DeadLoggerList.java,"@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class DeadLoggerList {","[{'comment': 'Ditto, probably unnecessary.', 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/RecentTracesInformation.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+import org.apache.accumulo.tracer.thrift.RemoteSpan;
+
+public class RecentTracesInformation {
+
+  public String type;
+  public Long avg;
+
+  public int total = 0;
+
+  public long min = Long.MAX_VALUE, max = Long.MIN_VALUE;","[{'comment': 'Nit: stylistically, please place these on their own line.', 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/RecentTracesInformation.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+import org.apache.accumulo.tracer.thrift.RemoteSpan;
+
+public class RecentTracesInformation {
+
+  public String type;
+  public Long avg;
+
+  public int total = 0;
+
+  public long min = Long.MAX_VALUE, max = Long.MIN_VALUE;
+  private long totalMS = 0l;
+  public long histogram[] = new long[] {0l, 0l, 0l, 0l, 0l, 0l};
+
+  public RecentTracesInformation() {}
+
+  public RecentTracesInformation(String type) {
+    this.type = type;
+  }
+
+  public void addSpan(RemoteSpan span) {","[{'comment': 'Should this be synchronized?', 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/TableInformation.java,"@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+import org.apache.accumulo.core.master.thrift.TableInfo;
+
+public class TableInformation {
+
+  public String tablename, tableId, tableState;
+
+  public int tablets, onlineTablets;
+  public long recs, recsInMemory;
+
+  public double ingest, ingestByteRate, query, queryByteRate;
+
+  public CompactionsList majorCompactions, minorCompactions, scans;
+
+  private int queuedMajorCompactions, runningMajorCompactions, queuedMinorCompactions, runningMinorCompactions, queuedScans, runningScans;
+
+  public double entriesRead, entriesReturned;
+  public Double holdTime;
+
+  public int offlineTablets;
+
+  public TableInformation() {}
+
+  public TableInformation(String tableName, String tableId, String tableState) {
+    this.tablename = tableName;
+    this.tableId = tableId;
+    this.tableState = tableState;
+  }
+
+  public TableInformation(String tableName, String tableId, TableInfo info, Double holdTime, String tableState) {
+    this.tablename = tableName;
+    this.tableId = tableId;
+
+    this.tablets = info.tablets;
+    this.offlineTablets = info.tablets - info.onlineTablets;
+    this.onlineTablets = info.onlineTablets;
+
+    this.recs = info.recs;
+    this.recsInMemory = info.recsInMemory;
+
+    this.ingest = info.getIngestRate();
+    this.ingestByteRate = info.getIngestByteRate();
+
+    this.query = info.getQueryRate();
+    this.queryByteRate = info.getQueryByteRate();
+
+    this.entriesRead = info.scanRate;
+    this.entriesReturned = info.queryRate;
+
+    this.holdTime = holdTime;
+
+    if (null != info.scans) {
+      this.queuedScans = info.scans.queued;
+      this.runningScans = info.scans.running;
+    } else {
+      this.queuedScans = 0;","[{'comment': 'Does it make sense to initialize these to `0` instead of setting them in the else-branch?', 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/api/TabletServerInformation.java,"@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.api;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import javax.xml.bind.annotation.XmlAttribute;
+
+import org.apache.accumulo.core.master.thrift.RecoveryStatus;
+import org.apache.accumulo.core.master.thrift.TableInfo;
+import org.apache.accumulo.core.master.thrift.TabletServerStatus;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.server.util.TableInfoUtil;
+
+public class TabletServerInformation {
+
+  @XmlAttribute(name = ""id"")
+  public String server;
+
+  public String hostname;
+  public long lastContact;
+  public double osload;
+
+  public CompactionsTypes compactions;
+
+  public int tablets;
+  public double ingest, query, ingestMB, queryMB;
+  public Integer scans; // For backwards compatibility, has same information as scansRunning
+  public Double scansessions;
+  public Double scanssessions; // For backwards compatibility
+  public long holdtime;
+
+  // New variables
+
+  public String ip;
+  private Integer scansRunning, scansQueued, minorRunning, minorQueued, majorRunning, majorQueued;
+  private CompactionsList scansCompacting, major, minor; // if scans is removed, change scansCompacting to scans
+  public long entries, lookups, indexCacheHits, indexCacheRequests, dataCacheHits, dataCacheRequests;
+  public double indexCacheHitRate, dataCacheHitRate;
+  public List<RecoveryStatusInformation> logRecoveries;
+
+  public TabletServerInformation() {}
+
+  public TabletServerInformation(TabletServerStatus thriftStatus) {
+    TableInfo summary = TableInfoUtil.summarizeTableStats(thriftStatus);
+    updateTabletServerInfo(thriftStatus, summary);
+  }
+
+  public void updateTabletServerInfo(TabletServerStatus thriftStatus, TableInfo summary) {
+
+    long now = System.currentTimeMillis();
+
+    this.server = this.ip = this.hostname = thriftStatus.name;
+    this.tablets = summary.tablets;","[{'comment': 'Maybe a check that TableInfo is non-null before using it the first time? `Objects.requireNonNull(summary).tablets` would be an easy modification.', 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/resources/GarbageCollectorResource.java,"@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.resources;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.gc.thrift.GCStatus;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.api.GarbageCollection;
+import org.apache.accumulo.monitor.rest.api.GarbageCollectorCycle;
+import org.apache.accumulo.monitor.rest.api.GarbageCollectorStatus;
+
+/**
+ * GarbageCollector metrics
+ */
+@Path(""/gc"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class GarbageCollectorResource {
+
+  @GET","[{'comment': 'I think the `@GET` could be lifted to the class-level and it would be implied on each method to remove some duplication.', 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/resources/ReplicationResource.java,"@@ -0,0 +1,193 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.resources;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.BatchScanner;
+import org.apache.accumulo.core.client.Connector;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.TableOfflineException;
+import org.apache.accumulo.core.client.admin.TableOperations;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.RootTable;
+import org.apache.accumulo.core.replication.ReplicationSchema.WorkSection;
+import org.apache.accumulo.core.replication.ReplicationTable;
+import org.apache.accumulo.core.replication.ReplicationTarget;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.api.ReplicationInformation;
+import org.apache.accumulo.server.replication.ReplicaSystem;
+import org.apache.accumulo.server.replication.ReplicaSystemFactory;
+import org.apache.hadoop.io.Text;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Path(""/replication"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})","[{'comment': 'I wonder if we should have a base resource class that we could put the `@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})` on once instead of duplicating it on each implementation.', 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/resources/ReplicationResource.java,"@@ -0,0 +1,193 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.resources;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.BatchScanner;
+import org.apache.accumulo.core.client.Connector;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.TableOfflineException;
+import org.apache.accumulo.core.client.admin.TableOperations;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.RootTable;
+import org.apache.accumulo.core.replication.ReplicationSchema.WorkSection;
+import org.apache.accumulo.core.replication.ReplicationTable;
+import org.apache.accumulo.core.replication.ReplicationTarget;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.api.ReplicationInformation;
+import org.apache.accumulo.server.replication.ReplicaSystem;
+import org.apache.accumulo.server.replication.ReplicaSystemFactory;
+import org.apache.hadoop.io.Text;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Path(""/replication"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class ReplicationResource {
+  private static final Logger log = LoggerFactory.getLogger(ReplicationResource.class);
+
+  @GET
+  public List<ReplicationInformation> getReplicationInformation() throws AccumuloException, AccumuloSecurityException {
+    final Connector conn = Monitor.getContext().getConnector();
+
+    final TableOperations tops = conn.tableOperations();
+
+    final Map<String,String> properties = conn.instanceOperations().getSystemConfiguration();
+    final Map<String,String> peers = new HashMap<>();
+    final String definedPeersPrefix = Property.REPLICATION_PEERS.getKey();
+    final ReplicaSystemFactory replicaSystemFactory = new ReplicaSystemFactory();
+
+    // Get the defined peers and what ReplicaSystem impl they're using
+    for (Entry<String,String> property : properties.entrySet()) {
+      String key = property.getKey();
+      // Filter out cruft that we don't want
+      if (key.startsWith(definedPeersPrefix) && !key.startsWith(Property.REPLICATION_PEER_USER.getKey())
+          && !key.startsWith(Property.REPLICATION_PEER_PASSWORD.getKey())) {
+        String peerName = property.getKey().substring(definedPeersPrefix.length());
+        ReplicaSystem replica;
+        try {
+          replica = replicaSystemFactory.get(property.getValue());
+        } catch (Exception e) {
+          log.warn(""Could not instantiate ReplicaSystem for {} with configuration {}"", property.getKey(), property.getValue(), e);
+          continue;
+        }
+
+        peers.put(peerName, replica.getClass().getName());
+      }
+    }
+
+    final String targetPrefix = Property.TABLE_REPLICATION_TARGET.getKey();
+
+    // The total set of configured targets
+    Set<ReplicationTarget> allConfiguredTargets = new HashSet<>();
+
+    // Number of files per target we have to replicate
+    Map<ReplicationTarget,Long> targetCounts = new HashMap<>();
+
+    Map<String,String> tableNameToId = tops.tableIdMap();
+    Map<String,String> tableIdToName = invert(tableNameToId);
+
+    for (String table : tops.list()) {
+      if (MetadataTable.NAME.equals(table) || RootTable.NAME.equals(table)) {
+        continue;
+      }
+      String localId = tableNameToId.get(table);
+      if (null == localId) {
+        log.trace(""Could not determine ID for {}"", table);
+        continue;
+      }
+
+      Iterable<Entry<String,String>> propertiesForTable;
+      try {
+        propertiesForTable = tops.getProperties(table);
+      } catch (TableNotFoundException e) {
+        log.warn(""Could not fetch properties for {}"", table, e);
+        continue;
+      }
+
+      for (Entry<String,String> prop : propertiesForTable) {
+        if (prop.getKey().startsWith(targetPrefix)) {
+          String peerName = prop.getKey().substring(targetPrefix.length());
+          String remoteIdentifier = prop.getValue();
+          ReplicationTarget target = new ReplicationTarget(peerName, remoteIdentifier, localId);
+
+          allConfiguredTargets.add(target);
+        }
+      }
+    }
+
+    // Read over the queued work
+    BatchScanner bs;
+    try {
+      bs = conn.createBatchScanner(ReplicationTable.NAME, Authorizations.EMPTY, 4);
+    } catch (TableOfflineException | TableNotFoundException e) {
+      log.error(""Could not read replication table"", e);","[{'comment': ""Looking back, this shouldn't be at `error`, more like trace."", 'commenter': 'joshelser'}]"
194,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/resources/TracesResource.java,"@@ -0,0 +1,356 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.resources;
+
+import static java.lang.Math.min;
+import static java.nio.charset.StandardCharsets.UTF_8;
+
+import java.io.IOException;
+import java.security.PrivilegedAction;
+import java.security.PrivilegedExceptionAction;
+import java.util.AbstractMap;
+import java.util.Collection;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.TreeMap;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.PathParam;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.Connector;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;
+import org.apache.accumulo.core.client.security.tokens.AuthenticationToken.Properties;
+import org.apache.accumulo.core.client.security.tokens.KerberosToken;
+import org.apache.accumulo.core.client.security.tokens.PasswordToken;
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.api.AddlInformation;
+import org.apache.accumulo.monitor.rest.api.AnnotationInformation;
+import org.apache.accumulo.monitor.rest.api.DataInformation;
+import org.apache.accumulo.monitor.rest.api.RecentTracesInformation;
+import org.apache.accumulo.monitor.rest.api.RecentTracesList;
+import org.apache.accumulo.monitor.rest.api.TraceInformation;
+import org.apache.accumulo.monitor.rest.api.TraceList;
+import org.apache.accumulo.monitor.rest.api.TraceType;
+import org.apache.accumulo.monitor.rest.api.TracesForTypeInformation;
+import org.apache.accumulo.monitor.servlets.trace.NullScanner;
+import org.apache.accumulo.server.client.HdfsZooInstance;
+import org.apache.accumulo.server.security.SecurityUtil;
+import org.apache.accumulo.tracer.SpanTree;
+import org.apache.accumulo.tracer.SpanTreeVisitor;
+import org.apache.accumulo.tracer.TraceDump;
+import org.apache.accumulo.tracer.TraceFormatter;
+import org.apache.accumulo.tracer.thrift.Annotation;
+import org.apache.accumulo.tracer.thrift.RemoteSpan;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.UserGroupInformation;
+
+@Path(""/trace"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class TracesResource {
+
+  @Path(""/summary/{minutes}"")
+  @GET
+  public RecentTracesList getTraces(@PathParam(""minutes"") int minutes) throws Exception {
+
+    RecentTracesList recentTraces = new RecentTracesList();
+
+    Entry<Scanner,UserGroupInformation> pair = getScanner();
+    final Scanner scanner = pair.getKey();
+    if (scanner == null) {
+      return null;
+    }
+
+    Range range = getRangeForTrace(minutes);
+    scanner.setRange(range);
+
+    final Map<String,RecentTracesInformation> summary = new TreeMap<>();
+    if (null != pair.getValue()) {
+      pair.getValue().doAs(new PrivilegedAction<Void>() {
+        @Override
+        public Void run() {
+          parseSpans(scanner, summary);
+          return null;
+        }
+      });
+    } else {
+      parseSpans(scanner, summary);
+    }
+
+    for (Entry<String,RecentTracesInformation> entry : summary.entrySet()) {
+      RecentTracesInformation stat = entry.getValue();
+      recentTraces.addTrace(stat);
+    }
+
+    return recentTraces;
+  }
+
+  @Path(""/listType/{type}/{minutes}"")
+  @GET
+  public TraceType getTracesType(@PathParam(""type"") String type, @PathParam(""minutes"") int minutes) throws Exception {
+
+    TraceType typeTraces = new TraceType(type);
+
+    Entry<Scanner,UserGroupInformation> pair = getScanner();
+    final Scanner scanner = pair.getKey();
+    if (scanner == null) {
+      return null;
+    }
+
+    Range range = getRangeForTrace(minutes);
+
+    scanner.setRange(range);
+
+    if (null != pair.getValue()) {
+      pair.getValue().doAs(new PrivilegedAction<Void>() {
+        @Override
+        public Void run() {
+          for (Entry<Key,Value> entry : scanner) {
+            RemoteSpan span = TraceFormatter.getRemoteSpan(entry);
+
+            if (span.description.equals(type)) {
+              typeTraces.addTrace(new TracesForTypeInformation(span));
+            }
+          }
+          return null;
+        }
+      });
+    } else {
+      for (Entry<Key,Value> entry : scanner) {
+        RemoteSpan span = TraceFormatter.getRemoteSpan(entry);
+        if (span.description.equals(type)) {
+          typeTraces.addTrace(new TracesForTypeInformation(span));
+        }
+
+      }
+    }
+
+    return typeTraces;
+  }
+
+  @Path(""/show/{id}"")
+  @GET
+  public TraceList getTracesType(@PathParam(""id"") String id) throws Exception {
+    TraceList traces = new TraceList(id);
+
+    if (id == null) {
+      return null;
+    }
+
+    Entry<Scanner,UserGroupInformation> entry = getScanner();
+    final Scanner scanner = entry.getKey();
+    if (scanner == null) {
+      return null;
+    }
+
+    Range range = new Range(new Text(id));
+    scanner.setRange(range);
+    final SpanTree tree = new SpanTree();
+    long start;
+
+    if (null != entry.getValue()) {
+      start = entry.getValue().doAs(new PrivilegedAction<Long>() {
+        @Override
+        public Long run() {
+          return addSpans(scanner, tree, Long.MAX_VALUE);
+        }
+      });
+    } else {
+      start = addSpans(scanner, tree, Long.MAX_VALUE);
+    }
+
+    final long finalStart = start;
+    Set<Long> visited = tree.visit(new SpanTreeVisitor() {
+      @Override
+      public void visit(int level, RemoteSpan parent, RemoteSpan node, Collection<RemoteSpan> children) {
+        traces.addTrace(addTraceInformation(level, node, finalStart));
+      }
+    });
+    tree.nodes.keySet().removeAll(visited);
+    if (!tree.nodes.isEmpty()) {
+      for (RemoteSpan span : TraceDump.sortByStart(tree.nodes.values())) {
+        traces.addTrace(addTraceInformation(0, span, finalStart));
+      }
+    }
+
+    return traces;
+  }
+
+  private static TraceInformation addTraceInformation(int level, RemoteSpan node, long finalStart) {
+
+    boolean hasData = node.data != null && !node.data.isEmpty();
+    boolean hasAnnotations = node.annotations != null && !node.annotations.isEmpty();
+
+    AddlInformation addlData = new AddlInformation();
+
+    if (hasData || hasAnnotations) {","[{'comment': 'This is unnecessary.', 'commenter': 'joshelser'}]"
196,core/src/main/java/org/apache/accumulo/core/iterators/IteratorUtil.java,"@@ -235,18 +235,27 @@ private static void mergeOptions(Map<String,Map<String,String>> ssio, Map<String
   public static <K extends WritableComparable<?>,V extends Writable> SortedKeyValueIterator<K,V> loadIterators(IteratorScope scope,
       SortedKeyValueIterator<K,V> source, KeyExtent extent, AccumuloConfiguration conf, List<IterInfo> ssiList, Map<String,Map<String,String>> ssio,
       IteratorEnvironment env, boolean useAccumuloClassLoader) throws IOException {
-    List<IterInfo> iters = new ArrayList<>(ssiList);
-    Map<String,Map<String,String>> allOptions = new HashMap<>();
-    parseIteratorConfiguration(scope, iters, ssio, allOptions, conf);
-    return loadIterators(source, iters, allOptions, env, useAccumuloClassLoader, conf.get(Property.TABLE_CLASSPATH));
+
+    return loadIteratorsHelper(scope, source, extent, conf, ssiList, ssio, env, useAccumuloClassLoader, null);
   }
 
   public static <K extends WritableComparable<?>,V extends Writable> SortedKeyValueIterator<K,V> loadIterators(IteratorScope scope,
       SortedKeyValueIterator<K,V> source, KeyExtent extent, AccumuloConfiguration conf, List<IterInfo> ssiList, Map<String,Map<String,String>> ssio,
       IteratorEnvironment env, boolean useAccumuloClassLoader, String classLoaderContext) throws IOException {
+
+    return loadIteratorsHelper(scope, source, extent, conf, ssiList, ssio, env, useAccumuloClassLoader, classLoaderContext);
+  }
+
+  private static <K extends WritableComparable<?>,V extends Writable> SortedKeyValueIterator<K,V> loadIteratorsHelper(IteratorScope scope,
+      SortedKeyValueIterator<K,V> source, KeyExtent extent, AccumuloConfiguration conf, List<IterInfo> ssiList, Map<String,Map<String,String>> ssio,
+      IteratorEnvironment env, boolean useAccumuloClassLoader, String classLoaderContext) throws IOException {
+
     List<IterInfo> iters = new ArrayList<>(ssiList);
     Map<String,Map<String,String>> allOptions = new HashMap<>();
     parseIteratorConfiguration(scope, iters, ssio, allOptions, conf);
+    if (classLoaderContext == null) {","[{'comment': 'I think this would make sense to leave in the callers. I\'m not a fan of null values having implicit ""logic"" behind them.', 'commenter': 'joshelser'}]"
196,core/src/main/java/org/apache/accumulo/core/iterators/IteratorUtil.java,"@@ -235,18 +235,27 @@ private static void mergeOptions(Map<String,Map<String,String>> ssio, Map<String
   public static <K extends WritableComparable<?>,V extends Writable> SortedKeyValueIterator<K,V> loadIterators(IteratorScope scope,
       SortedKeyValueIterator<K,V> source, KeyExtent extent, AccumuloConfiguration conf, List<IterInfo> ssiList, Map<String,Map<String,String>> ssio,
       IteratorEnvironment env, boolean useAccumuloClassLoader) throws IOException {
-    List<IterInfo> iters = new ArrayList<>(ssiList);
-    Map<String,Map<String,String>> allOptions = new HashMap<>();
-    parseIteratorConfiguration(scope, iters, ssio, allOptions, conf);
-    return loadIterators(source, iters, allOptions, env, useAccumuloClassLoader, conf.get(Property.TABLE_CLASSPATH));
+
+    return loadIteratorsHelper(scope, source, extent, conf, ssiList, ssio, env, useAccumuloClassLoader, null);","[{'comment': 'Related comment to below: would recommend switching the `null` back to `conf.get(Property.TABLE_CLASSPATH)`', 'commenter': 'joshelser'}, {'comment': 'Done :)', 'commenter': 'lstav'}]"
210,server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java,"@@ -194,10 +194,14 @@ public void grantSystemPermission(TInfo tinfo, TCredentials credentials, String
   }
 
   @Override
-  public void grantTablePermission(TInfo tinfo, TCredentials credentials, String user, String tableName, byte permission) throws ThriftSecurityException,
-      ThriftTableOperationException {
+  public void grantTablePermission(TInfo tinfo, TCredentials credentials, String user, String tableName, byte permission) throws TException {
     String tableId = checkTableId(instance, tableName, TableOperation.PERMISSION);
-    String namespaceId = Tables.getNamespaceId(instance, tableId);
+    String namespaceId;
+    try {
+      namespaceId = Tables.getNamespaceId(instance, tableId);
+    } catch (TableNotFoundException e) {
+      throw new TException(e);","[{'comment': 'I am not sure what the behavior of this is.  What will it cause to happen on the client side?', 'commenter': 'keith-turner'}, {'comment': 'Good question, I am not sure.  I will try and test this case to see what gets printed.', 'commenter': 'milleruntime'}]"
210,server/master/src/main/java/org/apache/accumulo/master/FateServiceHandler.java,"@@ -171,7 +171,12 @@ public String invalidMessage(String argument) {
         });
 
         String tableId = ClientServiceHandler.checkTableId(master.getInstance(), oldTableName, tableOp);
-        String namespaceId = Tables.getNamespaceId(master.getInstance(), tableId);
+        String namespaceId;","[{'comment': 'Could put these 5 lines in a method.  Also, I am thinking it should use NOT_FOUND instead of NAMEPSACE_NOTFOUND.', 'commenter': 'keith-turner'}, {'comment': 'Fixed in 5bd4b50', 'commenter': 'milleruntime'}]"
210,server/master/src/main/java/org/apache/accumulo/master/MasterClientServiceHandler.java,"@@ -112,7 +112,12 @@
 
   @Override
   public long initiateFlush(TInfo tinfo, TCredentials c, String tableId) throws ThriftSecurityException, ThriftTableOperationException {
-    String namespaceId = Tables.getNamespaceId(instance, tableId);
+    String namespaceId;","[{'comment': 'Could put these 6 lines in a method in the class.', 'commenter': 'keith-turner'}, {'comment': 'Fixed in 5bd4b50', 'commenter': 'milleruntime'}]"
210,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1320,9 +1356,9 @@ public void splitTablet(TInfo tinfo, TCredentials credentials, TKeyExtent tkeyEx
       String namespaceId;
       try {
         namespaceId = Tables.getNamespaceId(getInstance(), tableId);
-      } catch (IllegalArgumentException ex) {
-        // table does not exist, try to educate the client
-        throw new NotServingTabletException(tkeyExtent);
+      } catch (TableNotFoundException ex) {
+        // tableOperationsImpl catches ThriftSeccurityException and checks for missing table
+        throw new ThriftSecurityException(credentials.getPrincipal(), SecurityErrorCode.TABLE_DOESNT_EXIST);","[{'comment': 'Why switch from NotServingTabletException to ThriftSecurityException?', 'commenter': 'keith-turner'}, {'comment': 'The client method that calls TableOperationsImpl.addSplits() has a catch for ThriftSecurityException where it checks to see if the table exists.', 'commenter': 'milleruntime'}]"
210,server/base/src/main/java/org/apache/accumulo/server/conf/TableParentConfiguration.java,"@@ -35,6 +36,10 @@ public TableParentConfiguration(String tableId, Instance inst, AccumuloConfigura
 
   @Override
   protected String getNamespaceId() {
-    return Tables.getNamespaceId(inst, tableId);
+    try {
+      return Tables.getNamespaceId(inst, tableId);
+    } catch (TableNotFoundException e) {
+      return null;","[{'comment': 'why return null here?', 'commenter': 'keith-turner'}, {'comment': 'Perhaps this would be better: \r\nthrow new RuntimeException(e);', 'commenter': 'milleruntime'}, {'comment': '@ctubbsii maybe this class should not exist? Will the namespaceId every change?', 'commenter': 'milleruntime'}, {'comment': ""The class should not exist. I'm working on phasing it out as part of the changes for ACCUMULO-4050"", 'commenter': 'ctubbsii'}, {'comment': 'roger that... in the meantime I will just make it throw new RuntimeException(e);', 'commenter': 'milleruntime'}]"
210,server/master/src/main/java/org/apache/accumulo/master/tableOps/ExportTable.java,"@@ -27,12 +31,16 @@
 
   private final ExportInfo tableInfo;
 
-  public ExportTable(String tableName, String tableId, String exportDir) {
+  public ExportTable(String tableName, String tableId, String exportDir) throws ThriftTableOperationException {
     tableInfo = new ExportInfo();
     tableInfo.tableName = tableName;
     tableInfo.exportDir = exportDir;
     tableInfo.tableID = tableId;
-    tableInfo.namespaceID = Tables.getNamespaceId(HdfsZooInstance.getInstance(), tableId);
+    try {","[{'comment': 'Could refactor this to resolve the namespace id in FateServiceHandler (using the new method in FateServiceHandler) and just pass the namespaceId to the constructor.', 'commenter': 'keith-turner'}]"
210,server/master/src/main/java/org/apache/accumulo/master/tableOps/CloneTable.java,"@@ -44,12 +45,8 @@ public CloneTable(String user, String srcTableId, String tableName, Map<String,S
     Instance inst = HdfsZooInstance.getInstance();
     try {
       cloneInfo.srcNamespaceId = Tables.getNamespaceId(inst, cloneInfo.srcTableId);","[{'comment': 'could also resolve this in FateServiceHandler', 'commenter': 'keith-turner'}]"
210,server/master/src/main/java/org/apache/accumulo/master/MasterClientServiceHandler.java,"@@ -253,6 +253,16 @@ public void waitForFlush(TInfo tinfo, TCredentials c, String tableId, ByteBuffer
 
   }
 
+  private String getNamespaceId(TableOperation tableOp, String tableId) throws ThriftTableOperationException {","[{'comment': ' For consistency, could use the method name`getNamespaceIdFromTableId` like you did elsewhere.', 'commenter': 'keith-turner'}]"
210,server/master/src/main/java/org/apache/accumulo/master/FateServiceHandler.java,"@@ -232,7 +232,7 @@ public String invalidMessage(String argument) {
           propertiesToSet.put(entry.getKey(), entry.getValue());
         }
 
-        master.fate.seedTransaction(opid, new TraceRepo<>(new CloneTable(c.getPrincipal(), srcTableId, tableName, propertiesToSet, propertiesToExclude)),
+        master.fate.seedTransaction(opid, new TraceRepo<>(new CloneTable(c.getPrincipal(), srcTableId, tableName, propertiesToSet, propertiesToExclude, namespaceId)),","[{'comment': 'This is a style comment, feel free to ignore. For the recent FATE changes I made, when I added a namespaceId parameter I added it before the tableId.', 'commenter': 'keith-turner'}]"
220,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -192,6 +193,7 @@
   private ReplicationDriver replicationWorkDriver;
   private WorkDriver replicationWorkAssigner;
   RecoveryManager recoveryManager = null;
+  private AtomicLong replicationLatency = new AtomicLong(0l);","[{'comment': ""Personal preference: use upper-case `L` for Long literals. It's much easier to read (this looks like `01`)."", 'commenter': 'ctubbsii'}, {'comment': 'will fix', 'commenter': 'noedetore'}]"
220,server/master/src/main/java/org/apache/accumulo/master/replication/RemoveCompleteReplicationRecords.java,"@@ -57,9 +58,11 @@
   private static final Logger log = LoggerFactory.getLogger(RemoveCompleteReplicationRecords.class);
 
   private Connector conn;
+  private Master master;
 
-  public RemoveCompleteReplicationRecords(Connector conn) {
+  public RemoveCompleteReplicationRecords(Connector conn, Master master) {","[{'comment': 'Is this connector for the remote peer? If not, the master actually contains a client context and the connector is redundant if the master is being passed in.', 'commenter': 'ctubbsii'}, {'comment': 'bq. Is this connector for the remote peer?\r\n\r\nNo, it\'s for the ""local"" instance. This is just cleaning up stuff in the accumulo.replication table on the ""source"" instance.', 'commenter': 'joshelser'}]"
220,server/master/src/main/java/org/apache/accumulo/master/replication/RemoveCompleteReplicationRecords.java,"@@ -216,4 +220,13 @@ protected long removeRowIfNecessary(BatchWriter bw, SortedMap<Key,Value> columns
 
     return recordsRemoved;
   }
+
+  /*
+   * Metrics calls snapshot. Largest time stamp will be found for each snapshot then reset.
+   */
+  private void collectLatency(Long createdTime) {
+    long latency = System.currentTimeMillis() - createdTime;","[{'comment': '`System.currentTimeMillis()` is unreliable for tracking time differences, and this subtraction could even be negative. Use `System.nanoTime()` for time differences/durations. The difference is guaranteed to be positive.', 'commenter': 'ctubbsii'}, {'comment': 'I am not sure, but I think this createdTime may come from a persisted source?  @joshelser  is that right?  If its coming from a persisted source, then it could have come from another process.  In that case using `System.currentTimeMillis()` is reasonable.   `System.nanoTime()` can reliably compute time deltas within a process.', 'commenter': 'keith-turner'}, {'comment': 'Using nanoTime() is new to me. In making sure there is no conflict, I did find in org.apache.accumulo.tserver.log.TabletServerLogger setting createdTime\r\nStatus status = StatusUtil.fileCreated(System.currentTimeMillis());', 'commenter': 'noedetore'}, {'comment': ""I looked into this a bit more to see if using nanoTime would be reasonable across processes.  The following is from the nanoTime javaodc\r\n\r\n```\r\nReturns the current value of the running Java Virtual Machine's high-resolution time source, in nanoseconds. This method can only be used to measure elapsed time and is not related to any other notion of system or wall-clock time.\r\n```\r\n\r\nGiven this documentation, if the times are persisted and used across multiple processes then I think using `nanoTime()` would be inappropriate. "", 'commenter': 'keith-turner'}, {'comment': ""> I am not sure, but I think this createdTime may come from a persisted source\r\n\r\nI believe createdTime comes from the TabletServer, likely calling currentTimeMillis(). As long as we're reporting non-negative latencies, I think it will be fine (e.g. take the max of 0 and the computed latency).\r\n\r\nThe createdTime is coming from a remote JVM (the tserver), so I dont' think there's a need to use nanoTime here for the monotonically increasing time. We can just fudge it."", 'commenter': 'joshelser'}, {'comment': 'I like the idea of taking the max of 0 and computed latency.', 'commenter': 'keith-turner'}, {'comment': ""I guess... but there's really no reason to fudge when there's a built-in API for this in the JVM. I realize we're still using currentTimeMillis elsewhere, but I'm trying to encourage us to move away from that, for correctness."", 'commenter': 'ctubbsii'}, {'comment': ""Definitely don't use nanoTime across processes. It's only guaranteed that `(newtime - oldtime) > 0` within the same process."", 'commenter': 'ctubbsii'}]"
220,server/master/src/main/java/org/apache/accumulo/master/metrics/Metrics2ReplicationMetrics.java,"@@ -58,6 +59,7 @@ protected void snapshot() {
     registry.add(PENDING_FILES, getNumFilesPendingReplication());
     registry.add(NUM_PEERS, getNumConfiguredPeers());
     registry.add(MAX_REPLICATION_THREADS, getMaxReplicationThreads());
+    registry.add(LATENCY, getLatencyInSeconds());","[{'comment': 'You\'re collecting the ""maximum"" latency in the system which isn\'t ideal as it paints a very skewed picture of the actual system.\r\n\r\nFor example, if you have a single file which cannot be replicated (say, it\'s corrupted) it would have a very long latency. The rest of the files may be replicating in a (hypothetically) 5seconds. The maximum latency would continue to increase without bound, indicating to an operator that the system is not healthy, when the system is actually 99% healthy.\r\n\r\nHadoop metrics2 exposes a histogram class for metrics which automatically aggregates min, median, avg, max as well as percentiles (e.g. p75, p85, p95, p98, p99).', 'commenter': 'joshelser'}, {'comment': 'Screen Shot 2017-02-23 at 9.31.06 AM shows the spike but still helpful.\r\n<img width=""1440"" alt=""screen shot 2017-02-23 at 9 31 06 am"" src=""https://cloud.githubusercontent.com/assets/5384960/23276030/ad6c1b94-f9d6-11e6-9e45-32233a1bd64b.png"">\r\n', 'commenter': 'noedetore'}, {'comment': ""Yes, not saying that having the maximum isn't helpful, but it paints a very skewed picture.\r\n\r\nWe have the primitives to collect and display metrics which paint a complete picture. I really think that we should use them instead of only displaying the maximum."", 'commenter': 'joshelser'}, {'comment': 'Also, now that I think of this, ""latency"" is not the right word to use. Replication latency should be the time in which some record (maybe file, for simplicity) was created to when it was replicated. The ""latency"" which you have here is not that -- it\'s the current maximum time from WAL creation to being the replication record being cleaned up. This maximum value includes time for Accumulo to clean up the record as well as the time when a WAL was created before it received any data for a table being replicated -- it\'s an over-estimation.', 'commenter': 'joshelser'}, {'comment': ""I agree there are better ways of getting a better metric. This approach is a 90%(maybe to generous) solution using the information that already exists to give an end user some bearing of confidence as to current replication latency. From a users perspective, they want a historical way to know what is the greatest latency they can expect their data to be replicated at a given point in time. Spikes help explain the possible -1% unhealthy replicated files. As in the picture above there are couple spikes, but they don't seem to paint a bad picture. I will look into the histogram class. Are there any points of reference in the code you can point me to?"", 'commenter': 'noedetore'}, {'comment': 'Ah, crap. It looks like I was confusing what exists in other Metrics libraries as primitives.\r\n\r\nHadoop Metrics2 supports gauges, counters, stats and quantiles. You would want to create `MutableQuantiles` and some `MutableGauges` from the `MetricsRegistry`. You can maintain the min, median, avg, and max as gauges, and then present the percentiles using the quantiles. Essentially, you\'d have to track both separately. Like you have here, you could just instrument this class to compute the current metrics instead of doing it ""in-band"" in `RCRR`.', 'commenter': 'joshelser'}, {'comment': 'Oh, this is why I was confused. HBase has its own MutableHistogram metrics classes.\r\n\r\nhttps://github.com/apache/hbase/blob/bb3d9ccd489fb64e3cb2020583935a393382a678/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableTimeHistogram.java\r\nhttps://github.com/apache/hbase/blob/bb3d9ccd489fb64e3cb2020583935a393382a678/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableSizeHistogram.java\r\nhttps://github.com/apache/hbase/blob/c64a1d199402d6bf2d6ff4168c00c756dcaa59e4/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableHistogram.java\r\n\r\nWe could consider lifting these to make your life easier.', 'commenter': 'joshelser'}]"
220,server/master/src/main/java/org/apache/accumulo/master/replication/RemoveCompleteReplicationRecords.java,"@@ -197,6 +200,7 @@ protected long removeRowIfNecessary(BatchWriter bw, SortedMap<Key,Value> columns
     mutations.add(m);
     for (Entry<String,Long> entry : tableToTimeCreated.entrySet()) {
       log.info(""Removing order mutation for table {} at {} for {}"", entry.getKey(), entry.getValue(), row.toString());
+      collectLatency(entry.getValue());","[{'comment': 'Building on the point above: you would want to collect the latencies of each outstanding file to be replicated, likely not within the execution path of the Replication internals (metrics should not affect the latency of the system).\r\n\r\nIt would be better to pull this logic out of `RCRR`, and add it into the Metrics2ReplicationMetrics, enumerating the files and computing the histogram of metrics during the metrics collection cycle.', 'commenter': 'joshelser'}]"
221,assemble/conf/log4j-monitor.properties,"@@ -36,6 +36,17 @@ log4j.appender.GUI=org.apache.accumulo.server.monitor.LogService
 log4j.appender.GUI.Keep=50
 log4j.appender.GUI.Threshold=WARN
 
+## Configures Audit logs which are OFF by default.","[{'comment': 'Does the monitor need this?', 'commenter': 'keith-turner'}, {'comment': ""@joshelser brought this up.  it's not needed. I will remove."", 'commenter': 'mikewalch'}]"
221,assemble/conf/log4j-service.properties,"@@ -15,42 +15,48 @@
 ","[{'comment': 'Would it make sense to add some comments about this files intended use by Accumulo?\r\n\r\nAlso, it may be helpful to mention that these are log4j 1.2.X config files, to help users find the appropriate documentation. ', 'commenter': 'keith-turner'}]"
221,assemble/conf/templates/accumulo-env.sh,"@@ -25,6 +25,21 @@
 # Variables that must be set
 ############################
 
+## Accumulo logs directory. Referenced by logger config.
+export ACCUMULO_LOG_DIR=""${ACCUMULO_LOG_DIR:-$ACCUMULO_HOME/logs}""
+## Hadoop installation
+export HADOOP_PREFIX=""${HADOOP_PREFIX:-/path/to/hadoop}""
+## Hadoop configuration
+export HADOOP_CONF_DIR=""${HADOOP_CONF_DIR:-${HADOOP_PREFIX}/etc/hadoop}""
+## Zookeeper installation
+export ZOOKEEPER_HOME=""${ZOOKEEPER_HOME:-/path/to/zookeeper}""
+## See HADOOP-7154 and ACCUMULO-847
+export MALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-1}","[{'comment': 'Should this be under the ""Variables that must be set"" section? I wouldn\'t think the average user needs to set MALLOC_ARENA_MAX..', 'commenter': 'joshelser'}, {'comment': 'I will fix', 'commenter': 'mikewalch'}, {'comment': 'Maybe not ""must be set"", but certainly strongly recommended, even for the average user. It\'s not a high-performance tweak; it\'s a workaround for bad memory management in glibc. That can impact average users, too. Not sure if this is still needed post-RHEL6, though.', 'commenter': 'ctubbsii'}, {'comment': '> Maybe not ""must be set"", but certainly strongly recommended\r\n\r\nRight, but can\'t we provide a reasonable default? I\'m comparing it to the other ""you must set"" properties -- it\'s not something that is extremely dependent on the user\'s environment. We can default it to `1` which should be good enough. Maybe I am mis-remembering this property?', 'commenter': 'joshelser'}]"
221,start/src/test/java/org/apache/accumulo/start/util/AsyncSocketAppenderTest.java,"@@ -36,7 +31,7 @@
 
   @Before
   public void setUp() throws Exception {
-    sa = createMock(SocketAppender.class);
+    sa = EasyMock.createMock(SocketAppender.class);","[{'comment': 'Why make all of the changes in this file to remove the static imports? Seems unnecessary.', 'commenter': 'joshelser'}, {'comment': 'My IDE was having problems with the static imports.  Not sure why.  I could have looked into it more but I am not a fan of static imports of methods like that as it makes the code harder to read.  Therefore, I removed them. ', 'commenter': 'mikewalch'}]"
221,server/base/src/main/java/org/apache/accumulo/server/Accumulo.java,"@@ -108,69 +105,20 @@ public static synchronized Path getAccumuloInstanceIdPath(VolumeManager fs) {
     return ServerConstants.getInstanceIdLocation(v);
   }
 
-  /**
-   * Finds the best log4j configuration file. A generic file is used only if an application-specific file is not available. An XML file is preferred over a
-   * properties file, if possible.
-   *
-   * @param confDir
-   *          directory where configuration files should reside
-   * @param application
-   *          application name for configuration file name
-   * @return configuration file name
-   */
-  static String locateLogConfig(String confDir, String application) {
-    String explicitConfigFile = System.getProperty(""log4j.configuration"");
-    if (explicitConfigFile != null) {
-      return explicitConfigFile;
-    }
-    String[] configFiles = {String.format(""%s/%s_logger.xml"", confDir, application), String.format(""%s/%s_logger.properties"", confDir, application),
-        String.format(""%s/examples/%s_logger.xml"", confDir, application), String.format(""%s/examples/%s_logger.properties"", confDir, application),
-        String.format(""%s/generic_logger.xml"", confDir), String.format(""%s/generic_logger.properties"", confDir),
-        String.format(""%s/examples/generic_logger.xml"", confDir), String.format(""%s/examples/generic_logger.properties"", confDir),};
-    String defaultConfigFile = String.format(""%s/examples/generic_logger.xml"", confDir);
-    for (String f : configFiles) {
-      if (new File(f).exists()) {
-        return f;
-      }
-    }
-    return defaultConfigFile;
-  }
-
-  public static void setupLogging(String application) throws UnknownHostException {
-    System.setProperty(""org.apache.accumulo.core.application"", application);
-
-    if (System.getenv(""ACCUMULO_LOG_DIR"") != null) {
-      System.setProperty(""org.apache.accumulo.core.dir.log"", System.getenv(""ACCUMULO_LOG_DIR""));
-    }
-
-    String localhost = InetAddress.getLocalHost().getHostName();
-    System.setProperty(""org.apache.accumulo.core.ip.localhost.hostname"", localhost);
-
-    // Use a specific log config, if it exists
-    String logConfigFile = locateLogConfig(System.getenv(""ACCUMULO_CONF_DIR""), application);
-    // Turn off messages about not being able to reach the remote logger... we protect against that.
-    LogLog.setQuietMode(true);
-
-    // Set up local file-based logging right away
-    Log4jConfiguration logConf = new Log4jConfiguration(logConfigFile);
-    logConf.resetLogger();
-  }
-
   public static void init(VolumeManager fs, ServerConfigurationFactory serverConfig, String application) throws IOException {
     final AccumuloConfiguration conf = serverConfig.getConfiguration();
     final Instance instance = serverConfig.getInstance();
 
-    // Use a specific log config, if it exists
-    final String logConfigFile = locateLogConfig(System.getenv(""ACCUMULO_CONF_DIR""), application);
-
-    // Set up polling log4j updates and log-forwarding using information advertised in zookeeper by the monitor
-    MonitorLog4jWatcher logConfigWatcher = new MonitorLog4jWatcher(instance.getInstanceID(), logConfigFile);
-    logConfigWatcher.setDelay(5000L);
-    logConfigWatcher.start();
-
-    // Makes sure the log-forwarding to the monitor is configured
-    int[] logPort = conf.getPort(Property.MONITOR_LOG4J_PORT);
-    System.setProperty(""org.apache.accumulo.core.host.log.port"", Integer.toString(logPort[0]));
+    String logConfigFile = System.getProperty(""log4j.configuration"");
+    if (logConfigFile != null) {
+      if (logConfigFile.startsWith(""file:"")) {","[{'comment': 'Do we have to parse this ourselves? Is there some class that would parse it for us?', 'commenter': 'joshelser'}, {'comment': ""I don't know of a class that will do it for you.  I googled it and couldn't find one."", 'commenter': 'mikewalch'}]"
221,assemble/conf/log4j-service.properties,"@@ -15,42 +15,48 @@
 
 # Write out everything at the DEBUG level to the debug log
 log4j.appender.A2=org.apache.log4j.RollingFileAppender
-log4j.appender.A2.File=${org.apache.accumulo.core.dir.log}/${org.apache.accumulo.core.application}${accumulo.service.instance}_${org.apache.accumulo.core.ip.localhost.hostname}_fromprops.debug.log
-log4j.appender.A2.MaxFileSize=1000MB
-log4j.appender.A2.MaxBackupIndex=10
+log4j.appender.A2.File=${accumulo.log.dir}/${accumulo.application}${accumulo.service.instance}_${accumulo.local.hostname}.debug.log
+log4j.appender.A2.MaxFileSize=100MB
+log4j.appender.A2.MaxBackupIndex=5
 log4j.appender.A2.Threshold=DEBUG
 log4j.appender.A2.layout=org.apache.log4j.PatternLayout
 log4j.appender.A2.layout.ConversionPattern=%d{ISO8601} [%-8c{2}] %-5p: %m%n
 
 # Write out INFO and higher to the regular log
 log4j.appender.A3=org.apache.log4j.RollingFileAppender
-log4j.appender.A3.File=${org.apache.accumulo.core.dir.log}/${org.apache.accumulo.core.application}${accumulo.service.instance}_${org.apache.accumulo.core.ip.localhost.hostname}_fromprops.log
-log4j.appender.A3.MaxFileSize=1000MB
-log4j.appender.A3.MaxBackupIndex=10
+log4j.appender.A3.File=${accumulo.log.dir}/${accumulo.application}${accumulo.service.instance}_${accumulo.local.hostname}.log
+log4j.appender.A3.MaxFileSize=100MB
+log4j.appender.A3.MaxBackupIndex=5","[{'comment': 'Why change the default maxFileSize and number of backups?', 'commenter': 'joshelser'}, {'comment': ""They were set to keep 20GB (10G for log & 10G for debug.log) of logs for each Accumulo service (tserver, master, gc, etc).  I understand that a user might want to do that on a beefy production node but I don't think it should be the default for users out of the box.  I have run into the problem of disks filling up due to logs when running a SSD VMs in AWS if these defaults are used.  This change will keep up to 1GB of logs per service on each node."", 'commenter': 'mikewalch'}, {'comment': ""I think the reduction in MaxFileSize is OK but only having 5 for MaxBackupIndex files seems small.  That isn't even a full week.  You could keep it at 10 since you are already reducing the size."", 'commenter': 'milleruntime'}, {'comment': ""Ok, sounds good. Just wanted to call out the change since it wasn't included in the commit msg."", 'commenter': 'joshelser'}, {'comment': '@milleruntime, I change MaxBackupIndex back to 10 but RollingFileAppender rolls when MaxFileSize is reached.  There is a DailyRollingFileAppender used for audit log that rolls daily.  Logs will be rolled off by size rather than time so it depends on how much you are sending to logs. ', 'commenter': 'mikewalch'}, {'comment': 'You are right, I was thinking of DailyRollingFileAppender, thanks.  10 x 100MB = 1G for each I think is reasonable', 'commenter': 'milleruntime'}]"
221,assemble/conf/log4j-monitor.properties,"@@ -36,6 +36,17 @@ log4j.appender.GUI=org.apache.accumulo.server.monitor.LogService
 log4j.appender.GUI.Keep=50
 log4j.appender.GUI.Threshold=WARN
 
+## Configures Audit logs which are OFF by default.
+#log4j.appender.AUDIT=org.apache.log4j.DailyRollingFileAppender
+#log4j.appender.AUDIT.File=${accumulo.log.dir}/${accumulo.local.hostname}.audit
+#log4j.appender.AUDIT.DatePattern='.'yyyy-MM-dd
+#log4j.appender.AUDIT.layout=org.apache.log4j.PatternLayout
+#log4j.appender.AUDIT.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS/Z} [%c{2}] %-5p: %m%n
+#log4j.logger.org.apache.accumulo.audit=INFO, AUDIT
+#log4j.additivity.org.apache.accumulo.audit=false
+## Uncomment above and comment out line below to turn Audit logging ON
+log4j.logger.org.apache.accumulo.audit=OFF","[{'comment': 'Should this be included in log4j-monitor.properties? I would have only expected to see it in log4j-service.properties.', 'commenter': 'joshelser'}, {'comment': 'It looks like there is no auditing in the monitor so I will remove.  It can be added back if auditing is ever added.', 'commenter': 'mikewalch'}]"
221,assemble/conf/templates/accumulo-env.sh,"@@ -33,24 +48,22 @@ JAVA_OPTS=(""${ACCUMULO_JAVA_OPTS[@]}"" '-XX:+UseConcMarkSweepGC' '-XX:CMSInitiati
 ## JVM options set for individual applications
 case ""$ACCUMULO_CMD"" in
 master)  JAVA_OPTS=(""${JAVA_OPTS[@]}"" ${masterHigh_masterLow}) ;;
+monitor) JAVA_OPTS=(""${JAVA_OPTS[@]}"" ${monitorHigh_monitorLow}) ;;
 gc)      JAVA_OPTS=(""${JAVA_OPTS[@]}"" ${gcHigh_gcLow}) ;;
 tserver) JAVA_OPTS=(""${JAVA_OPTS[@]}"" ${tServerHigh_tServerLow}) ;;
-monitor) JAVA_OPTS=(""${JAVA_OPTS[@]}"" ${monitorHigh_monitorLow}) ;;
 shell)   JAVA_OPTS=(""${JAVA_OPTS[@]}"" ${shellHigh_shellLow}) ;;
 *)       JAVA_OPTS=(""${JAVA_OPTS[@]}"" ${otherHigh_otherLow}) ;;
 esac
-export JAVA_OPTS
 
-## Accumulo logs directory. Referenced by logger config.
-export ACCUMULO_LOG_DIR=""${ACCUMULO_LOG_DIR:-$ACCUMULO_HOME/logs}""
-## Hadoop installation
-export HADOOP_PREFIX=""${HADOOP_PREFIX:-/path/to/hadoop}""
-## Hadoop configuration
-export HADOOP_CONF_DIR=""${HADOOP_CONF_DIR:-${HADOOP_PREFIX}/etc/hadoop}""
-## Zookeeper installation
-export ZOOKEEPER_HOME=""${ZOOKEEPER_HOME:-/path/to/zookeeper}""
-## See HADOOP-7154 and ACCUMULO-847
-export MALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-1}
+## JVM options set for logging
+JAVA_OPTS=(""${JAVA_OPTS[@]}"" ""-Daccumulo.application=${ACCUMULO_CMD}"" ""-Daccumulo.log.dir=${ACCUMULO_LOG_DIR}"" ""-Daccumulo.local.hostname=$(hostname -s)"")","[{'comment': ""I'm not 100% sure off the top of my head, but shouldn't this be `hostname -f` for consistency with what we had before? I like using the FQDN (the canonical name) to refer to nodes as a general rule."", 'commenter': 'joshelser'}, {'comment': ""The 'accumulo.local.hostname' property is only used in the log4j properties files to set the hostname in file names for logs and to identify the host when logs are forwarded to the monitor.  I prefer the shorter hostname for these two cases but it's not a big issue for me and I will switch to `hostname -f` if you prefer."", 'commenter': 'mikewalch'}, {'comment': 'Ah, I could be swayed either way in that regard. Looking at a local install I have laying around, the FQDN is presently used:\r\n\r\n`master_hw10447.local.debug.log`, `master_hw10447.local.log`\r\n\r\nConsistency would probably be best.', 'commenter': 'joshelser'}, {'comment': 'Ok I will make it `hostname -f`', 'commenter': 'mikewalch'}, {'comment': ""The `man` page for `hostname` actually recommends avoiding the use of `-f`, because it may have multiple, or zero fully-qualified domain names, which is unreliable for how we're using it here. We probably just want `hostname`, without `-s`, because that's literally the local host name, unshortened. It'd be up to name service configuration to make sure this is consistent with the name on the network."", 'commenter': 'ctubbsii'}]"
221,docs/src/main/asciidoc/chapters/administration.txt,"@@ -1132,12 +1130,12 @@ can be exacerbated by resource constraints and clock drift.
 ==== Tested Versions
 Each release of Accumulo is built with a specific version of Apache
 Hadoop, Apache ZooKeeper and Apache Thrift.  We expect Accumulo to
-work with versions that are API compatable with those versions.
+work with versions that are API compatible with those versions.
 However this compatibility is not guaranteed because Hadoop, ZooKeeper
 and Thift may not provide guarantees between their own versions. We
 have also found that certain versions of Accumulo and Hadoop included
 bugs that greatly affected overall stability.  Thrift is particularly
-prone to compatablity changes between versions and you must use the
+prone to compatibility changes between versions and you must use the","[{'comment': 'A+ on spelling', 'commenter': 'milleruntime'}]"
221,server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java,"@@ -496,20 +497,27 @@ public void run(String hostname) {
       throw new RuntimeException(e);
     }
 
-    try {
-      log.debug(""Using "" + hostname + "" to advertise monitor location in ZooKeeper"");
-
-      String monitorAddress = HostAndPort.fromParts(hostname, server.getPort()).toString();
+    String advertiseHost = hostname;
+    if (advertiseHost.equals(""0.0.0.0"")) {
+      try {
+        advertiseHost = InetAddress.getLocalHost().getHostName();
+      } catch (UnknownHostException e) {
+        log.error(""Unable to get hostname"", e);
+      }
+    }
+    log.debug(""Using "" + advertiseHost + "" to advertise monitor location"");
 
+    try {
+      String monitorAddress = HostAndPort.fromParts(advertiseHost, server.getPort()).toString();
       ZooReaderWriter.getInstance().putPersistentData(ZooUtil.getRoot(instance) + Constants.ZMONITOR_HTTP_ADDR, monitorAddress.getBytes(UTF_8),
           NodeExistsPolicy.OVERWRITE);
       log.info(""Set monitor address in zookeeper to "" + monitorAddress);
     } catch (Exception ex) {
       log.error(""Unable to set monitor HTTP address in zookeeper"", ex);
     }
 
-    if (null != hostname) {
-      LogService.startLogListener(Monitor.getContext().getConfiguration(), instance.getInstanceID(), hostname);
+    if (null != advertiseHost) {
+      LogService.startLogListener(Monitor.getContext().getConfiguration(), instance.getInstanceID(), advertiseHost);","[{'comment': ""Wouldn't hurt to sync up with Luis on changes to the Monitor"", 'commenter': 'milleruntime'}]"
221,INSTALL.md,"@@ -83,7 +84,7 @@ must be provided.
    `accumulo-site.xml`.  If your namenode is running at 192.168.1.9:9000
    and you want to store data in `/accumulo` in HDFS, then set
   `instance.volumes` to `hdfs://192.168.1.9:9000/accumulo`.
- * **Location of Zoookeeper and Hadoop jars** :  Setting `ZOOKEEPER_HOME` and
+ * **Location of Zookeeper and Hadoop jars** :  Setting `ZOOKEEPER_HOME` and","[{'comment': 'I believe the correct capitalization is `ZooKeeper`. Someday, we should probably go through and make all our docs and method names consistent with this. I even saw one place the other day where we called it `zoo keeper` with a space!', 'commenter': 'ctubbsii'}]"
221,assemble/conf/log4j-monitor.properties,"@@ -15,18 +15,18 @@
 
 # Write out everything at the DEBUG level to the debug log
 log4j.appender.A2=org.apache.log4j.RollingFileAppender
-log4j.appender.A2.File=${org.apache.accumulo.core.dir.log}/${org.apache.accumulo.core.application}_${org.apache.accumulo.core.ip.localhost.hostname}_fromprops.debug.log
+log4j.appender.A2.File=${accumulo.log.dir}/${accumulo.application}_${accumulo.local.hostname}.debug.log","[{'comment': 'Where are all these properties being set? Are we to assume they will always be set in `accumulo-env.sh` (rather than being set by us in code)? If that\'s the case (and I hope it is... I don\'t want to start publishing system properties as public API), we should have a comment about where these properties come from, so that the user will understand that it\'s their responsibility to ensure they are set in their environment for this configuration to work.\r\n\r\nI also think this could be simpler... rather than having a huge concatenation of many system properties, all of which need to be set in the environment file, it\'d be better to have a single one which is set in the environment file. It\'s easier to document.\r\n\r\nExample:\r\n\r\nIn log4j config: `log4j.appender.A2.File=${accumulo.debug.log.filename}`\r\nIn accumulo-env.sh: `JAVA_OPTS=(... ""-Daccumulo.debug.log.filename=$file"")`', 'commenter': 'ctubbsii'}]"
221,assemble/conf/log4j-monitor.properties,"@@ -15,18 +15,18 @@
 
 # Write out everything at the DEBUG level to the debug log
 log4j.appender.A2=org.apache.log4j.RollingFileAppender
-log4j.appender.A2.File=${org.apache.accumulo.core.dir.log}/${org.apache.accumulo.core.application}_${org.apache.accumulo.core.ip.localhost.hostname}_fromprops.debug.log
+log4j.appender.A2.File=${accumulo.log.dir}/${accumulo.application}_${accumulo.local.hostname}.debug.log
 log4j.appender.A2.MaxFileSize=100MB
-log4j.appender.A2.MaxBackupIndex=10
+log4j.appender.A2.MaxBackupIndex=5
 log4j.appender.A2.Threshold=DEBUG
 log4j.appender.A2.layout=org.apache.log4j.PatternLayout
 log4j.appender.A2.layout.ConversionPattern=%d{ISO8601} [%-8c{2}] %-5p: %X{application} %m%n
 
 # Write out INFO and higher to the regular log
 log4j.appender.A3=org.apache.log4j.RollingFileAppender
-log4j.appender.A3.File=${org.apache.accumulo.core.dir.log}/${org.apache.accumulo.core.application}_${org.apache.accumulo.core.ip.localhost.hostname}_fromprops.log
+log4j.appender.A3.File=${accumulo.log.dir}/${accumulo.application}_${accumulo.local.hostname}.log","[{'comment': 'We definitely don\'t need both the ""regular log"" and ""debug log"" configured in the default configuration file. We could shorten, and dramatically simplify the default file by showing only one example, and indicating that appenders could be used with different settings. This makes the file easier to understand for users who wish to dive in and customize.', 'commenter': 'ctubbsii'}]"
221,assemble/conf/log4j-service.properties,"@@ -15,42 +15,48 @@
 
 # Write out everything at the DEBUG level to the debug log
 log4j.appender.A2=org.apache.log4j.RollingFileAppender
-log4j.appender.A2.File=${org.apache.accumulo.core.dir.log}/${org.apache.accumulo.core.application}${accumulo.service.instance}_${org.apache.accumulo.core.ip.localhost.hostname}_fromprops.debug.log
-log4j.appender.A2.MaxFileSize=1000MB
-log4j.appender.A2.MaxBackupIndex=10
+log4j.appender.A2.File=${accumulo.log.dir}/${accumulo.application}${accumulo.service.instance}_${accumulo.local.hostname}.debug.log
+log4j.appender.A2.MaxFileSize=100MB
+log4j.appender.A2.MaxBackupIndex=5
 log4j.appender.A2.Threshold=DEBUG
 log4j.appender.A2.layout=org.apache.log4j.PatternLayout
 log4j.appender.A2.layout.ConversionPattern=%d{ISO8601} [%-8c{2}] %-5p: %m%n
 
 # Write out INFO and higher to the regular log
 log4j.appender.A3=org.apache.log4j.RollingFileAppender
-log4j.appender.A3.File=${org.apache.accumulo.core.dir.log}/${org.apache.accumulo.core.application}${accumulo.service.instance}_${org.apache.accumulo.core.ip.localhost.hostname}_fromprops.log
-log4j.appender.A3.MaxFileSize=1000MB
-log4j.appender.A3.MaxBackupIndex=10
+log4j.appender.A3.File=${accumulo.log.dir}/${accumulo.application}${accumulo.service.instance}_${accumulo.local.hostname}.log
+log4j.appender.A3.MaxFileSize=100MB
+log4j.appender.A3.MaxBackupIndex=5
 log4j.appender.A3.Threshold=INFO
 log4j.appender.A3.layout=org.apache.log4j.PatternLayout
 log4j.appender.A3.layout.ConversionPattern=%d{ISO8601} [%-8c{2}] %-5p: %m%n
 
 # Send all logging data to a centralized logger
 # If the centralized logger is down, buffer the log events, but drop them if it stays down
-log4j.appender.ASYNC=org.apache.accumulo.core.util.AsyncSocketAppender
-log4j.appender.ASYNC.RemoteHost=${org.apache.accumulo.core.host.log}
-log4j.appender.ASYNC.Port=${org.apache.accumulo.core.host.log.port}
-log4j.appender.ASYNC.Application=${org.apache.accumulo.core.application}${accumulo.service.instance}:${org.apache.accumulo.core.ip.localhost.hostname}
+log4j.appender.ASYNC=org.apache.accumulo.start.util.AsyncSocketAppender","[{'comment': ""I have a replacement for the AsyncSocketAppender, in ACCUMULO-4409, but I'd prefer to make the change after your fixes get pushed."", 'commenter': 'ctubbsii'}]"
228,TESTING.md,"@@ -117,8 +117,8 @@ The following properties can be used to configure a standalone cluster:
 - `accumulo.it.cluster.standalone.zookeepers`, Required: ZooKeeper quorum used by the standalone cluster
 - `accumulo.it.cluster.standalone.instance.name`, Required: Accumulo instance name for the cluster
 - `accumulo.it.cluster.standalone.hadoop.conf`, Required: `HADOOP_CONF_DIR`","[{'comment': 'Change this one too if you changed the other two, please.', 'commenter': 'joshelser'}, {'comment': 'Fixed', 'commenter': 'mikewalch'}]"
228,TESTING.md,"@@ -117,8 +117,8 @@ The following properties can be used to configure a standalone cluster:
 - `accumulo.it.cluster.standalone.zookeepers`, Required: ZooKeeper quorum used by the standalone cluster
 - `accumulo.it.cluster.standalone.instance.name`, Required: Accumulo instance name for the cluster
 - `accumulo.it.cluster.standalone.hadoop.conf`, Required: `HADOOP_CONF_DIR`
-- `accumulo.it.cluster.standalone.home`, Optional: `ACCUMULO_HOME`
-- `accumulo.it.cluster.standalone.conf`, Optional: `ACCUMULO_CONF_DIR`
+- `accumulo.it.cluster.standalone.home`, Required: Accumulo home directory on cluster","[{'comment': 's/Accumulo home directory/Accumulo installation directory/ ?', 'commenter': 'joshelser'}, {'comment': 'Fixed', 'commenter': 'mikewalch'}]"
228,minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java,"@@ -57,7 +57,7 @@
 
   private Instance instance;
   private ClientConfiguration clientConf;
-  private String accumuloHome, clientAccumuloConfDir, serverAccumuloConfDir, hadoopConfDir;","[{'comment': ""There is a reason that both of these exist. Some tests do things that require knowledge of the `instance.secret` (generally, configuration values which are sensitive). Good deployment practices dictate that there is a separation here -- as such, I don't see how this simplification is helpful."", 'commenter': 'joshelser'}, {'comment': 'OK, I will bring back client and server config directories.  ', 'commenter': 'mikewalch'}]"
228,minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java,"@@ -123,11 +114,12 @@ public int exec(Class<?> clz, String[] args) throws IOException {
 
   public Entry<Integer,String> execMapreduceWithStdout(Class<?> clz, String[] args) throws IOException {
     String host = ""localhost"";
-    String[] cmd = new String[3 + args.length];
-    cmd[0] = getToolPath();
-    cmd[1] = getJarFromClass(clz);
-    cmd[2] = clz.getName();
-    for (int i = 0, j = 3; i < args.length; i++, j++) {
+    String[] cmd = new String[4 + args.length];
+    cmd[0] = getAccumuloUtilPath();
+    cmd[1] = ""hadoop-jar"";
+    cmd[2] = getJarFromClass(clz);
+    cmd[3] = clz.getName();","[{'comment': 'This is nice. How do this fail in the case of mis-configuration?', 'commenter': 'joshelser'}, {'comment': 'It fails just like the previous `tool.sh` command as `accumulo-util hadoop-jar` is the same underlying logic.', 'commenter': 'mikewalch'}, {'comment': ""Okie doke. Half of this was me wondering how it fails when the configuration is actually wrong (e.g. we can't find the hadoop installation/jars), but that isn't related to this changeset."", 'commenter': 'joshelser'}]"
228,minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java,"@@ -149,9 +141,8 @@ String getJarFromClass(Class<?> clz) {
 
   @Override
   public void adminStopAll() throws IOException {
-    File confDir = getConfDir();
-    String master = getHosts(new File(confDir, ""masters"")).get(0);
-    String[] cmd = new String[] {SUDO_CMD, ""-u"", user, ACCUMULO_CONF_DIR + serverAccumuloConfDir, accumuloPath, Admin.class.getName(), ""stopAll""};
+    String master = getHosts(MASTER_HOSTS_FILE).get(0);
+    String[] cmd = new String[] {SUDO_CMD, ""-u"", user, accumuloPath, Admin.class.getName(), ""stopAll""};","[{'comment': ""Ah, this is also a good example of what would be broken by your change.\r\n\r\nWe should not be requiring that a user be privileged to read the Accumulo services' configuration files."", 'commenter': 'joshelser'}, {'comment': 'Not sure about your comment.  It looks like it was previously set for user to read Accumulo services\' configuration files..\r\n\r\n```java\r\nString[] cmd = new String[] {SUDO_CMD, ""-u"", user, ACCUMULO_CONF_DIR + serverAccumuloConfDir, accumuloPath, Admin.class.getName(), ""stopAll""};\r\n```', 'commenter': 'mikewalch'}, {'comment': 'Same issue as above with dropping the server conf (so we can probably ignore it). `StopAll` is another utility which requires the correct `instance.secret`.', 'commenter': 'joshelser'}, {'comment': 'So you are not OK with the removal of `ACCUMULO_CONF_DIR + serverAccumuloConfDir`? Why do you need this? I am assuming the Accumulo installation has all server config in the default configuration directory.  We should be getting away from using `ACCUMULO_CONF_DIR` as it setting these env variables is very prone to errors. ', 'commenter': 'mikewalch'}, {'comment': '> So you are not OK with the removal of ACCUMULO_CONF_DIR + serverAccumuloConfDir\r\n\r\nOh, no, I\'m not because you\'re still breaking things. I didn\'t realize you didn\'t revert this change.\r\n\r\n> Why do you need this?\r\n\r\nSee my previous comment: ""StopAll is another utility which requires the correct instance.secret."". It\'s the same reason I -1\'ed the removal of the accumuloServerConf property in the first place..', 'commenter': 'joshelser'}, {'comment': ""> We should be getting away from using ACCUMULO_CONF_DIR as it setting these env variables is very prone to errors.\r\n\r\nThat's fine if you want to move away from them, but I don't see a solution presented as to how you don't break existing functionality :). As long as we have to keep the tracer user's password and instance.secret in accumulo-site.xml, we're stuck having a copy of those files which are not globally readable.\r\n\r\nPushing harder on use of the client.conf is one possibility but this is in a half-cocked state, IMO."", 'commenter': 'joshelser'}, {'comment': 'I see your point that a solution hasn\'t been presented yet.  I can add ACCUMULO_CONF_DIR back to this PR until there is one.  However, I don\'t think it\'s hard at this point to add a solution for separate client & server configuration directories that avoid use of ACCUMULO_CONF_DIR.\r\n\r\nAfter #223 is merged, the accumulo command will set up the java `CLASSPATH` using bash code below:\r\n```bash\r\nCLASSPATH=""${conf}:${lib}/*:${CLASSPATH}""\r\n```\r\n\r\n`${conf}` defaults to the `conf` directory in the Accumulo tarball installation but this can be overridden by downstream packaging to anything (i.e `/etc/accumulo`). This directory contains the server config. \r\n\r\nWe could also add some default client configuration directories after the server configuration directory. Something like below:\r\n  \r\n```bash\r\nCLASSPATH=""${conf}:${conf}/client:${HOME}/.accumulo:${lib}/*:${CLASSPATH}""\r\n```\r\nAny accumulo command would try to pull configuration from $conf before falling back to $conf/client, and then $HOME/.accumulo.  \r\n\r\nIf you are packaging downstream, you can even replace this line with other locations:\r\n\r\n```bash\r\nCLASSPATH=""${conf}:/etc/accumulo-client:${HOME}/.accumulo:${lib}/*:${CLASSPATH}""\r\n```\r\nDoes this work for you?\r\n', 'commenter': 'mikewalch'}, {'comment': ""Functionally, yes, I think that would work.\r\n\r\n> However, I don't think it's hard at this point to add a solution for separate client & server configuration directories that avoid use of ACCUMULO_CONF_DIR.\r\n\r\nYeah, it doesn't matter too much to me the means, just that the functionality exists.\r\n\r\n> ${conf} defaults to the conf directory in the Accumulo tarball installation\r\n\r\nOk, and that's extrapolated based on the assumption that the `bin/` dir is next to the `conf/` dir (or at least a symlink exists to enable that). Sounds fine.\r\n\r\nAs a general statement, I'm lamenting that task that this change will entail to update all of my downstream docs/code to make this work. This will be a very cross cutting change for me to have to make and will be a pain in the butt. Sadly, we don't have a good strategy for defining what compatibility is here (and what the 1.x to 2.x means).\r\n\r\nFYI @busbey as this may also affect things on the CM side.."", 'commenter': 'joshelser'}, {'comment': 'While all Java code and Accumulo documentation should remove use of `ACCUMULO_CONF_DIR`, I think we could support it for backwards compatibility in the scripts by replacing the following line: \r\n\r\n```bash\r\nexport conf=""${basedir}/conf""`\r\n``` \r\nin the `accumulo` & `accumulo-service` scripts with the following:\r\n\r\n```bash\r\nexport conf=""${ACCUMULO_CONF_DIR-${basedir}/conf}""\r\n```', 'commenter': 'mikewalch'}, {'comment': ""> I think we could support it for backwards compatibility in the scripts by replacing the following line:\r\n\r\nOh that would be wonderful. We can look into that separately (since it's not directly related to what you're doing here) if you'd like.."", 'commenter': 'joshelser'}]"
228,minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java,"@@ -170,9 +162,8 @@ public void adminStopAll() throws IOException {
   public void setGoalState(String goalState) throws IOException {
     requireNonNull(goalState, ""Goal state must not be null"");
     checkArgument(MasterGoalState.valueOf(goalState) != null, ""Unknown goal state: "" + goalState);
-    File confDir = getConfDir();
-    String master = getHosts(new File(confDir, ""masters"")).get(0);
-    String[] cmd = new String[] {SUDO_CMD, ""-u"", user, ACCUMULO_CONF_DIR + serverAccumuloConfDir, accumuloPath, SetGoalState.class.getName(), goalState};
+    String master = getHosts(MASTER_HOSTS_FILE).get(0);
+    String[] cmd = new String[] {SUDO_CMD, ""-u"", user, accumuloPath, SetGoalState.class.getName(), goalState};","[{'comment': 'Again, same thing as the above with the `stopAll`. Pretty sure this requires the correct `instance.secret`.', 'commenter': 'joshelser'}]"
228,TESTING.md,"@@ -142,11 +145,8 @@ what is executed for a standalone cluster.
   `mvn verify -Daccumulo.it.properties=/home/user/my_cluster.properties`
 
 For the optional properties, each of them will be extracted from the environment if not explicitly provided.
-Specifically, `ACCUMULO_HOME` and `ACCUMULO_CONF_DIR` are used to ensure the correct version of the bundled
-Accumulo scripts are invoked and, in the event that multiple Accumulo processes exist on the same physical machine,
-but for different instances, the correct version is terminated. `HADOOP_CONF_DIR` is used to ensure that the necessary
-files to construct the FileSystem object for the cluster can be constructed (e.g. core-site.xml and hdfs-site.xml),
-which is typically required to interact with HDFS.
+`HADOOP_CONF_DIR` is used to ensure that the necessary files to construct the FileSystem object for the cluster can","[{'comment': 'Could simplify this to just be ""additional classpath items"".', 'commenter': 'ctubbsii'}]"
228,assemble/bin/accumulo,"@@ -39,7 +39,7 @@ function main() {
   # Set up variables needed by accumulo-env.sh
   export bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
   export basedir=$( cd -P ""${bin}""/.. && pwd )
-  export conf=""${basedir}/conf""
+  export conf=""${ACCUMULO_CONF_DIR-${basedir}/conf}""","[{'comment': 'Is this supposed to be `:-` or just `-`? (same question in the repeated locations below)', 'commenter': 'ctubbsii'}, {'comment': ""Nice catch. It's supposed to be `:-`.  I will fix."", 'commenter': 'mikewalch'}]"
228,minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java,"@@ -123,11 +120,12 @@ public int exec(Class<?> clz, String[] args) throws IOException {
 
   public Entry<Integer,String> execMapreduceWithStdout(Class<?> clz, String[] args) throws IOException {
     String host = ""localhost"";
-    String[] cmd = new String[3 + args.length];
-    cmd[0] = getToolPath();
-    cmd[1] = getJarFromClass(clz);
-    cmd[2] = clz.getName();
-    for (int i = 0, j = 3; i < args.length; i++, j++) {
+    String[] cmd = new String[4 + args.length];
+    cmd[0] = getAccumuloUtilPath();
+    cmd[1] = ""hadoop-jar"";
+    cmd[2] = getJarFromClass(clz);
+    cmd[3] = clz.getName();
+    for (int i = 0, j = 4; i < args.length; i++, j++) {","[{'comment': 'This whole thing would be easier to read if it used ArrayList for cmd instead, and just appended to the end.\r\nAt the very least, we could protect against mistakes here, by setting `offset = 4` and using that to construct the array size (`new String[offset + args.length]`) and in this loop:\r\n```java\r\nfor (int i = offset; i < args.length; i++) {\r\n  cmd[i + offset] = ""\'"" + args[i] + ""\'"";\r\n}\r\n```', 'commenter': 'ctubbsii'}]"
228,minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java,"@@ -149,9 +147,8 @@ String getJarFromClass(Class<?> clz) {
 
   @Override
   public void adminStopAll() throws IOException {
-    File confDir = getConfDir();
-    String master = getHosts(new File(confDir, ""masters"")).get(0);
-    String[] cmd = new String[] {SUDO_CMD, ""-u"", user, ACCUMULO_CONF_DIR + serverAccumuloConfDir, accumuloPath, Admin.class.getName(), ""stopAll""};
+    String master = getHosts(MASTER_HOSTS_FILE).get(0);
+    String[] cmd = new String[] {SUDO_CMD, ""-u"", user, serverEnv, accumuloPath, Admin.class.getName(), ""stopAll""};","[{'comment': ""Not really sure I understand why we're doing anything with `sudo`. Escalating privileges for testing our code seems highly questionable. I realize this already existed, and you're not trying to address it in this issue, but editing the code around it has brought it to my attention, and I'm somewhat concerned about this."", 'commenter': 'ctubbsii'}, {'comment': ""Feel free to break this out. I'm not sure how else you think this could be worked around.."", 'commenter': 'joshelser'}, {'comment': ""I'm not sure what needs to be worked around. It's not clear to me why we would ever need to do that."", 'commenter': 'ctubbsii'}, {'comment': ""You're incorrectly assuming that the tests are always running as the user that is running Accumulo."", 'commenter': 'joshelser'}, {'comment': 'I wasn\'t assuming anything. I was observing my own ignorance about the reasons why this was being done. In any case, you answered the question I should have asked, instead (""Why is this being done?"").\r\n\r\nYour response indicates that the reason this is being done is so that this test framework can switch users to control an external running Accumulo instance, running as a specific user, assuming `sudo` is configured in a very particular way to allow running the necessary executables without a tty or authentication.\r\n\r\nI now understand why it uses `sudo`, but its inclusion suggests that this code exists not to test the upstream Accumulo project, but to test specific downstream products. That makes me a bit uncomfortable because I don\'t like the idea of the upstream project being burdened with the maintenance of tests for downstream vendors. However, I can also see it the other way around: that the upstream project has an interest in providing some test code to ensure any downstream packaging meets some minimum standards of quality so that Accumulo is well-represented downstream. So, I\'m not necessarily going to argue we should remove it.\r\n\r\nI would, however, argue that we ""genericize"" it a bit, so we don\'t have to directly support the use of privilege escalation features of an operating system for our tests. We can support that indirectly by allowing the user to supply the launch command. The direct use of `sudo` is mostly what makes me uncomfortable. I\'ve seen a lot of open source projects code, and this is the first time I\'ve ever seen that used in any project\'s test suite that wasn\'t `sudo` itself.', 'commenter': 'ctubbsii'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java,"@@ -461,25 +452,9 @@ public void run(String hostname) {
       try {
         log.debug(""Creating monitor on port "" + port);
         server = new EmbeddedWebServer(hostname, port);
-        server.addServlet(DefaultServlet.class, ""/"");
-        server.addServlet(OperationServlet.class, ""/op"");
-        server.addServlet(MasterServlet.class, ""/master"");
-        server.addServlet(TablesServlet.class, ""/tables"");
-        server.addServlet(TServersServlet.class, ""/tservers"");
-        server.addServlet(ProblemServlet.class, ""/problems"");
-        server.addServlet(GcStatusServlet.class, ""/gc"");
-        server.addServlet(LogServlet.class, ""/log"");
-        server.addServlet(XMLServlet.class, ""/xml"");
-        server.addServlet(JSONServlet.class, ""/json"");
-        server.addServlet(VisServlet.class, ""/vis"");
-        server.addServlet(ScanServlet.class, ""/scans"");
-        server.addServlet(BulkImportServlet.class, ""/bulkImports"");
-        server.addServlet(Summary.class, ""/trace/summary"");
-        server.addServlet(ListType.class, ""/trace/listType"");
-        server.addServlet(ShowTrace.class, ""/trace/show"");
-        server.addServlet(ReplicationServlet.class, ""/replication"");
-        if (server.isUsingSsl())
-          server.addServlet(ShellServlet.class, ""/shell"");
+        server.addServlet(getDefaultServlet(), ""/resources/*"");
+        server.addServlet(getRestServlet(), ""/rest/*"");","[{'comment': 'Nice reduction in servlets', 'commenter': 'mikewalch'}, {'comment': '+1', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/BasicServlet.java,"@@ -1,285 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.accumulo.monitor.servlets;
-
-import static java.nio.charset.StandardCharsets.UTF_8;
-
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.StringWriter;
-import java.io.UnsupportedEncodingException;
-import java.net.HttpURLConnection;
-import java.net.URLDecoder;
-import java.net.URLEncoder;
-import java.util.Date;
-import java.util.List;
-
-import javax.servlet.ServletException;
-import javax.servlet.http.Cookie;
-import javax.servlet.http.HttpServlet;
-import javax.servlet.http.HttpServletRequest;
-import javax.servlet.http.HttpServletResponse;
-
-import org.apache.accumulo.core.Constants;
-import org.apache.accumulo.core.conf.Property;
-import org.apache.accumulo.monitor.Monitor;
-import org.apache.accumulo.server.monitor.DedupedLogEvent;
-import org.apache.accumulo.server.monitor.LogService;
-import org.apache.log4j.Level;
-import org.apache.log4j.Logger;
-
-abstract public class BasicServlet extends HttpServlet {
-  public static final String STANDBY_MONITOR_MESSAGE = ""This is not the active Monitor"";
-
-  private static final long serialVersionUID = 1L;
-  protected static final Logger log = Logger.getLogger(BasicServlet.class);
-  private String bannerText;
-  private String bannerColor;
-  private String bannerBackground;
-
-  abstract protected String getTitle(HttpServletRequest req);
-
-  public boolean isActiveMonitor() {
-    // If the HighlyAvailableService is not initialized or it's not the active service, throw an exception
-    // to prevent processing of the servlet.
-    if (null == Monitor.HA_SERVICE_INSTANCE || !Monitor.HA_SERVICE_INSTANCE.isActiveService()) {
-      return false;
-    }
-    return true;
-  }
-
-  @Override
-  public void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
-    // Verify that this is the active Monitor instance
-    if (!isActiveMonitor()) {
-      resp.sendError(HttpURLConnection.HTTP_UNAVAILABLE, STANDBY_MONITOR_MESSAGE);
-      return;
-    }
-    StringBuilder sb = new StringBuilder();
-    try {
-      Monitor.fetchData();
-      bannerText = sanitize(Monitor.getContext().getConfiguration().get(Property.MONITOR_BANNER_TEXT));","[{'comment': 'The three monitor banner configuration properties in this file should be deprecated in the `Property` class.', 'commenter': 'mikewalch'}]"
242,server/monitor/src/main/resources/templates/index.ftl,"@@ -0,0 +1,72 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the ""License""); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<html>
+  <head>
+    <title>${title} - Accumulo ${version}</title>
+    <meta http-equiv=""Content-Type"" content=""test/html"" />
+    <meta http-equiv=""Content-Script-Type"" content=""text/javascript"" />
+    <meta http-equiv=""Content-Style-Type"" content=""text/css"" />
+    <link rel=""shortcut icon"" type=""image/jng"" href=""/resources/favicon.png"" />
+    <script src=""/resources/global.js"" type=""text/javascript""></script>
+    <script src=""/resources/functions.js"" type=""text/javascript""></script>
+    
+    <link rel=""stylesheet"" href=""//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css"">
+    <script src=""https://code.jquery.com/jquery-1.12.4.js""></script>
+    
+    <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js""></script>
+    <link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"" integrity=""sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"" crossorigin=""anonymous"">
+    <link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css"" integrity=""sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp"" crossorigin=""anonymous"">
+    <script src=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"" integrity=""sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"" crossorigin=""anonymous""></script>
+
+    <script src=""https://code.jquery.com/ui/1.12.1/jquery-ui.js""></script>
+    <script language=""javascript"" type=""text/javascript"" src=""/resources/flot/jquery.flot.js""></script>
+    <script language=""javascript"" type=""text/javascript"" src=""/resources/flot/jquery.flot.time.js""></script>
+    <link href=""https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.3/css/select2.min.css"" rel=""stylesheet"" />
+    <script src=""https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.3/js/select2.min.js""></script>
+    
+    <link rel=""stylesheet"" type=""text/css"" href=""/resources/screen.css"" media=""screen"" />
+    <script>
+      /**
+       * Sets up autorefresh on initial load
+       */
+      $(document).ready(function() {
+        setupAutoRefresh();
+      });
+    </script>
+    <#if js??>
+      <script src=""/resources/${js}""></script>
+    </#if>
+    <script src=""/resources/sidebar.js""></script>
+  </head>
+
+  <body>
+    <#include ""/templates/modals.ftl"">
+    <div id=""content-wrapper"">
+      <div id=""content"">
+        <div id=""sidebar"" class=""navbar navbar-inverse navbar-fixed-top"">
+          <!--<#include ""/templates/header.ftl"">-->
+          <#include ""/templates/sidebar.ftl"">","[{'comment': '`sidebar.ftl` could be renamed to something like `navbar.ftl` as its not one side.  also `header.ftl` is commented out.  Can the comment and the file be removed?', 'commenter': 'mikewalch'}, {'comment': 'Thanks, initially it was a sidebar that turned into a navbar and I forgot to rename it. Yes, the comment and the file can be removed. Will do this tomorrow.', 'commenter': 'lstav'}]"
242,server/monitor/src/main/resources/resources/vis.js,"@@ -0,0 +1,506 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+","[{'comment': 'There is a `vis.js.old` file that should be removed.', 'commenter': 'mikewalch'}, {'comment': 'Thanks, forgot to remove it.', 'commenter': 'lstav'}]"
242,server/monitor/src/main/resources/templates/overview.ftl,"@@ -0,0 +1,90 @@
+<#--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the ""License""); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+      <div><h3>${title}</h3></div>
+      <br>
+      <div class=""center-block"">
+        <table class=""overview-table"">","[{'comment': 'It would be really nice if this page and others used [bootstrap\'s grid system](http://getbootstrap.com/css/#grid) (rather than `<table>` tags to structure the page).  If they did, the charts and tables would collapse into a single column as screen size of the browser shrinks.  You can test this with a desktop browser.  Check out the Accumulo website for an example.  The bootstrap code is very simple.  Something like below:\r\n\r\n```html\r\n<div class=""container"">\r\n  <div class=""row"">\r\n    <div class=""col-md-6"">Accumulo master table html</div>\r\n    <div class=""col-md-6"">Zookeeper table html</div>\r\n  </div> \r\n  <div class=""row"">\r\n    <div class=""col-md-6"">Chart 1 html</div>\r\n    <div class=""col-md-6"">Chart 2 html</div>\r\n  </div> \r\n</div>\r\n```', 'commenter': 'mikewalch'}, {'comment': ""Nice suggestion! I'll look into it tomorrow and see if I can implement it with no major problems."", 'commenter': 'lstav'}]"
242,assemble/pom.xml,"@@ -195,6 +235,86 @@
       <optional>true</optional>
     </dependency>
     <dependency>
+      <groupId>org.freemarker</groupId>
+      <artifactId>freemarker</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.hk2</groupId>
+      <artifactId>hk2-api</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.hk2</groupId>
+      <artifactId>hk2-locator</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.hk2</groupId>
+      <artifactId>hk2-utils</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.hk2</groupId>
+      <artifactId>osgi-resource-locator</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.hk2.external</groupId>
+      <artifactId>aopalliance-repackaged</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.hk2.external</groupId>
+      <artifactId>javax.inject</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.bundles.repackaged</groupId>
+      <artifactId>jersey-guava</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.containers</groupId>
+      <artifactId>jersey-container-jetty-http</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.containers</groupId>
+      <artifactId>jersey-container-servlet</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.containers</groupId>
+      <artifactId>jersey-container-servlet-core</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.core</groupId>
+      <artifactId>jersey-client</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.core</groupId>
+      <artifactId>jersey-common</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.core</groupId>
+      <artifactId>jersey-server</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.ext</groupId>
+      <artifactId>jersey-entity-filtering</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.ext</groupId>
+      <artifactId>jersey-mvc</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.ext</groupId>
+      <artifactId>jersey-mvc-freemarker</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.media</groupId>
+      <artifactId>jersey-media-jaxb</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.media</groupId>
+      <artifactId>jersey-media-json-jackson</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.javassist</groupId>
+      <artifactId>javassist</artifactId>","[{'comment': 'Do we have an explicit dependency on this? I would have expected to see this transitively brought in.', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/EmbeddedWebServer.java,"@@ -118,19 +98,17 @@ public void start() {
     }
   }
 
-  public void stop() {
+  private void stop() {","[{'comment': 'why private?', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java,"@@ -778,77 +777,51 @@ public static long getStartTime() {
   }
 
   public static List<Pair<Long,Double>> getLoadOverTime() {
-    synchronized (loadOverTime) {
-      return new ArrayList<>(loadOverTime);
-    }
+    return new ArrayList<>(loadOverTime);","[{'comment': 'Nice simplification on these.', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/Totals.java,"@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest;
+
+/**
+ *
+ * Generates the totals for XML summary
+ *
+ * @since 2.0.0
+ *
+ */
+public class Totals {
+
+  // Variable names become JSON keys
+  public double ingestrate, queryrate, diskrate = 0.0;","[{'comment': ""The normal style is to define these each on their own line and not perform any inline assignment (do it in the constructor). Camel-case'ing the name would also be good.\r\n\r\nI assume this affects much of the other code in here. I won't comment everywhere.."", 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/XMLInformation.java,"@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import javax.xml.bind.annotation.XmlRootElement;
+
+import org.apache.accumulo.monitor.rest.logs.DeadLoggerList;
+import org.apache.accumulo.monitor.rest.master.MasterInformation;
+import org.apache.accumulo.monitor.rest.tables.TableInformation;
+import org.apache.accumulo.monitor.rest.tables.TableInformationList;
+import org.apache.accumulo.monitor.rest.tables.TableNamespace;
+import org.apache.accumulo.monitor.rest.tables.TablesList;
+import org.apache.accumulo.monitor.rest.tservers.BadTabletServers;
+import org.apache.accumulo.monitor.rest.tservers.DeadServerList;
+import org.apache.accumulo.monitor.rest.tservers.ServersShuttingDown;
+import org.apache.accumulo.monitor.rest.tservers.TabletServer;
+
+/**
+ *
+ * Generate XML summary of Monitor
+ *
+ * @since 2.0.0
+ *
+ */
+@XmlRootElement(name = ""stats"")
+public class XMLInformation {
+
+  // Variable names become JSON keys
+  public List<TabletServer> servers;
+
+  public String masterGoalState, masterState;
+
+  public BadTabletServers badTabletServers;
+  public ServersShuttingDown tabletServersShuttingDown;
+  public Integer unassignedTablets;
+  public DeadServerList deadTabletServers;
+
+  public DeadLoggerList deadLoggers;
+
+  public TableInformationList tables;
+
+  public Totals totals;
+
+  public XMLInformation() {
+    servers = new ArrayList<>();
+  }
+
+  /**
+   * Stores Monitor information as XML
+   *
+   * @param size
+   *          Number of tservers
+   * @param info
+   *          Master information
+   * @param tablesList
+   *          Table list
+   */
+  public XMLInformation(int size, MasterInformation info, TablesList tablesList) {
+    this.servers = new ArrayList<>(size);
+
+    this.masterGoalState = info.masterGoalState;
+    this.masterState = info.masterState;
+
+    this.badTabletServers = info.badTabletServers;
+    this.tabletServersShuttingDown = info.tabletServersShuttingDown;
+    this.unassignedTablets = info.unassignedTablets;
+    this.deadTabletServers = info.deadTabletServers;
+    this.deadLoggers = info.deadLoggers;
+
+    getTableInformationList(tablesList.tables);
+
+    this.totals = new Totals(info.ingestrate, info.queryrate, info.numentries);
+  }
+
+  /**
+   * Adds a new tablet to the XML
+   *
+   * @param tablet
+   *          Tablet to add
+   */
+  public void addTablet(TabletServer tablet) {","[{'comment': ' s/addTablet/addTabletServer/', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/XMLResource.java,"@@ -0,0 +1,67 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.WebApplicationException;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response.Status;
+
+import org.apache.accumulo.core.master.thrift.MasterMonitorInfo;
+import org.apache.accumulo.core.master.thrift.TabletServerStatus;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.master.MasterResource;
+import org.apache.accumulo.monitor.rest.tables.TablesResource;
+import org.apache.accumulo.monitor.rest.tservers.TabletServer;
+
+/**
+ *
+ * Responsible for generating an XML summary of the Monitor
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class XMLResource {
+
+  /**
+   * Generates an XML summary of the Monitor
+   *
+   * @return XML summary
+   */
+  @GET
+  public XMLInformation getXMLInformation() {
+
+    MasterMonitorInfo mmi = Monitor.getMmi();
+    if (null == mmi) {
+      throw new WebApplicationException(Status.INTERNAL_SERVER_ERROR);
+    }
+
+    // Add Monitor information
+    XMLInformation xml = new XMLInformation(mmi.tServerInfo.size(), new MasterResource().getTables(), new TablesResource().getTables());","[{'comment': 'Make `getTables()` on `MasterResources` and `TablesResources` static methods. They look stateless, so the static method could just invoke the existing non-static method on a static final singleton instance of the respective classes.', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/XMLResource.java,"@@ -0,0 +1,67 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.WebApplicationException;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response.Status;
+
+import org.apache.accumulo.core.master.thrift.MasterMonitorInfo;
+import org.apache.accumulo.core.master.thrift.TabletServerStatus;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.master.MasterResource;
+import org.apache.accumulo.monitor.rest.tables.TablesResource;
+import org.apache.accumulo.monitor.rest.tservers.TabletServer;
+
+/**
+ *
+ * Responsible for generating an XML summary of the Monitor
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class XMLResource {
+
+  /**
+   * Generates an XML summary of the Monitor
+   *
+   * @return XML summary
+   */
+  @GET
+  public XMLInformation getXMLInformation() {
+
+    MasterMonitorInfo mmi = Monitor.getMmi();
+    if (null == mmi) {
+      throw new WebApplicationException(Status.INTERNAL_SERVER_ERROR);
+    }
+
+    // Add Monitor information
+    XMLInformation xml = new XMLInformation(mmi.tServerInfo.size(), new MasterResource().getTables(), new TablesResource().getTables());
+
+    // Add tserver information
+    for (TabletServerStatus status : mmi.tServerInfo) {
+      xml.addTablet(new TabletServer(status));","[{'comment': ""Calling the metrics object TabletServer is a bit confusing to me. It's metrics/information, not the actual TabletServer instance (which already exists)."", 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/gc/GarbageCollection.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.gc;
+
+import org.apache.accumulo.core.gc.thrift.GcCycleStats;
+
+/**
+ *
+ * GarbageCollection is responsible for creating the gc JSON object
+ *
+ * @since 2.0.0
+ *
+ */
+public class GarbageCollection {
+
+  public static final GarbageCollection EMPTY = new GarbageCollection();","[{'comment': 'nit: make this private and expose it through a public static method.', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/gc/GarbageCollectorCycle.java,"@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.gc;
+
+import org.apache.accumulo.core.gc.thrift.GcCycleStats;
+
+/**
+ *
+ * Metrics about a single cycle of the garbage collector
+ *
+ * @since 2.0.0
+ *
+ */
+public class GarbageCollectorCycle {
+
+  public static final GarbageCollectorCycle EMPTY = new GarbageCollectorCycle();
+
+  // Variable names become JSON key
+  public long started, finished, candidates, inUse, deleted, errors;
+
+  public GarbageCollectorCycle() {
+    started = finished = candidates = inUse = deleted = errors = 0l;","[{'comment': ""Each on their own line, or do this when you declare the variables and remove the default constructor. This doesn't match existing style."", 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/gc/GarbageCollectorStatus.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.gc;
+
+import org.apache.accumulo.core.gc.thrift.GCStatus;
+
+/**
+ *
+ * Responsible for grouping files and wals into a JSON object
+ *
+ * @since 2.0.0
+ *
+ */
+public class GarbageCollectorStatus {
+
+  public static final GarbageCollectorStatus EMPTY = new GarbageCollectorStatus();
+
+  // variable names become JSON key
+  public GarbageCollection files = new GarbageCollection();","[{'comment': ""When invoking the constructor which takes a `GCStatus` , you're instantiating two extra GarbageCollection objects unnecessarily. Move these into the no-args constructor if you don't want `files` and `wals` to be null."", 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/logs/LogResource.java,"@@ -0,0 +1,79 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.logs;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.POST;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.server.monitor.DedupedLogEvent;
+import org.apache.accumulo.server.monitor.LogService;
+import org.apache.log4j.spi.LoggingEvent;
+
+/**
+ *
+ * Responsible for generating a new log JSON object
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/logs"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class LogResource {
+
+  /**
+   * Generates log event list as a JSON object
+   *
+   * @return log event array
+   */
+  @GET
+  public List<LogEvent> getRecentLogs() {
+    List<DedupedLogEvent> dedupedLogEvents = LogService.getInstance().getEvents();
+    ArrayList<LogEvent> logEvents = new ArrayList<>(dedupedLogEvents.size());
+
+    final StringBuilder msg = new StringBuilder(64);
+    for (DedupedLogEvent dev : dedupedLogEvents) {
+      msg.setLength(0);
+      final LoggingEvent ev = dev.getEvent();
+      Object application = ev.getMDC(""application"");
+      if (application == null)
+        application = """";
+
+      msg.append(ev.getMessage().toString());
+      if (ev.getThrowableStrRep() != null)
+        for (String line : ev.getThrowableStrRep())
+          msg.append(""\n\t"").append(line);
+
+      // Add a new log event to the list
+      logEvents.add(new LogEvent(ev.getTimeStamp(), application, dev.getCount(), ev.getLevel().toString(), msg.toString().trim()));","[{'comment': 'the `trim()` seems unnecessary at a glance.', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/master/MasterResource.java,"@@ -0,0 +1,235 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.master;
+
+import java.lang.management.ManagementFactory;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.master.thrift.DeadServer;
+import org.apache.accumulo.core.master.thrift.MasterMonitorInfo;
+import org.apache.accumulo.core.master.thrift.TabletServerStatus;
+import org.apache.accumulo.core.util.AddressUtil;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.logs.DeadLoggerInformation;
+import org.apache.accumulo.monitor.rest.logs.DeadLoggerList;
+import org.apache.accumulo.monitor.rest.tservers.BadTabletServerInformation;
+import org.apache.accumulo.monitor.rest.tservers.BadTabletServers;
+import org.apache.accumulo.monitor.rest.tservers.DeadServerInformation;
+import org.apache.accumulo.monitor.rest.tservers.DeadServerList;
+import org.apache.accumulo.monitor.rest.tservers.ServerShuttingDownInformation;
+import org.apache.accumulo.monitor.rest.tservers.ServersShuttingDown;
+import org.apache.accumulo.server.master.state.TabletServerState;
+
+/**
+ *
+ * Responsible for generating a new Master information JSON object
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/master"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class MasterResource {
+  public static final String NO_MASTERS = ""No Masters running"";
+
+  /**
+   * Gets the MasterMonitorInfo, allowing for mocking frameworks for testability
+   */
+  protected MasterMonitorInfo getMmi() {
+    return Monitor.getMmi();
+  }
+
+  /**
+   * Generates a master information JSON object
+   *
+   * @return master JSON object
+   */
+  @GET
+  public MasterInformation getTables() {
+
+    MasterInformation masterInformation;
+
+    if (Monitor.getMmi() != null) {
+      String gcStatus = ""Waiting"";
+      String label = """";
+      if (Monitor.getGcStatus() != null) {","[{'comment': ""Get this once and set a local variable. It's redundant to keep invoking `Monitor.getGcStatus()` over and over."", 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/problems/ProblemsResource.java,"@@ -0,0 +1,175 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.problems;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import javax.ws.rs.Consumes;
+import javax.ws.rs.GET;
+import javax.ws.rs.POST;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.QueryParam;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.client.impl.Tables;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.server.client.HdfsZooInstance;
+import org.apache.accumulo.server.problems.ProblemReport;
+import org.apache.accumulo.server.problems.ProblemReports;
+import org.apache.accumulo.server.problems.ProblemType;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ *
+ * Generates a problem summary and details as a JSON object
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/problems"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class ProblemsResource {
+
+  /**
+   * Generates a list with the problem summary
+   *
+   * @return problem summary list
+   */
+  @GET
+  @Path(""summary"")
+  public Map<String,List<ProblemSummaryInformation>> getSummary() {
+
+    Map<String,List<ProblemSummaryInformation>> jsonObj = new HashMap<String,List<ProblemSummaryInformation>>();
+
+    List<ProblemSummaryInformation> problems = new ArrayList<>();
+
+    Map<String,String> tidToNameMap = Tables.getIdToNameMap(HdfsZooInstance.getInstance());
+
+    if (Monitor.getProblemException() == null) {
+      for (Entry<String,Map<ProblemType,Integer>> entry : Monitor.getProblemSummary().entrySet()) {
+        Integer readCount = null, writeCount = null, loadCount = null;
+
+        for (ProblemType pt : ProblemType.values()) {
+          Integer pcount = entry.getValue().get(pt);
+          if (pt.equals(ProblemType.FILE_READ)) {
+            readCount = pcount;
+          } else if (pt.equals(ProblemType.FILE_WRITE)) {
+            writeCount = pcount;
+          } else if (pt.equals(ProblemType.TABLET_LOAD)) {
+            loadCount = pcount;
+          }
+        }
+
+        String tableName = Tables.getPrintableTableNameFromId(tidToNameMap, entry.getKey());
+
+        problems.add(new ProblemSummaryInformation(tableName, entry.getKey(), readCount, writeCount, loadCount));
+      }
+    }
+    jsonObj.put(""problemSummary"", problems);
+
+    return jsonObj;
+  }
+
+  /**
+   * REST call to clear problem reports from a table
+   *
+   * @param tableID
+   *          Table ID to clear problems
+   */
+  @POST
+  @Consumes(MediaType.TEXT_PLAIN)
+  @Path(""summary"")
+  public void clearTableProblems(@QueryParam(""s"") String tableID) {
+    Logger log = LoggerFactory.getLogger(Monitor.class);
+    try {
+      ProblemReports.getInstance(Monitor.getContext()).deleteProblemReports(tableID);
+    } catch (Exception e) {
+      log.error(""Failed to delete problem reports for table "" + tableID, e);
+    }
+  }
+
+  /**
+   * Generates a list of the problem details as a JSON object
+   *
+   * @return problem details list
+   */
+  @GET
+  @Path(""details"")
+  public Map<String,List<ProblemDetailInformation>> getDetails() {
+
+    Map<String,List<ProblemDetailInformation>> jsonObj = new HashMap<String,List<ProblemDetailInformation>>();","[{'comment': '`new HashMap<>()`', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/replication/ReplicationResource.java,"@@ -0,0 +1,204 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.replication;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.BatchScanner;
+import org.apache.accumulo.core.client.Connector;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.TableOfflineException;
+import org.apache.accumulo.core.client.admin.TableOperations;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.RootTable;
+import org.apache.accumulo.core.replication.ReplicationSchema.WorkSection;
+import org.apache.accumulo.core.replication.ReplicationTable;
+import org.apache.accumulo.core.replication.ReplicationTarget;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.server.replication.ReplicaSystem;
+import org.apache.accumulo.server.replication.ReplicaSystemFactory;
+import org.apache.hadoop.io.Text;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ *
+ * Generates the replication table with information from the Monitor
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/replication"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})","[{'comment': ""Not specific to this class (just happened to think about it): isn't the `@Produces` annotation inherited? e.g. we could make a base `Resource` class which has the `@Produces` marking and not have to worry about getting it right on every other resource we create?"", 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tables/TablesResource.java,"@@ -0,0 +1,232 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.tables;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.TreeSet;
+import java.util.stream.Collectors;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.PathParam;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.client.Instance;
+import org.apache.accumulo.core.client.impl.Namespaces;
+import org.apache.accumulo.core.client.impl.Tables;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.impl.KeyExtent;
+import org.apache.accumulo.core.master.thrift.TableInfo;
+import org.apache.accumulo.core.master.thrift.TabletServerStatus;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.RootTable;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.tservers.TabletServer;
+import org.apache.accumulo.monitor.rest.tservers.TabletServers;
+import org.apache.accumulo.server.client.HdfsZooInstance;
+import org.apache.accumulo.server.master.state.MetaDataTableScanner;
+import org.apache.accumulo.server.master.state.TabletLocationState;
+import org.apache.accumulo.server.tables.TableManager;
+import org.apache.accumulo.server.util.TableInfoUtil;
+import org.apache.hadoop.io.Text;
+
+/**
+ *
+ * Generates a tables list from the Monitor as a JSON object
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/tables"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class TablesResource {
+
+  private static final TabletServerStatus NO_STATUS = new TabletServerStatus();
+
+  /**
+   * Generates a table list based on the namespace
+   *
+   * @param namespace
+   *          Namespace used to filter the tables
+   * @return Table list
+   */
+  private TablesList generateTables(String namespace) {
+    Instance inst = Monitor.getContext().getInstance();
+    Map<String,String> tidToNameMap = Tables.getIdToNameMap(inst);
+    SortedMap<String,TableInfo> tableStats = new TreeMap<>();
+
+    if (Monitor.getMmi() != null && Monitor.getMmi().tableMap != null)
+      for (Entry<String,TableInfo> te : Monitor.getMmi().tableMap.entrySet())
+        tableStats.put(Tables.getPrintableTableNameFromId(tidToNameMap, te.getKey()), te.getValue());
+
+    Map<String,Double> compactingByTable = TableInfoUtil.summarizeTableStats(Monitor.getMmi());
+    TableManager tableManager = TableManager.getInstance();
+
+    SortedMap<String,String> namespaces = Namespaces.getNameToIdMap(Monitor.getContext().getInstance());
+
+    TablesList tableNamespace = new TablesList();
+    List<TableInformation> tables = new ArrayList<>();
+
+    // Add the tables that have the selected namespace
+    for (String key : namespaces.keySet()) {
+      if (namespace.equals(""*"") || namespace.equals(key) || (key.equals("""") && namespace.equals(""-""))) {","[{'comment': 'Make the asterisk a static-final constant if it has special meaning.\r\n\r\nWhat special meaning does the hyphen have? (comment)', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tables/TablesResource.java,"@@ -0,0 +1,232 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.tables;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.TreeSet;
+import java.util.stream.Collectors;
+
+import javax.ws.rs.GET;
+import javax.ws.rs.Path;
+import javax.ws.rs.PathParam;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.MediaType;
+
+import org.apache.accumulo.core.client.Instance;
+import org.apache.accumulo.core.client.impl.Namespaces;
+import org.apache.accumulo.core.client.impl.Tables;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.impl.KeyExtent;
+import org.apache.accumulo.core.master.thrift.TableInfo;
+import org.apache.accumulo.core.master.thrift.TabletServerStatus;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.RootTable;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.tservers.TabletServer;
+import org.apache.accumulo.monitor.rest.tservers.TabletServers;
+import org.apache.accumulo.server.client.HdfsZooInstance;
+import org.apache.accumulo.server.master.state.MetaDataTableScanner;
+import org.apache.accumulo.server.master.state.TabletLocationState;
+import org.apache.accumulo.server.tables.TableManager;
+import org.apache.accumulo.server.util.TableInfoUtil;
+import org.apache.hadoop.io.Text;
+
+/**
+ *
+ * Generates a tables list from the Monitor as a JSON object
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/tables"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class TablesResource {
+
+  private static final TabletServerStatus NO_STATUS = new TabletServerStatus();
+
+  /**
+   * Generates a table list based on the namespace
+   *
+   * @param namespace
+   *          Namespace used to filter the tables
+   * @return Table list
+   */
+  private TablesList generateTables(String namespace) {
+    Instance inst = Monitor.getContext().getInstance();
+    Map<String,String> tidToNameMap = Tables.getIdToNameMap(inst);
+    SortedMap<String,TableInfo> tableStats = new TreeMap<>();
+
+    if (Monitor.getMmi() != null && Monitor.getMmi().tableMap != null)
+      for (Entry<String,TableInfo> te : Monitor.getMmi().tableMap.entrySet())
+        tableStats.put(Tables.getPrintableTableNameFromId(tidToNameMap, te.getKey()), te.getValue());
+
+    Map<String,Double> compactingByTable = TableInfoUtil.summarizeTableStats(Monitor.getMmi());
+    TableManager tableManager = TableManager.getInstance();
+
+    SortedMap<String,String> namespaces = Namespaces.getNameToIdMap(Monitor.getContext().getInstance());
+
+    TablesList tableNamespace = new TablesList();
+    List<TableInformation> tables = new ArrayList<>();
+
+    // Add the tables that have the selected namespace
+    for (String key : namespaces.keySet()) {
+      if (namespace.equals(""*"") || namespace.equals(key) || (key.equals("""") && namespace.equals(""-""))) {
+        tableNamespace.addTable(new TableNamespace(key));
+      }
+    }
+
+    // Add tables to the list
+    for (Entry<String,String> entry : Tables.getNameToIdMap(HdfsZooInstance.getInstance()).entrySet()) {
+      String tableName = entry.getKey(), tableId = entry.getValue();
+      TableInfo tableInfo = tableStats.get(tableName);
+      if (null != tableInfo) {
+        Double holdTime = compactingByTable.get(tableId);
+        if (holdTime == null)
+          holdTime = Double.valueOf(0.);
+
+        for (TableNamespace name : tableNamespace.tables) {
+          // Check if table has the default namespace
+          if (!tableName.contains(""."") && name.namespace.equals("""")) {","[{'comment': '`name.namespace.isEmpty()` instead of allocating a new string.', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tservers/TabletServerResource.java,"@@ -0,0 +1,336 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.tservers;
+
+import java.lang.management.ManagementFactory;
+import java.security.MessageDigest;
+import java.util.ArrayList;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import javax.ws.rs.Consumes;
+import javax.ws.rs.GET;
+import javax.ws.rs.POST;
+import javax.ws.rs.Path;
+import javax.ws.rs.PathParam;
+import javax.ws.rs.Produces;
+import javax.ws.rs.QueryParam;
+import javax.ws.rs.WebApplicationException;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response.Status;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.client.impl.ClientContext;
+import org.apache.accumulo.core.client.impl.Tables;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.impl.KeyExtent;
+import org.apache.accumulo.core.master.thrift.MasterMonitorInfo;
+import org.apache.accumulo.core.master.thrift.RecoveryStatus;
+import org.apache.accumulo.core.master.thrift.TabletServerStatus;
+import org.apache.accumulo.core.rpc.ThriftUtil;
+import org.apache.accumulo.core.tabletserver.thrift.ActionStats;
+import org.apache.accumulo.core.tabletserver.thrift.TabletClientService;
+import org.apache.accumulo.core.tabletserver.thrift.TabletStats;
+import org.apache.accumulo.core.trace.Tracer;
+import org.apache.accumulo.core.util.AddressUtil;
+import org.apache.accumulo.core.zookeeper.ZooUtil;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.monitor.rest.master.MasterResource;
+import org.apache.accumulo.server.client.HdfsZooInstance;
+import org.apache.accumulo.server.master.state.DeadServerList;
+import org.apache.accumulo.server.util.ActionStatsUpdator;
+
+import com.google.common.net.HostAndPort;
+
+/**
+ *
+ * Generates tserver lists as JSON objects
+ *
+ * @since 2.0.0
+ *
+ */
+@Path(""/tservers"")
+@Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
+public class TabletServerResource {
+
+  // Variable names become JSON keys
+  private TabletStats total, historical;
+
+  /**
+   * Generates tserver summary
+   *
+   * @return tserver summary
+   */
+  @GET
+  public TabletServers getTserverSummary() {
+    MasterMonitorInfo mmi = Monitor.getMmi();
+    if (null == mmi) {
+      throw new WebApplicationException(Status.INTERNAL_SERVER_ERROR);
+    }
+
+    TabletServers tserverInfo = new TabletServers(mmi.tServerInfo.size());
+    for (TabletServerStatus status : mmi.tServerInfo) {
+      tserverInfo.addTablet(new TabletServer(status));
+    }
+
+    tserverInfo.addBadTabletServer(new MasterResource().getTables());
+
+    return tserverInfo;
+  }
+
+  /**
+   * REST call to clear dead servers from list
+   *
+   * @param server
+   *          Dead server to clear
+   */
+  @POST
+  @Consumes(MediaType.TEXT_PLAIN)
+  public void clearDeadServer(@QueryParam(""server"") String server) throws Exception {
+    DeadServerList obit = new DeadServerList(ZooUtil.getRoot(Monitor.getContext().getInstance()) + Constants.ZDEADTSERVERS);
+    obit.delete(server);
+  }
+
+  /**
+   * Generates a recovery tserver list
+   *
+   * @return Recovery tserver list
+   */
+  @Path(""recovery"")
+  @GET
+  public Map<String,List<Map<String,String>>> getTserverRecovery() {
+
+    Map<String,List<Map<String,String>>> jsonObj = new HashMap<String,List<Map<String,String>>>();
+    List<Map<String,String>> recoveryList = new ArrayList<>();
+    Map<String,String> recoveryObj = new HashMap<String,String>();
+
+    MasterMonitorInfo mmi = Monitor.getMmi();
+    if (null == mmi) {
+      throw new WebApplicationException(Status.INTERNAL_SERVER_ERROR);
+    }
+
+    for (TabletServerStatus server : mmi.tServerInfo) {
+      if (server.logSorts != null) {
+        for (RecoveryStatus recovery : server.logSorts) {
+          recoveryObj.put(""server"", AddressUtil.parseAddress(server.name, false).getHostText());
+          recoveryObj.put(""log"", recovery.name);
+          recoveryObj.put(""time"", Long.toString(recovery.runtime));
+          recoveryObj.put(""copySort"", Double.toString(recovery.progress));
+
+          recoveryList.add(recoveryObj);
+        }
+      }
+    }
+
+    jsonObj.put(""recoveryList"", recoveryList);
+
+    return jsonObj;
+  }
+
+  /**
+   * Generates details for the selected tserver
+   *
+   * @param tserverAddr
+   *          TServer name
+   * @return TServer details
+   */
+  @Path(""{address}"")
+  @GET
+  public TabletServerSummary getTserverDetails(@PathParam(""address"") String tserverAddr) throws Exception {
+
+    String tserverAddress = tserverAddr;
+
+    boolean tserverExists = false;
+    if (tserverAddress != null && tserverAddress.isEmpty() == false) {
+      for (TabletServerStatus ts : Monitor.getMmi().getTServerInfo()) {
+        if (tserverAddress.equals(ts.getName())) {
+          tserverExists = true;
+          break;
+        }
+      }
+    }
+
+    if (tserverAddress == null || tserverAddress.isEmpty() || tserverExists == false) {
+
+      return null;
+    }
+
+    double totalElapsedForAll = 0;
+    double splitStdDev = 0;
+    double minorStdDev = 0;
+    double minorQueueStdDev = 0;
+    double majorStdDev = 0;
+    double majorQueueStdDev = 0;
+    double currentMinorAvg = 0;
+    double currentMajorAvg = 0;
+    double currentMinorStdDev = 0;
+    double currentMajorStdDev = 0;
+    total = new TabletStats(null, new ActionStats(), new ActionStats(), new ActionStats(), 0, 0, 0, 0);
+    HostAndPort address = HostAndPort.fromString(tserverAddress);
+    historical = new TabletStats(null, new ActionStats(), new ActionStats(), new ActionStats(), 0, 0, 0, 0);
+    List<TabletStats> tsStats = new ArrayList<>();
+
+    try {
+      ClientContext context = Monitor.getContext();
+      TabletClientService.Client client = ThriftUtil.getClient(new TabletClientService.Client.Factory(), address, context);
+      try {
+        for (String tableId : Monitor.getMmi().tableMap.keySet()) {
+          tsStats.addAll(client.getTabletStats(Tracer.traceInfo(), context.rpcCreds(), tableId));
+        }
+        historical = client.getHistoricalStats(Tracer.traceInfo(), context.rpcCreds());
+      } finally {
+        ThriftUtil.returnClient(client);
+      }
+    } catch (Exception e) {
+      return null;
+    }
+
+    List<CurrentOperations> currentOps = doCurrentOperations(tsStats);
+
+    if (total.minors.num != 0)
+      currentMinorAvg = (long) (total.minors.elapsed / total.minors.num);
+    if (total.minors.elapsed != 0 && total.minors.num != 0)
+      currentMinorStdDev = stddev(total.minors.elapsed, total.minors.num, total.minors.sumDev);
+    if (total.majors.num != 0)
+      currentMajorAvg = total.majors.elapsed / total.majors.num;
+    if (total.majors.elapsed != 0 && total.majors.num != 0 && total.majors.elapsed > total.majors.num)
+      currentMajorStdDev = stddev(total.majors.elapsed, total.majors.num, total.majors.sumDev);
+
+    ActionStatsUpdator.update(total.minors, historical.minors);
+    ActionStatsUpdator.update(total.majors, historical.majors);
+    totalElapsedForAll += total.majors.elapsed + historical.splits.elapsed + total.minors.elapsed;
+
+    minorStdDev = stddev(total.minors.elapsed, total.minors.num, total.minors.sumDev);
+    minorQueueStdDev = stddev(total.minors.queueTime, total.minors.num, total.minors.queueSumDev);
+    majorStdDev = stddev(total.majors.elapsed, total.majors.num, total.majors.sumDev);
+    majorQueueStdDev = stddev(total.majors.queueTime, total.majors.num, total.majors.queueSumDev);
+    splitStdDev = stddev(historical.splits.num, historical.splits.elapsed, historical.splits.sumDev);
+
+    TabletServerDetailInformation details = doDetails(address, tsStats.size());
+
+    List<AllTimeTabletResults> allTime = doAllTimeResults(majorQueueStdDev, minorQueueStdDev, totalElapsedForAll, splitStdDev, majorStdDev, minorStdDev);
+
+    CurrentTabletResults currentRes = doCurrentTabletResults(currentMinorAvg, currentMinorStdDev, currentMajorAvg, currentMajorStdDev);
+
+    TabletServerSummary tserverDetails = new TabletServerSummary(details, allTime, currentRes, currentOps);
+
+    return tserverDetails;
+  }
+
+  private static final int concurrentScans = Monitor.getContext().getConfiguration().getCount(Property.TSERV_READ_AHEAD_MAXCONCURRENT);","[{'comment': 'This should be re-computed to ensure that the Monitor picks up new values for this configuration property.', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/zk/ZKInformation.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.rest.zk;
+
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ *
+ * Generates a list of Zookeeper server information
+ *
+ * @since 2.0.0
+ *
+ */
+public class ZKInformation {","[{'comment': ""Do we need this class to encapsulate this? Couldn't it just be replaced with List<ZooKeeper>?"", 'commenter': 'joshelser'}]"
242,server/monitor/src/main/resources/resources/bulkImport.js,"@@ -0,0 +1,213 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one or more
+* contributor license agreements.  See the NOTICE file distributed with
+* this work for additional information regarding copyright ownership.
+* The ASF licenses this file to You under the Apache License, Version 2.0
+* (the ""License""); you may not use this file except in compliance with
+* the License.  You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an ""AS IS"" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+
+/**
+ * Creates bulk import initial table
+ */
+$(document).ready(function() {
+  createBulkImportHeader();
+  createServerBulkHeader();
+  refreshBulkImport();
+});
+
+/**
+ * Makes the REST calls, generates the tables with the new information
+ */
+function refreshBulkImport() {
+  $.ajaxSetup({
+    async: false
+  });
+  getBulkImports();
+  $.ajaxSetup({
+    async: true
+  });
+  refreshBulkImportTable();
+  refreshServerBulkTable();
+}
+
+/**
+ * Used to redraw the page
+ */
+function refresh() {
+  refreshBulkImport();
+}
+
+/**
+ * Generates the master bulk import status table
+ */
+function refreshBulkImportTable() {
+
+  clearTable('masterBulkImportStatus');
+
+  /*
+   * Get the bulk import value obtained earlier, if it doesn't exists,
+   * create an empty array
+   */
+  var data = sessionStorage.bulkImports === undefined ?
+      [] : JSON.parse(sessionStorage.bulkImports);
+  var items = [];
+
+  /* If the data is empty, create an empty row, otherwise,
+   * create the rows for the table
+   */
+  if (data.length === 0 || data.bulkImport.length === 0) {
+    items.push('<td class=""center"" colspan=""3""><i>Empty</i></td>');
+  } else {
+    $.each(data.bulkImport, function(key, val) {
+      items.push('<td class=""firstcell left"" data-value=""' + val.filename +
+          '"">' + val.filename + '</td>');
+
+      items.push('<td class=""right"" data-value=""' + val.age + '"">' + val.age +
+          '</td>');
+
+      items.push('<td class=""right"" data-value=""' + val.state + '"">' +
+          val.state + '</td>');
+    });
+  }
+
+  $('<tr/>', {
+    html: items.join('')
+  }).appendTo('#masterBulkImportStatus');
+}
+
+/**
+ * Generates the bulk import status table
+ */
+function refreshServerBulkTable() {
+
+  clearTable('bulkImportStatus');
+
+  /* Get the bulk import value obtained earlier, if it doesn't exists,
+   * create an empty array
+   */
+  var data = sessionStorage.bulkImports === undefined ?
+   [] : JSON.parse(sessionStorage.bulkImports);
+  var items = [];
+
+  /* If the data is empty, create an empty row, otherwise
+   * create the rows for the table
+   */
+  if (data.length === 0 || data.tabletServerBulkImport.length === 0) {
+    items.push('<td class=""center"" colspan=""3""><i>Empty</i></td>');
+  } else {
+    $.each(data.tabletServerBulkImport, function(key, val) {
+      items.push('<td class=""firstcell left"" data-value=""' + val.server +
+          '""><a href=""/tservers?s=' + val.server + '"">' + val.server +
+          '</a></td>');
+
+      items.push('<td class=""right"" data-value=""' + val.importSize + '"">' +
+          val.importSize + '</td>');
+
+      items.push('<td class=""right"" data-value=""' + val.oldestAge + '"">' +
+          (val.oldestAge > 0 ? val.oldestAge : '&mdash;') + '</td>');
+    });
+  }
+
+  $('<tr/>', {
+    html: items.join('')
+  }).appendTo('#bulkImportStatus');
+}
+
+/**
+ * Sorts the bulkImportStatus table on the selected column
+ *
+ * @param {string} table Table ID to sort
+ * @param {number} n Column number to sort by
+ */
+function sortTable(table, n) {
+  var tableIDs = ['bulkImportStatus', 'masterBulkImportStatus'];
+
+  if (sessionStorage.tableColumnSort !== undefined &&
+      sessionStorage.tableColumnSort == n &&
+      sessionStorage.direction !== undefined) {
+    direction = sessionStorage.direction === 'asc' ? 'desc' : 'asc';
+  } else {
+    direction = sessionStorage.direction === undefined ?
+        'asc' : sessionStorage.direction;
+  }
+  sessionStorage.tableColumn = tableIDs[table];
+  sessionStorage.tableColumnSort = n;
+  sortTables(tableIDs[table], direction, n);
+}
+
+/**
+ * Create tooltip for table column information
+ */
+$(function() {
+  $(document).tooltip();","[{'comment': ""is there a reason this isn't in the `$(document).onReady`?"", 'commenter': 'joshelser'}]"
242,server/monitor/src/main/resources/resources/functions.js,"@@ -0,0 +1,686 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one or more
+* contributor license agreements.  See the NOTICE file distributed with
+* this work for additional information regarding copyright ownership.
+* The ASF licenses this file to You under the Apache License, Version 2.0
+* (the ""License""); you may not use this file except in compliance with
+* the License.  You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an ""AS IS"" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+
+// Suffixes for quantity
+var QUANTITY_SUFFIX = ['', 'K', 'M', 'B', 'T', 'e15', 'e18', 'e21'];","[{'comment': 'E+15 instead of e15? etc', 'commenter': 'joshelser'}]"
242,server/monitor/src/main/resources/resources/functions.js,"@@ -0,0 +1,686 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one or more
+* contributor license agreements.  See the NOTICE file distributed with
+* this work for additional information regarding copyright ownership.
+* The ASF licenses this file to You under the Apache License, Version 2.0
+* (the ""License""); you may not use this file except in compliance with
+* the License.  You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an ""AS IS"" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+
+// Suffixes for quantity
+var QUANTITY_SUFFIX = ['', 'K', 'M', 'B', 'T', 'e15', 'e18', 'e21'];
+// Suffixes for size
+var SIZE_SUFFIX = ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z'];
+
+/**
+ * Initializes Auto Refresh to false if it is not set,
+ * and creates listeners for auto refresh
+ */
+function setupAutoRefresh() {
+  // Sets auto refresh to true or false
+  if (!sessionStorage.autoRefresh) {
+    sessionStorage.autoRefresh = 'false';
+  }
+  // Need this to set the initial value for the autorefresh on page load
+  if (sessionStorage.autoRefresh == 'false') {
+    $('.auto-refresh').parent().removeClass('active');
+  } else {
+    $('.auto-refresh').parent().addClass('active');
+  }
+  // Initializes the auto refresh on click listener
+  $('.auto-refresh').click(function(e) {
+    if ($(this).parent().attr('class') == 'active') {
+      $(this).parent().removeClass('active');
+      sessionStorage.autoRefresh = 'false';
+    } else {
+      $(this).parent().addClass('active');
+      sessionStorage.autoRefresh = 'true';
+    }
+  });
+}
+
+/**
+ * Global timer that checks for auto refresh status every 5 seconds
+ */
+TIMER = setInterval(function() {
+  if (sessionStorage.autoRefresh == 'true') {
+    $('.auto-refresh').parent().addClass('active');
+    refresh();
+    refreshNavBar();
+  } else {
+    $('.auto-refresh').parent().removeClass('active');
+  }
+}, 5000);
+
+/**
+ * Empty function in case there is no refresh implementation
+ */
+function refresh() {
+}
+
+/**
+ * Converts a number to a size with suffix
+ *
+ * @param {number} size Number to convert
+ * @return {string} Number with suffix added
+ */
+function bigNumberForSize(size) {
+  if (size === null)
+    size = 0;
+  return bigNumber(size, SIZE_SUFFIX, 1024);
+}
+
+/**
+ * Converts a number to a quantity with suffix
+ *
+ * @param {number} quantity Number to convert
+ * @return {string} Number with suffix added
+ */
+function bigNumberForQuantity(quantity) {
+  if (quantity === null)
+    quantity = 0;
+  return bigNumber(quantity, QUANTITY_SUFFIX, 1000);
+}
+
+/**
+ * Adds the suffix to the number, converts the number to one close to the base
+ *
+ * @param {number} big Number to convert
+ * @param {array} suffixes Suffixes to use for convertion
+ * @param {number} base Base to use for convertion
+ * @return {string} The new value with the suffix
+ */
+function bigNumber(big, suffixes, base) {
+  // If the number is smaller than the base, return thee number with no suffix
+  if (big < base) {
+    return big + suffixes[0];
+  }
+  // Finds which suffix to use
+  var exp = Math.floor(Math.log(big) / Math.log(base));
+  // Divides the bumber by the equivalent suffix number
+  var val = big / Math.pow(base, exp);
+  // Keeps the number to 2 decimal places and adds the suffix
+  return val.toFixed(2) + suffixes[exp];
+}
+
+/**
+ * Converts the time to short number and adds unit
+ *
+ * @param {number} time Time in microseconds
+ * @return {string} The time with units
+ */
+function timeDuration(time) {
+  var ms, sec, min, hr, day, yr;
+  ms = sec = min = hr = day = yr = -1;
+
+  time = Math.floor(time);
+
+  // If time is 0 return a dash
+  if (time == 0) {
+    return '&mdash;';
+  }
+
+  // Obtains the milliseconds, if time is 0, return milliseconds, and units
+  ms = time % 1000;
+  time = Math.floor(time / 1000);
+  if (time == 0) {
+    return ms + 'ms';
+  }
+
+  // Obtains the seconds, if time is 0, return seconds, milliseconds, and units
+  sec = time % 60;
+  time = Math.floor(time / 60);
+  if (time == 0) {
+    return sec + 's' + '&nbsp;' + ms + 'ms';
+  }
+
+  // Obtains the minutes, if time is 0, return minutes, seconds, and units
+  min = time % 60;
+  time = Math.floor(time / 60);
+  if (time == 0) {
+    return min + 'm' + '&nbsp;' + sec + 's';","[{'comment': 'tsk, mixing backend work with presentation logic! ;)', 'commenter': 'joshelser'}]"
244,core/src/main/java/org/apache/accumulo/core/file/rfile/RFile.java,"@@ -746,50 +746,45 @@ public boolean hasTop() {
     @Override
     public void next() throws IOException {","[{'comment': 'Why the consolidation of `next()` and `_next()` into just `next()`?', 'commenter': 'joshelser'}, {'comment': ""A bunch of these changes are meant to reduce the call stack size and make the jit's job easier. I don't remember if the jit has trouble with optimizing this call in particular, or whether that has changed since the original tests."", 'commenter': 'scubafuchs'}]"
244,core/src/main/java/org/apache/accumulo/core/iterators/ServerFilter.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.iterators;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.accumulo.core.data.ByteSequence;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+
+/**
+ * A SortedKeyValueIterator that filters entries from its source iterator.
+ *
+ * Subclasses must implement an accept method: public boolean accept(Key k, Value v);
+ *
+ * Key/Value pairs for which the accept method returns true are said to match the filter. By default, this class iterates over entries that match its filter.
+ * This iterator takes an optional ""negate"" boolean parameter that defaults to false. If negate is set to true, this class instead omits entries that match its
+ * filter, thus iterating over entries that do not match its filter.
+ */
+public abstract class ServerFilter extends ServerWrappingIterator {
+
+  public ServerFilter(SortedKeyValueIterator<Key,Value> source) {
+    super(source);
+  }
+
+  @Override
+  public abstract SortedKeyValueIterator<Key,Value> deepCopy(IteratorEnvironment env);
+
+  @Override
+  public void next() throws IOException {
+    source.next();
+    findTop();
+  }
+
+  @Override
+  public void seek(Range range, Collection<ByteSequence> columnFamilies, boolean inclusive) throws IOException {
+    source.seek(range, columnFamilies, inclusive);
+    findTop();
+  }
+
+  /**
+   * Iterates over the source until an acceptable key/value pair is found.
+   */
+  private void findTop() throws IOException {
+    while (source.hasTop()) {
+      Key top = source.getTopKey();
+      if (top.isDeleted() || (accept(top, source.getTopValue()))) {","[{'comment': ""This condition seems strange. We want to accept deletes we see?\r\n\r\nI'd have expected `if (!top.isDeleted() && accept(top, source.getTopValue()))`, avoiding the `accept()` call when we know the key is actually a delete."", 'commenter': 'joshelser'}, {'comment': 'If `top.isDeleted()` returns true, then would `||` short circuit and not execute accept?  I think so.', 'commenter': 'keith-turner'}, {'comment': ""That's how I read it."", 'commenter': 'joshelser'}, {'comment': 'I think this code is just letting deletes pass through for compaction and not forcing users to think about deletes.', 'commenter': 'keith-turner'}, {'comment': 'But the `break` is going to pop out of the `while`, no?\r\n\r\nI could see:\r\n\r\n```java\r\nif (top.isDeleted()) {\r\n  source.next();\r\n  continue;\r\n} else if (accept(top, source.getTopValue()) {\r\n  break;\r\n}\r\n```', 'commenter': 'joshelser'}, {'comment': 'I think breaking out of the while and leaving the delete on top of the source (not calling next) is the goal.  This allows the delete to pass through for compaction.', 'commenter': 'keith-turner'}, {'comment': ""Hm. What about `IteratorUtil.setupSystemScanIterators(..)` though?\r\n\r\n```java\r\n  public static SortedKeyValueIterator<Key,Value> setupSystemScanIterators(SortedKeyValueIterator<Key,Value> source, Set<Column> cols, Authorizations auths,\r\n      byte[] defaultVisibility) throws IOException {\r\n    DeletingIterator delIter = new DeletingIterator(source, false);\r\n    ColumnFamilySkippingIterator cfsi = new ColumnFamilySkippingIterator(delIter);\r\n    ColumnQualifierFilter colFilter = new ColumnQualifierFilter(cfsi, cols);\r\n    return new VisibilityFilter(colFilter, auths, defaultVisibility);\r\n  }\r\n```\r\n\r\nWe wouldn't even get deletes at this point, right?"", 'commenter': 'joshelser'}, {'comment': 'Deletes need to pass through this filter. If we filter out a delete then we could end up un-deleting records in a partial compaction. The break happens as an indication that a top key is ready.', 'commenter': 'scubafuchs'}]"
244,core/src/main/java/org/apache/accumulo/core/iterators/ServerSkippingIterator.java,"@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.iterators;
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.accumulo.core.data.ByteSequence;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+
+public abstract class ServerSkippingIterator extends ServerWrappingIterator {","[{'comment': 'Javadoc, please.', 'commenter': 'joshelser'}, {'comment': 'Similarly, to the same question as to ""why do we need ServerWrappingIterator and WrappingIterator"", why do we need ""ServerSkippingIterator"" and not just ""SkippingIterator""?', 'commenter': 'joshelser'}, {'comment': 'Basically, because of inheritance.', 'commenter': 'scubafuchs'}, {'comment': 'Specifically, do you mean because the new Server* iterators directly access the protected source?  While the original WrappingIterator had a private source that was accessed through a getter and setter.', 'commenter': 'milleruntime'}, {'comment': ""Yes, in the case of ServerSkippingIterator it's because we wanted to do the same thing that's done in ServerWrappingIterator. ServerWrappingIterator exists because direct access to a final source wasn't feasible in WrappingIterator."", 'commenter': 'scubafuchs'}]"
244,core/src/main/java/org/apache/accumulo/core/iterators/ServerWrappingIterator.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.iterators;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.accumulo.core.data.ByteSequence;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+
+/**
+ * A convenience class for implementing iterators that select, but do not modify, entries read from a source iterator. Default implementations exist for all
+ * methods, but {@link #deepCopy} will throw an <code>UnsupportedOperationException</code>.
+ *
+ * This iterator has some checks in place to enforce the iterator contract. Specifically, it verifies that it has a source iterator and that {@link #seek} has
+ * been called before any data is read. If either of these conditions does not hold true, an <code>IllegalStateException</code> will be thrown. In particular,
+ * this means that <code>getSource().seek</code> and <code>super.seek</code> no longer perform identical actions. Implementors should take note of this and if
+ * <code>seek</code> is overridden, ensure that <code>super.seek</code> is called before data is read.
+ */
+public abstract class ServerWrappingIterator implements SortedKeyValueIterator<Key,Value> {
+
+  protected final SortedKeyValueIterator<Key,Value> source;
+
+  public ServerWrappingIterator(SortedKeyValueIterator<Key,Value> source) {
+    this.source = source;
+  }
+
+  @Override
+  public SortedKeyValueIterator<Key,Value> deepCopy(IteratorEnvironment env) {
+    throw new UnsupportedOperationException();","[{'comment': 'Why the decision to throw this exception and not let the compiler tell the user (at compile time, not run time) that they need to implement this method?', 'commenter': 'joshelser'}, {'comment': ""At least at the time, I believe that a bunch of the extending classes would have implement this method the same way, so it reduces the amount of code to have a common implementation here. I think this is more of a question about whether deepCopy should really be common across all uses of the SortedKeyValueIterator interface, or whether we should have organized it differently. Interesting topic, but probably shouldn't hold up this project."", 'commenter': 'scubafuchs'}, {'comment': 'FYI WrappingIterator currently does this for deepCopy but not init', 'commenter': 'milleruntime'}]"
244,core/src/main/java/org/apache/accumulo/core/iterators/ServerWrappingIterator.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.iterators;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.accumulo.core.data.ByteSequence;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+
+/**
+ * A convenience class for implementing iterators that select, but do not modify, entries read from a source iterator. Default implementations exist for all
+ * methods, but {@link #deepCopy} will throw an <code>UnsupportedOperationException</code>.
+ *
+ * This iterator has some checks in place to enforce the iterator contract. Specifically, it verifies that it has a source iterator and that {@link #seek} has
+ * been called before any data is read. If either of these conditions does not hold true, an <code>IllegalStateException</code> will be thrown. In particular,
+ * this means that <code>getSource().seek</code> and <code>super.seek</code> no longer perform identical actions. Implementors should take note of this and if
+ * <code>seek</code> is overridden, ensure that <code>super.seek</code> is called before data is read.
+ */
+public abstract class ServerWrappingIterator implements SortedKeyValueIterator<Key,Value> {
+
+  protected final SortedKeyValueIterator<Key,Value> source;
+
+  public ServerWrappingIterator(SortedKeyValueIterator<Key,Value> source) {
+    this.source = source;
+  }
+
+  @Override
+  public SortedKeyValueIterator<Key,Value> deepCopy(IteratorEnvironment env) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public Key getTopKey() {
+    return source.getTopKey();
+  }
+
+  @Override
+  public Value getTopValue() {
+    return source.getTopValue();
+  }
+
+  @Override
+  public boolean hasTop() {
+    return source.hasTop();
+  }
+
+  @Override
+  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options, IteratorEnvironment env) throws IOException {
+    throw new UnsupportedOperationException();","[{'comment': 'Ditto WRT the `deepCopy()` comment', 'commenter': 'joshelser'}, {'comment': ""Same as above -- a bunch of the system iterators don't use or support the init method, so this is a common implementation."", 'commenter': 'scubafuchs'}]"
244,core/src/main/java/org/apache/accumulo/core/iterators/ServerWrappingIterator.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.iterators;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.accumulo.core.data.ByteSequence;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+
+/**
+ * A convenience class for implementing iterators that select, but do not modify, entries read from a source iterator. Default implementations exist for all
+ * methods, but {@link #deepCopy} will throw an <code>UnsupportedOperationException</code>.
+ *
+ * This iterator has some checks in place to enforce the iterator contract. Specifically, it verifies that it has a source iterator and that {@link #seek} has
+ * been called before any data is read. If either of these conditions does not hold true, an <code>IllegalStateException</code> will be thrown. In particular,
+ * this means that <code>getSource().seek</code> and <code>super.seek</code> no longer perform identical actions. Implementors should take note of this and if
+ * <code>seek</code> is overridden, ensure that <code>super.seek</code> is called before data is read.
+ */
+public abstract class ServerWrappingIterator implements SortedKeyValueIterator<Key,Value> {","[{'comment': 'How/why does this need to exist when compared to the `WrappingIterator`. Going off memory, these are very similar.', 'commenter': 'joshelser'}, {'comment': 'Two differences: the source member is exposed directly to extending classes, and there is a constructor to set the source rather than a setSource() method. This allows the source to be final which I think maybe helps with runtime optimizations?', 'commenter': 'scubafuchs'}, {'comment': 'Another diff w/ Wrapping iterator is there are sanity checks every time `getSource()` is called.  ServerWrappingIterator does not have these checks.', 'commenter': 'keith-turner'}]"
244,core/src/main/java/org/apache/accumulo/core/iterators/system/VisibilityFilter.java,"@@ -16,63 +16,59 @@
  */
 package org.apache.accumulo.core.iterators.system;
 
+import org.apache.accumulo.core.data.ArrayByteSequence;
+import org.apache.accumulo.core.data.ByteSequence;
 import org.apache.accumulo.core.data.Key;
 import org.apache.accumulo.core.data.Value;
-import org.apache.accumulo.core.iterators.Filter;
 import org.apache.accumulo.core.iterators.IteratorEnvironment;
 import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
+import org.apache.accumulo.core.iterators.SynchronizedServerFilter;
 import org.apache.accumulo.core.security.Authorizations;
 import org.apache.accumulo.core.security.ColumnVisibility;
 import org.apache.accumulo.core.security.VisibilityEvaluator;
 import org.apache.accumulo.core.security.VisibilityParseException;
 import org.apache.accumulo.core.util.BadArgumentException;
-import org.apache.accumulo.core.util.TextUtil;
 import org.apache.commons.collections.map.LRUMap;
-import org.apache.hadoop.io.Text;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class VisibilityFilter extends Filter {
+public class VisibilityFilter extends SynchronizedServerFilter {","[{'comment': ""This looks to me that, for every call to this Filter we're now grabbing a lock?\r\n\r\nThat sounds *really* bad to me. What's the reasoning here?"", 'commenter': 'joshelser'}, {'comment': ""We used to have a separate SynchronizedIterator on scan scope that wrapped all of the system iterators and this moves that same functionality down into the VisibilityFilter. The purpose of synchronization is to prevent leakage of intermediate values through methods like getTopKey and getTopValue while another thread is calling next. This is only necessary when a configurable iterator does something multi-threaded. There are at least a couple of ways to avoid the need to make this iterator synchronized: (1) make the system iterators thread safe as exposed by the top iterator in some other way, at least from a security perspective, or (2) detect and/or disallow any use of threading in configurable iterators. Maybe you can do (1) by eliminating the possibility of leaking top keys and values that are still being evaluated by the filter? Probably needs a test to make sure we don't break that in the future."", 'commenter': 'scubafuchs'}]"
244,core/src/main/java/org/apache/accumulo/core/iterators/WrappingIterator.java,"@@ -58,21 +58,21 @@ protected void setSource(SortedKeyValueIterator<Key,Value> source) {
   public Key getTopKey() {
     if (seenSeek == false)
       throw new IllegalStateException(""never been seeked"");
-    return getSource().getTopKey();
+    return source.getTopKey();","[{'comment': ""This could possibly break iterators that extend WrappingIterator and override getSource().  I don't think we should make this change.\r\n\r\nMaybe it would be better to deprecate WrappingIterator and introduce another version for users that more efficient.  I don't think we have a lot of wiggle room to make this one more efficient without possibly breaking user iterators."", 'commenter': 'keith-turner'}, {'comment': ""In Sqrrl we use a LightWrappingIterator that does this. I'd be in favor of deprecating the WrappingIterator, but we might want to couple that with the bigger iterator rewrite project (ACCUMULO-3751)."", 'commenter': 'scubafuchs'}, {'comment': 'Is LightWrappingIterator the same as ServerWrappingIterator in this PR?', 'commenter': 'keith-turner'}, {'comment': ""No, LightWrappingIterator is a user-level iterator, so it supports a no-argument constructor and non-final source to be compatible with the initialization via init method. It basically just removes some check, doesn't call getSource(), and exposes the source field as protected instead of private."", 'commenter': 'scubafuchs'}, {'comment': ""Interesting, the init method prevents making the source final.  Didn't see that one coming."", 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -245,7 +245,8 @@
   TSERV_PREFIX(""tserver."", null, PropertyType.PREFIX, ""Properties in this category affect the behavior of the tablet servers""),
   TSERV_CLIENT_TIMEOUT(""tserver.client.timeout"", ""3s"", PropertyType.TIMEDURATION, ""Time to wait for clients to continue scans before closing a session.""),
   TSERV_DEFAULT_BLOCKSIZE(""tserver.default.blocksize"", ""1M"", PropertyType.BYTES, ""Specifies a default blocksize for the tserver caches""),
-  TSERV_CACHE_POLICY(""tserver.cache.policy"", ""LRU"", PropertyType.STRING, ""Specifies the eviction policy of the file data caches (LRU or TinyLFU).""),","[{'comment': 'Need to support backwards compatibility for configuration properties.', 'commenter': 'joshelser'}, {'comment': 'for a 2.0 release?\r\nedit: I think we just need to mark the property deprecated in an earlier release (1.8.2?)', 'commenter': 'dlmarion'}, {'comment': 'Yes.\r\n\r\nAnd, before we get there, I am aware that this is technically not covered by our semver use. However, historically, we have treated configuration properties as such. We should not be dropping old, user-defined configuration keys without a deprecation cycle.', 'commenter': 'joshelser'}, {'comment': 'I am not sure this was in Accumulo 1.8', 'commenter': 'keith-turner'}, {'comment': 'It looks like you are correct @keith-turner , this property does not appear in the 1.8 branch.', 'commenter': 'dlmarion'}, {'comment': 'Sweet.  I am glad you are making this change now before 2.0.0.  Much easier to make this change.', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/BlockCache.java,"@@ -17,10 +17,30 @@
  */
 package org.apache.accumulo.core.file.blockfile.cache;
 
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+
 /**
  * Block cache interface.
  */
 public interface BlockCache {
+
+  /**
+   * Start the block cache
+   *
+   * @param conf
+   *          Accumulo configuration object
+   * @param maxSize
+   *          maximum size of the on-heap cache
+   * @param blockSize
+   *          size of the default RFile blocks
+   */
+  void start(AccumuloConfiguration conf, long maxSize, long blockSize) throws Exception;","[{'comment': ""If `Exception` is just being caught and re-thrown as RTE, isn't it cleaner to just removes the `throws` here and inform, via javadoc, implementations should throw an RTE if they cannot start successfully?"", 'commenter': 'joshelser'}, {'comment': 'Also, we could separate the configuration and start logic into separate methods.\r\n\r\nUnrelated thought: what about creating a POJO to encapsulate this information. It would prevent drift in the configuration details from breaking blockcache impls (e.g. BlockCacheConfiguration, presently made up of an AccuConf, maxSize, and blockSize).', 'commenter': 'joshelser'}, {'comment': ""re: POJO -> I was going to just use the AccumuloConfiguration and the custom properties for this. I'm not sure how a single BlockCacheConfiguration class would work across different BlockCache impls that have different configuration parameters."", 'commenter': 'dlmarion'}, {'comment': 'Hmm, yeah, I later realized most impls would find some way to piggy-back properties into accumulo-site. Mostly, I figured that, eventually, we\'d also have other ""universal"" properties (max size and block size) that all of the implementations would use.\r\n\r\nHaving some object there (or, using the Factory as Keith\'s suggestion pushes towards) would at least avoid runtime issues (where the new Accumulo expects a `start(AccumuloConfiguration, long, long, ...)` instead of just `start(AccumuloConfiguration, long, long)`.', 'commenter': 'joshelser'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/LruBlockCache.java,"@@ -84,7 +85,7 @@
   static final int statThreadPeriod = 60;
 
   /** Concurrent map (the cache) */
-  private final ConcurrentHashMap<String,CachedBlock> map;","[{'comment': 'It may be nice to keep the ability to have these final for performance reasons.  I think we could do this if we made the BlockCacheFactory configurable instead of the BlockCache.  For example if we had an interface like\r\n\r\n```java \r\npublic interface BlockCacheFactory {\r\n  /**\r\n   * TODO document how this can utilize the new arbitrary instance config\r\n   */\r\n  public BlockCache getBlockCache(AccumuloConfiguration conf);\r\n} \r\n```  \r\n\r\nIf we had that then we could configure an LruBlockCacheFactory that actually calls the constructor, enabling us to keep using final.\r\n\r\n```java\r\npublic class LruBlockCacheFactory implement BlockCacheFactory {\r\n   public BlockCache getBlockCache(AccumuloConfiguration conf) {\r\n        //read whatever is needed from config in local vars\r\n        return new LruBlockCache(localVar1, localVar2);\r\n   }\r\n}\r\n```\r\n\r\nWith this, the `start()` method would not be needed also.  The `start()` method makes assumptions about the required config.  Using the configurable factory makes no assumptions about the config needed.', 'commenter': 'keith-turner'}, {'comment': 'One more thought.. may want to have an enum cache type.\r\n\r\n```java\r\nenum CacheType {\r\n  INDEX,\r\n  DATA,\r\n  SUMMARY\r\n}\r\n```\r\n\r\nThen the factory could key use this.\r\n\r\n```java\r\npublic interface BlockCacheFactory {\r\n  /**\r\n   * TODO document how this can utilize the new arbitrary instance config\r\n   */\r\n  public BlockCache getBlockCache(CacheType ct, AccumuloConfiguration conf);\r\n} \r\n```\r\n\r\nThen the implementation could use this for differentiated config.\r\n\r\n```java\r\npublic class SuperDuperBlockCacheFactory implement BlockCacheFactory {\r\n   public BlockCache getBlockCache(CacheType ct, AccumuloConfiguration conf) {\r\n       long knob42 = conf.getMemory(""general.custom.superdupercacher.""+ct.name()+"".knob42"");\r\n        //read whatever is needed from config in local vars\r\n        return new SuperDuperBlockCacheFactory(knob42);\r\n   }\r\n}\r\n```\r\n', 'commenter': 'keith-turner'}, {'comment': 'The Lru cache factory could use the CacheType to get `""tserver.cache.data.size` or `tserver.cache.index.size`', 'commenter': 'keith-turner'}, {'comment': 'I like these suggestions -- more than my own :)', 'commenter': 'joshelser'}, {'comment': ""If have a CacheType, then could possibly deprecate the `tserver.cache.data.size` type settings in favor of every implementation using `general.custom.xyz.` settings.  Or we could assume that every implementation will at least want a max size and keep these settings around.  I don't know whats best."", 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactory.java,"@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.file.blockfile.cache;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+
+public abstract class BlockCacheFactory {","[{'comment': 'Why not make this an interface?', 'commenter': 'keith-turner'}, {'comment': 'I think it was initially, I will look at that again.', 'commenter': 'dlmarion'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactory.java,"@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.file.blockfile.cache;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+
+public abstract class BlockCacheFactory {
+
+  public static BlockCacheFactory getBlockCacheFactory(AccumuloConfiguration conf) throws Exception {
+    String impl = conf.get(Property.TSERV_CACHE_IMPL);
+    Class<? extends BlockCacheFactory> clazz = AccumuloVFSClassLoader.loadClass(impl, BlockCacheFactory.class);
+    return clazz.newInstance();
+  }
+
+  public abstract BlockCache getBlockCache(AccumuloConfiguration conf);","[{'comment': 'This method could have java with `@see BlockCacheConfiguration a helper class for configuring cache instances`', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/lru/LruBlockCacheConfiguration.java,"@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.file.blockfile.cache.lru;
+
+import java.util.Map;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.file.blockfile.cache.BlockCacheConfiguration;
+
+public final class LruBlockCacheConfiguration extends BlockCacheConfiguration {","[{'comment': 'Nice that this class is final and all of its members a final.  I am not sure if LruBlockCache accesses these frequently, but if it does making stuff final will help.', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -245,7 +245,8 @@
   TSERV_PREFIX(""tserver."", null, PropertyType.PREFIX, ""Properties in this category affect the behavior of the tablet servers""),
   TSERV_CLIENT_TIMEOUT(""tserver.client.timeout"", ""3s"", PropertyType.TIMEDURATION, ""Time to wait for clients to continue scans before closing a session.""),
   TSERV_DEFAULT_BLOCKSIZE(""tserver.default.blocksize"", ""1M"", PropertyType.BYTES, ""Specifies a default blocksize for the tserver caches""),
-  TSERV_CACHE_POLICY(""tserver.cache.policy"", ""LRU"", PropertyType.STRING, ""Specifies the eviction policy of the file data caches (LRU or TinyLFU).""),
+  TSERV_CACHE_IMPL(""tserver.cache.factory.class"", ""org.apache.accumulo.core.file.blockfile.cache.lru.LRUBlockCacheFactory.class"", PropertyType.STRING,","[{'comment': 'Should include `FACTORY` in the enum name.  Maybe call it `TSERV_CACHE_FACTORY` or `TSERV_CACHE_FACTORY_IMPL`', 'commenter': 'keith-turner'}]"
256,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java,"@@ -175,19 +176,27 @@ public TabletServerResourceManager(TabletServer tserver, VolumeManager fs) {
     long sCacheSize = acuConf.getAsBytes(Property.TSERV_SUMMARYCACHE_SIZE);
     long totalQueueSize = acuConf.getAsBytes(Property.TSERV_TOTAL_MUTATION_QUEUE_MAX);
 
-    String policy = acuConf.get(Property.TSERV_CACHE_POLICY);
-    if (policy.equalsIgnoreCase(""LRU"")) {
-      _iCache = new LruBlockCache(iCacheSize, blockSize);
-      _dCache = new LruBlockCache(dCacheSize, blockSize);
-      _sCache = new LruBlockCache(sCacheSize, blockSize);
-    } else if (policy.equalsIgnoreCase(""TinyLFU"")) {
-      _iCache = new TinyLfuBlockCache(iCacheSize, blockSize);
-      _dCache = new TinyLfuBlockCache(dCacheSize, blockSize);
-      _sCache = new TinyLfuBlockCache(sCacheSize, blockSize);
-    } else {
-      throw new IllegalArgumentException(""Unknown Block cache policy "" + policy);
+    BlockCacheFactory factory;
+    try {
+      factory = BlockCacheFactory.getBlockCacheFactory(acuConf);
+    } catch (Exception e) {
+      throw new RuntimeException(""Error creating Block Cache Factory"", e);
     }
 
+    ConfigurationCopy copy = new ConfigurationCopy(acuConf);
+    copy.set(BlockCacheConfiguration.BLOCK_SIZE_PROPERTY, Long.toString(blockSize));
+    copy.set(BlockCacheConfiguration.MAX_SIZE_PROPERTY, Long.toString(iCacheSize));","[{'comment': 'This behavior is kind of tricky.  What if these properties were already set in acuConf, what should the behavior be?  The javadoc for BlockCacheFactory would need to mention this behavior.  \r\n\r\n', 'commenter': 'keith-turner'}, {'comment': '@dlmarion did you see my earlier comment about having a CacheType enum?  That could make the need for this to go away.  The constuctor for `BlockCacheFactory` could take a `CacheType` and use it to get the correct max size property.', 'commenter': 'keith-turner'}, {'comment': 'Also, putting the CacheType i the factory API allows the user to tune the cache differently for index vs data vs summary.  For example if my cache impl has 5 tuning parameters, I may want to set tune differently for index vs data cache.', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/lru/LruBlockCacheConfiguration.java,"@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.file.blockfile.cache.lru;
+
+import java.util.Map;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.file.blockfile.cache.BlockCacheConfiguration;
+
+public final class LruBlockCacheConfiguration extends BlockCacheConfiguration {
+
+  /** Default Configuration Parameters */
+
+  /** Backing Concurrent Map Configuration */
+  public static final Float DEFAULT_LOAD_FACTOR = 0.75f;
+  public static final Integer DEFAULT_CONCURRENCY_LEVEL = 16;
+
+  /** Eviction thresholds */
+  public static final Float DEFAULT_MIN_FACTOR = 0.75f;
+  public static final Float DEFAULT_ACCEPTABLE_FACTOR = 0.85f;
+
+  /** Priority buckets */
+  public static final Float DEFAULT_SINGLE_FACTOR = 0.25f;
+  public static final Float DEFAULT_MULTI_FACTOR = 0.50f;
+  public static final Float DEFAULT_MEMORY_FACTOR = 0.25f;
+
+  // property names
+  private static final String PREFIX = Property.GENERAL_ARBITRARY_PROP_PREFIX + ""cache.block.lru."";","[{'comment': 'I think it would be nice if the super class handled things in such a way that all subclasses did not have deal with the full property name.  Not sure the best way to do this.  One way I thought of was the following.\r\n\r\nIn `LruBlockCacheConfiguration` constuctor call super constructor with a desired prefix.  This prefix will be used with all gets.\r\n```java\r\npublic LruBlockCacheConfiguration(CacheType ct, AccumuloConfiguration conf) {\r\n    super(ct, ""lru"", conf);\r\n    // this will actually get ""general.custom.cache.lru.all.acceptable.factor"" ... saving the subclass some work... also encourages use of a standard prefix\r\n    this.acceptableFactor = getOrDefault(""acceptable.factor"", DEFAULT_ACCEPTABLE_FACTOR);\r\n\r\n   // could suppor the CacheType concept... this could get \r\n   // ""general.custom.cache.lru.index.acceptable.factor"" or ""general.custom.cache.lru.data.acceptable.factor""\r\n   // depending on value of ct\r\n    this.acceptableFactor = getOrDefault(ct, ""acceptable.factor"", DEFAULT_ACCEPTABLE_FACTOR);\r\n```\r\n\r\n\r\n', 'commenter': 'keith-turner'}, {'comment': 'The `getOrDefault(CacheType ct, String key, T default)` method could fall back to calling `getOrDefault(String key, T default)` in the case where nothing is set for the specific cache type.', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactory.java,"@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.file.blockfile.cache;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public abstract class BlockCacheFactory<B extends BlockCache,C extends BlockCacheConfiguration> {","[{'comment': 'There are two significant changes that I am wondering what the motivations for was.  First, this factory seems to cahce cahces and now be responsible for starting and stopping them.  Earlier you had the start and stop method on the cache itself.  Why make the BlockCacheFactory stateful?\r\n\r\nSecond, BlockCacheConfiguration used to be a helper class that someone implementing a BlockCacheFactory could optionally use.  Why tightly couple these?', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactory.java,"@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.file.blockfile.cache;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public abstract class BlockCacheFactory<B extends BlockCache,C extends BlockCacheConfiguration> {
+
+  public static final String CACHE_PROPERTY_BASE = Property.GENERAL_ARBITRARY_PROP_PREFIX + ""cache.block."";
+
+  private static final Logger LOG = LoggerFactory.getLogger(BlockCacheFactory.class);
+  private static BlockCacheFactory<?,?> factory = null;
+
+  private final Map<CacheType,B> caches = new HashMap<>();
+
+  /**
+   * Initialize the caches for each CacheType based on the configuration
+   * 
+   * @param conf
+   *          accumulo configuration
+   */
+  public void start(AccumuloConfiguration conf) {
+    for (CacheType type : CacheType.values()) {
+      ConfigurationCopy props = type.getCacheProperties(conf, getCacheImplName());
+      if (null != props) {
+        C cc = this.createConfiguration(props, type, this);
+        B cache = this.createCache(cc);
+        LOG.info(""Created {} cache with configuration {}"", type, cc);
+        this.caches.put(type, cache);
+      }
+    }
+  }
+
+  /**
+   * Stop caches and release resources
+   */
+  public abstract void stop();
+
+  /**
+   * Get the block cache of the given type
+   * 
+   * @param type
+   *          block cache type
+   * @return BlockCache or null if not enabled
+   */
+  public B getBlockCache(CacheType type) {
+    return caches.get(type);
+  }
+
+  /**
+   * Parse and validate the configuration
+   * 
+   * @param conf
+   *          accumulo configuration
+   * @param type
+   *          cache type
+   * @param name
+   *          cache implementation name
+   * @return validated block cache configuration
+   */
+  protected abstract C createConfiguration(AccumuloConfiguration conf, CacheType type, BlockCacheFactory<B,C> factory);
+
+  /**
+   * Create a block cache using the supplied configuration
+   * 
+   * @param conf
+   *          cache configuration
+   * @return configured block cache
+   */
+  protected abstract B createCache(C conf);
+
+  /**
+   * Cache implementation name (e.g lru, tinylfu, etc)
+   * 
+   * @return name of cache implementation in lowercase
+   */
+  public abstract String getCacheImplName();
+
+  /**
+   * Get the BlockCacheFactory specified by the property 'tserver.cache.factory.class'
+   * 
+   * @param conf
+   *          accumulo configuration
+   * @return BlockCacheFactory instance
+   * @throws Exception
+   */
+  public static synchronized BlockCacheFactory<?,?> getInstance(AccumuloConfiguration conf) throws Exception {
+    if (null == factory) {
+      String impl = conf.get(Property.TSERV_CACHE_FACTORY_IMPL);
+      @SuppressWarnings(""rawtypes"")
+      Class<? extends BlockCacheFactory> clazz = AccumuloVFSClassLoader.loadClass(impl, BlockCacheFactory.class);
+      factory = (BlockCacheFactory<?,?>) clazz.newInstance();","[{'comment': 'I Think we should avoid static caching of this.  The user can can keep a ref while the keep their caches. This could cause problems with code that attempts to recreate caches because the config changed.', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheConfiguration.java,"@@ -0,0 +1,92 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.file.blockfile.cache;
+
+import java.util.Map;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+
+public class BlockCacheConfiguration {
+
+  public static final String MAX_SIZE_PROPERTY = ""max.size"";
+  public static final String BLOCK_SIZE_PROPERTY = ""block.size"";
+
+  private static final Long DEFAULT = Long.valueOf(-1);
+
+  /** Maximum allowable size of cache (block put if size > max, evict) */
+  private final long maxSize;
+
+  /** Approximate block size */
+  private final long blockSize;
+
+  protected final BlockCacheConfigurationHelper helper;
+
+  public BlockCacheConfiguration(AccumuloConfiguration conf, CacheType type, BlockCacheFactory<?,?> factory) {
+
+    helper = new BlockCacheConfigurationHelper(conf, type, factory);
+
+    Map<String,String> props = conf.getAllPropertiesWithPrefix(Property.GENERAL_ARBITRARY_PROP_PREFIX);
+    this.maxSize = getOrDefault(props, helper.getFullPropertyName(MAX_SIZE_PROPERTY), DEFAULT);","[{'comment': 'Could do the following here and avoid having to create the copy config elsewhere.\r\n\r\n```java\r\nthis.blockSize = conf.getAsBytes(Property.TSERV_DEFAULT_BLOCKSIZE);\r\nswitch(type) {\r\n  case INDEX:\r\n    this.maxSize = conf.getAsBytes(Property.TSERV_INDEXCACHE_SIZE);\r\n    break;\r\n  case DATA:\r\n    this.maxSize = conf.getAsBytes(Property.TSERV_DATACACHE_SIZE);\r\n    break;\r\n  case SUMMARY:\r\n    this.maxSize =  conf.getAsBytes(Property.TSERV_SUMMARYCACHE_SIZE);\r\n    break;\r\n  default:\r\n    throw new IllegalArgumentException();\r\n}\r\n```', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java,"@@ -135,16 +142,41 @@ public long requestCount() {
     }
 
     this.opts = opts;
-    if (opts.indexCacheSize > 0) {
-      this.indexCache = new LruBlockCache(opts.indexCacheSize, CACHE_BLOCK_SIZE);
-    } else {
-      this.indexCache = new NoopCache();
-    }
 
-    if (opts.dataCacheSize > 0) {
-      this.dataCache = new LruBlockCache(opts.dataCacheSize, CACHE_BLOCK_SIZE);
-    } else {
-      this.dataCache = new NoopCache();
+    if (opts.indexCacheSize > 0 || opts.dataCacheSize > 0) {
+      ConfigurationCopy cc = new ConfigurationCopy();","[{'comment': 'RFileScanner has `opts.tableConfig` which allows the user to set accumulo properties.  Could use this create this cache.  This would allow a user to change the cache factory impl.   ', 'commenter': 'keith-turner'}]"
256,core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java,"@@ -135,16 +142,41 @@ public long requestCount() {
     }
 
     this.opts = opts;
-    if (opts.indexCacheSize > 0) {
-      this.indexCache = new LruBlockCache(opts.indexCacheSize, CACHE_BLOCK_SIZE);
-    } else {
-      this.indexCache = new NoopCache();
-    }
 
-    if (opts.dataCacheSize > 0) {
-      this.dataCache = new LruBlockCache(opts.dataCacheSize, CACHE_BLOCK_SIZE);
-    } else {
-      this.dataCache = new NoopCache();
+    if (opts.indexCacheSize > 0 || opts.dataCacheSize > 0) {
+      ConfigurationCopy cc = new ConfigurationCopy();
+      cc.set(Property.TSERV_CACHE_FACTORY_IMPL, LruBlockCacheFactory.class.getName());
+      try {
+        factory = BlockCacheFactory.getInstance(cc);","[{'comment': 'This will use Accumulo start to load classes.  Currently, Accumulo code that loads classes client side avoids using Accumulo start.', 'commenter': 'keith-turner'}]"
270,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -894,12 +894,19 @@ public static void removeBulkLoadEntries(Connector conn, String tableId, long ti
         BatchWriter bw = conn.createBatchWriter(MetadataTable.NAME, new BatchWriterConfig())) {
       mscanner.setRange(new KeyExtent(tableId, null, null).toMetadataRange());
       mscanner.fetchColumnFamily(TabletsSection.BulkFileColumnFamily.NAME);
+      boolean shouldTrace = log.isTraceEnabled();
+      String tidString = Long.toString(tid);
       for (Entry<Key,Value> entry : mscanner) {
-        log.debug(""Looking at entry "" + entry + "" with tid "" + tid);
-        if (Long.parseLong(entry.getValue().toString()) == tid) {
-          log.debug(""deleting entry "" + entry);
-          Mutation m = new Mutation(entry.getKey().getRow());
-          m.putDelete(entry.getKey().getColumnFamily(), entry.getKey().getColumnQualifier());
+        if (shouldTrace) {
+          log.trace(""Looking at entry "" + entry + "" with tid "" + tidString);","[{'comment': 'We are using SLF4J in this class, right? You should be able to use the message syntax with object place holders here.', 'commenter': 'dlmarion'}, {'comment': 'I don\'t think it buys us anything to use that syntax in this case.  I would still want to check the log level outside the loop to avoid calling the logger for every ""loaded"" entry.  The logger calls were where I saw the blocking, even when the log level was at INFO.', 'commenter': 'matthpeterson'}, {'comment': 'Where was that blocking? The problem of using the log(string) is that it calls the forced logger, whereas using the message syntax as @dlmarion  should bypass the synchronized check in the log4j category class ( if I recall correctly ). Can you double check that? ', 'commenter': 'phrocker'}, {'comment': 'Yes it was related to the forced logger:\r\norg.apache.log4j.Category.callAppenders(Category:204)\r\nwaiting for lock .. (a org.apache.log4j.Logger)\r\n..Category.forcedLog..\r\n..Category.debug..\r\n... removeBulkEntries(MetadataTableUtil.java:911)\r\n\r\nYes, it looks like the slf4j implementation I checked avoids the forceLog call and instead checks against the level which would not block.', 'commenter': 'matthpeterson'}, {'comment': ""It's ultimately cleaner to keep the conditional in slf4j. Does this change actually provide benefit over using SLF4j with the better call? "", 'commenter': 'phrocker'}, {'comment': ""The version i modified wasn't using SLF4j yet so I'm more confident using the approach I took given that I haven't tested whether SLF4j achieves the same result."", 'commenter': 'matthpeterson'}, {'comment': 'That\'s a pretty common paradigm that\'s used in SLF4J. what do you mean by ""Achieves the same result?"" It should achieve the same result. If it does not then there is an issue with SLF4J, at which point I would agree with you. If it does achieve the same result and the conditional is unnecessary, then why does it exist?', 'commenter': 'phrocker'}]"
270,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -894,12 +894,19 @@ public static void removeBulkLoadEntries(Connector conn, String tableId, long ti
         BatchWriter bw = conn.createBatchWriter(MetadataTable.NAME, new BatchWriterConfig())) {
       mscanner.setRange(new KeyExtent(tableId, null, null).toMetadataRange());
       mscanner.fetchColumnFamily(TabletsSection.BulkFileColumnFamily.NAME);
+      boolean shouldTrace = log.isTraceEnabled();
+      String tidString = Long.toString(tid);","[{'comment': 'This is a nice optimization.  Could take it a step further and convert the string to a byte array and compare (using Arrays class) byte arrays in the loop.  This avoids converting each vals byte array to a string in the loop. ', 'commenter': 'keith-turner'}, {'comment': 'Good idea.  Updated', 'commenter': 'matthpeterson'}]"
270,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -894,12 +895,19 @@ public static void removeBulkLoadEntries(Connector conn, String tableId, long ti
         BatchWriter bw = conn.createBatchWriter(MetadataTable.NAME, new BatchWriterConfig())) {
       mscanner.setRange(new KeyExtent(tableId, null, null).toMetadataRange());
       mscanner.fetchColumnFamily(TabletsSection.BulkFileColumnFamily.NAME);
+      boolean shouldTrace = log.isTraceEnabled();
+      byte[] tidAsBytes = Long.toString(tid).getBytes();","[{'comment': ""For predictability in different environments, should call `getBytes(StandardCharsets.UTF_8)`.  Value's toString method did something like this in the prev code."", 'commenter': 'keith-turner'}, {'comment': 'Updated, thanks.', 'commenter': 'matthpeterson'}]"
270,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -894,12 +895,19 @@ public static void removeBulkLoadEntries(Connector conn, String tableId, long ti
         BatchWriter bw = conn.createBatchWriter(MetadataTable.NAME, new BatchWriterConfig())) {
       mscanner.setRange(new KeyExtent(tableId, null, null).toMetadataRange());
       mscanner.fetchColumnFamily(TabletsSection.BulkFileColumnFamily.NAME);
+      boolean shouldTrace = log.isTraceEnabled();
+      byte[] tidAsBytes = Long.toString(tid).getBytes(UTF_8);
       for (Entry<Key,Value> entry : mscanner) {
-        log.debug(""Looking at entry "" + entry + "" with tid "" + tid);
-        if (Long.parseLong(entry.getValue().toString()) == tid) {
-          log.debug(""deleting entry "" + entry);
-          Mutation m = new Mutation(entry.getKey().getRow());
-          m.putDelete(entry.getKey().getColumnFamily(), entry.getKey().getColumnQualifier());
+        if (shouldTrace) {
+          log.trace(""Looking at entry "" + entry + "" with tid "" + tid);","[{'comment': 'log.trace(""Looking at entry {} with tid {}"", entry, tid);\r\n\r\nThen, I think you can remove shouldTrace.', 'commenter': 'dlmarion'}, {'comment': 'I\'m fine with that syntax but I\'d still want the shouldTrace check:\r\n\r\n> I would still want to check the log level outside the loop to avoid calling the logger for every ""loaded"" entry. The logger calls were where I saw the blocking, even when the log level was at INFO.', 'commenter': 'matthpeterson'}, {'comment': ""Fair enough - just realize it's being checked twice.\r\n\r\nhttps://github.com/qos-ch/slf4j/blob/master/slf4j-log4j12/src/main/java/org/slf4j/impl/Log4jLoggerAdapter.java#L175\r\n"", 'commenter': 'dlmarion'}, {'comment': 'This is not related to this PR, but its something I learned about slf4j vs log4j.  This conversation reminded me of it and I thought it was worth sharing.\r\n\r\nVarargs methods can be expensive in a tight loop, because it allocates an array and copies to it.  Slf4j avoids this for one and two arguments by providing specialized methods.  However 3 or more args will use varargs.  \r\n\r\nI learned from @EdColeman that log4j V2  provides specialized methods for up to 10 arguments to avoid varargs. For example this is the [debug method that takes 10 args](https://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Logger.html#debug-org.apache.logging.log4j.Marker-java.lang.String-java.lang.Object-java.lang.Object-java.lang.Object-java.lang.Object-java.lang.Object-java.lang.Object-java.lang.Object-java.lang.Object-java.lang.Object-java.lang.Object-)\r\n\r\nWish slf4j had this.', 'commenter': 'keith-turner'}]"
270,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -894,12 +895,19 @@ public static void removeBulkLoadEntries(Connector conn, String tableId, long ti
         BatchWriter bw = conn.createBatchWriter(MetadataTable.NAME, new BatchWriterConfig())) {
       mscanner.setRange(new KeyExtent(tableId, null, null).toMetadataRange());
       mscanner.fetchColumnFamily(TabletsSection.BulkFileColumnFamily.NAME);
+      boolean shouldTrace = log.isTraceEnabled();
+      byte[] tidAsBytes = Long.toString(tid).getBytes(UTF_8);
       for (Entry<Key,Value> entry : mscanner) {
-        log.debug(""Looking at entry "" + entry + "" with tid "" + tid);
-        if (Long.parseLong(entry.getValue().toString()) == tid) {
-          log.debug(""deleting entry "" + entry);
-          Mutation m = new Mutation(entry.getKey().getRow());
-          m.putDelete(entry.getKey().getColumnFamily(), entry.getKey().getColumnQualifier());
+        if (shouldTrace) {","[{'comment': 'unnecessary and this should be removed when you merge it @mjwall ', 'commenter': 'phrocker'}, {'comment': 'agreed and will do.  ', 'commenter': 'mjwall'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/AbstractId.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+
+import java.io.Serializable;
+import java.util.Objects;
+
+/**
+ * An abstract identifier class for comparing equality of identifiers of the same type.
+ */
+public abstract class AbstractId implements Comparable<AbstractId>, Serializable {
+
+  private static final long serialVersionUID = -155513612834787244L;
+  private final String canonical;
+  private Integer hashCode = null;
+
+  protected AbstractId(final String canonical) {
+    requireNonNull(canonical, ""canonical cannot be null"");
+    this.canonical = canonical;
+  }
+
+  /**
+   * The canonical ID
+   */
+  public final String canonicalID() {
+    return canonical;
+  }
+
+  public boolean isEmpty() {
+    return canonical.isEmpty();
+  }
+
+  /**
+   * AbstractID objects are considered equal if, and only if, they are of the same type and have the same canonical identifier.
+   */
+  @Override
+  public boolean equals(final Object obj) {
+    return obj != null && Objects.equals(getClass(), obj.getClass()) && Objects.equals(canonicalID(), ((AbstractId) obj).canonicalID());
+  }
+
+  @Override
+  public int hashCode() {
+    if (hashCode == null) {
+      hashCode = Objects.hash(canonicalID());
+    }
+    return hashCode;
+  }
+
+  /**
+   * Returns a string of the canonical ID
+   */
+  @Override
+  public String toString() {
+    return canonical;
+  }
+
+  /**
+   * Return a UTF_8 byte[] of the canonical ID.
+   */
+  public final byte[] getUtf8Bytes() {
+    return canonical.getBytes(UTF_8);
+  }
+
+  public int compareTo(AbstractId id) {","[{'comment': 'Missing Override annotation?', 'commenter': 'ctubbsii'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/AbstractId.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+
+import java.io.Serializable;
+import java.util.Objects;
+
+/**
+ * An abstract identifier class for comparing equality of identifiers of the same type.
+ */
+public abstract class AbstractId implements Comparable<AbstractId>, Serializable {","[{'comment': 'Do we need this to be serializable right now? Where are these serialized? Will that break compatibility with previously serialized objects?', 'commenter': 'ctubbsii'}, {'comment': 'I am not sure exactly why but without it findbugs will flag a bunch of classes in master with this bug:\r\n[INFO] Class org.apache.accumulo.master.tableOps.BulkImport defines non-transient non-serializable instance field tableId [org.apache.accumulo.master.tableOps.BulkImport] In BulkImport.java SE_BAD_FIELD\r\n\r\nIt seems like everything in tableOps with a tableId requires it (I guess because of FATE operations?) \r\n', 'commenter': 'milleruntime'}, {'comment': ""Stuff in tableOps are FATE operations, which are serialized and stored in ZK. So, this object must also be Serializable, so it can be included in the FATE serialization. That's unfortunate. It's probably okay, as long as we check for outstanding FATE operations from the previous shutdown, before we modify *ANYTHING* during the upgrade process, because we won't be able to deserialize FATEs from before the upgrade."", 'commenter': 'ctubbsii'}, {'comment': 'From https://docs.oracle.com/javase/tutorial/java/javaOO/nested.html it looks serialization of nested classes is strongly discouraged.', 'commenter': 'mikewalch'}, {'comment': 'I believe this only applies to non static inner classes.  https://www.securecoding.cert.org/confluence/display/java/SER05-J.+Do+not+serialize+instances+of+inner+classes', 'commenter': 'milleruntime'}]"
279,core/src/main/java/org/apache/accumulo/core/client/TableOfflineException.java,"@@ -26,7 +27,7 @@ private static String getTableName(Instance instance, String tableId) {
     if (tableId == null)
       return "" <unknown table> "";
     try {
-      String tableName = Tables.getTableName(instance, tableId);
+      String tableName = Tables.getTableName(instance, new Table.ID(tableId));","[{'comment': ""Could maybe avoid duplicates by making constructor private and doing `Table.ID.of(tableId)`, which draws from an internal `WeakReference` map. I think we do this in one place with table names. It's not necessary on a first pass, but maybe something to think about if this object creation starts killing performance."", 'commenter': 'ctubbsii'}, {'comment': 'I like this idea.  I can definitely create a follow on ticket.', 'commenter': 'milleruntime'}, {'comment': 'HBase does this with their `TableName` class to avoid tons of objects hanging around needing GC. The API is pretty nice as an end-user:\r\n\r\n```java\r\nConnection conn = ...;\r\nTable t = conn.getTable(TableName.valueOf(""josh_table1""));\r\n```', 'commenter': 'joshelser'}]"
279,server/master/src/main/java/org/apache/accumulo/master/tableOps/WriteExportFiles.java,"@@ -144,16 +145,15 @@ public static void exportTable(VolumeManager fs, AccumuloServerContext context,
     BufferedOutputStream bufOut = new BufferedOutputStream(zipOut);
     DataOutputStream dataOut = new DataOutputStream(bufOut);
 
-    try {
+    try (OutputStreamWriter osw = new OutputStreamWriter(dataOut, UTF_8)) {","[{'comment': 'This got dinged by Findbugs for not closing the stream so I had put the OutputStreamWriter in the try with resources.', 'commenter': 'milleruntime'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Namespaces.java,"@@ -72,10 +78,10 @@ public String invalidMessage(String namespace) {
     }
   };
 
-  public static final String DEFAULT_NAMESPACE_ID = ""+default"";
-  public static final String DEFAULT_NAMESPACE = """";
-  public static final String ACCUMULO_NAMESPACE_ID = ""+accumulo"";
-  public static final String ACCUMULO_NAMESPACE = ""accumulo"";
+  public static final Namespace.ID DEFAULT_NAMESPACE_ID = Namespace.ID.DEFAULT;
+  public static final String DEFAULT_NAMESPACE = Namespace.DEFAULT;
+  public static final Namespace.ID ACCUMULO_NAMESPACE_ID = Namespace.ID.ACCUMULO;
+  public static final String ACCUMULO_NAMESPACE = Namespace.ACCUMULO;","[{'comment': 'Could probably remove these in future, replacing them with what they point to.', 'commenter': 'ctubbsii'}, {'comment': ""Agreed.  This was an attempt to minimize number of changes in one PR.  Assuming there aren't any showstoppers with these changes, I will create follow on ticket."", 'commenter': 'milleruntime'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Namespaces.java,"@@ -100,50 +106,150 @@ private static ZooCache getZooCache(Instance instance) {
     return namespaceMap;
   }
 
-  public static boolean exists(Instance instance, String namespaceId) {
+  public static boolean exists(Instance instance, Namespace.ID namespaceId) {
     ZooCache zc = getZooCache(instance);
     List<String> namespaceIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES);
-    return namespaceIds.contains(namespaceId);
-  }
-
-  public static String getNamespaceId(Instance instance, String namespace) throws NamespaceNotFoundException {
-    String id = getNameToIdMap(instance).get(namespace);
-    if (id == null)
-      throw new NamespaceNotFoundException(null, namespace, ""getNamespaceId() failed to find namespace"");
-    return id;
+    return namespaceIds.contains(namespaceId.canonicalID());
   }
 
+  /**
+   * @deprecated Do not use - String for namespace ID is not type safe. Use {@link #getNamespaceName(Instance, Namespace.ID)}
+   */
+  @Deprecated","[{'comment': ""This is internal only. We don't need to deprecate it. We can just remove it. Same with other deprecations below."", 'commenter': 'ctubbsii'}, {'comment': 'I kept the deprecated methods around because of the 2 API methods NamespaceOperations Map<String,String> namespaceIdMap() and TableOperations Map<String,String> tableIdMap().  If I remove the deprecated methods then I either have to make populateMap(Instance, BiConsumer<String,String>) public or create a similar method that returns Map<String,String>.  I figured at least with the methods deprecated, we can communicate the preferred methods.', 'commenter': 'milleruntime'}, {'comment': 'Rather than keep the old ones around, I would just change the implementation of the API methods to one which does something like:\r\n```java\r\nreturn Tables.getNameToIdMap().entrySet().stream()\r\n        .collect(Collectors.toMap(Map.Entry::getKey,\r\n                                  e -> e.canonicalId(),\r\n                                  (v1,v2) ->{ throw new RuntimeException(String.format(""Duplicate key for values %s and %s"", v1, v2));},\r\n                                  TreeMap::new)));\r\n```', 'commenter': 'ctubbsii'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Namespaces.java,"@@ -100,50 +106,150 @@ private static ZooCache getZooCache(Instance instance) {
     return namespaceMap;
   }
 
-  public static boolean exists(Instance instance, String namespaceId) {
+  public static boolean exists(Instance instance, Namespace.ID namespaceId) {
     ZooCache zc = getZooCache(instance);
     List<String> namespaceIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES);
-    return namespaceIds.contains(namespaceId);
-  }
-
-  public static String getNamespaceId(Instance instance, String namespace) throws NamespaceNotFoundException {
-    String id = getNameToIdMap(instance).get(namespace);
-    if (id == null)
-      throw new NamespaceNotFoundException(null, namespace, ""getNamespaceId() failed to find namespace"");
-    return id;
+    return namespaceIds.contains(namespaceId.canonicalID());
   }
 
+  /**
+   * @deprecated Do not use - String for namespace ID is not type safe. Use {@link #getNamespaceName(Instance, Namespace.ID)}
+   */
+  @Deprecated
   public static String getNamespaceName(Instance instance, String namespaceId) throws NamespaceNotFoundException {
     String namespaceName = getIdToNameMap(instance).get(namespaceId);
     if (namespaceName == null)
       throw new NamespaceNotFoundException(namespaceId, null, ""getNamespaceName() failed to find namespace"");
     return namespaceName;
   }
 
+  /**
+   * @deprecated Do not use - Not type safe, ambiguous String-String map. Use {@link #getNameMap(Instance)}
+   */
+  @Deprecated","[{'comment': ""I see the name was changed in order to preserve the old method as deprecated, but since it can be removed in favor of the new method (no need to deprecate internal-only APIs), the original name can be preserved with the new behavior. Granted, the name isn't spectacularly clever, but it was very convenient in that it was very descriptive."", 'commenter': 'ctubbsii'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Namespaces.java,"@@ -100,50 +106,150 @@ private static ZooCache getZooCache(Instance instance) {
     return namespaceMap;
   }
 
-  public static boolean exists(Instance instance, String namespaceId) {
+  public static boolean exists(Instance instance, Namespace.ID namespaceId) {
     ZooCache zc = getZooCache(instance);
     List<String> namespaceIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES);
-    return namespaceIds.contains(namespaceId);
-  }
-
-  public static String getNamespaceId(Instance instance, String namespace) throws NamespaceNotFoundException {
-    String id = getNameToIdMap(instance).get(namespace);
-    if (id == null)
-      throw new NamespaceNotFoundException(null, namespace, ""getNamespaceId() failed to find namespace"");
-    return id;
+    return namespaceIds.contains(namespaceId.canonicalID());
   }
 
+  /**
+   * @deprecated Do not use - String for namespace ID is not type safe. Use {@link #getNamespaceName(Instance, Namespace.ID)}
+   */
+  @Deprecated
   public static String getNamespaceName(Instance instance, String namespaceId) throws NamespaceNotFoundException {
     String namespaceName = getIdToNameMap(instance).get(namespaceId);
     if (namespaceName == null)
       throw new NamespaceNotFoundException(namespaceId, null, ""getNamespaceName() failed to find namespace"");
     return namespaceName;
   }
 
+  /**
+   * @deprecated Do not use - Not type safe, ambiguous String-String map. Use {@link #getNameMap(Instance)}
+   */
+  @Deprecated
   public static SortedMap<String,String> getNameToIdMap(Instance instance) {
     return getMap(instance, true);
   }
 
+  /**
+   * @deprecated Do not use - Not type safe, ambiguous String-String map. Use {@link #getIdMap(Instance)}
+   */
+  @Deprecated
   public static SortedMap<String,String> getIdToNameMap(Instance instance) {
     return getMap(instance, false);
   }
 
-  public static List<String> getTableIds(Instance instance, String namespaceId) throws NamespaceNotFoundException {
+  public static List<Table.ID> getTableIds(Instance instance, Namespace.ID namespaceId) throws NamespaceNotFoundException {
     String namespace = getNamespaceName(instance, namespaceId);
-    List<String> names = new LinkedList<>();
-    for (Entry<String,String> nameToId : Tables.getNameToIdMap(instance).entrySet())
+    List<Table.ID> tableIds = new LinkedList<>();
+    for (Entry<String,Table.ID> nameToId : Tables.getNameMap(instance).entrySet())
       if (namespace.equals(Tables.qualify(nameToId.getKey()).getFirst()))
-        names.add(nameToId.getValue());
-    return names;
+        tableIds.add(nameToId.getValue());
+    return tableIds;","[{'comment': 'I bet this could be turned into a clever one-liner with Java 8 lambdas:\r\n\r\n```java\r\nString namespace = getNamespaceName(instance, namespaceId);\r\nreturn Tables.getNameMap(instance).entrySet().stream().filter(e -> namespace.equals(Tables.qualify(e.getKey()).getFirst())).collect(Collectors.toList());\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Depending on how the caller uses the return value, it could also be lazily constructed instead of instantiating a new Collection each time.', 'commenter': 'joshelser'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Table.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import org.apache.accumulo.core.client.Instance;
+import org.apache.hadoop.io.Text;
+
+public class Table {
+
+  /**
+   * Object representing an internal table ID. This class was created to help with type safety. For help obtaining the value of a table ID from Zookeeper, see
+   * {@link Tables#getTableId(Instance, String)}
+   */
+  public static class ID extends AbstractId {
+    private static final long serialVersionUID = 7399913185860577809L;
+
+    public static final ID METADATA = new ID(""!0"");
+    public static final ID REPLICATION = new ID(""+rep"");
+    public static final ID ROOT = new ID(""+r"");
+
+    public static ID empty() {
+      return new ID("""");
+    }
+
+    public ID(final String canonical) {
+      super(canonical);
+    }
+
+    public ID(Text textID) {
+      super(textID.toString());
+    }
+  }
+","[{'comment': 'Noticed you had constants for namespace ""ACCUMULO"" and ""DEFAULT"" names, but no constants here for built-in table names. Not strictly necessary, but would be nice to be consistent.', 'commenter': 'ctubbsii'}]"
279,core/src/main/java/org/apache/accumulo/core/data/impl/TabletIdImpl.java,"@@ -36,7 +36,7 @@ public int compareTo(TabletId o) {
 
   @Override
   public Text getTableId() {
-    return new Text(ke.getTableId());
+    return new Text(ke.getTableId().getUtf8Bytes());","[{'comment': 'Could also rely on the string constructor for Text here. It should use UTF8 bytes internally.', 'commenter': 'ctubbsii'}]"
279,server/base/src/main/java/org/apache/accumulo/server/master/balancer/GroupBalancer.java,"@@ -70,15 +70,15 @@
 
 public abstract class GroupBalancer extends TabletBalancer {
 
-  private final String tableId;
+  private final Table.ID tableId;
   private long lastRun = 0;
 
   /**
    * @return A function that groups tablets into named groups.
    */
   protected abstract Function<KeyExtent,String> getPartitioner();
 
-  public GroupBalancer(String tableId) {
+  public GroupBalancer(Table.ID tableId) {","[{'comment': 'This isn\'t strictly public API, but the balancer is one of those internal ""points of code injection"", for users to change Accumulo\'s default behavior. If API change can be avoided, that is preferred here.', 'commenter': 'ctubbsii'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Table.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import org.apache.accumulo.core.client.Instance;
+import org.apache.hadoop.io.Text;
+
+public class Table {
+
+  /**
+   * Object representing an internal table ID. This class was created to help with type safety. For help obtaining the value of a table ID from Zookeeper, see
+   * {@link Tables#getTableId(Instance, String)}
+   */
+  public static class ID extends AbstractId {
+    private static final long serialVersionUID = 7399913185860577809L;
+
+    public static final ID METADATA = new ID(""!0"");
+    public static final ID REPLICATION = new ID(""+rep"");
+    public static final ID ROOT = new ID(""+r"");
+
+    public static ID empty() {
+      return new ID("""");
+    }
+
+    public ID(final String canonical) {
+      super(canonical);
+    }
+
+    public ID(Text textID) {","[{'comment': ""Could avoid use of Hadoop's  `Text` object and just call `new ID(textObj.toString())` any where in code where it is used."", 'commenter': 'mikewalch'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/AbstractId.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+
+import java.io.Serializable;
+import java.util.Objects;
+
+/**
+ * An abstract identifier class for comparing equality of identifiers of the same type.
+ */
+public abstract class AbstractId implements Comparable<AbstractId>, Serializable {
+
+  private static final long serialVersionUID = -155513612834787244L;
+  private final String canonical;
+  private Integer hashCode = null;
+
+  protected AbstractId(final String canonical) {","[{'comment': 'Could change variable name to `identifier`.  The name `canonical` is confusing to me.', 'commenter': 'mikewalch'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/AbstractId.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+
+import java.io.Serializable;
+import java.util.Objects;
+
+/**
+ * An abstract identifier class for comparing equality of identifiers of the same type.
+ */
+public abstract class AbstractId implements Comparable<AbstractId>, Serializable {
+
+  private static final long serialVersionUID = -155513612834787244L;
+  private final String canonical;
+  private Integer hashCode = null;
+
+  protected AbstractId(final String canonical) {
+    requireNonNull(canonical, ""canonical cannot be null"");
+    this.canonical = canonical;
+  }
+
+  /**
+   * The canonical ID
+   */
+  public final String canonicalID() {","[{'comment': 'This is the same as `toString()` so it could be removed', 'commenter': 'mikewalch'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/AbstractId.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+
+import java.io.Serializable;
+import java.util.Objects;
+
+/**
+ * An abstract identifier class for comparing equality of identifiers of the same type.
+ */
+public abstract class AbstractId implements Comparable<AbstractId>, Serializable {
+
+  private static final long serialVersionUID = -155513612834787244L;
+  private final String canonical;
+  private Integer hashCode = null;
+
+  protected AbstractId(final String canonical) {
+    requireNonNull(canonical, ""canonical cannot be null"");
+    this.canonical = canonical;
+  }
+
+  /**
+   * The canonical ID
+   */
+  public final String canonicalID() {
+    return canonical;
+  }
+
+  public boolean isEmpty() {
+    return canonical.isEmpty();
+  }
+
+  /**
+   * AbstractID objects are considered equal if, and only if, they are of the same type and have the same canonical identifier.
+   */
+  @Override
+  public boolean equals(final Object obj) {
+    return obj != null && Objects.equals(getClass(), obj.getClass()) && Objects.equals(canonicalID(), ((AbstractId) obj).canonicalID());","[{'comment': 'Would be nice to do a reference check (`if (this == obj) return true;`)', 'commenter': 'joshelser'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/AbstractId.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+
+import java.io.Serializable;
+import java.util.Objects;
+
+/**
+ * An abstract identifier class for comparing equality of identifiers of the same type.
+ */
+public abstract class AbstractId implements Comparable<AbstractId>, Serializable {
+
+  private static final long serialVersionUID = -155513612834787244L;
+  private final String canonical;
+  private Integer hashCode = null;
+
+  protected AbstractId(final String canonical) {
+    requireNonNull(canonical, ""canonical cannot be null"");
+    this.canonical = canonical;
+  }
+
+  /**
+   * The canonical ID
+   */
+  public final String canonicalID() {
+    return canonical;
+  }
+
+  public boolean isEmpty() {
+    return canonical.isEmpty();
+  }
+
+  /**
+   * AbstractID objects are considered equal if, and only if, they are of the same type and have the same canonical identifier.
+   */
+  @Override
+  public boolean equals(final Object obj) {
+    return obj != null && Objects.equals(getClass(), obj.getClass()) && Objects.equals(canonicalID(), ((AbstractId) obj).canonicalID());
+  }
+
+  @Override
+  public int hashCode() {
+    if (hashCode == null) {
+      hashCode = Objects.hash(canonicalID());
+    }
+    return hashCode;
+  }
+
+  /**
+   * Returns a string of the canonical ID
+   */
+  @Override
+  public String toString() {
+    return canonical;
+  }
+
+  /**
+   * Return a UTF_8 byte[] of the canonical ID.
+   */
+  public final byte[] getUtf8Bytes() {
+    return canonical.getBytes(UTF_8);","[{'comment': ""Why this distinction? Is there a reason that we'd ever _not_ want the canonical name to be UTF-8?"", 'commenter': 'joshelser'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Table.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import org.apache.accumulo.core.client.Instance;
+import org.apache.hadoop.io.Text;
+
+public class Table {
+
+  /**
+   * Object representing an internal table ID. This class was created to help with type safety. For help obtaining the value of a table ID from Zookeeper, see
+   * {@link Tables#getTableId(Instance, String)}
+   */
+  public static class ID extends AbstractId {
+    private static final long serialVersionUID = 7399913185860577809L;
+
+    public static final ID METADATA = new ID(""!0"");
+    public static final ID REPLICATION = new ID(""+rep"");
+    public static final ID ROOT = new ID(""+r"");
+
+    public static ID empty() {
+      return new ID("""");","[{'comment': 'Make this a singleton.', 'commenter': 'joshelser'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Namespace.java,"@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import org.apache.accumulo.core.client.Instance;
+
+public class Namespace {","[{'comment': ""The outer-class of `Namespace` (as with `Table`) seems to exist solely for the purpose of the naming.\r\n\r\nIs there some kind of longer-term goal with what the `Namespace` and `Table` classes will become? If not, I'd suggest just collapsing this hierarchy to be `TableId` and `NamespaceId`. "", 'commenter': 'joshelser'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Namespaces.java,"@@ -100,50 +106,150 @@ private static ZooCache getZooCache(Instance instance) {
     return namespaceMap;
   }
 
-  public static boolean exists(Instance instance, String namespaceId) {
+  public static boolean exists(Instance instance, Namespace.ID namespaceId) {
     ZooCache zc = getZooCache(instance);
     List<String> namespaceIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES);
-    return namespaceIds.contains(namespaceId);
-  }
-
-  public static String getNamespaceId(Instance instance, String namespace) throws NamespaceNotFoundException {
-    String id = getNameToIdMap(instance).get(namespace);
-    if (id == null)
-      throw new NamespaceNotFoundException(null, namespace, ""getNamespaceId() failed to find namespace"");
-    return id;
+    return namespaceIds.contains(namespaceId.canonicalID());
   }
 
+  /**
+   * @deprecated Do not use - String for namespace ID is not type safe. Use {@link #getNamespaceName(Instance, Namespace.ID)}
+   */
+  @Deprecated
   public static String getNamespaceName(Instance instance, String namespaceId) throws NamespaceNotFoundException {
     String namespaceName = getIdToNameMap(instance).get(namespaceId);
     if (namespaceName == null)
       throw new NamespaceNotFoundException(namespaceId, null, ""getNamespaceName() failed to find namespace"");
     return namespaceName;
   }
 
+  /**
+   * @deprecated Do not use - Not type safe, ambiguous String-String map. Use {@link #getNameMap(Instance)}
+   */
+  @Deprecated
   public static SortedMap<String,String> getNameToIdMap(Instance instance) {
     return getMap(instance, true);
   }
 
+  /**
+   * @deprecated Do not use - Not type safe, ambiguous String-String map. Use {@link #getIdMap(Instance)}
+   */
+  @Deprecated
   public static SortedMap<String,String> getIdToNameMap(Instance instance) {
     return getMap(instance, false);
   }
 
-  public static List<String> getTableIds(Instance instance, String namespaceId) throws NamespaceNotFoundException {
+  public static List<Table.ID> getTableIds(Instance instance, Namespace.ID namespaceId) throws NamespaceNotFoundException {
     String namespace = getNamespaceName(instance, namespaceId);
-    List<String> names = new LinkedList<>();
-    for (Entry<String,String> nameToId : Tables.getNameToIdMap(instance).entrySet())
+    List<Table.ID> tableIds = new LinkedList<>();
+    for (Entry<String,Table.ID> nameToId : Tables.getNameMap(instance).entrySet())
       if (namespace.equals(Tables.qualify(nameToId.getKey()).getFirst()))
-        names.add(nameToId.getValue());
-    return names;
+        tableIds.add(nameToId.getValue());
+    return tableIds;
   }
 
-  public static List<String> getTableNames(Instance instance, String namespaceId) throws NamespaceNotFoundException {
+  public static List<String> getTableNames(Instance instance, Namespace.ID namespaceId) throws NamespaceNotFoundException {
     String namespace = getNamespaceName(instance, namespaceId);
     List<String> names = new LinkedList<>();
-    for (String name : Tables.getNameToIdMap(instance).keySet())
+    for (String name : Tables.getNameMap(instance).keySet())
       if (namespace.equals(Tables.qualify(name).getFirst()))
         names.add(name);
     return names;
   }
 
+  /**
+   * Populate map passed in as the BiConsumer. key = ID, value = namespaceName
+   */
+  private static void populateMap(Instance instance, BiConsumer<String,String> biConsumer) {
+    final ZooCache zc = getZooCache(instance);
+    List<String> namespaceIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES);
+    for (String id : namespaceIds) {
+      byte[] path = zc.get(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES + ""/"" + id + Constants.ZNAMESPACE_NAME);
+      if (path != null) {
+        biConsumer.accept(id, new String(path, UTF_8));
+      }
+    }
+  }
+
+  /**
+   * Return sorted map with key = ID, value = namespaceName
+   */
+  public static SortedMap<Namespace.ID,String> getIdMap(Instance instance) {
+    SortedMap<Namespace.ID,String> idMap = new TreeMap<>();
+    populateMap(instance, (id, name) -> idMap.put(new Namespace.ID(id), name));
+    return idMap;
+  }
+
+  /**
+   * Return sorted map with key = namespaceName, value = ID
+   */
+  public static SortedMap<String,Namespace.ID> getNameMap(Instance instance) {
+    SortedMap<String,Namespace.ID> nameMap = new TreeMap<>();
+    populateMap(instance, (id, name) -> nameMap.put(name, new Namespace.ID(id)));
+    return nameMap;
+  }
+
+  /**
+   * Look for namespace ID in ZK. Throw NamespaceNotFoundException if not found.
+   */
+  public static Namespace.ID getNamespaceId(Instance instance, String namespaceName) throws NamespaceNotFoundException {
+    final ArrayList<Namespace.ID> singleId = new ArrayList<>(1);
+    populateMap(instance, (id, name) -> {
+      if (name.equals(namespaceName))
+        singleId.add(new Namespace.ID(id));
+    });
+    if (singleId.isEmpty())
+      throw new NamespaceNotFoundException(null, namespaceName, ""getNamespaceId() failed to find namespace"");
+    return singleId.get(0);
+  }
+
+  /**
+   * Look for namespace ID in ZK. Fail quietly by logging and returning null.
+   */
+  public static Namespace.ID lookupNamespaceId(Instance instance, String namespaceName) {
+    Namespace.ID id = null;
+    try {
+      id = getNamespaceId(instance, namespaceName);
+    } catch (NamespaceNotFoundException e) {
+      if (log.isDebugEnabled())
+        log.debug(""Failed to find namespace ID from name: "" + namespaceName, e);
+    }
+    return id;
+  }
+
+  /**
+   * Return true if namespace name exists
+   */
+  public static boolean namespaceNameExists(Instance instance, String namespaceName) {
+    return lookupNamespaceId(instance, namespaceName) != null;
+  }
+
+  /**
+   * Look for namespace name in ZK. Throw NamespaceNotFoundException if not found.
+   */
+  public static String getNamespaceName(Instance instance, Namespace.ID namespaceId) throws NamespaceNotFoundException {
+    final ArrayList<String> singleName = new ArrayList<>(1);
+    populateMap(instance, (id, name) -> {","[{'comment': 'Why iterate over every namespace to just find a single one? Creating a new List also seems unnecessary to just find the namespace from an ID..', 'commenter': 'joshelser'}]"
279,core/src/main/java/org/apache/accumulo/core/client/impl/Namespaces.java,"@@ -100,50 +106,150 @@ private static ZooCache getZooCache(Instance instance) {
     return namespaceMap;
   }
 
-  public static boolean exists(Instance instance, String namespaceId) {
+  public static boolean exists(Instance instance, Namespace.ID namespaceId) {
     ZooCache zc = getZooCache(instance);
     List<String> namespaceIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES);
-    return namespaceIds.contains(namespaceId);
-  }
-
-  public static String getNamespaceId(Instance instance, String namespace) throws NamespaceNotFoundException {
-    String id = getNameToIdMap(instance).get(namespace);
-    if (id == null)
-      throw new NamespaceNotFoundException(null, namespace, ""getNamespaceId() failed to find namespace"");
-    return id;
+    return namespaceIds.contains(namespaceId.canonicalID());
   }
 
+  /**
+   * @deprecated Do not use - String for namespace ID is not type safe. Use {@link #getNamespaceName(Instance, Namespace.ID)}
+   */
+  @Deprecated
   public static String getNamespaceName(Instance instance, String namespaceId) throws NamespaceNotFoundException {
     String namespaceName = getIdToNameMap(instance).get(namespaceId);
     if (namespaceName == null)
       throw new NamespaceNotFoundException(namespaceId, null, ""getNamespaceName() failed to find namespace"");
     return namespaceName;
   }
 
+  /**
+   * @deprecated Do not use - Not type safe, ambiguous String-String map. Use {@link #getNameMap(Instance)}
+   */
+  @Deprecated
   public static SortedMap<String,String> getNameToIdMap(Instance instance) {
     return getMap(instance, true);
   }
 
+  /**
+   * @deprecated Do not use - Not type safe, ambiguous String-String map. Use {@link #getIdMap(Instance)}
+   */
+  @Deprecated
   public static SortedMap<String,String> getIdToNameMap(Instance instance) {
     return getMap(instance, false);
   }
 
-  public static List<String> getTableIds(Instance instance, String namespaceId) throws NamespaceNotFoundException {
+  public static List<Table.ID> getTableIds(Instance instance, Namespace.ID namespaceId) throws NamespaceNotFoundException {
     String namespace = getNamespaceName(instance, namespaceId);
-    List<String> names = new LinkedList<>();
-    for (Entry<String,String> nameToId : Tables.getNameToIdMap(instance).entrySet())
+    List<Table.ID> tableIds = new LinkedList<>();
+    for (Entry<String,Table.ID> nameToId : Tables.getNameMap(instance).entrySet())
       if (namespace.equals(Tables.qualify(nameToId.getKey()).getFirst()))
-        names.add(nameToId.getValue());
-    return names;
+        tableIds.add(nameToId.getValue());
+    return tableIds;
   }
 
-  public static List<String> getTableNames(Instance instance, String namespaceId) throws NamespaceNotFoundException {
+  public static List<String> getTableNames(Instance instance, Namespace.ID namespaceId) throws NamespaceNotFoundException {
     String namespace = getNamespaceName(instance, namespaceId);
     List<String> names = new LinkedList<>();
-    for (String name : Tables.getNameToIdMap(instance).keySet())
+    for (String name : Tables.getNameMap(instance).keySet())
       if (namespace.equals(Tables.qualify(name).getFirst()))
         names.add(name);
     return names;
   }
 
+  /**
+   * Populate map passed in as the BiConsumer. key = ID, value = namespaceName
+   */
+  private static void populateMap(Instance instance, BiConsumer<String,String> biConsumer) {
+    final ZooCache zc = getZooCache(instance);
+    List<String> namespaceIds = zc.getChildren(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES);
+    for (String id : namespaceIds) {
+      byte[] path = zc.get(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES + ""/"" + id + Constants.ZNAMESPACE_NAME);
+      if (path != null) {
+        biConsumer.accept(id, new String(path, UTF_8));
+      }
+    }
+  }
+
+  /**
+   * Return sorted map with key = ID, value = namespaceName
+   */
+  public static SortedMap<Namespace.ID,String> getIdMap(Instance instance) {
+    SortedMap<Namespace.ID,String> idMap = new TreeMap<>();
+    populateMap(instance, (id, name) -> idMap.put(new Namespace.ID(id), name));
+    return idMap;
+  }
+
+  /**
+   * Return sorted map with key = namespaceName, value = ID
+   */
+  public static SortedMap<String,Namespace.ID> getNameMap(Instance instance) {
+    SortedMap<String,Namespace.ID> nameMap = new TreeMap<>();
+    populateMap(instance, (id, name) -> nameMap.put(name, new Namespace.ID(id)));
+    return nameMap;
+  }
+
+  /**
+   * Look for namespace ID in ZK. Throw NamespaceNotFoundException if not found.
+   */
+  public static Namespace.ID getNamespaceId(Instance instance, String namespaceName) throws NamespaceNotFoundException {
+    final ArrayList<Namespace.ID> singleId = new ArrayList<>(1);
+    populateMap(instance, (id, name) -> {
+      if (name.equals(namespaceName))
+        singleId.add(new Namespace.ID(id));
+    });
+    if (singleId.isEmpty())
+      throw new NamespaceNotFoundException(null, namespaceName, ""getNamespaceId() failed to find namespace"");
+    return singleId.get(0);
+  }
+
+  /**
+   * Look for namespace ID in ZK. Fail quietly by logging and returning null.
+   */
+  public static Namespace.ID lookupNamespaceId(Instance instance, String namespaceName) {
+    Namespace.ID id = null;
+    try {
+      id = getNamespaceId(instance, namespaceName);
+    } catch (NamespaceNotFoundException e) {
+      if (log.isDebugEnabled())
+        log.debug(""Failed to find namespace ID from name: "" + namespaceName, e);
+    }
+    return id;
+  }
+
+  /**
+   * Return true if namespace name exists
+   */
+  public static boolean namespaceNameExists(Instance instance, String namespaceName) {
+    return lookupNamespaceId(instance, namespaceName) != null;
+  }
+
+  /**
+   * Look for namespace name in ZK. Throw NamespaceNotFoundException if not found.
+   */
+  public static String getNamespaceName(Instance instance, Namespace.ID namespaceId) throws NamespaceNotFoundException {
+    final ArrayList<String> singleName = new ArrayList<>(1);
+    populateMap(instance, (id, name) -> {
+      if (id.equals(namespaceId.canonicalID()))
+        singleName.add(name);
+    });
+    if (singleName.isEmpty())
+      throw new NamespaceNotFoundException(namespaceId.canonicalID(), null, ""getNamespaceName() failed to find namespace"");
+    return singleName.get(0);
+  }
+
+  /**
+   * Look for namespace name in ZK. Fail quietly by logging and returning null.
+   */
+  public static String lookupNamespaceName(Instance instance, Namespace.ID namespaceId) {
+    String name = null;
+    try {
+      getNamespaceName(instance, namespaceId);","[{'comment': 'Why suppress this exception? It seems like something that should propagate to the caller.', 'commenter': 'joshelser'}]"
283,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/BoundedRangeFileInputStream.java,"@@ -95,18 +92,7 @@ public int read(final byte[] b, final int off, int len) throws IOException {
         throw new IOException(""Stream closed"");
       }
       in.seek(pos);
-      try {
-        ret = AccessController.doPrivileged(new PrivilegedExceptionAction<Integer>() {","[{'comment': ""This is the one change which I'm not quite sure about.\r\n\r\nIt having been there since the dawn of time and without comment, I'm really not sure what it was intended to do. This predates all of the Kerberos work -- the only thing I can think of would have been the old SecurityManager constraints (but, if memory serves, those were more around SKVI's, not reading data from HDFS).\r\n\r\n@billierinaldi, @keith-turner, @ericnewton, @scubafuchs: any guesses? :)"", 'commenter': 'joshelser'}, {'comment': ""I removed this in master branch with ACCUMULO-4498, and think it's safe to remove, but also think it could cause confusion for anybody who might have worked out a custom security policy for their environment. I think that's probably extremely low risk, though. I think it's fine to remove."", 'commenter': 'ctubbsii'}, {'comment': 'Ahh, ok. Great. Not being able to attend that hackathon kept me in the dark, I guess :)', 'commenter': 'joshelser'}, {'comment': ""I don't remember any discussion at the hackathon... I'm pretty sure it just happened to be where I was when I committed it. I don't think you missed anything."", 'commenter': 'ctubbsii'}, {'comment': 'I went back to the FSDataInputStream class all the way through 2006 and I cannot a version of the read(byte[], int, int) method which would require privledged access.  This one is a mystery to me.', 'commenter': 'ivakegg'}, {'comment': ""Oh, no no no, that's not what I meant. I only meant that I didn't recall seeing you make that change. It happening at the hackathon simply explains why I missed it."", 'commenter': 'joshelser'}, {'comment': 'Thanks, @ivakegg! Sounds like a good one to just pull the trigger on it and see if anything breaks ;)', 'commenter': 'joshelser'}]"
283,core/src/main/java/org/apache/accumulo/core/util/Jar.java,"@@ -41,8 +41,9 @@ public void execute(final String[] args) throws Exception {
     String jarFileName = args[0];
     String candidateMainClass = args.length > 1 ? args[1] : null;
     Class<?> mainClass = null;
+    JarFile f = null;
     try {
-      JarFile f = new JarFile(jarFileName);","[{'comment': 'Why not try-with-resources?', 'commenter': 'madrob'}, {'comment': '+1 for try-with-resources', 'commenter': 'ctubbsii'}, {'comment': ""At a glance, I didn't think JarFile was Closeable, but that seems silly now.\r\n\r\nEdit: Yup, very silly. Just pushed a fix. Thanks!"", 'commenter': 'joshelser'}]"
283,server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/BasicServlet.java,"@@ -264,4 +264,12 @@ protected static void banner(StringBuilder sb, String klass, String text) {
     sb.append(""<br />\n<h2 class='"").append(klass).append(""'>"").append(text).append(""</h2>\n"");
   }
 
+  /**
+   * Creates a {@link Cookie} with the given name and value, also setting the HttpOnly attribute on the cookie.
+   */
+  protected static Cookie createCookie(String name, String value) {
+    Cookie c = new Cookie(name, value);
+    c.setHttpOnly(true);","[{'comment': ""The only cookie which actually needed to be protected was the session cookie addressed in ACCUMULO-4676. All of these cookies are simple user UI preferences (table column sort order, etc.) and it doesn't matter if they are manipulated in client-side javascript. Are we certain we're not manipulating them in client-side javascript now? If we are, this will break that."", 'commenter': 'ctubbsii'}, {'comment': ""> Are we certain we're not manipulating them in client-side javascript now? If we are, this will break that.\r\n\r\nI can double-check, but I'm not aware of that in 1.x. Not sure how the table sorting stuff works. It would be nice to quell these warnings in the future, but I may begrudgingly have to revert."", 'commenter': 'joshelser'}, {'comment': ""> I can double-check, but I'm not aware of that in 1.x. Not sure how the table sorting stuff works.\r\n\r\nSo I'm surprised, but this actually just doesn't break anything. Sorting on tables still works. Returning to a page after leaving retains the sort column+order."", 'commenter': 'joshelser'}, {'comment': ""We may only be using these server-side. Pretty sure we're refreshing the page and getting the resorted version from the server. I just wasn't sure if that was always the case, or if there was some fringe place where it would break."", 'commenter': 'ctubbsii'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/trace/TracesResource.java,"@@ -84,6 +86,10 @@
   @GET
   public RecentTracesList getTraces(@DefaultValue(""10"") @PathParam(""minutes"") int minutes) throws Exception {
 
+    if (minutes >= 60 || minutes <= 0) {","[{'comment': 'These should be MIN and MAX constants in ParameterValidator. ', 'commenter': 'milleruntime'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/trace/TracesResource.java,"@@ -176,12 +193,15 @@ public Void run() {
   @Path(""show/{id}"")
   @GET
   public TraceList getTracesType(@PathParam(""id"") String id) throws Exception {
-    TraceList traces = new TraceList(id);
 
-    if (id == null) {
+    if (StringUtils.isEmpty(id)) {","[{'comment': ""I don't know what type of ID this is referring to but if it is a table or namespace, should create ID type objects."", 'commenter': 'milleruntime'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/trace/TracesResource.java,"@@ -119,16 +125,27 @@ public Void run() {
   /**
    * Generates a list of traces filtered by type and range of minutes
    *
-   * @param type
+   * @param typeParameter
    *          Type of the trace
    * @param minutes
    *          Range of minutes
    * @return List of traces filtered by type and range
    */
   @Path(""listType/{type}/{minutes}"")
   @GET
-  public TraceType getTracesType(@PathParam(""type"") String type, @PathParam(""minutes"") int minutes) throws Exception {
+  public TraceType getTracesType(@PathParam(""type"") String typeParameter, @PathParam(""minutes"") int minutes) throws Exception {
+
+    if (StringUtils.isEmpty(typeParameter)) {
+      throw new Exception(""Specified type was empty"");
+    }
 
+    // Need finalized value for use in anonymous function below.
+    final String type = ParameterValidator.sanitizeParameter(typeParameter);
+    
+    if (minutes >= 60 || minutes < 0) {
+      throw new Exception(""minutes was not of range [0-59], was "" + String.valueOf(minutes));","[{'comment': 'Should also use constants here.', 'commenter': 'milleruntime'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/trace/TracesResource.java,"@@ -258,8 +278,8 @@ protected Range getRangeForTrace(long minutesSince) {
     long startTime = endTime - millisSince;
 
     String startHexTime = Long.toHexString(startTime), endHexTime = Long.toHexString(endTime);
-    while (startHexTime.length() < endHexTime.length()) {
-      startHexTime = ""0"" + startHexTime;
+    if (startHexTime.length() < endHexTime.length()) {
+      StringUtils.leftPad(startHexTime, endHexTime.length(), '0');","[{'comment': 'Nice improvement!', 'commenter': 'milleruntime'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tservers/TabletServerResource.java,"@@ -141,28 +146,27 @@ public TabletServersRecovery getTserverRecovery() {
   /**
    * Generates details for the selected tserver
    *
-   * @param tserverAddr
+   * @param tserverAddress
    *          TServer name
    * @return TServer details
    */
   @Path(""{address}"")
   @GET
-  public TabletServerSummary getTserverDetails(@PathParam(""address"") String tserverAddr) throws Exception {
-
-    String tserverAddress = tserverAddr;
+  public TabletServerSummary getTserverDetails(@PathParam(""address"") String tserverAddress) throws Exception {","[{'comment': '+1 I prefer full english words for parameter variable names (especially with Strings).  Unless you are using VIM as your IDE, abbreviation doesn\'t help anyone and only leads to confusion.  Granted, this example 99% of developers will know what ""addr"" means but who knows... there could be another variable to come along with the name tserverAdder or something. ', 'commenter': 'milleruntime'}]"
289,server/monitor/pom.xml,"@@ -45,6 +45,10 @@
       <artifactId>javax.servlet-api</artifactId>
     </dependency>
     <dependency>
+      <groupId>javax.validation</groupId>
+      <artifactId>validation-api</artifactId>","[{'comment': ""This is interesting. What does the enforcement of these annotations?\r\n\r\nAlso, if this is used, it should be added to the binary tarball (`assemble/src/main/assemblies/component.xml` and `assemble/pom.xml`), as well as the root `pom.xml`'s `<dependencyManagement>` section with a `<version>`. It's AL2 licensed, and no NOTICE file, and has no dependencies, so IP is pretty simple to deal with if we go this direction."", 'commenter': 'ctubbsii'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/problems/ProblemsResource.java,"@@ -96,9 +98,13 @@ public ProblemSummary getSummary() {
   public void clearTableProblems(@QueryParam(""s"") String tableID) {
     Logger log = LoggerFactory.getLogger(Monitor.class);
     try {
+      tableID = ParameterValidator.sanitizeParameter(tableID);
+      if (StringUtils.isEmpty(tableID)) {","[{'comment': 'Probably not worth bringing in StringUtils for simple `s == null || s.isEmpty()` checks. More use of commons-lang means more possible points of breakage with transitive dependencies.', 'commenter': 'ctubbsii'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tables/TablesResource.java,"@@ -65,6 +69,7 @@
 public class TablesResource {
 
   private static final TabletServerStatus NO_STATUS = new TabletServerStatus();
+  private static final Pattern COMMA = Pattern.compile(""(,|%2[cC])"");","[{'comment': ""Should not be doing any URL decoding here. This should already be URL-decoded by the framework. This is double-decoding. Although that's probably okay here (because namespaces can't contain commas), it's a bad precedent to set."", 'commenter': 'ctubbsii'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/trace/TracesResource.java,"@@ -77,12 +81,12 @@
    * Generates a trace summary
    *
    * @param minutes
-   *          Range of minutes to filter traces
+   *          Range of minutes to filter traces Min of 0 minutes, Max of 30 days
    * @return Trace summary in specified range
    */
   @Path(""summary/{minutes}"")
   @GET
-  public RecentTracesList getTraces(@DefaultValue(""10"") @PathParam(""minutes"") int minutes) throws Exception {
+  public RecentTracesList getTraces(@DefaultValue(""10"") @PathParam(""minutes"") @Min(0) @Max(2592000) int minutes) throws Exception {","[{'comment': 'Very interested in how this is enforced. Any insight?', 'commenter': 'ctubbsii'}, {'comment': ""The annotations are part of the javax.validation api and Jersey 2.0+ has built in support for them.  According to the documentation the auto-discovery service will auto-enable them on the server without the need of manual configuration.  The reference implementation is the hibernate validator based on their documentation:\r\n\r\nhttps://jersey.github.io/documentation/latest/bean-validation.html\r\n\r\nI'm still reading up on it and it sounds like it could negate the need for the ParameterValidator class in this PR (which I'll end up removing)."", 'commenter': 'glitch'}, {'comment': ""Ah, interesting. So, it looks like there's a jersey extension which implements the validation for this API. Without the jersey-bean-validation, these are nothing more than ignored annotations. I'm curious what dependencies that might bring in. I don't think it'll be too bad, but need to be careful to include whatever we need in the assembly."", 'commenter': 'ctubbsii'}, {'comment': ""Wouldn't defaulting to ignored annotation introduce a vulnerability in the validation? Say if an admin doesn't setup the monitor correctly or disables the auto-discovery.  I am wondering why we would ever want to disable the validation and can we prevent it from being disabled?"", 'commenter': 'milleruntime'}, {'comment': ""Presumably, we'd configure the monitor's servlet properly so that Jersey enforces them. This PR does not yet have those bits."", 'commenter': 'ctubbsii'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/util/ParameterValidator.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.util;
+
+import java.io.UnsupportedEncodingException;
+import java.net.URLEncoder;
+
+import org.apache.commons.lang.StringUtils;
+
+/**
+ * Simple utility class to validate Accumulo Monitor Query and Path parameters
+ */
+public class ParameterValidator {
+
+  /**
+   * @param s
+   *          String parameter to sanitize. Common usages are tableId, type, tserverAddress
+   * @return URLEncoder encoded version of the passed in String parameter
+   * @throws UnsupportedEncodingException
+   *           if the string cannot be encoded to UTF-8
+   */
+  public static String sanitizeParameter(String s) throws UnsupportedEncodingException {
+    return StringUtils.isEmpty(s) ? StringUtils.EMPTY : URLEncoder.encode(s, ""UTF-8"").trim();","[{'comment': 'I don\'t think this should be returning a URL-encoded version of the string. In most cases, we actually need to use the value after it has been decoded by Jersey. The only time we\'d want to re-encode it is before display... but this method doesn\'t seem to be limited to only being used prior to display in the view.\r\n\r\nIt should also not throw an UnsupportedEncodingException. UTF-8 is always supported. This exception should be suppressed inside the method, instead of leaking out.\r\n\r\nAlso, `StandardCharsets.UTF_8.name()` should be used instead of `""UTF-8""`, whenever the String version of the charset is required.', 'commenter': 'ctubbsii'}, {'comment': ""@ctubbsii I've been researching the annotation route using the javax.validation api which would remove the necessity of this class & methods altogether."", 'commenter': 'glitch'}]"
289,server/monitor/src/main/java/org/apache/accumulo/monitor/util/ParameterValidator.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.util;
+
+import java.io.UnsupportedEncodingException;
+import java.net.URLEncoder;
+
+import org.apache.commons.lang.StringUtils;
+
+/**
+ * Simple utility class to validate Accumulo Monitor Query and Path parameters
+ */
+public class ParameterValidator {
+
+  /**
+   * @param s
+   *          String parameter to sanitize. Common usages are tableId, type, tserverAddress
+   * @return URLEncoder encoded version of the passed in String parameter
+   * @throws UnsupportedEncodingException
+   *           if the string cannot be encoded to UTF-8
+   */
+  public static String sanitizeParameter(String s) throws UnsupportedEncodingException {
+    return StringUtils.isEmpty(s) ? StringUtils.EMPTY : URLEncoder.encode(s, ""UTF-8"").trim();
+  }
+
+  public static String sanitizeParameter(String s, String defaultValue) throws UnsupportedEncodingException {","[{'comment': 'Is this method needed? It seems redundant with the default value annotation.', 'commenter': 'ctubbsii'}]"
289,pom.xml,"@@ -167,6 +167,11 @@
         <version>1.48</version>
       </dependency>
       <dependency>
+        <groupId>com.fasterxml</groupId>
+        <artifactId>classmate</artifactId>","[{'comment': 'What is this used for?', 'commenter': 'milleruntime'}, {'comment': ""It's part of the transitive dependencies of using jersey's jersey-bean-validation and hibernate-validator default implementation (along with the javax.el deps, jboss-logging dep, etc.)"", 'commenter': 'glitch'}]"
289,server/monitor/src/test/java/org/apache/accumulo/monitor/view/WebViewsTest.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.monitor.view;
+
+import static org.easymock.EasyMock.expect;
+import static org.powermock.api.easymock.PowerMock.mockStatic;
+import static org.powermock.api.easymock.PowerMock.replayAll;
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.lang.annotation.Annotation;
+import java.lang.reflect.Type;
+import java.util.HashMap;
+
+import javax.ws.rs.WebApplicationException;
+import javax.ws.rs.core.Application;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.MultivaluedMap;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.ext.MessageBodyWriter;
+
+import org.apache.accumulo.core.client.Instance;
+import org.apache.accumulo.core.client.impl.Table;
+import org.apache.accumulo.core.client.impl.Tables;
+import org.apache.accumulo.monitor.Monitor;
+import org.apache.accumulo.server.AccumuloServerContext;
+import org.easymock.EasyMock;
+import org.glassfish.jersey.client.ClientConfig;
+import org.glassfish.jersey.server.ResourceConfig;
+import org.glassfish.jersey.test.JerseyTest;
+import org.glassfish.jersey.test.TestProperties;
+import org.junit.Assert;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+import org.powermock.modules.junit4.PowerMockRunner;
+
+/**
+ * Basic unit test for parameter validation constraints
+ */
+@RunWith(PowerMockRunner.class)
+@PrepareForTest({Monitor.class, Tables.class})
+public class WebViewsTest extends JerseyTest {
+
+  @Override
+  public Application configure() {
+    enable(TestProperties.LOG_TRAFFIC);
+    enable(TestProperties.DUMP_ENTITY);
+    ResourceConfig config = new ResourceConfig(WebViews.class);
+    config.register(org.apache.accumulo.monitor.view.WebViewsTest.HashMapWriter.class);
+    return config;
+  }
+
+  @Override
+  protected void configureClient(ClientConfig config) {
+    super.configureClient(config);
+    config.register(org.apache.accumulo.monitor.view.WebViewsTest.HashMapWriter.class);
+  }
+
+  /**
+   * Expect to fail the constraint validation on the REST endpoint. The constraint is the pre-defined word character class Pattern so passing a table name with
+   * punctuation will cause a 400 response code.
+   */
+  @Test
+  public void testGetTablesConstraintViolations() {
+    Response output = target(""tables/f+o*o"").request().get();
+    Assert.assertEquals(""should return status 400"", 400, output.getStatus());
+  }
+
+  /**
+   * Test path tables/{tableID} which passes constraints. On passing constraints underlying logic will be executed so we need to mock a certain amount of it.
+   * Note: to get the proper response code back, you need to make sure jersey has a registered MessageBodyWriter capable of serializing/writing the object
+   * returned from your endpoint. We're using a simple stubbed out inner class HashMapWriter for this.
+   *
+   * @throws Exception
+   *           not expected
+   */
+  @Test
+  public void testGetTablesConstraintPassing() throws Exception {
+    Instance instanceMock = EasyMock.createMock(Instance.class);
+    expect(instanceMock.getInstanceID()).andReturn(""foo"").anyTimes();
+    AccumuloServerContext contextMock = EasyMock.createMock(AccumuloServerContext.class);
+    expect(contextMock.getInstance()).andReturn(instanceMock).anyTimes();
+
+    mockStatic(Monitor.class);
+    expect(Monitor.getContext()).andReturn(contextMock).anyTimes();
+
+    mockStatic(Tables.class);
+    expect(Tables.getTableName(instanceMock, new Table.ID(""foo""))).andReturn(""bar"");
+    replayAll();
+    org.easymock.EasyMock.replay(instanceMock, contextMock);
+
+    // Using the mocks we can verify that the getModel method gets called via debugger
+    // however it's difficult to continue to mock through the jersey MVC code for the properly built response.
+    // Our silly HashMapWriter registered in the configure method gets wired in and used here.
+    Response output = target(""tables/foo"").request().get();
+    Assert.assertEquals(""should return status 200"", 200, output.getStatus());
+    String responseBody = output.readEntity(String.class);
+    Assert.assertTrue(responseBody.contains(""tableID=foo"") && responseBody.contains(""table=bar""));
+  }
+
+  /**
+   * Test minutes parameter constraints. When outside range we should get a 400 response.
+   */
+  @Test
+  public void testGetTracesSummaryValidationConstraint() {
+    // Test upper bounds of constraint
+    Response output = target(""trace/summary"").queryParam(""minutes"", 5000000).request().get();
+    Assert.assertEquals(""should return status 400"", 400, output.getStatus());
+
+    // Test lower bounds of constraint
+    output = target(""trace/summary"").queryParam(""minutes"", -27).request().get();
+    Assert.assertEquals(""should return status 400"", 400, output.getStatus());
+  }
+
+  /**
+   * Silly stub to handle MessageBodyWriter for Hashmap. Registered in configure method and auto-wired by Jersey.
+   */
+  public static class HashMapWriter implements MessageBodyWriter<HashMap> {","[{'comment': 'Should specify generic type parameters here on HashMap (and also line 140 and 145) for type safety and to avoid introducing new warnings.', 'commenter': 'ctubbsii'}, {'comment': 'Reminder of these raw types warnings.', 'commenter': 'ctubbsii'}]"
289,assemble/src/main/assemblies/component.xml,"@@ -75,12 +77,17 @@
         <include>org.glassfish.jersey.core:jersey-client</include>
         <include>org.glassfish.jersey.core:jersey-common</include>
         <include>org.glassfish.jersey.core:jersey-server</include>
+        <include>org.glassfish.jersey.ext:jersey-bean-validation</include>
         <include>org.glassfish.jersey.ext:jersey-entity-filtering</include>
         <include>org.glassfish.jersey.ext:jersey-mvc-freemarker</include>
         <include>org.glassfish.jersey.ext:jersey-mvc</include>
         <include>org.glassfish.jersey.media:jersey-media-jaxb</include>
         <include>org.glassfish.jersey.media:jersey-media-json-jackson</include>
+        <include>org.glassfish.web:javax.el</include>
+        <include>org.glassfish.web:javax.el-impl</include>","[{'comment': ""javax.el-impl isn't going to be included unless it's also a dependency of the assembly module, which does not appear to be the case."", 'commenter': 'ctubbsii'}]"
289,pom.xml,"@@ -703,6 +738,12 @@
         <artifactId>slf4j-nop</artifactId>
         <version>${slf4j.version}</version>
       </dependency>
+      <dependency>
+        <groupId>org.glassfish.jersey.test-framework.providers</groupId>
+        <artifactId>jersey-test-framework-provider-grizzly2</artifactId>
+        <version>${jersey.version}</version>
+        <scope>test</scope>","[{'comment': ""We typically don't set the scope in the parent POM. We manage the dependency versions in the parent POM, and choose the scope in the module where it is used."", 'commenter': 'ctubbsii'}]"
289,server/monitor/pom.xml,"@@ -123,6 +127,32 @@
       <artifactId>junit</artifactId>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.easymock</groupId>
+      <artifactId>easymock</artifactId>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.ext</groupId>
+      <artifactId>jersey-bean-validation</artifactId>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.glassfish.jersey.test-framework.providers</groupId>
+      <artifactId>jersey-test-framework-provider-grizzly2</artifactId>
+      <version>${jersey.version}</version>","[{'comment': 'Should not be overriding version from parent POM dependencyManagement section here.', 'commenter': 'ctubbsii'}]"
289,server/monitor/src/test/java/org/apache/accumulo/monitor/view/WebViewsTest.java,"@@ -130,6 +130,7 @@ public void testGetTracesSummaryValidationConstraint() {
   /**
    * Silly stub to handle MessageBodyWriter for Hashmap. Registered in configure method and auto-wired by Jersey.
    */
+  @SuppressWarnings(""rawtypes"")","[{'comment': ""`<?,?>` as the generic type would be much preferred over suppressing the warning. It's always better to be explicit, and I'm not a fan of instructing the compiler to ignore warnings when there's an easy fix.\r\n\r\nAlso, I just noticed that this test creates an external web service to test against. As such, this test should be renamed to be `WebViewsIT`, because it's an integration test, and not just a unit test.\r\n\r\nAnd, don't worry about the conflicts with master. I can take care of that for you (going to rebase before merge). I can also address these last few minor issues, too, during the rebase/merge process."", 'commenter': 'ctubbsii'}, {'comment': 'RE: IntegrationTests which @Category class should this annotation point at?  SunnyDay?\r\n\r\nEDIT:  Also, presumably I will need to move it to the **accumulo-test** module.  I see things under src/main/java/... with test annotations in them and I see things under /src/test/java/... so which one does it go under, main or test?\r\nThanks', 'commenter': 'glitch'}, {'comment': ""Ugh, I forgot about categories. Don't worry about that. I'll deal with it as a follow-on issue. It's fine as it is for now (most likely by creating a new category specifically for monitor ITs)."", 'commenter': 'ctubbsii'}]"
292,core/src/main/java/org/apache/accumulo/core/client/impl/Table.java,"@@ -16,24 +16,59 @@
  */
 package org.apache.accumulo.core.client.impl;
 
+import java.lang.ref.WeakReference;
+import java.util.WeakHashMap;
+
 import org.apache.accumulo.core.client.Instance;
 
 public class Table {
 
   /**
    * Object representing an internal table ID. This class was created to help with type safety. For help obtaining the value of a table ID from Zookeeper, see
    * {@link Tables#getTableId(Instance, String)}
+   *
+   * Uses an internal WeakHashMap and private constructor for storing a WeakReference of every Table.ID. Therefore, a Table.ID can't be instantiated outside
+   * this class and is accessed by calling Table.ID.{@link #of(String)}.
    */
   public static class ID extends AbstractId {
     private static final long serialVersionUID = 7399913185860577809L;
+    static final WeakHashMap<String,WeakReference<Table.ID>> tableIds = new WeakHashMap<>();
 
-    public static final ID METADATA = new ID(""!0"");
-    public static final ID REPLICATION = new ID(""+rep"");
-    public static final ID ROOT = new ID(""+r"");
+    public static final ID METADATA = of(""!0"");
+    public static final ID REPLICATION = of(""+rep"");
+    public static final ID ROOT = of(""+r"");
 
-    public ID(final String canonical) {
+    private ID(final String canonical) {
       super(canonical);
     }
+
+    /**
+     * Get a Table.ID object for the provided canonical string.
+     *
+     * @param canonical
+     *          table ID string
+     * @return Table.ID object
+     */
+    public static Table.ID of(final String canonical) {
+      return dedupeTableId(canonical);
+    }
+
+    private static Table.ID dedupeTableId(String tableIdString) {","[{'comment': 'Might be able to move the dedupe logic to the AbstractId class, with a generic method for the ID type. Would need to pass in the WeakHashMap as a parameter, and possibly a pre-constructed instance (to avoid reflection) to use if the map does not contain that ID. May not be consolidating this logic.', 'commenter': 'ctubbsii'}, {'comment': 'I thought about this but it would require reflection or passing a pre-constructed instance object as a parameter to the method.  I wanted to avoid reflection since we lose some of the type safety. I also wanted to avoid created objects every time for parameters since they may not be needed if the map already contains the id.  \r\n\r\nPassing in a constructor object as a parameter may be a nice compromise but this introduces a bunch of potential exceptions:\r\n<pre>\r\ntry {\r\n        id = constructor.newInstance(idString);\r\n      } catch (InstantiationException e) {\r\n        e.printStackTrace();\r\n      } catch (IllegalAccessException e) {\r\n        e.printStackTrace();\r\n      } catch (InvocationTargetException e) {\r\n        e.printStackTrace();\r\n      }\r\n      idHashMap.put(idString, new WeakReference<>(id));\r\n</pre>', 'commenter': 'milleruntime'}, {'comment': ""Passing in a `Function<String,T>` might be a nice compromise. Or a `Supplier<T>`:\r\n\r\n```java\r\nprotected static <T extends AbstractId> T dedupe(WeakHashMap<String,WeakReference<T>> cache, String key, Supplier<T> newInstanceFunction) {\r\n    //....\r\n}\r\n```\r\nIn Table.ID:\r\n```java\r\npublic ID of(String canonical) {\r\n    return dedupe(cache, canonical, () -> new ID(canonical));\r\n}\r\n```\r\n\r\nShared implementation. No reflection. No unnecessary construction. If you use the ID itself as the cache key, then you have to construct regardless... you just don't have to keep it around after deduplicating."", 'commenter': 'ctubbsii'}]"
292,core/src/main/java/org/apache/accumulo/core/client/impl/Table.java,"@@ -16,24 +16,59 @@
  */
 package org.apache.accumulo.core.client.impl;
 
+import java.lang.ref.WeakReference;
+import java.util.WeakHashMap;
+
 import org.apache.accumulo.core.client.Instance;
 
 public class Table {
 
   /**
    * Object representing an internal table ID. This class was created to help with type safety. For help obtaining the value of a table ID from Zookeeper, see
    * {@link Tables#getTableId(Instance, String)}
+   *
+   * Uses an internal WeakHashMap and private constructor for storing a WeakReference of every Table.ID. Therefore, a Table.ID can't be instantiated outside
+   * this class and is accessed by calling Table.ID.{@link #of(String)}.
    */
   public static class ID extends AbstractId {
     private static final long serialVersionUID = 7399913185860577809L;
+    static final WeakHashMap<String,WeakReference<Table.ID>> tableIds = new WeakHashMap<>();","[{'comment': ""It occurs to me that because of Java string interning, using a String as a key may mean that entries are kept around longer than necessary. The `WeakReference<Table.ID>` will still remove unused `Table.ID` objects and it will dedupe when it does exist, but the map entry might stick around unnecessarily because the String still has a strong reference because it's used in more places than just the Table.ID. This could be avoided by always constructing a new String and using that as the canonical parameter and the map key... but not sure it's worth it."", 'commenter': 'ctubbsii'}, {'comment': 'I did see this happen while tinkering around with the Tests.  The keys were staying around but the WeakReference value was null.', 'commenter': 'milleruntime'}]"
292,core/src/main/java/org/apache/accumulo/core/client/impl/Table.java,"@@ -16,24 +16,59 @@
  */
 package org.apache.accumulo.core.client.impl;
 
+import java.lang.ref.WeakReference;
+import java.util.WeakHashMap;
+
 import org.apache.accumulo.core.client.Instance;
 
 public class Table {
 
   /**
    * Object representing an internal table ID. This class was created to help with type safety. For help obtaining the value of a table ID from Zookeeper, see
    * {@link Tables#getTableId(Instance, String)}
+   *
+   * Uses an internal WeakHashMap and private constructor for storing a WeakReference of every Table.ID. Therefore, a Table.ID can't be instantiated outside
+   * this class and is accessed by calling Table.ID.{@link #of(String)}.
    */
   public static class ID extends AbstractId {
     private static final long serialVersionUID = 7399913185860577809L;
+    static final WeakHashMap<String,WeakReference<Table.ID>> tableIds = new WeakHashMap<>();
 
-    public static final ID METADATA = new ID(""!0"");
-    public static final ID REPLICATION = new ID(""+rep"");
-    public static final ID ROOT = new ID(""+r"");
+    public static final ID METADATA = of(""!0"");
+    public static final ID REPLICATION = of(""+rep"");
+    public static final ID ROOT = of(""+r"");
 
-    public ID(final String canonical) {
+    private ID(final String canonical) {
       super(canonical);
     }
+
+    /**
+     * Get a Table.ID object for the provided canonical string.
+     *
+     * @param canonical
+     *          table ID string
+     * @return Table.ID object
+     */
+    public static Table.ID of(final String canonical) {
+      return dedupeTableId(canonical);
+    }
+
+    private static Table.ID dedupeTableId(String tableIdString) {
+      Table.ID tableId;
+      synchronized (tableIds) {
+        WeakReference<Table.ID> tableIdRef = tableIds.get(tableIdString);
+        if (tableIdRef != null) {
+          tableId = tableIdRef.get();
+          if (tableId != null) {
+            return tableId;
+          }
+        }
+
+        tableId = new ID(tableIdString);
+        tableIds.put(tableIdString, new WeakReference<>(tableId));","[{'comment': 'I was thinking through the logic of this put method (and it\'s ""compute"" alternatives), and realized that there\'s no way to get a strong reference to the key itself from the map. You can only do comparisons. So, there\'s a chance that when you do a ""put"" here, it will reuse the old key, but the new value (in the case where the map entry exists, but the WeakReference is empty). The result is that the entry could disappear from the map even though the object in the WeakReference is still being used. In this case, we lose deduplication. There does not appear to be a solution to this (short of writing our own map class, or iterating over the keyset and comparing keys ourselves), and our deduplication is ""best effort"", so it\'s not a big deal... but it\'s worth keeping in mind, so we don\'t even try to use this paradigm for guaranteed deduplication.', 'commenter': 'ctubbsii'}, {'comment': 'To clarify what I mean, ""put"" doesn\'t replace a key if it already exists. It only updates the value. This also might be mitigated by using Table.ID as the key, since it\'s less likely that the Table.ID will exist as a key in the map and point to an empty WeakReference.', 'commenter': 'ctubbsii'}, {'comment': ""Perhaps a WeakHashMap isn't the correct data structure for our situation.  Aren't we trying to simply cache IDs to prevent unnecessary object creation?  Perhaps a cache is all we want."", 'commenter': 'milleruntime'}, {'comment': 'These comments made me think this: https://stackoverflow.com/questions/1802809/javas-weakhashmap-and-caching-why-is-it-referencing-the-keys-not-the-values/1803213', 'commenter': 'milleruntime'}, {'comment': ""Talked to @keith-turner about this briefly. I think the `WeakHashMap<String,WeakReference<>>` is the right type, but the String that's used as a key should be a new one, so its presence in the map is not dependent on the caller's strong reference to the key, but on the caller's strong reference to the returned Table.ID object. In other words, it should be:\r\n\r\n```java\r\n    T ret;\r\n    WeakReference<T> ref = cache.get(canonical);\r\n    if (ref == null || (ret = ref.get()) == null) {\r\n        String s = new String(canonical);\r\n        cache.put(s, ret = new Table.ID(s));\r\n    }\r\n    return ret;\r\n```\r\n\r\nThat way, the new String is only contained inside the ID and its presence in the map is contingent on the ID object still having a strong reference to it. Of course, the user could call `ID.getCanonical()` and get a strong reference to the key, thus holding it in the map even when the ID object is gone. This could be avoided by the constructor creating a second copy... the first is held by the ID only for the purposes of having a strong reference to the map key, and the second would be returned in `.getCanonical()` and `.toString()`."", 'commenter': 'ctubbsii'}, {'comment': ""The main point is that these objects, even the extra String instances, would only be created when needed. Usually, the previous instance would be used. Still can't avoid the potential duplicates I mentioned earlier, unless you force removal of the old value before calling `put` in the case where `ref != null && ref.get() == null`, but the new String instances should dramatically decrease the liklihood of this happening, and a few dupes are probably okay."", 'commenter': 'ctubbsii'}]"
292,core/src/test/java/org/apache/accumulo/core/client/impl/TableTest.java,"@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import org.junit.After;
+import org.junit.Test;
+
+/**
+ * Tests the Table ID class, mainly the internal WeakHashMap.
+ */
+public class TableTest {
+
+  @After
+  public void cleanup() {
+    garbageCollect(Table.ID.tableIds.size());
+  }
+
+  @Test
+  public void testWeakHashMapIncreases() {
+    // ensure initial size contains just the built in Table IDs (metadata, root, replication)
+    assertEquals(3, Table.ID.tableIds.size());
+
+    String tableString = new String(""table-testWeakHashMapIncreases"");
+    Table.ID table1 = Table.ID.of(tableString);
+    assertEquals(4, Table.ID.tableIds.size());
+    assertEquals(tableString, table1.canonicalID());
+  }
+
+  @Test
+  public void testWeakHashMapNoDuplicates() {
+    String tableString = new String(""table-testWeakHashMapNoDuplicates"");
+    Table.ID table1 = Table.ID.of(tableString);
+    assertEquals(4, Table.ID.tableIds.size());
+    assertEquals(tableString, table1.canonicalID());
+
+    // ensure duplicates are not created
+    Table.ID builtInTableId = Table.ID.of(""!0"");
+    assertEquals(Table.ID.METADATA, builtInTableId);
+    builtInTableId = Table.ID.of(""+r"");
+    assertEquals(Table.ID.ROOT, builtInTableId);
+    builtInTableId = Table.ID.of(""+rep"");
+    assertEquals(Table.ID.REPLICATION, builtInTableId);
+    table1 = Table.ID.of(tableString);
+    assertEquals(4, Table.ID.tableIds.size());
+    assertEquals(tableString, table1.canonicalID());
+  }
+
+  @Test
+  public void testWeakHashMapDecreases() {
+    // get bunch of table IDs that are not used
+    for (int i = 0; i < 1000; i++)
+      Table.ID.of(new String(""table"" + i));","[{'comment': 'what is this testing?', 'commenter': 'mjwall'}, {'comment': 'oh, I see it in the cleanup', 'commenter': 'mjwall'}, {'comment': ""Yeah I found that was the best way to get the GC to collect.  I will add some comments since this isn't clear."", 'commenter': 'milleruntime'}]"
292,core/src/main/java/org/apache/accumulo/core/data/impl/KeyExtent.java,"@@ -60,28 +58,11 @@
 
 public class KeyExtent implements WritableComparable<KeyExtent> {
 
-  private static final WeakHashMap<Table.ID,WeakReference<Table.ID>> tableIds = new WeakHashMap<>();
-
-  private static Table.ID dedupeTableId(Table.ID tableId) {
-    synchronized (tableIds) {
-      WeakReference<Table.ID> etir = tableIds.get(tableId);
-      if (etir != null) {
-        Table.ID eti = etir.get();
-        if (eti != null) {
-          return eti;
-        }
-      }
-
-      tableIds.put(tableId, new WeakReference<>(tableId));
-      return tableId;
-    }
-  }
-
   private Table.ID tableId;
   private Text textEndRow;
   private Text textPrevEndRow;
 
-  private final Table.ID EMPTY_ID = new Table.ID("""");
+  private final Table.ID EMPTY_ID = Table.ID.of("""");","[{'comment': 'This could be static.', 'commenter': 'ctubbsii'}]"
292,core/src/main/java/org/apache/accumulo/core/data/impl/KeyExtent.java,"@@ -173,9 +154,9 @@ public KeyExtent(Text flattenedExtent, Text prevEndRow) {
   public void setTableId(Table.ID tId) {
 
     if (tId == null)
-      throw new IllegalArgumentException(""null table name not allowed"");
+      throw new IllegalArgumentException(""null table id not allowed"");","[{'comment': 'This could be `Objects.requiresNonNull(tId, ""null table id not allowed"")`', 'commenter': 'ctubbsii'}]"
292,core/src/test/java/org/apache/accumulo/core/client/impl/NamespaceTest.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static org.junit.Assert.assertEquals;
+
+import org.junit.Test;
+
+/**
+ * Tests the Namespace ID class, mainly the internal WeakHashMap.
+ */
+public class NamespaceTest {
+
+  @Test
+  public void testWeakHashMapIncreases() {
+    String namespaceString = ""namespace-testWeakHashMapIncreases"";
+    Integer initialSize = Namespace.ID.cache.asMap().size();
+    Namespace.ID nsId = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(namespaceString, nsId.canonicalID());","[{'comment': 'Could also test that it does not increase after calling again with the same parameter (`Namespace.ID nsId2 = ....`), and that `assertSame(nsId, nsId2)`.', 'commenter': 'ctubbsii'}]"
292,core/src/test/java/org/apache/accumulo/core/client/impl/NamespaceTest.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static org.junit.Assert.assertEquals;
+
+import org.junit.Test;
+
+/**
+ * Tests the Namespace ID class, mainly the internal WeakHashMap.
+ */
+public class NamespaceTest {
+
+  @Test
+  public void testWeakHashMapIncreases() {
+    String namespaceString = ""namespace-testWeakHashMapIncreases"";
+    Integer initialSize = Namespace.ID.cache.asMap().size();
+    Namespace.ID nsId = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(namespaceString, nsId.canonicalID());
+  }
+
+  @Test
+  public void testWeakHashMapNoDuplicates() {
+    String namespaceString = ""namespace-testWeakHashMapNoDuplicates"";","[{'comment': 'Could guarantee these strings are unique using JUnit `@Rule TestName`.', 'commenter': 'ctubbsii'}, {'comment': 'Cool never knew about this.', 'commenter': 'milleruntime'}]"
292,core/src/test/java/org/apache/accumulo/core/client/impl/NamespaceTest.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static org.junit.Assert.assertEquals;
+
+import org.junit.Test;
+
+/**
+ * Tests the Namespace ID class, mainly the internal WeakHashMap.
+ */
+public class NamespaceTest {
+
+  @Test
+  public void testWeakHashMapIncreases() {
+    String namespaceString = ""namespace-testWeakHashMapIncreases"";
+    Integer initialSize = Namespace.ID.cache.asMap().size();
+    Namespace.ID nsId = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(namespaceString, nsId.canonicalID());
+  }
+
+  @Test
+  public void testWeakHashMapNoDuplicates() {
+    String namespaceString = ""namespace-testWeakHashMapNoDuplicates"";
+    Integer initialSize = Namespace.ID.cache.asMap().size();
+    Namespace.ID nsId = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(namespaceString, nsId.canonicalID());
+
+    // ensure duplicates are not created
+    Namespace.ID builtInNamespaceId = Namespace.ID.of(""+accumulo"");
+    assertEquals(Namespace.ID.ACCUMULO, builtInNamespaceId);
+    builtInNamespaceId = Namespace.ID.of(""+default"");
+    assertEquals(Namespace.ID.DEFAULT, builtInNamespaceId);","[{'comment': 'These should be `assertSame` instead of `assertEquals`.', 'commenter': 'ctubbsii'}, {'comment': ""Cool.  I've never used `assertSame`"", 'commenter': 'milleruntime'}]"
292,core/src/test/java/org/apache/accumulo/core/client/impl/NamespaceTest.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import static org.junit.Assert.assertEquals;
+
+import org.junit.Test;
+
+/**
+ * Tests the Namespace ID class, mainly the internal WeakHashMap.
+ */
+public class NamespaceTest {
+
+  @Test
+  public void testWeakHashMapIncreases() {
+    String namespaceString = ""namespace-testWeakHashMapIncreases"";
+    Integer initialSize = Namespace.ID.cache.asMap().size();
+    Namespace.ID nsId = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(namespaceString, nsId.canonicalID());
+  }
+
+  @Test
+  public void testWeakHashMapNoDuplicates() {
+    String namespaceString = ""namespace-testWeakHashMapNoDuplicates"";
+    Integer initialSize = Namespace.ID.cache.asMap().size();
+    Namespace.ID nsId = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(namespaceString, nsId.canonicalID());
+
+    // ensure duplicates are not created
+    Namespace.ID builtInNamespaceId = Namespace.ID.of(""+accumulo"");
+    assertEquals(Namespace.ID.ACCUMULO, builtInNamespaceId);
+    builtInNamespaceId = Namespace.ID.of(""+default"");
+    assertEquals(Namespace.ID.DEFAULT, builtInNamespaceId);
+    nsId = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(namespaceString, nsId.canonicalID());
+  }
+}","[{'comment': 'Could add a small test to ensure size decreases once strong refs go away. Might not be able to use `.size()` because Cache may not clean up immediately, but should be able to use `.stream().count()` to get an accurate size of remaining elements after GC.', 'commenter': 'ctubbsii'}, {'comment': ""I didn't have much luck measuring `.size()` or `asMap.size()` after GC using their cleanup method.  I will try again using streams.  "", 'commenter': 'milleruntime'}]"
292,server/master/src/main/java/org/apache/accumulo/master/tableOps/Utils.java,"@@ -59,21 +57,16 @@ static void checkTableDoesNotExist(Instance instance, String tableName, Table.ID
       throw new AcceptableThriftTableOperationException(null, tableName, operation, TableOperationExceptionType.EXISTS, null);
   }
 
-  static <T extends AbstractId> T getNextTableId(String tableName, Instance instance, Class<T> idClassType) throws AcceptableThriftTableOperationException {
+  static String getNextTableId(String tableName, Instance instance) throws AcceptableThriftTableOperationException {","[{'comment': 'If you wanted to keep the original return type, you could pass in a function to map from String to ID:\r\n\r\n```java\r\nstatic <T extends AbstractId> T getNextId(String name, Instance instance, Function<String, T> newIdFunction) throws AcceptableThriftTableOperationException {\r\n    // ...\r\n    return newIdFunction.apply(new String(nid, UTF_8));\r\n}\r\n```\r\n\r\nCan use it like this:\r\n```java\r\nTable.ID tableId = Util.getNextId(tableName, instance, Table.ID::of);\r\n```\r\n', 'commenter': 'ctubbsii'}, {'comment': ""Other than using the Function interface, this doesn't seem all that different from the reflection code that I just removed. "", 'commenter': 'milleruntime'}, {'comment': ""It's not different, but it prevents passing a String around (which I believe was the original reason you added the reflection code), but without reflection (lambdas are faster). It's up to you. I like the elegance of the functional stuff, but I don't think it matters much, as long as we're avoiding reflection."", 'commenter': 'ctubbsii'}]"
292,server/master/src/main/java/org/apache/accumulo/master/tableOps/Utils.java,"@@ -59,21 +57,16 @@ static void checkTableDoesNotExist(Instance instance, String tableName, Table.ID
       throw new AcceptableThriftTableOperationException(null, tableName, operation, TableOperationExceptionType.EXISTS, null);
   }
 
-  static <T extends AbstractId> T getNextTableId(String tableName, Instance instance, Class<T> idClassType) throws AcceptableThriftTableOperationException {
+  static String getNextTableId(String tableName, Instance instance) throws AcceptableThriftTableOperationException {
     try {
       IZooReaderWriter zoo = ZooReaderWriter.getInstance();
       final String ntp = ZooUtil.getRoot(instance) + Constants.ZTABLES;
-      byte[] nid = zoo.mutate(ntp, ZERO_BYTE, ZooUtil.PUBLIC, new Mutator() {
-        @Override
-        public byte[] mutate(byte[] currentValue) throws Exception {
-          BigInteger nextId = new BigInteger(new String(currentValue, UTF_8), Character.MAX_RADIX);
-          nextId = nextId.add(BigInteger.ONE);
-          return nextId.toString(Character.MAX_RADIX).getBytes(UTF_8);
-        }
+      byte[] nid = zoo.mutate(ntp, ZERO_BYTE, ZooUtil.PUBLIC, currentValue -> {
+        BigInteger nextId = new BigInteger(new String(currentValue, UTF_8), Character.MAX_RADIX);
+        nextId = nextId.add(BigInteger.ONE);
+        return nextId.toString(Character.MAX_RADIX).getBytes(UTF_8);
       });
-      Constructor<T> constructor = idClassType.getConstructor(String.class);
-      return constructor.newInstance(new String(nid, UTF_8));","[{'comment': 'Very big +1 to getting rid of this reflection implementation.', 'commenter': 'ctubbsii'}]"
292,core/src/test/java/org/apache/accumulo/core/client/impl/NamespaceTest.java,"@@ -17,38 +17,73 @@
 package org.apache.accumulo.core.client.impl;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
+import org.junit.Rule;
 import org.junit.Test;
+import org.junit.rules.TestName;
 
 /**
- * Tests the Namespace ID class, mainly the internal WeakHashMap.
+ * Tests the Namespace ID class, mainly the internal cache.
  */
 public class NamespaceTest {
+  @Rule
+  public TestName name = new TestName();
 
   @Test
-  public void testWeakHashMapIncreases() {
-    String namespaceString = ""namespace-testWeakHashMapIncreases"";
-    Integer initialSize = Namespace.ID.cache.asMap().size();
+  public void testCacheIncreases() {
+    String namespaceString = ""namespace-"" + name.getMethodName();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();
     Namespace.ID nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
   }
 
   @Test
-  public void testWeakHashMapNoDuplicates() {
-    String namespaceString = ""namespace-testWeakHashMapNoDuplicates"";
-    Integer initialSize = Namespace.ID.cache.asMap().size();
+  public void testCacheNoDuplicates() {
+    String namespaceString = ""namespace-"" + name.getMethodName();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();
     Namespace.ID nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
 
     // ensure duplicates are not created
     Namespace.ID builtInNamespaceId = Namespace.ID.of(""+accumulo"");
-    assertEquals(Namespace.ID.ACCUMULO, builtInNamespaceId);
+    assertSame(Namespace.ID.ACCUMULO, builtInNamespaceId);
     builtInNamespaceId = Namespace.ID.of(""+default"");
-    assertEquals(Namespace.ID.DEFAULT, builtInNamespaceId);
+    assertSame(Namespace.ID.DEFAULT, builtInNamespaceId);
     nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
+    Namespace.ID nsId2 = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
+    assertSame(nsId, nsId2);
+  }
+
+  @Test(timeout = 60_000)
+  public void testCacheDecreasesAfterGC() {
+    generateJunkCacheEntries();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();
+    Long postGCSize;
+    System.out.println(""Namespace.ID.cache.asMap().entrySet().stream().count() = "" + initialSize);
+    do {
+      System.gc();
+      try {
+        Thread.sleep(5000);
+      } catch (InterruptedException e) {
+        fail(""Thread interrupted while waiting for GC"");
+      }
+      postGCSize = Namespace.ID.cache.asMap().entrySet().stream().count();
+      System.out.println(""After GC: Namespace.ID.cache.asMap().entrySet().stream().count() = "" + postGCSize);","[{'comment': 'Should use logger or remove these debugging lines.', 'commenter': 'ctubbsii'}]"
292,core/src/test/java/org/apache/accumulo/core/client/impl/NamespaceTest.java,"@@ -17,38 +17,73 @@
 package org.apache.accumulo.core.client.impl;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
+import org.junit.Rule;
 import org.junit.Test;
+import org.junit.rules.TestName;
 
 /**
- * Tests the Namespace ID class, mainly the internal WeakHashMap.
+ * Tests the Namespace ID class, mainly the internal cache.
  */
 public class NamespaceTest {
+  @Rule
+  public TestName name = new TestName();
 
   @Test
-  public void testWeakHashMapIncreases() {
-    String namespaceString = ""namespace-testWeakHashMapIncreases"";
-    Integer initialSize = Namespace.ID.cache.asMap().size();
+  public void testCacheIncreases() {
+    String namespaceString = ""namespace-"" + name.getMethodName();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();
     Namespace.ID nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
   }
 
   @Test
-  public void testWeakHashMapNoDuplicates() {
-    String namespaceString = ""namespace-testWeakHashMapNoDuplicates"";
-    Integer initialSize = Namespace.ID.cache.asMap().size();
+  public void testCacheNoDuplicates() {
+    String namespaceString = ""namespace-"" + name.getMethodName();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();
     Namespace.ID nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
 
     // ensure duplicates are not created
     Namespace.ID builtInNamespaceId = Namespace.ID.of(""+accumulo"");
-    assertEquals(Namespace.ID.ACCUMULO, builtInNamespaceId);
+    assertSame(Namespace.ID.ACCUMULO, builtInNamespaceId);
     builtInNamespaceId = Namespace.ID.of(""+default"");
-    assertEquals(Namespace.ID.DEFAULT, builtInNamespaceId);
+    assertSame(Namespace.ID.DEFAULT, builtInNamespaceId);
     nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
+    Namespace.ID nsId2 = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
+    assertSame(nsId, nsId2);
+  }
+
+  @Test(timeout = 60_000)
+  public void testCacheDecreasesAfterGC() {
+    generateJunkCacheEntries();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();
+    Long postGCSize;
+    System.out.println(""Namespace.ID.cache.asMap().entrySet().stream().count() = "" + initialSize);
+    do {
+      System.gc();
+      try {
+        Thread.sleep(5000);","[{'comment': ""Could shorten this to 1 second or 500ms, since there's a good chance it the condition will be met quickly."", 'commenter': 'ctubbsii'}]"
292,core/src/test/java/org/apache/accumulo/core/client/impl/NamespaceTest.java,"@@ -17,38 +17,73 @@
 package org.apache.accumulo.core.client.impl;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
+import org.junit.Rule;
 import org.junit.Test;
+import org.junit.rules.TestName;
 
 /**
- * Tests the Namespace ID class, mainly the internal WeakHashMap.
+ * Tests the Namespace ID class, mainly the internal cache.
  */
 public class NamespaceTest {
+  @Rule
+  public TestName name = new TestName();
 
   @Test
-  public void testWeakHashMapIncreases() {
-    String namespaceString = ""namespace-testWeakHashMapIncreases"";
-    Integer initialSize = Namespace.ID.cache.asMap().size();
+  public void testCacheIncreases() {
+    String namespaceString = ""namespace-"" + name.getMethodName();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();
     Namespace.ID nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
   }
 
   @Test
-  public void testWeakHashMapNoDuplicates() {
-    String namespaceString = ""namespace-testWeakHashMapNoDuplicates"";
-    Integer initialSize = Namespace.ID.cache.asMap().size();
+  public void testCacheNoDuplicates() {
+    String namespaceString = ""namespace-"" + name.getMethodName();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();
     Namespace.ID nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
 
     // ensure duplicates are not created
     Namespace.ID builtInNamespaceId = Namespace.ID.of(""+accumulo"");
-    assertEquals(Namespace.ID.ACCUMULO, builtInNamespaceId);
+    assertSame(Namespace.ID.ACCUMULO, builtInNamespaceId);
     builtInNamespaceId = Namespace.ID.of(""+default"");
-    assertEquals(Namespace.ID.DEFAULT, builtInNamespaceId);
+    assertSame(Namespace.ID.DEFAULT, builtInNamespaceId);
     nsId = Namespace.ID.of(namespaceString);
-    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().size());
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
     assertEquals(namespaceString, nsId.canonicalID());
+    Namespace.ID nsId2 = Namespace.ID.of(namespaceString);
+    assertEquals(initialSize + 1, Namespace.ID.cache.asMap().entrySet().stream().count());
+    assertSame(nsId, nsId2);
+  }
+
+  @Test(timeout = 60_000)
+  public void testCacheDecreasesAfterGC() {
+    generateJunkCacheEntries();
+    Long initialSize = Namespace.ID.cache.asMap().entrySet().stream().count();","[{'comment': 'This could have already garbage collected some of the entries, after leaving the method, so further GC may not do anything. Instead, consider setting the initialSize before the junk entries are added, and the loop condition should be: `while (postGCSize > initialSize);`', 'commenter': 'ctubbsii'}]"
305,server/master/src/main/java/org/apache/accumulo/master/metrics/Metrics2ReplicationMetrics.java,"@@ -124,4 +142,55 @@ protected int getNumConfiguredPeers() {
   protected int getMaxReplicationThreads() {
     return replicationUtil.getMaxReplicationThreads(master.getMasterMonitorInfo());
   }
+
+  protected void addReplicationQueueTimeMetrics() {
+    // Exit early if replication table is offline
+    if (TableState.ONLINE != Tables.getTableState(master.getInstance(), ReplicationTable.ID)) {
+      return;
+    }
+
+    // Exit early if we have no replication peers configured
+    if (replicationUtil.getPeers().isEmpty()) {
+      return;
+    }
+
+    Set<Path> paths = replicationUtil.getPendingReplicationPaths();
+
+    // We'll take a snap of the current time and use this as a diff between any deleted
+    // file's modification time and now. The reported latency will be off by at most a
+    // number of seconds equal to the metric polling period
+    long currentTime = System.currentTimeMillis();
+
+    // Iterate through all the pending paths and update the mod time if we don't know it yet
+    for (Path path : paths) {
+      if (!pathModTimes.containsKey(path)) {
+        try {
+          pathModTimes.put(path, master.getFileSystem().getFileStatus(path).getModificationTime());
+        } catch (IOException e) {
+          // Ignore all IOExceptions
+          // Either the system is unavailable or the file was deleted
+          // since the initial scan and this check","[{'comment': 'Add a `log.trace` in case you/someone ever runs into troubles and wants to try to debug this.', 'commenter': 'joshelser'}]"
305,server/master/src/main/java/org/apache/accumulo/master/metrics/Metrics2ReplicationMetrics.java,"@@ -124,4 +142,55 @@ protected int getNumConfiguredPeers() {
   protected int getMaxReplicationThreads() {
     return replicationUtil.getMaxReplicationThreads(master.getMasterMonitorInfo());
   }
+
+  protected void addReplicationQueueTimeMetrics() {
+    // Exit early if replication table is offline
+    if (TableState.ONLINE != Tables.getTableState(master.getInstance(), ReplicationTable.ID)) {
+      return;
+    }
+
+    // Exit early if we have no replication peers configured
+    if (replicationUtil.getPeers().isEmpty()) {
+      return;
+    }
+
+    Set<Path> paths = replicationUtil.getPendingReplicationPaths();
+
+    // We'll take a snap of the current time and use this as a diff between any deleted
+    // file's modification time and now. The reported latency will be off by at most a
+    // number of seconds equal to the metric polling period
+    long currentTime = System.currentTimeMillis();
+
+    // Iterate through all the pending paths and update the mod time if we don't know it yet
+    for (Path path : paths) {
+      if (!pathModTimes.containsKey(path)) {
+        try {
+          pathModTimes.put(path, master.getFileSystem().getFileStatus(path).getModificationTime());
+        } catch (IOException e) {
+          // Ignore all IOExceptions
+          // Either the system is unavailable or the file was deleted
+          // since the initial scan and this check
+        }
+      }
+    }
+
+    // Remove all currently pending files
+    Set<Path> deletedPaths = new HashSet<>(pathModTimes.keySet());
+    deletedPaths.removeAll(paths);
+
+    // Exit early if we have no replicated files to report on
+    if (deletedPaths.isEmpty()) {
+      return;
+    }
+
+    replicationQueueTimeStat.resetMinMax();
+
+    for (Path path : deletedPaths) {
+      // Remove this path and add the latency
+      long modTime = pathModTimes.remove(path);","[{'comment': ""This will throw an error is `pathModTimes` ever does not have a mapping for `path` because you're unboxing the `Long` into a `long`."", 'commenter': 'joshelser'}, {'comment': ""I was actually thinking how best to handle this.  It'd be an implementation error if the path to be removed wasn't in `pathModTimes`, since `deletedPaths` is a subset of `pathModTimes`.  Maybe check for existence and log an error if it doesn't exist?"", 'commenter': 'adamjshook'}, {'comment': ""It's just the unboxing you really have to worry about. Something like the following would be fine:\r\n\r\n```java\r\nLong modTime = pathModTimes.remove(path);\r\nif (modTime != null) {\r\n  ...\r\n}\r\n```\r\n\r\nJava will handle the unboxing for you once you guaranteed that `modTime` is definitely non-null."", 'commenter': 'joshelser'}]"
305,server/master/src/main/java/org/apache/accumulo/master/metrics/Metrics2ReplicationMetrics.java,"@@ -124,4 +142,55 @@ protected int getNumConfiguredPeers() {
   protected int getMaxReplicationThreads() {
     return replicationUtil.getMaxReplicationThreads(master.getMasterMonitorInfo());
   }
+
+  protected void addReplicationQueueTimeMetrics() {","[{'comment': 'Could you add a simple test case for this, please?', 'commenter': 'joshelser'}]"
305,server/master/src/main/java/org/apache/accumulo/master/metrics/Metrics2ReplicationMetrics.java,"@@ -26,38 +29,51 @@
 import org.apache.accumulo.master.Master;
 import org.apache.accumulo.server.metrics.Metrics;
 import org.apache.accumulo.server.replication.ReplicationUtil;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.metrics2.MetricsCollector;
 import org.apache.hadoop.metrics2.MetricsRecordBuilder;
 import org.apache.hadoop.metrics2.MetricsSource;
 import org.apache.hadoop.metrics2.MetricsSystem;
 import org.apache.hadoop.metrics2.lib.Interns;
 import org.apache.hadoop.metrics2.lib.MetricsRegistry;
+import org.apache.hadoop.metrics2.lib.MutableQuantiles;
+import org.apache.hadoop.metrics2.lib.MutableStat;
 
 /**
  *
  */
 public class Metrics2ReplicationMetrics implements Metrics, MetricsSource {
   public static final String NAME = MASTER_NAME + "",sub=Replication"", DESCRIPTION = ""Data-Center Replication Metrics"", CONTEXT = ""master"",
       RECORD = ""MasterReplication"";
-  public static final String PENDING_FILES = ""filesPendingReplication"", NUM_PEERS = ""numPeers"", MAX_REPLICATION_THREADS = ""maxReplicationThreads"";
+  public static final String PENDING_FILES = ""filesPendingReplication"", NUM_PEERS = ""numPeers"", MAX_REPLICATION_THREADS = ""maxReplicationThreads"",
+      REPLICATION_QUEUE_TIME_QUANTILES = ""replicationQueue10m"", REPLICATION_QUEUE_TIME = ""replicationQueue"";
 
   private final Master master;
   private final MetricsSystem system;
   private final MetricsRegistry registry;
   private final ReplicationUtil replicationUtil;
+  private final MutableQuantiles replicationQueueTimeQuantiles;
+  private final MutableStat replicationQueueTimeStat;
+  private final Map<Path,Long> pathModTimes;
 
   Metrics2ReplicationMetrics(Master master, MetricsSystem system) {
     this.master = master;
     this.system = system;
 
+    pathModTimes = new HashMap<>();
+
     registry = new MetricsRegistry(Interns.info(NAME, DESCRIPTION));
     replicationUtil = new ReplicationUtil(master);
+    replicationQueueTimeQuantiles = registry.newQuantiles(REPLICATION_QUEUE_TIME_QUANTILES, ""replication queue time quantiles in milliseconds"", ""ops"",","[{'comment': 'nit: capitalize ""Replication"" please', 'commenter': 'joshelser'}]"
305,server/master/src/main/java/org/apache/accumulo/master/metrics/Metrics2ReplicationMetrics.java,"@@ -26,38 +29,51 @@
 import org.apache.accumulo.master.Master;
 import org.apache.accumulo.server.metrics.Metrics;
 import org.apache.accumulo.server.replication.ReplicationUtil;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.metrics2.MetricsCollector;
 import org.apache.hadoop.metrics2.MetricsRecordBuilder;
 import org.apache.hadoop.metrics2.MetricsSource;
 import org.apache.hadoop.metrics2.MetricsSystem;
 import org.apache.hadoop.metrics2.lib.Interns;
 import org.apache.hadoop.metrics2.lib.MetricsRegistry;
+import org.apache.hadoop.metrics2.lib.MutableQuantiles;
+import org.apache.hadoop.metrics2.lib.MutableStat;
 
 /**
  *
  */
 public class Metrics2ReplicationMetrics implements Metrics, MetricsSource {
   public static final String NAME = MASTER_NAME + "",sub=Replication"", DESCRIPTION = ""Data-Center Replication Metrics"", CONTEXT = ""master"",
       RECORD = ""MasterReplication"";
-  public static final String PENDING_FILES = ""filesPendingReplication"", NUM_PEERS = ""numPeers"", MAX_REPLICATION_THREADS = ""maxReplicationThreads"";
+  public static final String PENDING_FILES = ""filesPendingReplication"", NUM_PEERS = ""numPeers"", MAX_REPLICATION_THREADS = ""maxReplicationThreads"",
+      REPLICATION_QUEUE_TIME_QUANTILES = ""replicationQueue10m"", REPLICATION_QUEUE_TIME = ""replicationQueue"";
 
   private final Master master;
   private final MetricsSystem system;
   private final MetricsRegistry registry;
   private final ReplicationUtil replicationUtil;
+  private final MutableQuantiles replicationQueueTimeQuantiles;
+  private final MutableStat replicationQueueTimeStat;
+  private final Map<Path,Long> pathModTimes;
 
   Metrics2ReplicationMetrics(Master master, MetricsSystem system) {
     this.master = master;
     this.system = system;
 
+    pathModTimes = new HashMap<>();
+
     registry = new MetricsRegistry(Interns.info(NAME, DESCRIPTION));
     replicationUtil = new ReplicationUtil(master);
+    replicationQueueTimeQuantiles = registry.newQuantiles(REPLICATION_QUEUE_TIME_QUANTILES, ""replication queue time quantiles in milliseconds"", ""ops"",
+        ""latency"", 600);
+    replicationQueueTimeStat = registry.newStat(REPLICATION_QUEUE_TIME, ""replication queue time stat in milliseconds"", ""ops"", ""latency"", true);","[{'comment': 'nit: ""statistics"" instead of ""stat"" (and capitalize Replication too)', 'commenter': 'joshelser'}]"
326,server/monitor/src/main/resources/resources/js/tables.js,"@@ -184,7 +184,7 @@ function populateTable(ns) {
 
         var row = [];
         row.push(createFirstCell(val.tablename,
-            '<a href=""/tables/' + val.tableId + '"">' + val.tablename + '</a>'));
+            '<a href=""/tables/' + atob(val.tableId.utf8) + '"">' + val.tablename + '</a>'));","[{'comment': 'Good find! While this is one possible workaround, I think the core of this problem is actually that the Table.ID object is being stored in the first place. I think the real problem is that `org.apache.accumulo.monitor.rest.tables.TableInformation` is storing a `Table.ID` instead of its canonical string. Since the point of this class is to create a serialized version of the table id, I think it should store it as a String in that class. Fixing it there, will also fix the JSON objects in the REST calls, wherever it is used, rather than just in this code.', 'commenter': 'ctubbsii'}]"
326,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tables/TableInformation.java,"@@ -30,7 +29,7 @@
 
   // Variable names become JSON keys
   public String tablename;
-  public Table.ID tableId;
+  public String tableId;","[{'comment': 'Could change the variable name, like you did below.  You would probably have to change it on the front end as well though.  You could use tableIdString if the other is too long.  It is just nice to know you are working with a string and not the Table.ID object.', 'commenter': 'milleruntime'}]"
326,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tables/TablesResource.java,"@@ -112,6 +112,7 @@ private static TablesList generateTables(TablesList tableNamespace) {
     for (Entry<String,Table.ID> entry : Tables.getNameToIdMap(HdfsZooInstance.getInstance()).entrySet()) {
       String tableName = entry.getKey();
       Table.ID tableId = entry.getValue();
+      String canonicalTableId = tableId.canonicalID();","[{'comment': ""I was thinking this could be deferred to the constructors of `TableInformation`; that way, this class wouldn't need to change at all, and we can keep the `Table.ID` object around as long as possible before it is serialized in `TableInformation`."", 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii I implemented this, keeping `TablesResource` the way it was and having `TableInformation` still take the `Table.ID` object as a parameter, but the constructor takes the `canonicalID` from it to store.', 'commenter': 'blue-diode'}]"
331,server/monitor/src/main/java/org/apache/accumulo/monitor/util/ParameterValidator.java,"@@ -22,6 +22,8 @@
 public interface ParameterValidator {
 
   String ALPHA_NUM_REGEX = ""\\w+"";
+  // Allow the special default table IDs
+  String ALPHA_NUM_REGEX_TABLE_ID = ""[\\!\\+]?(\\w)+"";","[{'comment': ""I don't think you need to escape either the exclamation or the plus sign in the character class. Also, you should not put the `\\\\w` in a capture group (remove parens)."", 'commenter': 'ctubbsii'}, {'comment': 'I agree ""[!+]?\\\\w+"" should work just fine. ', 'commenter': 'jkrdev'}, {'comment': 'Yup, thanks.  Fixed in 19ba632', 'commenter': 'milleruntime'}]"
331,server/monitor/src/main/java/org/apache/accumulo/monitor/util/ParameterValidator.java,"@@ -22,6 +22,8 @@
 public interface ParameterValidator {
 
   String ALPHA_NUM_REGEX = ""\\w+"";
+  // Allow the special default table IDs
+  String ALPHA_NUM_REGEX_TABLE_ID = ""[!+]?\\w+"";","[{'comment': ""It occurs to me that these all should probably be `final`. I think `public static` is implied since this is an interface, but I don't think `final` is implied. I'm actually surprised this works, because I thought you could only reference final strings in annotations. So, maybe `final` is implied for strings in interfaces?"", 'commenter': 'ctubbsii'}, {'comment': 'IntelliJ gives me the suggestion: ""Modifier \'final\' is redundant for interface fields""', 'commenter': 'milleruntime'}, {'comment': 'TIL. Awesome. :smile:', 'commenter': 'ctubbsii'}]"
336,core/pom.xml,"@@ -67,6 +67,11 @@
       <groupId>commons-logging</groupId>
       <artifactId>commons-logging</artifactId>
     </dependency>
+    <dependency>
+      <groupId>javax.xml.bind</groupId>
+      <artifactId>jaxb-api</artifactId>
+      <version>2.2.12</version>","[{'comment': ""The version should go in the parent POM's dependencyManagement section. The child POMs should omit the version."", 'commenter': 'ctubbsii'}]"
336,core/src/main/java/org/apache/accumulo/core/client/impl/AbstractId.java,"@@ -39,7 +39,7 @@ protected AbstractId(final String canonical) {
   /**
    * The canonical ID
    */
-  public final String canonicalID() {
+  public String canonicalID() {","[{'comment': ""Why make this non-final? Given how important this method is for its serialization/internal representation, it's pretty important that it not be overridden."", 'commenter': 'ctubbsii'}, {'comment': 'Definitely an oversight on my part, fixed.', 'commenter': 'blue-diode'}, {'comment': 'Ah, good. I was worried that this was required for the serialization to work for some reason.', 'commenter': 'ctubbsii'}]"
336,core/src/main/java/org/apache/accumulo/core/client/impl/JaxbAbstractIdSerializer.java,"@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import javax.xml.bind.annotation.adapters.XmlAdapter;
+
+public class JaxbAbstractIdSerializer extends XmlAdapter<String,AbstractId> {
+
+  @Override
+  public String marshal(AbstractId id) {
+    return id.canonicalID();
+  }
+
+  @Override
+  public AbstractId unmarshal(String id) {
+    // should not unmarshal from String
+    throw new AssertionError(""Cannot unmarshal from String"");","[{'comment': 'UnsupportedOperationException might be more appropriate here.', 'commenter': 'ctubbsii'}]"
336,core/src/main/java/org/apache/accumulo/core/client/impl/Namespace.java,"@@ -35,8 +37,9 @@
    * Uses an internal cache and private constructor for storing a WeakReference of every Namespace.ID. Therefore, a Namespace.ID can't be instantiated outside
    * this class and is accessed by calling Namespace.ID.{@link #of(String)}.
    */
+  @XmlJavaTypeAdapter(JaxbAbstractIdSerializer.class)
   public static class ID extends AbstractId {
-    private static final long serialVersionUID = 8931104141709170293L;
+    private static final long serialVersionUID = -155513612834787244L;","[{'comment': ""I don't think this needs to change."", 'commenter': 'ctubbsii'}]"
336,core/src/main/java/org/apache/accumulo/core/client/impl/Table.java,"@@ -32,8 +34,9 @@
    * Uses an internal cache and private constructor for storing a WeakReference of every Table.ID. Therefore, a Table.ID can't be instantiated outside this
    * class and is accessed by calling Table.ID.{@link #of(String)}.
    */
+  @XmlJavaTypeAdapter(JaxbAbstractIdSerializer.class)
   public static class ID extends AbstractId {
-    private static final long serialVersionUID = 7399913185860577809L;
+    private static final long serialVersionUID = -155513612834787244L;","[{'comment': ""I don't think the serialVersionUID needs to change."", 'commenter': 'ctubbsii'}]"
336,core/src/main/java/org/apache/accumulo/core/client/impl/JaxbAbstractIdSerializer.java,"@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.core.client.impl;
+
+import javax.xml.bind.annotation.adapters.XmlAdapter;
+
+public class JaxbAbstractIdSerializer extends XmlAdapter<String,AbstractId> {
+
+  @Override
+  public String marshal(AbstractId id) {
+    return id.canonicalID();","[{'comment': ""Should guard against NPE here. Serialization shouldn't break if ID is null."", 'commenter': 'ctubbsii'}]"
336,core/src/main/java/org/apache/accumulo/core/client/impl/Namespace.java,"@@ -35,6 +37,7 @@
    * Uses an internal cache and private constructor for storing a WeakReference of every Namespace.ID. Therefore, a Namespace.ID can't be instantiated outside
    * this class and is accessed by calling Namespace.ID.{@link #of(String)}.
    */
+  @XmlJavaTypeAdapter(JaxbAbstractIdSerializer.class)","[{'comment': 'Too bad `@XmlJavaTypeAdapter` is not marked with `@Inherited`. Then, you could just annotate `AbstractId` instead of both `Table.ID` and `Namespace.ID`. It may be possible to register the serializer directly, using a method call in the monitor code instead of annotations. This would also save from adding the compile-time dependency to the core module. But, this is almost certainly not worth it.', 'commenter': 'ctubbsii'}]"
337,core/src/main/java/org/apache/accumulo/core/client/admin/NewTableConfiguration.java,"@@ -146,4 +163,116 @@ public NewTableConfiguration enableSummarization(SummarizerConfiguration... conf
     summarizerProps = tmp;
     return this;
   }
+
+  /**
+   * Configures a table's locality groups prior to initial table creation.
+   *
+   * Allows locality groups to be set prior to table creation. Additional calls to this method prior to table creation will overwrite previous locality group
+   * mappings.
+   *
+   * @param groups
+   *          mapping of locality group names to column families in the locality group
+   *
+   * @since 2.0.0
+   */
+  public void setLocalityGroups(Map<String,Set<Text>> groups) {","[{'comment': 'For consistency with existing methods, would be nice to return `this`', 'commenter': 'keith-turner'}, {'comment': ""@keith-turner I went back and forth initially and at one point considered having two versions, one returning this and one returning void. I eventually went with void to match the return type of the TableOperations version. I can modify to return 'this' instead though. Would you suggest that the attachIterator methods return 'this' as well?"", 'commenter': 'jmark99'}, {'comment': 'Yeah I think both should return this.  IMO its better to match the norms in this class rather than what tableoperations does.  Also it allows method calls the be chained.', 'commenter': 'keith-turner'}, {'comment': ""Updated the methods to return 'this' and created new test to verify behavior."", 'commenter': 'jmark99'}]"
337,core/src/main/java/org/apache/accumulo/core/client/admin/NewTableConfiguration.java,"@@ -146,4 +163,116 @@ public NewTableConfiguration enableSummarization(SummarizerConfiguration... conf
     summarizerProps = tmp;
     return this;
   }
+
+  /**
+   * Configures a table's locality groups prior to initial table creation.
+   *
+   * Allows locality groups to be set prior to table creation. Additional calls to this method prior to table creation will overwrite previous locality group
+   * mappings.
+   *
+   * @param groups
+   *          mapping of locality group names to column families in the locality group
+   *
+   * @since 2.0.0
+   */
+  public void setLocalityGroups(Map<String,Set<Text>> groups) {
+    // ensure locality groups do not overlap
+    LocalityGroupUtil.ensureNonOverlappingGroups(groups);
+    localityProps = new HashMap<>();
+    for (Entry<String,Set<Text>> entry : groups.entrySet()) {
+      Set<Text> colFams = entry.getValue();
+      String value = LocalityGroupUtil.encodeColumnFamilies(colFams);
+      localityProps.put(Property.TABLE_LOCALITY_GROUP_PREFIX + entry.getKey(), value);
+    }
+    // localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), Joiner.on("","").join(groups.keySet()));
+    localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), groups.keySet().stream().collect(Collectors.joining("","")));
+    // localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), Stream.of(groups.keySet().collect(Collectors.joining("",""));","[{'comment': 'why the commented out code?', 'commenter': 'keith-turner'}, {'comment': 'Just an oversight. Thanks for the catch.', 'commenter': 'jmark99'}]"
337,core/src/main/java/org/apache/accumulo/core/client/admin/NewTableConfiguration.java,"@@ -146,4 +163,116 @@ public NewTableConfiguration enableSummarization(SummarizerConfiguration... conf
     summarizerProps = tmp;
     return this;
   }
+
+  /**
+   * Configures a table's locality groups prior to initial table creation.
+   *
+   * Allows locality groups to be set prior to table creation. Additional calls to this method prior to table creation will overwrite previous locality group
+   * mappings.
+   *
+   * @param groups
+   *          mapping of locality group names to column families in the locality group
+   *
+   * @since 2.0.0
+   */","[{'comment': 'A javadoc `@see` tag linking to locality groups methods on table operation might be a nice thing to add. ', 'commenter': 'keith-turner'}, {'comment': 'Added @see tag', 'commenter': 'jmark99'}]"
337,core/src/main/java/org/apache/accumulo/core/client/admin/NewTableConfiguration.java,"@@ -146,4 +163,116 @@ public NewTableConfiguration enableSummarization(SummarizerConfiguration... conf
     summarizerProps = tmp;
     return this;
   }
+
+  /**
+   * Configures a table's locality groups prior to initial table creation.
+   *
+   * Allows locality groups to be set prior to table creation. Additional calls to this method prior to table creation will overwrite previous locality group
+   * mappings.
+   *
+   * @param groups
+   *          mapping of locality group names to column families in the locality group
+   *
+   * @since 2.0.0
+   */
+  public void setLocalityGroups(Map<String,Set<Text>> groups) {
+    // ensure locality groups do not overlap
+    LocalityGroupUtil.ensureNonOverlappingGroups(groups);
+    localityProps = new HashMap<>();
+    for (Entry<String,Set<Text>> entry : groups.entrySet()) {
+      Set<Text> colFams = entry.getValue();
+      String value = LocalityGroupUtil.encodeColumnFamilies(colFams);
+      localityProps.put(Property.TABLE_LOCALITY_GROUP_PREFIX + entry.getKey(), value);
+    }
+    // localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), Joiner.on("","").join(groups.keySet()));
+    localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), groups.keySet().stream().collect(Collectors.joining("","")));
+    // localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), Stream.of(groups.keySet().collect(Collectors.joining("",""));
+    // Stream.of(groups.keySet()).collect(joining("",""));
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * Additional calls to this method before table creation will overwrite previous iterator settings.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @throws AccumuloSecurityException
+   *           thrown if the user does not have the ability to set properties on the table
+   * @throws AccumuloException
+   *           if a general error occurs
+   * @throws TableNotFoundException
+   *           if the table does not exist
+   *
+   * @since 2.0.0
+   */
+  public void attachIterator(IteratorSetting setting) throws AccumuloException, TableNotFoundException {
+    attachIterator(setting, EnumSet.allOf(IteratorScope.class));
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @param scopes
+   *          enumerated set of iterator scopes
+   * @throws AccumuloException
+   *           if a general error occurs
+   * @throws AccumuloSecurityException","[{'comment': 'This exception is not thrown.', 'commenter': 'keith-turner'}, {'comment': 'Removed. Leftover from some previous incarnation of the code.', 'commenter': 'jmark99'}]"
337,core/src/main/java/org/apache/accumulo/core/client/admin/NewTableConfiguration.java,"@@ -146,4 +163,116 @@ public NewTableConfiguration enableSummarization(SummarizerConfiguration... conf
     summarizerProps = tmp;
     return this;
   }
+
+  /**
+   * Configures a table's locality groups prior to initial table creation.
+   *
+   * Allows locality groups to be set prior to table creation. Additional calls to this method prior to table creation will overwrite previous locality group
+   * mappings.
+   *
+   * @param groups
+   *          mapping of locality group names to column families in the locality group
+   *
+   * @since 2.0.0
+   */
+  public void setLocalityGroups(Map<String,Set<Text>> groups) {
+    // ensure locality groups do not overlap
+    LocalityGroupUtil.ensureNonOverlappingGroups(groups);
+    localityProps = new HashMap<>();
+    for (Entry<String,Set<Text>> entry : groups.entrySet()) {
+      Set<Text> colFams = entry.getValue();
+      String value = LocalityGroupUtil.encodeColumnFamilies(colFams);
+      localityProps.put(Property.TABLE_LOCALITY_GROUP_PREFIX + entry.getKey(), value);
+    }
+    // localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), Joiner.on("","").join(groups.keySet()));
+    localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), groups.keySet().stream().collect(Collectors.joining("","")));
+    // localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), Stream.of(groups.keySet().collect(Collectors.joining("",""));
+    // Stream.of(groups.keySet()).collect(joining("",""));
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * Additional calls to this method before table creation will overwrite previous iterator settings.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @throws AccumuloSecurityException
+   *           thrown if the user does not have the ability to set properties on the table
+   * @throws AccumuloException
+   *           if a general error occurs
+   * @throws TableNotFoundException
+   *           if the table does not exist
+   *
+   * @since 2.0.0
+   */
+  public void attachIterator(IteratorSetting setting) throws AccumuloException, TableNotFoundException {
+    attachIterator(setting, EnumSet.allOf(IteratorScope.class));
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @param scopes
+   *          enumerated set of iterator scopes
+   * @throws AccumuloException
+   *           if a general error occurs
+   * @throws AccumuloSecurityException
+   *           thrown if the user does not have the ability to set properties on the table
+   * @throws TableNotFoundException
+   *           if the table does not exist
+   *
+   * @since 2.0.0
+   */
+  public void attachIterator(IteratorSetting setting, EnumSet<IteratorScope> scopes) throws AccumuloException, TableNotFoundException {
+    checkArgument(setting != null, ""setting is null"");
+    checkArgument(scopes != null, ""scopes is null"");
+    if (iteratorProps.isEmpty()) {
+      iteratorProps = new HashMap<>();
+    }
+    checkIteratorConflicts(setting, scopes);
+    for (IteratorScope scope : scopes) {
+      String root = String.format(""%s%s.%s"", Property.TABLE_ITERATOR_PREFIX, scope.name().toLowerCase(), setting.getName());
+      for (Entry<String,String> prop : setting.getOptions().entrySet()) {
+        iteratorProps.put(root + "".opt."" + prop.getKey(), prop.getValue());
+      }
+      iteratorProps.put(root, setting.getPriority() + "","" + setting.getIteratorClass());
+    }
+  }
+
+  private void checkIteratorConflicts(IteratorSetting setting, EnumSet<IteratorScope> scopes) throws AccumuloException, TableNotFoundException {
+    checkArgument(setting != null, ""setting is null"");","[{'comment': 'Could the code in TableOperationsHelper be refactored such it could be called from here.  I am thinking of the following refactoring.\r\n\r\n```java\r\nclass TableOperationsHelper {\r\n  //add this method and call it from NewTableConfig\r\n  public void checkIteratorConflicts(Map<String, String> props, IteratorSetting setting, EnumSet<IteratorScope> scopes) throws AccumuloException, TableNotFoundException {\r\n }\r\n  //refactor this existing method to call above method\r\n  public void checkIteratorConflicts(String tablename, IteratorSetting setting, EnumSet<IteratorScope> scopes) throws AccumuloException, TableNotFoundException {\r\n }\r\n}\r\n```', 'commenter': 'keith-turner'}, {'comment': ""@keith-turner I have a question concerning this. It's strightforward enough to create the new method and refactor the old. Perhaps I'm not completely versed on its usage yet, but wouldn't calling the method from NewTableConfiguration require instantiation of the TableOperationsImpl object? When I was playing around with it, this required a ClientConfiguration object which then required an Instance object,etc. It seemed to get messy attempting the call. Am I missing something obvious?"", 'commenter': 'jmark99'}, {'comment': 'There is at least one more very similar instance of checkIteratorConflicts for namespaces as well. Would it make sense to create a new ticket to consolidate the various permutations of this method and perhaps place the new versions into  the iteratorUtil class where it could more easily be called by the various classes that need that check?', 'commenter': 'jmark99'}, {'comment': 'Can you make the new method in TableOpHelper static?', 'commenter': 'keith-turner'}, {'comment': ""I'll give it a shot and see how that works."", 'commenter': 'jmark99'}, {'comment': 'Created static version of checkIteratorConflicts and refactored existing version of method within TableOptionsHelper to make use of static version. Used static version to check conflicts in new NewTableConfiguration methods.', 'commenter': 'jmark99'}]"
337,test/src/main/java/org/apache/accumulo/test/NewConfigurationTestIT.java,"@@ -0,0 +1,473 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.test;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+
+import java.util.Collections;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.Connector;
+import org.apache.accumulo.core.client.IteratorSetting;
+import org.apache.accumulo.core.client.TableExistsException;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.admin.NewTableConfiguration;
+import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;
+import org.apache.accumulo.harness.SharedMiniClusterBase;
+import org.apache.hadoop.io.Text;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableSet;
+
+public class NewConfigurationTestIT extends SharedMiniClusterBase {
+
+  private static final Logger log = LoggerFactory.getLogger(NewConfigurationTestIT.class);
+
+  @Override
+  protected int defaultTimeoutSeconds() {
+    return 30;
+  }
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    SharedMiniClusterBase.startMiniCluster();
+  }
+
+  @AfterClass
+  public static void teardown() throws Exception {
+    SharedMiniClusterBase.stopMiniCluster();
+  }
+
+  /**
+   * Test that setting properties more than once overwrites the previous property settings.
+   */
+  @Test
+  public void testSetPropertiesOverwriteOlderProperties() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    Map<String,String> initialProps = new HashMap<>();
+    initialProps.put(""prop1"", ""val1"");
+    initialProps.put(""prop2"", ""val2"");
+    ntc.setProperties(initialProps);
+    // Create a new set of properties and set them with setProperties
+    Map<String,String> updatedProps = new HashMap<>();
+    updatedProps.put(""newerprop1"", ""newerval1"");
+    updatedProps.put(""newerprop2"", ""newerval2"");
+    ntc.setProperties(updatedProps);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,String> props = ntc.getProperties();
+    assertEquals(props.get(""newerprop1""), ""newerval1"");
+    assertEquals(props.get(""newerprop2""), ""newerval2"");
+    assertFalse(props.keySet().contains(""prop1""));
+    assertFalse(props.keySet().contains(""prop2""));
+  }
+
+  /**
+   * Verify that you cannot have overlapping locality groups.
+   *
+   * Attempt to set a locality group with overlapping groups. This test should throw an IllegalArgumentException indicating that groups overlap.
+   */
+  @Test(expected = IllegalArgumentException.class)
+  public void testOverlappingGroupsFail() throws AccumuloSecurityException, AccumuloException, TableExistsException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    Map<String,Set<Text>> lgroups = new HashMap<>();
+    lgroups.put(""lg1"", ImmutableSet.of(new Text(""colFamA""), new Text(""colFamB"")));
+    lgroups.put(""lg2"", ImmutableSet.of(new Text(""colFamC""), new Text(""colFamB"")));
+    ntc.setLocalityGroups(lgroups);
+    conn.tableOperations().create(tableName, ntc);
+  }
+
+  /**
+   * Test simplest case of setting locality groups at table creation.
+   */
+  @Test
+  public void testSimpleLocalityGroupCreation() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    // set locality groups map
+    Map<String,Set<Text>> lgroups = new HashMap<>();
+    lgroups.put(""lg1"", ImmutableSet.of(new Text(""dog""), new Text(""cat"")));
+    lgroups.put(""lg2"", ImmutableSet.of(new Text(""lion""), new Text(""tiger"")));
+    // set groups via NewTableConfiguration
+    ntc.setLocalityGroups(lgroups);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,Set<Text>> createdLocalityGroups = conn.tableOperations().getLocalityGroups(tableName);
+    assertEquals(2, createdLocalityGroups.size());
+    assertEquals(createdLocalityGroups.get(""lg1""), ImmutableSet.of(new Text(""dog""), new Text(""cat"")));
+    assertEquals(createdLocalityGroups.get(""lg2""), ImmutableSet.of(new Text(""lion""), new Text(""tiger"")));
+  }
+
+  /**
+   * Verify that setting locality groups more than once overwrite initial locality settings.
+   */
+  @Test
+  public void testMulitpleCallsToSetLocalityGroups() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    // set first locality groups map
+    Map<String,Set<Text>> initalGroup = new HashMap<>();
+    initalGroup.put(""lg1"", ImmutableSet.of(new Text(""dog""), new Text(""cat"")));
+    ntc.setLocalityGroups(initalGroup);
+    // set a second locality groups map and set in method call
+    Map<String,Set<Text>> secondGroup = new HashMap<>();
+    secondGroup.put(""lg1"", ImmutableSet.of(new Text(""blue""), new Text(""red"")));
+    ntc.setLocalityGroups(secondGroup);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,Set<Text>> createdLocalityGroups = conn.tableOperations().getLocalityGroups(tableName);
+    assertEquals(1, createdLocalityGroups.size());
+    assertEquals(createdLocalityGroups.get(""lg1""), ImmutableSet.of(new Text(""red""), new Text(""blue"")));
+  }
+
+  /**
+   * Verify that setting locality groups along with other properties works.
+   */
+  @Test
+  public void testSetPropertiesAndGroups() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+
+    Map<String,String> props = new HashMap<>();
+    props.put(""prop1"", ""val1"");
+    props.put(""prop2"", ""val2"");
+    ntc.setProperties(props);
+
+    Map<String,Set<Text>> lgroups = new HashMap<>();
+    lgroups.put(""lg1"", ImmutableSet.of(new Text(""dog"")));
+    ntc.setLocalityGroups(lgroups);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,String> ntcProps = ntc.getProperties();
+    assertEquals(ntcProps.get(""prop1""), ""val1"");
+    assertEquals(ntcProps.get(""prop2""), ""val2"");
+    assertEquals(ntcProps.get(""table.group.lg1""), ""dog"");
+    assertEquals(ntcProps.get(""table.groups.enabled""), ""lg1"");
+    Map<String,Set<Text>> createdLocalityGroups = conn.tableOperations().getLocalityGroups(tableName);
+    assertEquals(1, createdLocalityGroups.size());
+    assertEquals(createdLocalityGroups.get(""lg1""), ImmutableSet.of(new Text(""dog"")));
+  }
+
+  /**
+   * Create table with initial locality groups but no default iterators
+   */
+  @Test
+  public void testSetGroupsWithoutDefaultIterators() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration().withoutDefaultIterators();
+
+    Map<String,Set<Text>> lgroups = new HashMap<>();
+    lgroups.put(""lg1"", ImmutableSet.of(new Text(""colF"")));
+    ntc.setLocalityGroups(lgroups);
+    conn.tableOperations().create(tableName, ntc);
+    // verify groups and verify no iterators
+    Map<String,Set<Text>> createdLocalityGroups = conn.tableOperations().getLocalityGroups(tableName);
+    assertEquals(1, createdLocalityGroups.size());
+    assertEquals(createdLocalityGroups.get(""lg1""), ImmutableSet.of(new Text(""colF"")));
+    Map<String,EnumSet<IteratorScope>> iterators = conn.tableOperations().listIterators(tableName);
+    assertEquals(0, iterators.size());
+  }
+
+  /**
+   * Test pre-configuring iterator along with default iterator. Configure IteratorSetting values within method call.
+   */
+  @Test
+  public void testPreconfigureIteratorWithDefaultIterator1() throws AccumuloException, TableNotFoundException, AccumuloSecurityException, TableExistsException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    ntc.attachIterator(new IteratorSetting(10, ""anIterator"", ""it.class"", Collections.emptyMap()), EnumSet.of(IteratorScope.scan));
+    conn.tableOperations().create(tableName, ntc);
+
+    Map<String,EnumSet<IteratorScope>> iteratorList = conn.tableOperations().listIterators(tableName);
+    // should count the created iterator plus the default iterator
+    assertEquals(2, iteratorList.size());
+    check(conn, tableName, new String[] {""table.iterator.scan.anIterator=10,it.class""}, true);
+    conn.tableOperations().removeIterator(tableName, ""anIterator"", EnumSet.of(IteratorScope.scan));
+    check(conn, tableName, new String[] {}, true);
+    iteratorList = conn.tableOperations().listIterators(tableName);
+    assertEquals(1, iteratorList.size());
+  }
+
+  /**
+   * Test pre-configuring iterator with default iterator. Configure IteratorSetting values into method call.
+   */
+  @Test
+  public void testPreconfiguredIteratorWithDefaultIterator2() throws AccumuloException, TableNotFoundException, AccumuloSecurityException, TableExistsException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    IteratorSetting setting = new IteratorSetting(10, ""someName"", ""foo.bar"");
+    ntc.attachIterator(setting);
+    conn.tableOperations().create(tableName, ntc);
+
+    Map<String,EnumSet<IteratorScope>> iteratorList = conn.tableOperations().listIterators(tableName);
+    // should count the created iterator plus the default iterator
+    assertEquals(2, iteratorList.size());
+    check(conn, tableName, new String[] {""table.iterator.scan.someName=10,foo.bar""}, true);
+    conn.tableOperations().removeIterator(tableName, ""someName"", EnumSet.allOf((IteratorScope.class)));
+    check(conn, tableName, new String[] {}, true);
+    Map<String,EnumSet<IteratorScope>> iteratorList2 = conn.tableOperations().listIterators(tableName);
+    assertEquals(1, iteratorList2.size());
+  }
+
+  /**
+   * Test pre-configuring iterator with default iterator. Pass in IteratorScope value in method arguments.
+   */
+  @Test
+  public void testPreconfiguredIteratorWithDefaultIterator3() throws AccumuloException, TableNotFoundException, AccumuloSecurityException, TableExistsException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    IteratorSetting setting = new IteratorSetting(10, ""someName"", ""foo.bar"");
+    ntc.attachIterator(setting, EnumSet.of(IteratorScope.scan));
+    conn.tableOperations().create(tableName, ntc);
+
+    check(conn, tableName, new String[] {""table.iterator.scan.someName=10,foo.bar""}, true);
+    Map<String,EnumSet<IteratorScope>> iteratorList = conn.tableOperations().listIterators(tableName);
+    assertEquals(2, iteratorList.size());","[{'comment': 'could also check the EnumSet inthe value to ensure only scan scope is set.', 'commenter': 'keith-turner'}, {'comment': 'added extra check in testPreconfiguredIteratorWithDefaultIterator3', 'commenter': 'jmark99'}]"
337,core/src/main/java/org/apache/accumulo/core/client/admin/NewTableConfiguration.java,"@@ -146,4 +163,116 @@ public NewTableConfiguration enableSummarization(SummarizerConfiguration... conf
     summarizerProps = tmp;
     return this;
   }
+
+  /**
+   * Configures a table's locality groups prior to initial table creation.
+   *
+   * Allows locality groups to be set prior to table creation. Additional calls to this method prior to table creation will overwrite previous locality group
+   * mappings.
+   *
+   * @param groups
+   *          mapping of locality group names to column families in the locality group
+   *
+   * @since 2.0.0
+   */
+  public void setLocalityGroups(Map<String,Set<Text>> groups) {
+    // ensure locality groups do not overlap
+    LocalityGroupUtil.ensureNonOverlappingGroups(groups);
+    localityProps = new HashMap<>();
+    for (Entry<String,Set<Text>> entry : groups.entrySet()) {
+      Set<Text> colFams = entry.getValue();
+      String value = LocalityGroupUtil.encodeColumnFamilies(colFams);
+      localityProps.put(Property.TABLE_LOCALITY_GROUP_PREFIX + entry.getKey(), value);
+    }
+    // localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), Joiner.on("","").join(groups.keySet()));
+    localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), groups.keySet().stream().collect(Collectors.joining("","")));
+    // localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), Stream.of(groups.keySet().collect(Collectors.joining("",""));
+    // Stream.of(groups.keySet()).collect(joining("",""));
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * Additional calls to this method before table creation will overwrite previous iterator settings.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @throws AccumuloSecurityException
+   *           thrown if the user does not have the ability to set properties on the table
+   * @throws AccumuloException
+   *           if a general error occurs
+   * @throws TableNotFoundException","[{'comment': 'does not seem like this exception should be thrown', 'commenter': 'keith-turner'}, {'comment': 'You are correct. It initially had been propagated up from a utility method. Forgot to remove when that utility method was modified.', 'commenter': 'jmark99'}]"
337,test/src/main/java/org/apache/accumulo/test/NewConfigurationTestIT.java,"@@ -0,0 +1,473 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.test;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+
+import java.util.Collections;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.Connector;
+import org.apache.accumulo.core.client.IteratorSetting;
+import org.apache.accumulo.core.client.TableExistsException;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.admin.NewTableConfiguration;
+import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;
+import org.apache.accumulo.harness.SharedMiniClusterBase;
+import org.apache.hadoop.io.Text;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableSet;
+
+public class NewConfigurationTestIT extends SharedMiniClusterBase {
+
+  private static final Logger log = LoggerFactory.getLogger(NewConfigurationTestIT.class);
+
+  @Override
+  protected int defaultTimeoutSeconds() {
+    return 30;
+  }
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    SharedMiniClusterBase.startMiniCluster();
+  }
+
+  @AfterClass
+  public static void teardown() throws Exception {
+    SharedMiniClusterBase.stopMiniCluster();
+  }
+
+  /**
+   * Test that setting properties more than once overwrites the previous property settings.
+   */
+  @Test
+  public void testSetPropertiesOverwriteOlderProperties() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    Map<String,String> initialProps = new HashMap<>();
+    initialProps.put(""prop1"", ""val1"");
+    initialProps.put(""prop2"", ""val2"");
+    ntc.setProperties(initialProps);
+    // Create a new set of properties and set them with setProperties
+    Map<String,String> updatedProps = new HashMap<>();
+    updatedProps.put(""newerprop1"", ""newerval1"");
+    updatedProps.put(""newerprop2"", ""newerval2"");
+    ntc.setProperties(updatedProps);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,String> props = ntc.getProperties();
+    assertEquals(props.get(""newerprop1""), ""newerval1"");
+    assertEquals(props.get(""newerprop2""), ""newerval2"");
+    assertFalse(props.keySet().contains(""prop1""));
+    assertFalse(props.keySet().contains(""prop2""));
+  }
+
+  /**
+   * Verify that you cannot have overlapping locality groups.
+   *
+   * Attempt to set a locality group with overlapping groups. This test should throw an IllegalArgumentException indicating that groups overlap.
+   */
+  @Test(expected = IllegalArgumentException.class)
+  public void testOverlappingGroupsFail() throws AccumuloSecurityException, AccumuloException, TableExistsException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    Map<String,Set<Text>> lgroups = new HashMap<>();
+    lgroups.put(""lg1"", ImmutableSet.of(new Text(""colFamA""), new Text(""colFamB"")));
+    lgroups.put(""lg2"", ImmutableSet.of(new Text(""colFamC""), new Text(""colFamB"")));
+    ntc.setLocalityGroups(lgroups);
+    conn.tableOperations().create(tableName, ntc);","[{'comment': 'can this line be omitted?', 'commenter': 'keith-turner'}, {'comment': 'deleted.', 'commenter': 'jmark99'}]"
337,test/src/main/java/org/apache/accumulo/test/NewConfigurationTestIT.java,"@@ -0,0 +1,473 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.test;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+
+import java.util.Collections;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.Connector;
+import org.apache.accumulo.core.client.IteratorSetting;
+import org.apache.accumulo.core.client.TableExistsException;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.admin.NewTableConfiguration;
+import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;
+import org.apache.accumulo.harness.SharedMiniClusterBase;
+import org.apache.hadoop.io.Text;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableSet;
+
+public class NewConfigurationTestIT extends SharedMiniClusterBase {
+
+  private static final Logger log = LoggerFactory.getLogger(NewConfigurationTestIT.class);
+
+  @Override
+  protected int defaultTimeoutSeconds() {
+    return 30;
+  }
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    SharedMiniClusterBase.startMiniCluster();
+  }
+
+  @AfterClass
+  public static void teardown() throws Exception {
+    SharedMiniClusterBase.stopMiniCluster();
+  }
+
+  /**
+   * Test that setting properties more than once overwrites the previous property settings.
+   */
+  @Test
+  public void testSetPropertiesOverwriteOlderProperties() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    Map<String,String> initialProps = new HashMap<>();
+    initialProps.put(""prop1"", ""val1"");
+    initialProps.put(""prop2"", ""val2"");
+    ntc.setProperties(initialProps);
+    // Create a new set of properties and set them with setProperties
+    Map<String,String> updatedProps = new HashMap<>();
+    updatedProps.put(""newerprop1"", ""newerval1"");
+    updatedProps.put(""newerprop2"", ""newerval2"");
+    ntc.setProperties(updatedProps);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,String> props = ntc.getProperties();
+    assertEquals(props.get(""newerprop1""), ""newerval1"");
+    assertEquals(props.get(""newerprop2""), ""newerval2"");
+    assertFalse(props.keySet().contains(""prop1""));
+    assertFalse(props.keySet().contains(""prop2""));
+  }
+
+  /**
+   * Verify that you cannot have overlapping locality groups.
+   *
+   * Attempt to set a locality group with overlapping groups. This test should throw an IllegalArgumentException indicating that groups overlap.
+   */
+  @Test(expected = IllegalArgumentException.class)
+  public void testOverlappingGroupsFail() throws AccumuloSecurityException, AccumuloException, TableExistsException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    Map<String,Set<Text>> lgroups = new HashMap<>();
+    lgroups.put(""lg1"", ImmutableSet.of(new Text(""colFamA""), new Text(""colFamB"")));
+    lgroups.put(""lg2"", ImmutableSet.of(new Text(""colFamC""), new Text(""colFamB"")));
+    ntc.setLocalityGroups(lgroups);
+    conn.tableOperations().create(tableName, ntc);
+  }
+
+  /**
+   * Test simplest case of setting locality groups at table creation.
+   */
+  @Test
+  public void testSimpleLocalityGroupCreation() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    // set locality groups map
+    Map<String,Set<Text>> lgroups = new HashMap<>();
+    lgroups.put(""lg1"", ImmutableSet.of(new Text(""dog""), new Text(""cat"")));
+    lgroups.put(""lg2"", ImmutableSet.of(new Text(""lion""), new Text(""tiger"")));
+    // set groups via NewTableConfiguration
+    ntc.setLocalityGroups(lgroups);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,Set<Text>> createdLocalityGroups = conn.tableOperations().getLocalityGroups(tableName);
+    assertEquals(2, createdLocalityGroups.size());
+    assertEquals(createdLocalityGroups.get(""lg1""), ImmutableSet.of(new Text(""dog""), new Text(""cat"")));
+    assertEquals(createdLocalityGroups.get(""lg2""), ImmutableSet.of(new Text(""lion""), new Text(""tiger"")));
+  }
+
+  /**
+   * Verify that setting locality groups more than once overwrite initial locality settings.
+   */
+  @Test
+  public void testMulitpleCallsToSetLocalityGroups() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    // set first locality groups map
+    Map<String,Set<Text>> initalGroup = new HashMap<>();
+    initalGroup.put(""lg1"", ImmutableSet.of(new Text(""dog""), new Text(""cat"")));
+    ntc.setLocalityGroups(initalGroup);
+    // set a second locality groups map and set in method call
+    Map<String,Set<Text>> secondGroup = new HashMap<>();
+    secondGroup.put(""lg1"", ImmutableSet.of(new Text(""blue""), new Text(""red"")));
+    ntc.setLocalityGroups(secondGroup);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,Set<Text>> createdLocalityGroups = conn.tableOperations().getLocalityGroups(tableName);
+    assertEquals(1, createdLocalityGroups.size());
+    assertEquals(createdLocalityGroups.get(""lg1""), ImmutableSet.of(new Text(""red""), new Text(""blue"")));
+  }
+
+  /**
+   * Verify that setting locality groups along with other properties works.
+   */
+  @Test
+  public void testSetPropertiesAndGroups() throws AccumuloSecurityException, AccumuloException, TableExistsException, TableNotFoundException {
+    Connector conn = getConnector();
+    String tableName = getUniqueNames(2)[0];
+    NewTableConfiguration ntc = new NewTableConfiguration();
+
+    Map<String,String> props = new HashMap<>();
+    props.put(""prop1"", ""val1"");
+    props.put(""prop2"", ""val2"");
+    ntc.setProperties(props);
+
+    Map<String,Set<Text>> lgroups = new HashMap<>();
+    lgroups.put(""lg1"", ImmutableSet.of(new Text(""dog"")));
+    ntc.setLocalityGroups(lgroups);
+    conn.tableOperations().create(tableName, ntc);
+    // verify
+    Map<String,String> ntcProps = ntc.getProperties();","[{'comment': 'why not get the props from the table?', 'commenter': 'keith-turner'}, {'comment': 'updated to use tableOperations.getProperties', 'commenter': 'jmark99'}]"
337,test/src/main/resources/log4j.properties,"@@ -13,7 +13,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-log4j.rootLogger=DEBUG, CA
+log4j.rootLogger=ERROR, CA","[{'comment': 'Is this necessary for the test? We typically need DEBUG logs when a test fails, to figure out what happened.', 'commenter': 'ctubbsii'}, {'comment': 'No. I had changed it to reduce some output at one point and overlooked it when committing. I reverted back to DEBUG ', 'commenter': 'jmark99'}]"
337,core/src/main/java/org/apache/accumulo/core/util/LocalityGroupUtil.java,"@@ -340,4 +340,14 @@ public static void seek(FileSKVIterator reader, Range range, String lgName, Map<
 
     reader.seek(range, families, inclusive);
   }
+
+  static public void ensureNonOverlappingGroups(Map<String,Set<Text>> groups) {
+    HashSet<Text> all = new HashSet<>();
+    for (Entry<String,Set<Text>> entry : groups.entrySet()) {
+      if (!Collections.disjoint(all, entry.getValue())) {
+        throw new IllegalArgumentException(""Group "" + entry.getKey() + "" overlaps with another group"");
+      }
+      all.addAll(entry.getValue());
+    }
+  }","[{'comment': ""This location would make this public API... but I'm not sure we want to do that for an internal utility method."", 'commenter': 'ctubbsii'}]"
337,core/src/main/java/org/apache/accumulo/core/client/admin/NewTableConfiguration.java,"@@ -146,4 +161,75 @@ public NewTableConfiguration enableSummarization(SummarizerConfiguration... conf
     summarizerProps = tmp;
     return this;
   }
+
+  /**
+   * Configures a table's locality groups prior to initial table creation.
+   *
+   * Allows locality groups to be set prior to table creation. Additional calls to this method prior to table creation will overwrite previous locality group
+   * mappings.
+   *
+   * @param groups
+   *          mapping of locality group names to column families in the locality group
+   *
+   * @since 2.0.0
+   *
+   * @see TableOperations#setLocalityGroups
+   */
+  public NewTableConfiguration setLocalityGroups(Map<String,Set<Text>> groups) {
+    // ensure locality groups do not overlap
+    LocalityGroupUtil.ensureNonOverlappingGroups(groups);
+    localityProps = new HashMap<>();
+    for (Entry<String,Set<Text>> entry : groups.entrySet()) {
+      Set<Text> colFams = entry.getValue();
+      String value = LocalityGroupUtil.encodeColumnFamilies(colFams);
+      localityProps.put(Property.TABLE_LOCALITY_GROUP_PREFIX + entry.getKey(), value);
+    }
+    localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), groups.keySet().stream().collect(Collectors.joining("","")));
+    return this;
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * Additional calls to this method before table creation will overwrite previous iterator settings.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @throws AccumuloException
+   *           if a general error occurs
+   *
+   * @since 2.0.0
+   */
+  public NewTableConfiguration attachIterator(IteratorSetting setting) throws AccumuloException {
+    return attachIterator(setting, EnumSet.allOf(IteratorScope.class));
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @param scopes
+   *          enumerated set of iterator scopes
+   * @throws AccumuloException
+   *           if a general error occurs
+   *
+   * @since 2.0.0
+   */
+  public NewTableConfiguration attachIterator(IteratorSetting setting, EnumSet<IteratorScope> scopes) throws AccumuloException {
+    checkArgument(setting != null, ""setting is null"");
+    checkArgument(scopes != null, ""scopes is null"");","[{'comment': 'Would `Objects.requiresNonNull()` be better here?', 'commenter': 'ctubbsii'}, {'comment': 'Yes. Like that better. Perhaps should update the TableOperationsHelper attachIterator version as well.', 'commenter': 'jmark99'}]"
337,core/src/main/java/org/apache/accumulo/core/client/admin/NewTableConfiguration.java,"@@ -146,4 +161,75 @@ public NewTableConfiguration enableSummarization(SummarizerConfiguration... conf
     summarizerProps = tmp;
     return this;
   }
+
+  /**
+   * Configures a table's locality groups prior to initial table creation.
+   *
+   * Allows locality groups to be set prior to table creation. Additional calls to this method prior to table creation will overwrite previous locality group
+   * mappings.
+   *
+   * @param groups
+   *          mapping of locality group names to column families in the locality group
+   *
+   * @since 2.0.0
+   *
+   * @see TableOperations#setLocalityGroups
+   */
+  public NewTableConfiguration setLocalityGroups(Map<String,Set<Text>> groups) {
+    // ensure locality groups do not overlap
+    LocalityGroupUtil.ensureNonOverlappingGroups(groups);
+    localityProps = new HashMap<>();
+    for (Entry<String,Set<Text>> entry : groups.entrySet()) {
+      Set<Text> colFams = entry.getValue();
+      String value = LocalityGroupUtil.encodeColumnFamilies(colFams);
+      localityProps.put(Property.TABLE_LOCALITY_GROUP_PREFIX + entry.getKey(), value);
+    }
+    localityProps.put(Property.TABLE_LOCALITY_GROUPS.getKey(), groups.keySet().stream().collect(Collectors.joining("","")));
+    return this;
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * Additional calls to this method before table creation will overwrite previous iterator settings.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @throws AccumuloException
+   *           if a general error occurs
+   *
+   * @since 2.0.0
+   */
+  public NewTableConfiguration attachIterator(IteratorSetting setting) throws AccumuloException {
+    return attachIterator(setting, EnumSet.allOf(IteratorScope.class));
+  }
+
+  /**
+   * Configure iterator settings for a table prior to its creation.
+   *
+   * @param setting
+   *          object specifying the properties of the iterator
+   * @param scopes
+   *          enumerated set of iterator scopes
+   * @throws AccumuloException
+   *           if a general error occurs
+   *
+   * @since 2.0.0
+   */
+  public NewTableConfiguration attachIterator(IteratorSetting setting, EnumSet<IteratorScope> scopes) throws AccumuloException {
+    checkArgument(setting != null, ""setting is null"");
+    checkArgument(scopes != null, ""scopes is null"");
+    if (iteratorProps.isEmpty())
+      iteratorProps = new HashMap<>();","[{'comment': 'If it\'s already empty, then there\'s no point in allocating a new empty one, assuming the previously allocated empty one can be updated. If not, we shouldn\'t use that empty one as a placeholder for ""unset"".', 'commenter': 'ctubbsii'}]"
339,test/src/main/java/org/apache/accumulo/test/replication/StatusMakerIT.java,"@@ -246,4 +253,97 @@ public void closedMessagesCreateOrderRecords() throws Exception {
     Assert.assertFalse(""Found more files unexpectedly"", expectedFiles.hasNext());
     Assert.assertFalse(""Found more entries in replication table unexpectedly"", iter.hasNext());
   }
+
+  @Test
+  public void orderRecordsCreatedWithNoCreatedTime() throws Exception {
+    String sourceTable = testName.getMethodName();
+    conn.tableOperations().create(sourceTable);
+    ReplicationTableUtil.configureMetadataTable(conn, sourceTable);
+
+    BatchWriter bw = conn.createBatchWriter(sourceTable, new BatchWriterConfig());
+    String walPrefix = ""hdfs://localhost:8020/accumulo/wals/tserver+port/"";
+    List<String> files = Arrays.asList(walPrefix + UUID.randomUUID(), walPrefix + UUID.randomUUID(), walPrefix + UUID.randomUUID(),
+        walPrefix + UUID.randomUUID());
+    Map<String,Integer> fileToTableId = new HashMap<>();
+
+    Status.Builder statBuilder = Status.newBuilder().setBegin(0).setEnd(0).setInfiniteEnd(true).setClosed(true);
+
+    Map<String,Long> statuses = new HashMap<>();
+    int index = 1;","[{'comment': 'Just make this a `long` and change the `Integer.toString()` to `Long.toString()`?', 'commenter': 'joshelser'}]"
339,server/master/src/main/java/org/apache/accumulo/master/replication/StatusMaker.java,"@@ -197,8 +202,18 @@ protected boolean addStatusRecord(Text file, String tableId, Value v) {
   protected boolean addOrderRecord(Text file, String tableId, Status stat, Value value) {
     try {
       if (!stat.hasCreatedTime()) {
-        log.error(""Status record ({}) for {} in table {} was written to metadata table which lacked createdTime"", ProtobufUtil.toString(stat), file, tableId);
-        return false;
+        try {
+          long createdTime = setAndGetCreatedTime(new Path(file.toString()), tableId);","[{'comment': 'Add a comment here alluding to the problem and this workaround, please? ', 'commenter': 'joshelser'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -66,18 +68,44 @@
    */
   static final int VALUE_SIZE_COPY_CUTOFF = 1 << 15;
 
+  /**
+   * Maximum size of a mutation (2GB).
+   */
+  static final long MAX_MUTATION_SIZE = 2l * (1 << 30);","[{'comment': ""Minor nit: prefer uppercase L for long literals. It's easier to read, especially for programmers with poor eyesight.\r\n\r\nAlso, `1L << 31` is simpler."", 'commenter': 'ctubbsii'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -66,18 +68,44 @@
    */
   static final int VALUE_SIZE_COPY_CUTOFF = 1 << 15;
 
+  /**
+   * Maximum size of a mutation (2GB).
+   */
+  static final long MAX_MUTATION_SIZE = 2l * (1 << 30);
+
   /**
    * Formats available for serializing Mutations. The formats are described in a <a href=""doc-files/mutation-serialization.html"">separate document</a>.
    */
   public static enum SERIALIZED_FORMAT {
     VERSION1, VERSION2
   }
 
+  final class MutationSize {
+    private int entries = 0;
+    private long sizeInBytes = 0;
+
+    private MutationSize() {}
+
+    private MutationSize(long rowLength) {
+      sizeInBytes = rowLength;
+    }
+
+    private MutationSize(int entries, long sizeInBytes) {
+      this.entries = entries;
+      this.sizeInBytes = sizeInBytes;
+    }
+
+    void update(long entrySize) {","[{'comment': '`update` could be renamed to `addEntryWithSize` or something else more descriptive.', 'commenter': 'ctubbsii'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -66,18 +68,44 @@
    */
   static final int VALUE_SIZE_COPY_CUTOFF = 1 << 15;
 
+  /**
+   * Maximum size of a mutation (2GB).
+   */
+  static final long MAX_MUTATION_SIZE = 2l * (1 << 30);
+
   /**
    * Formats available for serializing Mutations. The formats are described in a <a href=""doc-files/mutation-serialization.html"">separate document</a>.
    */
   public static enum SERIALIZED_FORMAT {
     VERSION1, VERSION2
   }
 
+  final class MutationSize {
+    private int entries = 0;","[{'comment': '`entries` should be `numEntries` instead, just to add a bit more clarity.', 'commenter': 'ctubbsii'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -256,7 +285,7 @@ public Mutation(Mutation m) {
     m.serialize();
     this.row = m.row;
     this.data = m.data;
-    this.entries = m.entries;
+    this.size = new MutationSize(m.size.entries, m.size.sizeInBytes);","[{'comment': 'maybe make a copy constructor for this case?', 'commenter': 'ctubbsii'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -306,6 +340,9 @@ private void put(byte[] cf, int cfLength, byte[] cq, int cqLength, byte[] cv, bo
     if (buffer == null) {
       throw new IllegalStateException(""Can not add to mutation after serializing it"");
     }
+    long length = cfLength + cqLength + valLength;","[{'comment': 'This does not capture the complete picture.  When data is serialized, the length is written in addition to the data.  For example if a 20 byte family is written it may actually write 21 bytes including the length.  The length is written as a variable length long, so its hard to know how many bytes it will take beforehand.  Because this hard to know beforehand, I would suggest just doing the worst case and assuming 5 bytes.  \r\n\r\nAlso could take the strategy of only tracking the length of long values and asking the buffer for the size of everything else.  This way an accurate size for what is already serialized is obtained and we only need to estimated what is about to be serialized. \r\n\r\n```java\r\n  long estimatedNewSize = (cfLength + cqLength + valLength + cv.lenght() + 4* 5 + 1 + (hasTs ? 8 : 0)) + buffer.currentSize() + valueLenghts;\r\n  //sanity check estimated new size\r\n\r\n// if large value, then increment valueLenghts\r\n```', 'commenter': 'keith-turner'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -306,6 +340,9 @@ private void put(byte[] cf, int cfLength, byte[] cq, int cqLength, byte[] cv, bo
     if (buffer == null) {","[{'comment': 'After a mutation is serialized, put methods can not be called.  Therefore I do not think the serialization needs to change to transfer the total length information.  Also modifying the serialization may have far reaching implications as mutations are serialized in write ahead logs and replication data.', 'commenter': 'keith-turner'}, {'comment': 'Wanted to maintain only one size attribute without recalculating size after serialization. Now I try another way.', 'commenter': 'ghajos'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -277,6 +288,7 @@ private void put(byte b[]) {
   private void put(byte b[], int length) {
     buffer.writeVLong(length);
     buffer.add(b, 0, length);
+    estimatedSize += length + SERIALIZATION_OVERHEAD;","[{'comment': 'seems like this will double count some data', 'commenter': 'keith-turner'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -169,6 +179,7 @@ public Mutation(byte[] row, int start, int length, int initialBufferSize) {
     this.row = new byte[length];
     System.arraycopy(row, start, this.row, 0, length);
     buffer = new UnsynchronizedBuffer.Writer(initialBufferSize);
+    estimatedSize = length + SERIALIZATION_OVERHEAD;","[{'comment': 'Nice, included the row.', 'commenter': 'keith-turner'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -306,6 +318,8 @@ private void put(byte[] cf, int cfLength, byte[] cq, int cqLength, byte[] cv, bo
     if (buffer == null) {
       throw new IllegalStateException(""Can not add to mutation after serializing it"");
     }
+    estimatedSize += cfLength + cqLength + (hasts ? 8 : 0) + valLength + 2 * 1 + 4 * SERIALIZATION_OVERHEAD;
+    Preconditions.checkArgument(estimatedSize < MAX_MUTATION_SIZE && estimatedSize >= 0, ""Maximum mutation size must be less than 2GB "");","[{'comment': 'I was thinking of a slightly different approach that will give more accurate size estimates.  Something like the following.\r\n\r\n * Use estimatedSize to track row and large values only.\r\n * Add size() method to `UnsynchronizedBuffer.Writer` that returns its offset.  Then can call `buffer.size()` to get an accurate count of amount of data written.\r\n\r\nThen this check could be something like\r\n\r\n```java\r\n  long estSizeAfterPut = estimatedSize + buffer.size() + cfLength + cqLength + (hasts ? 8 : 0) + valLength + 2 * 1 + 4 * SERIALIZATION_OVERHEAD;\r\nPreconditions.checkArgument(estSizeAfterPut < MAX_MUTATION_SIZE && estSizeAfterPut >= 0, ""Maximum mutation size must be less than 2GB "");\r\n```', 'commenter': 'keith-turner'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -87,7 +87,8 @@
   private byte[] data;
   private int entries;
   private List<byte[]> values;
-  long estimatedSize = 0;
+  // tracks estimated size of row.length + largeValues.length
+  long estRowAndLargeValSize = 0;","[{'comment': 'could add `@VisibleForTesting` annotation here.', 'commenter': 'keith-turner'}]"
373,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -318,8 +318,8 @@ private void put(byte[] cf, int cfLength, byte[] cq, int cqLength, byte[] cv, bo
     if (buffer == null) {
       throw new IllegalStateException(""Can not add to mutation after serializing it"");
     }
-    estimatedSize += cfLength + cqLength + (hasts ? 8 : 0) + valLength + 2 * 1 + 4 * SERIALIZATION_OVERHEAD;
-    Preconditions.checkArgument(estimatedSize < MAX_MUTATION_SIZE && estimatedSize >= 0, ""Maximum mutation size must be less than 2GB "");
+    long estimatedSizeAfterPut = estRowAndLargeValSize + buffer.size() + cfLength + cqLength + (hasts ? 8 : 0) + valLength + 2 * 1 + 4 * SERIALIZATION_OVERHEAD;","[{'comment': 'Should include `cv.length` in the sum.  \r\n\r\nIs `2*1` for the two booleans (deleted and hasts)?   This could be simplified to `2`.', 'commenter': 'keith-turner'}]"
403,assemble/pom.xml,"@@ -274,6 +274,15 @@
           </execution>
         </executions>
       </plugin>
+      <plugin>
+        <groupId>org.apache.rat</groupId>
+        <artifactId>apache-rat-plugin</artifactId>
+        <configuration>
+          <excludes>
+            <exclude>conf/templates/grafana-dashboard.json</exclude>","[{'comment': ""This isn't needed anymore."", 'commenter': 'ctubbsii'}, {'comment': 'It has been removed.', 'commenter': 'mikewalch'}]"
410,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthorizor.java,"@@ -163,11 +163,19 @@ public void changeAuthorizations(String user, Authorizations authorizations) thr
 
   @Override
   public boolean isValidAuthorizations(String user, List<ByteBuffer> auths) throws AccumuloSecurityException {
-    Collection<ByteBuffer> userauths = getCachedUserAuthorizations(user).getAuthorizationsBB();
-    for (ByteBuffer auth : auths)
-      if (!userauths.contains(auth))
+    if (auths.size() == 0) {","[{'comment': 'You should call `.isEmpty()`', 'commenter': 'ctubbsii'}]"
424,server/monitor/src/main/java/org/apache/accumulo/monitor/util/Table.java,"@@ -110,7 +110,7 @@ public synchronized void generate(HttpServletRequest req, StringBuilder sb) {
       if (row.size() != columns.size())
         throw new RuntimeException(""Each row must have the same number of columns"");
 
-    boolean sortAscending = !""false"".equals(BasicServlet.getCookieValue(req, ""tableSort.""
+    boolean sortDescending = ""false"".equals(BasicServlet.getCookieValue(req, ""tableSort.""","[{'comment': 'I tested your changes on localhost using Uno and the sorting isn\'t working correctly for me.  I believe the solution is actually simpler than the changes you proposed.  You should keep the sortAscending variable (since the cookie and http request still use ""asc"") and only change the initial assingment.  I think this will work:\r\n\r\n`boolean sortAscending = ""true"".equals(BasicServlet.getCookieValue(req, ""tableSort.""\r\n        + BasicServlet.encode(page) + ""."" + BasicServlet.encode(tableName) + ""."" + ""sortAsc""));`\r\n\r\nThis should work since the initial value of the cookie is null,the first pass will produce sortAscending = false. Then it should store the value correctly whatever it is from then on.', 'commenter': 'milleruntime'}]"
444,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectWriteAheadLogs.java,"@@ -301,9 +324,8 @@ private UUID path2uuid(Path path) {
           // There's a reference to a log file, so skip that server's logs
           Set<UUID> idsToIgnore = candidates.remove(dead);
           if (idsToIgnore != null) {
-            for (UUID id : idsToIgnore) {
-              result.remove(id);
-            }
+            result.keySet().removeAll(idsToIgnore);
+            recoveryLogs.keySet().removeAll(idsToIgnore);","[{'comment': 'Will removing from the keyset remove it from the underlying map?', 'commenter': 'ctubbsii'}, {'comment': 'yes', 'commenter': 'keith-turner'}]"
444,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectWriteAheadLogs.java,"@@ -199,6 +203,9 @@ public void collect(GCStatus status) {
       long removeStop = System.currentTimeMillis();
       log.info(String.format(""%d total logs removed from %d servers in %.2f seconds"", count,
           logsByServer.size(), (removeStop - logEntryScanStop) / 1000.));
+
+      count = removeFiles(recoveryLogs.values());","[{'comment': 'Should this be `count += `?', 'commenter': 'ctubbsii'}, {'comment': 'no, its used for the following log message about the number of recovery logs removed', 'commenter': 'keith-turner'}]"
444,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectWriteAheadLogs.java,"@@ -244,36 +251,53 @@ private long removeTabletServerMarkers(Map<UUID,TServerInstance> uidMap,
     return result;
   }
 
+  private long removeFile(Path path) {
+    try {
+      if (!useTrash || !fs.moveToTrash(path)) {
+        fs.deleteRecursively(path);
+      }
+      return 1;","[{'comment': 'Recursive delete counts as 1 removed?', 'commenter': 'ctubbsii'}, {'comment': 'yes, because a dir with mulitple files represents one WAL', 'commenter': 'keith-turner'}]"
444,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectWriteAheadLogs.java,"@@ -244,36 +251,53 @@ private long removeTabletServerMarkers(Map<UUID,TServerInstance> uidMap,
     return result;
   }
 
+  private long removeFile(Path path) {
+    try {
+      if (!useTrash || !fs.moveToTrash(path)) {
+        fs.deleteRecursively(path);
+      }
+      return 1;
+    } catch (FileNotFoundException ex) {
+      // ignored","[{'comment': ""If this can't happen, it should probably throw an AssertionError. Can this normally happen?\r\n\r\nCould also consider adding a debug or trace log here, whether or not it's normal, because it could indicate problems (instead of throwing an AssertionError)."", 'commenter': 'ctubbsii'}, {'comment': 'I think logging a debug message would make sense', 'commenter': 'keith-turner'}]"
446,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -106,7 +113,9 @@ public String description() {
   public Options getOptions() {
     final Options opts = super.getOptions();
     numThreadsOpt = new Option(""nt"", ""num-threads"", true, ""number of threads to use"");
+    negateOpt = new Option(""v"", ""negate"", false, ""only include rows without search term"");","[{'comment': '`-v` is typically reserved for `--verbose`; I think it could be confusing to use `-v` for this option.', 'commenter': 'ctubbsii'}, {'comment': 'I purposely choose v because it is the negate option to grep in bash. I could still change it if you want but anyone that uses grep in bash would be familiar with this', 'commenter': 'alerman'}, {'comment': ""Hmm, that's a good point. Okay, you've convinced me."", 'commenter': 'ctubbsii'}]"
528,core/src/main/java/org/apache/accumulo/core/client/impl/ClientContext.java,"@@ -50,11 +49,11 @@
  */
 public class ClientContext {
 
-  protected final Instance inst;
+  private ClientInfo info;
+  protected Instance inst;","[{'comment': 'Should this be protected, or should we add a getter?', 'commenter': 'ctubbsii'}, {'comment': 'There is already a getter.  It was protected because it is used by `AccumuloServerContext` which uses it with `instanceof`.', 'commenter': 'mikewalch'}]"
546,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -340,6 +340,7 @@
   MASTER_REPLICATION_COORDINATOR_THREADCHECK(""master.replication.coordinator.threadcheck.time"",
       ""5s"", PropertyType.TIMEDURATION,
       ""The time between adjustments of the coordinator thread pool""),
+  @Deprecated","[{'comment': ""Description should be updated to reflect when it was deprecated. I noticed while reviewing deprecated items that we hadn't been doing this for configuration properties, and it's very useful."", 'commenter': 'ctubbsii'}]"
546,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1147,11 +1149,16 @@ private long balanceTablets() {
   private SortedMap<TServerInstance,TabletServerStatus> gatherTableInformation(
       Set<TServerInstance> currentServers) {
     long start = System.currentTimeMillis();
-    int threads = Math.max(getConfiguration().getCount(Property.MASTER_STATUS_THREAD_POOL_SIZE), 1);
-    ExecutorService tp = Executors.newFixedThreadPool(threads);
-    final SortedMap<TServerInstance,TabletServerStatus> result = new TreeMap<>();
+    ExecutorService tp = Executors.newCachedThreadPool();
+    // use ConcurrentSkipListMap for two reasons. First, multiple threads may concurrently put.
+    // Second, its ok for one thread to iterate over map entries while another thread puts.
+    final SortedMap<TServerInstance,TabletServerStatus> result = new ConcurrentSkipListMap<>();
     for (TServerInstance serverInstance : currentServers) {
       final TServerInstance server = serverInstance;
+      // Since an unbounded thread pool is being used, rate limit how fast task are added to the
+      // executor. This prevents the threads from growing large unless there are lots of
+      // unresponsive tservers.
+      sleepUninterruptibly(5, TimeUnit.MILLISECONDS);","[{'comment': 'I think maybe this should be some fraction of the client timeout, because the client timeout indicates some user awareness of how long the RPCs should take.', 'commenter': 'ctubbsii'}]"
546,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1196,13 +1203,21 @@ public void run() {
     } catch (InterruptedException e) {
       log.debug(""Interrupted while fetching status"");
     }
+
+    tp.shutdownNow();
+
+    // Because result is a ConcurrentSkipListMap will not see a concurrent modification exception,
+    // even though background threads may still try to put.
+    SortedMap<TServerInstance,TabletServerStatus> info = ImmutableSortedMap.copyOf(result);","[{'comment': ""If something isn't in this map by the time we get here, we could just add the remaining directly to the badServers."", 'commenter': 'ctubbsii'}, {'comment': 'The bad server processing needs some attention, I will open up a follow on issue.', 'commenter': 'keith-turner'}]"
546,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -340,6 +340,10 @@
   MASTER_REPLICATION_COORDINATOR_THREADCHECK(""master.replication.coordinator.threadcheck.time"",
       ""5s"", PropertyType.TIMEDURATION,
       ""The time between adjustments of the coordinator thread pool""),
+  /**
+   * @deprecated since 1.9.1","[{'comment': ""I actually meant in the text in the property description. The javadoc comment here won't really be useful. Also, it should be 1.9.2."", 'commenter': 'ctubbsii'}, {'comment': 'oh yeah, should be be 1.9.2.  Not exactly sure what you are looking for, can you give a code example?', 'commenter': 'keith-turner'}, {'comment': 'See line 348 below. `s/The number of threads/Deprecated since 1.9.2. The number of threads/`\r\nThe `@Deprecated` annotation is sufficient for devs, but these String descriptions get published in our docs for users, where they are useful for both users and devs.', 'commenter': 'ctubbsii'}, {'comment': 'I updated the docs.   I looked into the the code that generates prop docs and it generates documentation for deprecated props.  So users looking at the docs will know which props are deprecated, but they will not know when.  If [ACCUMULO-4592](https://issues.apache.org/jira/browse/ACCUMULO-4592) were done, could also add a deprecated version.', 'commenter': 'keith-turner'}]"
546,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1146,12 +1148,20 @@ private long balanceTablets() {
 
   private SortedMap<TServerInstance,TabletServerStatus> gatherTableInformation(
       Set<TServerInstance> currentServers) {
+    final long rpcTimeout = getConfiguration().getTimeInMillis(Property.GENERAL_RPC_TIMEOUT);
+    int threads = Math.max(getConfiguration().getCount(Property.MASTER_STATUS_THREAD_POOL_SIZE), 0);","[{'comment': 'This line seems to imply that negative values will result in unlimited threads. However, the documentation only describes zero threads behaving this way. The docs should say ""non-positive"" or ""zero and negative"" just to leave no ambiguity.', 'commenter': 'ctubbsii'}, {'comment': 'That was not my intent.  I will removed the max, then the following line would fail if negative.', 'commenter': 'keith-turner'}]"
615,pom.xml,"@@ -793,22 +793,30 @@
           </configuration>
         </plugin>
         <plugin>
-          <groupId>org.codehaus.mojo</groupId>
-          <artifactId>findbugs-maven-plugin</artifactId>
+          <groupId>com.github.spotbugs</groupId>
+          <artifactId>spotbugs</artifactId>
           <version>${findbugs.version}</version>
           <configuration>
             <xmlOutput>true</xmlOutput>
             <effort>Max</effort>
             <failOnError>true</failOnError>
             <includeTests>true</includeTests>
             <maxRank>16</maxRank>
+            <!-- findbugs plugin for webapp security bugs -->
+            <includeFilterFile>${session.executionRootDirectory}/spotbugs-security-include.xml</includeFilterFile>
+            <excludeFilterFile>${session.executionRootDirectory}/spotbugs-security-exclude.xml</excludeFilterFile>
             <jvmArgs>-Dcom.overstock.findbugs.ignore=com.google.common.util.concurrent.RateLimiter,com.google.common.hash.Hasher,com.google.common.hash.HashCode,com.google.common.hash.HashFunction,com.google.common.hash.Hashing,com.google.common.cache.Cache,com.google.common.io.CountingOutputStream,com.google.common.io.ByteStreams,com.google.common.cache.LoadingCache,com.google.common.base.Stopwatch,com.google.common.cache.RemovalNotification,com.google.common.util.concurrent.Uninterruptibles,com.google.common.reflect.ClassPath,com.google.common.reflect.ClassPath$ClassInfo,com.google.common.base.Throwables,com.google.common.collect.Iterators</jvmArgs>
             <plugins>
               <plugin>
                 <groupId>com.overstock.findbugs</groupId>
                 <artifactId>library-detectors</artifactId>
                 <version>1.2.0</version>
               </plugin>
+              <plugin>
+                <groupId>com.h3xstream.findsecbugs</groupId>
+                <artifactId>findsecbugs-plugin</artifactId>
+                <version>LATEST</version>","[{'comment': 'Should specify exact version, and periodically check for updates.', 'commenter': 'ctubbsii'}]"
615,pom.xml,"@@ -793,22 +793,30 @@
           </configuration>
         </plugin>
         <plugin>
-          <groupId>org.codehaus.mojo</groupId>
-          <artifactId>findbugs-maven-plugin</artifactId>
+          <groupId>com.github.spotbugs</groupId>
+          <artifactId>spotbugs</artifactId>
           <version>${findbugs.version}</version>
           <configuration>
             <xmlOutput>true</xmlOutput>
             <effort>Max</effort>
             <failOnError>true</failOnError>
             <includeTests>true</includeTests>
             <maxRank>16</maxRank>
+            <!-- findbugs plugin for webapp security bugs -->
+            <includeFilterFile>${session.executionRootDirectory}/spotbugs-security-include.xml</includeFilterFile>
+            <excludeFilterFile>${session.executionRootDirectory}/spotbugs-security-exclude.xml</excludeFilterFile>","[{'comment': ""I don't think this path will work in all cases (has it been tested with Jenkins?). Also, it'd be better not to have these files clutter the root directory.\r\n\r\nThe `excludeFilterFile` is also optional, and setting it here globally is going to conflict with the `add-findbugs-excludes` profile which sets the property for excludes. If we set the xml config here for the `excludeFilterFile`, then the property will be ignored when that profile is activated."", 'commenter': 'ctubbsii'}]"
615,pom.xml,"@@ -122,7 +122,7 @@
     <extraReleaseArguments />
     <failsafe.excludedGroups />
     <failsafe.groups />
-    <findbugs.version>3.0.5</findbugs.version>
+    <findbugs.version>3.1.3</findbugs.version>","[{'comment': ""Might as well rename this variable to `spotbugs.version`. I'm actually not sure we even need this variable. It's leftover from when we had to support different versions of findbugs, depending on whether we were running on JDK7 or JDK8... but spotbugs only supports JDK8, so we can just inline this variable now."", 'commenter': 'ctubbsii'}]"
615,spotbugs-security-include.xml,"@@ -0,0 +1,21 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the ""License""); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<FindBugsFilter>
+    <Match>
+        <Bug category=""SECURITY""/>","[{'comment': ""It looks to me that this file is overriding all the default includes, so that *only* `SECURITY` bugs are being checked for now. That's not what we want."", 'commenter': 'ctubbsii'}]"
615,spotbugs-security-exclude.xml,"@@ -0,0 +1,18 @@
+<!--","[{'comment': 'This file is not needed.', 'commenter': 'ctubbsii'}]"
620,core/src/main/java/org/apache/accumulo/core/client/admin/ActiveCompaction.java,"@@ -77,7 +77,7 @@
   public abstract String getTable() throws TableNotFoundException;
 
   /**
-   * @return tablet thats is compacting
+   * @return tablet that's is compacting","[{'comment': 'Should  be ""that is"" or ""that\'s""', 'commenter': 'milleruntime'}, {'comment': 'Fixed', 'commenter': 'cjmctague'}]"
620,core/src/main/java/org/apache/accumulo/core/client/sample/Sampler.java,"@@ -56,7 +56,7 @@
   /**
    * @param k
    *          A key that was written to a rfile.
-   * @return True if the key (and its associtated value) should be stored in the rfile's sample.
+   * @return True if the key (and its associated value) should be stored in the rfiles sample.","[{'comment': 'The apostrophe for rfile is showing possession here so should be ""rfile\'s""', 'commenter': 'milleruntime'}, {'comment': 'Fixed', 'commenter': 'cjmctague'}]"
620,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1535,7 +1535,8 @@ public TConditionalSession startConditionalUpdate(TInfo tinfo, TCredentials cred
         try {
           TabletServer.this.resourceManager.waitUntilCommitsAreEnabled();
         } catch (HoldTimeoutException hte) {
-          // Assumption is that the client has timed out and is gone. If thats not the case throw an
+          // Assumption is that the client has timed out and is gone. If that's not the case throw
+          // an","[{'comment': 'Could combine this line with the next line.', 'commenter': 'milleruntime'}, {'comment': ""I don't recall changing that. Might have been the formatter. I'll move the an to the next line"", 'commenter': 'cjmctague'}, {'comment': 'It was probably from a recent commit. Thanks for the fix!', 'commenter': 'milleruntime'}]"
623,core/src/main/java/org/apache/accumulo/core/conf/SiteConfiguration.java,"@@ -110,8 +118,15 @@ private static URL toURL(File f) {
     }
   }
 
-  public static URL getAccumuloSiteLocation() {
-    String configFile = System.getProperty(""accumulo.configuration"", ""accumulo-site.xml"");
+  public static URL getAccumuloPropsLocation() {
+
+    URL siteUrl = SiteConfiguration.class.getClassLoader().getResource(""accumulo-site.xml"");
+    if (siteUrl != null) {
+      throw new IllegalArgumentException(""Found deprecated config file 'accumulo-site.xml' on ""
+          + ""classpath. Since 2.0.0, this file was replaced by 'accumulo.properties'."");
+    }
+
+    String configFile = System.getProperty(""accumulo.configuration"", ""accumulo.properties"");","[{'comment': 'This seems to imply that the `--props` argument is redundant. It might be confusing if we have too many different ways to do this. Personally, I prefer the `-Daccumulo.configuration=...` method, although perhaps it could be called `-Daccumulo.properties=...` instead for naming consistency and clarity?', 'commenter': 'ctubbsii'}]"
641,core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java,"@@ -1787,137 +1789,146 @@ private void runVersionTest(int version, AccumuloConfiguration aconf) throws IOE
     reader.close();
   }
 
-  public static AccumuloConfiguration getAccumuloConfig(String cryptoConfSetting) {
-    return new SiteConfiguration(CryptoTest.class.getClassLoader().getResource(cryptoConfSetting));
+  public static AccumuloConfiguration getAccumuloConfig(boolean cryptoOn) {
+    ConfigurationCopy cfg = new ConfigurationCopy(DefaultConfiguration.getInstance());
+    if (cryptoOn) {","[{'comment': ""Bools are frustrating, because they don't carry semantic intent. Java 8 supports switch statements on String now, so you could just use a switch statement here, and that makes the patch significantly smaller.\r\n\r\nOr you could use an enum instead of Strings. Either way, the patch becomes much smaller if you can keep the `CryptoTest.CRYPTO_ON_CONF` and `CryptoTest.CRYPTO_OFF_CONF` constants throughout."", 'commenter': 'ctubbsii'}, {'comment': 'I like the simplicity of boolean but I guess I am spoiled by Intellij... When I look at a function I see: getAccumuloConfig(cryptoOn: true);\r\n\r\nIn this case, keeping it a string would at least minimize the changes.', 'commenter': 'milleruntime'}]"
641,core/src/test/resources/test-crypto-key,"@@ -0,0 +1 @@
+sixteenbytekey4u","[{'comment': 'This new file results in a rat failure. It also need not be checked in. It can be created dynamically as part of the test, in the target directory.', 'commenter': 'ctubbsii'}, {'comment': ""I have WriteAheadLogEncryptedIT doing this and wasn't sure which was better.  Since the file is so small its probably better to create it every time."", 'commenter': 'milleruntime'}]"
664,minicluster/src/main/java/org/apache/accumulo/cluster/AccumuloCluster.java,"@@ -53,14 +52,6 @@
    */
   ServerContext getServerContext();
 
-  /**
-   * Utility method to get a connector to the cluster.
-   *
-   * @deprecated since 2.0.0, replaced by {@link #getAccumuloClient(String, AuthenticationToken)}
-   */
-  Connector getConnector(String user, AuthenticationToken token)","[{'comment': ""This is public API. It can't be removed unless deprecated first. Same with the subclass impl of this method in StandaloneAccumuloCluster."", 'commenter': 'ctubbsii'}, {'comment': 'Its not in a [public API package](http://accumulo.apache.org/api/)', 'commenter': 'keith-turner'}, {'comment': ""You're right. My mistake. I misread o.a.a.cluster (which isn't public API) as o.a.a.minicluster (which is public API)."", 'commenter': 'ctubbsii'}]"
667,assemble/src/main/assemblies/component.xml,"@@ -23,7 +23,7 @@
       <directoryMode>0755</directoryMode>
       <fileMode>0644</fileMode>
       <useProjectArtifact>false</useProjectArtifact>
-      <outputFileNameMapping>${artifact.artifactId}${dashClassifier?}.${artifact.extension}</outputFileNameMapping>
+      <outputFileNameMapping>${artifact.artifactId}-${artifact.version}${dashClassifier?}.${artifact.extension}</outputFileNameMapping>","[{'comment': 'This line should just be deleted. This is the default `outputFileNameMapping`.', 'commenter': 'ctubbsii'}, {'comment': 'Fixed in 0247ac376dcf123', 'commenter': 'mikewalch'}]"
685,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/navbar.ftl,"@@ -61,7 +61,7 @@
             </li>
             <li class=""dropdown"">
               <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">
-                API <span class=""caret""></span>
+                Rest <span class=""caret""></span>","[{'comment': ""REST should be all uppercase, because it's an acronym (REpresentational State Transfer)."", 'commenter': 'ctubbsii'}, {'comment': 'Roger.  The link being ""REST"" OK for a name though?', 'commenter': 'milleruntime'}, {'comment': 'Works for me.', 'commenter': 'ctubbsii'}]"
792,core/src/main/java/org/apache/accumulo/core/client/AccumuloClient.java,"@@ -58,6 +58,14 @@
  */
 public interface AccumuloClient extends AutoCloseable {
 
+  /**
+   * Verifies credentials of client. Calling this method is optional.
+   *
+   * @throws AccumuloSecurityException
+   *           if authentication token has expired or access is denied
+   */
+  void authenticate() throws AccumuloSecurityException, AccumuloException;","[{'comment': ""I don't think this method is necessary as public API. It is redundant with the security operations method to authenticate the user."", 'commenter': 'ctubbsii'}, {'comment': 'I removed it', 'commenter': 'mikewalch'}]"
792,core/src/main/java/org/apache/accumulo/core/clientImpl/AccumuloClientImpl.java,"@@ -71,19 +70,30 @@ private void ensureOpen() {
     }
   }
 
-  public AccumuloClientImpl(SingletonReservation reservation, final ClientContext context)
-      throws AccumuloSecurityException, AccumuloException {
+  public AccumuloClientImpl(SingletonReservation reservation, final ClientContext context) {
     checkArgument(context != null, ""Context is null"");
     checkArgument(context.getCredentials() != null, ""Credentials are null"");
     checkArgument(context.getCredentials().getToken() != null, ""Authentication token is null"");
-    if (context.getCredentials().getToken().isDestroyed())
-      throw new AccumuloSecurityException(context.getCredentials().getPrincipal(),
-          SecurityErrorCode.TOKEN_EXPIRED);
 
     this.singletonReservation = Objects.requireNonNull(reservation);
     this.context = context;
     instanceID = context.getInstanceID();
+    this.tableops = new TableOperationsImpl(context);
+    this.namespaceops = new NamespaceOperationsImpl(context, tableops);
+  }
+
+  Table.ID getTableId(String tableName) throws TableNotFoundException {
+    Table.ID tableId = Tables.getTableId(context, tableName);
+    if (Tables.getTableState(context, tableId) == TableState.OFFLINE)
+      throw new TableOfflineException(Tables.getTableOfflineMsg(context, tableId));
+    return tableId;
+  }
 
+  @Override
+  public void authenticate() throws AccumuloSecurityException, AccumuloException {","[{'comment': ""Moving this out of the constructor is good, but I don't think it needs to be in the public API. And, in fact, the current implementation is bad for the public API anyway, because it has special handling for system users, which would be inappropriate for public API.\r\n\r\nThe only reason we need any of this code at all, is to retain the existing behavior in ConnectorImpl... which, as far as I can tell, not been modified to call this new method anyway.\r\n\r\nI would prefer we:\r\n1. keep this out of the public API, and\r\n2. retain the check after constructing the AccumuloClientImpl on behalf of the ConnectorImpl, but not used by anybody else."", 'commenter': 'ctubbsii'}, {'comment': 'This is done. I left the authenticate method in AccumuloClientImpl but it has been removed from the AccumuloClient interface.', 'commenter': 'mikewalch'}]"
792,core/src/main/java/org/apache/accumulo/core/clientImpl/AccumuloClientImpl.java,"@@ -298,82 +311,93 @@ private ClientInfo getClientInfo() {
     }
 
     @Override
-    public AccumuloClient build() throws AccumuloException, AccumuloSecurityException {
+    public T build() {
+      return builderFunction.build(this);
+    }
+
+    public static ClientBuilderImpl<AccumuloClient> newClientBuilder() {
+      return new ClientBuilderImpl<>(ClientBuilderImpl::buildClient);
+    }
+
+    public static ClientBuilderImpl<Properties> newPropertiesBuilder() {
+      return new ClientBuilderImpl<>(ClientBuilderImpl::buildProps);
+    }
+
+    private static AccumuloClient buildClient(ClientBuilderImpl<AccumuloClient> cbi) {
       SingletonReservation reservation = SingletonManager.getClientReservation();
       try {
-        return new AccumuloClientImpl(reservation, new ClientContext(getClientInfo()));
-      } catch (AccumuloException | AccumuloSecurityException | RuntimeException e) {
+        return new AccumuloClientImpl(reservation, new ClientContext(cbi.getClientInfo()));
+      } catch (RuntimeException e) {","[{'comment': 'This could use a comment explaining that AccumuloClientImpl is responsible for closing the reservation if it is constructed successfully. Otherwise, this code looks like it could be cleaned up using try-with-resources (which it cannot be, because we only want to close the reservation on RTE).', 'commenter': 'ctubbsii'}, {'comment': 'Done', 'commenter': 'mikewalch'}]"
792,core/src/main/java/org/apache/accumulo/core/clientImpl/AccumuloClientImpl.java,"@@ -298,82 +311,93 @@ private ClientInfo getClientInfo() {
     }
 
     @Override
-    public AccumuloClient build() throws AccumuloException, AccumuloSecurityException {
+    public T build() {
+      return builderFunction.build(this);
+    }
+
+    public static ClientBuilderImpl<AccumuloClient> newClientBuilder() {","[{'comment': ""This method could be inline'd into the public API method where it is used."", 'commenter': 'ctubbsii'}]"
792,core/src/main/java/org/apache/accumulo/core/clientImpl/AccumuloClientImpl.java,"@@ -298,82 +311,93 @@ private ClientInfo getClientInfo() {
     }
 
     @Override
-    public AccumuloClient build() throws AccumuloException, AccumuloSecurityException {
+    public T build() {
+      return builderFunction.build(this);
+    }
+
+    public static ClientBuilderImpl<AccumuloClient> newClientBuilder() {
+      return new ClientBuilderImpl<>(ClientBuilderImpl::buildClient);
+    }
+
+    public static ClientBuilderImpl<Properties> newPropertiesBuilder() {","[{'comment': ""This method can also be inline'd."", 'commenter': 'ctubbsii'}, {'comment': 'Done', 'commenter': 'mikewalch'}]"
792,core/src/main/java/org/apache/accumulo/core/clientImpl/AccumuloClientImpl.java,"@@ -283,12 +287,21 @@ public void close() {
     }
   }
 
-  public static class AccumuloClientBuilderImpl
-      implements InstanceArgs, PropertyOptions, ClientInfoOptions, AuthenticationArgs,
-      ConnectionOptions, SslOptions, SaslOptions, AccumuloClientFactory, FromOptions {
+  public static class ClientBuilderImpl<T>
+      implements InstanceArgs<T>, PropertyOptions<T>, AuthenticationArgs<T>, ConnectionOptions<T>,
+      SslOptions<T>, SaslOptions<T>, ClientFactory<T>, FromOptions<T> {
 
     private Properties properties = new Properties();
     private AuthenticationToken token = null;
+    private BuildFunction<T> builderFunction;
+
+    private interface BuildFunction<T2> {
+      T2 build(ClientBuilderImpl cbi);
+    }
+
+    ClientBuilderImpl(BuildFunction<T> builderFunction) {","[{'comment': 'Very clever.', 'commenter': 'ctubbsii'}]"
792,core/src/main/java/org/apache/accumulo/core/clientImpl/AccumuloClientImpl.java,"@@ -283,12 +287,21 @@ public void close() {
     }
   }
 
-  public static class AccumuloClientBuilderImpl
-      implements InstanceArgs, PropertyOptions, ClientInfoOptions, AuthenticationArgs,
-      ConnectionOptions, SslOptions, SaslOptions, AccumuloClientFactory, FromOptions {
+  public static class ClientBuilderImpl<T>
+      implements InstanceArgs<T>, PropertyOptions<T>, AuthenticationArgs<T>, ConnectionOptions<T>,
+      SslOptions<T>, SaslOptions<T>, ClientFactory<T>, FromOptions<T> {
 
     private Properties properties = new Properties();
     private AuthenticationToken token = null;
+    private BuildFunction<T> builderFunction;","[{'comment': ""The `BuildFunction` interface isn't needed. Can just use `Function<ClientBuilderImpl, T>` here."", 'commenter': 'ctubbsii'}, {'comment': 'Done', 'commenter': 'mikewalch'}]"
792,core/src/main/java/org/apache/accumulo/core/clientImpl/AccumuloClientImpl.java,"@@ -298,82 +311,93 @@ private ClientInfo getClientInfo() {
     }
 
     @Override
-    public AccumuloClient build() throws AccumuloException, AccumuloSecurityException {
+    public T build() {
+      return builderFunction.build(this);","[{'comment': 'If using `Function<ClientBuilderImpl, T>` for the build function type, instead of a custom interface, this method becomes `return builderFunction.apply(this);`', 'commenter': 'ctubbsii'}, {'comment': 'Done', 'commenter': 'mikewalch'}]"
792,core/src/main/java/org/apache/accumulo/core/clientImpl/ConnectorImpl.java,"@@ -47,9 +49,10 @@
 
   private final AccumuloClientImpl impl;
 
-  public ConnectorImpl(AccumuloClientImpl impl) {
+  public ConnectorImpl(AccumuloClientImpl impl) throws AccumuloSecurityException, AccumuloException {
     this.impl = impl;
     SingletonManager.setMode(Mode.CONNECTOR);
+    impl.authenticate();","[{'comment': 'Is this the only usage of this method? Maybe could inline it here.', 'commenter': 'ctubbsii'}, {'comment': ""yes it is. i'll look into it"", 'commenter': 'mikewalch'}]"
796,hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoop/mapreduce/AccumuloInputFormat.java,"@@ -41,7 +41,7 @@
  * job using the {@link #configure()} method, which provides a fluent API. For Example:
  *
  * <pre>
- * AccumuloInputFormat.configure().clientInfo(info).table(name).auths(auths) // required
+ * AccumuloInputFormat.configure().clientProperties(info).table(name).auths(auths) // required","[{'comment': 'could use props instead of info', 'commenter': 'mikewalch'}]"
796,hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoop/mapreduce/AccumuloRowInputFormat.java,"@@ -43,7 +43,7 @@
  * For Example:
  *
  * <pre>
- * AccumuloRowInputFormat.configure().clientInfo(info).table(name).auths(auths) // required
+ * AccumuloRowInputFormat.configure().clientProperties(info).table(name).auths(auths) // required","[{'comment': 'props instead of info', 'commenter': 'mikewalch'}]"
796,hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoopImpl/mapreduce/OutputFormatBuilderImpl.java,"@@ -39,7 +40,7 @@
   boolean simulationMode = false;
 
   @Override
-  public OutputFormatBuilder.OutputOptions<T> clientInfo(ClientInfo clientInfo) {
+  public OutputFormatBuilder.OutputOptions<T> clientProperties(Properties clientProperties) {","[{'comment': 'ClientInfo.from() is needed here', 'commenter': 'mikewalch'}, {'comment': 'Thanks, fixed in 1a5df1a', 'commenter': 'milleruntime'}]"
868,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -456,6 +456,8 @@
       ""The amount of time an assignment can run before the server will print a""
           + "" warning along with the current stack trace. Meant to help debug stuck""
           + "" assignments""),
+  TSERV_REPLICATION_ENABLED(""tserver.replication.services.enabled"", ""false"", PropertyType.BOOLEAN,","[{'comment': 'Could make this property name more generic like `replication.enabled` so it can be used by other processes like the master.', 'commenter': 'mikewalch'}, {'comment': 'Looking at the services that the Master starts up for replication, it is a bit trickier.  And I am not sure how much we save since there should only be one Master running versus X TabletServers.  I think making it a tserver property makes the most sense here.  There may also be a use case where you want certain tservers to participate in replication and disabled on others.', 'commenter': 'milleruntime'}, {'comment': ""Even if you are not going to touch replication in other processes, I would keep the name `replication.enabled`.  It's either enabled or it's not. If can confusing if we have other similar properties like `master.replication.enabled` as you don't want users enabling it in tserver but not the master."", 'commenter': 'mikewalch'}, {'comment': "">  It's either enabled or it's not\r\n\r\nThis.\r\n\r\nEither replication is on (and all the necessary services are running) or it's off (and none of the services are running). I thought you were advocating to moving 100% of the way to the latter with this change, @milleruntime."", 'commenter': 'joshelser'}, {'comment': 'Ideally, yes.', 'commenter': 'milleruntime'}, {'comment': 'As I understand it, the current design doesn\'t make a single `replication.enabled` option as simple to do as it first appears. The elements in the master that are replication related appear to be part of the replica ""send"" code path.... the elements in the tserver that we\'re talking about appear related to the ""AccumuloReplicaSystem"", which is specifically a tserver feature for handling received replication data at the destination (the replica ""receive"" code path). It should be entirely possible to enable these features independently (to support basically any use case that isn\'t bi-directional replication between two Accumulo instances).', 'commenter': 'ctubbsii'}, {'comment': ""> the current design doesn't make a single replication.enabled option as simple to do as it first appears.\r\n\r\nI'm not sure what you think is untenable here. There are multiple services that are started, in different processes, but it all uses the single configuration option: a global on/off.\r\n\r\n> It should be entirely possible to enable these features independently (to support basically any use case that isn't bi-directional replication between two Accumulo instances).\r\n\r\nThe current implementation is definitely not intended to be used in this fashion. I'd caution against trying to separate these as it's going to open a can of worms. If nothing else, if this is an itch you want to scratch, do that on its own to avoid conflating these issues."", 'commenter': 'joshelser'}]"
868,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1391,27 +1376,20 @@ public void run() {
       sleepUninterruptibly(100, TimeUnit.MILLISECONDS);
     }
 
-    // Start the daemon to scan the replication table and make units of work
-    replicationWorkDriver = new ReplicationDriver(this);
-    replicationWorkDriver.start();
-
-    // Start the daemon to assign work to tservers to replicate to our peers
-    replicationWorkAssigner = new WorkDriver(this);
-    replicationWorkAssigner.start();
-
-    // Advertise that port we used so peers don't have to be told what it is
-    context.getZooReaderWriter().putPersistentData(
-        getZooKeeperRoot() + Constants.ZMASTER_REPLICATION_COORDINATOR_ADDR,
-        replAddress.address.toString().getBytes(UTF_8), NodeExistsPolicy.OVERWRITE);
-
-    // Register replication metrics
-    MasterMetricsFactory factory = new MasterMetricsFactory(getConfiguration(), this);
-    Metrics replicationMetrics = factory.createReplicationMetrics();
-    try {
-      replicationMetrics.register();
-    } catch (Exception e) {
-      log.error(""Failed to register replication metrics"", e);
-    }
+    // if the replication name is ever set, then start replication services
+    final ReplTServer replServer = new ReplTServer();
+    SimpleTimer.getInstance(getConfiguration()).schedule(() -> {
+      try {
+        if (!getConfiguration().get(Property.REPLICATION_NAME).isEmpty()) {
+          if (replServer.server == null) {
+            log.info(Property.REPLICATION_NAME.getKey() + "" was set, starting repl services."");
+            replServer.setServer(setupReplication());
+          }
+        }
+      } catch (UnknownHostException | KeeperException | InterruptedException e) {
+        log.error(""Error occurred starting replication services. "", e);
+      }
+    }, 1000, 5000);","[{'comment': 'Change we make this initial delay `0` to mimic the current semantics (we start the service right away)?', 'commenter': 'joshelser'}]"
868,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1427,9 +1405,13 @@ public void run() {
 
     final long deadline = System.currentTimeMillis() + MAX_CLEANUP_WAIT_TIME;
     statusThread.join(remaining(deadline));
-    replicationWorkAssigner.join(remaining(deadline));
-    replicationWorkDriver.join(remaining(deadline));
-    replAddress.server.stop();
+    if (!getConfiguration().get(Property.REPLICATION_NAME).isEmpty()) {","[{'comment': ""What about always checking if these are non-null and stopping them if they are?\r\n\r\nChecking the configuration for the property (while it should always be accurate) has the potential to introduce a bug where we could just check everything and be certain it's all stopped."", 'commenter': 'joshelser'}, {'comment': 'I am not sure what you are asking.  Are you saying, kill the timer watch threads once the replication.name is set?', 'commenter': 'milleruntime'}, {'comment': ""Nope, I'm just saying to always check if we have replication service threads to stop, regardless of whether or not `Property.REPLICATION_NAME` is set."", 'commenter': 'joshelser'}, {'comment': 'Ahhh I see.  Yes, that would be better.  Good idea. ', 'commenter': 'milleruntime'}]"
868,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1444,6 +1426,55 @@ public void run() {
     log.info(""exiting"");
   }
 
+  class ReplTServer {","[{'comment': 'What value is this class giving us? Just seems like an extra object we create.', 'commenter': 'joshelser'}, {'comment': 'This was so we can pass a final object to the timer thread.  ', 'commenter': 'milleruntime'}, {'comment': 'What about just using `AtomicReference` instead of defining our own class?', 'commenter': 'joshelser'}, {'comment': 'Oh yeah good idea, that would be better.', 'commenter': 'milleruntime'}]"
868,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -2841,35 +2839,14 @@ public void run() {
       log.error(""Error setting watches for recoveries"");
       throw new RuntimeException(ex);
     }
-
-    // Start the thrift service listening for incoming replication requests
-    try {
-      startReplicationService();
-    } catch (UnknownHostException e) {
-      throw new RuntimeException(""Failed to start replication service"", e);
-    }
-
-    // Start the pool to handle outgoing replications
-    final ThreadPoolExecutor replicationThreadPool = new SimpleThreadPool(
-        getConfiguration().getCount(Property.REPLICATION_WORKER_THREADS), ""replication task"");
-    replWorker.setExecutor(replicationThreadPool);
-    replWorker.run();
-
-    // Check the configuration value for the size of the pool and, if changed, resize the pool,
-    // every 5 seconds);
     final AccumuloConfiguration aconf = getConfiguration();
-    Runnable replicationWorkThreadPoolResizer = new Runnable() {
-      @Override
-      public void run() {
-        int maxPoolSize = aconf.getCount(Property.REPLICATION_WORKER_THREADS);
-        if (replicationThreadPool.getMaximumPoolSize() != maxPoolSize) {
-          log.info(""Resizing thread pool for sending replication work from {} to {}"",
-              replicationThreadPool.getMaximumPoolSize(), maxPoolSize);
-          replicationThreadPool.setMaximumPoolSize(maxPoolSize);
-        }
+    // if the replication name is ever set, then start replication services
+    SimpleTimer.getInstance(aconf).schedule(() -> {
+      if (!getConfiguration().get(Property.REPLICATION_NAME).isEmpty()) {
+        log.info(Property.REPLICATION_NAME.getKey() + "" was set, starting repl services."");
+        setupReplication(aconf);
       }
-    };
-    SimpleTimer.getInstance(aconf).schedule(replicationWorkThreadPoolResizer, 10000, 30000);
+    }, 1000, 5000);","[{'comment': '`0` initial delay here too, please.', 'commenter': 'joshelser'}]"
868,test/src/main/java/org/apache/accumulo/test/replication/MultiTserverReplicationIT.java,"@@ -47,6 +48,7 @@
 
   @Override
   public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+    cfg.setProperty(Property.REPLICATION_NAME.getKey(), ""test"");","[{'comment': 'Can you add a comment here as to why this is needed, please?', 'commenter': 'joshelser'}]"
868,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1427,9 +1406,12 @@ public void run() {
 
     final long deadline = System.currentTimeMillis() + MAX_CLEANUP_WAIT_TIME;
     statusThread.join(remaining(deadline));
-    replicationWorkAssigner.join(remaining(deadline));
-    replicationWorkDriver.join(remaining(deadline));
-    replAddress.server.stop();
+    if (replicationWorkAssigner != null)
+      replicationWorkAssigner.join(remaining(deadline));
+    if (replicationWorkDriver != null)
+      replicationWorkDriver.join(remaining(deadline));
+    TServerUtils.stopTServer(replServer.get());","[{'comment': 'Is it ok to pass stopTServer null?', 'commenter': 'keith-turner'}, {'comment': 'could do \r\n\r\n```java\r\nif(replServer.get() != null){\r\n     replicationWorkAssigner.join(remaining(deadline));\t\t￼   \r\n￼     replicationWorkDriver.join(remaining(deadline));\t\t￼ \r\n￼     replAddress.server.stop();\r\n}\r\n\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Yeah the TServerUtils method checks for null initially. I had something similar earlier but Josh suggest to always stop the thread.  ', 'commenter': 'milleruntime'}]"
868,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1391,27 +1377,20 @@ public void run() {
       sleepUninterruptibly(100, TimeUnit.MILLISECONDS);
     }
 
-    // Start the daemon to scan the replication table and make units of work
-    replicationWorkDriver = new ReplicationDriver(this);
-    replicationWorkDriver.start();
-
-    // Start the daemon to assign work to tservers to replicate to our peers
-    replicationWorkAssigner = new WorkDriver(this);
-    replicationWorkAssigner.start();
-
-    // Advertise that port we used so peers don't have to be told what it is
-    context.getZooReaderWriter().putPersistentData(
-        getZooKeeperRoot() + Constants.ZMASTER_REPLICATION_COORDINATOR_ADDR,
-        replAddress.address.toString().getBytes(UTF_8), NodeExistsPolicy.OVERWRITE);
-
-    // Register replication metrics
-    MasterMetricsFactory factory = new MasterMetricsFactory(getConfiguration(), this);
-    Metrics replicationMetrics = factory.createReplicationMetrics();
-    try {
-      replicationMetrics.register();
-    } catch (Exception e) {
-      log.error(""Failed to register replication metrics"", e);
-    }
+    // if the replication name is ever set, then start replication services
+    final AtomicReference<TServer> replServer = new AtomicReference<>();
+    SimpleTimer.getInstance(getConfiguration()).schedule(() -> {
+      try {
+        if (!getConfiguration().get(Property.REPLICATION_NAME).isEmpty()) {","[{'comment': 'Could check if `replServer.get() == null` before checking config as this may be a much faster check and the timer has less to do after enabling.', 'commenter': 'keith-turner'}, {'comment': 'Fixed in 0016dce1a80f7c8f8552252f148ba77f94da2725.  Also did the same in Tserver', 'commenter': 'milleruntime'}, {'comment': 'Only concern with how this is written now is that it\'s subject to a race condition if we ever had multiple invocations of this ""block"" being executed concurrently (we could start two replication Thrift servers). I think `SimpleTimer` does not allow this to happen though.', 'commenter': 'joshelser'}, {'comment': 'The race condition exists prior to this update, when we were just checking if the name was empty, because there is no synchronization.  But I think we are OK because the ScheduleExecutorService should prevent two concurrent threads from ever running for the same task.  The javadoc states:\r\n""If any execution of this task\r\n     * takes longer than its period, then subsequent executions\r\n     * may start late, but will not concurrently execute.""', 'commenter': 'milleruntime'}]"
868,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -2952,8 +2929,10 @@ public void run() {
         }
       }
     }
-    log.debug(""Stopping Replication Server"");
-    TServerUtils.stopTServer(this.replServer);
+    if (this.replServer != null) {","[{'comment': 'If `replServer` is not volatile then may not see changes from another thread.', 'commenter': 'keith-turner'}, {'comment': 'Fixed in 0016dce1a80f7c8f8552252f148ba77f94da2725', 'commenter': 'milleruntime'}]"
906,server/master/src/test/java/org/apache/accumulo/master/replication/SequentialWorkAssignerTest.java,"@@ -66,30 +66,29 @@ public void basicZooKeeperCleanup() {
 
     queuedWork.put(""cluster1"", cluster1Work);
 
-    assigner.setClient(client);
-    assigner.setZooCache(zooCache);
-    assigner.setWorkQueue(workQueue);
+    assigner.zooCache = zooCache;
+    assigner.workQueue = workQueue;","[{'comment': 'These should be replaced with a single call to `assigner.assignWork()`', 'commenter': 'milleruntime'}, {'comment': 'Gotcha that would create the zooCache and workQueue but we would still need to assign them to a ""Mock"" version of it right?', 'commenter': 'cjmctague'}, {'comment': 'You would have to do it a bit differently and setup the EasyMock objects first in the init() method, similar to how we do it in other tests (see [InMemoryMapTest](https://github.com/milleruntime/accumulo/blob/c0f9e322111c8f6bcbfcaf1c67160756153b63a5/server/tserver/src/test/java/org/apache/accumulo/tserver/InMemoryMapTest.java#L112)).  Then call replay at the end of the init method.  You will have to mock each object first calling _EasyMock.expect(..)_ with _andReturn(..)_ and _anyTimes()_ using the EasyMock framework for any objects that the code being called requires.  This usually takes some trial and error, running the test to see what gets called.  The EasyMock framework will throw exceptions telling you what it is missing.  \r\n\r\nFor example, looking at `DistributedWorkQueueWorkAssigner.initializeWorkQueue()`  you will need to add this to init to mock up the InstanceID:\r\n`    EasyMock.expect(client.getInstanceID()).andReturn(""testID"").anyTimes();\r\n`', 'commenter': 'milleruntime'}]"
943,core/src/main/java/org/apache/accumulo/core/client/ClientConfiguration.java,"@@ -50,7 +50,7 @@
  * @deprecated since 2.0.0, replaced by {@link Accumulo#newClient()}
  */
 @Deprecated
-public class ClientConfiguration {
+public class ClientConfiguration extends CompositeConfiguration {","[{'comment': 'This went through a deprecation cycle in 1.9. It should not be re-added. We intentionally removed this to avoid non-public API leakage of commons-configuration stuff.', 'commenter': 'ctubbsii'}, {'comment': ""Why remove the inheritance?  If we are keeping the class for backwards compatibility, removing the super class may still break existing code that uses it. \r\n\r\nAlso looks like this class wasn't deprecated in 1.9: https://github.com/apache/accumulo/blob/1.9/core/src/main/java/org/apache/accumulo/core/client/ClientConfiguration.java#L61"", 'commenter': 'milleruntime'}, {'comment': ""Yes, the whole point was to break code that used it in 2.0. This is why we bumped versions to 1.9 instead of staying on 1.8... for the sole purpose of deprecating out this superclass. This was documented in the release notes for 1.9.0.\r\n\r\nhttps://accumulo.apache.org/release/accumulo-1.9.0/#deprecated-clientconfiguration-api-using-commons-config\r\n\r\nAs for marking it deprecated, Java has no way of marking the superclass as deprecated. So, what we did instead was deprecate all of the methods inherited from the superclass:\r\n\r\nhttps://github.com/apache/accumulo/blob/1.9/core/src/main/java/org/apache/accumulo/core/client/ClientConfiguration.java#L140\r\n\r\nIt was only during the 2.0 development that we decided to deprecate the class in its entirety, in favor of using properties... but that's a separate issue. The methods inherited from the superclass were declared as deprecated in 1.9... all of them, and we documented this in the release notes.\r\n\r\nWe should *not* re-add this API leakage back into the API after already removing it, when we've already done as much as we have to prepare users, and it fixes a legitimate API leakage.\r\n"", 'commenter': 'ctubbsii'}, {'comment': 'IIRC this type is from commons config 1 and causes problems when trying to use commons config 2? ', 'commenter': 'keith-turner'}, {'comment': 'Yes, the type is incompatible with commons-configuration2. However, that problem exists regardless of whether it is in the public API or not. But, having it in the public API prevents us from migrating to commons-configuration2. So, it needs to be out of the public API so we can update it, whether to match what Hadoop is using, for class path convergence, or for security updates. In all such cases, we cannot upgrade it if it remains in the public API.', 'commenter': 'ctubbsii'}, {'comment': ""Gotcha.  I'll drop this fix."", 'commenter': 'milleruntime'}, {'comment': '> But, having it in the public API prevents us from migrating to commons-configuration2.\r\n\r\nAlso causes problems for users or dependencies using commons config 2', 'commenter': 'keith-turner'}]"
943,core/src/main/java/org/apache/accumulo/core/client/rfile/RFile.java,"@@ -408,7 +408,9 @@ public static SummaryInputArguments summaries() {
      *          Configuration for summarizer to run.
      * @since 2.0.0
      */
-    WriterOptions withSummarizers(SummarizerConfiguration... summarizerConf);
+    default WriterOptions withSummarizers(SummarizerConfiguration... summarizerConf) {","[{'comment': ""This is an interesting constraint on fluent interfaces. Since this should only matter to implementing classes, and we should be the only ones providing an implementation for these inner-interfaces which exist for fluent APIs, I don't think it's necessary to provide a default method like this. However, it's probably not going to hurt, either. Perhaps it's good practice to just always create these interfaces as default methods rather than abstract ones?"", 'commenter': 'ctubbsii'}, {'comment': ""Yeah I wasn't sure if we should but wanted to put this change out there for discussion.  I was trying to think of a scenario where adding this to the fluent API would break existing code..."", 'commenter': 'milleruntime'}, {'comment': ""The default method is definitely a clever solution that helps with strict adherence to Semver. So, it's probably worth doing for that reason alone. I don't think it would affect users unless they implemented the interfaces in their own code, which I can't imagine a sane reason for doing, but people are unpredictable and have done crazier things. :smiley_cat:"", 'commenter': 'ctubbsii'}, {'comment': 'Haha true that!  It would also just be a good habit for us to start doing for all new methods to existing interfaces.', 'commenter': 'milleruntime'}]"
943,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -950,8 +950,9 @@ SamplerConfiguration getSamplerConfiguration(String tableName)
    * @since 2.0.0
    * @see Summarizer
    */
-  SummaryRetriever summaries(String tableName)
-      throws TableNotFoundException, AccumuloException, AccumuloSecurityException;
+  default SummaryRetriever summaries(String tableName) {","[{'comment': 'I like adding these default methods.', 'commenter': 'keith-turner'}]"
1018,core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java,"@@ -406,7 +406,7 @@ public void close() {
       if (this.blockCacheManager != null) {
         this.blockCacheManager.stop();
       }
-    } catch (Exception e1) {
+    } catch (RuntimeException e1) {","[{'comment': 'This catch block could probably be removed.', 'commenter': 'keith-turner'}, {'comment': '`this.blockCacheManager.stop();` has the possibility to throw an `UnsupportedOperationException` so I can catch/throw that here to narrow it.', 'commenter': 'cjmctague'}]"
1018,core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java,"@@ -406,8 +406,8 @@ public void close() {
       if (this.blockCacheManager != null) {
         this.blockCacheManager.stop();
       }
-    } catch (RuntimeException e1) {
-      throw new RuntimeException(e1);
+    } catch (UnsupportedOperationException e1) {
+      throw new UnsupportedOperationException(e1);","[{'comment': ""I'm confused by this. Why not just let e1 get thrown all the way through?"", 'commenter': 'ctubbsii'}, {'comment': 'Is this what you meant? 51afbce', 'commenter': 'cjmctague'}, {'comment': ""Or just don't catch it in the first place."", 'commenter': 'ctubbsii'}]"
1018,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooLock.java,"@@ -286,7 +286,7 @@ public void process(WatchedEvent event) {
                   else if (asyncLock != null)
                     failedToAcquireLock();
                 }
-              } catch (Throwable e) {
+              } catch (KeeperException | InterruptedException e) {","[{'comment': ""I don't think this catch should be narrowed because any unexpected runtime exception needs to be properly handled for zookeeper locks.  We don't want to ignore an NPE and think the lock is held when it is not.\r\n\r\nI worry about the correctness of this change because of possible issues like this.  Narrowing could lead to handling runtime exceptions differently which in some case could cause bugs.  Its hard to review all of the narrowing for correctness."", 'commenter': 'keith-turner'}, {'comment': ""Could just do `(KeeperException | InterruptedException | RuntimeException e)`. I still think there's merit in no longer catching `Error` and thinking we have any clue how to handle that case (especially with merely logging it)."", 'commenter': 'ctubbsii'}, {'comment': ""This code does more than log, it calls `lockWatcher.unableToMonitorLockNode(e)` letting an observer know that the state of the lock is now unknown.  With ZK lock issues we do want to make the best possible effort to halt the process, even if we fail trying.  It may make sense to catch Throwable in case, I don't know without further inspection of the code. "", 'commenter': 'keith-turner'}]"
1022,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/BCFile.java,"@@ -634,6 +635,7 @@ public long getRawSize() {
       // backwards compatibility
       if (version.equals(API_VERSION_1)) {
         LOG.trace(""Found a version 1 file to read."");
+        decryptionParams = new NoFileEncrypter().getDecryptionParameters();","[{'comment': 'Should probably just call ""this.decrypter.getDecryptionParameters()"" (or assign it to a local variable of that type first, if the field is a super-type) after the next line instead of constructing ""NoFileEncrypter"" twice.', 'commenter': 'ctubbsii'}, {'comment': 'The NoFileEncrypter class is only used to get the ""no crypto"" decryption params.  The next line uses NoFile**D**ecrypter and stores that locally to pass around. ', 'commenter': 'milleruntime'}, {'comment': 'Oh my mistake. I misread both objects as the same class.', 'commenter': 'ctubbsii'}]"
1040,core/src/main/java/org/apache/accumulo/core/client/mapreduce/lib/partition/KeyRangePartitioner.java,"@@ -29,7 +29,6 @@
  * @deprecated since 2.0.0; Use org.apache.accumulo.hadoop.mapreduce.partition instead from the
  *             accumulo-hadoop-mapreduce.jar
  */
-@Deprecated","[{'comment': 'No, these are intended to be deleted in future versions. It makes no sense to undeprecate them and deduplicate. They are duplicates specifically so we can delete the ones in core in future.', 'commenter': 'ctubbsii'}]"
1042,server/master/src/test/java/org/apache/accumulo/master/tableOps/ImportTableTest.java,"@@ -38,21 +38,20 @@ public void testTabletDir() {
     iti.tableId = ""5"";
 
     // Different volumes with different paths
-    String[] tableDirs = new String[] {""hdfs://nn1:8020/apps/accumulo1/tables"",
-        ""hdfs://nn2:8020/applications/accumulo/tables""};
+    String[] volumes = {""hdfs://nn1:8020/apps/accumulo1"", ""hdfs://nn2:8020/applications/accumulo""};","[{'comment': ""Accumulo 1.9 still only requires Java 1.7. The above won't compile with Java 1.7. When we build with Maven, we ensure we require at least 1.8, but this may cause compilation failures in an IDE when the IDE itself assumes 1.7 compliance."", 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii thanks for pointing this out - fixed.', 'commenter': 'drewfarris'}]"
1068,core/src/main/java/org/apache/accumulo/core/client/ClientConfiguration.java,"@@ -656,16 +656,6 @@ public void addConfiguration(Configuration config) {
     super.addConfiguration(config);
   }
 
-  /**
-   * @deprecated since 1.9.2; will be removed in 2.0.0 to eliminate commons config leakage into
-   *             Accumulo API
-   */
-  @Deprecated
-  @Override
-  public void addConfiguration(Configuration config, boolean asInMemory) {","[{'comment': 'Did this actually go out in 1.9.2?', 'commenter': 'busbey'}, {'comment': 'No. This was specifically added in your previous PR for 1.9.3 https://github.com/apache/accumulo/pull/659/files\r\n\r\nRecall that we deprecated the superclass of ClientConfiguration, and added in all the methods inherited from the superclass, to mark them with the Deprecated annotation. This was a new method between 1.6 and 1.10, that was included here merely to declare it as Deprecated (since once cannot annotate the superclass directly, only its inherited methods).', 'commenter': 'ctubbsii'}, {'comment': ""okay so it's just a docs error on the deprecated since line. lgtm."", 'commenter': 'busbey'}, {'comment': '@busbey Not necessarily a docs error. There was a discussion about how to annotate the deprecated override on your original PR. This new method was added to the superclass by the dependency update during 1.9.3 development... but the superclass itself was deprecated in 1.9.2. Since these overridden methods are being used as a proxy to annotate that the superclass was deprecated, it could go either way.', 'commenter': 'ctubbsii'}]"
1075,server/base/src/main/java/org/apache/accumulo/server/rpc/ThriftServerType.java,"@@ -43,7 +43,7 @@ private ThriftServerType(String name) {
   public static ThriftServerType get(String name) {
     // Our custom HsHa server is the default (if none is provided)
     if (StringUtils.isBlank(name)) {
-      return THREADED_SELECTOR;
+      return CUSTOM_HS_HA;","[{'comment': 'Should this `return getDefault();` instead, so have the default specified in fewer places?', 'commenter': 'ctubbsii'}]"
1075,server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java,"@@ -607,18 +607,18 @@ public static ServerAddress startTServer(ThriftServerType serverType, TimedProce
             serverAddress = createBlockingServer(address, processor, protocolFactory,
                 maxMessageSize, serverName, numThreads, numSTThreads, timeBetweenThreadChecks);
             break;
-          case CUSTOM_HS_HA:
-            log.debug(""Instantiating unsecure custom half-async Thrift server"");
-            serverAddress = createNonBlockingServer(address, processor, protocolFactory, serverName,
-                numThreads, numSTThreads, timeBetweenThreadChecks, maxMessageSize);
-            break;
-          case THREADED_SELECTOR: // Intentional passthrough -- Our custom wrapper around threaded
-                                  // selector is the default
-          default:
+          case THREADED_SELECTOR:
             log.debug(""Instantiating default, unsecure Threaded selector Thrift server"");
             serverAddress = createThreadedSelectorServer(address, processor, protocolFactory,
                 serverName, numThreads, numSTThreads, timeBetweenThreadChecks, maxMessageSize);
             break;
+          case CUSTOM_HS_HA: // Intentional passthrough -- Our custom wrapper around threaded
+                             // selector is the default
+          default:","[{'comment': ""default should fail here, instead of relying on this fall-through. We should ensure the default is set to one of the known types before we get here by setting the value to `ThriftServerType.getDefault()` if it is blank. If an unknown type is specified, we should simply fail... since we cannot recognize and satisfy the user's chosen configuration.\r\n\r\nThis buys us 3 things:\r\n\r\n1. We don't have to re-arrange the switch statement whenever we change the default,\r\n2. We can specify the default in a single place, and\r\n3. We don't do auto-magic behavior, and instead fail sanely when we don't recognize the user's configuration."", 'commenter': 'ctubbsii'}, {'comment': ""That's a subtle change to the behavior of an existing property, 2.0.0 seems like a good time to do something like that."", 'commenter': 'keith-turner'}, {'comment': 'I might argue it\'s a subtle ""fix"" in behavior... :smiley_cat:', 'commenter': 'ctubbsii'}]"
1075,server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java,"@@ -607,18 +607,18 @@ public static ServerAddress startTServer(ThriftServerType serverType, TimedProce
             serverAddress = createBlockingServer(address, processor, protocolFactory,
                 maxMessageSize, serverName, numThreads, numSTThreads, timeBetweenThreadChecks);
             break;
+          case THREADED_SELECTOR:","[{'comment': 'This could be a smaller diff, if left after `CUSTOM_HS_HA`, but here is fine.', 'commenter': 'ctubbsii'}]"
1082,test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java,"@@ -204,6 +207,8 @@ public void getFateStatus() {
     Instance instance = connector.getInstance();
     String tableId;
 
+    runMultipleCompactions();","[{'comment': 'why are we running the multiple compactions here.  I think this should replace the starting of a compaction on line 226 or should not be called at all.  ', 'commenter': 'ivakegg'}, {'comment': 'Ah, I see that this was disabled by the boolean, and this was only to force the error.  At least add a comment to that effect here.', 'commenter': 'ivakegg'}]"
1082,test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java,"@@ -220,7 +225,15 @@ public void getFateStatus() {
 
     Future<?> compactTask = startCompactTask();
 
-    assertTrue(""compaction fate transaction exits"", findFate(tableName));
+    try {
+
+      assertTrue(""compaction fate transaction exits"", findFate(tableName));","[{'comment': 'The blockUntilCompactionsRunning() essentially does this test.  Perhaps that should return a boolean as to whether a compaction was found and that should be tested instead of depending on the timing of the compaction start on 226.', 'commenter': 'ivakegg'}, {'comment': ""I'll incorporate both suggestions and push changes in near future."", 'commenter': 'EdColeman'}]"
1082,test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java,"@@ -322,25 +352,38 @@ public void getFateStatus() {
    */
   private boolean blockUntilCompactionRunning(final String tableName) {
 
-    int runningCompactions = 0;
+    long maxWait = defaultTimeoutSeconds() <= 0 ? 60_000 : ((defaultTimeoutSeconds() * 1000) / 2);
+
+    long startWait = System.currentTimeMillis();
 
     List<String> tservers = connector.instanceOperations().getTabletServers();
 
     /*
-     * wait for compaction to start - The compaction will acquire a fate transaction lock that used
-     * to block a subsequent online command while the fate transaction lock was held.
+     * wait for compaction to start on table - The compaction will acquire a fate transaction lock
+     * that used to block a subsequent online command while the fate transaction lock was held.
      */
-    while (runningCompactions == 0) {
+    while (System.currentTimeMillis() < (startWait + maxWait)) {
 
       try {
 
+        int runningCompactions = 0;
+
         for (String tserver : tservers) {
           runningCompactions += connector.instanceOperations().getActiveCompactions(tserver).size();
           log.trace(""tserver {}, running compactions {}"", tservers, runningCompactions);
         }
 
+        if (runningCompactions > 0) {
+          // Validate that there is a compaction fate transaction - otherwise test is invalid.
+          if (findFate(tableName)) {
+            return true;
+          }
+        }
+
       } catch (AccumuloSecurityException | AccumuloException ex) {
         throw new IllegalStateException(""failed to get active compactions, test fails."", ex);
+      } catch (KeeperException ex) {
+        log.trace(""Saw possible transient zookeeper error"");","[{'comment': 'Since this is a Test, I would make this at least error.  This seems very useful to know. ', 'commenter': 'milleruntime'}, {'comment': 'Or maybe info... just saying that this is OK but I did see this exception. ', 'commenter': 'milleruntime'}, {'comment': 'This is not an abnormal condition - it\'s just the way that zookeeper works and that it throws an exception to signal that a zk node no longer exists - sometimes, that could be an error if code was depending on something that should be there, other times, it\'s just that the node was removed by other threads or processes as a normal operation.  The logging at trace was so that if anyone was specifically interested logging could be enabled, but normally it\'s just zookeeper being zookeeper - the major change was to stop failing the test if this ""normal"", but transient  condition occurred, during the test.', 'commenter': 'EdColeman'}]"
1082,test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java,"@@ -204,6 +207,11 @@ public void getFateStatus() {
     Instance instance = connector.getInstance();
     String tableId;
 
+    // development testing - force multiple compactions to see that it's handled.
+    if (runMultipleCompactions) {
+      runMultipleCompactions();
+    }
+","[{'comment': 'If it is a more reliable test when running more than one compaction, I say just have it always do that.', 'commenter': 'milleruntime'}, {'comment': 'Nevermind, I see what you are doing... maybe just update the comment to make it more clear.  Maybe use the word ""debug"" somewhere so its clear this is just to help debug a test failure. ', 'commenter': 'milleruntime'}]"
1082,test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java,"@@ -302,6 +308,30 @@ public void getFateStatus() {
     }
   }
 
+  /**
+   * Helper method for testing findFate refactor - creates multiple tables and launches compactions
+   * so that more that 1 FATE compaction operation is running during the test. This forces the
+   * condition where multiple compactions could be running - but it is not necessary for the test
+   * itself.
+   */
+  private void runMultipleCompactions() {","[{'comment': 'Could just say ""Helper method for debugging possible test failure""', 'commenter': 'milleruntime'}]"
1088,server/base/src/main/java/org/apache/accumulo/server/util/Admin.java,"@@ -506,7 +507,7 @@ private void printNameSpaceConfiguration(AccumuloClient accumuloClient, String n
       File outputDirectory)
       throws IOException, AccumuloException, AccumuloSecurityException, NamespaceNotFoundException {
     File namespaceScript = new File(outputDirectory, namespace + NS_FILE_SUFFIX);
-    FileWriter nsWriter = new FileWriter(namespaceScript);
+    BufferedWriter nsWriter = new BufferedWriter(new FileWriter(namespaceScript));","[{'comment': 'This should be done in a try-with-resources block.', 'commenter': 'ctubbsii'}, {'comment': 'Hi @ctubbsii  , thanks for your reply. I additionally move the BufferedWriter into the resource block of try. Please check. Thanks.', 'commenter': 'bd2019us'}]"
1096,core/src/main/scripts/generate-thrift.sh,"@@ -78,6 +78,9 @@ done
 find $BUILD_DIR/gen-java -name '*.java' -exec grep -Zl '^public class ' {} + | xargs -0 sed -i -e 's/^[}]$/  private static void unusedMethod() {}\
 }/'
 
+# Remove usage of the javax.annotation.Generated because its no longer present in Java 11
+find $BUILD_DIR/gen-java -name ""*.java"" | xargs sed -i -e '/[@]javax[.]annotation[.]Generated.*/d'","[{'comment': ""There's actually a thrift generator flag to omit these.\r\n\r\nInstead of: `java:generated_annotations=undated`, we should use `java:generated_annotations=suppress`"", 'commenter': 'ctubbsii'}, {'comment': 'We could also do this for the 1.9 branch.', 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -62,6 +63,8 @@
     IS_CONFIGURED, PRINCIPAL, TOKEN
   }
 
+  public static final String tokenCacheMnemonic = ""tokenfile"";","[{'comment': 'Isn\'t this just the name of the tokenFile? This is less confusing to me:\r\n`public static final String tokenFileName = ""tokenfile"";`', 'commenter': 'milleruntime'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -189,7 +192,7 @@ public static void setConnectorInfo(Class<?> implementingClass, Configuration co
     checkArgument(tokenFile != null, ""tokenFile is null"");
 
     try {
-      DistributedCacheHelper.addCacheFile(new URI(tokenFile), conf);
+      DistributedCacheHelper.addCacheFile(new URI(tokenFile + ""#"" + tokenCacheMnemonic), conf);","[{'comment': 'If the names are confusing, you could just rename `tokenFile` to `tokenFilePath`.  Just make sure to update the javadoc', 'commenter': 'milleruntime'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -286,14 +289,29 @@ public static AuthenticationToken getTokenFromFile(Configuration conf, String pr
     try {
       URI[] uris = DistributedCacheHelper.getCacheFiles(conf);","[{'comment': ""I don't think we need to call this anymore."", 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -286,14 +289,29 @@ public static AuthenticationToken getTokenFromFile(Configuration conf, String pr
     try {
       URI[] uris = DistributedCacheHelper.getCacheFiles(conf);
       Path path = null;
-      for (URI u : uris) {
-        if (u.toString().equals(tokenFile)) {
-          path = new Path(u);
-        }
+
+      // See if the ""tokenfile"" symlink was created and try to open the file it points to by it.
+      File tempFile = new File(ConfiguratorBase.tokenFileName);
+      if (tempFile.exists()) {
+        path = new Path(ConfiguratorBase.tokenFileName);","[{'comment': ""This looks like it's creating an HDFS `Path` object pointing to the `File` object on the local filesystem. That doesn't look right to me."", 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -286,14 +289,29 @@ public static AuthenticationToken getTokenFromFile(Configuration conf, String pr
     try {
       URI[] uris = DistributedCacheHelper.getCacheFiles(conf);
       Path path = null;
-      for (URI u : uris) {
-        if (u.toString().equals(tokenFile)) {
-          path = new Path(u);
-        }
+
+      // See if the ""tokenfile"" symlink was created and try to open the file it points to by it.
+      File tempFile = new File(ConfiguratorBase.tokenFileName);
+      if (tempFile.exists()) {
+        path = new Path(ConfiguratorBase.tokenFileName);
       }
+
+      if (path == null && uris != null)
+        for (URI u : uris) {","[{'comment': ""We don't need to do this looping at all anymore. If we can't load the local `File` which was cached on the local filesystem, we can just use the `tokenFile` to create an HDFS `Path` object, so we can read the file directly from HDFS instead of the local cache."", 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -286,14 +289,29 @@ public static AuthenticationToken getTokenFromFile(Configuration conf, String pr
     try {
       URI[] uris = DistributedCacheHelper.getCacheFiles(conf);
       Path path = null;
-      for (URI u : uris) {
-        if (u.toString().equals(tokenFile)) {
-          path = new Path(u);
-        }
+
+      // See if the ""tokenfile"" symlink was created and try to open the file it points to by it.
+      File tempFile = new File(ConfiguratorBase.tokenFileName);
+      if (tempFile.exists()) {
+        path = new Path(ConfiguratorBase.tokenFileName);
       }
+
+      if (path == null && uris != null)
+        for (URI u : uris) {
+          if (u.toString().equals(tokenFile)) {
+            path = new Path(u);
+            break;
+
+          } else if (u.toString().replace(""#"" + ConfiguratorBase.tokenFileName, """")
+              .equals(tokenFile)) {
+            path = new Path(u.toString().replace(""#"" + ConfiguratorBase.tokenFileName, """"));","[{'comment': ""We already have `tokenFile`. We don't need to do any of this. We can just do `path = new Path(tokenFile)`."", 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -286,14 +289,29 @@ public static AuthenticationToken getTokenFromFile(Configuration conf, String pr
     try {
       URI[] uris = DistributedCacheHelper.getCacheFiles(conf);
       Path path = null;
-      for (URI u : uris) {
-        if (u.toString().equals(tokenFile)) {
-          path = new Path(u);
-        }
+
+      // See if the ""tokenfile"" symlink was created and try to open the file it points to by it.
+      File tempFile = new File(ConfiguratorBase.tokenFileName);
+      if (tempFile.exists()) {
+        path = new Path(ConfiguratorBase.tokenFileName);
       }
+
+      if (path == null && uris != null)
+        for (URI u : uris) {
+          if (u.toString().equals(tokenFile)) {
+            path = new Path(u);
+            break;
+
+          } else if (u.toString().replace(""#"" + ConfiguratorBase.tokenFileName, """")
+              .equals(tokenFile)) {
+            path = new Path(u.toString().replace(""#"" + ConfiguratorBase.tokenFileName, """"));
+            break;
+          }
+        }
+
       if (path == null) {
         throw new IllegalArgumentException(
-            ""Couldn't find password file called \"""" + tokenFile + ""\"" in cache."");
+            ""Couldn't find password file called \"""" + tokenFile + ""\""or default in cache."");","[{'comment': '```suggestion\r\n            ""Couldn\'t find password file called \\"""" + tokenFile + ""\\"" in the distributed cache or at the specified path in the distributed filesystem."");\r\n```', 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -62,6 +63,8 @@
     IS_CONFIGURED, PRINCIPAL, TOKEN
   }
 
+  public static final String tokenFileName = ""tokenfile"";","[{'comment': 'This represents the URI fragment after `#` which is to be used for the local file name in the distributed cache. So, the variable name could be something like ""localFileName"" or ""cachedFileName"" or similar.', 'commenter': 'ctubbsii'}]"
1112,test/src/main/java/org/apache/accumulo/test/mapreduce/TokenFileIT.java,"@@ -84,6 +89,40 @@ protected void cleanup(Context context) throws IOException, InterruptedException
         m.put("""", """", Integer.toString(count));
         context.write(new Text(), m);
       }
+
+      @Override
+      protected void setup(Context context) throws IOException, InterruptedException {
+        if (context.getCacheFiles() != null && context.getCacheFiles().length > 0) {
+          // At this point in the MapReduce Job you can get the cached files in HDFS if you want
+          URI[] cachedFiles = context.getCacheFiles();
+          // On the line below we access the file by the pseudonym created during caching in
+          // ConfiguratorBase
+          String fileByPsuedonym = getFileContents(ConfiguratorBase.tokenFileName);
+          assertTrue(!fileByPsuedonym.isEmpty());
+          assertTrue(cachedFiles.length > 0);
+        }
+        super.setup(context);
+      }
+
+      private String getFileContents(String filename) throws FileNotFoundException, IOException {
+
+        StringBuilder sb = new StringBuilder();
+        BufferedReader br = new BufferedReader(new FileReader(filename));","[{'comment': 'I think you could read the file contents with many fewer lines of code, especially in Java 8.\r\n\r\nSomething like:\r\n```java\r\n    return Files.lines(filename).collect(Collectors.joining(System.lineSeparator()));\r\n```', 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/client/mapreduce/lib/partition/RangePartitioner.java,"@@ -16,21 +16,22 @@
  */
 package org.apache.accumulo.core.client.mapreduce.lib.partition;
 
-import static java.nio.charset.StandardCharsets.UTF_8;
-
-import java.io.BufferedReader;
-import java.io.FileInputStream;
+import java.io.File;
 import java.io.FileNotFoundException;
 import java.io.IOException;
-import java.io.InputStreamReader;
 import java.net.URI;
+import java.net.URISyntaxException;
 import java.util.Arrays;
 import java.util.Base64;
 import java.util.Scanner;
 import java.util.TreeSet;
 
+import javax.imageio.IIOException;","[{'comment': 'This should be `IOException` here.', 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/client/mapreduce/lib/partition/RangePartitioner.java,"@@ -90,26 +91,31 @@ private synchronized int getNumSubBins() {
       justification = ""path provided by distributed cache framework, not user input"")
   private synchronized Text[] getCutPoints() throws IOException {
     if (cutPointArray == null) {
+      Path path;
       String cutFileName = conf.get(CUTFILE_KEY);
-      Path[] cf = Job.getInstance().getLocalCacheFiles();
-
-      if (cf != null) {
-        for (Path path : cf) {
-          if (path.toUri().getPath()
-              .endsWith(cutFileName.substring(cutFileName.lastIndexOf('/')))) {
-            TreeSet<Text> cutPoints = new TreeSet<>();
-            try (Scanner in = new Scanner(new BufferedReader(
-                new InputStreamReader(new FileInputStream(path.toString()), UTF_8)))) {
-              while (in.hasNextLine())
-                cutPoints.add(new Text(Base64.getDecoder().decode(in.nextLine())));
-            }
-            cutPointArray = cutPoints.toArray(new Text[cutPoints.size()]);
-            break;
-          }
+      File tempFile = new File(CUTFILE_KEY);
+      if (tempFile.exists()) {
+        path = new Path(CUTFILE_KEY);
+      } else {
+        path = new Path(cutFileName);
+      }
+
+      if (path == null)
+        throw new FileNotFoundException(""Cut point file not found in distributed cache"");","[{'comment': 'This is dead code.', 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/client/mapreduce/lib/partition/RangePartitioner.java,"@@ -90,26 +91,31 @@ private synchronized int getNumSubBins() {
       justification = ""path provided by distributed cache framework, not user input"")
   private synchronized Text[] getCutPoints() throws IOException {
     if (cutPointArray == null) {
+      Path path;
       String cutFileName = conf.get(CUTFILE_KEY);
-      Path[] cf = Job.getInstance().getLocalCacheFiles();
-
-      if (cf != null) {
-        for (Path path : cf) {
-          if (path.toUri().getPath()
-              .endsWith(cutFileName.substring(cutFileName.lastIndexOf('/')))) {
-            TreeSet<Text> cutPoints = new TreeSet<>();
-            try (Scanner in = new Scanner(new BufferedReader(
-                new InputStreamReader(new FileInputStream(path.toString()), UTF_8)))) {
-              while (in.hasNextLine())
-                cutPoints.add(new Text(Base64.getDecoder().decode(in.nextLine())));
-            }
-            cutPointArray = cutPoints.toArray(new Text[cutPoints.size()]);
-            break;
-          }
+      File tempFile = new File(CUTFILE_KEY);
+      if (tempFile.exists()) {
+        path = new Path(CUTFILE_KEY);
+      } else {
+        path = new Path(cutFileName);
+      }
+
+      if (path == null)
+        throw new FileNotFoundException(""Cut point file not found in distributed cache"");
+
+      TreeSet<Text> cutPoints = new TreeSet<>();
+      FileSystem fs = FileSystem.get(conf);
+      FSDataInputStream inputStream = fs.open(path);
+      try (Scanner in = new Scanner(inputStream)) {
+        while (in.hasNextLine()) {
+          cutPoints.add(new Text(Base64.getDecoder().decode(in.nextLine())));
         }
       }
+
+      cutPointArray = cutPoints.toArray(new Text[cutPoints.size()]);
+
       if (cutPointArray == null)
-        throw new FileNotFoundException(cutFileName + "" not found in distributed cache"");
+        throw new IIOException(""Cutpoint array not properly created from file"" + path.getName());","[{'comment': 'This is dead code.', 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/client/mapreduce/lib/partition/RangePartitioner.java,"@@ -90,26 +91,31 @@ private synchronized int getNumSubBins() {
       justification = ""path provided by distributed cache framework, not user input"")
   private synchronized Text[] getCutPoints() throws IOException {
     if (cutPointArray == null) {
+      Path path;
       String cutFileName = conf.get(CUTFILE_KEY);
-      Path[] cf = Job.getInstance().getLocalCacheFiles();
-
-      if (cf != null) {
-        for (Path path : cf) {
-          if (path.toUri().getPath()
-              .endsWith(cutFileName.substring(cutFileName.lastIndexOf('/')))) {
-            TreeSet<Text> cutPoints = new TreeSet<>();
-            try (Scanner in = new Scanner(new BufferedReader(
-                new InputStreamReader(new FileInputStream(path.toString()), UTF_8)))) {
-              while (in.hasNextLine())
-                cutPoints.add(new Text(Base64.getDecoder().decode(in.nextLine())));
-            }
-            cutPointArray = cutPoints.toArray(new Text[cutPoints.size()]);
-            break;
-          }
+      File tempFile = new File(CUTFILE_KEY);
+      if (tempFile.exists()) {
+        path = new Path(CUTFILE_KEY);","[{'comment': 'This is converting the local File to an HDFS Path. This does not look like it will read from the cache properly.', 'commenter': 'ctubbsii'}]"
1112,core/src/main/java/org/apache/accumulo/core/clientImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -284,16 +287,17 @@ public static AuthenticationToken getTokenFromFile(Configuration conf, String pr
       String tokenFile) {
     FSDataInputStream in = null;
     try {
-      URI[] uris = DistributedCacheHelper.getCacheFiles(conf);
-      Path path = null;
-      for (URI u : uris) {
-        if (u.toString().equals(tokenFile)) {
-          path = new Path(u);
-        }
+      Path path;
+      // See if the ""tokenfile"" symlink was created and try to open the file it points to by it.
+      File tempFile = new File(ConfiguratorBase.cachedFileName);
+      if (tempFile.exists()) {
+        path = new Path(ConfiguratorBase.cachedFileName);","[{'comment': ""Another conversion of local File to HDFS Path, which I'm not sure will work correctly."", 'commenter': 'ctubbsii'}]"
1112,hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitioner.java,"@@ -88,26 +89,31 @@ private synchronized int getNumSubBins() {
       justification = ""path provided by distributed cache framework, not user input"")
   private synchronized Text[] getCutPoints() throws IOException {
     if (cutPointArray == null) {
+      Path path;
       String cutFileName = conf.get(CUTFILE_KEY);
-      Path[] cf = Job.getInstance().getLocalCacheFiles();
-
-      if (cf != null) {
-        for (Path path : cf) {
-          if (path.toUri().getPath()
-              .endsWith(cutFileName.substring(cutFileName.lastIndexOf('/')))) {
-            TreeSet<Text> cutPoints = new TreeSet<>();
-            try (Scanner in = new Scanner(new BufferedReader(
-                new InputStreamReader(new FileInputStream(path.toString()), UTF_8)))) {
-              while (in.hasNextLine())
-                cutPoints.add(new Text(Base64.getDecoder().decode(in.nextLine())));
-            }
-            cutPointArray = cutPoints.toArray(new Text[cutPoints.size()]);
-            break;
-          }
+      File tempFile = new File(CUTFILE_KEY);
+      if (tempFile.exists()) {
+        path = new Path(CUTFILE_KEY);
+      } else {
+        path = new Path(cutFileName);
+      }
+
+      if (path == null)
+        throw new FileNotFoundException(""Cut point file not found in distributed cache"");","[{'comment': 'Dead code.', 'commenter': 'ctubbsii'}]"
1112,hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitioner.java,"@@ -16,21 +16,22 @@
  */
 package org.apache.accumulo.hadoop.mapreduce.partition;
 
-import static java.nio.charset.StandardCharsets.UTF_8;
-
-import java.io.BufferedReader;
-import java.io.FileInputStream;
+import java.io.File;
 import java.io.FileNotFoundException;
 import java.io.IOException;
-import java.io.InputStreamReader;
 import java.net.URI;
+import java.net.URISyntaxException;
 import java.util.Arrays;
 import java.util.Base64;
 import java.util.Scanner;
 import java.util.TreeSet;
 
+import javax.imageio.IIOException;","[{'comment': 'Wrong exception.', 'commenter': 'ctubbsii'}]"
1112,hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitioner.java,"@@ -88,26 +89,31 @@ private synchronized int getNumSubBins() {
       justification = ""path provided by distributed cache framework, not user input"")
   private synchronized Text[] getCutPoints() throws IOException {
     if (cutPointArray == null) {
+      Path path;
       String cutFileName = conf.get(CUTFILE_KEY);
-      Path[] cf = Job.getInstance().getLocalCacheFiles();
-
-      if (cf != null) {
-        for (Path path : cf) {
-          if (path.toUri().getPath()
-              .endsWith(cutFileName.substring(cutFileName.lastIndexOf('/')))) {
-            TreeSet<Text> cutPoints = new TreeSet<>();
-            try (Scanner in = new Scanner(new BufferedReader(
-                new InputStreamReader(new FileInputStream(path.toString()), UTF_8)))) {
-              while (in.hasNextLine())
-                cutPoints.add(new Text(Base64.getDecoder().decode(in.nextLine())));
-            }
-            cutPointArray = cutPoints.toArray(new Text[cutPoints.size()]);
-            break;
-          }
+      File tempFile = new File(CUTFILE_KEY);
+      if (tempFile.exists()) {
+        path = new Path(CUTFILE_KEY);
+      } else {
+        path = new Path(cutFileName);
+      }
+
+      if (path == null)
+        throw new FileNotFoundException(""Cut point file not found in distributed cache"");
+
+      TreeSet<Text> cutPoints = new TreeSet<>();
+      FileSystem fs = FileSystem.get(conf);
+      FSDataInputStream inputStream = fs.open(path);
+      try (Scanner in = new Scanner(inputStream)) {
+        while (in.hasNextLine()) {
+          cutPoints.add(new Text(Base64.getDecoder().decode(in.nextLine())));
         }
       }
+
+      cutPointArray = cutPoints.toArray(new Text[cutPoints.size()]);
+
       if (cutPointArray == null)
-        throw new FileNotFoundException(cutFileName + "" not found in distributed cache"");
+        throw new IIOException(""Cutpoint array not properly created from file"" + path.getName());","[{'comment': 'Dead code.', 'commenter': 'ctubbsii'}]"
1112,hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoopImpl/mapreduce/lib/ConfiguratorBase.java,"@@ -107,13 +112,18 @@ public static Properties getClientProperties(Class<?> implementingClass, Configu
         .get(enumToConfKey(implementingClass, ClientOpts.CLIENT_PROPS_FILE), """");
     if (!clientPropsFile.isEmpty()) {
       try {
-        URI[] uris = DistributedCacheHelper.getCacheFiles(conf);
-        Path path = null;
-        for (URI u : uris) {
-          if (u.toString().equals(clientPropsFile)) {
-            path = new Path(u);
-          }
+        Path path;
+        // See if the ""propsfile"" symlink was created and try to open the file it points to by it.
+        File tempFile = new File(ConfiguratorBase.clientPropsFileName);
+        if (tempFile.exists()) {
+          path = new Path(ConfiguratorBase.clientPropsFileName);
+        } else {
+          path = new Path(clientPropsFile);
         }
+
+        if (path == null)
+          throw new IllegalStateException(""Could not initialize properties file"");","[{'comment': 'Dead code.', 'commenter': 'ctubbsii'}]"
1135,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -114,8 +144,11 @@ public Options getOptions() {
     final Options opts = super.getOptions();
     numThreadsOpt = new Option(""nt"", ""num-threads"", true, ""number of threads to use"");
     negateOpt = new Option(""v"", ""negate"", false, ""only include rows without search term"");
+    profileNameOpt = new Option(""pn"", ""profile-name"", true,","[{'comment': 'Not completely sure, but I think the earlier call to `super.getOptions();` may already add a profile option.', 'commenter': 'keith-turner'}, {'comment': 'agreed, code updated', 'commenter': 'DonResnik'}]"
1135,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -93,10 +102,31 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
   }
 
   protected void setUpIterator(final int prio, final String name, final String term,","[{'comment': ""This method is called for each grep term.  I don't think it needs to resetup the profile iters for each term."", 'commenter': 'keith-turner'}, {'comment': 'yes, code updated to remove this', 'commenter': 'DonResnik'}]"
1135,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -63,6 +66,12 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       if (cl.hasOption(negateOpt.getOpt())) {
         negate = true;
       }
+","[{'comment': 'The super class `ScanCommand` has a protected method called `addScanIterators()`.  I am not completely sure, but I think you may be able to call it to setup the profile iters.  If so, very little changes would be needed for this class.', 'commenter': 'keith-turner'}, {'comment': 'Thanks for the suggestion.  I had to make a small change to addScanIterators signature to accept ScannerBase instead of Scanner, but it removed most of my code update, so I hope you are ok with the change.', 'commenter': 'DonResnik'}, {'comment': 'Also, I re-ran the test I documented in the ticket and I got the same results with this new update.', 'commenter': 'DonResnik'}]"
1135,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -71,10 +71,11 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       scanner.setTimeout(getTimeout(cl), TimeUnit.MILLISECONDS);
 
       setupSampling(tableName, cl, shellState, scanner);
+      addScanIterators(shellState, cl, scanner, """");
 
       for (int i = 0; i < cl.getArgs().length; i++) {
         setUpIterator(Integer.MAX_VALUE - cl.getArgs().length + i, ""grep"" + i, cl.getArgs()[i],
-            scanner, cl, negate);
+            scanner, cl, negate, shellState);","[{'comment': '`shellState` is newly passed, but it seems like its not used.', 'commenter': 'keith-turner'}, {'comment': 'correct, removed shellState from method signature', 'commenter': 'DonResnik'}]"
1135,shell/src/main/java/org/apache/accumulo/shell/commands/EGrepCommand.java,"@@ -31,7 +30,7 @@
 
   @Override
   protected void setUpIterator(final int prio, final String name, final String term,
-      final BatchScanner scanner, CommandLine cl, boolean negate) throws IOException {
+      final BatchScanner scanner, CommandLine cl, boolean negate, final Shell shellState) throws Exception {","[{'comment': 'Seems like `shellState` is not used.', 'commenter': 'keith-turner'}, {'comment': 'correct again, removed shellState from method signature', 'commenter': 'DonResnik'}]"
1135,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -71,10 +71,11 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       scanner.setTimeout(getTimeout(cl), TimeUnit.MILLISECONDS);
 
       setupSampling(tableName, cl, shellState, scanner);
+      addScanIterators(shellState, cl, scanner, """");","[{'comment': 'Does this change also benefit the EgrepCommand?', 'commenter': 'keith-turner'}, {'comment': 'Yes, egrep uses grep execute() method', 'commenter': 'DonResnik'}]"
1144,assemble/src/main/assemblies/component.xml,"@@ -47,7 +47,7 @@
         <include>commons-cli:commons-cli</include>
         <include>commons-codec:commons-codec</include>
         <include>commons-collections:commons-collections</include>
-        <include>commons-configuration:commons-configuration</include>
+        <include>org.apache.commons:commons-collections4</include>","[{'comment': ""You've incorrectly replaced the wrong artifact. This should replace commons-collections, not commons-configuration."", 'commenter': 'ctubbsii'}, {'comment': 'I did thank you!', 'commenter': 'milleruntime'}]"
1150,hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoopImpl/mapreduce/InputFormatBuilderTest.java,"@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.hadoopImpl.mapreduce;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.LinkedHashMap;
+import java.util.Map;
+import java.util.Optional;
+
+import org.apache.accumulo.hadoop.mapreduce.InputFormatBuilder;
+import org.apache.hadoop.mapred.JobConf;
+import org.junit.Before;
+import org.junit.Test;
+
+import com.google.common.collect.ImmutableMap;
+
+public class InputFormatBuilderTest {","[{'comment': 'Please add some comments as to what we are testing here.', 'commenter': 'ivakegg'}]"
1150,hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoopImpl/mapreduce/InputFormatBuilderTest.java,"@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.hadoopImpl.mapreduce;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.LinkedHashMap;
+import java.util.Map;
+import java.util.Optional;
+
+import org.apache.accumulo.hadoop.mapreduce.InputFormatBuilder;
+import org.apache.hadoop.mapred.JobConf;
+import org.junit.Before;
+import org.junit.Test;
+
+import com.google.common.collect.ImmutableMap;
+
+public class InputFormatBuilderTest {
+
+  private class InputFormatBuilderImplTest<T> extends InputFormatBuilderImpl<T> {
+    private String currentTable;
+    private Map<String,InputTableConfig> tableConfigMap = new LinkedHashMap<>();
+    private Map<String,String> newHints;
+
+    private InputFormatBuilderImplTest(Class callingClass) {
+      super(callingClass);
+    }
+
+    public InputFormatBuilder.InputFormatOptions<T> table(String tableName) {
+      this.currentTable = tableName;
+      tableConfigMap.put(currentTable, new InputTableConfig());
+      return this;
+    }
+
+    public InputFormatBuilder.InputFormatOptions<T> classLoaderContext(String context) {
+      tableConfigMap.get(currentTable).setContext(context);
+      return this;
+    }
+
+    private Optional<String> getClassLoaderContext() {
+      return tableConfigMap.get(currentTable).getContext();
+    }
+
+    public InputFormatBuilder.InputFormatOptions<T> executionHints(Map<String,String> hints) {
+      this.newHints = ImmutableMap.copyOf(hints);
+      tableConfigMap.get(currentTable).setExecutionHints(hints);
+      return this;
+    }
+
+    private Map<String,String> getExecutionHints() {
+      return newHints;
+    }
+  }
+
+  private InputTableConfig tableQueryConfig;
+  private InputFormatBuilderImplTest formatBuilderTest;
+
+  @Before
+  public void setUp() {
+    tableQueryConfig = new InputTableConfig();
+    formatBuilderTest = new InputFormatBuilderImplTest(InputFormatBuilderTest.class);
+    formatBuilderTest.table(""test"");
+  }
+
+  @Test
+  public void testInputFormatBuilder_ClassLoaderContext() {
+    String context = ""classLoaderContext"";
+
+    InputFormatBuilderImpl<JobConf> formatBuilder =
+        new InputFormatBuilderImpl<>(InputFormatBuilderTest.class);
+    formatBuilder.table(""test"");
+    formatBuilder.classLoaderContext(context);
+
+    Optional<String> classLoaderContextStr = tableQueryConfig.getContext();
+    assertTrue(classLoaderContextStr.toString().contains(""empty"")); // returns Optional.empty
+  }
+
+  @Test
+  public void testInputFormatBuilderImplTest_ClassLoaderContext() {
+    String context = ""classLoaderContext"";
+
+    formatBuilderTest.classLoaderContext(context);
+
+    Optional<String> classLoaderContextStr = formatBuilderTest.getClassLoaderContext();
+    assertEquals(classLoaderContextStr.get(), context);
+  }
+
+  @Test
+  public void testInputFormatBuilderImplTest_ExecuteHints() {
+    Map<String,String> hints = ImmutableMap.<String,String>builder().put(""key1"", ""value1"")
+        .put(""key2"", ""value2"").put(""key3"", ""value3"").build();
+
+    formatBuilderTest.executionHints(hints);
+
+    Map<String,String> executionHints = formatBuilderTest.getExecutionHints();
+    assertEquals(executionHints.toString(), hints.toString());","[{'comment': 'I think the order of entries in these maps are undetermined.  You probably want to sort both the expected and actual to ensure this does not randomly break in the future.', 'commenter': 'ivakegg'}]"
1150,hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoopImpl/mapreduce/InputFormatBuilderTest.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.hadoopImpl.mapreduce;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.Map;
+import java.util.Optional;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.accumulo.hadoop.mapreduce.InputFormatBuilder;
+import org.apache.hadoop.mapred.JobConf;
+import org.junit.Before;
+import org.junit.Test;
+
+/*
+ This unit tests ClassLoaderContext and ExecuteHints functionality
+ */
+public class InputFormatBuilderTest {
+
+  private class InputFormatBuilderImplTest<T> extends InputFormatBuilderImpl<T> {
+    private String currentTable;
+    private SortedMap<String,InputTableConfig> tableConfigMap = new TreeMap<>();
+    private SortedMap<String,String> newHints = new TreeMap<>();
+
+    private InputFormatBuilderImplTest(Class callingClass) {
+      super(callingClass);
+    }
+
+    public InputFormatBuilder.InputFormatOptions<T> table(String tableName) {
+      this.currentTable = tableName;
+      tableConfigMap.put(currentTable, new InputTableConfig());
+      return this;
+    }
+
+    public InputFormatBuilder.InputFormatOptions<T> classLoaderContext(String context) {
+      tableConfigMap.get(currentTable).setContext(context);
+      return this;
+    }
+
+    private Optional<String> getClassLoaderContext() {
+      return tableConfigMap.get(currentTable).getContext();
+    }
+
+    public InputFormatBuilder.InputFormatOptions<T> executionHints(Map<String,String> hints) {
+      this.newHints.putAll(hints);
+      tableConfigMap.get(currentTable).setExecutionHints(hints);
+      return this;
+    }
+
+    private SortedMap<String,String> getExecutionHints() {
+      return newHints;
+    }
+  }
+
+  private InputTableConfig tableQueryConfig;
+  private InputFormatBuilderImplTest formatBuilderTest;
+
+  @Before
+  public void setUp() {
+    tableQueryConfig = new InputTableConfig();
+    formatBuilderTest = new InputFormatBuilderImplTest(InputFormatBuilderTest.class);
+    formatBuilderTest.table(""test"");
+  }
+
+  @Test
+  public void testInputFormatBuilder_ClassLoaderContext() {
+    String context = ""classLoaderContext"";
+
+    InputFormatBuilderImpl<JobConf> formatBuilder =
+        new InputFormatBuilderImpl<>(InputFormatBuilderTest.class);
+    formatBuilder.table(""test"");
+    formatBuilder.classLoaderContext(context);
+
+    Optional<String> classLoaderContextStr = tableQueryConfig.getContext();
+    assertTrue(classLoaderContextStr.toString().contains(""empty"")); // returns Optional.empty
+  }
+
+  @Test
+  public void testInputFormatBuilderImplTest_ClassLoaderContext() {
+    String context = ""classLoaderContext"";
+
+    formatBuilderTest.classLoaderContext(context);
+
+    Optional<String> classLoaderContextStr = formatBuilderTest.getClassLoaderContext();
+    assertEquals(classLoaderContextStr.get(), context);
+  }
+
+  @Test
+  public void testInputFormatBuilderImplTest_ExecuteHints() {
+    SortedMap<String,String> hints = new TreeMap<>();
+    hints.put(""key1"", ""value1"");
+    hints.put(""key2"", ""value2"");
+    hints.put(""key3"", ""value3"");
+
+    formatBuilderTest.executionHints(hints);
+
+    SortedMap<String,String> executionHints = formatBuilderTest.getExecutionHints();
+    assertEquals(executionHints.toString(), hints.toString());","[{'comment': 'In an asserts statement, the expected values is the first argument. Please switch the order of the arguments.', 'commenter': 'ivakegg'}]"
1165,test/src/main/java/org/apache/accumulo/test/functional/DurabilityIT.java,"@@ -218,7 +192,7 @@ private void restartTServer() throws Exception {
     cluster.start();
   }
 
-  private long writeSome(AccumuloClient c, String table, long count) throws Exception {
+  private void writeSome(AccumuloClient c, String table, long count) throws Exception {
     int iterations = 5;","[{'comment': '@Manno15 could `iterations`, `attempts`, and the for loop be removed?  Seems like this loop only existed to write multiple times and get the median time.', 'commenter': 'keith-turner'}, {'comment': ""Yeah, that appears to be the case. I'll remove it and then test it. Thanks"", 'commenter': 'Manno15'}]"
1178,core/src/main/java/org/apache/accumulo/core/client/admin/NamespaceOperations.java,"@@ -84,7 +84,18 @@
 
   /**
    * Create an empty namespace with no initial configuration. Valid names for a namespace contain
-   * letters, numbers, and the underscore character.
+   * letters, numbers, and the underscore character. A safe way to ignore namespaces that do exist
+   * would be to do something like the following:
+   *
+   * <pre>
+   * {@code}","[{'comment': ""This won't work. You need:\r\n\r\n```java\r\n  /**\r\n   * <pre>\r\n   * {@code\r\n   * ... code here; note the @code tag is open ...\r\n   * }\r\n   * </pre>\r\n   */\r\n```\r\n\r\n```"", 'commenter': 'ctubbsii'}, {'comment': ""Actually it does work and that's because I have the pre which I don't need if I use the {@code} correctly.  Thanks.\r\n"", 'commenter': 'hkeebler'}, {'comment': 'Took the @code out and left the ""pre"" tags.  the @code did not really do anything useful.', 'commenter': 'hkeebler'}, {'comment': ""The main reason to use `{@code ... some code here ... }` is to not have to escape reserved HTML characters, like `<` and `>` as `&lt;` and `&gt;`, respectively. You weren't using any reserved characters, so you're fine either way. It can also be useful to some renderers to do syntax highlighting, but that's less important that displaying the contents in HTML correctly."", 'commenter': 'ctubbsii'}]"
1261,core/src/main/java/org/apache/accumulo/core/data/AbstractId.java,"@@ -30,7 +30,9 @@
   private final String canonical;
 
   protected AbstractId(final String canonical) {
-    this.canonical = Objects.requireNonNull(canonical, ""canonical cannot be null"");
+    if (canonical == null || canonical.trim().isEmpty())
+      throw new IllegalArgumentException(""Id string provided can't be empty or null."");","[{'comment': 'Since the parameter name is ""canonical"", it\'d be good to use that word when referencing the parameter.\r\n\r\n```suggestion\r\n      throw new IllegalArgumentException(""canonical id provided can\'t be empty or null."");\r\n```', 'commenter': 'ctubbsii'}]"
1302,server/tserver/src/test/java/org/apache/accumulo/tserver/CheckTabletMetadataTest.java,"@@ -66,7 +72,9 @@ private static void assertFail(TreeMap<Key,Value> tabletMeta, KeyExtent ke, TSer
     TreeMap<Key,Value> copy = new TreeMap<>(tabletMeta);
     assertNotNull(copy.remove(keyToDelete));
     try {
-      assertNull(TabletServer.checkTabletMetadata(ke, tsi, copy, ke.getMetadataEntry()));
+      TabletMetadata tm = TabletMetadata.convertRow(copy.entrySet().iterator(),
+          EnumSet.allOf(ColumnType.class), true);
+      assertFalse(TabletServer.checkTabletMetadata(ke, tsi, tm));
     } catch (Exception e) {
 ","[{'comment': ""This line is now causing a failure in spotbugs, because it is ignoring `Exception`. If the intent is to ignore the exceptions caught, spotbugs can generally be made happy by adding a comment in the block, like:\r\n```suggestion\r\n      // exception is expected\r\n```\r\n\r\nI don't know why this didn't bother spotbugs before."", 'commenter': 'ctubbsii'}]"
1302,core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java,"@@ -167,6 +173,20 @@ public static Text getRow(TableId tableId, Text endRow) {
     public static class BulkFileColumnFamily {
       public static final String STR_NAME = ""loaded"";
       public static final Text NAME = new Text(STR_NAME);
+
+      public static long getBulkLoadTid(Value v) {
+        return getBulkLoadTid(v.toString());
+      }
+
+      public static long getBulkLoadTid(String vs) {
+        if (FateTxId.isFormatedTid(vs)) {
+          return FateTxId.fromString(vs);
+        } else {
+          // a new serialization format was introduce in 2.0. This code support deserializing the
+          // old format.
+          return Long.parseLong(vs);","[{'comment': 'I think this might be causing a NumberFormat exception on empty string in the tests.', 'commenter': 'ctubbsii'}, {'comment': 'Do you know which test?', 'commenter': 'keith-turner'}, {'comment': 'https://travis-ci.org/apache/accumulo/jobs/566489719 shows:\r\n\r\n```\r\n[ERROR]   TabletMetadataTest.testAllColumns:93 » NumberFormat For input string: """"\r\n```', 'commenter': 'ctubbsii'}]"
1302,core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java,"@@ -102,7 +103,7 @@ public void testAllColumns() {
     assertEquals(Map.of(""df1"", dfv1, ""df2"", dfv2), tm.getFilesMap());
     assertEquals(6L, tm.getFlushId().getAsLong());
     assertEquals(rowMap, tm.getKeyValues());
-    assertEquals(Set.of(""bf1"", ""bf2""), Set.copyOf(tm.getLoaded().keySet()));
+    assertEquals(Map.of(""bf1"", 56L, ""bf2"", 59L), tm.getLoaded());","[{'comment': 'Does ordering matter for this check? One thing I\'ve seen is that `Map.of` does not necessarily preserve ""insertion order"" (order based on args).', 'commenter': 'ctubbsii'}, {'comment': 'I would not expect `Map.equals()` to care about the insertion order.', 'commenter': 'keith-turner'}]"
1339,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -872,8 +872,16 @@ TabletGoalState getGoalState(TabletLocationState tls, MergeInfo mergeInfo) {
       }
       // Handle merge transitions
       if (mergeInfo.getExtent() != null) {
-        log.debug(""mergeInfo overlaps: "" + extent + "" "" + mergeInfo.overlaps(extent));
-        if (mergeInfo.overlaps(extent)) {
+
+        final boolean overlaps = mergeInfo.overlaps(extent);
+
+        if (overlaps && log.isDebugEnabled()) {","[{'comment': ""slf4j already checks to see if the level is enabled, so you don't need that part of the conditional. The only time you need to do that check is if you're doing work to prepare the log message that you can avoid. That's not the case here, since you're using the format string mechanism.\r\n```suggestion\r\n        if (overlaps) {\r\n```\r\n\r\nAlso, now that this is the same if condition as the below... you could combine this if/else with the one below."", 'commenter': 'ctubbsii'}, {'comment': ""I'll make a change.  The intent of the first if/else test for debug was so that if trace was enabled, there would only be one message, not both debug and a trace."", 'commenter': 'EdColeman'}]"
1339,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -872,8 +872,16 @@ TabletGoalState getGoalState(TabletLocationState tls, MergeInfo mergeInfo) {
       }
       // Handle merge transitions
       if (mergeInfo.getExtent() != null) {
-        log.debug(""mergeInfo overlaps: "" + extent + "" "" + mergeInfo.overlaps(extent));
-        if (mergeInfo.overlaps(extent)) {
+
+        final boolean overlaps = mergeInfo.overlaps(extent);
+
+        if (overlaps && log.isDebugEnabled()) {
+          log.debug(""mergeInfo overlaps: {} {}"", extent, overlaps);
+        } else if (log.isTraceEnabled()) {","[{'comment': 'same here. this can just be a simple else\r\n```suggestion\r\n        } else {\r\n```', 'commenter': 'ctubbsii'}]"
1339,server/master/src/main/java/org/apache/accumulo/master/Master.java,"@@ -872,8 +872,16 @@ TabletGoalState getGoalState(TabletLocationState tls, MergeInfo mergeInfo) {
       }
       // Handle merge transitions
       if (mergeInfo.getExtent() != null) {
-        log.debug(""mergeInfo overlaps: "" + extent + "" "" + mergeInfo.overlaps(extent));
-        if (mergeInfo.overlaps(extent)) {
+
+        final boolean overlaps = mergeInfo.overlaps(extent);
+
+        if (overlaps && log.isDebugEnabled()) {
+          log.debug(""mergeInfo overlaps: {} {}"", extent, overlaps);
+        } else if (log.isTraceEnabled()) {
+          log.trace(""mergeInfo overlaps: {} {}"", extent, overlaps);","[{'comment': ""Do we even need this message at all when there's no overlap? If you don't have this (and I'm not sure it's useful at all), you could just make a simple change to put the existing debug message inside the if block below and drop the trace log."", 'commenter': 'ctubbsii'}, {'comment': ""I'm not sure what the usefulness of the logging here is - it does not seem to provide me with information that I need, but maybe it is useful if there was a merge issue?\r\n\r\nThe info printed when the merge starts, and completes is good and is enough to monitor progress, but may not help with troubleshooting...\r\n\r\nI'll add some more info on the issue and hopefully can get some feedback."", 'commenter': 'EdColeman'}, {'comment': 'I think it\'d be best to change this implementation by removing:\r\n\r\n```java\r\n  if (log.isTraceEnabled()) {\r\n    log.trace(""mergeInfo overlaps: {} {}"", extent, overlaps);\r\n  } else if (overlaps) {\r\n    log.debug(""mergeInfo overlaps: {} true"", extent);\r\n  }\r\n```\r\n\r\nand putting the debug log message into the if statement below, to be:\r\n\r\n```java\r\n  if (overlaps) {\r\n    log.debug(""mergeInfo overlaps {}"", extent);\r\n    // ...\r\n  } else {\r\n    log.trace(""mergeInfo does not overlap {}"", extent);\r\n  }\r\n```', 'commenter': 'ctubbsii'}]"
1339,server/master/src/main/java/org/apache/accumulo/master/TabletGroupWatcher.java,"@@ -100,11 +100,11 @@
 abstract class TabletGroupWatcher extends Daemon {
   // Constants used to make sure assignment logging isn't excessive in quantity or size
   private static final String ASSIGNMENT_BUFFER_SEPARATOR = "", "";
-  private static final int ASSINGMENT_BUFFER_MAX_LENGTH = 4096;
+  private static final int ASSIGMENT_BUFFER_MAX_LENGTH = 4096;","[{'comment': 'Just a nit but assuming this is meant to correct the spelliing of assignment then it is still incorrect. ;) ', 'commenter': 'hkeebler'}]"
1339,server/master/src/main/java/org/apache/accumulo/master/TabletGroupWatcher.java,"@@ -701,12 +711,17 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
       targetSystemTable = RootTable.NAME;
     }
 
-    BatchWriter bw = null;
+    // BatchWriter bw = null;","[{'comment': 'Nice change with the autoclosable.  Commented out line can be dropped though.', 'commenter': 'milleruntime'}]"
1339,server/master/src/main/java/org/apache/accumulo/master/TabletGroupWatcher.java,"@@ -288,9 +293,10 @@ public void run() {
                   // Old tablet server is back. Return this tablet to its previous owner.
                   if (returnInstance != null) {
                     assignments.add(new Assignment(tls.extent, returnInstance));
-                  } else {
-                    // leave suspended, don't ask for a new assignment.
                   }
+
+                  // leave suspended, don't ask for a new assignment.","[{'comment': 'This comment doesn\'t make sense outside the else statement, since it implies it is left suspended regardless of whether a new assignment is added above. Could remove the blank line above it, and add the word ""else"" to the beginning o the comment to clarify.', 'commenter': 'ctubbsii'}]"
1339,server/master/src/main/java/org/apache/accumulo/master/TabletGroupWatcher.java,"@@ -693,12 +700,16 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
       targetSystemTable = RootTable.NAME;
     }
 
-    BatchWriter bw = null;
+    Connector conn;
+
     try {
+      conn = this.master.getConnector();
+    } catch (AccumuloSecurityException ex) {
+      throw new AccumuloException(ex);
+    }
+
+    try (BatchWriter bw = conn.createBatchWriter(targetSystemTable, new BatchWriterConfig())) {","[{'comment': ""I like these changes, but they are very likely going to create a merge conflict, and they are unrelated to the PR scope. They don't have to be removed, but care should be taken while merging forward to newer branches."", 'commenter': 'ctubbsii'}]"
1381,server/gc/src/main/java/org/apache/accumulo/gc/metrics2/package-info.java,"@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.gc.metrics2;
+
+/**
+ * This package contains classes to publish Accumulo Garbage Collection run cycle statistics to the
+ * hadoop metrics2 system. The hadoop metrics2 system publishes to jmx and can be configured, via a
+ * configuration file, to publish to other metric collection systems (files,...)
+ * <p>
+ * Accumulo will search for a file named hadoop-metrics2-accumulo.properties on the Accumulo
+ * classpath.
+ * <p>
+ * A note on naming: The naming for jmx vs the metrics2 systems are slightly different. Hadoop
+ * metrics2 records will start with CONTEXT.RECORD (accgc.AccGcCycleMetrics). The value for context
+ * is also used by the configuration file for sink configuration.
+ * <p>
+ * In JMX, the hierarchy is: Hadoop..Accumulo..[jmxName]..[processName]..attributes..[name]
+ * <p>
+ * For jvm metrics, the hierarchy is Hadoop..Accumulo..JvmMetrics..attributes..[name]
+ */","[{'comment': 'Rather than a `package-info.java` file, some of this info might be more useful as a class-level javadoc on `server/base/src/main/java/org/apache/accumulo/server/metrics/Metrics.java`, and the remaining would probably be useful on either `GcMetrics` or `GcMetricsFactory`.', 'commenter': 'ctubbsii'}]"
1381,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -529,8 +529,9 @@
       ""When the gc runs it can make a lot of changes to the metadata, on completion, ""
           + "" to force the changes to be written to disk, the metadata and root tables can be flushed""
           + "" and possibly compacted. Legal values are: compact - which both flushes and compacts the""
-          + "" metadata; flush - which flushes only (compactions may be triggered if required); or none.""
-          + "" Since 2.0, the default is flush. Previously the default action was a full compaction.""),
+          + "" metadata; flush - which flushes only (compactions may be triggered if required); or none""),
+  GC_ENABLE_METRICS2(""gc.enable.metrics2"", ""true"", PropertyType.BOOLEAN,","[{'comment': 'I think the following name change would make the property more consistent with similar props:\r\n\r\n```suggestion\r\n  GC_METRICS_ENABLED(""gc.metrics.enabled"", ""true"", PropertyType.BOOLEAN,\r\n```', 'commenter': 'ctubbsii'}]"
1381,server/gc/src/main/java/org/apache/accumulo/gc/metrics2/GcCycleMetrics.java,"@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.gc.metrics2;","[{'comment': 'I think the package name could be more consistent with metrics packages in other components:\r\n\r\n```suggestion\r\npackage org.apache.accumulo.gc.metrics;\r\n```', 'commenter': 'ctubbsii'}]"
1381,server/gc/src/main/java/org/apache/accumulo/gc/metrics2/GcCycleMetrics.java,"@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.gc.metrics2;
+
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReference;
+
+import org.apache.accumulo.core.gc.thrift.GcCycleStats;
+
+/**
+ * Wrapper class for GcCycleStats so that underlying thrift code in GcCycleStats is not modified.
+ * Provides Thread safe access to the gc cycle stats for metrics reporting.
+ */
+public class GcCycleMetrics {
+
+  private AtomicReference<GcCycleStats> lastCollect = new AtomicReference<>(new GcCycleStats());
+  private AtomicReference<GcCycleStats> lastWalCollect = new AtomicReference<>(new GcCycleStats());
+
+  private AtomicLong postOpDurationNanos = new AtomicLong(0);
+  private AtomicLong runCycleCount = new AtomicLong(0);
+
+  public GcCycleMetrics() {}
+
+  /**
+   * Get the last gc run statistics.
+   *
+   * @return the statistics for the last gc run.
+   */
+  public GcCycleStats getLastCollect() {
+    return lastCollect.get();
+  }
+
+  /**
+   * Set the last gc run statistics. Makes a defensive deep copy so that if the gc implementation
+   * modifies the values.
+   *
+   * @param lastCollect
+   *          the last gc run statistics.
+   */
+  public void setLastCollect(final GcCycleStats lastCollect) {
+    this.lastCollect.set(new GcCycleStats(lastCollect));
+  }
+
+  /**
+   * The statistics from the last wal collection.
+   *
+   * @return the last wal collection statistics.
+   */
+  public GcCycleStats getLastWalCollect() {
+    return lastWalCollect.get();
+  }
+
+  /**
+   * Set the lost wal collection statistics
+   *
+   * @param lastWalCollect
+   *          last wal statistics
+   */
+  public void setLastWalCollect(final GcCycleStats lastWalCollect) {
+    this.lastWalCollect.set(new GcCycleStats(lastWalCollect));
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @return duration in nanoseconds.
+   */
+  public long getPostOpDurationNanos() {
+    return postOpDurationNanos.get();
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @param postOpDurationNanos
+   *          the duration, in nanoseconds.
+   */
+  public void setPostOpDurationNanos(long postOpDurationNanos) {
+    this.postOpDurationNanos.set(postOpDurationNanos);
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @return duration in nanoseconds.
+   */
+  public long getRunCycleCount() {","[{'comment': 'Copy/paste error. This javadoc is wrong.', 'commenter': 'ctubbsii'}]"
1381,server/gc/src/main/java/org/apache/accumulo/gc/metrics2/GcCycleMetrics.java,"@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.gc.metrics2;
+
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReference;
+
+import org.apache.accumulo.core.gc.thrift.GcCycleStats;
+
+/**
+ * Wrapper class for GcCycleStats so that underlying thrift code in GcCycleStats is not modified.
+ * Provides Thread safe access to the gc cycle stats for metrics reporting.
+ */
+public class GcCycleMetrics {
+
+  private AtomicReference<GcCycleStats> lastCollect = new AtomicReference<>(new GcCycleStats());
+  private AtomicReference<GcCycleStats> lastWalCollect = new AtomicReference<>(new GcCycleStats());
+
+  private AtomicLong postOpDurationNanos = new AtomicLong(0);
+  private AtomicLong runCycleCount = new AtomicLong(0);
+
+  public GcCycleMetrics() {}
+
+  /**
+   * Get the last gc run statistics.
+   *
+   * @return the statistics for the last gc run.
+   */
+  public GcCycleStats getLastCollect() {
+    return lastCollect.get();
+  }
+
+  /**
+   * Set the last gc run statistics. Makes a defensive deep copy so that if the gc implementation
+   * modifies the values.
+   *
+   * @param lastCollect
+   *          the last gc run statistics.
+   */
+  public void setLastCollect(final GcCycleStats lastCollect) {
+    this.lastCollect.set(new GcCycleStats(lastCollect));
+  }
+
+  /**
+   * The statistics from the last wal collection.
+   *
+   * @return the last wal collection statistics.
+   */
+  public GcCycleStats getLastWalCollect() {
+    return lastWalCollect.get();
+  }
+
+  /**
+   * Set the lost wal collection statistics
+   *
+   * @param lastWalCollect
+   *          last wal statistics
+   */
+  public void setLastWalCollect(final GcCycleStats lastWalCollect) {
+    this.lastWalCollect.set(new GcCycleStats(lastWalCollect));
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @return duration in nanoseconds.
+   */
+  public long getPostOpDurationNanos() {
+    return postOpDurationNanos.get();
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @param postOpDurationNanos
+   *          the duration, in nanoseconds.
+   */
+  public void setPostOpDurationNanos(long postOpDurationNanos) {
+    this.postOpDurationNanos.set(postOpDurationNanos);
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @return duration in nanoseconds.
+   */
+  public long getRunCycleCount() {
+    return runCycleCount.get();
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @param runCycleCount
+   *          the number of gc collect cycles completed.
+   */
+  public void setRunCycleCount(long runCycleCount) {","[{'comment': 'Copy/paste error. This javadoc is half-wrong.', 'commenter': 'ctubbsii'}]"
1381,server/gc/src/main/java/org/apache/accumulo/gc/metrics2/GcHadoopMetrics2.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.gc.metrics2;
+
+import static org.apache.accumulo.gc.metrics2.GcMetrics.GC_METRIC_PREFIX;
+
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.gc.thrift.GcCycleStats;
+import org.apache.hadoop.metrics2.lib.MetricsRegistry;
+import org.apache.hadoop.metrics2.lib.MutableGaugeLong;
+
+/**
+ * Hadoop metrics2 implementation of Accumulo GC cycle metrics.
+ */
+class GcHadoopMetrics2 {","[{'comment': 'This separate class looks like a holdover from when the legacy metrics and the Hadoop Metrics2 implementations were split. Since we dropped the legacy metrics, there is only the Metrics2, so this can be merged into the GcMetrics class, following the same pattern as all the other metrics implementations.', 'commenter': 'ctubbsii'}]"
1381,test/src/main/java/org/apache/accumulo/test/metrics/MetricsFileTailer.java,"@@ -0,0 +1,255 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.test.metrics;
+
+import java.io.File;
+import java.io.RandomAccessFile;
+import java.net.URL;
+import java.util.Iterator;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.commons.configuration2.Configuration;
+import org.apache.commons.configuration2.FileBasedConfiguration;
+import org.apache.commons.configuration2.PropertiesConfiguration;
+import org.apache.commons.configuration2.builder.FileBasedConfigurationBuilder;
+import org.apache.commons.configuration2.builder.fluent.Parameters;
+import org.apache.commons.configuration2.ex.ConfigurationException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * This class allows testing of the publishing to the hadoop metrics2 system by processing a file
+ * for metric records (written as a line.) The file should be configured using the hadoop metrics2
+ * properties as a file based sink with the prefix that is provided on instantiation of the
+ * instance.
+ *
+ * This class will simulate tail-ing a file and is intended to be run in a separate thread. When the
+ * underlying file has data written, the vaule returned by getLastUpdate will change, and the last
+ * line can be retrieved with getLast().
+ */
+public class MetricsFileTailer implements Runnable, AutoCloseable {
+
+  private static final Logger log = LoggerFactory.getLogger(MetricsFileTailer.class);
+
+  private static final int BUFFER_SIZE = 4;
+
+  private final String metricsPrefix;
+
+  private Lock lock = new ReentrantLock();
+  private AtomicBoolean running = new AtomicBoolean(Boolean.TRUE);
+
+  private AtomicLong lastUpdate = new AtomicLong(0);
+  private long startTime = System.nanoTime();
+
+  private int lineCounter = 0;
+  private String[] lineBuffer = new String[BUFFER_SIZE];
+
+  private final String metricsFilename;
+
+  /**
+   * Create an instance that will tail a metrics2 file. The filename / path is determined by the
+   * hadoop-metrics2-accumulo.properties sink configuration for the metrics prefix that is provided.
+   *
+   * @param metricsPrefix
+   *          the prefix in the metrics2 configuration.
+   */
+  public MetricsFileTailer(final String metricsPrefix) {","[{'comment': 'I wonder if it would be easier if we wrote our own metrics sink to use for testing, rather than use the FileSink with something like this. Could be something for future.', 'commenter': 'ctubbsii'}, {'comment': 'The first attempt did have a custom sink that used http / REST, but in minimizing adding external dependencies, it made porting from 1.9 to 2.x more difficult.  There were also conflicts with hadoop metrics2 and the hadoop shaded jars that I did not resolve.\r\n\r\nThe approach may still be valid and will be revisited going forward, but the file sync does provide some basic functionality that was useful to testing during development.', 'commenter': 'EdColeman'}]"
1381,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -529,8 +529,9 @@
       ""When the gc runs it can make a lot of changes to the metadata, on completion, ""
           + "" to force the changes to be written to disk, the metadata and root tables can be flushed""
           + "" and possibly compacted. Legal values are: compact - which both flushes and compacts the""
-          + "" metadata; flush - which flushes only (compactions may be triggered if required); or none.""
-          + "" Since 2.0, the default is flush. Previously the default action was a full compaction.""),
+          + "" metadata; flush - which flushes only (compactions may be triggered if required); or none""),
+  GC_METRICS_ENABLED(""gc.enable.metrics"", ""true"", PropertyType.BOOLEAN,","[{'comment': 'For consistency, with other configs, I suggest:\r\n\r\n```suggestion\r\n  GC_METRICS_ENABLED(""gc.metrics.enabled"", ""true"", PropertyType.BOOLEAN,\r\n```', 'commenter': 'ctubbsii'}]"
1381,server/gc/src/main/java/org/apache/accumulo/gc/metrics/GcCycleMetrics.java,"@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.gc.metrics;
+
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReference;
+
+import org.apache.accumulo.core.gc.thrift.GcCycleStats;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Wrapper class for GcCycleStats so that underlying thrift code in GcCycleStats is not modified.
+ * Provides Thread safe access to the gc cycle stats for metrics reporting.
+ */
+public class GcCycleMetrics {
+
+  private static final Logger log = LoggerFactory.getLogger(GcCycleMetrics.class);
+
+  private AtomicReference<GcCycleStats> lastCollect = new AtomicReference<>(new GcCycleStats());
+  private AtomicReference<GcCycleStats> lastWalCollect = new AtomicReference<>(new GcCycleStats());
+
+  private AtomicLong postOpDurationNanos = new AtomicLong(0);
+  private AtomicLong runCycleCount = new AtomicLong(0);
+
+  public GcCycleMetrics() {}
+
+  /**
+   * Get the last gc run statistics.
+   *
+   * @return the statistics for the last gc run.
+   */
+  public GcCycleStats getLastCollect() {
+    return lastCollect.get();
+  }
+
+  /**
+   * Set the last gc run statistics. Makes a defensive deep copy so that if the gc implementation
+   * modifies the values.
+   *
+   * @param lastCollect
+   *          the last gc run statistics.
+   */
+  public void setLastCollect(final GcCycleStats lastCollect) {
+    this.lastCollect.set(new GcCycleStats(lastCollect));
+  }
+
+  /**
+   * The statistics from the last wal collection.
+   *
+   * @return the last wal collection statistics.
+   */
+  public GcCycleStats getLastWalCollect() {
+    return lastWalCollect.get();
+  }
+
+  /**
+   * Set the lost wal collection statistics
+   *
+   * @param lastWalCollect
+   *          last wal statistics
+   */
+  public void setLastWalCollect(final GcCycleStats lastWalCollect) {
+    this.lastWalCollect.set(new GcCycleStats(lastWalCollect));
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @return duration in nanoseconds.
+   */
+  public long getPostOpDurationNanos() {
+    return postOpDurationNanos.get();
+  }
+
+  /**
+   * Set the duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @param postOpDurationNanos
+   *          the duration, in nanoseconds.
+   */
+  public void setPostOpDurationNanos(long postOpDurationNanos) {
+    this.postOpDurationNanos.set(postOpDurationNanos);
+  }
+
+  /**
+   * The number of gc cycles that have completed since initialization at process start.
+   *
+   * @return current run cycle count.
+   */
+  public long getRunCycleCount() {
+    return runCycleCount.get();
+  }
+
+  /**
+   * Set the counter for number of completed gc collection cycles p the provided value. The value is","[{'comment': '```suggestion\r\n   * Set the counter for number of completed gc collection cycles to the provided value. The value is\r\n```', 'commenter': 'ctubbsii'}]"
1381,server/gc/src/main/java/org/apache/accumulo/gc/metrics/GcCycleMetrics.java,"@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.accumulo.gc.metrics;
+
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReference;
+
+import org.apache.accumulo.core.gc.thrift.GcCycleStats;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Wrapper class for GcCycleStats so that underlying thrift code in GcCycleStats is not modified.
+ * Provides Thread safe access to the gc cycle stats for metrics reporting.
+ */
+public class GcCycleMetrics {
+
+  private static final Logger log = LoggerFactory.getLogger(GcCycleMetrics.class);
+
+  private AtomicReference<GcCycleStats> lastCollect = new AtomicReference<>(new GcCycleStats());
+  private AtomicReference<GcCycleStats> lastWalCollect = new AtomicReference<>(new GcCycleStats());
+
+  private AtomicLong postOpDurationNanos = new AtomicLong(0);
+  private AtomicLong runCycleCount = new AtomicLong(0);
+
+  public GcCycleMetrics() {}
+
+  /**
+   * Get the last gc run statistics.
+   *
+   * @return the statistics for the last gc run.
+   */
+  public GcCycleStats getLastCollect() {
+    return lastCollect.get();
+  }
+
+  /**
+   * Set the last gc run statistics. Makes a defensive deep copy so that if the gc implementation
+   * modifies the values.
+   *
+   * @param lastCollect
+   *          the last gc run statistics.
+   */
+  public void setLastCollect(final GcCycleStats lastCollect) {
+    this.lastCollect.set(new GcCycleStats(lastCollect));
+  }
+
+  /**
+   * The statistics from the last wal collection.
+   *
+   * @return the last wal collection statistics.
+   */
+  public GcCycleStats getLastWalCollect() {
+    return lastWalCollect.get();
+  }
+
+  /**
+   * Set the lost wal collection statistics
+   *
+   * @param lastWalCollect
+   *          last wal statistics
+   */
+  public void setLastWalCollect(final GcCycleStats lastWalCollect) {
+    this.lastWalCollect.set(new GcCycleStats(lastWalCollect));
+  }
+
+  /**
+   * Duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @return duration in nanoseconds.
+   */
+  public long getPostOpDurationNanos() {
+    return postOpDurationNanos.get();
+  }
+
+  /**
+   * Set the duration of post operation (compact, flush, none) in nanoseconds.
+   *
+   * @param postOpDurationNanos
+   *          the duration, in nanoseconds.
+   */
+  public void setPostOpDurationNanos(long postOpDurationNanos) {
+    this.postOpDurationNanos.set(postOpDurationNanos);
+  }
+
+  /**
+   * The number of gc cycles that have completed since initialization at process start.
+   *
+   * @return current run cycle count.
+   */
+  public long getRunCycleCount() {
+    return runCycleCount.get();
+  }
+
+  /**
+   * Set the counter for number of completed gc collection cycles p the provided value. The value is
+   * expected to be &gt;= 0. If a negative value is provided, the count is set to zero and a warning
+   * is logged rather than throwing an exception.","[{'comment': ""I don't see the method setting this to zero as described. The code should reflect the description.\r\n(Also, another option to setting it to zero would be to leave it as is, and not set it at all.)"", 'commenter': 'ctubbsii'}, {'comment': 'This method was unused - so I opted to remove it - the not setting to zero was an error on my part.', 'commenter': 'EdColeman'}]"
1409,server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ShowTrace.java,"@@ -45,9 +46,11 @@
   private static final long serialVersionUID = 1L;
   private static final String checkboxIdSuffix = ""_checkbox"";
   private static final String pageLoadFunctionName = ""pageload"";
+  private static final Pattern TRACE_ID_PATTERN = Pattern.compile(""\\p{XDigit}{16}"");
 
   String getTraceId(HttpServletRequest req) {
-    return getStringParameter(req, ""id"", null);
+    final String stringValue = getStringParameter(req, ""id"", null);
+    return TRACE_ID_PATTERN.matcher(stringValue).matches() ? stringValue : null;","[{'comment': 'This looks like it could produce an NPE from the matcher if `stringValue` is null. We probably want to add a check for that:\r\n\r\n```suggestion\r\n    if (stringValue == null) {\r\n      return null;\r\n    }\r\n    return TRACE_ID_PATTERN.matcher(stringValue).matches() ? stringValue : null;\r\n```\r\n', 'commenter': 'ctubbsii'}]"
1409,server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/Basic.java,"@@ -51,13 +51,14 @@
 
   public static String getStringParameter(HttpServletRequest req, String name,
       String defaultValue) {
-    String result = req.getParameter(name).replaceAll(""[^A-Za-z]"", """");
+    final String result = req.getParameter(name);
     if (result == null) {
       return defaultValue;
     }
     return result;
   }
 
+","[{'comment': 'This extra whitespace will probably get deleted when the project is built, due to the formatter plugin. Please run the formatter with `mvn package -DskipTests` and update the PR with any changes.', 'commenter': 'ctubbsii'}, {'comment': 'Will do.', 'commenter': 'reggert'}, {'comment': 'Done.', 'commenter': 'reggert'}]"
1412,core/src/test/java/org/apache/accumulo/core/data/ValueTest.java,"@@ -194,7 +194,7 @@ public void testEquals() {
   @Test
   public void testString() {
     Value v1 = new Value(""abc"");
-    Value v2 = new Value(""abc"".getBytes(UTF_8));
+    Value v2 = new Value(""abc"");","[{'comment': 'This one shouldn\'t be changed, because it\'s explicitly testing the `CharSequence` and `byte[]` equivalence.\r\n```suggestion\r\n    Value v2 = new Value(""abc"".getBytes(UTF_8));\r\n```', 'commenter': 'ctubbsii'}]"
1432,server/monitor/pom.xml,"@@ -36,6 +36,11 @@
       <artifactId>auto-service</artifactId>
       <optional>true</optional>
     </dependency>
+    <dependency>
+      <groupId>com.sun.xml.bind</groupId>
+      <artifactId>jaxb-core</artifactId>
+      <version>2.3.0.1</version>","[{'comment': ""The version should be specified in the the root `pom.xml`'s `<dependencyManagement />` section instead of here, and the dependency here should be listed as `<scope>runtime</scope>`\r\n\r\n(Fixing the scope, may fix the build. I'm not sure.)"", 'commenter': 'ctubbsii'}]"
1432,assemble/pom.xml,"@@ -92,6 +92,12 @@
       <artifactId>protobuf-java</artifactId>
       <optional>true</optional>
     </dependency>
+    <dependency>
+      <groupId>com.sun.xml.bind</groupId>
+      <artifactId>jaxb-core</artifactId>
+      <version>2.3.0.1</version>","[{'comment': ""This version can be omitted if the dependency is listed in the root `pom.xml`'s `<dependencyManagement/>` section."", 'commenter': 'ctubbsii'}, {'comment': 'Thanks @ctubbsii for your comments and suggestions, indeed helpful. I was checking the failure from the logs and I could co-relate the pom conventions. generally I follow https://accumulo.apache.org/contributor/building and run mvn verify -Psunny. For this run probably focus was more on testing it manually after deployment. I am correcting the fix and will be pushing the change with detailed comments. ', 'commenter': 'mandarinamdar'}, {'comment': 'And yes, had manually tested with Uno deployment.\r\n\r\nOutput from /rest/xml as below.(snip)\r\n`<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><stats><servers><server id=""man-dev:9997""><hostname>man-dev:9997</hostname><lastContact>6211</lastContact><responseTime>0</responseTime><osload>0.37</osload><version>2.1.0-SNAPSHOT</version><compactions.........</stats>`\r\n\r\noutput from /rest/json as below. (snip)\r\n`{""servers"":[{""server"":{""hostname"":""man-dev:9997"",""lastContact"":7436,""responseTime"":2,""osload"":0.02,""version"":""2.1.0-SNAPSHOT"",""compactions"".....`', 'commenter': 'mandarinamdar'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -948,6 +948,11 @@ public long startUpdate(TInfo tinfo, TCredentials credentials, TDurability tdura
     }
 
     private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
+
+      int activeThreadCount = Thread.activeCount();","[{'comment': 'A few general questions about using Thread.activeCount() for this situation.  \r\n\r\nWhat thread group would this measure?  From a quick look, I do not see where the calling thread is launched from a specific thread group.  Is this going to return the active threads for the tserver in general? If that would be the case, how would you recommend a ""good"" range for setting the limit?  The number of active threads is a tserver seems like it would be highly variable and complex to reason about and may not be a suitable surrogate for measuring the number of active writers. I don\'t believe that Thread.activeCount() is equivalent to the ThreadPoolExecutor getActiveCount method, and getActiveCount() requires that these writing threads submitted to run in a pool.\r\n\r\nAssuming that Thread.activeCount is returning the number of active tserver threads, there is the potential for a lot of concurrent activity in a tserver, scans, bulk imports,... - would a ""busy"" tserver begin to defer writing mutations even if there were a limited number of writers active?   \r\n \r\nSecond, the javadoc states that \r\n\r\n> This method is intended primarily for debugging and monitoring purposes\r\n\r\nDoes that suggest that it may not be the best choice for run-time decisions?', 'commenter': 'EdColeman'}, {'comment': ""If we're going implement a thread limit for any feature, we should do so with an explicit thread pool, not by monitoring the total number of threads in the process."", 'commenter': 'ctubbsii'}, {'comment': 'Ok, that makes sense. Could you elaborate on how I would do so?', 'commenter': 'rcarterjr'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1005,19 +1012,37 @@ private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
         updateMetrics.addPermissionErrors(0);
         return;
       }
+
     }
 
     @Override
     public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
-        List<TMutation> tmutations) {
+        List<TMutation> tmutations) throws TException {
       UpdateSession us = (UpdateSession) sessionManager.reserveSession(updateID);
       if (us == null) {
         return;
       }
 
       boolean reserved = true;
+      boolean allowWriteThreadSemaphore = false;
       try {
         KeyExtent keyExtent = new KeyExtent(tkeyExtent);
+        maxThreads = getServerConfig().getConfiguration().getCount(Property.TSERV_MAX_WRITETHREADS);
+        maxThreadPermits = maxThreads == 0 ? Integer.MAX_VALUE : maxThreads;
+        sem = new Semaphore(maxThreadPermits);","[{'comment': 'The semaphore should be shared between function calls and thread.  This is continually creating new semaphores, so different threads will have different semaphores. With this pattern the semaphore object aquired from could change before the thread returns.  I I think it should be created once for the tserver.', 'commenter': 'keith-turner'}, {'comment': 'Ok, I can move the semaphore to the constructor. I had done that initially, but I was thinking that we may want to change the maxThreadPermits property value without restarting the tablet server.', 'commenter': 'rcarterjr'}, {'comment': 'I moved the semaphore to a synchronized getter method that only allows for one creation in my latest commit.\xa0', 'commenter': 'rcarterjr'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1005,19 +1012,37 @@ private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
         updateMetrics.addPermissionErrors(0);
         return;
       }
+
     }
 
     @Override
     public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
-        List<TMutation> tmutations) {
+        List<TMutation> tmutations) throws TException {
       UpdateSession us = (UpdateSession) sessionManager.reserveSession(updateID);
       if (us == null) {
         return;
       }
 
       boolean reserved = true;
+      boolean allowWriteThreadSemaphore = false;
       try {
         KeyExtent keyExtent = new KeyExtent(tkeyExtent);
+        maxThreads = getServerConfig().getConfiguration().getCount(Property.TSERV_MAX_WRITETHREADS);
+        maxThreadPermits = maxThreads == 0 ? Integer.MAX_VALUE : maxThreads;
+        sem = new Semaphore(maxThreadPermits);
+
+        if (TabletType.type(keyExtent) == TabletType.USER) {
+          if (!sem.tryAcquire()) {
+            us.failures.put(keyExtent, 0L);
+            updateMetrics.addUnknownTabletErrors(0);
+            log.error(""Mutation failed - No more threads available for mutations at "" + new Date());","[{'comment': 'Seems like this should throw an exception or return?  Not sure which, but it doesn;t seem like it should keep executing the rest of the method.', 'commenter': 'keith-turner'}, {'comment': 'I now throw a TException here so we can have feedback when a mutation fails. ', 'commenter': 'rcarterjr'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -495,6 +501,14 @@ private static long jitter() {
     // add a random 10% wait
     return (long) ((1. + (r.nextDouble() / 10)) * TabletServer.TIME_BETWEEN_LOCATOR_CACHE_CLEARS);
   }
+  
+    //synchronized method to control simultaneous access
+  synchronized private Semaphore getSemaphore() {
+    if (sem == null) {","[{'comment': 'to avoid some synchronization here, try something like this:\r\n```\r\n private Semaphore getSemaphore() {\r\n  if (sem == null) {\r\n    synchronized(this) {\r\n      if (sem == null) {\r\n        sem = new Semaphore(maxThreadPermits);\r\n      }\r\n    }\r\n    return sem;\r\n  }\r\n```', 'commenter': 'ivakegg'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -355,6 +356,11 @@ public TabletServerMinCMetrics getMinCMetrics() {
   private final ZooAuthenticationKeyWatcher authKeyWatcher;
   private final WalStateManager walMarker;
 
+  private int maxThreads =
+      getServerConfig().getConfiguration().getCount(Property.TSERV_MAX_WRITETHREADS);
+  private int maxThreadPermits = maxThreads == 0 ? Integer.MAX_VALUE : maxThreads;","[{'comment': 'I would really like to be able to set this property dynamically (i.e. while the tservers are running).  Perhaps in the getSemaphore method we could periodically (once every 5 seconds or so) check the TSERV_MAX_WRITETHREADS property and then recreate the Semaphore with the new max and copy in the current value.  This could be a separate ticket however.', 'commenter': 'ivakegg'}, {'comment': 'One way to do what I suggest is to rewrite the getSemaphore method as follows:\r\n```\r\n  private Semaphore getSemaphore() {\r\n    int writeThreads = getServerConfig().getConfiguration().getCount(Property.TSERV_MAX_WRITETHREADS);\r\n    if (writeThreads == 0) writeThreads = Integer.MAX_VALUE;\r\n    if (sem == null || maxThreadPermits != writeThreads) {\r\n      synchronized (this) {\r\n        maxThreadPermits = writeThreads;\r\n        sem = new Semaphore(maxThreadPermits);\r\n      }\r\n    }\r\n    return sem;\r\n  }\r\n```\r\n\r\nThen down below where you get the semaphore, preserve the semaphore that you use for the tryAcquire and use the same one for the release.', 'commenter': 'ivakegg'}, {'comment': 'The semaphoreAcquired boolean could be replaced with the Semaphore instance instead, then instead of ""if (semaphoreAcquired)"" it would be ""if (semaphore != null)"".\r\n', 'commenter': 'ivakegg'}, {'comment': 'my only problem with this is the getServerConfig().getConfiguration().getCount(property) may be too expensive to do every time.  Adding a counter or a time mechanism may be required there.', 'commenter': 'ivakegg'}, {'comment': 'Alternatively we add a timer that will check the property every N seconds and update the semaphore as required.', 'commenter': 'ivakegg'}, {'comment': 'if it is not too expensive then this simple solution may suffice.', 'commenter': 'ivakegg'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1011,19 +1026,32 @@ private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
         updateMetrics.addPermissionErrors(0);
         return;
       }
+
     }
 
     @Override
     public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
-        List<TMutation> tmutations) {
+        List<TMutation> tmutations) throws TException {
       UpdateSession us = (UpdateSession) sessionManager.reserveSession(updateID);
       if (us == null) {
         return;
       }
 
       boolean reserved = true;
+      boolean allowWriteThreadSemaphore = false;","[{'comment': 'minor nit: call this something like semaphoreAcquired.  Then the check at line 1083 would be if (semaphoreAcquired) which would be much more readable.', 'commenter': 'ivakegg'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1011,19 +1030,33 @@ private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
         updateMetrics.addPermissionErrors(0);
         return;
       }
+
     }
 
     @Override
     public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
-        List<TMutation> tmutations) {
+        List<TMutation> tmutations) throws TException {
       UpdateSession us = (UpdateSession) sessionManager.reserveSession(updateID);
       if (us == null) {
         return;
       }
 
+      Semaphore semaphoreCopy = getSemaphore();
       boolean reserved = true;
+      boolean semaphoreAcquired = false;","[{'comment': 'This boolean is no longer needed.', 'commenter': 'ivakegg'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1011,19 +1030,33 @@ private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
         updateMetrics.addPermissionErrors(0);
         return;
       }
+
     }
 
     @Override
     public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
-        List<TMutation> tmutations) {
+        List<TMutation> tmutations) throws TException {
       UpdateSession us = (UpdateSession) sessionManager.reserveSession(updateID);
       if (us == null) {
         return;
       }
 
+      Semaphore semaphoreCopy = getSemaphore();","[{'comment': 'Move this allocation just before the ""if (!semaphoreCopy.tryAcquire())""', 'commenter': 'ivakegg'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1052,10 +1085,13 @@ public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
           }
         }
       } finally {
+        if (semaphoreAcquired)","[{'comment': 'This can now be ""if (semaphoreCopy != null)""', 'commenter': 'ivakegg'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1011,19 +1030,33 @@ private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
         updateMetrics.addPermissionErrors(0);
         return;
       }
+
     }
 
     @Override
     public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
-        List<TMutation> tmutations) {
+        List<TMutation> tmutations) throws TException {
       UpdateSession us = (UpdateSession) sessionManager.reserveSession(updateID);
       if (us == null) {
         return;
       }
 
+      Semaphore semaphoreCopy = getSemaphore();
       boolean reserved = true;
+      boolean semaphoreAcquired = false;
       try {
         KeyExtent keyExtent = new KeyExtent(tkeyExtent);
+
+        if (TabletType.type(keyExtent) == TabletType.USER) {
+          if (!semaphoreCopy.tryAcquire()) {
+            throw new TException(""Mutation failed. No threads available."");
+          } else {
+            semaphoreAcquired = true;","[{'comment': 'This line can be removed', 'commenter': 'ivakegg'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -355,6 +356,11 @@ public TabletServerMinCMetrics getMinCMetrics() {
   private final ZooAuthenticationKeyWatcher authKeyWatcher;
   private final WalStateManager walMarker;
 
+  private int maxThreads =","[{'comment': 'no longer need maxThreads.  initialize maxThreadPermits to 0.', 'commenter': 'ivakegg'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1011,19 +1030,33 @@ private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
         updateMetrics.addPermissionErrors(0);
         return;
       }
+
     }
 
     @Override
     public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
-        List<TMutation> tmutations) {
+        List<TMutation> tmutations) throws TException {
       UpdateSession us = (UpdateSession) sessionManager.reserveSession(updateID);
       if (us == null) {
         return;
       }
 
+      Semaphore semaphoreCopy = getSemaphore();
       boolean reserved = true;
+      boolean semaphoreAcquired = false;
       try {
         KeyExtent keyExtent = new KeyExtent(tkeyExtent);
+
+        if (TabletType.type(keyExtent) == TabletType.USER) {
+          if (!semaphoreCopy.tryAcquire()) {
+            throw new TException(""Mutation failed. No threads available."");","[{'comment': 'We could consider having a specialized thrift exception that is thrown to indicate resource exhaustion on tserver.  When the batch writer sees this it could retry with exponential backoff up to its configured time-out.', 'commenter': 'keith-turner'}]"
1487,server/tserver/src/main/java/org/apache/accumulo/tserver/ThriftClientHandler.java,"@@ -668,15 +669,27 @@ private void setUpdateTablet(UpdateSession us, KeyExtent keyExtent) {
 
   @Override
   public void applyUpdates(TInfo tinfo, long updateID, TKeyExtent tkeyExtent,
-      List<TMutation> tmutations) {
+      List<TMutation> tmutations) throws TException {
     UpdateSession us = (UpdateSession) server.sessionManager.reserveSession(updateID);
     if (us == null) {
       return;
     }
 
+    Semaphore semaphoreCopy = null;
     boolean reserved = true;
+
     try {
       KeyExtent keyExtent = new KeyExtent(tkeyExtent);
+
+      if (TabletType.type(keyExtent) == TabletType.USER) {
+        semaphoreCopy = server.getSemaphore();
+        if (!semaphoreCopy.tryAcquire()) {
+          throw new TException(""Mutation failed. No threads available."");
+        } else {
+          log.info(""Available permits: {}"", semaphoreCopy.availablePermits());","[{'comment': 'This will get logged at ever mutation.  Perhaps this should be a log.trace instead.', 'commenter': 'ivakegg'}]"
1487,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -381,6 +381,9 @@
   TSERV_MEM_MGMT(""tserver.memory.manager"",
       ""org.apache.accumulo.server.tabletserver.LargestFirstMemoryManager"", PropertyType.CLASSNAME,
       ""An implementation of MemoryManger that accumulo will use.""),
+  TSERV_MAX_WRITETHREADS(""tserver.max.writethreads"", ""0"", PropertyType.COUNT,","[{'comment': 'I would suggest changing the property name to something like the following.  Going from general to specific in the property name allows like properties to sort together.\r\n\r\n```suggestion\r\n  TSERV_MAX_WRITETHREADS(""tserver.write.threads.max"", ""0"", PropertyType.COUNT,\r\n```', 'commenter': 'keith-turner'}, {'comment': '+1', 'commenter': 'ivakegg'}]"
1489,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooSession.java,"@@ -119,8 +119,19 @@ public static ZooKeeper connect(String host, int timeout, String scheme, byte[]
     boolean tryAgain = true;
     long sleepTime = 100;
     ZooKeeper zooKeeper = null;
-
-    long startTime = System.currentTimeMillis();
+    /*
+     * Originally, startTime = System.currentTimeMillis(). Changed to System.nanoTime() to more
+     * accurately compute durations because it is not based on system clock variations. The
+     * ZooKeeper method signature expects an int value for 'timeout' and performs several
+     * calculations that can result in Numeric Expression Overflow errors if large, nanosecond
+     * values are used. For ths reason, System.nanoTime() is is converted to MS units prior to being
+     * used in calculations. Although, the resolution of 'startTime' is still in the MS range, the
+     * value from which it is calculated is nanoTime. Also, the MS units are preserved in the
+     * original method code, without the need for conversion. TimeUnit.convert is not used because
+     * ""conversions from fine to coarser granularities truncate, so lose precision"" (Oracle Java 7
+     * API)
+     */","[{'comment': ""I don't think any of this is necessary to keep in the code, since it is explaining why the code was changed, and not explaining the code that is there at the end."", 'commenter': 'ctubbsii'}]"
1489,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooSession.java,"@@ -119,8 +119,19 @@ public static ZooKeeper connect(String host, int timeout, String scheme, byte[]
     boolean tryAgain = true;
     long sleepTime = 100;
     ZooKeeper zooKeeper = null;
-
-    long startTime = System.currentTimeMillis();
+    /*
+     * Originally, startTime = System.currentTimeMillis(). Changed to System.nanoTime() to more
+     * accurately compute durations because it is not based on system clock variations. The
+     * ZooKeeper method signature expects an int value for 'timeout' and performs several
+     * calculations that can result in Numeric Expression Overflow errors if large, nanosecond
+     * values are used. For ths reason, System.nanoTime() is is converted to MS units prior to being
+     * used in calculations. Although, the resolution of 'startTime' is still in the MS range, the
+     * value from which it is calculated is nanoTime. Also, the MS units are preserved in the
+     * original method code, without the need for conversion. TimeUnit.convert is not used because
+     * ""conversions from fine to coarser granularities truncate, so lose precision"" (Oracle Java 7
+     * API)
+     */
+    long startTime = System.nanoTime() / 1000000;","[{'comment': ""It's not a good idea to divide here, since it is the differences between two nano-times that matter, and you are losing precision here in a way that could affect the subtraction later.\r\n\r\nThe correct pattern to do the conversion to millis would be something like this:\r\n\r\n```java\r\nlong startTime = System.nanoTime();\r\n// code here...\r\nlong laterTime = System.nanoTime();\r\nlong timeDurationMillis = TimeUnit.NANOSECONDS.toMillis(laterTime - startTime);\r\n```\r\n\r\nIt is okay to use the TimeUnit conversions here, and lose precision, because it's okay to lose the precision in the difference. It is not okay to lose precision on the values before computing the difference in times. (Also, using this method to convert is the same as dividing... they both lose precision in the same way.)"", 'commenter': 'ctubbsii'}, {'comment': 'Indeed, that makes more sense. Working the issue along those lines, there are two sections in ZooSession where calculations are performed using millis values for all the operands:\r\n\r\n```java\r\n      if (System.currentTimeMillis() - startTime > 2L * timeout) {\r\n        throw new RuntimeException(""Failed to connect to zookeeper ("" + host\r\n            + "") within 2x zookeeper timeout period "" + timeout);\r\n      }\r\n```\r\n\r\n```java\r\n      if (tryAgain) {\r\n        if (startTime + 2L * timeout < System.currentTimeMillis() + sleepTime + connectTimeWait)\r\n          sleepTime = startTime + 2L * timeout - System.currentTimeMillis() - connectTimeWait;\r\n        if (sleepTime < 0) {\r\n          connectTimeWait -= sleepTime;\r\n          sleepTime = 0;\r\n        }\r\n        UtilWaitThread.sleep(sleepTime);\r\n        if (sleepTime < 10000)\r\n          sleepTime = sleepTime + (long) (sleepTime * secureRandom.nextDouble());\r\n      }\r\n```\r\n\r\nOne solution is to convert from millis to nano when the values are first assigned. There are a couple issues I see with doing that. One, timeout is still being passed into the method in millis and would need to be converted at the point-of-use (or assigned a value within the method, which breaks convention). Two, there are calculations that use these values, but do not use startTime, that are already working correctly. Also, TIME_BETWEEN_CONNECT_CHECKS_MS would likely need to be refactored to ...-NS.\r\n\r\nAnother solution (pasted below) is to  do conversions on the operands. This will prevent the Numeric Overflow Errors, but kind of decreases the readability of the code. Scaling up the operand values might not make them more accurate, but we should not _lose_ accuracy this way.\r\n\r\n```java\r\n      if (TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime) > 2L * timeout) {\r\n        throw new RuntimeException(""Failed to connect to zookeeper ("" + host\r\n            + "") within 2x zookeeper timeout period "" + timeout);\r\n      }\r\n```\r\n```java\r\n    if (tryAgain) {\r\n        if (TimeUnit.NANOSECONDS.toMillis(startTime + 2L * TimeUnit.MILLISECONDS.toNanos(timeout))\r\n            < TimeUnit.NANOSECONDS\r\n                .toMillis(System.nanoTime() + TimeUnit.MILLISECONDS.toNanos(sleepTime)\r\n                    + TimeUnit.MILLISECONDS.toNanos(connectTimeWait))) {\r\n          sleepTime =\r\n              TimeUnit.NANOSECONDS.toMillis(startTime + 2L * TimeUnit.MILLISECONDS.toNanos(timeout)\r\n                  - System.nanoTime() - TimeUnit.MILLISECONDS.toNanos(connectTimeWait));\r\n        }\r\n```\r\n\r\n\r\n\r\n\r\n\r\n', 'commenter': 'cradal'}, {'comment': ""@cradal I updated your comment's Markdown to use syntax highlighting, to make it easier to read, but I think it'd be easier if you just updated the PR with the new changes you are proposing, rather than paste it in to a comment. It will be easier to review the proposed changes inline, rather than in a discussion thread. Also, if you want to make your code more readable, you could statically import TimeUnit.NANOSECONDS and TimeUnit.MILLISECONDS."", 'commenter': 'ctubbsii'}]"
1541,core/src/main/java/org/apache/accumulo/core/client/admin/DiskUsage.java,"@@ -47,9 +48,9 @@ public boolean equals(Object o) {
 
     DiskUsage diskUsage = (DiskUsage) o;
 
-    if (tables != null ? !tables.equals(diskUsage.tables) : diskUsage.tables != null)
+    if (!Objects.equals(tables, diskUsage.tables))
       return false;
-    return usage != null ? usage.equals(diskUsage.usage) : diskUsage.usage == null;
+    return Objects.equals(usage, diskUsage.usage);","[{'comment': 'This could be reduced further using AND, something like:\r\n`return Objects.equals(tables, diskUsage.tables) && Objects.equals(usage, diskUsage.usage);`\r\n\r\nAlso, any thought to replacing hashCode at the same time to use Objects.hash(tables, usage); - that might help with the constraint that equals and hashCode are consistent.', 'commenter': 'EdColeman'}, {'comment': 'Thanks @EdColeman . Updated to add further simplification and updated hashCode.', 'commenter': 'jmark99'}, {'comment': 'I think the same thing could apply to the other 2 classes.', 'commenter': 'EdColeman'}, {'comment': '@EdColeman pushed up changes to the other two classes.', 'commenter': 'jmark99'}]"
1541,core/src/main/java/org/apache/accumulo/core/data/Condition.java,"@@ -308,41 +309,20 @@ public Condition setIterators(IteratorSetting... iterators) {
 
   @Override
   public boolean equals(Object o) {
-    if (o == this) {
+    if (this == o)
       return true;
-    }
-    if (o == null || !(o instanceof Condition)) {
-      return false;
-    }
-    Condition c = (Condition) o;
-    if (!(c.cf.equals(cf))) {
-      return false;
-    }
-    if (!(c.cq.equals(cq))) {
-      return false;
-    }
-    if (!(c.cv.equals(cv))) {
+    if (o == null || getClass() != o.getClass())","[{'comment': ""This was changed from `instanceof` to the use of `getClass()`, which is not the same. The latter is more narrow. I think it should be changed back to `instanceof`, although the check for null would be redundant, since `instanceof` is always false if it's null."", 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii, I reverted the aforementioned check back to its original form. Thanks.', 'commenter': 'jmark99'}]"
1541,core/src/main/java/org/apache/accumulo/core/data/Condition.java,"@@ -308,41 +309,20 @@ public Condition setIterators(IteratorSetting... iterators) {
 
   @Override
   public boolean equals(Object o) {
-    if (o == this) {
+    if (this == o)
       return true;
-    }
-    if (o == null || !(o instanceof Condition)) {
-      return false;
-    }
-    Condition c = (Condition) o;
-    if (!(c.cf.equals(cf))) {
-      return false;
-    }
-    if (!(c.cq.equals(cq))) {
-      return false;
-    }
-    if (!(c.cv.equals(cv))) {
+    if (o == null || !(o instanceof Condition))","[{'comment': 'This could be simplified to:\r\n\r\n```suggestion\r\n    if (!(o instanceof Condition))\r\n```', 'commenter': 'ctubbsii'}]"
1552,test/src/main/java/org/apache/accumulo/test/TransportCachingIT.java,"@@ -18,87 +18,53 @@
  */
 package org.apache.accumulo.test;
 
-import static java.nio.charset.StandardCharsets.UTF_8;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNotSame;
 import static org.junit.Assert.assertSame;
 
-import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.accumulo.core.Constants;
 import org.apache.accumulo.core.client.Accumulo;
 import org.apache.accumulo.core.client.AccumuloClient;
 import org.apache.accumulo.core.clientImpl.ClientContext;
 import org.apache.accumulo.core.clientImpl.ThriftTransportKey;
 import org.apache.accumulo.core.clientImpl.ThriftTransportPool;
 import org.apache.accumulo.core.conf.ConfigurationTypeHelper;
 import org.apache.accumulo.core.conf.Property;
-import org.apache.accumulo.core.util.ServerServices;
-import org.apache.accumulo.core.util.ServerServices.Service;
-import org.apache.accumulo.fate.zookeeper.ZooCache;
+import org.apache.accumulo.core.util.HostAndPort;
 import org.apache.accumulo.harness.AccumuloClusterHarness;
 import org.apache.thrift.transport.TTransport;
 import org.apache.thrift.transport.TTransportException;
 import org.junit.Test;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.google.common.collect.Lists;
+
 /**
  * Test that {@link ThriftTransportPool} actually adheres to the cachedConnection argument
  */
 public class TransportCachingIT extends AccumuloClusterHarness {
   private static final Logger log = LoggerFactory.getLogger(TransportCachingIT.class);
-  private static int ATTEMPTS = 0;
 
   @Test
   public void testCachedTransport() throws InterruptedException {
     try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {
-      while (client.instanceOperations().getTabletServers().isEmpty()) {
+
+      List<String> tservers;
+
+      while ((tservers = client.instanceOperations().getTabletServers()).isEmpty()) {","[{'comment': 'Instead of waiting for non-empty, could wait for the expected number to start (I think mini starts up 2 by default, but this number can probably be retrieved from the config).', 'commenter': 'ctubbsii'}, {'comment': ""I don't think the test needs more than one tserver."", 'commenter': 'keith-turner'}, {'comment': ""In that case, we should override mini's config to only start up one."", 'commenter': 'ctubbsii'}, {'comment': 'I think two are fine now.  I would prefer for test to not have config unless needed.', 'commenter': 'keith-turner'}, {'comment': ""That's fair."", 'commenter': 'ctubbsii'}]"
1552,test/src/main/java/org/apache/accumulo/test/TransportCachingIT.java,"@@ -18,87 +18,53 @@
  */
 package org.apache.accumulo.test;
 
-import static java.nio.charset.StandardCharsets.UTF_8;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNotSame;
 import static org.junit.Assert.assertSame;
 
-import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.accumulo.core.Constants;
 import org.apache.accumulo.core.client.Accumulo;
 import org.apache.accumulo.core.client.AccumuloClient;
 import org.apache.accumulo.core.clientImpl.ClientContext;
 import org.apache.accumulo.core.clientImpl.ThriftTransportKey;
 import org.apache.accumulo.core.clientImpl.ThriftTransportPool;
 import org.apache.accumulo.core.conf.ConfigurationTypeHelper;
 import org.apache.accumulo.core.conf.Property;
-import org.apache.accumulo.core.util.ServerServices;
-import org.apache.accumulo.core.util.ServerServices.Service;
-import org.apache.accumulo.fate.zookeeper.ZooCache;
+import org.apache.accumulo.core.util.HostAndPort;
 import org.apache.accumulo.harness.AccumuloClusterHarness;
 import org.apache.thrift.transport.TTransport;
 import org.apache.thrift.transport.TTransportException;
 import org.junit.Test;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.google.common.collect.Lists;
+
 /**
  * Test that {@link ThriftTransportPool} actually adheres to the cachedConnection argument
  */
 public class TransportCachingIT extends AccumuloClusterHarness {
   private static final Logger log = LoggerFactory.getLogger(TransportCachingIT.class);
-  private static int ATTEMPTS = 0;
 
   @Test
   public void testCachedTransport() throws InterruptedException {
     try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {
-      while (client.instanceOperations().getTabletServers().isEmpty()) {
+
+      List<String> tservers;
+
+      while ((tservers = client.instanceOperations().getTabletServers()).isEmpty()) {
         // sleep until a tablet server is up
         Thread.sleep(50);
       }
+
       ClientContext context = (ClientContext) client;
       long rpcTimeout =
           ConfigurationTypeHelper.getTimeInMillis(Property.GENERAL_RPC_TIMEOUT.getDefaultValue());
 
-      ZooCache zc = context.getZooCache();
-      final String zkRoot = context.getZooKeeperRoot();
-
-      // wait until Zookeeper is populated
-      List<String> children = zc.getChildren(zkRoot + Constants.ZTSERVERS);
-      while (children.isEmpty()) {
-        Thread.sleep(100);
-        children = zc.getChildren(zkRoot + Constants.ZTSERVERS);
-      }
-
-      ArrayList<ThriftTransportKey> servers = new ArrayList<>();
-      while (servers.isEmpty()) {
-        for (String tserver : children) {
-          String path = zkRoot + Constants.ZTSERVERS + ""/"" + tserver;
-          byte[] data = zc.getLockData(path);
-          if (data != null) {
-            String strData = new String(data, UTF_8);
-            if (!strData.equals(""master""))
-              servers.add(new ThriftTransportKey(
-                  new ServerServices(strData).getAddress(Service.TSERV_CLIENT), rpcTimeout,
-                  context));
-          }
-        }
-        ATTEMPTS++;
-        if (!servers.isEmpty())
-          break;
-        else {
-          if (ATTEMPTS < 100) {
-            log.warn(""Making another attempt to add ThriftTransportKey servers"");
-            Thread.sleep(100);
-          } else {
-            log.error(""Failed to add ThriftTransportKey servers - Failing TransportCachingIT test"");
-            org.junit.Assert
-                .fail(""Failed to add ThriftTransportKey servers - Failing TransportCachingIT test"");
-          }
-        }
-      }
+      List<ThriftTransportKey> servers = Lists.transform(tservers,
+          serverStr -> new ThriftTransportKey(HostAndPort.fromString(serverStr), rpcTimeout,
+              context));","[{'comment': ""This might snag on the modernizer plugin. Even if it doesn't, I think the pure Java streams form looks a little cleaner (or maybe I've just been using streams a lot to have acquired an affinity for them):\r\n\r\n```suggestion\r\n      List<ThriftTransportKey> servers = tservers.stream().map(serverStr -> {\r\n        return new ThriftTransportKey(HostAndPort.fromString(serverStr), rpcTimeout, context);\r\n      }).collect(Collectors.toList());\r\n```\r\n(not necessarily formatted)"", 'commenter': 'ctubbsii'}, {'comment': 'I doubt modernizer would complain because there is no exact analogue.  Using streams requires copying/allocating a new list where Lists.transform does not.  However in this case performance does not matter so using streams is better.', 'commenter': 'keith-turner'}]"
1552,test/src/main/java/org/apache/accumulo/test/TransportCachingIT.java,"@@ -158,7 +124,7 @@ public void testCachedTransport() throws InterruptedException {
       while (fifth == null) {
         try {
           // Get a cached transport
-          fifth = pool.getAnyTransport(servers, true).getSecond();
+          fifth = pool.getAnyTransport(servers.subList(0, 1), true).getSecond();","[{'comment': 'Do we want this to loop indefinitely? If this is null, could it ever become non-null?', 'commenter': 'ctubbsii'}, {'comment': ""The looping seemed kinda sketchy to me, but I didn't look into it in detail."", 'commenter': 'keith-turner'}, {'comment': 'If on the first attempt it gets a TTransportException and on the 2nd attempt it succeeds then it seems like it could become non-null, right?  So maybe the looping is not so sketchy after all.', 'commenter': 'keith-turner'}, {'comment': ""I'm not really sure. I was confused by what the test was trying to do, and the possible outcomes it was trying to account for with the loop, since none of that is documented :smiley_cat:\r\nI will defer to you."", 'commenter': 'ctubbsii'}]"
1552,test/src/main/java/org/apache/accumulo/test/TransportCachingIT.java,"@@ -18,87 +18,53 @@
  */
 package org.apache.accumulo.test;
 
-import static java.nio.charset.StandardCharsets.UTF_8;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNotSame;
 import static org.junit.Assert.assertSame;
 
-import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.accumulo.core.Constants;
 import org.apache.accumulo.core.client.Accumulo;
 import org.apache.accumulo.core.client.AccumuloClient;
 import org.apache.accumulo.core.clientImpl.ClientContext;
 import org.apache.accumulo.core.clientImpl.ThriftTransportKey;
 import org.apache.accumulo.core.clientImpl.ThriftTransportPool;
 import org.apache.accumulo.core.conf.ConfigurationTypeHelper;
 import org.apache.accumulo.core.conf.Property;
-import org.apache.accumulo.core.util.ServerServices;
-import org.apache.accumulo.core.util.ServerServices.Service;
-import org.apache.accumulo.fate.zookeeper.ZooCache;
+import org.apache.accumulo.core.util.HostAndPort;
 import org.apache.accumulo.harness.AccumuloClusterHarness;
 import org.apache.thrift.transport.TTransport;
 import org.apache.thrift.transport.TTransportException;
 import org.junit.Test;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.google.common.collect.Lists;","[{'comment': 'I think the imports need to be cleaned up after applying the suggested change to use Java streams. After that, this PR should be good to merge.', 'commenter': 'ctubbsii'}]"
1552,test/src/main/java/org/apache/accumulo/test/TransportCachingIT.java,"@@ -148,7 +114,7 @@ public void testCachedTransport() throws InterruptedException {
       while (fourth == null) {
         try {
           // Get a non-cached transport
-          fourth = pool.getAnyTransport(servers, false).getSecond();
+          fourth = pool.getAnyTransport(servers.subList(0, 1), false).getSecond();","[{'comment': ""How does getting the same transport from a list of one test anything?  The last ```assertSame``` now doesn't seem like it is testing anything."", 'commenter': 'milleruntime'}, {'comment': ""I think the test is similar between the first/second. Where those were checking basic caching behavior, this one was specifically testing LIFO behavior, as mentioned in the comment above: *// ensure the LIFO scheme with a fourth and fifth entry*\r\n\r\nHowever, there are still some nuances I don't understand. Like this fourth seems like it is supposed to be for the same server as one of first, second, or third. Fifth is supposed to make sure that it matches fourth, and not the same server from first/second/third (LIFO behavior). However, because we have more than one server, it doesn't seem like there's any guarantee that fourth/fifth refer to the same server as one of first/second/third, so this test might occasionally not test LIFO behavior, and basically do the same as first/second (basic caching) but for a different server than that one chose.\r\n\r\nI think some of this can be resolved if we select `servers.subList(0,1)` at the beginning of the test, and stick with that one the whole time, so we make sure we're testing with the same server."", 'commenter': 'ctubbsii'}, {'comment': 'I made the following improvements to the test\r\n\r\n- Use servers.subList(0,1) for everything\r\n- Strengthened the LIFO check by pulling three things off to check for LIFO order instead of just one\r\n- Pulled the redundant while loops into a function\r\n- Improved the comments', 'commenter': 'keith-turner'}]"
1552,test/src/main/java/org/apache/accumulo/test/TransportCachingIT.java,"@@ -49,122 +45,78 @@
  */
 public class TransportCachingIT extends AccumuloClusterHarness {
   private static final Logger log = LoggerFactory.getLogger(TransportCachingIT.class);
-  private static int ATTEMPTS = 0;
 
   @Test
   public void testCachedTransport() throws InterruptedException {
     try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {
-      while (client.instanceOperations().getTabletServers().isEmpty()) {
+
+      List<String> tservers;
+
+      while ((tservers = client.instanceOperations().getTabletServers()).isEmpty()) {
         // sleep until a tablet server is up
         Thread.sleep(50);
       }
+
       ClientContext context = (ClientContext) client;
       long rpcTimeout =
           ConfigurationTypeHelper.getTimeInMillis(Property.GENERAL_RPC_TIMEOUT.getDefaultValue());
 
-      ZooCache zc = context.getZooCache();
-      final String zkRoot = context.getZooKeeperRoot();
+      List<ThriftTransportKey> servers = tservers.stream().map(serverStr -> {
+        return new ThriftTransportKey(HostAndPort.fromString(serverStr), rpcTimeout, context);
+      }).collect(Collectors.toList());
 
-      // wait until Zookeeper is populated
-      List<String> children = zc.getChildren(zkRoot + Constants.ZTSERVERS);
-      while (children.isEmpty()) {
-        Thread.sleep(100);
-        children = zc.getChildren(zkRoot + Constants.ZTSERVERS);
-      }
-
-      ArrayList<ThriftTransportKey> servers = new ArrayList<>();
-      while (servers.isEmpty()) {
-        for (String tserver : children) {
-          String path = zkRoot + Constants.ZTSERVERS + ""/"" + tserver;
-          byte[] data = zc.getLockData(path);
-          if (data != null) {
-            String strData = new String(data, UTF_8);
-            if (!strData.equals(""master""))
-              servers.add(new ThriftTransportKey(
-                  new ServerServices(strData).getAddress(Service.TSERV_CLIENT), rpcTimeout,
-                  context));
-          }
-        }
-        ATTEMPTS++;
-        if (!servers.isEmpty())
-          break;
-        else {
-          if (ATTEMPTS < 100) {
-            log.warn(""Making another attempt to add ThriftTransportKey servers"");
-            Thread.sleep(100);
-          } else {
-            log.error(""Failed to add ThriftTransportKey servers - Failing TransportCachingIT test"");
-            org.junit.Assert
-                .fail(""Failed to add ThriftTransportKey servers - Failing TransportCachingIT test"");
-          }
-        }
-      }
+      // only want to use one server for all subsequent test
+      servers = servers.subList(0, 1);
 
       ThriftTransportPool pool = ThriftTransportPool.getInstance();
-      TTransport first = null;
-      while (first == null) {
-        try {
-          // Get a transport (cached or not)
-          first = pool.getAnyTransport(servers, true).getSecond();
-        } catch (TTransportException e) {
-          log.warn(""Failed to obtain transport to {}"", servers);
-        }
-      }
+      TTransport first = getAnyTransport(servers, pool, true);
 
       assertNotNull(first);
       // Return it to unreserve it
       pool.returnTransport(first);
 
-      TTransport second = null;
-      while (second == null) {
-        try {
-          // Get a cached transport (should be the first)
-          second = pool.getAnyTransport(servers, true).getSecond();
-        } catch (TTransportException e) {
-          log.warn(""Failed obtain 2nd transport to {}"", servers);
-        }
-      }
+      TTransport second = getAnyTransport(servers, pool, true);
 
       // We should get the same transport
       assertSame(""Expected the first and second to be the same instance"", first, second);
-      // Return the 2nd
       pool.returnTransport(second);
 
-      TTransport third = null;
-      while (third == null) {
-        try {
-          // Get a non-cached transport
-          third = pool.getAnyTransport(servers, false).getSecond();
-        } catch (TTransportException e) {
-          log.warn(""Failed obtain 3rd transport to {}"", servers);
-        }
-      }
-
+      // Ensure does not get cached connection just returned
+      TTransport third = getAnyTransport(servers, pool, false);
       assertNotSame(""Expected second and third transport to be different instances"", second, third);
-      pool.returnTransport(third);
 
-      // ensure the LIFO scheme with a fourth and fifth entry
-      TTransport fourth = null;
-      while (fourth == null) {
-        try {
-          // Get a non-cached transport
-          fourth = pool.getAnyTransport(servers, false).getSecond();
-        } catch (TTransportException e) {
-          log.warn(""Failed obtain 4th transport to {}"", servers);
-        }
-      }
+      TTransport fourth = getAnyTransport(servers, pool, false);
+      assertNotSame(""Expected third and fourth transport to be different instances"", third, fourth);
+
+      pool.returnTransport(third);
       pool.returnTransport(fourth);
-      TTransport fifth = null;
-      while (fifth == null) {
-        try {
-          // Get a cached transport
-          fifth = pool.getAnyTransport(servers, true).getSecond();
-        } catch (TTransportException e) {
-          log.warn(""Failed obtain 5th transport to {}"", servers);
-        }
-      }
+
+      // The following three asserts ensure the per server queue is LIFO
+      TTransport fifth = getAnyTransport(servers, pool, true);
       assertSame(""Expected fourth and fifth transport to be the same instance"", fourth, fifth);
+
+      TTransport sixth = getAnyTransport(servers, pool, true);
+      assertSame(""Expected third and sixth transport to be the same instance"", third, sixth);
+
+      TTransport seventh = getAnyTransport(servers, pool, true);
+      assertSame(""Expected third and sixth transport to be the same instance"", second, seventh);","[{'comment': 'Comment is incorrect:\r\n```suggestion\r\n      assertSame(""Expected second and seventh transport to be the same instance"", second, seventh);\r\n```', 'commenter': 'ctubbsii'}]"
1556,test/src/main/java/org/apache/accumulo/test/functional/ManyWriteAheadLogsIT.java,"@@ -198,16 +198,33 @@ public void testMany() throws Exception {
   }
 
   private void addOpenWals(ServerContext c, Set<String> allWalsSeen) throws Exception {
-    Map<String,WalState> wals = WALSunnyDayIT._getWals(c);
-    Set<Entry<String,WalState>> es = wals.entrySet();
+
     int open = 0;
-    for (Entry<String,WalState> entry : es) {
-      if (entry.getValue() == WalState.OPEN) {
-        open++;
-        allWalsSeen.add(entry.getKey());
+    int attempts = 0;
+    boolean foundWal = false;
+
+    while (open == 0) {
+      attempts++;
+      Map<String,WalState> wals = WALSunnyDayIT._getWals(c);
+      Set<Entry<String,WalState>> es = wals.entrySet();
+      for (Entry<String,WalState> entry : es) {
+        if (entry.getValue() == WalState.OPEN) {
+          open++;
+          allWalsSeen.add(entry.getKey());
+          foundWal = true;
+        } else {
+          log.error(""The WalState is "" + entry.getValue());// CLOSED or UNREFERENCED
+        }
+      }
+
+      if (!foundWal) {
+        Thread.sleep(50);
+        if (attempts % 50 == 0)
+          log.info(""Has not found an open WAL in "" + attempts + "" attempts."");
       }
     }
 
+    log.debug(""It took "" + attempts + "" attempt(s) to find an open WAL"");","[{'comment': '```suggestion\r\n    log.debug(""It took {} attempt(s) to find {} open WALs"", attempts, open);\r\n```', 'commenter': 'ctubbsii'}]"
1556,test/src/main/java/org/apache/accumulo/test/functional/ManyWriteAheadLogsIT.java,"@@ -198,16 +198,33 @@ public void testMany() throws Exception {
   }
 
   private void addOpenWals(ServerContext c, Set<String> allWalsSeen) throws Exception {
-    Map<String,WalState> wals = WALSunnyDayIT._getWals(c);
-    Set<Entry<String,WalState>> es = wals.entrySet();
+
     int open = 0;
-    for (Entry<String,WalState> entry : es) {
-      if (entry.getValue() == WalState.OPEN) {
-        open++;
-        allWalsSeen.add(entry.getKey());
+    int attempts = 0;
+    boolean foundWal = false;
+
+    while (open == 0) {
+      attempts++;
+      Map<String,WalState> wals = WALSunnyDayIT._getWals(c);
+      Set<Entry<String,WalState>> es = wals.entrySet();
+      for (Entry<String,WalState> entry : es) {
+        if (entry.getValue() == WalState.OPEN) {
+          open++;
+          allWalsSeen.add(entry.getKey());
+          foundWal = true;
+        } else {
+          log.error(""The WalState is "" + entry.getValue());// CLOSED or UNREFERENCED
+        }
+      }
+
+      if (!foundWal) {
+        Thread.sleep(50);
+        if (attempts % 50 == 0)
+          log.info(""Has not found an open WAL in "" + attempts + "" attempts."");","[{'comment': '```suggestion\r\n          log.debug(""No open WALs found in {} attempts."", attempts);\r\n```', 'commenter': 'ctubbsii'}]"
1556,test/src/main/java/org/apache/accumulo/test/functional/ManyWriteAheadLogsIT.java,"@@ -198,16 +198,33 @@ public void testMany() throws Exception {
   }
 
   private void addOpenWals(ServerContext c, Set<String> allWalsSeen) throws Exception {
-    Map<String,WalState> wals = WALSunnyDayIT._getWals(c);
-    Set<Entry<String,WalState>> es = wals.entrySet();
+
     int open = 0;
-    for (Entry<String,WalState> entry : es) {
-      if (entry.getValue() == WalState.OPEN) {
-        open++;
-        allWalsSeen.add(entry.getKey());
+    int attempts = 0;
+    boolean foundWal = false;
+
+    while (open == 0) {
+      attempts++;
+      Map<String,WalState> wals = WALSunnyDayIT._getWals(c);
+      Set<Entry<String,WalState>> es = wals.entrySet();
+      for (Entry<String,WalState> entry : es) {
+        if (entry.getValue() == WalState.OPEN) {
+          open++;
+          allWalsSeen.add(entry.getKey());
+          foundWal = true;
+        } else {
+          log.error(""The WalState is "" + entry.getValue());// CLOSED or UNREFERENCED","[{'comment': 'The formatter will reform that line, and possibly others. Try running `mvn clean verify -DskipTests` on this branch to run the code formatter, and to check for other style issues.\r\n\r\n```suggestion\r\n          log.debug(""The WalState for {} is {}"", entry.getKey(), entry.getValue()); // CLOSED or UNREFERENCED\r\n```', 'commenter': 'ctubbsii'}]"
1568,assemble/bin/accumulo-cluster,"@@ -100,15 +100,45 @@ function get_ip() {
   echo ""$ip_addr""
 }
 
+function set_last_instance_id() {
+  if [[ ""$service"" == ""tserver"" ]]; then
+    last_instance_id=${NUM_TSERVERS:-1}
+  else
+    last_instance_id=1
+  fi
+}
+
+function set_service_instance_if_needed() {
+  if [[ ""$service"" == ""tserver"" && ${NUM_TSERVERS:-1} > 1 ]]; then
+    export ACCUMULO_SERVICE_INSTANCE=$inst_id
+    service_instance_cmd=""export ACCUMULO_SERVICE_INSTANCE=${inst_id};""
+  else
+    export ACCUMULO_SERVICE_INSTANCE=
+    service_instance_cmd=","[{'comment': 'Not sure the following will work, the thought is to make running on localhost and remote consistent.  Both ensuring the env var is not set when running the script.\r\n\r\n```suggestion\r\n    service_instance_cmd=""export ACCUMULO_SERVICE_INSTANCE=;""\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Will consider and implement. Thanks!', 'commenter': 'arvindshmicrosoft'}, {'comment': 'Thanks Keith. With the suggestions that Christopher made, this has been handled in a different way now. Please take a look.', 'commenter': 'arvindshmicrosoft'}, {'comment': 'Those changes look good and consistent.', 'commenter': 'keith-turner'}]"
1568,assemble/bin/accumulo-cluster,"@@ -100,15 +100,45 @@ function get_ip() {
   echo ""$ip_addr""
 }
 
+function set_last_instance_id() {
+  if [[ ""$service"" == ""tserver"" ]]; then
+    last_instance_id=${NUM_TSERVERS:-1}
+  else
+    last_instance_id=1
+  fi
+}
+
+function set_service_instance_if_needed() {
+  if [[ ""$service"" == ""tserver"" && ${NUM_TSERVERS:-1} > 1 ]]; then
+    export ACCUMULO_SERVICE_INSTANCE=$inst_id
+    service_instance_cmd=""export ACCUMULO_SERVICE_INSTANCE=${inst_id};""
+  else
+    export ACCUMULO_SERVICE_INSTANCE=
+    service_instance_cmd=
+  fi
+}
+
+function control_service() {
+  set_last_instance_id
+
+  for (( inst_id=1; inst_id<=last_instance_id; inst_id++ ))
+  do
+    set_service_instance_if_needed
+
+    if [[ $host == localhost || $host = ""$(hostname -s)"" || $host = ""$(hostname -f)"" || $host = $(get_ip) ]] ; then","[{'comment': ""This mixes double equals and single equals. Both are equivalent in bash double-square braces, so it's fine either way, but it'd be nice to be consistent."", 'commenter': 'ctubbsii'}, {'comment': 'Cool! Since this was existing code, I did not pay attention. I will address it.', 'commenter': 'arvindshmicrosoft'}, {'comment': ""I didn't notice it was existing code. It's fine if you want to leave it. It's entirely up to you."", 'commenter': 'ctubbsii'}]"
1568,assemble/bin/accumulo-cluster,"@@ -100,15 +100,45 @@ function get_ip() {
   echo ""$ip_addr""
 }
 
+function set_last_instance_id() {
+  if [[ ""$service"" == ""tserver"" ]]; then
+    last_instance_id=${NUM_TSERVERS:-1}
+  else
+    last_instance_id=1
+  fi
+}
+
+function set_service_instance_if_needed() {
+  if [[ ""$service"" == ""tserver"" && ${NUM_TSERVERS:-1} > 1 ]]; then
+    export ACCUMULO_SERVICE_INSTANCE=$inst_id
+    service_instance_cmd=""export ACCUMULO_SERVICE_INSTANCE=${inst_id};""
+  else
+    export ACCUMULO_SERVICE_INSTANCE=
+    service_instance_cmd=
+  fi
+}
+
+function control_service() {
+  set_last_instance_id","[{'comment': 'Could use a simple local variable here.\r\n\r\n```suggestion\r\n  local last_instance_id; last_instance_id=1\r\n  [[ ""$service"" = ""tserver"" ]] && last_instance_id=${NUM_TSERVERS:-1}\r\n```', 'commenter': 'ctubbsii'}]"
1568,assemble/bin/accumulo-cluster,"@@ -100,15 +100,45 @@ function get_ip() {
   echo ""$ip_addr""
 }
 
+function set_last_instance_id() {
+  if [[ ""$service"" == ""tserver"" ]]; then
+    last_instance_id=${NUM_TSERVERS:-1}
+  else
+    last_instance_id=1
+  fi
+}
+","[{'comment': ""This can be inline'd. It's only used once, and doesn't do much.\r\n\r\n```suggestion\r\n```"", 'commenter': 'ctubbsii'}]"
1568,assemble/bin/accumulo-cluster,"@@ -100,15 +100,45 @@ function get_ip() {
   echo ""$ip_addr""
 }
 
+function set_last_instance_id() {
+  if [[ ""$service"" == ""tserver"" ]]; then
+    last_instance_id=${NUM_TSERVERS:-1}
+  else
+    last_instance_id=1
+  fi
+}
+
+function set_service_instance_if_needed() {
+  if [[ ""$service"" == ""tserver"" && ${NUM_TSERVERS:-1} > 1 ]]; then","[{'comment': 'Should use numeric greater-than comparison here, instead of ASCII lexical comparison:\r\n```suggestion\r\n  if [[ ""$service"" == ""tserver"" && ${NUM_TSERVERS:-1} -gt 1 ]]; then\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""Thanks. I had run shellcheck initially but forgot to run it before pushing this round of updates. Indeed that caught this issue as well. I'll make sure I fix it."", 'commenter': 'arvindshmicrosoft'}, {'comment': 'I saw you already fixed the ""unnecesarry $ in math expression"" issue. shellcheck is awesome, isn\'t it? :smiley_cat:', 'commenter': 'ctubbsii'}]"
1568,assemble/bin/accumulo-cluster,"@@ -100,15 +100,45 @@ function get_ip() {
   echo ""$ip_addr""
 }
 
+function set_last_instance_id() {
+  if [[ ""$service"" == ""tserver"" ]]; then
+    last_instance_id=${NUM_TSERVERS:-1}
+  else
+    last_instance_id=1
+  fi
+}
+
+function set_service_instance_if_needed() {
+  if [[ ""$service"" == ""tserver"" && ${NUM_TSERVERS:-1} > 1 ]]; then
+    export ACCUMULO_SERVICE_INSTANCE=$inst_id
+    service_instance_cmd=""export ACCUMULO_SERVICE_INSTANCE=${inst_id};""
+  else
+    export ACCUMULO_SERVICE_INSTANCE=
+    service_instance_cmd=
+  fi
+}
+
+function control_service() {
+  set_last_instance_id
+
+  for (( inst_id=1; inst_id<=last_instance_id; inst_id++ ))
+  do
+    set_service_instance_if_needed
+
+    if [[ $host == localhost || $host = ""$(hostname -s)"" || $host = ""$(hostname -f)"" || $host = $(get_ip) ]] ; then
+      ""${bin}/accumulo-service"" ""$service"" ""$control_cmd""
+    else
+      $SSH ""$host"" ""bash -c '${service_instance_cmd} ${bin}/accumulo-service \""$service\"" \""$control_cmd\""'""","[{'comment': 'Instead of creating a second variable and exporting here, you can probably just execute it with the environment variable set for the execution:\r\n\r\n```suggestion\r\n      $SSH ""$host"" ""bash -c \'ACCUMULO_SERVICE_INSTANCE=\'""$ACCUMULO_SERVICE_INSTANCE""\' ${bin}/accumulo-service \\""$service\\"" \\""$control_cmd\\""\'""\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Ah - I think the usage of the semicolon in my script was the reason the variable was not visible within the accumulo-service script. I guess I was going off the syntax in the [docs](https://accumulo.apache.org/docs/2.x/administration/in-depth-install#running-multiple-tabletservers-on-a-single-node) which has a semicolon between the ACCUMULO_SEVICE_INSTANCE variable and the accumulo script. I wonder now if that is a typo which needs to be addressed?', 'commenter': 'arvindshmicrosoft'}, {'comment': ""Ah, yeah, that's definitely a typo in the docs. I'll fix that."", 'commenter': 'ctubbsii'}, {'comment': 'The docs will further need to be updated to mention the use of `NUM_TSERVERS` for 2.1 and later, once this PR gets merged.', 'commenter': 'ctubbsii'}, {'comment': 'Thank you and I did plan to submit a docs PR for `NUM_TSERVERS` usage when this change is done.', 'commenter': 'arvindshmicrosoft'}, {'comment': 'The changes have been done - please review when you have a chance.', 'commenter': 'arvindshmicrosoft'}]"
1568,assemble/bin/accumulo-cluster,"@@ -100,15 +100,45 @@ function get_ip() {
   echo ""$ip_addr""
 }
 
+function set_last_instance_id() {
+  if [[ ""$service"" == ""tserver"" ]]; then
+    last_instance_id=${NUM_TSERVERS:-1}
+  else
+    last_instance_id=1
+  fi
+}
+
+function set_service_instance_if_needed() {
+  if [[ ""$service"" == ""tserver"" && ${NUM_TSERVERS:-1} > 1 ]]; then
+    export ACCUMULO_SERVICE_INSTANCE=$inst_id
+    service_instance_cmd=""export ACCUMULO_SERVICE_INSTANCE=${inst_id};""
+  else
+    export ACCUMULO_SERVICE_INSTANCE=
+    service_instance_cmd=
+  fi
+}
+
+function control_service() {
+  set_last_instance_id
+
+  for (( inst_id=1; inst_id<=last_instance_id; inst_id++ ))
+  do
+    set_service_instance_if_needed
+
+    if [[ $host == localhost || $host = ""$(hostname -s)"" || $host = ""$(hostname -f)"" || $host = $(get_ip) ]] ; then
+      ""${bin}/accumulo-service"" ""$service"" ""$control_cmd""
+    else
+      $SSH ""$host"" ""bash -c '${service_instance_cmd} ${bin}/accumulo-service \""$service\"" \""$control_cmd\""'""
+    fi
+  done
+}
+
 function start_service() {
   host=""$1""
   service=""$2""
+  control_cmd=""start""
 
-  if [[ $host == ""localhost"" || $host == $(hostname -f) || $host == $(hostname -s) || $host == $(get_ip) ]]; then
-    ""${bin}/accumulo-service"" ""$service"" start
-  else
-    $SSH ""$host"" ""bash -c '${bin}/accumulo-service \""$service\"" start'""
-  fi
+  control_service","[{'comment': 'Could pass the arguments to the function instead of setting up global variables and picking them up in the function.\r\n\r\n```suggestion\r\n  control_service start ""$@""\r\n```\r\n', 'commenter': 'ctubbsii'}]"
1579,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -176,11 +176,11 @@ void create(String tableName, NewTableConfiguration ntc)
    *
    * @param tableName
    *          Name of a table to create and import into.
-   * @param importDir
-   *          Directory that contains the files copied by distcp from exportTable
+   * @param importDirs
+   *          A set of directories containing the files copied by distcp from exportTable
    * @since 1.5.0
    */
-  void importTable(String tableName, String importDir)
+  void importTable(String tableName, Set<String> importDirs)","[{'comment': ""This is public API and cannot change without a deprecation cycle.\r\nA new API is needed, which should co-exist with this one, until this one is removed.\r\nI didn't look at the rest of this PR, since this is a blocker right away."", 'commenter': 'ctubbsii'}, {'comment': 'I took a crack at this in 7f241a70c3be2d0d5c5b4159684fc5d6646bdcc7 - let me know if this is sufficient.', 'commenter': 'drewfarris'}]"
1579,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -1544,22 +1544,29 @@ public static Path findExportFile(ClientContext context, List<String> importDirs
   @Override
   public void importTable(String tableName, String importDir)
       throws TableExistsException, AccumuloException, AccumuloSecurityException {
+    importTable(tableName, Set.of(importDir.split("","")));","[{'comment': 'Maybe the existing API should maintain its current behavior and only support a single dir. This avoids any surprises for existing code.  If someone wants to use the new functionality, they need to use the new API.\r\n```suggestion\r\n    importTable(tableName, Set.of(importDir));\r\n```', 'commenter': 'keith-turner'}, {'comment': ':+1: I had the same thought.', 'commenter': 'ctubbsii'}]"
1579,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -176,11 +176,27 @@ void create(String tableName, NewTableConfiguration ntc)
    *
    * @param tableName
    *          Name of a table to create and import into.
-   * @param importDir
-   *          Directory that contains the files copied by distcp from exportTable
+   * @param importDirs
+   *          A comma-delimited list of directories containing the files copied by distcp from
+   *          exportTable
    * @since 1.5.0
+   *
+   * @deprecated since 2.1.0 use {@link #importTable(String, Set)} instead.
+   */
+  @Deprecated","[{'comment': 'This does not have to be deprecated because a new method was added.  Also, do you want the method to be removed in the future?  Starting with Java 9, two parameters were added to the deprecated annotation `since` and `forRemoval`.  The default for `forRemoval` is false.  If deprecating, it may also be good explicitly set these.  Not sure if my syntax is correct below, I have not used these yet.\r\n\r\n```suggestion\r\n  @Deprecated(since=""2.1.0"",forRemoval=false)\r\n```\r\n\r\nPersonally, I don\'t think the method needs to be deprecated.', 'commenter': 'keith-turner'}, {'comment': ""I don't care whether it is deprecated or not, but would prefer the original method keep the behavior from before #1060 ; I think it's fine if not."", 'commenter': 'ctubbsii'}, {'comment': '> This does not have to be deprecated because a new method was added. \r\n\r\nHmm, I don\'t understand why adding a new method eliminates the need for deprecation. The idea here would be to remove the original `importTable(String, String)` at some point when it is safe to do so.\r\n\r\nThanks for mentioning the Java 9 parameters. As such, I suspect the right annotation would be:\r\n```\r\n@Deprecated(since=""2.1.0"",forRemoval=true)\r\n```\r\n', 'commenter': 'drewfarris'}, {'comment': '> Hmm, I don\'t understand why adding a new method eliminates the need for deprecation.\r\n\r\nThe deprecation has nothing to do with the addition of the new method. It pertains to the removal of the old method.\r\n\r\n> The idea here would be to remove the original `importTable(String, String)` at some point when it is safe to do so.\r\n\r\nIf you think it\'s better to remove the old method, that\'s fine and it should be deprecated. It\'s just not a requirement to remove it.\r\n\r\n> `@Deprecated(since=""2.1.0"",forRemoval=true)`\r\n\r\nIf it is going to be deprecated, I agree it should be marked this way. I don\'t think we ever want to deprecate something and still retain it forever. The reason we deprecate stuff is so we can purge it (eventually). I wish `forRemoval=true` were default when they added this, because that\'s how I\'ve always interpreted `@Deprecated`, but I recognize others may interpret it differently.', 'commenter': 'ctubbsii'}, {'comment': ""> Hmm, I don't understand why adding a new method eliminates the need for deprecation.\r\n\r\nI was just thinking that adding the new method does not necessitate deprecating the existing one.  The existing method is not broken in any way, it still satisfies its original functionality for anyone who invested the time to write code against it in the past.     "", 'commenter': 'keith-turner'}, {'comment': ""> I was just thinking that adding the new method does not necessitate deprecating the existing one. The existing method is not broken in any way, it still satisfies its original functionality for anyone who invested the time to write code against it in the past.\r\n\r\nI'm happy to go either way. I suppose I'm in favor of keeping the number of public api methods to a minimum, but I don't feel strongly about leaving the existing method around if that's a better fit for the project."", 'commenter': 'drewfarris'}, {'comment': "">  I suppose I'm in favor of keeping the number of public api methods to a minimum\r\n\r\nThat is a good goal, not sure whats best in the big scheme of things.   In my experience removing methods is most painful for indirect users of Accumulo (like someone using Accumulo via something like Rya).  Can get into a situation where you can not use the latest version of Accumulo w/o patching and rebuilding Rya. \r\n\r\nCould minimize clutter in the Accumulo source using a default method in the interface for the existing method (that calls the new method) and have it nowhere else.   "", 'commenter': 'keith-turner'}, {'comment': "">That is a good goal, not sure whats best in the big scheme of things.\r\n\r\nOk, thanks for explaining your perspective. The makes a great deal of sense. Based on this, I'm going to avoid deprecating the original method. It looks like theres precedent for `default` implementations in the `TableOperations` interface, so I'll proceed in that way."", 'commenter': 'drewfarris'}]"
1579,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -1544,22 +1544,29 @@ public static Path findExportFile(ClientContext context, List<String> importDirs
   @Override
   public void importTable(String tableName, String importDir)
       throws TableExistsException, AccumuloException, AccumuloSecurityException {
+    importTable(tableName, Set.of(importDir.split("","")));
+  }
+
+  @Override
+  public void importTable(String tableName, Set<String> importDirs)
+      throws TableExistsException, AccumuloException, AccumuloSecurityException {
     checkArgument(tableName != null, ""tableName is null"");
-    checkArgument(importDir != null, ""importDir is null"");
+    checkArgument(importDirs != null, ""importDir is null"");
     checkArgument(tableName.length() <= MAX_TABLE_NAME_LEN,
         ""Table name is longer than "" + MAX_TABLE_NAME_LEN + "" characters"");
 
-    List<String> importDirs = new ArrayList<>();
-    for (String dir : importDir.split("","")) {
-      try {
-        importDirs.add(checkPath(dir, ""Table"", """").toString());
-      } catch (IOException e) {
-        throw new AccumuloException(e);
+    Set<String> checkedImportDirs = new HashSet<String>();
+    try {
+      for (String s : importDirs) {
+        checkedImportDirs.add(checkPath(s, ""Table"", """").toString());
       }
+    } catch (IOException e) {
+      throw new AccumuloException(e);
     }
+    String normedImportDir = StringUtils.join(checkedImportDirs, "","");","[{'comment': ""This isn't used."", 'commenter': 'ctubbsii'}]"
1579,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -1544,22 +1544,29 @@ public static Path findExportFile(ClientContext context, List<String> importDirs
   @Override
   public void importTable(String tableName, String importDir)
       throws TableExistsException, AccumuloException, AccumuloSecurityException {
+    importTable(tableName, Set.of(importDir.split("","")));
+  }
+
+  @Override
+  public void importTable(String tableName, Set<String> importDirs)
+      throws TableExistsException, AccumuloException, AccumuloSecurityException {
     checkArgument(tableName != null, ""tableName is null"");
-    checkArgument(importDir != null, ""importDir is null"");
+    checkArgument(importDirs != null, ""importDir is null"");
     checkArgument(tableName.length() <= MAX_TABLE_NAME_LEN,
         ""Table name is longer than "" + MAX_TABLE_NAME_LEN + "" characters"");
 
-    List<String> importDirs = new ArrayList<>();
-    for (String dir : importDir.split("","")) {
-      try {
-        importDirs.add(checkPath(dir, ""Table"", """").toString());
-      } catch (IOException e) {
-        throw new AccumuloException(e);
+    Set<String> checkedImportDirs = new HashSet<String>();
+    try {
+      for (String s : importDirs) {
+        checkedImportDirs.add(checkPath(s, ""Table"", """").toString());
       }
+    } catch (IOException e) {
+      throw new AccumuloException(e);
     }
+    String normedImportDir = StringUtils.join(checkedImportDirs, "","");
 
     try {
-      Path exportFilePath = findExportFile(context, importDirs);
+      Path exportFilePath = findExportFile(context, checkedImportDirs);","[{'comment': 'Since the variable was renamed, there are other places below that need to be changed from `importDirs` to `checkedImportDirs` as well.', 'commenter': 'ctubbsii'}]"
1579,server/master/src/main/java/org/apache/accumulo/master/tableOps/tableImport/ImportTable.java,"@@ -53,16 +54,17 @@
 public class ImportTable extends MasterRepo {
   private static final Logger log = LoggerFactory.getLogger(ImportTable.class);
 
-  private static final long serialVersionUID = 1L;
+  private static final long serialVersionUID = 2L;","[{'comment': 'Since the Fate serialization has changed, it will be very important that there are no outstanding import table fate operations outstanding before upgrading. I suspect this is unlikely for most people, but may be something to call out in the release notes.', 'commenter': 'ctubbsii'}, {'comment': 'I think we check before upgrade if there are any outstanding FATE ops and if so upgrade will not proceed.  So usually do not need to worry about FATE persistence across versions.', 'commenter': 'keith-turner'}]"
1579,server/master/src/main/java/org/apache/accumulo/master/tableOps/tableImport/ImportedTableInfo.java,"@@ -38,6 +38,10 @@
   static class DirectoryMapping implements Serializable {
     private static final long serialVersionUID = 1L;
 
+    public DirectoryMapping(String exportDir) {
+      this.exportDir = exportDir;","[{'comment': 'Since the `exportDir` is set in the constructor, can we now make the field `final`?', 'commenter': 'ctubbsii'}]"
1579,test/src/main/java/org/apache/accumulo/test/ImportExportIT.java,"@@ -151,7 +151,8 @@ public void testExportImportThenScan() throws Exception {
       log.info(""Import dir B: {}"", Arrays.toString(fs.listStatus(importDirB)));
 
       // Import the exported data into a new table
-      client.tableOperations().importTable(destTable, importDirDlm);
+      client.tableOperations().importTable(destTable,
+          Set.of(importDirA.toString(), importDirB.toString()));","[{'comment': ""If you define this set earlier, you wouldn't need to manually concatenate them for the log message, because you could just rely on `Set.toString()`."", 'commenter': 'ctubbsii'}]"
1579,test/src/main/java/org/apache/accumulo/test/NamespacesIT.java,"@@ -980,8 +980,8 @@ public void verifyTableOperationsExceptions() throws Exception {
         () -> ops.clone(""a"", tableName, true, Collections.emptyMap(), Collections.emptySet()));
     ops.offline(""a"", true);
     ops.exportTable(""a"", System.getProperty(""user.dir"") + ""/target"");
-    assertAccumuloExceptionNoNamespace(
-        () -> ops.importTable(tableName, System.getProperty(""user.dir"") + ""/target""));
+    assertAccumuloExceptionNoNamespace(() -> ops.importTable(tableName,
+        Collections.singleton(System.getProperty(""user.dir"") + ""/target"")));","[{'comment': '`Set.of(X)` is shorter than `Collections.singleton(X)`.', 'commenter': 'ctubbsii'}]"
1579,server/master/src/main/java/org/apache/accumulo/master/tableOps/tableImport/ImportTable.java,"@@ -19,13 +19,14 @@
 package org.apache.accumulo.master.tableOps.tableImport;
 
 import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.function.Predicate.not;","[{'comment': ""@ctubbsii @keith-turner - `Predicate.not` was added in Java 11 and I expected the build to fail when I used it, but it didn't. What runtimes are we supporting for 2.1.x?"", 'commenter': 'drewfarris'}, {'comment': '2.1 is targeting Java 11.', 'commenter': 'ctubbsii'}]"
1579,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -176,11 +176,27 @@ void create(String tableName, NewTableConfiguration ntc)
    *
    * @param tableName
    *          Name of a table to create and import into.
-   * @param importDir
-   *          Directory that contains the files copied by distcp from exportTable
+   * @param importDirs
+   *          A comma-delimited list of directories containing the files copied by distcp from
+   *          exportTable","[{'comment': 'This javadoc is no longer true.', 'commenter': 'ctubbsii'}]"
1579,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -176,11 +176,27 @@ void create(String tableName, NewTableConfiguration ntc)
    *
    * @param tableName
    *          Name of a table to create and import into.
-   * @param importDir
-   *          Directory that contains the files copied by distcp from exportTable
+   * @param importDirs
+   *          A comma-delimited list of directories containing the files copied by distcp from
+   *          exportTable
    * @since 1.5.0
+   *
+   * @deprecated since 2.1.0 use {@link #importTable(String, Set)} instead.
+   */
+  @Deprecated(since = ""2.1.0"", forRemoval = true)
+  void importTable(String tableName, String importDirs)","[{'comment': 'I think this should be singular ""importDir"" now (or whatever it was called before the changes in #1060 )', 'commenter': 'ctubbsii'}]"
1579,core/src/test/java/org/apache/accumulo/core/clientImpl/TableOperationsHelperTest.java,"@@ -199,6 +199,9 @@ public void clearLocatorCache(String tableName) {}
     @Override
     public void importTable(String tableName, String exportDir) {}","[{'comment': ""If you're using a default method in the interface, the corresponding methods can be deleted from the implementing classes. I would check all subclasses and remove any that aren't needed."", 'commenter': 'ctubbsii'}]"
1579,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -177,10 +177,25 @@ void create(String tableName, NewTableConfiguration ntc)
    * @param tableName
    *          Name of a table to create and import into.
    * @param importDir
-   *          Directory that contains the files copied by distcp from exportTable
+   *          A directory containing the files copied by distcp from exportTable
    * @since 1.5.0
+   *
+   */
+  default void importTable(String tableName, String importDir)
+      throws TableExistsException, AccumuloException, AccumuloSecurityException {
+    importTable(tableName, Set.of(importDir));
+  }
+
+  /**
+   * Imports a table exported via exportTable and copied via hadoop distcp.
+   *
+   * @param tableName
+   *          Name of a table to create and import into.
+   * @param importDirs
+   *          A set of directories containing the files copied by distcp from exportTable","[{'comment': 'Is there anything that would be worth mentioning about the metadata zip file and multiple dirs here?', 'commenter': 'keith-turner'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java,"@@ -67,7 +67,7 @@
  */
 public class TabletsMetadata implements Iterable<TabletMetadata>, AutoCloseable {
 
-  private static class Builder implements TableRangeOptions, TableOptions, RangeOptions, Options {
+  public static class Builder implements TableRangeOptions, TableOptions, RangeOptions, Options {","[{'comment': 'How about package private?', 'commenter': 'ivakegg'}, {'comment': ""@ivakegg I think this is the way to go. One tumbling block is crafting a unit test that is outside the ...metadata.schema package. A test created outside that package won't have access to the Builder methods. For the moment, I will push a version with a public Builder class that is tested using a mock table. "", 'commenter': 'cradal'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java,"@@ -79,6 +79,7 @@
     private boolean checkConsistency = false;
     private boolean saveKeyValues;
     private TableId tableId;
+    public AccumuloClient _client;","[{'comment': 'public non-final variables should be avoided.', 'commenter': 'ctubbsii'}, {'comment': 'This was done as part of an overall attempt take the builder interfaces in TabletsMetadata and move them to Ample, while leaving the implementation of the interfaces in TabletsMetadata.The readTablets() method in AmplImpl generates a TabletsMetadata builder object and assigns the AmpleImpl client to the public client in that builder object. This allows readTablets() to be called from a ClientContext. However, at this point, I think I should rethink the whole design because it breaks too many coding conventions (case in point, above).', 'commenter': 'cradal'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java,"@@ -93,6 +94,20 @@ public TabletsMetadata build(AccumuloClient client) {
       }
     }
 
+    // No-parameter build method that allows the use of AmpleImpl's Accumulo client.
+    // Ample methods can assign AmpleImple's Accumulo client to _client of this Builder object.
+    public TabletsMetadata build() {
+      Preconditions.checkState(level == null ^ table == null);","[{'comment': 'This might be slightly more readable as:\r\n```suggestion\r\n      Preconditions.checkState((level == null) != (table == null));\r\n```', 'commenter': 'ctubbsii'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java,"@@ -371,6 +388,10 @@ private TabletsMetadata(Scanner scanner, Iterable<TabletMetadata> tmi) {
     this.tablets = tmi;
   }
 
+  public TabletsMetadata() {
+
+  }
+","[{'comment': ""This doesn't appear to be used. Not sure it's needed."", 'commenter': 'ctubbsii'}]"
1651,test/src/main/java/org/apache/accumulo/test/functional/AccumuloClientIT.java,"@@ -220,6 +238,120 @@ public void testClose() throws Exception {
     expectClosed(() -> tops.create(""expectFail""));
     expectClosed(() -> tops.cancelCompaction(tableName));
     expectClosed(() -> tops.listSplits(tableName));
+  }
 
+  @Test
+  public void testAmpleReadTablets() throws Exception {
+
+    try (AccumuloClient accumuloClient = Accumulo.newClient().from(getClientProps()).build()) {
+      accumuloClient.securityOperations().grantTablePermission(accumuloClient.whoami(),","[{'comment': 'An alternative way to write this test would be to create a table w/ desired splits and then get the table ID.  ', 'commenter': 'keith-turner'}]"
1651,test/src/main/java/org/apache/accumulo/test/functional/AccumuloClientIT.java,"@@ -220,6 +238,120 @@ public void testClose() throws Exception {
     expectClosed(() -> tops.create(""expectFail""));
     expectClosed(() -> tops.cancelCompaction(tableName));
     expectClosed(() -> tops.listSplits(tableName));
+  }
 
+  @Test
+  public void testAmpleReadTablets() throws Exception {","[{'comment': 'Why put this test in AccumuloClientIT?', 'commenter': 'keith-turner'}, {'comment': 'This is here to facilitate calling the method from a AccumuloClient. ', 'commenter': 'cradal'}, {'comment': ""I don't think AccumuloClientIT is the appropriate place for testing this method, this test is mainly used to test the AccumuloClient.  I think somewhere like MetadataIT would be a better place for verifying the metadata."", 'commenter': 'milleruntime'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/AmpleImpl.java,"@@ -42,4 +43,12 @@ public TabletMetadata readTablet(KeyExtent extent, ColumnType... colsToFetch) {
       return Iterables.getOnlyElement(tablets);
     }
   }
+
+  @Override
+  public TabletsMetadata.Builder readTablets() {
+    this.builder = new TabletsMetadata.Builder();
+    builder._client = this.client;","[{'comment': 'It would be cleaner to pass the client to the constructor if possible.', 'commenter': 'keith-turner'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/AmpleImpl.java,"@@ -27,6 +27,7 @@
 
 public class AmpleImpl implements Ample {
   private final AccumuloClient client;
+  private TabletsMetadata.Builder builder;","[{'comment': 'Why is this an instance variable?', 'commenter': 'keith-turner'}, {'comment': 'This was done as part of an overall attempt take the builder interfaces in TabletsMetadata and move them to Ample, while leaving the implementation of the interfaces in TabletsMetadata.The readTablets() method in AmplImpl generates a TabletsMetadata builder object and assigns the AmpleImpl client to the public client in that builder object. This allows readTablets() to be called from a ClientContext. However, at this point, I think I should rethink the whole design because it breaks too many coding conventions (case in point, above).', 'commenter': 'cradal'}, {'comment': 'Latest push removes this var.', 'commenter': 'cradal'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java,"@@ -119,6 +119,15 @@ public static DataLevel of(TableId tableId) {
    */
   TabletMetadata readTablet(KeyExtent extent, ColumnType... colsToFetch);
 
+  /**
+   * Entry point for reading multiple tablets' metadata. Generates a TabletsMetadata builder object
+   * and assigns the AmpleImpl client to that builder object. This allows readTablets() to be called
+   * from a ClientContext. Associated methods of the TabletsMetadata Builder class are used to
+   * generate the metadata.
+   */
+
+  AmpleImpl.Builder readTablets();","[{'comment': 'This exposes the Impl class on the interface.  This breaks the contract of the interface.', 'commenter': 'milleruntime'}, {'comment': 'I think this method should return `TabletsMetadata.TableOptions`.  That will limit the next method calls to what is needed for the fluent entry point and help prevent coding errors.\r\n\r\nWe want to select the table first like this:\r\n```\r\nample.readTablets().forTable(tableId).fetch(LOCATION, PREV_ROW).build();\r\n```\r\nAnd not be able to skip the required parameters like this:\r\n```\r\nample.readTablets().build(); //coding error\r\n```\r\n', 'commenter': 'milleruntime'}, {'comment': 'Incorporated suggestions on latest push.', 'commenter': 'cradal'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/AmpleImpl.java,"@@ -34,12 +70,212 @@ public AmpleImpl(AccumuloClient client) {
 
   @Override
   public TabletMetadata readTablet(KeyExtent extent, ColumnType... colsToFetch) {
-    Options builder = TabletsMetadata.builder().forTablet(extent);
+    TabletsMetadata.Options builder = TabletsMetadata.builder().forTablet(extent);
     if (colsToFetch.length > 0)
       builder.fetch(colsToFetch);
 
     try (TabletsMetadata tablets = builder.build(client)) {
       return Iterables.getOnlyElement(tablets);","[{'comment': ""A very simple alternative to this change, would be to just have a shared method return `TabletsMetadata` (before calling getOnlyElement).  Is there a reason just returning `TabletsMetadata` for readTablets() wouldn't suffice?\r\n\r\nThe method on the interface could look like this:\r\n`public TabletsMetadata readTablets(KeyExtent extent, ColumnType... colsToFetch)`"", 'commenter': 'milleruntime'}, {'comment': ""Nevermind, it doesn't look like this would work with the way that we use TabletsMetadata."", 'commenter': 'milleruntime'}]"
1651,server/base/src/test/java/org/apache/accumulo/server/master/state/RootTabletStateStoreTest.java,"@@ -54,6 +55,11 @@ public TabletMetadata readTablet(KeyExtent extent, ColumnType... colsToFetch) {
       return RootTabletMetadata.fromJson(json).convertToTabletMetadata();
     }
 
+    @Override
+    public AmpleImpl.Builder readTablets() {
+      return null;
+    }","[{'comment': 'This should throw an exception so it is more clear that its not being used.', 'commenter': 'milleruntime'}, {'comment': 'Added UnsupportedOperationException.', 'commenter': 'cradal'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/AmpleImpl.java,"@@ -34,12 +70,212 @@ public AmpleImpl(AccumuloClient client) {
 
   @Override
   public TabletMetadata readTablet(KeyExtent extent, ColumnType... colsToFetch) {
-    Options builder = TabletsMetadata.builder().forTablet(extent);
+    TabletsMetadata.Options builder = TabletsMetadata.builder().forTablet(extent);
     if (colsToFetch.length > 0)
       builder.fetch(colsToFetch);
 
     try (TabletsMetadata tablets = builder.build(client)) {
       return Iterables.getOnlyElement(tablets);
     }
   }
+
+  @Override
+  public AmpleImpl.Builder readTablets() {
+    Builder builder = new Builder(client);
+    return builder;
+  }
+
+  public static class Builder implements Iterable<TabletMetadata> {","[{'comment': 'I think this class should stay where it is now but just be made public.  It is difficult to review what you changed when you moved it here.  It looks like you made the correct changes with the `Builder(client)` constructor and `build()` method but it is difficult to know for sure.', 'commenter': 'milleruntime'}, {'comment': 'Moved test to MetadataIT.', 'commenter': 'cradal'}, {'comment': 'Incorporated suggestions on latest push.', 'commenter': 'cradal'}]"
1651,test/src/main/java/org/apache/accumulo/test/functional/MetadataIT.java,"@@ -156,4 +171,122 @@ public void batchScanTest() throws Exception {
       }
     }
   }
+
+  @Test
+  public void testAmpleReadTablets() throws Exception {
+
+    try (AccumuloClient accumuloClient = Accumulo.newClient().from(getClientProps()).build()) {
+      accumuloClient.securityOperations().grantTablePermission(accumuloClient.whoami(),
+          MetadataTable.NAME, TablePermission.WRITE);
+      BatchWriter bw =
+          accumuloClient.createBatchWriter(MetadataTable.NAME, new BatchWriterConfig());
+      ClientContext cc = (ClientContext) accumuloClient;
+      // Create a fake METADATA table with these splits","[{'comment': 'Why not just use the API to create a table with those splits and let Accumulo populate the metadata?', 'commenter': 'milleruntime'}, {'comment': 'That idea is more economical. I guess an effective test does not always need a full-blown client. I will update the test.', 'commenter': 'cradal'}]"
1651,core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java,"@@ -79,6 +79,15 @@
     private boolean checkConsistency = false;
     private boolean saveKeyValues;
     private TableId tableId;
+    private AccumuloClient _client;
+
+    Builder(AccumuloClient client) {
+      this._client = client;
+    }
+
+    Builder() {
+
+    }","[{'comment': 'Should only have one constructor and one build method to prevent coding errors.  Since you have access to the AccumuloClient object at the time you are calling the method, I think it would be best to go with the `Builder(AccumuloClient client)` constructor and then end it with the `build()` method.  This will only pass in the client once and prevent it from being null when calling `build()`. ', 'commenter': 'milleruntime'}, {'comment': 'One issue with this is that there are ~15 classes that already use the pre-existing no-parameter constructor. Perhaps I should find a different way to pass the AmpleImpl Accumulo client into the TabletsMetadata builder object.', 'commenter': 'cradal'}, {'comment': 'I might be able to refactor those classes to use the new Builder(Accumulo client) constructor. ', 'commenter': 'cradal'}, {'comment': 'That is OK.  I should only be 1 or 2 lines per class.', 'commenter': 'milleruntime'}]"
1653,core/src/main/java/org/apache/accumulo/core/util/HostAndPort.java,"@@ -171,6 +171,10 @@ public static HostAndPort fromString(String hostPortString) {
       // JDK7 accepts leading plus signs. We don't want to.
       checkArgument(!portString.startsWith(""+""), ""Unparseable port number: %s"", hostPortString);
       try {
+        if (portString.contains(""["")) {
+          int endIndex = portString.indexOf(""["");
+          portString = portString.substring(0, endIndex);
+        }","[{'comment': 'This could use a small comment above it to explain what situation it is checking for.', 'commenter': 'ctubbsii'}, {'comment': ""I will add that. This is to ensure it actually grabs the port number, due to some change I made, `MasterRepairsDualAssignmentIT`fails because the portString contains more than just the port number. Example below:\r\n```\r\n38277[10005df00740005]\r\n```\r\nI'll take a look at the cause but this if statement does solve it temporarily. "", 'commenter': 'Manno15'}, {'comment': ""That's fine. It just needs a comment in the code."", 'commenter': 'ctubbsii'}, {'comment': 'I will add it this Monday. ', 'commenter': 'Manno15'}]"
1653,server/base/src/main/java/org/apache/accumulo/server/util/MasterMetadataUtil.java,"@@ -227,14 +204,6 @@ public static StoredTabletFile updateTabletDataFile(ServerContext context, KeyEx
       tablet.putFile(path, dfv);
       tablet.putTime(time);
       newFile = path.insert();
-
-      TServerInstance self = getTServerInstance(address, zooLock);
-      tablet.putLocation(self, LocationType.LAST);
-
-      // remove the old location
-      if (lastLocation != null && !lastLocation.equals(self)) {
-        tablet.deleteLocation(lastLocation, LocationType.LAST);
-      }","[{'comment': 'Why is it the case that these location entries no longer need to be updated here? Where is the replacement code?', 'commenter': 'ctubbsii'}, {'comment': ""The replacement code is my changes in #1616. That is what spawned most of these things. Though maybe this is still considered necessary but if lastLocation is properly set through those other changes then it doesn't have to be set here. I believe after #1616 this function wasn't used anymore but it has been awhile, I can go back and confirm that aspect. "", 'commenter': 'Manno15'}, {'comment': ""Okay, that makes sense. I tried to trace the callers to this method, to see if they instead ended up reaching the new code path in #1616, but got tired last night. I'll look at it again today. I just want to make sure that if we're not updating it here, that it's because we definitely don't need to."", 'commenter': 'ctubbsii'}]"
1653,server/base/src/main/java/org/apache/accumulo/server/util/MasterMetadataUtil.java,"@@ -197,13 +181,6 @@ public static void replaceDatafiles(ServerContext context, KeyExtent extent,
     if (compactionId != null)
       tablet.putCompactionId(compactionId);
 
-    TServerInstance self = getTServerInstance(address, zooLock);
-    tablet.putLocation(self, LocationType.LAST);
-
-    // remove the old location
-    if (lastLocation != null && !lastLocation.equals(self))
-      tablet.deleteLocation(lastLocation, LocationType.LAST);
-","[{'comment': 'Why is it the case that these location entries no longer need to be updated here? Where is the replacement code?', 'commenter': 'ctubbsii'}, {'comment': 'Same, as above.', 'commenter': 'Manno15'}]"
1653,server/base/src/main/java/org/apache/accumulo/server/master/state/MetaDataStateStore.java,"@@ -58,137 +54,86 @@ protected MetaDataStateStore(ClientContext context, CurrentState state, String t
     return new MetaDataTableScanner(context, TabletsSection.getRange(), state, targetTableName);
   }
 
-  @Override
-  public void setLocations(Collection<Assignment> assignments) throws DistributedStoreException {
-    BatchWriter writer = createBatchWriter();
-    try {
-      for (Assignment assignment : assignments) {
-        Mutation m = new Mutation(assignment.tablet.toMetaRow());
-        assignment.server.putLocation(m);
-        assignment.server.putLastLocation(m);
-        assignment.server.clearFutureLocation(m);
-        SuspendingTServer.clearSuspension(m);
-        writer.addMutation(m);
-      }
-    } catch (Exception ex) {
-      throw new DistributedStoreException(ex);
-    } finally {
-      try {
-        writer.close();
-      } catch (MutationsRejectedException e) {
-        throw new DistributedStoreException(e);
-      }","[{'comment': ""I'm not sure these catch clauses should be removed, and that DistributedStoreException should be deleted. While the conversion to Ample does eliminate the checked exception of MutationsRejectedException, by wrapping it (and all other exceptions) with a RuntimeException, the RuntimeException was previously being handled here, and now it falls through."", 'commenter': 'ctubbsii'}, {'comment': 'I will add this back. Intellij listed DistributedStoreException as not being used after my changes which is why I removed it in the first place. ', 'commenter': 'Manno15'}, {'comment': ""I added the try/catch back to these functions in MetaDataStateStore, do you think I should do the same to the other state stores even though they didn't have them before?"", 'commenter': 'Manno15'}, {'comment': ""I'm not sure if it should be added to other locations. Probably not. I just know it is a behavior change to not handle the RuntimeExceptions that are thrown. I'm okay with that behavior change if it's justified (like, because we know for sure that we actually don't want to catch them here). I'm just not sure whether it's justified."", 'commenter': 'ctubbsii'}]"
1653,server/manager/src/main/java/org/apache/accumulo/master/TabletGroupWatcher.java,"@@ -871,7 +870,9 @@ private void flushChanges(SortedMap<TServerInstance,TabletServerStatus> currentT
 
     if (!assignments.isEmpty()) {
       Master.log.info(String.format(""Assigning %d tablets"", assignments.size()));
-      store.setFutureLocations(assignments);
+
+      for (Assignment assignment : assignments)
+        store.setFutureLocation(assignment);","[{'comment': 'It seems like this code would be less efficient, because it creates a new batch writer for each assignment, and that the method that handles the updates in Ample should deal with updating all of the assignments in one pass. What do you know about the performance implications of this change?', 'commenter': 'ctubbsii'}, {'comment': 'I am not too sure about the performance implications. I remember this part being one of the ones I struggled with trying to figure out how to apply my other changes too. Part of the original goal of making these changes to the state stores was to pass in a single assignment instead of a collection, along with applying ample to it. Some of the state stores already did this before these changes like in ZooTabletStateStore.setFutureLocation. Its possible the easiest and best solution is to add the collection<assignment> back and use: \r\n``` java\r\n if (assignments.size() != 1)\r\n      throw new IllegalArgumentException(""There is only one root tablet"");\r\n    Assignment assignment = assignments.iterator().next();\r\n    if (assignment.tablet.compareTo(RootTable.EXTENT) != 0)\r\n      throw new IllegalArgumentException(""You can only store the root tablet `location"");\r\n```\r\nat the beginning of the various state store functions. ', 'commenter': 'Manno15'}, {'comment': ""Well, the reason the ZooTabletStateStore only accepted one was because it can only ever have one... because the root table that uses the ZooTabletStateStore can never have more than one tablet, and therefore, there can never be more than one assignment being updated. That's not true for the other stores, because the metadata table can split into multiple tablets, and so can user tables."", 'commenter': 'ctubbsii'}, {'comment': 'That makes sense. I was told that was the case(only one assignment at a time) for all the stores even though we implemented it to be able to handle multiple assignments. Just to be clear, I should revert back to using collection<assignments> for these functions? ', 'commenter': 'Manno15'}, {'comment': 'Based on the code you changed, it does not appear that\'s the case. Specifically, where you made this update in a loop, it wasn\'t one at a time. I think you could probably have both the single-case version, and the collection version, so we don\'t have to do ""Collections.singletonList"" everywhere when we only have one. The Collection one could be optimized to reuse the same batchwriter somehow (not sure exactly what would need to change in the tablet mutator / Ample code to do that; the old code that didn\'t use Ample was obviously reusing a batch writer, but I\'m not sure how to do it in an Ample way). I\'m also not sure how you\'d pass the previous location to clear if you\'re passing a collection... you might have a different previous location for each assignment, right? I\'m not sure without analyzing the code in a lot more detail.', 'commenter': 'ctubbsii'}]"
1653,server/tserver/src/main/java/org/apache/accumulo/tserver/AssignmentHandler.java,"@@ -177,7 +177,7 @@ public void run() {
         throw new RuntimeException(""Minor compaction after recovery fails for "" + extent);
       }
       Assignment assignment = new Assignment(extent, server.getTabletSession());
-      TabletStateStore.setLocation(server.getContext(), assignment);
+      TabletStateStore.setLocation(server.getContext(), assignment, assignment.server);","[{'comment': ""Is `assignment.server` really the previously assigned location to clear? This doesn't seem right because MetaDataStateStore.setLocation just compares this `prevLastLoc` (passed in here as `assignment.server` against `assignment.server`. It seems like it would never be different, and the last location entries would remain (and possibly keep accruing?)"", 'commenter': 'ctubbsii'}, {'comment': 'Yeah, that appears to have been a oversight on my part. I will use the tabletMetadate to get the last location. ', 'commenter': 'Manno15'}]"
1653,server/tserver/src/main/java/org/apache/accumulo/tserver/UnloadTabletHandler.java,"@@ -123,8 +122,6 @@ public void run() {
         TabletStateStore.suspend(server.getContext(), tls, null,
             requestTimeSkew + MILLISECONDS.convert(System.nanoTime(), NANOSECONDS));
       }
-    } catch (DistributedStoreException ex) {
-      log.warn(""Unable to update storage"", ex);","[{'comment': 'After your changes, this catch clause has no effect (since it was removed), and the RuntimeExceptions now thrown will continue falling through this, completely unhandled.', 'commenter': 'ctubbsii'}]"
1653,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -444,7 +442,7 @@ StoredTabletFile bringMajorCompactionOnline(Set<StoredTabletFile> oldDatafiles,
 
       tablet.computeNumEntries();
 
-      lastLocation = tablet.resetLastLocation();
+      tablet.resetLastLocation();","[{'comment': 'This seems likely where the actual lastLocation information is lost.', 'commenter': 'ctubbsii'}]"
1653,server/base/src/main/java/org/apache/accumulo/server/metadata/TabletMutatorBase.java,"@@ -202,6 +203,23 @@ private String getLocationFamily(LocationType type) {
     return this;
   }
 
+  @Override
+  public Ample.TabletMutator putSuspension(Ample.TServer tServer, long suspensionTime) {
+    Preconditions.checkState(updatesEnabled, ""Cannot make updates after calling mutate."");
+    mutation.put(SuspendLocationColumn.SUSPEND_COLUMN.getColumnFamily(),
+        SuspendLocationColumn.SUSPEND_COLUMN.getColumnQualifier(),
+        new Value(tServer + ""|"" + suspensionTime));","[{'comment': ""This replaces `SuspendingTServer.toValue()`, which does not appear to be used any longer. The old method stored a `HostAndPort.toString()`, but this stores `Ample.TServer.toString()`. Are these identical?\r\n\r\nThe serialization for `toValue` should probably remain in the `SuspendingTServer` class, since that's where the corresponding `fromValue` method is. So, I recommend changing `SuspendingTServer.toValue` that is called from here, so if the serialization changes, it can be changed in both `toValue` and `fromValue` at the same time."", 'commenter': 'ctubbsii'}, {'comment': 'Thanks for pointing that out, that was something I didn\'t notice. I replaced with ` new Value(tServer.getLocation() + ""|"" + suspensionTime))` which more accurately matches `HostAndPort.toString()`. This also solves the issue I was facing with `MasterRepairsDualAssignmentIT` so I no longer need the if statement I added in the `HostAndPort` file.\r\n\r\nI don\'t fully understand your second paragraph. Are you suggesting I use the `SuspendingTServer.fromValue()` function in this mutation instead of doing the `new Value(tServer.getLocation() + ""|"" + suspensionTime))`?\r\n', 'commenter': 'Manno15'}, {'comment': '> I don\'t fully understand your second paragraph. Are you suggesting I use the `SuspendingTServer.fromValue()` function in this mutation instead of doing the `new Value(tServer.getLocation() + ""|"" + suspensionTime))`?\r\n\r\nPrior to your changes, the `SuspendingTServer.toValue()` was responsible for creating this `new Value`. I\'m suggesting that we leave that class responsible for creating the `new Value`,  so it can live alongside that class\' existing `fromValue` method. In order to do that, you would need to turn it into a static method, so you can use it in your code.\r\n\r\nIn other words, this code should change to something like:\r\n\r\n```suggestion\r\n        SuspendingTServer.toValue(tServer, suspensionTime);\r\n```\r\n\r\nAnd then that method should look something like:\r\n\r\n```java\r\n    public static Value toValue(HostAndPort tServer, long suspensionTime) {\r\n      return new Value(tServer + ""|"" + suspensionTime);\r\n    }\r\n```\r\n\r\nI probably don\'t have the types correct in this code, but something like that is what I\'m suggesting.', 'commenter': 'ctubbsii'}]"
1653,server/base/src/main/java/org/apache/accumulo/server/master/state/MetaDataStateStore.java,"@@ -58,57 +54,31 @@ protected MetaDataStateStore(ClientContext context, CurrentState state, String t
     return new MetaDataTableScanner(context, TabletsSection.getRange(), state, targetTableName);
   }
 
-  @Override
   public void setLocations(Collection<Assignment> assignments) throws DistributedStoreException {
-    BatchWriter writer = createBatchWriter();
     try {
       for (Assignment assignment : assignments) {
-        Mutation m = new Mutation(assignment.tablet.toMetaRow());
-        assignment.server.putLocation(m);
-        assignment.server.clearFutureLocation(m);
-        SuspendingTServer.clearSuspension(m);
-        writer.addMutation(m);
+        TabletMutator tabletMutator = ample.mutateTablet(assignment.tablet);
+        tabletMutator.putLocation(assignment.server, LocationType.CURRENT);
+        tabletMutator.deleteLocation(assignment.server, LocationType.FUTURE);
+        tabletMutator.mutate();
       }
     } catch (Exception ex) {
       throw new DistributedStoreException(ex);
-    } finally {
-      try {
-        writer.close();
-      } catch (MutationsRejectedException e) {
-        throw new DistributedStoreException(e);
-      }
-    }
-  }
-
-  BatchWriter createBatchWriter() {
-    try {
-      return context.createBatchWriter(targetTableName,
-          new BatchWriterConfig().setMaxMemory(MAX_MEMORY)
-              .setMaxLatency(LATENCY, TimeUnit.MILLISECONDS).setMaxWriteThreads(THREADS));
-    } catch (Exception e) {
-      throw new RuntimeException(e);
     }
   }
 
   @Override
   public void setFutureLocations(Collection<Assignment> assignments)
       throws DistributedStoreException {
-    BatchWriter writer = createBatchWriter();
     try {
       for (Assignment assignment : assignments) {
-        Mutation m = new Mutation(assignment.tablet.toMetaRow());
-        SuspendingTServer.clearSuspension(m);
-        assignment.server.putFutureLocation(m);
-        writer.addMutation(m);
+        TabletMutator tabletMutator = ample.mutateTablet(assignment.tablet);
+        tabletMutator.deleteSuspension();
+        tabletMutator.putLocation(assignment.server, LocationType.FUTURE);
+        tabletMutator.mutate();","[{'comment': 'I think this loop could benefit from the same batch writer reuse that you did in the unassign method below this, where you called `try (var tabletsMutator = ample.mutateTablets()) {` outside the loop. (Same comment applies to the `setLocations` method above.)', 'commenter': 'ctubbsii'}]"
1653,server/base/src/main/java/org/apache/accumulo/server/master/state/MetaDataStateStore.java,"@@ -58,57 +54,31 @@ protected MetaDataStateStore(ClientContext context, CurrentState state, String t
     return new MetaDataTableScanner(context, TabletsSection.getRange(), state, targetTableName);
   }
 
-  @Override
   public void setLocations(Collection<Assignment> assignments) throws DistributedStoreException {
-    BatchWriter writer = createBatchWriter();
-    try {
+    try (var tabletsMutator = ample.mutateTablets()) {
       for (Assignment assignment : assignments) {
-        Mutation m = new Mutation(assignment.tablet.toMetaRow());
-        assignment.server.putLocation(m);
-        assignment.server.clearFutureLocation(m);
-        SuspendingTServer.clearSuspension(m);
-        writer.addMutation(m);
+        TabletMutator tabletMutator = tabletsMutator.mutateTablet(assignment.tablet);
+        tabletMutator.putLocation(assignment.server, LocationType.CURRENT);
+        tabletMutator.deleteLocation(assignment.server, LocationType.FUTURE);
+        tabletMutator.mutate();","[{'comment': 'Did you forget to include the `deleteSuspension()` here?\r\n\r\nAlso, the code might look cleaner if you chained the methods, something like (not formatted):\r\n\r\n```suggestion\r\n        tabletsMutator.mutateTablet(assignment.tablet)\r\n          .putLocation(assignment.server, LocationType.CURRENT)\r\n          .deleteLocation(assignment.server, LocationType.FUTURE)\r\n          .deleteSuspension()\r\n          .mutate();\r\n```\r\n\r\nIf the formatter makes it look too ugly, you could prevent the formatter from messing with it by wrapping it with a comment line before and after, `// @formatter:off` and `// @formatter:on`.\r\n\r\nThis kind of simplification can be done in other places in this PR also (I made a review comment elsewhere where it can be a one-liner.', 'commenter': 'ctubbsii'}, {'comment': 'Yes, that was an oversight on my part. Added it back to it and I will test how the one-liner gets formatted.', 'commenter': 'Manno15'}]"
1653,server/base/src/main/java/org/apache/accumulo/server/master/state/MetaDataStateStore.java,"@@ -128,73 +98,56 @@ public void suspend(Collection<TabletLocationState> tablets,
   private void unassign(Collection<TabletLocationState> tablets,
       Map<TServerInstance,List<Path>> logsForDeadServers, long suspensionTimestamp)
       throws DistributedStoreException {
-    BatchWriter writer = createBatchWriter();
-    try {
+    try (var tabletsMutator = ample.mutateTablets()) {
       for (TabletLocationState tls : tablets) {
-        Mutation m = new Mutation(tls.extent.toMetaRow());
+        TabletMutator tabletMutator = tabletsMutator.mutateTablet(tls.extent);
         if (tls.current != null) {
-          tls.current.clearLocation(m);
+          tabletMutator.deleteLocation(tls.current, LocationType.CURRENT);
           if (logsForDeadServers != null) {
             List<Path> logs = logsForDeadServers.get(tls.current);
             if (logs != null) {
               for (Path log : logs) {
                 LogEntry entry =
                     new LogEntry(tls.extent, 0, tls.current.hostPort(), log.toString());
-                m.put(entry.getColumnFamily(), entry.getColumnQualifier(), entry.getValue());
+                tabletMutator.putWal(entry);
               }
             }
           }
           if (suspensionTimestamp >= 0) {
-            SuspendingTServer suspender =
-                new SuspendingTServer(tls.current.getLocation(), suspensionTimestamp);
-            suspender.setSuspension(m);
+            tabletMutator.putSuspension(tls.current, suspensionTimestamp);
           }
         }
         if (tls.suspend != null && suspensionTimestamp < 0) {
-          SuspendingTServer.clearSuspension(m);
+          tabletMutator.deleteSuspension();
         }
         if (tls.future != null) {
-          tls.future.clearFutureLocation(m);
+          tabletMutator.deleteLocation(tls.future, LocationType.FUTURE);
         }
-        writer.addMutation(m);
+        tabletMutator.mutate();
       }
     } catch (Exception ex) {","[{'comment': ""If all of these code blocks that were updated to use the Ample mutator utility don't throw any checked exceptions, these catch blocks can be updated to only catch the more specific, `RuntimeException`, as in:\r\n```suggestion\r\n    } catch (RuntimeException ex) {\r\n```"", 'commenter': 'ctubbsii'}]"
1653,server/base/src/main/java/org/apache/accumulo/server/master/state/MetaDataStateStore.java,"@@ -128,73 +98,56 @@ public void suspend(Collection<TabletLocationState> tablets,
   private void unassign(Collection<TabletLocationState> tablets,
       Map<TServerInstance,List<Path>> logsForDeadServers, long suspensionTimestamp)
       throws DistributedStoreException {
-    BatchWriter writer = createBatchWriter();
-    try {
+    try (var tabletsMutator = ample.mutateTablets()) {
       for (TabletLocationState tls : tablets) {
-        Mutation m = new Mutation(tls.extent.toMetaRow());
+        TabletMutator tabletMutator = tabletsMutator.mutateTablet(tls.extent);
         if (tls.current != null) {
-          tls.current.clearLocation(m);
+          tabletMutator.deleteLocation(tls.current, LocationType.CURRENT);
           if (logsForDeadServers != null) {
             List<Path> logs = logsForDeadServers.get(tls.current);
             if (logs != null) {
               for (Path log : logs) {
                 LogEntry entry =
                     new LogEntry(tls.extent, 0, tls.current.hostPort(), log.toString());
-                m.put(entry.getColumnFamily(), entry.getColumnQualifier(), entry.getValue());
+                tabletMutator.putWal(entry);
               }
             }
           }
           if (suspensionTimestamp >= 0) {
-            SuspendingTServer suspender =
-                new SuspendingTServer(tls.current.getLocation(), suspensionTimestamp);
-            suspender.setSuspension(m);
+            tabletMutator.putSuspension(tls.current, suspensionTimestamp);
           }
         }
         if (tls.suspend != null && suspensionTimestamp < 0) {
-          SuspendingTServer.clearSuspension(m);
+          tabletMutator.deleteSuspension();
         }
         if (tls.future != null) {
-          tls.future.clearFutureLocation(m);
+          tabletMutator.deleteLocation(tls.future, LocationType.FUTURE);
         }
-        writer.addMutation(m);
+        tabletMutator.mutate();
       }
     } catch (Exception ex) {
       throw new DistributedStoreException(ex);
-    } finally {
-      try {
-        writer.close();
-      } catch (MutationsRejectedException e) {
-        throw new DistributedStoreException(e);
-      }
     }
   }
 
   @Override
   public void unsuspend(Collection<TabletLocationState> tablets) throws DistributedStoreException {
-    BatchWriter writer = createBatchWriter();
-    try {
+    try (var tabletsMutator = ample.mutateTablets()) {
       for (TabletLocationState tls : tablets) {
         if (tls.suspend != null) {
           continue;
         }
-        Mutation m = new Mutation(tls.extent.toMetaRow());
-        SuspendingTServer.clearSuspension(m);
-        writer.addMutation(m);
+        TabletMutator tabletMutator = tabletsMutator.mutateTablet(tls.extent);
+        tabletMutator.deleteSuspension();
+        tabletMutator.mutate();","[{'comment': '```suggestion\r\n        tabletsMutator.mutateTablet(tls.extent).deleteSuspension().mutate();\r\n```', 'commenter': 'ctubbsii'}]"
1667,pom.xml,"@@ -658,6 +658,9 @@
           <groupId>org.apache.maven.plugins</groupId>
           <artifactId>maven-jar-plugin</artifactId>
           <configuration>
+            <excludes>
+              <exclude>**/hadoop-metrics2-accumulo.properties</exclude>
+            </excludes>","[{'comment': 'This should probably exist only in the test module, rather than in the root pom.', 'commenter': 'ctubbsii'}, {'comment': ""Should I add a complete maven-jar-plugin to the test module pom?  I looked there but I did not see one configured. I thought about it, but was unsure if a new plugin configuration or promoting it to the parent would be preferred.  \r\n\r\nHaving it in the parent does have a side benefit that if any module ends up needing a metrics file for testing it would be excluded by default.\r\n\r\nIt's your call as to the preferred approach."", 'commenter': 'EdColeman'}, {'comment': ""Placing the plugin section in the test module pom with this configured there would be preferred, so it doesn't unintentionally apply to other modules, and also so it's more visible to people modifying the test module in the future, in case they modify it in a way that is impacted by this configuration.\r\n\r\nOther modules wouldn't be affected anyway, since they would use `src/test/resources`. The only reason the test module is affected is because it uses `src/main/resources`. If we actually wanted to intentionally put this file in another module's `src/main/resources`, we wouldn't want this configuration to accidentally omit it."", 'commenter': 'ctubbsii'}]"
1667,test/pom.xml,"@@ -270,6 +270,21 @@
           </execution>
         </executions>
       </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+        <configuration>
+          <excludes>
+            <exclude>**/hadoop-metrics2-accumulo.properties</exclude>
+          </excludes>
+          <archive>
+            <manifestEntries>
+              <Sealed>true</Sealed>
+              <Implementation-Build>${mvngit.commit.id}</Implementation-Build>
+            </manifestEntries>
+          </archive>","[{'comment': 'This can be omitted, as it would be merged with the configuration from the parent anyway. We only need to specify the portions that we wish to override in here.\r\n```suggestion\r\n          </excludes>\r\n```', 'commenter': 'ctubbsii'}]"
1667,test/pom.xml,"@@ -270,6 +270,15 @@
           </execution>
         </executions>
       </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+        <configuration>
+          <excludes>
+            <exclude>**/hadoop-metrics2-accumulo.properties</exclude>","[{'comment': ""It occurs to me... does this still allow metrics to work when we're doing testing? If it's not in the jar, how does it get on the class path?"", 'commenter': 'ctubbsii'}, {'comment': ""I was under the impression that the maven-jar-plugin would be called during assembly (package?)  If you build / test under maven, the property file will still be there.  So, any testing that uses classes / files under ..../target/...  should still work.\r\n\r\nYou are correct though, if you tried to run it tests using just the jars - unless you provided a metrics configuration file with metrics output configured, then the tests that check those files would fail. If you go to the trouble to run the tests outside of maven using a command line, then if you got that far, adding a suitable config file may not be much of a hurdle and forces you to provide a configuration that you want for your needs.\r\n\r\nOne issue is that the metrics config file is specifying ./target/name as a know location for the output files - this was so that mvn clean would remove them and they would not be left hanging around. \r\n\r\nI suppose that if there was another well know location that would always be suitable for writes, then that could be used instead.  Sounds like just specified /tmp, but that's making an assumption that /tmp would always be suitable - for example would docker / containers / aws / ... prefer not to use /tmp but something else?  And then there's the clean-up - would need something besides (or modification) mvn clean.  Sure, /tmp may eventually get cleaned, but seems good practice not to depend on that."", 'commenter': 'EdColeman'}, {'comment': ""`maven-jar-plugin` runs during the `package` phase of the default lifecycle. Usually, the `maven-assembly-plugin` is also configured to run during the `package` phase, but only if a user manually configured it, since it's not bound to the default lifecycle by default.\r\n\r\nIf our tests pass, I have no problem excluding it from the jar. Like you said, our metrics file is set for target directory, specifically, so it's clear it's only for our tests anyway. And, if our test code were reused, it wouldn't be hard for the caller to provide its own metrics config file.\r\n\r\nI would avoid `/tmp`. It's best to use the `target/` directory, and write to no other directories in a Maven build.\r\n\r\nMy final point of feedback, then, is just a suggestion to use the path explicitly, rather than wildcards:\r\n\r\n```suggestion\r\n            <exclude>src/main/resources/hadoop-metrics2-accumulo.properties</exclude>\r\n```"", 'commenter': 'ctubbsii'}, {'comment': ""Hit commit via ui, becuase - sure that's reasonable - and then decided to test - oops.  Anyway I don't think that works as intended - might be src is assumed?  Anyway, checking now..."", 'commenter': 'EdColeman'}, {'comment': ""Oh! The jar plugin's base directory might be target/classes, where the compiler plugin leaves class files and the resource plugin leaves resources. It might just work with the file name only, since this resource isn't in a subdirectory. Not sure."", 'commenter': 'ctubbsii'}, {'comment': 'So I basically tried more restrictive paths combinations and **/filename seems to be the ""most"" specific that actually excluded the file from the created jar - basically revered the UI generated commit. I examined the jars created with the current changes and run -Psunny and things look okay as it stands now.', 'commenter': 'EdColeman'}, {'comment': 'I tested the following more narrow construction, and it seems to work fine (see my last comment for why):\r\n\r\n```suggestion\r\n            <exclude>hadoop-metrics2-accumulo.properties</exclude>\r\n```', 'commenter': 'ctubbsii'}]"
1677,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooLock.java,"@@ -69,13 +70,14 @@
   private boolean watchingParent = false;
   private String asyncLock;
 
-  public ZooLock(ZooReaderWriter zoo, String path) {
+  public ZooLock(ZooReaderWriter zoo, String path, RetryFactory retryFactory) {","[{'comment': 'A new parameter is added, but not used.', 'commenter': 'ctubbsii'}]"
1677,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooReader.java,"@@ -38,24 +38,36 @@
 public class ZooReader {
   private static final Logger log = LoggerFactory.getLogger(ZooReader.class);
 
-  protected static final RetryFactory RETRY_FACTORY = Retry.builder().maxRetries(10)
+  public static final RetryFactory DEFAULT_RETRY_FACTORY = Retry.builder().maxRetries(10)
+      .retryAfter(250, MILLISECONDS).incrementBy(250, MILLISECONDS).maxWait(5, TimeUnit.SECONDS)
+      .backOffFactor(1.5).logInterval(3, TimeUnit.MINUTES).createFactory();
+
+  public static final RetryFactory DISABLED_RETRY_FACTORY = Retry.builder().maxRetries(0)
       .retryAfter(250, MILLISECONDS).incrementBy(250, MILLISECONDS).maxWait(5, TimeUnit.SECONDS)
       .backOffFactor(1.5).logInterval(3, TimeUnit.MINUTES).createFactory();","[{'comment': ""Can probably remove all this extra retry configuration since the max retries is zero here. I don't think these are all required parameters are they?"", 'commenter': 'ctubbsii'}, {'comment': 'Looking at the interfaces they are chained together such that they are needed.', 'commenter': 'dlmarion'}]"
1677,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooReaderWriter.java,"@@ -49,13 +49,14 @@
   public ZooReaderWriter(AccumuloConfiguration conf) {
     this(conf.get(Property.INSTANCE_ZK_HOST),
         (int) conf.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT),
-        conf.get(Property.INSTANCE_SECRET));
+        conf.get(Property.INSTANCE_SECRET), ZooReader.DISABLED_RETRY_FACTORY);","[{'comment': ""The behavioral differences between the constructors is unintuitive. Can we instead use a well-named static factory method, instead of the constructor to construct a ZRW for the specific intended purpose, so the constructor isn't unintentionally used by other code, and subject to the non-retrying behavior accidentally?\r\n\r\n```java\r\n// e.g.\r\nZooReaderWriter zrw1 = ZooReaderWriter.retrying(conf);\r\nZooReaderWriter zrw2 = ZooReaderWriter.nonRetrying(conf);\r\n// or\r\nZooReaderWriter zrw = ZooReaderWriter.forXuseOnly(conf);\r\n```"", 'commenter': 'ctubbsii'}]"
1677,server/manager/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1422,7 +1423,8 @@ private void getMasterLock(final String zMasterLoc) throws KeeperException, Inte
     while (true) {
 
       MasterLockWatcher masterLockWatcher = new MasterLockWatcher();
-      masterLock = new ZooLock(context.getZooReaderWriter(), zMasterLoc);
+      masterLock =
+          new ZooLock(context.getZooReaderWriter(), zMasterLoc, ZooReader.DISABLED_RETRY_FACTORY);","[{'comment': ""In these kinds of uses, the code reads as though ZooLock itself won't retry... this is a bit unintuitive, because of the unfortunate name."", 'commenter': 'ctubbsii'}]"
1677,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -1384,4 +1385,14 @@ public final RateLimiter getMajorCompactionWriteLimiter() {
   public CompactionManager getCompactionManager() {
     return compactionManager;
   }
+
+  /**
+   * For the tablet server we don't want ZooReaderWriter to keep retrying after a error
+   * communicating with ZooKeeper.
+   */
+  @Override
+  protected ServerContext createServerContext(SiteConfiguration siteConfig) {
+    return new ServerContext(siteConfig, false);
+  }
+","[{'comment': 'I worry that this will have a larger impact on the various ways ZooKeeper is used in a TabletServer, rather than be a narrow fix for ZooLock. The ServerContext is universal for the process. The only ZooReaderWriter instance for which we want to avoid retries is the one used by ZooLock. Is there a reason we need to change the behavior across the entire tserver?', 'commenter': 'ctubbsii'}]"
1677,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooReader.java,"@@ -38,24 +38,34 @@
 public class ZooReader {
   private static final Logger log = LoggerFactory.getLogger(ZooReader.class);
 
-  protected static final RetryFactory RETRY_FACTORY = Retry.builder().maxRetries(10)
+  private static final RetryFactory DEFAULT_RETRY_FACTORY = Retry.builder().maxRetries(10)
+      .retryAfter(250, MILLISECONDS).incrementBy(250, MILLISECONDS).maxWait(5, TimeUnit.SECONDS)
+      .backOffFactor(1.5).logInterval(3, TimeUnit.MINUTES).createFactory();
+
+  private static final RetryFactory DISABLED_RETRY_FACTORY = Retry.builder().maxRetries(0)
       .retryAfter(250, MILLISECONDS).incrementBy(250, MILLISECONDS).maxWait(5, TimeUnit.SECONDS)
       .backOffFactor(1.5).logInterval(3, TimeUnit.MINUTES).createFactory();
 
   protected final String keepers;
   protected final int timeout;
+  protected final RetryFactory retryFactory;
 
   public ZooReader(String keepers, int timeout) {
+    this(keepers, timeout, true);
+  }
+
+  public ZooReader(String keepers, int timeout, boolean enableRetries) {","[{'comment': 'Could pass in the RetryFactory rather than have a boolean param.', 'commenter': 'ctubbsii'}]"
1677,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooReaderWriter.java,"@@ -46,16 +46,32 @@
 
   private static final Logger log = LoggerFactory.getLogger(ZooReaderWriter.class);
 
-  public ZooReaderWriter(AccumuloConfiguration conf) {
-    this(conf.get(Property.INSTANCE_ZK_HOST),
+  public static ZooReaderWriter retriesEnabled(AccumuloConfiguration conf) {
+    return new ZooReaderWriter(conf.get(Property.INSTANCE_ZK_HOST),
         (int) conf.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT),
-        conf.get(Property.INSTANCE_SECRET));
+        conf.get(Property.INSTANCE_SECRET), true);
+  }
+
+  public static ZooReaderWriter retriesDisabled(AccumuloConfiguration conf) {
+    return new ZooReaderWriter(conf.get(Property.INSTANCE_ZK_HOST),
+        (int) conf.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT),
+        conf.get(Property.INSTANCE_SECRET), false);
+  }
+
+  public static ZooReaderWriter retriesEnabled(String keepers, int timeoutInMillis, String secret) {
+    return new ZooReaderWriter(keepers, timeoutInMillis, secret, true);
+  }
+
+  public static ZooReaderWriter retriesDisabled(String keepers, int timeoutInMillis,
+      String secret) {
+    return new ZooReaderWriter(keepers, timeoutInMillis, secret, false);
   }
 
   private final byte[] auth;
 
-  public ZooReaderWriter(String keepers, int timeoutInMillis, String secret) {
-    super(keepers, timeoutInMillis);
+  private ZooReaderWriter(String keepers, int timeoutInMillis, String secret,
+      boolean enableRetries) {
+    super(keepers, timeoutInMillis, enableRetries);","[{'comment': 'Could pass in the retry factory rather than pass a boolean around. That would make the code more clear.', 'commenter': 'ctubbsii'}]"
1677,server/base/src/main/java/org/apache/accumulo/server/ServerContext.java,"@@ -68,13 +68,22 @@
   private CryptoService cryptoService = null;
 
   public ServerContext(SiteConfiguration siteConfig) {
-    this(new ServerInfo(siteConfig));
+    this(new ServerInfo(siteConfig), true);
+  }
+
+  public ServerContext(SiteConfiguration siteConfig, boolean enableZookeeperRetries) {
+    this(new ServerInfo(siteConfig), enableZookeeperRetries);","[{'comment': 'Could pass in the retry factory rather than pass around booleans.', 'commenter': 'ctubbsii'}]"
1677,server/base/src/main/java/org/apache/accumulo/server/ServerContext.java,"@@ -68,13 +68,22 @@
   private CryptoService cryptoService = null;
 
   public ServerContext(SiteConfiguration siteConfig) {
-    this(new ServerInfo(siteConfig));
+    this(new ServerInfo(siteConfig), true);
+  }
+
+  public ServerContext(SiteConfiguration siteConfig, boolean enableZookeeperRetries) {
+    this(new ServerInfo(siteConfig), enableZookeeperRetries);
   }
 
   private ServerContext(ServerInfo info) {
+    this(info, true);
+  }
+
+  private ServerContext(ServerInfo info, boolean enableRetries) {
     super(SingletonReservation.noop(), info, info.getSiteConfiguration());
     this.info = info;
-    zooReaderWriter = new ZooReaderWriter(info.getSiteConfiguration());
+    zooReaderWriter = enableRetries ? ZooReaderWriter.retriesEnabled(info.getSiteConfiguration())
+        : ZooReaderWriter.retriesDisabled(info.getSiteConfiguration());","[{'comment': ""This worries me. It seems outside the scope of ServerContext to be handling this kind of thing. ServerContext should merely be a holder of the ZRW that is available to the server for updating/reading from ZK. The affected code is in ZooLock, and is not a problem with ZooReaderWriter's retry behavior more generally."", 'commenter': 'ctubbsii'}, {'comment': 'I am not sure this a good idea, but its something I thought of after reading this comment.  Could make every ZooReaderWriter method accept a retry factory and have overloaded versions that do not take a retry factory and use a default.  This  seems tedious though, but its the best I could think of so far to associate the retry logic with method calls instead of the object. ', 'commenter': 'keith-turner'}, {'comment': ""If we do something like add a retry to every method, I'd like to revisit my effort to clean up the ZooReaderWriter stuff, like what I had started to do in #1459 ; it would be easier to add something like a retry to every method, if the code were cleaned up a bit first. I can revisit my previous cleanup effort over the next week.\r\n\r\nIn the meantime, I still think it's probably best to simply avoid the problematic ZRW code in the ZooLock. ZooLock does not need to use the general-purpose utility code, when it can use something specific to its needs."", 'commenter': 'ctubbsii'}, {'comment': ""Thanks for the feedback on the PR. I read back through the comments on #1086 and the comments above. My previous attempts were to create a code path through the existing objects that didn't retry instead of creating something new that also talks with ZK. It sounds as if you both might be leaning toward a solution that does not use ZooLock for the tablet server lock, but uses something else that does not retry and communicates directly with ZooKeeper. Is that correct?"", 'commenter': 'dlmarion'}, {'comment': '@ctubbsii - did you want me to wait for #1459 to be completed before revisiting this?', 'commenter': 'dlmarion'}, {'comment': ""> It sounds as if you both might be leaning toward a solution that does not use ZooLock for the tablet server lock, but uses something else that does not retry and communicates directly with ZooKeeper. Is that correct?\r\n\r\nI don't think that's quite right. I don't think it makes sense to distance ourselves from using ZooLock for locking (doesn't it exist for that purpose?). Rather, I think we're suggesting that the implementation of ZooLock should be changed to distance itself from the general-purpose utilities of ZooReaderWriter/ZooSession. This can be done by making ZooLock never use those at all and use something of its own, or as Keith suggested, adding special-purpose methods on those utilities for ZooLock to safely use.\r\n\r\n> @ctubbsii - did you want me to wait for #1459 to be completed before revisiting this?\r\n\r\nThat depends. #1459 had no intention of touching ZooLock, so if you want to try to make ZooLock work without ZooReaderWriter/ZooSession, then your efforts should not conflict with that and can proceed independently. If, however, you want to try to add special-purpose methods for ZooLock on ZooReaderWriter/ZooSession, then it should wait, and I can prioritize my efforts next week to clean those up to make it easier for you.\r\n"", 'commenter': 'ctubbsii'}, {'comment': ""> Rather, I think we're suggesting that the implementation of ZooLock should be changed to distance itself from the general-purpose utilities of ZooReaderWriter/ZooSession. This can be done by making ZooLock never use those at all and use something of its own, or as Keith suggested, adding special-purpose methods on those utilities for ZooLock to safely use.\r\n\r\nUnderstood, I'll work down that path. Thanks."", 'commenter': 'dlmarion'}, {'comment': '@dlmarion  I was thinking of looking into making ZooLock use Zookeeper directly to see if that is workable.', 'commenter': 'keith-turner'}]"
1701,core/src/main/java/org/apache/accumulo/core/client/PluginEnvironment.java,"@@ -55,6 +55,15 @@
      */
     String get(String key);
 
+    /**
+     * Returns all properties with a given prefix
+     *
+     * @param prefix
+     *          prefix of properties to be returned
+     * @return all properties with a given prefix
+     */","[{'comment': ""I know this should probably have a `@since` tag, I'm just not sure what version to put. Please let me know and I'll add it."", 'commenter': 'jkosh44'}, {'comment': '2.1.0 would be the version we would add this kind of thing to.', 'commenter': 'ctubbsii'}, {'comment': 'Thanks, added', 'commenter': 'jkosh44'}]"
1701,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -1233,6 +1233,18 @@ public static boolean isValidPropertyKey(String key) {
     return validProperties.contains(key) || isKeyValidlyPrefixed(key);
   }
 
+  /**
+   * Checks if the given property prefix is valid. A valid property prefix is equal to some prefix
+   * defined in this class.
+   *
+   * @param prefix
+   *          property prefix
+   * @return true if prefix is valid (recognized)
+   */
+  public static boolean isValidPropertyPrefix(String prefix) {
+    return validPrefixes.contains(prefix);
+  }
+","[{'comment': ""This method isn't necessary, since this loop can be avoided.\r\n```suggestion\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Removed', 'commenter': 'jkosh44'}]"
1701,server/base/src/main/java/org/apache/accumulo/server/ServiceEnvironmentImpl.java,"@@ -78,6 +79,22 @@ public String get(String key) {
       }
     }
 
+    @Override
+    public Map<String,String> getWithPrefix(String prefix) {
+      if (Property.isValidPropertyPrefix(prefix)) {
+        Property propertyPrefix = Property.getPropertyByKey(prefix);","[{'comment': 'This check for valid prefix property can be avoided, by checking the returned object from `Property.getPropertyByKey(prefix)`:\r\n\r\n```suggestion\r\n      Property propertyPrefix = Property.getPropertyByKey(prefix);\r\n      if (propertyPrefix != null && propertyPrefix.getType() == PropertyType.PREFIX) {\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Fixed', 'commenter': 'jkosh44'}]"
1701,server/base/src/main/java/org/apache/accumulo/server/ServiceEnvironmentImpl.java,"@@ -78,6 +79,22 @@ public String get(String key) {
       }
     }
 
+    @Override
+    public Map<String,String> getWithPrefix(String prefix) {
+      if (Property.isValidPropertyPrefix(prefix)) {
+        Property propertyPrefix = Property.getPropertyByKey(prefix);
+        return acfg.getAllPropertiesWithPrefix(propertyPrefix);
+      } else {
+        Map<String,String> properties = new HashMap<>();
+        for (Entry<String,String> prop : acfg) {
+          if (prop.getKey().startsWith(prefix)) {
+            properties.put(prop.getKey(), prop.getValue());
+          }
+        }
+        return properties;
+      }","[{'comment': 'This can be more succinctly written with streams:\r\n```suggestion\r\n        return StreamSupport.stream(acfg.spliterator(), false)\r\n            .filter(prop -> prop.getKey().startsWith(prefix))\r\n            .collect(Collectors.toMap(Entry::getKey, Entry::getValue));\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Fixed', 'commenter': 'jkosh44'}]"
1701,server/base/src/test/java/org/apache/accumulo/server/ServiceEnvironmentImplTest.java,"@@ -44,6 +46,12 @@ public void setUp() {
     serviceEnvironment = new ServiceEnvironmentImpl(srvCtx);
   }
 
+  @After
+  public void verifyMocks() {
+    verify(srvCtx);
+    verify(acfg);
+  }
+","[{'comment': 'The verify method is actually a varargs, so you can pass both in one:\r\n\r\n```suggestion\r\n  @After\r\n  public void verifyMocks() {\r\n    verify(srvCtx, acfg);\r\n  }\r\n\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Fixed', 'commenter': 'jkosh44'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/AccumuloClassLoader.java,"@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.lang.reflect.InvocationTargetException;
+
+import org.apache.accumulo.core.spi.common.ClassLoaderFactory;
+import org.apache.accumulo.core.spi.common.ClassLoaderFactory.Printer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class AccumuloClassLoader {
+
+  private static final Logger LOG = LoggerFactory.getLogger(AccumuloClassLoader.class);
+
+  private static final String CLASS_LOADER_FACTORY = ""general.class.loader.factory"";","[{'comment': 'It may be nice to have an `accumulo` prefix in this property, since its a java system property and not an Accumulo configuration property.  Maybe something like `accumulo.server.class.loader.factory`.  This would be similar to the way log4j prefixes its Java system properties.', 'commenter': 'keith-turner'}]"
1715,core/src/main/java/org/apache/accumulo/core/spi/common/ClassLoaderFactory.java,"@@ -18,44 +18,33 @@
  */
 package org.apache.accumulo.core.spi.common;
 
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.Map.Entry;
-
 /**
- * The ClassLoaderFactory is defined by the property general.context.factory. The factory
- * implementation is configured externally to Accumulo and will return a ClassLoader for a given
- * contextName.
- *
+ * The ClassLoaderFactory implementation is defined by the property general.class.loader.factory.","[{'comment': 'This javadoc could mention its a Java system property.', 'commenter': 'keith-turner'}]"
1715,start/src/main/java/org/apache/accumulo/start/Main.java,"@@ -86,42 +63,42 @@ public static void main(final String[] args) {
       // determine whether a keyword was used or a class name, and execute it with the remaining
       // args
       String keywordOrClassName = args[0];
-      KeywordExecutable keywordExec = getExecutables(loader).get(keywordOrClassName);
+      KeywordExecutable keywordExec = getExecutables(CLASSLOADER).get(keywordOrClassName);
       if (keywordExec != null) {
         execKeyword(keywordExec, stripArgs(args, 1));
       } else {
         execMainClassName(keywordOrClassName, stripArgs(args, 1));
       }
 
     } catch (Throwable t) {
-      log.error(""Uncaught exception"", t);
+      LOG.error(""Uncaught exception"", t);
       System.exit(1);
     }
   }
 
-  public static synchronized ClassLoader getClassLoader() {
-    if (classLoader == null) {
-      try {
-        classLoader = (ClassLoader) getVFSClassLoader().getMethod(""getClassLoader"").invoke(null);
-        Thread.currentThread().setContextClassLoader(classLoader);
-      } catch (IOException | IllegalArgumentException | ReflectiveOperationException
-          | SecurityException e) {
-        log.error(""Problem initializing the class loader"", e);
-        System.exit(1);
-      }
-    }
-    return classLoader;
-  }
-
-  public static synchronized Class<?> getVFSClassLoader()
-      throws IOException, ClassNotFoundException {
-    if (vfsClassLoader == null) {
-      Thread.currentThread().setContextClassLoader(AccumuloClassLoader.getClassLoader());
-      vfsClassLoader = AccumuloClassLoader.getClassLoader()
-          .loadClass(""org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader"");
-    }
-    return vfsClassLoader;
-  }
+  // public static synchronized ClassLoader getClassLoader() {","[{'comment': 'Why is this commented out code here?', 'commenter': 'keith-turner'}]"
1715,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.common;
+
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.Map.Entry;
+
+/**
+ * The ContextClassLoaderFactory implementation is defined by the property
+ * general.context.class.loader.factory. The implementation is configured externally to Accumulo and
+ * will return a ClassLoader for a given contextName.","[{'comment': 'Javadoc needs a since tag', 'commenter': 'keith-turner'}]"
1715,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.common;
+
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.Map.Entry;
+
+/**
+ * The ContextClassLoaderFactory implementation is defined by the property
+ * general.context.class.loader.factory. The implementation is configured externally to Accumulo and
+ * will return a ClassLoader for a given contextName.
+ */
+public interface ContextClassLoaderFactory {
+
+  static class ClassLoaderFactoryConfiguration {","[{'comment': ""I suspect this needs javadoc w/ since tag.  I don't think Inner classes inherit since tags from outer classes javadoc, but not 100% sure."", 'commenter': 'keith-turner'}, {'comment': ""They shouldn't inherit from the outer class, but it's also package-private, so it shouldn't matter."", 'commenter': 'ctubbsii'}, {'comment': 'I think all members of interfaces are implicitly public.', 'commenter': 'keith-turner'}]"
1715,core/src/main/java/org/apache/accumulo/core/spi/common/ClassLoaderFactory.java,"@@ -18,44 +18,33 @@
  */
 package org.apache.accumulo.core.spi.common;
 
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.Map.Entry;
-
 /**
- * The ClassLoaderFactory is defined by the property general.context.factory. The factory
- * implementation is configured externally to Accumulo and will return a ClassLoader for a given
- * contextName.
- *
+ * The ClassLoaderFactory implementation is defined by the property general.class.loader.factory.
+ * The implementation will return a ClassLoader to be used for dynamically loading classes.
  */
 public interface ClassLoaderFactory {
 
-  static class ClassLoaderFactoryConfiguration {
-
-    public Iterator<Entry<String,String>> get() {
-      return Collections.emptyIterator();
-    }
+  public interface Printer {
+    void print(String s);
   }
 
   /**
-   * Initialize the ClassLoaderFactory. Implementations may need a reference to the configuration so
-   * that it can clean up contexts that are no longer being used.
+   * Return the configured classloader
    *
-   * @param conf
-   *          Accumulo configuration properties
-   * @throws Exception
-   *           if error initializing ClassLoaderFactory
+   * @return classloader the configured classloader
    */
-  void initialize(ClassLoaderFactoryConfiguration conf) throws Exception;
+  ClassLoader getClassLoader() throws Exception;
 
   /**
+   * Print the classpath to the Printer
    *
-   * @param contextName
-   *          name of classloader context
-   * @return classloader configured for the context
-   * @throws IllegalArgumentException
-   *           if contextName is not supported
+   * @param cl
+   *          classloader
+   * @param out
+   *          printer
+   * @param debug
+   *          enable debug output
    */
-  ClassLoader getClassLoader(String contextName) throws IllegalArgumentException;
+  void printClassPath(ClassLoader cl, Printer out, boolean debug);","[{'comment': 'For SPI methods that take multiple arguments other SPI interfaces use Parameters interfaces.  This makes it easy to add more parameters to the method in the future w/o causing issues for existing implementers.  Below is one example of this.\r\n\r\nhttps://github.com/apache/accumulo/blob/80ee9ca8092dc4bf07aa1a046d6e668f0e16300e/core/src/main/java/org/apache/accumulo/core/spi/scan/ScanDispatcher.java#L62', 'commenter': 'keith-turner'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -0,0 +1,32 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+public class ClassLoaderUtil {
+
+  public static synchronized <U> Class<? extends U> loadClass(String contextName, String className,","[{'comment': ""In the past when running test against Accumulo w/ lots of concurrent scans and profiling tablets servers, the static synchronization around class loading was something that showed up in profiling data.  Every scan uses class loaders to create iterators and this static synch causes locking contention between scans.   This in an existing problem with the current code and I don't think changes are needed here, seeing this reminded me of the problem.  I need to see if I opened an issue for this."", 'commenter': 'keith-turner'}, {'comment': ""Now that you bring it up, I'm not sure that this needs to be synchronized."", 'commenter': 'dlmarion'}, {'comment': 'Looking into this overall issue a bit further in the existing code, iterators call AccumuloVFSClassLoader.loadClass() which is static and synchronized.  Also the iterator loading code has a class cache, maybe this was created to avoid the global synchronization.  Also I did not find an issue, will open one after looking into it a bit more.', 'commenter': 'keith-turner'}, {'comment': ""The cache is only narrowly used, so the synchronization is probably still problematic I'll open an issue."", 'commenter': 'keith-turner'}, {'comment': 'Created #1730', 'commenter': 'keith-turner'}]"
1715,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.common;
+
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.Map.Entry;
+
+/**
+ * The ContextClassLoaderFactory implementation is defined by the property
+ * general.context.class.loader.factory. The implementation is configured externally to Accumulo and
+ * will return a ClassLoader for a given contextName.
+ */
+public interface ContextClassLoaderFactory {
+
+  static class ClassLoaderFactoryConfiguration {
+
+    public Iterator<Entry<String,String>> get() {
+      return Collections.emptyIterator();
+    }
+  }
+
+  /**
+   * Initialize the ClassLoaderFactory. Implementations may need a reference to the configuration so
+   * that it can clean up contexts that are no longer being used.
+   *
+   * @param conf
+   *          Accumulo configuration properties
+   * @throws Exception
+   *           if error initializing ClassLoaderFactory
+   */
+  void initialize(ClassLoaderFactoryConfiguration conf) throws Exception;","[{'comment': 'Could name the class InitializationParameters to be consistent w/ other SPI interfaces.', 'commenter': 'keith-turner'}]"
1715,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.common;
+
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.Map.Entry;
+
+/**
+ * The ContextClassLoaderFactory implementation is defined by the property
+ * general.context.class.loader.factory. The implementation is configured externally to Accumulo and
+ * will return a ClassLoader for a given contextName.
+ */
+public interface ContextClassLoaderFactory {
+
+  static class ClassLoaderFactoryConfiguration {
+
+    public Iterator<Entry<String,String>> get() {","[{'comment': 'Other SPI interfaces use ServiceEnvironment to expose Accumulo configuration to plugins.\r\n\r\nhttps://github.com/apache/accumulo/blob/80ee9ca8092dc4bf07aa1a046d6e668f0e16300e/core/src/main/java/org/apache/accumulo/core/spi/common/ServiceEnvironment.java#L35', 'commenter': 'keith-turner'}]"
1715,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.common;
+
+import java.util.Map;
+import java.util.function.Supplier;
+
+/**
+ * The ClassLoaderFactory is defined by the property general.context.factory. The factory
+ * implementation is configured externally to Accumulo and will return a ClassLoader for a given
+ * contextName.
+ *
+ * @since 2.1.0
+ *
+ */
+public interface ContextClassLoaderFactory {
+
+  /**
+   * Initialize the ClassLoaderFactory. Implementations may need a reference to the configuration so
+   * that it can clean up contexts that are no longer being used.
+   *
+   * @param contextProperties
+   *          Accumulo configuration properties
+   * @throws Exception
+   *           if error initializing ClassLoaderFactory
+   */
+  void initialize(Supplier<Map<String,String>> contextProperties) throws Exception;","[{'comment': ""The following are the rational behind the suggestion below.\r\n * Using a parameters interface makes evolving the SPI over time much simpler.  Its easy to add additional parameters in the future w/o breaking existing code that was written against the SPI.\r\n * Using the [Configuration](https://github.com/apache/accumulo/blob/353c611891a8b88b307dcba5e884467d4191b1f2/core/src/main/java/org/apache/accumulo/core/spi/common/ServiceEnvironment.java#L40) interface used by other SPI interface is consistent and offers more options.  For example this interface offers an isSet() method that allows checking if a user set a property.  There is no way to do this by analyzing the values because some props have default values.\r\n\r\n```suggestion\r\n  public interface InitParameters {\r\n      /**\r\n   * @return A view of Accumulo's system level configuration. This is backed by system level config\r\n   *         in zookeeper, which falls back to site configuration, which falls back to the default\r\n   *         configuration.\r\n   */\r\n    Configuration getConfiguration();\r\n  }\r\n\r\n  /**\r\n   * Initialize the ClassLoaderFactory. Implementations may need a reference to the configuration so\r\n   * that it can clean up contexts that are no longer being used.\r\n   *\r\n   * @throws Exception\r\n   *           if error initializing ClassLoaderFactory\r\n   */\r\n  void initialize(InitParameters initParam) throws Exception;\r\n```"", 'commenter': 'keith-turner'}, {'comment': '@keith-turner It looks like ServiceEnvironmentImpl and ConfigurationImpl are the classes used to create the Configuration used in this manner. Both of these objects are in server-base, which depends on accumulo-core. Suggestions on how to move forward? I think ContextClassLoaderFactory should remain in core as that is where the SPI classes are. It looks like I might be able to move ServiceEnvironmentImpl up to accumulo-core.', 'commenter': 'dlmarion'}, {'comment': ""I can't move ServiceEnvironmentImpl up to accumulo-core due to its depedency on ServerContext."", 'commenter': 'dlmarion'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ContextClassLoaders.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.lang.reflect.InvocationTargetException;
+import java.util.Map;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ContextClassLoaders {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ContextClassLoaders.class);
+
+  public static final String CONTEXT_CLASS_LOADER_FACTORY = ""general.context.class.loader.factory"";
+
+  private static ContextClassLoaderFactory FACTORY;","[{'comment': ""Since initialize isn't synchronized, do we want to do something here to prevent this from being changed (by a competing thread) once set?"", 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ContextClassLoaders.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.lang.reflect.InvocationTargetException;
+import java.util.Map;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ContextClassLoaders {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ContextClassLoaders.class);
+
+  public static final String CONTEXT_CLASS_LOADER_FACTORY = ""general.context.class.loader.factory"";
+
+  private static ContextClassLoaderFactory FACTORY;
+  private static Supplier<Map<String,String>> CONF;
+
+  /**
+   * Initialize the ContextClassLoaderFactory
+   *
+   * @param conf
+   *          AccumuloConfiguration object
+   */
+  @SuppressWarnings(""unchecked"")","[{'comment': 'Can we move this warnings suppression closer to the objects that it applies to, if we must have it?', 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ContextClassLoaders.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.lang.reflect.InvocationTargetException;
+import java.util.Map;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ContextClassLoaders {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ContextClassLoaders.class);
+
+  public static final String CONTEXT_CLASS_LOADER_FACTORY = ""general.context.class.loader.factory"";
+
+  private static ContextClassLoaderFactory FACTORY;
+  private static Supplier<Map<String,String>> CONF;
+
+  /**
+   * Initialize the ContextClassLoaderFactory
+   *
+   * @param conf
+   *          AccumuloConfiguration object
+   */
+  @SuppressWarnings(""unchecked"")
+  public static void initialize(Supplier<Map<String,String>> conf) throws Exception {
+    if (null == CONF) {
+      CONF = conf;
+      LOG.info(""Creating ContextClassLoaderFactory"");
+      var factoryName = CONF.get().get(Property.GENERAL_CONTEXT_CLASSLOADER_FACTORY.toString());","[{'comment': 'I think this should call `.getKey()` instead of `.toString()`', 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/LegacyVFSContextClassLoaderFactory.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+import org.apache.accumulo.start.classloader.vfs.ContextManager;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Deprecated(since = ""2.1.0"", forRemoval = true)
+public class LegacyVFSContextClassLoaderFactory implements ContextClassLoaderFactory {
+
+  private static final Logger LOG =
+      LoggerFactory.getLogger(LegacyVFSContextClassLoaderFactory.class);
+
+  public void initialize(Supplier<Map<String,String>> contextProperties) {
+    try {
+      AccumuloVFSClassLoader.getContextManager()
+          .setContextConfig(new ContextManager.DefaultContextsConfig() {
+            @Override
+            public Map<String,String> getVfsContextClasspathProperties() {
+              return contextProperties.get();
+            }
+          });
+      LOG.debug(""ContextManager configuration set"");
+      new Timer(""LegacyVFSContextClassLoaderFactory-cleanup"", true)
+          .scheduleAtFixedRate(new TimerTask() {
+            @Override
+            public void run() {
+              try {
+                if (LOG.isTraceEnabled()) {
+                  LOG.trace(""LegacyVFSContextClassLoaderFactory-cleanup thread, properties: {}"",
+                      contextProperties.get());
+                }
+                Set<String> configuredContexts = new HashSet<>();
+                contextProperties.get().keySet().forEach(k -> {
+                  if (k.startsWith(Property.VFS_CONTEXT_CLASSPATH_PROPERTY.toString())) {","[{'comment': 'Should use `.getKey()` instead of `.toString()`.', 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/LegacyVFSContextClassLoaderFactory.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+import org.apache.accumulo.start.classloader.vfs.ContextManager;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Deprecated(since = ""2.1.0"", forRemoval = true)
+public class LegacyVFSContextClassLoaderFactory implements ContextClassLoaderFactory {
+
+  private static final Logger LOG =
+      LoggerFactory.getLogger(LegacyVFSContextClassLoaderFactory.class);
+
+  public void initialize(Supplier<Map<String,String>> contextProperties) {
+    try {
+      AccumuloVFSClassLoader.getContextManager()
+          .setContextConfig(new ContextManager.DefaultContextsConfig() {
+            @Override
+            public Map<String,String> getVfsContextClasspathProperties() {
+              return contextProperties.get();
+            }
+          });
+      LOG.debug(""ContextManager configuration set"");
+      new Timer(""LegacyVFSContextClassLoaderFactory-cleanup"", true)
+          .scheduleAtFixedRate(new TimerTask() {
+            @Override
+            public void run() {
+              try {
+                if (LOG.isTraceEnabled()) {
+                  LOG.trace(""LegacyVFSContextClassLoaderFactory-cleanup thread, properties: {}"",
+                      contextProperties.get());
+                }
+                Set<String> configuredContexts = new HashSet<>();
+                contextProperties.get().keySet().forEach(k -> {
+                  if (k.startsWith(Property.VFS_CONTEXT_CLASSPATH_PROPERTY.toString())) {
+                    configuredContexts.add(
+                        k.substring(Property.VFS_CONTEXT_CLASSPATH_PROPERTY.toString().length()));
+                  }
+                });
+                LOG.trace(""LegacyVFSContextClassLoaderFactory-cleanup thread, contexts in use: {}"",
+                    configuredContexts);
+                AccumuloVFSClassLoader.getContextManager().removeUnusedContexts(configuredContexts);
+              } catch (IOException e) {
+                LOG.warn(""{}"", e.getMessage(), e);
+              }
+            }
+          }, 60000, 60000);
+      LOG.debug(""Context cleanup timer started at 60s intervals"");
+    } catch (IOException e) {
+      throw new RuntimeException(e);","[{'comment': 'UncheckedIOException should be used to wrap IOException as a runtime exception.', 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/LegacyVFSContextClassLoaderFactory.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+import org.apache.accumulo.start.classloader.vfs.ContextManager;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Deprecated(since = ""2.1.0"", forRemoval = true)
+public class LegacyVFSContextClassLoaderFactory implements ContextClassLoaderFactory {
+
+  private static final Logger LOG =
+      LoggerFactory.getLogger(LegacyVFSContextClassLoaderFactory.class);
+
+  public void initialize(Supplier<Map<String,String>> contextProperties) {
+    try {
+      AccumuloVFSClassLoader.getContextManager()
+          .setContextConfig(new ContextManager.DefaultContextsConfig() {
+            @Override
+            public Map<String,String> getVfsContextClasspathProperties() {
+              return contextProperties.get();
+            }
+          });
+      LOG.debug(""ContextManager configuration set"");
+      new Timer(""LegacyVFSContextClassLoaderFactory-cleanup"", true)
+          .scheduleAtFixedRate(new TimerTask() {
+            @Override
+            public void run() {
+              try {
+                if (LOG.isTraceEnabled()) {
+                  LOG.trace(""LegacyVFSContextClassLoaderFactory-cleanup thread, properties: {}"",
+                      contextProperties.get());
+                }
+                Set<String> configuredContexts = new HashSet<>();
+                contextProperties.get().keySet().forEach(k -> {
+                  if (k.startsWith(Property.VFS_CONTEXT_CLASSPATH_PROPERTY.toString())) {
+                    configuredContexts.add(
+                        k.substring(Property.VFS_CONTEXT_CLASSPATH_PROPERTY.toString().length()));
+                  }
+                });
+                LOG.trace(""LegacyVFSContextClassLoaderFactory-cleanup thread, contexts in use: {}"",
+                    configuredContexts);
+                AccumuloVFSClassLoader.getContextManager().removeUnusedContexts(configuredContexts);
+              } catch (IOException e) {
+                LOG.warn(""{}"", e.getMessage(), e);
+              }
+            }
+          }, 60000, 60000);
+      LOG.debug(""Context cleanup timer started at 60s intervals"");
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+
+  }
+
+  @Override
+  public ClassLoader getClassLoader(String contextName) throws IllegalArgumentException {
+    try {
+      return AccumuloVFSClassLoader.getContextManager().getClassLoader(contextName);
+    } catch (IOException e) {
+      throw new RuntimeException(""Error getting context class loader for context: "" + contextName,","[{'comment': 'UncheckedIOException', 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+
+public class ClassLoaderUtil {
+
+  public static <U> Class<? extends U> loadClass(String contextName, String className,
+      Class<U> extension) throws ClassNotFoundException {
+    if (contextName != null && !contextName.equals(""""))
+      return ContextClassLoaders.getClassLoader(contextName).loadClass(className)
+          .asSubclass(extension);
+    else
+      return AccumuloVFSClassLoader.loadClass(className, extension);
+
+  }","[{'comment': ""Is there not an appropriate existing utility class where this single static method can live? I think there's similar methods in the ConfigurationTypeHelper class."", 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;
+
+public class ClassLoaderUtil {
+
+  public static <U> Class<? extends U> loadClass(String contextName, String className,
+      Class<U> extension) throws ClassNotFoundException {
+    if (contextName != null && !contextName.equals(""""))","[{'comment': '```suggestion\r\n    if (contextName != null && !contextName.isEmpty())\r\n```', 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ContextClassLoaders.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.lang.reflect.InvocationTargetException;
+import java.util.Map;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ContextClassLoaders {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ContextClassLoaders.class);
+
+  public static final String CONTEXT_CLASS_LOADER_FACTORY = ""general.context.class.loader.factory"";
+
+  private static ContextClassLoaderFactory FACTORY;
+  private static Supplier<Map<String,String>> CONF;
+
+  /**
+   * Initialize the ContextClassLoaderFactory
+   *
+   * @param conf
+   *          AccumuloConfiguration object
+   */
+  @SuppressWarnings(""unchecked"")
+  public static void initialize(Supplier<Map<String,String>> conf) throws Exception {
+    if (null == CONF) {
+      CONF = conf;
+      LOG.info(""Creating ContextClassLoaderFactory"");
+      var factoryName = CONF.get().get(Property.GENERAL_CONTEXT_CLASSLOADER_FACTORY.toString());
+      if (null == factoryName || factoryName.isBlank()) {
+        LOG.info(""No ClassLoaderFactory specified"");
+        return;
+      }
+      try {
+        var factoryClass = Class.forName(factoryName);","[{'comment': ""You used `isBlank` above to check this instead of `isEmpty`, which implies that the string is not already stripped of whitespace. So, that implies `factoryName` might need to be stripped of surrounding whitespace. If it's not possible for it to have surrounding whitespace, then `isEmpty` might be more appropriate above than `isBlank`."", 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ContextClassLoaders.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.lang.reflect.InvocationTargetException;
+import java.util.Map;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ContextClassLoaders {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ContextClassLoaders.class);
+
+  public static final String CONTEXT_CLASS_LOADER_FACTORY = ""general.context.class.loader.factory"";
+
+  private static ContextClassLoaderFactory FACTORY;
+  private static Supplier<Map<String,String>> CONF;
+
+  /**
+   * Initialize the ContextClassLoaderFactory
+   *
+   * @param conf
+   *          AccumuloConfiguration object
+   */
+  @SuppressWarnings(""unchecked"")
+  public static void initialize(Supplier<Map<String,String>> conf) throws Exception {
+    if (null == CONF) {
+      CONF = conf;
+      LOG.info(""Creating ContextClassLoaderFactory"");
+      var factoryName = CONF.get().get(Property.GENERAL_CONTEXT_CLASSLOADER_FACTORY.toString());
+      if (null == factoryName || factoryName.isBlank()) {
+        LOG.info(""No ClassLoaderFactory specified"");
+        return;
+      }
+      try {
+        var factoryClass = Class.forName(factoryName);
+        if (ContextClassLoaderFactory.class.isAssignableFrom(factoryClass)) {
+          LOG.info(""Creating ContextClassLoaderFactory: {}"", factoryName);
+          FACTORY = ((Class<? extends ContextClassLoaderFactory>) factoryClass)
+              .getDeclaredConstructor().newInstance();
+          FACTORY.initialize(new Supplier<Map<String,String>>() {
+            @Override
+            public Map<String,String> get() {
+              return CONF.get();
+            }
+          });","[{'comment': '```suggestion\r\n          FACTORY.initialize(() -> CONF.get());\r\n```', 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/classloader/ContextClassLoaders.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.classloader;
+
+import java.lang.reflect.InvocationTargetException;
+import java.util.Map;
+import java.util.function.Supplier;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.spi.common.ContextClassLoaderFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ContextClassLoaders {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ContextClassLoaders.class);
+
+  public static final String CONTEXT_CLASS_LOADER_FACTORY = ""general.context.class.loader.factory"";
+
+  private static ContextClassLoaderFactory FACTORY;
+  private static Supplier<Map<String,String>> CONF;
+
+  /**
+   * Initialize the ContextClassLoaderFactory
+   *
+   * @param conf
+   *          AccumuloConfiguration object
+   */
+  @SuppressWarnings(""unchecked"")
+  public static void initialize(Supplier<Map<String,String>> conf) throws Exception {
+    if (null == CONF) {
+      CONF = conf;
+      LOG.info(""Creating ContextClassLoaderFactory"");
+      var factoryName = CONF.get().get(Property.GENERAL_CONTEXT_CLASSLOADER_FACTORY.toString());
+      if (null == factoryName || factoryName.isBlank()) {
+        LOG.info(""No ClassLoaderFactory specified"");
+        return;
+      }
+      try {
+        var factoryClass = Class.forName(factoryName);
+        if (ContextClassLoaderFactory.class.isAssignableFrom(factoryClass)) {
+          LOG.info(""Creating ContextClassLoaderFactory: {}"", factoryName);
+          FACTORY = ((Class<? extends ContextClassLoaderFactory>) factoryClass)
+              .getDeclaredConstructor().newInstance();
+          FACTORY.initialize(new Supplier<Map<String,String>>() {
+            @Override
+            public Map<String,String> get() {
+              return CONF.get();
+            }
+          });
+        } else {
+          throw new RuntimeException(factoryName + "" does not implement ContextClassLoaderFactory"");
+        }
+      } catch (ClassNotFoundException | InstantiationException | IllegalAccessException
+          | IllegalArgumentException | InvocationTargetException | NoSuchMethodException
+          | SecurityException e) {
+        LOG.error(
+            ""Unable to load and initialize class: {}. Ensure that the jar containing the ContextClassLoaderFactory is on the classpath"",
+            factoryName);
+        throw e;
+      }
+    } else {
+      LOG.debug(""ContextClassLoaderFactory already initialized."");
+    }
+  }
+
+  /**
+   * Return the ClassLoader for the given contextName
+   *
+   * @param contextName
+   *          name
+   * @return ClassLoader for contextName, do not cache this
+   * @throws RuntimeException
+   *           if contextName not configured
+   */
+  public static ClassLoader getClassLoader(String contextName) {
+    try {
+      // Cannot cache the ClassLoader result as it
+      // may change when the ClassLoader reloads
+      return FACTORY.getClassLoader(contextName);
+    } catch (IllegalArgumentException e) {
+      LOG.error(""ContextClassLoaderFactory is not configured for context: {}"", contextName);
+      throw new RuntimeException(
+          ""ContextClassLoaderFactory is not configured for context: "" + contextName);","[{'comment': ""You're catching an RTE to log and then throw a different RTE. This adds redundant logs and makes it harder to get a meaningful stack trace. It'd be better to just let the IAE fall through."", 'commenter': 'ctubbsii'}]"
1715,core/src/main/java/org/apache/accumulo/core/conf/ConfigurationCopy.java,"@@ -49,9 +50,17 @@ public ConfigurationCopy(Map<String,String> config) {
    *          configuration property iterable to use for copying
    */
   public ConfigurationCopy(Iterable<Entry<String,String>> config) {
-    for (Entry<String,String> entry : config) {
-      copy.put(entry.getKey(), entry.getValue());
-    }
+    this(config.iterator());
+  }
+
+  /**
+   * Creates a new configuration.
+   *
+   * @param config
+   *          configuration property iterator to use for copying
+   */
+  public ConfigurationCopy(Iterator<Entry<String,String>> config) {
+    config.forEachRemaining(e -> copy.put(e.getKey(), e.getValue()));","[{'comment': 'It looks like this constructor was added, but not used.', 'commenter': 'ctubbsii'}]"
1716,assemble/bin/tool.sh,"@@ -36,7 +36,10 @@ if [[ -z ""$ZOOKEEPER_HOME"" ]] ; then
    exit 1
 fi
 
-ZOOKEEPER_CMD='ls -1 $ZOOKEEPER_HOME/zookeeper-[0-9]*[^csn].jar '
+ZOOKEEPER_CMD='ls -1 $ZOOKEEPER_HOME/lib/zookeeper-[0-9]*[^csn].jar '
+if [[ ""${ZOOKEEPER_VERSION}"" = 3.[01234].* ]]; then","[{'comment': 'I find the regex pattern matching feature of bash to be more reliable than the glob equality feature. Also, the quotes and braces on the left side aren\'t neeeded.\r\n\r\nHowever, a more fundamental problem: where is `ZOOKEEPER_VERSION` defined? There is no guarantee it is defined anywhere by anything that calls this script. It should probably do something like what `assemble/bin/start-all.sh` does.\r\n\r\n```suggestion\r\nZOOKEEPER_VERSION=$(find -L ""$ZOOKEEPER_HOME"" -maxdepth 2 -name ""zookeeper-[0-9]*.jar"" | head -1)\r\nif [[ $ZOOKEEPER_VERSION =~ ^3[.][01234].*$ ]]; then\r\n```', 'commenter': 'ctubbsii'}]"
1735,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -861,6 +861,23 @@ void online(String tableName)
   void online(String tableName, boolean wait)
       throws AccumuloSecurityException, AccumuloException, TableNotFoundException;
 
+  /**
+   *
+   * Check if a table is online through it's current goal state only. Could run into issues if the","[{'comment': ""```suggestion\r\n   * Check if a table is online through it's current goal state only. Could run into issues if the\r\n```"", 'commenter': 'ctubbsii'}]"
1735,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -861,6 +861,23 @@ void online(String tableName)
   void online(String tableName, boolean wait)
       throws AccumuloSecurityException, AccumuloException, TableNotFoundException;
 
+  /**
+   *
+   * Check if a table is online through it's current goal state only. Could run into issues if the
+   * current state of the table is inbetween states. If you require a specific state call","[{'comment': '```suggestion\r\n   * current state of the table is in between states. If you require a specific state, call\r\n```', 'commenter': 'ctubbsii'}]"
1735,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -861,6 +861,23 @@ void online(String tableName)
   void online(String tableName, boolean wait)
       throws AccumuloSecurityException, AccumuloException, TableNotFoundException;
 
+  /**
+   *
+   * Check if a table is online through it's current goal state only. Could run into issues if the
+   * current state of the table is inbetween states. If you require a specific state call
+   * online(tableName, true) or offline(tableName, true), this will wait until the table reaches the","[{'comment': '```suggestion\r\n   * <code>online(tableName, true)</code> or <code>offline(tableName, true)</code>, this will wait until the table reaches the\r\n```', 'commenter': 'ctubbsii'}]"
1735,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -861,6 +861,23 @@ void online(String tableName)
   void online(String tableName, boolean wait)
       throws AccumuloSecurityException, AccumuloException, TableNotFoundException;
 
+  /**
+   *
+   * Check if a table is online through it's current goal state only. Could run into issues if the
+   * current state of the table is inbetween states. If you require a specific state call
+   * online(tableName, true) or offline(tableName, true), this will wait until the table reaches the
+   * desired state before preceeding.","[{'comment': '```suggestion\r\n   * desired state before proceeding.\r\n```', 'commenter': 'ctubbsii'}]"
1735,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -861,6 +861,23 @@ void online(String tableName)
   void online(String tableName, boolean wait)
       throws AccumuloSecurityException, AccumuloException, TableNotFoundException;
 
+  /**
+   *
+   * Check if a table is online through it's current goal state only. Could run into issues if the
+   * current state of the table is inbetween states. If you require a specific state call
+   * online(tableName, true) or offline(tableName, true), this will wait until the table reaches the
+   * desired state before preceeding.
+   *
+   * @param tableName
+   *          the table to check if online
+   * @throws AccumuloException
+   *           when there is a general accumulo error
+   * @return true if table is online","[{'comment': ""```suggestion\r\n   * @return true if table's goal state is online\r\n```"", 'commenter': 'ctubbsii'}]"
1735,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -861,6 +861,22 @@ void online(String tableName)
   void online(String tableName, boolean wait)
       throws AccumuloSecurityException, AccumuloException, TableNotFoundException;
 
+  /**
+   * Check if a table is online through it's current goal state only. Could run into issues if the","[{'comment': '```suggestion\r\n   * Check if a table is online through its current goal state only. Could run into issues if the\r\n```', 'commenter': 'ctubbsii'}]"
1752,core/src/main/thrift/tabletserver.thrift,"@@ -348,6 +350,16 @@ service TabletClientService extends client.ClientService {
     7:i64 requestTime
   )
 
+  // since 2.1
+  oneway void unload(
+      5:trace.TInfo tinfo
+      1:security.TCredentials credentials
+      4:string lock
+      2:data.TKeyExtent extent
+      6:string goal
+      7:i64 requestTime","[{'comment': ""These fields could be numbered sanely, if you're creating a new function. The old fields were numbered that way because it evolved over time."", 'commenter': 'ctubbsii'}]"
1752,server/tserver/src/main/java/org/apache/accumulo/tserver/ThriftClientHandler.java,"@@ -1454,9 +1454,16 @@ public void run() {
     }
   }
 
+  @Deprecated
   @Override
   public void unloadTablet(TInfo tinfo, TCredentials credentials, String lock, TKeyExtent textent,
       TUnloadTabletGoal goal, long requestTime) {
+","[{'comment': 'For RPC compatibility for rolling restarts, you should probably have a real implementation here, by calling the new unload method with the toString version of the goal.', 'commenter': 'ctubbsii'}]"
1759,server/manager/src/main/java/org/apache/accumulo/master/tableOps/create/ChooseDir.java,"@@ -60,9 +63,16 @@ public long isReady(long tid, Master environment) {
 
   @Override
   public void undo(long tid, Master master) throws Exception {
-    Path p = tableInfo.getSplitDirsPath();
-    FileSystem fs = p.getFileSystem(master.getContext().getHadoopConf());
-    fs.delete(p, true);
+    // Clean up split files if ChooseDir operation fails
+    try {
+      if (tableInfo.getInitialSplitSize() > 0) {
+        Path p = tableInfo.getSplitDirsPath();
+        FileSystem fs = p.getFileSystem(master.getContext().getHadoopConf());
+        fs.delete(p, true);
+      }
+    } catch (NullPointerException | IOException e) {
+      log.error(""Failed to undo ChooseDir operation"", e);","[{'comment': 'When on the receiving end of an error like this its really nice to have some information that helps correlate it with other activity in the system.  However the danger of trying to get more information for an error is that the code may fail.  I think the following is pretty safe and gives a good bit of helpful info. \r\n\r\n```suggestion\r\n      var spdir = Optional.ofNullable(tableInfo).map(TableInfo::getSplitDirsPath).orElse(null);\r\n      log.error(""{} Failed to undo ChooseDir operation, split dir {} "",FateTxId.format(tid), spdir, e);\r\n```', 'commenter': 'keith-turner'}, {'comment': 'I made the requested error changes or each of the catch blocks. Let me know if there is anything else I need to tweak.', 'commenter': 'Manno15'}, {'comment': 'Looks good.', 'commenter': 'keith-turner'}]"
1759,server/manager/src/main/java/org/apache/accumulo/master/tableOps/create/CreateTable.java,"@@ -72,6 +77,9 @@ public long isReady(long tid, Master environment) throws Exception {
     Utils.getIdLock().lock();
     try {
       String tName = tableInfo.getTableName();
+      if(tName.equals(""ci"")){
+        Thread.sleep(10000000);
+      }","[{'comment': 'This needs to be removed. A user could have a table by this name.', 'commenter': 'ctubbsii'}, {'comment': ""That is my mistake, this shouldn't be here. I used this to force it to hang. Forgot to remove it."", 'commenter': 'Manno15'}]"
1759,server/manager/src/main/java/org/apache/accumulo/master/tableOps/create/FinishCreateTable.java,"@@ -62,17 +67,23 @@ public long isReady(long tid, Master environment) {
     env.getEventCoordinator().event(""Created table %s "", tableInfo.getTableName());
 
     if (tableInfo.getInitialSplitSize() > 0) {
-      cleanupSplitFiles(env);
+      cleanupSplitFiles(tid, env);
     }
     return null;
   }
 
-  private void cleanupSplitFiles(Master env) throws IOException {
+  private void cleanupSplitFiles(long tid, Master env) throws IOException {
     // it is sufficient to delete from the parent, because both files are in the same directory, and
     // we want to delete the directory also
-    Path p = tableInfo.getSplitPath().getParent();
-    FileSystem fs = p.getFileSystem(env.getContext().getHadoopConf());
-    fs.delete(p, true);
+    try {
+      Path p = tableInfo.getSplitPath().getParent();
+      FileSystem fs = p.getFileSystem(env.getContext().getHadoopConf());
+      fs.delete(p, true);
+    } catch (NullPointerException | IOException e) {
+      var spdir = Optional.ofNullable(tableInfo).map(TableInfo::getSplitDirsPath).orElse(null);
+      log.error(""{} Failed to cleanup splits file after table was created, split dir {} "",
+          FateTxId.formatTid(tid), spdir, e);
+    }","[{'comment': 'I don\'t think it\'s possible for tableInfo to be null here, given how this class is constructed. And, if the split size is `>0`, it\'s also not possible for the splits file locations to be null. Also, I think we should focus just on handling exceptions pertaining to the specific cleanup task that we\'re performing. So, if these do happen to be null because of some serialization bug or whatever, that shouldn\'t be handled here, but handled by the framework if the exception is thrown from the method (if the framework isn\'t capable of handling exceptions thrown by this method, the interface shouldn\'t have `throws Exception` on the method, and if that\'s a problem, it is a separate issue that should be fixed in a separate PR to fix the FaTE framework itself).\r\n\r\nAlso, the tx id is already in the path to the splits file, so it\'s possibly redundant to have that here (but I\'m not opposed to keeping it).\r\n\r\nAlso, we probably want to log the parent path (the one we\'re trying to delete recursively), rather than just one of the two file names.\r\n\r\nSo, all told, I think something like this would be a little cleaner (with similar changes made in the other places):\r\n\r\n```suggestion\r\n    Path p = null;\r\n    try {\r\n      p = tableInfo.getSplitPath().getParent();\r\n      FileSystem fs = p.getFileSystem(env.getContext().getHadoopConf());\r\n      fs.delete(p, true);\r\n    } catch (IOException e) {\r\n      log.error(""Table was created, but failed to clean up temporary splits files at {}"", p, e);\r\n    }\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""> I don't think it's possible for tableInfo to be null here, given how this class is constructed. And, if the split size is >0, it's also not possible for the splits file locations to be null. \r\n\r\nI agree. I wasn't quite sure so I kept the null exception handling in there. I will remove and look to make the other changes you suggested as well. "", 'commenter': 'Manno15'}]"
1765,core/src/main/java/org/apache/accumulo/core/client/ScannerBase.java,"@@ -356,4 +357,10 @@ default void fetchColumn(CharSequence colFam, CharSequence colQual) {
   default void setExecutionHints(Map<String,String> hints) {
     throw new UnsupportedOperationException();
   }
+
+  default void forEach(BiConsumer<? super Key,? super Value> keyValueConsumer) {","[{'comment': 'Needs a `@since 2.1.0` javadoc tag. ', 'commenter': 'keith-turner'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.clientImpl;
+
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.function.BiConsumer;
+
+import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Value;
+import org.apache.hadoop.io.Text;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ScannerBaseTest {
+
+  private ScannerBase s;
+  private Map<Key,Value> scannerMap;
+  private Iterator<Map.Entry<Key,Value>> it;
+  private Key key;
+  private Value val;
+  private BiConsumer<Key,Value> keyValueConsumer;
+  private forEachTester fet;
+
+  private static class forEachTester {
+
+    private Map<Key,Value> map;
+
+    forEachTester(Map<Key,Value> map) {
+      this.map = map;
+    }
+
+    public void forEach(BiConsumer<? super Key,? super Value> keyValueConsumer) {
+      for (Map.Entry<Key,Value> entry : this.map.entrySet()) {
+        keyValueConsumer.accept(entry.getKey(), entry.getValue());
+      }
+    }
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    s = createMock(ScannerBase.class);
+  }
+
+  @Test
+  public void testScannerBaseForEach() throws Exception {
+    key = new Key(new Text(""a""), new Text(""cf1""), new Text(""cq1""));
+    val = new Value(new Text(""v1""));
+    scannerMap = new HashMap<>();
+
+    scannerMap.put(key, val);
+
+    fet = new forEachTester(scannerMap);
+
+    it = scannerMap.entrySet().iterator();
+
+    expect(s.iterator()).andReturn(it).times(3);
+    replay(s);
+
+    Map<Key,Value> map = new HashMap<>();
+
+    class MyBiConsumer implements BiConsumer<Key,Value> {
+      @Override
+      public void accept(Key key, Value value) {
+        map.put(key, value);
+      }
+    }
+
+    keyValueConsumer = new MyBiConsumer();
+
+    fet.forEach(keyValueConsumer);
+
+    // Test the Scanner values put into the map via keyValueConsumer
+
+    for (Map.Entry<Key,Value> entry : map.entrySet()) {
+      Map.Entry<Key,Value> expectedEntry = s.iterator().next();
+      Key expectedKey = expectedEntry.getKey();
+      Value expectedValue = expectedEntry.getValue();
+
+      String expectedCf = expectedKey.getColumnFamily().toString();
+      String actualCf = entry.getKey().getColumnFamily().toString();
+
+      String expectedCq = expectedKey.getColumnQualifier().toString();
+      String actualCq = entry.getKey().getColumnQualifier().toString();
+
+      String expectedVal = expectedValue.toString();
+      String actualVal = entry.getValue().toString();
+
+      assertEquals(expectedCf, actualCf);
+      assertEquals(expectedCq, actualCq);
+      assertEquals(expectedVal, actualVal);","[{'comment': ""You don't need to compare CF and CQ, you can compare Key equality directly. That will shorten the test code a bit. You also don't need to call `toString()` on anything.\r\n\r\n```suggestion\r\n      Map.Entry<Key,Value> expectedEntry = s.iterator().next();\r\n      assertEquals(expectedEntry.getKey(), entry.getKey());\r\n      assertEquals(expectedEntry.getValue(), entry.getValue());\r\n```"", 'commenter': 'ctubbsii'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.clientImpl;
+
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.function.BiConsumer;
+
+import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Value;
+import org.apache.hadoop.io.Text;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ScannerBaseTest {
+
+  private ScannerBase s;
+  private Map<Key,Value> scannerMap;
+  private Iterator<Map.Entry<Key,Value>> it;
+  private Key key;
+  private Value val;
+  private BiConsumer<Key,Value> keyValueConsumer;
+  private forEachTester fet;","[{'comment': ""I don't think most (if any) of these should be class members. They can be local variables in the test case."", 'commenter': 'ctubbsii'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.clientImpl;
+
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.function.BiConsumer;
+
+import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Value;
+import org.apache.hadoop.io.Text;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ScannerBaseTest {
+
+  private ScannerBase s;
+  private Map<Key,Value> scannerMap;
+  private Iterator<Map.Entry<Key,Value>> it;
+  private Key key;
+  private Value val;
+  private BiConsumer<Key,Value> keyValueConsumer;
+  private forEachTester fet;
+
+  private static class forEachTester {
+
+    private Map<Key,Value> map;
+
+    forEachTester(Map<Key,Value> map) {
+      this.map = map;
+    }
+
+    public void forEach(BiConsumer<? super Key,? super Value> keyValueConsumer) {
+      for (Map.Entry<Key,Value> entry : this.map.entrySet()) {
+        keyValueConsumer.accept(entry.getKey(), entry.getValue());
+      }
+    }
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    s = createMock(ScannerBase.class);
+  }
+
+  @Test
+  public void testScannerBaseForEach() throws Exception {
+    key = new Key(new Text(""a""), new Text(""cf1""), new Text(""cq1""));
+    val = new Value(new Text(""v1""));
+    scannerMap = new HashMap<>();
+
+    scannerMap.put(key, val);
+
+    fet = new forEachTester(scannerMap);
+
+    it = scannerMap.entrySet().iterator();
+
+    expect(s.iterator()).andReturn(it).times(3);
+    replay(s);
+
+    Map<Key,Value> map = new HashMap<>();
+
+    class MyBiConsumer implements BiConsumer<Key,Value> {
+      @Override
+      public void accept(Key key, Value value) {
+        map.put(key, value);
+      }
+    }
+
+    keyValueConsumer = new MyBiConsumer();
+
+    fet.forEach(keyValueConsumer);","[{'comment': '```suggestion\r\n    Map<Key,Value> map = new HashMap<>();\r\n    fet.forEach((k,v) -> map.put(k,v));\r\n```', 'commenter': 'ctubbsii'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.clientImpl;
+
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.function.BiConsumer;
+
+import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Value;
+import org.apache.hadoop.io.Text;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ScannerBaseTest {
+
+  private ScannerBase s;
+  private Map<Key,Value> scannerMap;
+  private Iterator<Map.Entry<Key,Value>> it;
+  private Key key;
+  private Value val;
+  private BiConsumer<Key,Value> keyValueConsumer;
+  private forEachTester fet;
+
+  private static class forEachTester {
+
+    private Map<Key,Value> map;
+
+    forEachTester(Map<Key,Value> map) {
+      this.map = map;
+    }
+
+    public void forEach(BiConsumer<? super Key,? super Value> keyValueConsumer) {
+      for (Map.Entry<Key,Value> entry : this.map.entrySet()) {
+        keyValueConsumer.accept(entry.getKey(), entry.getValue());
+      }
+    }
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    s = createMock(ScannerBase.class);
+  }
+
+  @Test
+  public void testScannerBaseForEach() throws Exception {
+    key = new Key(new Text(""a""), new Text(""cf1""), new Text(""cq1""));
+    val = new Value(new Text(""v1""));
+    scannerMap = new HashMap<>();
+
+    scannerMap.put(key, val);
+
+    fet = new forEachTester(scannerMap);
+
+    it = scannerMap.entrySet().iterator();
+
+    expect(s.iterator()).andReturn(it).times(3);
+    replay(s);
+
+    Map<Key,Value> map = new HashMap<>();
+
+    class MyBiConsumer implements BiConsumer<Key,Value> {
+      @Override
+      public void accept(Key key, Value value) {
+        map.put(key, value);
+      }
+    }
+
+    keyValueConsumer = new MyBiConsumer();
+
+    fet.forEach(keyValueConsumer);
+
+    // Test the Scanner values put into the map via keyValueConsumer
+
+    for (Map.Entry<Key,Value> entry : map.entrySet()) {
+      Map.Entry<Key,Value> expectedEntry = s.iterator().next();
+      Key expectedKey = expectedEntry.getKey();
+      Value expectedValue = expectedEntry.getValue();
+
+      String expectedCf = expectedKey.getColumnFamily().toString();
+      String actualCf = entry.getKey().getColumnFamily().toString();
+
+      String expectedCq = expectedKey.getColumnQualifier().toString();
+      String actualCq = entry.getKey().getColumnQualifier().toString();
+
+      String expectedVal = expectedValue.toString();
+      String actualVal = entry.getValue().toString();
+
+      assertEquals(expectedCf, actualCf);
+      assertEquals(expectedCq, actualCq);
+      assertEquals(expectedVal, actualVal);
+    }","[{'comment': 'Should verify mock objects at the end of the test.', 'commenter': 'ctubbsii'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.clientImpl;
+
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.function.BiConsumer;
+
+import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Value;
+import org.apache.hadoop.io.Text;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ScannerBaseTest {
+
+  private ScannerBase s;
+  private Map<Key,Value> scannerMap;
+  private Iterator<Map.Entry<Key,Value>> it;
+  private Key key;
+  private Value val;
+  private BiConsumer<Key,Value> keyValueConsumer;
+  private forEachTester fet;
+
+  private static class forEachTester {
+
+    private Map<Key,Value> map;
+
+    forEachTester(Map<Key,Value> map) {
+      this.map = map;
+    }
+
+    public void forEach(BiConsumer<? super Key,? super Value> keyValueConsumer) {
+      for (Map.Entry<Key,Value> entry : this.map.entrySet()) {
+        keyValueConsumer.accept(entry.getKey(), entry.getValue());
+      }
+    }
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    s = createMock(ScannerBase.class);
+  }","[{'comment': ""This can be done in the test. It doesn't need to be here, unless it's common code for multiple tests."", 'commenter': 'ctubbsii'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.clientImpl;
+
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.junit.Assert.assertEquals;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.function.BiConsumer;
+
+import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Value;
+import org.apache.hadoop.io.Text;
+import org.junit.Test;
+
+public class ScannerBaseTest {
+
+  private static class forEachTester {","[{'comment': ""Didn't notice this before, but Java class names should start uppercase.\r\n```suggestion\r\n  private static class ForEachTester {\r\n```"", 'commenter': 'ctubbsii'}, {'comment': ""Later comment makes this suggestion unnecessary. The class isn't needed at all."", 'commenter': 'ctubbsii'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.clientImpl;
+
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.junit.Assert.assertEquals;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.function.BiConsumer;
+
+import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Value;
+import org.apache.hadoop.io.Text;
+import org.junit.Test;
+
+public class ScannerBaseTest {
+
+  private static class forEachTester {
+
+    private Map<Key,Value> map;
+
+    forEachTester(Map<Key,Value> map) {
+      this.map = map;
+    }
+
+    public void forEach(BiConsumer<? super Key,? super Value> keyValueConsumer) {
+      for (Map.Entry<Key,Value> entry : this.map.entrySet()) {
+        keyValueConsumer.accept(entry.getKey(), entry.getValue());
+      }
+    }
+  }
+
+  @Test
+  public void testScannerBaseForEach() throws Exception {
+
+    ScannerBase s;
+    Map<Key,Value> scannerMap;
+    Iterator<Map.Entry<Key,Value>> it;
+    Key key;
+    Value val;
+    BiConsumer<Key,Value> keyValueConsumer;
+    forEachTester fet;
+
+    s = createMock(ScannerBase.class);
+    key = new Key(new Text(""a""), new Text(""cf1""), new Text(""cq1""));
+    val = new Value(new Text(""v1""));
+    scannerMap = new HashMap<>();
+
+    scannerMap.put(key, val);
+
+    fet = new forEachTester(scannerMap);
+
+    it = scannerMap.entrySet().iterator();","[{'comment': '```suggestion\r\n    ScannerBase s = createMock(ScannerBase.class);\r\n    Map<Key,Value> scannerMap = new HashMap<>();\r\n    scannerMap.put(new Key(new Text(""a""), new Text(""cf1""), new Text(""cq1"")), new Value(new Text(""v1"")));\r\n    scannerMap.put(new Key(new Text(""b""), new Text(""cf1""), new Text(""cq1"")), new Value(new Text(""v2"")));\r\n    ForEachTester fet = new ForEachTester(scannerMap);\r\n\r\n    Iterator<Map.Entry<Key,Value>> it = scannerMap.entrySet().iterator();\r\n```', 'commenter': 'ctubbsii'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.clientImpl;
+
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.junit.Assert.assertEquals;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.function.BiConsumer;
+
+import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Value;
+import org.apache.hadoop.io.Text;
+import org.junit.Test;
+
+public class ScannerBaseTest {
+
+  private static class forEachTester {
+
+    private Map<Key,Value> map;
+
+    forEachTester(Map<Key,Value> map) {
+      this.map = map;
+    }
+
+    public void forEach(BiConsumer<? super Key,? super Value> keyValueConsumer) {
+      for (Map.Entry<Key,Value> entry : this.map.entrySet()) {
+        keyValueConsumer.accept(entry.getKey(), entry.getValue());
+      }
+    }
+  }
+
+  @Test
+  public void testScannerBaseForEach() throws Exception {
+
+    ScannerBase s;
+    Map<Key,Value> scannerMap;
+    Iterator<Map.Entry<Key,Value>> it;
+    Key key;
+    Value val;
+    BiConsumer<Key,Value> keyValueConsumer;
+    forEachTester fet;
+
+    s = createMock(ScannerBase.class);
+    key = new Key(new Text(""a""), new Text(""cf1""), new Text(""cq1""));
+    val = new Value(new Text(""v1""));
+    scannerMap = new HashMap<>();
+
+    scannerMap.put(key, val);
+
+    fet = new forEachTester(scannerMap);
+
+    it = scannerMap.entrySet().iterator();
+
+    expect(s.iterator()).andReturn(it).times(1);
+    replay(s);
+
+    Map<Key,Value> map = new HashMap<>();
+    fet.forEach(map::put);","[{'comment': ""After simplifying a bit myself, it seems like that the `forEach` concept is being tested on the `ForEachTester` object, but the test is never actually calling the `ScannerBase.forEach` method... which is the method that this test case should be covering.\r\n\r\nIt would be better if `ScannerBase` was a partial mock... overriding `iterator()` (as you've already done), but leaving the implementation of `forEach` alone, and calling that in your test. The `ForEachTester` class doesn't seem necessary at all. It can simply be deleted."", 'commenter': 'ctubbsii'}]"
1765,core/src/test/java/org/apache/accumulo/core/clientImpl/ScannerBaseTest.java,"@@ -20,71 +20,121 @@
 
 import static org.easymock.EasyMock.createMock;
 import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.expectLastCall;
+import static org.easymock.EasyMock.partialMockBuilder;
 import static org.easymock.EasyMock.replay;
 import static org.easymock.EasyMock.verify;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
 
+import java.awt.*;
+import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
+import java.util.Scanner;
+import java.util.concurrent.TimeUnit;
 import java.util.function.BiConsumer;
 
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.IteratorSetting;
 import org.apache.accumulo.core.client.ScannerBase;
+import org.apache.accumulo.core.client.sample.SamplerConfiguration;
 import org.apache.accumulo.core.data.Key;
 import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.security.Authorizations;
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.shaded.org.mockito.Mockito;
+import org.easymock.EasyMock;
 import org.junit.Test;
 
 public class ScannerBaseTest {
 
-  private static class forEachTester {
-
-    private Map<Key,Value> map;
-
-    forEachTester(Map<Key,Value> map) {
-      this.map = map;
-    }
-
-    public void forEach(BiConsumer<? super Key,? super Value> keyValueConsumer) {
-      for (Map.Entry<Key,Value> entry : this.map.entrySet()) {
-        keyValueConsumer.accept(entry.getKey(), entry.getValue());
-      }
-    }
-  }
-
   @Test
   public void testScannerBaseForEach() throws Exception {
 
-    ScannerBase s;
-    Map<Key,Value> scannerMap;
-    Iterator<Map.Entry<Key,Value>> it;
-    Key key;
-    Value val;
-    BiConsumer<Key,Value> keyValueConsumer;
-    forEachTester fet;
+    //This subclass of ScannerBase contains a List that ScannerBase.forEach() can
+    //iterate over for testing purposes.
+    class MockScanner extends List implements ScannerBase {","[{'comment': ""Extending List forces you to add implementations for a bunch of methods. You can just implement ScannerBase, and have a constructor that takes a `map`, which itself is easily constructed, using `Map.of(...)`. MockScanner's `iterator()` method can just return `map.entrySet().iterator()`."", 'commenter': 'ctubbsii'}]"
1771,shell/src/main/java/org/apache/accumulo/shell/commands/InsertCommand.java,"@@ -63,81 +64,96 @@ protected long getTimeout(final CommandLine cl) {
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws AccumuloException, AccumuloSecurityException, TableNotFoundException, IOException,
       ConstraintViolationException {
-    shellState.checkTableState();
 
-    final Mutation m = new Mutation(new Text(cl.getArgs()[0].getBytes(Shell.CHARSET)));
-    final Text colf = new Text(cl.getArgs()[1].getBytes(Shell.CHARSET));
-    final Text colq = new Text(cl.getArgs()[2].getBytes(Shell.CHARSET));
-    final Value val = new Value(cl.getArgs()[3].getBytes(Shell.CHARSET));
-
-    if (cl.hasOption(insertOptAuths.getOpt())) {
-      final ColumnVisibility le = new ColumnVisibility(cl.getOptionValue(insertOptAuths.getOpt()));
-      Shell.log.debug(""Authorization label will be set to: {}"", le);
-
-      if (cl.hasOption(timestampOpt.getOpt()))
-        m.put(colf, colq, le, Long.parseLong(cl.getOptionValue(timestampOpt.getOpt())), val);
-      else
-        m.put(colf, colq, le, val);
-    } else if (cl.hasOption(timestampOpt.getOpt()))
-      m.put(colf, colq, Long.parseLong(cl.getOptionValue(timestampOpt.getOpt())), val);
-    else
-      m.put(colf, colq, val);
-
-    final BatchWriterConfig cfg =
-        new BatchWriterConfig().setMaxMemory(Math.max(m.estimatedMemoryUsed(), 1024))
-            .setMaxWriteThreads(1).setTimeout(getTimeout(cl), TimeUnit.MILLISECONDS);
-    if (cl.hasOption(durabilityOption.getOpt())) {
-      String userDurability = cl.getOptionValue(durabilityOption.getOpt());
-      switch (userDurability) {
-        case ""sync"":
-          cfg.setDurability(Durability.SYNC);
-          break;
-        case ""flush"":
-          cfg.setDurability(Durability.FLUSH);
-          break;
-        case ""none"":
-          cfg.setDurability(Durability.NONE);
-          break;
-        case ""log"":
-          cfg.setDurability(Durability.NONE);
-          break;
-        default:
-          throw new IllegalArgumentException(""Unknown durability: "" + userDurability);
-      }
-    }
-    final BatchWriter bw =
-        shellState.getAccumuloClient().createBatchWriter(shellState.getTableName(), cfg);
-    bw.addMutation(m);
+    String initialTable = null;
     try {
-      bw.close();
-    } catch (MutationsRejectedException e) {
-      final ArrayList<String> lines = new ArrayList<>();
-      if (!e.getSecurityErrorCodes().isEmpty()) {
-        lines.add(""\tAuthorization Failures:"");
+      if (cl.hasOption(tableNameOption.getOpt())) {
+        initialTable = shellState.getTableName();
+        shellState.setTableName(cl.getOptionValue(tableNameOption.getOpt()));","[{'comment': ""This command should use the table specified, but it should not update the table state for the entire shell. All you need to do is set a local variable to either the specified table, or default to the current shell state's table, and use that local variable in the line below that creates the batch writer."", 'commenter': 'ctubbsii'}, {'comment': ""Thanks for pointing that out @ctubbsii. I've updated the code to use a temp variable and left the shellstate as is. "", 'commenter': 'jmark99'}]"
1771,shell/src/main/java/org/apache/accumulo/shell/commands/InsertCommand.java,"@@ -171,6 +180,11 @@ public Options getOptions() {
         ""durability to use for insert, should be one of \""none\"" \""log\"" \""flush\"" or \""sync\"""");
     o.addOption(durabilityOption);
 
+    tableNameOption =
+        new Option("""", ""tablename"", true, ""name of table in which data is being inserted"");
+    tableNameOption.setArgName(""tablename"");
+    o.addOption(tableNameOption);
+","[{'comment': ""What's the reason for not supporting `-t` short name?"", 'commenter': 'ctubbsii'}, {'comment': ""I chose not to use the short form to minimize any confusion with the <code>timestamp</code> parameter (-ts). There is also a <code>timeout</code> parameter that does not off a shortened name. If you think it would be nice to include one, I have no problem adding one. Could always use '-n' for 'name' perhaps. Let me know your thoughts."", 'commenter': 'jmark99'}, {'comment': ""If there's not already a `-t`, I don't think it's a problem. I don't think there's any concern over conflicting with `-ts`. In fact, most commands use the standard `OptUtil.tableOpt`. See  https://github.com/apache/accumulo/blob/main/shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java#L93 and https://github.com/apache/accumulo/blob/main/shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java#L395.\r\nI would recommend doing the same here, if there's no existing `-t`."", 'commenter': 'ctubbsii'}]"
1771,shell/src/main/java/org/apache/accumulo/shell/commands/InsertCommand.java,"@@ -171,6 +171,8 @@ public Options getOptions() {
         ""durability to use for insert, should be one of \""none\"" \""log\"" \""flush\"" or \""sync\"""");
     o.addOption(durabilityOption);
 
+    o.addOption(OptUtil.tableOpt(""table in which data is to be inserted""));","[{'comment': 'Minor grammatical nit:\r\n```suggestion\r\n    o.addOption(OptUtil.tableOpt(""table into which data will be inserted""));\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""No problem. Didn't quite sound right to me, but I was blanking on how to re-word it."", 'commenter': 'jmark99'}]"
1786,core/src/main/java/org/apache/accumulo/core/client/ClientConfiguration.java,"@@ -194,14 +196,17 @@ public static ClientConfiguration create() {
    *          the path to the configuration file
    * @since 1.9.0
    */
-  public static ClientConfiguration fromFile(File file) {
-    FileBasedConfigurationBuilder<PropertiesConfiguration> propsBuilder =
-        new FileBasedConfigurationBuilder<>(PropertiesConfiguration.class)
-            .configure(new Parameters().properties().setFile(file));
-    try {
-      return new ClientConfiguration(Collections.singletonList(propsBuilder.getConfiguration()));
+  public static ClientConfiguration fromFile(File file) throws FileNotFoundException {","[{'comment': 'This changes exceptions thrown on a public API method.', 'commenter': 'ctubbsii'}]"
1786,core/src/main/java/org/apache/accumulo/core/client/ClientConfiguration.java,"@@ -194,14 +196,17 @@ public static ClientConfiguration create() {
    *          the path to the configuration file
    * @since 1.9.0
    */
-  public static ClientConfiguration fromFile(File file) {
-    FileBasedConfigurationBuilder<PropertiesConfiguration> propsBuilder =
-        new FileBasedConfigurationBuilder<>(PropertiesConfiguration.class)
-            .configure(new Parameters().properties().setFile(file));
-    try {
-      return new ClientConfiguration(Collections.singletonList(propsBuilder.getConfiguration()));
+  public static ClientConfiguration fromFile(File file) throws FileNotFoundException {
+    var config = new PropertiesConfiguration();
+    try (var reader = new FileReader(file)) {
+      config.getLayout().load(config, reader);
+      return new ClientConfiguration(Collections.singletonList(config));
     } catch (ConfigurationException e) {
       throw new IllegalArgumentException(""Bad configuration file: "" + file, e);
+    } catch (FileNotFoundException fnfe) {
+      throw fnfe;
+    } catch (IOException e1) {
+      throw new UncheckedIOException(""IOExcetion creating configuration"", e1);","[{'comment': ""It'd be better to add these exceptions to the existing `ConfigurationException` using multi-catch. That way, we don't introduce new exceptions in the public API method."", 'commenter': 'ctubbsii'}]"
1786,core/src/main/java/org/apache/accumulo/core/client/ClientConfiguration.java,"@@ -225,14 +230,15 @@ private static ClientConfiguration loadFromSearchPath(List<String> paths) {
     for (String path : paths) {
       File conf = new File(path);
       if (conf.isFile() && conf.canRead()) {
-        FileBasedConfigurationBuilder<PropertiesConfiguration> propsBuilder =
-            new FileBasedConfigurationBuilder<>(PropertiesConfiguration.class)
-                .configure(new Parameters().properties().setFile(conf));
-        try {
-          configs.add(propsBuilder.getConfiguration());
+        var config = new PropertiesConfiguration();
+        try (var reader = new FileReader(conf)) {
+          config.getLayout().load(config, reader);
+          configs.add(config);
           log.info(""Loaded client configuration file {}"", conf);
         } catch (ConfigurationException e) {
           throw new IllegalStateException(""Error loading client configuration file "" + conf, e);
+        } catch (IOException e1) {
+          throw new UncheckedIOException(""IOExcetion creating configuration"", e1);","[{'comment': 'This can be combined with the existing `ConfigurationException`', 'commenter': 'ctubbsii'}]"
1786,core/src/main/java/org/apache/accumulo/core/conf/SiteConfiguration.java,"@@ -203,15 +204,18 @@ private SiteConfiguration(Map<String,String> config) {
   }
 
   // load properties from config file
+  @SuppressFBWarnings(value = ""PATH_TRAVERSAL_IN"", justification = ""path provided by test"")
   private static AbstractConfiguration getPropsFileConfig(URL accumuloPropsLocation) {
     if (accumuloPropsLocation != null) {
-      var propsBuilder = new FileBasedConfigurationBuilder<>(PropertiesConfiguration.class)
-          .configure(new Parameters().properties().setURL(accumuloPropsLocation));
-      try {
-        return propsBuilder.getConfiguration();
+      var config = new PropertiesConfiguration();
+      try (var reader = new FileReader(accumuloPropsLocation.getFile())) {
+        config.getLayout().load(config, reader);
       } catch (ConfigurationException e) {
         throw new IllegalArgumentException(e);
+      } catch (IOException e1) {
+        throw new UncheckedIOException(""IOExcetion creating configuration"", e1);","[{'comment': 'This can be combined with the existing `ConfigurationException`', 'commenter': 'ctubbsii'}]"
1786,core/src/test/java/org/apache/accumulo/core/conf/SiteConfigurationTest.java,"@@ -77,6 +78,8 @@ public void testFile() {
     assertEquals(""256M"", conf.get(Property.TSERV_WALOG_MAX_SIZE));
     assertEquals(""org.apache.accumulo.core.cryptoImpl.AESCryptoService"",
         conf.get(Property.INSTANCE_CRYPTO_SERVICE));
+    assertEquals(System.getenv(""USER""), conf.get(""general.test.user.name""));
+    assertEquals(""/tmp/test/dir"", conf.get(""general.test.user.dir""));","[{'comment': 'Nice checks to test interpolation!', 'commenter': 'ctubbsii'}]"
1786,test/src/main/java/org/apache/accumulo/test/metrics/MetricsFileTailer.java,"@@ -153,11 +147,12 @@ private Configuration loadMetricsConfig() {
       }
 
       return sub;
-
-    } catch (ConfigurationException ex) {
+    } catch (ConfigurationException e) {
       throw new IllegalStateException(
           String.format(""Could not find configuration file \'%s\' on classpath"",
               MetricsTestSinkProperties.METRICS_PROP_FILENAME));
+    } catch (IOException e1) {
+      throw new UncheckedIOException(""IOExcetion creating configuration"", e1);","[{'comment': 'This can be combined with the existing `ConfigurationException`', 'commenter': 'ctubbsii'}]"
1786,assemble/pom.xml,"@@ -106,11 +106,6 @@
       <artifactId>jaxb-core</artifactId>
       <optional>true</optional>
     </dependency>
-    <dependency>
-      <groupId>commons-beanutils</groupId>
-      <artifactId>commons-beanutils</artifactId>
-      <optional>true</optional>
-    </dependency>","[{'comment': 'Can probably also remove the entry from `assemble/src/main/assemblies/component.xml`', 'commenter': 'ctubbsii'}]"
1786,core/src/main/java/org/apache/accumulo/core/conf/SiteConfiguration.java,"@@ -140,8 +140,8 @@ public Buildable withOverrides(Map<String,String> overrides) {
       return this;
     }
 
-    @SuppressFBWarnings(value = ""URLCONNECTION_SSRF_FD"",
-        justification = ""location of props is specified by an admin"")
+    @SuppressFBWarnings(value = {""URLCONNECTION_SSRF_FD"", ""PATH_TRAVERSAL_IN""},
+        justification = ""location of props is specified by an admin, path provided by test"")","[{'comment': ""Is this suppression necessary on this method? This method doesn't look like it changed. It might be covered by the suppression elsewhere."", 'commenter': 'ctubbsii'}]"
1786,start/src/main/java/org/apache/accumulo/start/classloader/AccumuloClassLoader.java,"@@ -86,10 +86,10 @@ public static String getAccumuloProperty(String propertyName, String defaultValu
       return defaultValue;
     }
     try {
-      FileBasedConfigurationBuilder<PropertiesConfiguration> propsBuilder =
-          new FileBasedConfigurationBuilder<>(PropertiesConfiguration.class)
-              .configure(new Parameters().properties().setURL(accumuloConfigUrl));
-      PropertiesConfiguration config = propsBuilder.getConfiguration();
+      var config = new PropertiesConfiguration();
+      try (var reader = new FileReader(accumuloConfigUrl.getFile())) {
+        config.getLayout().load(config, reader);","[{'comment': ""I wasn't aware of the `read()` method before. Apparently, we can just call `config.read(reader)` here. I also wasn't aware of `FileHandler` before, which I see is being used when saving files.\r\n\r\nI think we should try to be consistent. We should either use:\r\n```java\r\nvar config = PropertiesConfiguration();\r\ntry (var reader = new FileReader()) {\r\n  config.read(reader);\r\n}\r\n// and\r\ntry (var writer = new FileWriter()) {\r\n  config.write(writer);\r\n}\r\n```\r\n\r\nOR\r\n\r\n```java\r\nvar config = new PropertiesConfiguration();\r\nvar fileHandler = new FileHandler(config);\r\nfileHandler.load(file);\r\n// and\r\nfileHandler.save(file);\r\n```\r\n\r\nUltimately, they will both do the same thing, but I'm not sure which one is cleaner, since we still need to handle exceptions. Either way, but we should consistently use one pattern or the other."", 'commenter': 'ctubbsii'}]"
1797,test/src/main/java/org/apache/accumulo/test/ShellIT.java,"@@ -236,6 +236,17 @@ public void insertDeleteScanTest() throws IOException {
     exec(""delete \\x90 \\xa0 \\xb0"", true);
     exec(""scan"", true, ""\\x90 \\xA0:\\xB0 []    \\xC0"", false);
     exec(""deletetable test -f"", true, ""Table: [test] has been deleted"");
+    // Add tests to verify use of --table parameter
+    exec(""createtable test2"", true);
+    exec(""notable"", true);","[{'comment': ""What is 'notable' here?"", 'commenter': 'ctubbsii'}, {'comment': ""It verifies that one can call the 'notable' command without error."", 'commenter': 'jmark99'}, {'comment': ""Oh, sorry. I guess I didn't realize we had such a command. I'm guessing you use that to change to a context without a table before proceeding with the test with the table arguments."", 'commenter': 'ctubbsii'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java,"@@ -52,6 +52,23 @@ public void initialize(ServerContext context) {
     this.context = context;
     zooCache = new ZooCache(context.getZooReaderWriter(), null);
     ZKUserPath = Constants.ZROOT + ""/"" + context.getInstanceID() + ""/users"";
+    checkOutdatedHashes();
+  }
+
+  private void checkOutdatedHashes() {
+    try {
+      listUsers().forEach(user -> {
+        String zpath = ZKUserPath + ""/"" + user;
+        byte[] zkData = zooCache.get(zpath);
+        if (ZKSecurityTool.isOutdatedPass(zkData)) {
+          log.warn(""Found user(s) with outdated password hash. These will be re-hashed""
+              + "" on successful authentication."");","[{'comment': ""since we aren't including any details about which user(s) are impacted, I'd rather we not get a WARN for each user. could we move this to after we finish iterating with a summary of how many users?\r\n\r\nIf an operator needed to move towards eliminating these warn messages, how would they get the list of users that need to authenticate to the system?"", 'commenter': 'busbey'}, {'comment': 'One option is to provide the user name during these warnings. Another is to provide single warning, rather than one per user, and then provide some other mechanism to list outdated entries.', 'commenter': 'ctubbsii'}, {'comment': ""I'm never sure exactly how much detail is too much when it comes to logging security features. I can add an affected user count.\r\nI'm not sure the operator has to or should do anything, the update is automatic. But with their current tools they could replicate what the code does and could go through the users zNode checking hash lengths. Alternately I could list the affected user principals in the warning but that feels unsafe. Do you have something in mind?"", 'commenter': 'BukrosSzabolcs'}, {'comment': ""The list of users in the server logs is okay. However, I just realized... this method is only ever called when we're starting from an empty user database (on initialize... or re-initialize), so there shouldn't be any matching the old hashes anyway at the point this code is run. Or did I miss something?\r\n\r\nI do think maybe it would be worth the master server checking to see if any users are using the old hashing, and listing them in the server logs on startup, as an upgrade warning. In future, we can turn that into an upgrade blocker (so we can eventually drop the code that supports reading the old hashes)."", 'commenter': 'ctubbsii'}, {'comment': 'Added the check to master startup through SecurityOperation.', 'commenter': 'BukrosSzabolcs'}, {'comment': '@BukrosSzabolcs Feel free to mark the conversation as ""Resolved"" if you\'ve taken steps to address the comments. It cleans up the interface and makes it easier for subsequent reviews. :smiley_cat: ', 'commenter': 'ctubbsii'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java,"@@ -52,6 +52,23 @@ public void initialize(ServerContext context) {
     this.context = context;
     zooCache = new ZooCache(context.getZooReaderWriter(), null);
     ZKUserPath = Constants.ZROOT + ""/"" + context.getInstanceID() + ""/users"";
+    checkOutdatedHashes();
+  }
+
+  private void checkOutdatedHashes() {
+    try {
+      listUsers().forEach(user -> {
+        String zpath = ZKUserPath + ""/"" + user;
+        byte[] zkData = zooCache.get(zpath);
+        if (ZKSecurityTool.isOutdatedPass(zkData)) {
+          log.warn(""Found user(s) with outdated password hash. These will be re-hashed""
+              + "" on successful authentication."");
+          return;
+        }
+      });
+    } catch (NullPointerException e) {
+      // initializeSecurity was not called yet, there could be no outdated passwords stored","[{'comment': 'log a DEBUG message with these details.', 'commenter': 'busbey'}, {'comment': 'Could also avoid this by checking if the zk node exists first.', 'commenter': 'ctubbsii'}, {'comment': ""The zknode missing should be a corner case so I would prefer to keep the try/catch. I'll add a debug log message."", 'commenter': 'BukrosSzabolcs'}, {'comment': 'It looks like initialize itself is already an edge case... see my comment at https://github.com/apache/accumulo/pull/1798#discussion_r528903518', 'commenter': 'ctubbsii'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java,"@@ -116,6 +133,31 @@ public void createUser(String principal, AuthenticationToken token)
     }
   }
 
+  /**
+   * Creates user with outdated password hash for testing
+   *
+   * @deprecated since 2.1.0, only present for testing DO NOT USE!
+   */","[{'comment': 'could we log a WARN message that this method has been used? that way it would show up in operator logs should we mistakenly use it in a non-test context.', 'commenter': 'busbey'}, {'comment': ""Why do you feel a warning is necessary? It's not a widely used method and the original name was re-used for the new functionality. But if we would add a safeguard I would prefer to throw an AccumuloSecurityException at the end of the method. We can catch it in test context and would be harder to miss for a dev than a log msg. What do you think?"", 'commenter': 'BukrosSzabolcs'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java,"@@ -180,18 +222,43 @@ public boolean authenticateUser(String principal, AuthenticationToken token)
     if (!(token instanceof PasswordToken))
       throw new AccumuloSecurityException(principal, SecurityErrorCode.INVALID_TOKEN);
     PasswordToken pt = (PasswordToken) token;
-    byte[] pass;
+    byte[] zkData;
     String zpath = ZKUserPath + ""/"" + principal;
-    pass = zooCache.get(zpath);
-    boolean result = ZKSecurityTool.checkPass(pt.getPassword(), pass);
+    zkData = zooCache.get(zpath);
+    boolean result = authenticateUser(principal, pt, zkData);
     if (!result) {
       zooCache.clear(zpath);
-      pass = zooCache.get(zpath);
-      result = ZKSecurityTool.checkPass(pt.getPassword(), pass);
+      zkData = zooCache.get(zpath);
+      result = authenticateUser(principal, pt, zkData);
     }
     return result;
   }
 
+  private boolean authenticateUser(String principal, PasswordToken pt, byte[] zkData) {
+    if (zkData == null) {
+      return false;
+    }
+
+    // if the hash does not match the outdated format use Crypt to verify it
+    if (!ZKSecurityTool.isOutdatedPass(zkData)) {
+      return ZKSecurityTool.checkCryptPass(pt.getPassword(), zkData);
+    }
+
+    if (!ZKSecurityTool.checkPass(pt.getPassword(), zkData)) {
+      // if password does not match we are done
+      return false;
+    }
+
+    // if the password is correct we have to update the stored hash with new algorithm
+    try {
+      changePassword(principal, pt);
+      return true;
+    } catch (AccumuloSecurityException e) {
+      log.error(""Failed to update hashed user password for user: {}"", principal, e);
+    }
+    return false;","[{'comment': ""if we fail to update the password for some reason (like a transient zk write failure), at this point shouldn't we still return that they correctly authenticated?"", 'commenter': 'busbey'}, {'comment': 'My reasoning was that we are trying to re-hash the password if possible and on failure we are triggering retry and it should not consistently fail. I guess the question is how aggressively are we trying to re-hash. It would be fine for me either way. Please let me know if you would prefer it changed.', 'commenter': 'BukrosSzabolcs'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -47,6 +49,7 @@
 class ZKSecurityTool {
   private static final Logger log = LoggerFactory.getLogger(ZKSecurityTool.class);
   private static final int SALT_LENGTH = 8;
+  private static final Charset CRYPT_CHARSET = Charset.forName(""UTF-8"");","[{'comment': 'add a comment about why a new Charset instead of relying on `StandardCharsets.UTF_8`', 'commenter': 'busbey'}, {'comment': 'Switching to `StandardCharsets.UTF_8`', 'commenter': 'BukrosSzabolcs'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -56,16 +59,26 @@
     return salt;
   }
 
+  // only present for testing DO NOT USE!
+  @Deprecated(since = ""2.1.0"")
+  static byte[] createOutdatedPass(byte[] password) throws AccumuloException {
+    byte[] salt = generateSalt();
+    try {
+      return convertPass(password, salt);
+    } catch (NoSuchAlgorithmException e) {
+      log.error(""Count not create hashed password"", e);
+      throw new AccumuloException(""Count not create hashed password"", e);
+    }
+  }
+
   private static byte[] hash(byte[] raw) throws NoSuchAlgorithmException {
-    MessageDigest md = MessageDigest.getInstance(Constants.PW_HASH_ALGORITHM);
+    MessageDigest md = MessageDigest.getInstance(Constants.PW_HASH_ALGORITHM_OUTDATED);
     md.update(raw);
     return md.digest();
   }
 
+  @Deprecated(since = ""2.1.0"")
   public static boolean checkPass(byte[] password, byte[] zkData) {
-    if (zkData == null)
-      return false;
-","[{'comment': ""just to make sure I understand the reasoning here, the removal of this check is because ZKSecurityTool is package private and all current calls ensure zkData isn't null?\r\n\r\nif that's correct please add javadocs that say zkData can't be null. an alternative is to leave the check in place and rely on the jit to optimize it away."", 'commenter': 'busbey'}, {'comment': 'I re-added the check to be on the safe side, but and made it package private as I should have done already.', 'commenter': 'BukrosSzabolcs'}]"
1798,core/src/main/java/org/apache/accumulo/core/Constants.java,"@@ -99,7 +99,8 @@
   public static final long SCANNER_DEFAULT_READAHEAD_THRESHOLD = 3L;
 
   // Security configuration
-  public static final String PW_HASH_ALGORITHM = ""SHA-256"";
+  public static final String PW_HASH_ALGORITHM = ""SHA-512"";","[{'comment': ""We still need this constant updated because we rely on it for hashing the system credentials? won't that prevent a rolling upgrade?\r\n\r\nCould we have system credentials fall back to SHA-256 with a warning? or require a configurable flag to switch it?"", 'commenter': 'busbey'}, {'comment': ""changing this is also going to change some non-security uses, e.g. we optionally use it to obscure values printed from rfile metrics gathering. We'll need to enumerate these and release note the change in behavior. (or we could make something like a `NON_CRYPTO_USE_HASH_ALGORITHM` that we keep as SHA-256)"", 'commenter': 'busbey'}, {'comment': ""You are right. I'll make the system credentials hash customizable while defaulting to 256,  and renaming this as suggested to reflect it's for unsecure use."", 'commenter': 'BukrosSzabolcs'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -99,6 +102,24 @@ public static boolean checkPass(byte[] password, byte[] zkData) {
     return saltedHash; // contains salt+hash(password+salt)
   }
 
+  public static byte[] createPass(byte[] password) throws AccumuloException {
+    // we rely on default algorithm and hash length (SHA-512 and 8 byte)
+    String cryptHash = Crypt.crypt(password);
+    return cryptHash.getBytes(CRYPT_CHARSET);
+  }
+
+  public static boolean checkCryptPass(byte[] password, byte[] zkData) {
+    String zkDataString = new String(zkData, CRYPT_CHARSET);
+    String cryptHash;
+    try {
+      cryptHash = Crypt.crypt(password, zkDataString);
+    } catch (IllegalArgumentException e) {
+      log.error(""Unrecognized hash format"", e);
+      return false;
+    }
+    return MessageDigest.isEqual(zkData, cryptHash.getBytes(CRYPT_CHARSET));","[{'comment': ""What's the advantage of using `MessageDigest.isEqual`, vs. comparing as Strings or comparing as the UTF-8 byte-array?"", 'commenter': 'ctubbsii'}, {'comment': 'String comparisons is not secure against timing attacks, `MessageDigest.isEqual` is.', 'commenter': 'BukrosSzabolcs'}]"
1798,server/base/src/test/java/org/apache/accumulo/server/security/handler/ZKAuthenticatorTest.java,"@@ -88,14 +90,40 @@ public void testTableConversion() {
 
   @Test
   public void testEncryption() {
+    byte[] rawPass = ""myPassword"".getBytes(Charset.forName(""UTF-8""));
+    byte[] storedBytes;
+    try {
+      storedBytes = ZKSecurityTool.createPass(rawPass.clone());
+      assertTrue(ZKSecurityTool.checkCryptPass(rawPass.clone(), storedBytes));
+    } catch (AccumuloException e) {
+      log.error(""{}"", e.getMessage(), e);
+      fail();
+    }","[{'comment': 'You can simplify this JUnit test by just throwing this out of the method. JUnit will show the details of the exception in its logs.', 'commenter': 'ctubbsii'}, {'comment': 'Thank you for the suggestion!', 'commenter': 'BukrosSzabolcs'}]"
1798,test/src/main/java/org/apache/accumulo/test/ZKAuthenticatorIT.java,"@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test;
+
+import static org.junit.Assert.assertTrue;
+
+import java.nio.charset.Charset;
+
+import org.apache.accumulo.core.client.security.tokens.PasswordToken;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.accumulo.server.security.handler.ZKAuthenticator;
+import org.junit.Test;
+
+public class ZKAuthenticatorIT extends AccumuloClusterHarness {","[{'comment': 'If we can avoid running a full ZK instance using Mini, I think we should. I think ZKAuthenticator can be tested fully using unit tests with mocking, without a need to start up an instance of a cluster.', 'commenter': 'ctubbsii'}, {'comment': ""It can. I'm rewriting it."", 'commenter': 'BukrosSzabolcs'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/SystemCredentials.java,"@@ -103,7 +102,8 @@ private static SystemToken get(String instanceID, SiteConfiguration siteConfig)
       byte[] confChecksum;
       MessageDigest md;
       try {
-        md = MessageDigest.getInstance(Constants.PW_HASH_ALGORITHM);
+        String hashAlgorithm = siteConfig.get(Property.SYSTEM_TOKEN_HASH_TYPE);
+        md = MessageDigest.getInstance(hashAlgorithm);","[{'comment': ""Since system credentials are not serialized anywhere, but we do want them to be a strong hash, I think it might be best to make use of crypt(3) here also, so we're using the default best hash available in the commons-codec library.\r\n\r\nWe can use a fixed salt for this. We could make the salt configurable or based on `instance.secret`, but there's no additional security added by doing so, since `instance.secret` is configurable and is already included in the message digest. So, a fixed salt would suffice here. If you're not interested in pursuing this, here, it can be done as a follow-on issue. Just let me know if that's the case, so I can create a new issue for it.\r\n\r\nAs you've identified, the one benefit to making it configurable and preserving the current one is to support rolling restart. However, we don't support rolling restarts very well already, and certainly not across major or minor releases, and we've likely already broken compatibility of that sort during 2.1's development (or will, if we upgrade Thrift). Also, I think having a strong has for the system user is important enough to break that, even if it did otherwise work. So, I'm inclined to favor not making it configurable."", 'commenter': 'ctubbsii'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java,"@@ -54,6 +56,33 @@ public void initialize(ServerContext context) {
     ZKUserPath = Constants.ZROOT + ""/"" + context.getInstanceID() + ""/users"";
   }
 
+  /**
+   * Checks stored users and logs a warning containing the ones with outdated hashes.
+   */
+  public boolean hasOutdatedHashes() {","[{'comment': 'If it makes more readable code in the callers (fewer negations), you could also flip the return values, and call this method `areHashesCurrent()` or similar.', 'commenter': 'ctubbsii'}]"
1798,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -47,6 +51,7 @@
 class ZKSecurityTool {
   private static final Logger log = LoggerFactory.getLogger(ZKSecurityTool.class);
   private static final int SALT_LENGTH = 8;
+  private static final Charset CRYPT_CHARSET = UTF_8;","[{'comment': 'Could inline this constant, to avoid the extra `CRYPT_CHARSET` variable and corresponding Charset import.', 'commenter': 'ctubbsii'}]"
1803,server/base/src/main/java/org/apache/accumulo/server/master/recovery/RecoveryPath.java,"@@ -18,13 +18,19 @@
  */
 package org.apache.accumulo.server.master.recovery;
 
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.cli.ServerUtilOpts;
 import org.apache.accumulo.server.fs.VolumeManager.FileType;
 import org.apache.hadoop.fs.Path;
 
 public class RecoveryPath {
 
   // given a wal path, transform it to a recovery path
   public static Path getRecoveryPath(Path walPath) {
+
+    ServerUtilOpts opts = new ServerUtilOpts();
+    ServerContext context = opts.getServerContext();","[{'comment': ""This isn't the right way to get the ServerContext. The caller should pass in the context reference that they have, as an additional parameter to this method. I see two callers for this method. `TabletServer` should have a `getContext()` method on it, and `RecoveryManager` can use `master.getContext()`."", 'commenter': 'ctubbsii'}]"
1803,server/base/src/main/java/org/apache/accumulo/server/master/recovery/RecoveryPath.java,"@@ -42,7 +48,8 @@ public static Path getRecoveryPath(Path walPath) {
       // drop wal
       walPath = walPath.getParent();
 
-      walPath = new Path(walPath, FileType.RECOVERY.getDirectory());
+      walPath = new Path(walPath,
+          FileType.RECOVERY.getDirectory() + '/' + context.getUniqueNameAllocator().getNextName());","[{'comment': ""It would be good to log a message about the name that was chosen, before being used here, as a debug message, so that a an admin can trace the specific recovery directory back to a specific server from the server logs if necessary.\r\n\r\nInstead of using `'/'`, you can use `'-'` so that way the directories look like `recovery-<uniquePart>`."", 'commenter': 'ctubbsii'}]"
1803,server/base/src/main/java/org/apache/accumulo/server/master/recovery/RecoveryPath.java,"@@ -42,14 +48,14 @@ public static Path getRecoveryPath(Path walPath) {
       // drop wal
       walPath = walPath.getParent();
 
-      walPath = new Path(walPath, FileType.RECOVERY.getDirectory());
+      walPath = new Path(walPath,
+          FileType.RECOVERY.getDirectory() + '-' + context.getUniqueNameAllocator().getNextName());
+      log.debug(""Unique Name Allocated:  "" + walPath);","[{'comment': '```suggestion\r\n      log.debug(""Directory selected for WAL recovery:  "" + walPath);\r\n```', 'commenter': 'ctubbsii'}]"
1803,server/manager/src/main/java/org/apache/accumulo/master/recovery/RecoveryManager.java,"@@ -171,6 +171,8 @@ public boolean recoverLogs(KeyExtent extent, Collection<Collection<String>> walo
         String dest =
             RecoveryPath.getRecoveryPath(new Path(filename), master.getContext()).toString();
 
+        log.debug(""This is walPath in RecoveryManager: "" + filename);
+","[{'comment': 'You could use parameter replacement in log statements\r\n`log.debug(""This is walPath in RecoveryManager: {}"", filename);`', 'commenter': 'EdColeman'}, {'comment': 'Thank you EdColeman, I am making the change now.', 'commenter': 'tynyttie'}, {'comment': 'I see you updated this log statement in the latest commit. Looks like there may be a couple others spots that could be changed to use this as well for the sake of consistency.', 'commenter': 'DomGarguilo'}]"
1803,server/base/src/main/java/org/apache/accumulo/server/master/recovery/RecoveryPath.java,"@@ -41,15 +47,14 @@ public static Path getRecoveryPath(Path walPath) {
 
       // drop wal
       walPath = walPath.getParent();
+      walPath = new Path(walPath, FileType.RECOVERY.getDirectory() + '-' + context.getInstanceID());","[{'comment': ""Perhaps I am missing something here, but isn't context.getInstanceID() the accumulo instance id?  How does this make the directory any more unique?"", 'commenter': 'ivakegg'}, {'comment': ""You're not missing anything. You're correct. This does not make it unique per tserver process."", 'commenter': 'ctubbsii'}]"
1803,server/base/src/main/java/org/apache/accumulo/server/master/recovery/RecoveryPath.java,"@@ -41,15 +50,46 @@ public static Path getRecoveryPath(Path walPath) {
 
       // drop wal
       walPath = walPath.getParent();
+      walPath = new Path(walPath, FileType.RECOVERY.getDirectory() + ""-"" + convertToSha1(portHost));","[{'comment': 'commons-codec, which is one of our dependencies, has a method, `DigestUtils.sha1Hex(string)`', 'commenter': 'ctubbsii'}, {'comment': 'Got it. Thank you. ', 'commenter': 'tynyttie'}]"
1803,server/manager/src/main/java/org/apache/accumulo/master/recovery/RecoveryManager.java,"@@ -148,6 +149,18 @@ private boolean exists(final Path path) throws IOException {
     }
   }
 
+  public String getHostPort() {
+
+    String hostPort = """";
+    Set<TServerInstance> tserverInstances = master.onlineTabletServers();
+
+    if (tserverInstances.isEmpty() && tserverInstances.size() < 0) {
+      return hostPort = null;
+
+    } else
+      return hostPort = tserverInstances.stream().findFirst().get().toString();
+  }
+","[{'comment': ""Size can't be less than 0.\r\n\r\n```suggestion\r\n  public String getHostPort() {\r\n    Set<TServerInstance> tserverInstances = master.onlineTabletServers();\r\n    return tserverInstances.isEmpty() ? null : tserverInstances.stream().findFirst().get().toString();\r\n  }\r\n\r\n```\r\n\r\nHowever, this won't work either, because this doesn't return the current tserver's host and port, it returns whichever the master.onlineTabletServers() returns first."", 'commenter': 'ctubbsii'}, {'comment': 'Do you have an idea on how to properly implement this? `RecoveryManager.recoverLogs` gets called in `tabletGroupWatcher.run` when the tablets are not hosted and there are walogs. From testing, `tls.current` is null at this point. We could use `tls.last` but I am unsure if last will always be set and is prone to change.  Thoughts?', 'commenter': 'Manno15'}, {'comment': ""@Manno15 If the question is directed at me: I don't, not without digging into this to come up with a solution myself. If `tls.last` isn't guaranteed to be the tserver that is going to be assigned this, and performing the recovery, then that would not be appropriate either.\r\n\r\nIt seems to me that the basic requirement is that the manager must decide the destination path, then somehow communicate that to the tserver that was assigned to do the sorting work. If that tserver doesn't finish, then the manager must decide on a new destination path, and communicate that to the next tserver requested to do the work (possibly the same tserver, if it came back online). For any given work order to sort the logs, the tserver must use the directory specified, and presumably, the manager must be able to know which ones have succeeded and which have failed. But, I'm really not that familiar with the recovery process, and would have to do a lot of digging to understand this more."", 'commenter': 'ctubbsii'}, {'comment': ""> @Manno15 If the question is directed at me: I don't, not without digging into this to come up with a solution myself. If tls.last isn't guaranteed to be the tserver that is going to be assigned this, and performing the recovery, then that would not be appropriate either.\r\n\r\nI am not sure yet if tls.last is guranteed. I or @tynyttie  would have to do more research on a bigger cluster to get to the bottom of this. \r\n\r\n> it seems to me that the basic requirement is that the manager must decide the destination path, then somehow communicate that to the tserver that was assigned to do the sorting work. \r\n\r\nBased on what I am reading, the Tserver side of recover is only called in `Tablet.java`. This is after `RecoveryManager` is finishing do its side of te work. Not sure how everything fits in to get the full picture yet. \r\n\r\n\r\n"", 'commenter': 'Manno15'}]"
1803,server/base/src/main/java/org/apache/accumulo/server/master/recovery/RecoveryPath.java,"@@ -50,7 +47,8 @@ public static Path getRecoveryPath(Path walPath, String portHost) {
 
       // drop wal
       walPath = walPath.getParent();
-      walPath = new Path(walPath, FileType.RECOVERY.getDirectory() + ""-"" + convertToSha1(portHost));
+      walPath =
+          new Path(walPath, FileType.RECOVERY.getDirectory() + ""-"" + DigestUtils.sha1Hex(portHost));","[{'comment': 'Recently it was noted that using sha1 can be flagged by some code security scan utilities - would using sha256 or even sha512 eliminate those utilities from flagging ""unsecure usages""?  This is not a security related hash so it does not matter, but those utilities don\'t take that into account.\r\n\r\nThe length of the string could also be shortened using something like:\r\n\r\n```String s = new BigInteger(DigestUtils.sha256(portHost).toString(MAX_RADIX).substring(0,40)```\r\n\r\nThis would have the same number of characters as the sha1 digest, but more ""bits"" using base36 vs base16 - or the string length could be shortened so that it was equivalent to the 40 digit sha1 without increasing the (small) risk of a collision. This also would allow using sha512 without a ridiculous string length.', 'commenter': 'EdColeman'}, {'comment': ""I believe there's a variant of the sha256 method in DigestUtils that returns a hex-encoded string, that can simply be truncated directly, rather than using BigInteger, which shouldn't be necessary.\r\n\r\nHowever, I'm not sure that this hashing of a location strategy is going to work at all, based on the conversation above in https://github.com/apache/accumulo/pull/1803#discussion_r568151689"", 'commenter': 'ctubbsii'}]"
1816,server/base/src/test/java/org/apache/accumulo/server/util/time/SimpleTimerTest.java,"@@ -98,11 +97,15 @@ public void testFailure() throws InterruptedException {
     assertTrue(r.wasRun);
   }
 
-  @Test
-  public void testSingleton() {
-    assertEquals(1, SimpleTimer.getInstanceThreadPoolSize());
-    SimpleTimer t2 = SimpleTimer.getInstance(2);
-    assertSame(t, t2);
+  @Test(timeout = 5000)
+  public void testSingleton() throws InterruptedException {
+    while (true) {
+      SimpleTimer t2 = SimpleTimer.getInstance(2);
+      if((SimpleTimer.getInstanceThreadPoolSize() == 1) && (t == t2)) {
+        break;
+      }
+      Thread.sleep(PAD);
+    }","[{'comment': 'This is the right strategy (albeit with a few extra parenthesis than necessary on that `if` statement :wink: )', 'commenter': 'ctubbsii'}]"
1816,server/base/src/test/java/org/apache/accumulo/server/util/time/SimpleTimerTest.java,"@@ -70,35 +69,50 @@ public void run() {
     }
   }
 
-  @Test
+  @Test(timeout = 5000)
   public void testOneTimeSchedule() throws InterruptedException {
     AtomicInteger i = new AtomicInteger();
     Incrementer r = new Incrementer(i);
     t.schedule(r, DELAY);
     Thread.sleep(DELAY + PAD);
-    assertEquals(1, i.get());
+    while (true) {
+      if (i.get() == 1) {
+        break;
+      }
+      Thread.sleep(PAD);
+    }","[{'comment': 'Just realized that you can simplify this to `while (i.get() != 1) { sleep }`; you can do similar things for the other tests as well.', 'commenter': 'ctubbsii'}, {'comment': 'Was considering that as well. I will add this change.', 'commenter': 'DomGarguilo'}]"
1824,server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java,"@@ -271,19 +273,23 @@ static boolean checkInit(VolumeManager fs, SiteConfiguration sconf, Configuratio
       return false;
     }
     if (sconf.get(Property.INSTANCE_SECRET).equals(Property.INSTANCE_SECRET.getDefaultValue())) {
-      ConsoleReader c = getConsoleReader();
-      c.beep();
-      c.println();
-      c.println();
-      c.println(""Warning!!! Your instance secret is still set to the default,""
-          + "" this is not secure. We highly recommend you change it."");
-      c.println();
-      c.println();
-      c.println(""You can change the instance secret in accumulo by using:"");
-      c.println(""   bin/accumulo "" + org.apache.accumulo.server.util.ChangeSecret.class.getName());
-      c.println(""You will also need to edit your secret in your configuration""
-          + "" file by adding the property instance.secret to your""
-          + "" accumulo.properties. Without this accumulo will not operate"" + "" correctly"");
+      LineReader c = getLineReader();
+      c.getTerminal().puts(InfoCmp.Capability.bell);
+      c.getTerminal().writer().println();","[{'comment': 'Does anyone know why the print statements are like this as opposed to one/fewer prints with linebreaks (`\\n`). It seems as though there might be a reason `println`s are used in this way, but it seems like it would be cleaner to consolidate this text into less prints if possible. I see that it was this way before this PR so its not quite related but I was just curious.', 'commenter': 'DomGarguilo'}, {'comment': ""Sometimes this is done for readability, sometimes to reuse a buffer for performance to avoid unnecessary string concatenation (as in StringBuilder append methods), and sometimes to defer to environment-specific implementations (like cross-platform newline differences). It could also just be more convenient for a developer to copy/paste as many `println` calls as they want, rather than edit in a specific number of `\\n` in several different String literals.\r\n\r\nI'm not sure why it's used this way in this particular case, but probably some combination of readability and developer convenience. But, it probably also makes sense in this case to defer to the terminal's environment-specific implementation, since the terminal itself is intended as an abstraction around the command-line environment."", 'commenter': 'ctubbsii'}, {'comment': 'Okay great. Thanks for the explanation.', 'commenter': 'DomGarguilo'}]"
1824,shell/src/main/java/org/apache/accumulo/shell/format/DeleterFormatter.java,"@@ -79,33 +79,36 @@ public String next() {
     Mutation m = new Mutation(key.getRow());
     String entryStr = formatEntry(next, isDoTimestamps());
     boolean delete = force;
-    try {
-      if (!force) {
-        shellState.getReader().flush();
+    if (!force) {
+      try {
+        shellState.getWriter().flush();
+        // this will cause end of file Exception for one of the formatter tests
         String line = shellState.getReader().readLine(""Delete { "" + entryStr + "" } ? "");
         more = line != null;
         delete = line != null && (line.equalsIgnoreCase(""y"") || line.equalsIgnoreCase(""yes""));","[{'comment': 'Could the portion before `&&` be replaced with `more`?  I know this is unrelated but thought I would point it out.\r\n```suggestion\r\n        delete = more && (line.equalsIgnoreCase(""y"") || line.equalsIgnoreCase(""yes""));\r\n```', 'commenter': 'DomGarguilo'}]"
1824,shell/src/main/java/org/apache/accumulo/shell/Shell.java,"@@ -584,40 +586,39 @@ public int start() throws IOException {
 
         // If tab completion is true we need to reset
         if (tabCompletion) {
-          if (userCompletor != null) {
-            reader.removeCompleter(userCompletor);
-          }
-
           userCompletor = setupCompletion();
-          reader.addCompleter(userCompletor);
+          ((LineReaderImpl) reader).setCompleter(userCompletor);
         }
 
-        reader.setPrompt(getDefaultPrompt());
-        input = reader.readLine();
+        input = reader.readLine(getDefaultPrompt());
         if (input == null) {
-          reader.println();
           return exitCode;
         } // User Canceled (Ctrl+D)
 
         execCommand(input, disableAuthTimeout, false);
       } catch (UserInterruptException uie) {
         // User Cancelled (Ctrl+C)
-        reader.println();
+        writer.println();
 
         String partialLine = uie.getPartialLine();
         if (partialLine == null || """".equals(uie.getPartialLine().trim())) {
           // No content, actually exit
           return exitCode;
         }
       } finally {
-        reader.flush();
+        writer.flush();
       }
     }
   }
 
   public void shutdown() {
     if (reader != null) {
-      reader.close();
+      try {
+        terminal.close();
+        reader = null;
+      } catch (IOException e) {
+        e.printStackTrace();","[{'comment': 'I suggest adding a log.warn or log.error here to capture the exception within the logs.', 'commenter': 'lbschanno'}, {'comment': ""I concur. We don't generally just print stack traces with `e.printStackTrace()`."", 'commenter': 'ctubbsii'}, {'comment': 'I will add. ', 'commenter': 'Manno15'}, {'comment': 'I noticed a printExcetion function included in the shell class already so I used that for this. Let me know if that suffices', 'commenter': 'Manno15'}]"
1824,shell/src/main/java/org/apache/accumulo/shell/Shell.java,"@@ -543,23 +551,17 @@ public int start() throws IOException {
     if (!accumuloDir.exists() && !accumuloDir.mkdirs()) {
       log.warn(""Unable to make directory for history at {}"", accumuloDir);
     }
-    try {
-      final FileHistory history = new FileHistory(new File(historyPath));
-      reader.setHistory(history);
-      // Add shutdown hook to flush file history, per jline javadocs
-      Runtime.getRuntime().addShutdownHook(new Thread(() -> {
-        try {
-          history.flush();
-        } catch (IOException e) {
-          log.warn(""Could not flush history to file."");
-        }
-      }));
-    } catch (IOException e) {
-      log.warn(""Unable to load history file at {}"", historyPath);
-    }
+
+    // Remove Timestamps for history file. Fixes incompatibility issues
+    reader.unsetOpt(LineReader.Option.HISTORY_TIMESTAMPED);
+
+    // Set history file
+    reader.setVariable(LineReader.HISTORY_FILE, new File(historyPath));
 
     // Turn Ctrl+C into Exception instead of JVM exit
-    reader.setHandleUserInterrupt(true);
+    // Not 100% sure this is necessary anymore.","[{'comment': 'Did you test CTRL-C without the lines below? Or `kill -2 <pid>` ?', 'commenter': 'dlmarion'}, {'comment': 'I believe CTRL-C still worked without it but I will double check tomorrow to verify. ', 'commenter': 'Manno15'}, {'comment': 'In my testing, both the main branch and these changes do have the same functionality for CTRL-C but they both might be wrong. When I  hit CTRL-C, it exits the shell which I do not think is what we are going for. I tested the jline3 demo and CTRL-C will reset back to the shell but not exit the shell. That is probably the functionality we want. The funny thing is, I am implementing what they did in their demo but for some reason, it does not seem to be working. So this will take some more investigating. ', 'commenter': 'Manno15'}, {'comment': ""I did find that when I CTRL-C in the middle of the help command, it does work properly. As in it throws the interrupt exception but doesn't exit the process. It only exits if I CTRL-C when just sitting in the shell. So this signal handle is at least needed for that, unsure on why it still exits when CTRL-C not during a command. "", 'commenter': 'Manno15'}]"
1824,server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java,"@@ -271,19 +273,23 @@ static boolean checkInit(VolumeManager fs, SiteConfiguration sconf, Configuratio
       return false;
     }
     if (sconf.get(Property.INSTANCE_SECRET).equals(Property.INSTANCE_SECRET.getDefaultValue())) {
-      ConsoleReader c = getConsoleReader();
-      c.beep();
-      c.println();
-      c.println();
-      c.println(""Warning!!! Your instance secret is still set to the default,""
-          + "" this is not secure. We highly recommend you change it."");
-      c.println();
-      c.println();
-      c.println(""You can change the instance secret in accumulo by using:"");
-      c.println(""   bin/accumulo "" + org.apache.accumulo.server.util.ChangeSecret.class.getName());
-      c.println(""You will also need to edit your secret in your configuration""
-          + "" file by adding the property instance.secret to your""
-          + "" accumulo.properties. Without this accumulo will not operate"" + "" correctly"");
+      LineReader c = getLineReader();
+      c.getTerminal().puts(InfoCmp.Capability.bell);
+      c.getTerminal().writer().println();
+      c.getTerminal().writer().println();
+
+      c.getTerminal().writer()","[{'comment': ""It might make sense to assign the writer to a new variable, so that way all this line wrapping doesn't occur.\r\n\r\n```java\r\nvar w = c.getTerminal().writer();\r\nw.println(...);\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Done in latest commit', 'commenter': 'Manno15'}]"
1824,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -895,7 +904,7 @@ public void clonetableOffline() throws Exception {
     ts.exec(""scan"", false, ""TableOfflineException"", true);
     ts.exec(""constraint --list -t "" + clone, true, ""VisibilityConstraint=2"", true);
     ts.exec(""config -t "" + clone + "" -np"", true, ""123M"", true);
-    ts.exec(""getsplits -t "" + clone, true, ""a\nb\nc\n"");
+    ts.exec(""getsplits -t "" + clone, true, ""abc"");","[{'comment': ""These splits should be on separate lines. It seems like either this behavior has changed, or there's a problem with these tests."", 'commenter': 'ctubbsii'}]"
1824,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -182,14 +186,19 @@ public String getErrorMessage() {
     public TestOutputStream output;
     public StringInputStream input;
     public Shell shell;
+    public LineReader reader;
+    public Terminal terminal;
 
     TestShell(String user, String rootPass, String instanceName, String zookeepers, File configFile)
         throws IOException {
       ClientInfo info = ClientInfo.from(configFile.toPath());
       // start the shell
       output = new TestOutputStream();
       input = new StringInputStream();
-      shell = new Shell(new ConsoleReader(input, output));
+      terminal = new DumbTerminal(input, output);
+      terminal.setSize(new Size(80, 24));
+      reader = LineReaderBuilder.builder().terminal(terminal).build();
+      shell = new Shell(reader);","[{'comment': 'I wonder if it would make more sense to pass in ""terminal"" instead of just the reader?', 'commenter': 'ctubbsii'}, {'comment': 'I am unsure if it would change anything since the terminal gets attached to the reader. We could just pass in ""terminal"" and then build the reader inside `Shell.java` so keep things consistent there. Not sure what that would gain ultimately. ', 'commenter': 'Manno15'}]"
1824,test/src/main/java/org/apache/accumulo/test/ShellIT.java,"@@ -207,9 +215,9 @@ public void addGetSplitsTest() throws IOException {
     exec(""addsplits arg"", false, ""java.lang.IllegalStateException: Not in a table context"");
     exec(""createtable test"", true);
     exec(""addsplits 1 \\x80"", true);
-    exec(""getsplits"", true, ""1\n\\x80"");
+    exec(""getsplits"", true, ""1\\x80"");
     exec(""getsplits -m 1"", true, ""1"");
-    exec(""getsplits -b64"", true, ""MQ==\ngA=="");
+    exec(""getsplits -b64"", true, ""MQ==gA=="");","[{'comment': 'If these tests pass without the newline, then that seems like it would indicate a breaking change to getsplits.', 'commenter': 'ctubbsii'}]"
1824,shell/src/test/java/org/apache/accumulo/shell/format/DeleterFormatterTest.java,"@@ -133,6 +141,7 @@ public void testNo() throws IOException {
     assertTrue(formatter.hasNext());
   }
 
+  // Need to properly fix this test.","[{'comment': ""What's this comment about?"", 'commenter': 'ctubbsii'}, {'comment': 'Related to your other comment below in `DeleterFormatter`. ', 'commenter': 'Manno15'}]"
1824,shell/src/test/java/org/apache/accumulo/shell/commands/HistoryCommandTest.java,"@@ -56,38 +62,41 @@ public void setUp() throws Exception {
     expect(cl.hasOption(""np"")).andReturn(true);
     replay(cl);
 
-    History history = new MemoryHistory();
+    History history = new DefaultHistory();
     history.add(""foo"");
     history.add(""bar"");
 
     baos = new ByteArrayOutputStream();
 
     String input = String.format(""!1%n""); // Construct a platform dependent new-line
-    reader = new ConsoleReader(new ByteArrayInputStream(input.getBytes()), baos);
-    reader.setHistory(history);
+    terminal = TerminalBuilder.builder().system(false)
+        .streams(new ByteArrayInputStream(input.getBytes()), baos).build();
+    reader = LineReaderBuilder.builder().history(history).terminal(terminal).build();","[{'comment': 'Is there a way to add the history directly to the terminal, and using its line reader, or do we have to create a new line reader separately from the terminal?', 'commenter': 'ctubbsii'}]"
1824,shell/src/test/java/org/apache/accumulo/shell/ShellConfigTest.java,"@@ -29,14 +29,14 @@
 import java.io.PrintStream;
 import java.nio.file.Files;
 
+import org.jline.reader.impl.LineReaderImpl;","[{'comment': 'Is it safe to use the impl here? Doing so seems fragile.', 'commenter': 'ctubbsii'}]"
1824,shell/src/main/java/org/apache/accumulo/shell/format/DeleterFormatter.java,"@@ -79,33 +79,36 @@ public String next() {
     Mutation m = new Mutation(key.getRow());
     String entryStr = formatEntry(next, isDoTimestamps());
     boolean delete = force;
-    try {
-      if (!force) {
-        shellState.getReader().flush();
+    if (!force) {
+      try {
+        shellState.getWriter().flush();
+        // this will cause end of file Exception for one of the formatter tests","[{'comment': 'Which one? Why? Can it be fixed?', 'commenter': 'ctubbsii'}, {'comment': 'This is related to the other comment you mentioned in the `DeleterFormatterTest`. I will have to refresh my memory on it a bit. ', 'commenter': 'Manno15'}, {'comment': 'the failure happens on line 86 where it tries to read the delete prompt.  When `DeleterFormatterTest.testNoConfirmation()` is ran, it hits that line but gets an end of file marker instead. ', 'commenter': 'Manno15'}]"
1824,shell/src/main/java/org/apache/accumulo/shell/commands/HistoryCommand.java,"@@ -39,11 +39,11 @@
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws IOException {
     if (cl.hasOption(clearHist.getOpt())) {
-      shellState.getReader().getHistory().clear();
+      shellState.getReader().getHistory().purge();
     } else {
-      Iterator<Entry> source = shellState.getReader().getHistory().entries();
+      Iterator<History.Entry> source = shellState.getReader().getHistory().iterator();
       Iterator<String> historyIterator = Iterators.transform(source,
-          input -> String.format(""%d: %s"", input.index() + 1, input.value()));
+          input -> String.format(""%d: %s"", input.index() + 1, input.line()));","[{'comment': ""It looks like Iterator was used here previously because `.entries()` returned a type of `Iterator`. However, it now looks like `.getHistory()` returns a type of `Iterable`, which gives an opportunity to write this differently, using Java streams or something, and to avoid the use of Guava's `Iterators.transform()` in favor of Java built-ins. It's not important, though. It's fine either way."", 'commenter': 'ctubbsii'}]"
1824,shell/src/main/java/org/apache/accumulo/shell/commands/HiddenCommand.java,"@@ -42,9 +43,9 @@ public String description() {
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws Exception {
     if (rand.nextInt(10) == 0) {
-      shellState.getReader().beep();
-      shellState.getReader().println();
-      shellState.getReader()
+      shellState.getTerminal().puts(InfoCmp.Capability.bell);","[{'comment': 'Have you tested the bell to make sure it actually rings? (might depend on your local development environment).', 'commenter': 'ctubbsii'}, {'comment': 'I have not but I will try on my end. This was how the devs on the mailing list said to do it. I never heard the bell on the old shell so I am not sure if I will hear this one.', 'commenter': 'Manno15'}]"
1824,shell/src/main/java/org/apache/accumulo/shell/ShellCompletor.java,"@@ -45,13 +48,16 @@ public ShellCompletor(Token rootToken, Map<CompletionSet,Set<String>> options) {
 
   @Override
   @SuppressWarnings({""unchecked"", ""rawtypes""})","[{'comment': ""These warning suppressions might not make sense anymore, now that you've added the appropriate generics to the candidates list."", 'commenter': 'ctubbsii'}]"
1824,shell/src/main/java/org/apache/accumulo/shell/Shell.java,"@@ -584,40 +586,39 @@ public int start() throws IOException {
 
         // If tab completion is true we need to reset
         if (tabCompletion) {
-          if (userCompletor != null) {
-            reader.removeCompleter(userCompletor);
-          }
-
           userCompletor = setupCompletion();
-          reader.addCompleter(userCompletor);
+          ((LineReaderImpl) reader).setCompleter(userCompletor);
         }
 
-        reader.setPrompt(getDefaultPrompt());
-        input = reader.readLine();
+        input = reader.readLine(getDefaultPrompt());
         if (input == null) {
-          reader.println();","[{'comment': ""This println previously existed, so the user's bash shell or whatever, wouldn't print its prompt at the end of Accumulo's prompt, when existing by closing the input stream. I assume by removing this println, that no longer occurs? If it does, that should probably be put back."", 'commenter': 'ctubbsii'}]"
1832,test/src/main/java/org/apache/accumulo/test/AuditMessageIT.java,"@@ -482,6 +482,10 @@ public void testDeniedAudits() throws AccumuloSecurityException, AccumuloExcepti
       auditConnector.tableOperations().deleteRows(OLD_TEST_TABLE_NAME, new Text(""myRow""),
           new Text(""myRow~""));
     } catch (AccumuloSecurityException ex) {}
+    try {
+      auditConnector.tableOperations().flush(OLD_TEST_TABLE_NAME, new Text(""start""),
+              new Text(""end""), false);","[{'comment': ""Normally, a range would be specified so that the start sorts before the end, but 's' sorts after 'e'. You should change it to match `myRow` and `myRow~` like the previous line, instead, so that way this will still work, even if we later add additional argument checks to make sure the start sorts before the end."", 'commenter': 'ctubbsii'}, {'comment': 'Funny thing is, that is actually what I had originally but I changed it to mimic the parameters names for whatever reason. ', 'commenter': 'Manno15'}]"
1833,core/src/main/java/org/apache/accumulo/core/util/HostAndPort.java,"@@ -105,7 +105,7 @@ public boolean hasPort() {
    *           occurring.
    */
   public int getPort() {
-    checkState(hasPort());
+    checkState(hasPort(), ""The given address does not include a port"");","[{'comment': 'Just a minor wording suggestion:\r\n\r\n```suggestion\r\n    checkState(hasPort(), ""the address does not include a port"");\r\n```\r\n\r\nThe reason for this is that the exception could also occur when the user has not ""given"" any address. For example, if it\'s our own internal code. In that case, the word ""given"" might be confusing.', 'commenter': 'ctubbsii'}]"
1833,shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java,"@@ -86,6 +90,7 @@ private static String formatActiveCompactionLine(String tserver, ActiveCompactio
         compactions.add(formatActiveCompactionLine(tserver, ac));
       }
     } catch (Exception e) {
+      log.debug(""Exception thrown while attempting to list active compactions"", e);","[{'comment': '```suggestion\r\n      log.debug(""Failed to list active compactions for server {}"", tserver, e);\r\n```', 'commenter': 'ctubbsii'}]"
1847,test/src/main/java/org/apache/accumulo/test/functional/ConcurrentDeleteTableIT.java,"@@ -52,7 +52,7 @@
 
 public class ConcurrentDeleteTableIT extends AccumuloClusterHarness {
 
-  @Test
+  @Test(timeout = 3 * 60 * 1000)","[{'comment': 'In the super class, we have a method that can be overridden called `defaultTimeoutSeconds`, which probably would work better for this. The timeout for that is scalable on the command-line with `-Dtimeout.factor=2` or similar. See `SplitIT.java` for an example.', 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;","[{'comment': ""commons-codec is not currently a direct dependency of the test module. I don't think it's worth adding it here, since the use of sha1 will likely trigger the secbugs plugin anyway. I would use Guava's Hasher.sha512 instead. It has a convenient hashString method."", 'commenter': 'ctubbsii'}, {'comment': ""I didn't realize it was not a direct dep, good catch.  I started off using Guava Hasher, but I think it was marked `@Beta` so I looked for an alternative.  I will look for some other way to do this."", 'commenter': 'keith-turner'}, {'comment': 'Tried another way in 00b3542', 'commenter': 'keith-turner'}, {'comment': ""It's fine to use the Beta method for a test. We don't need to worry about version incompatibilities in our test cases. It would be shorter code. Also, we already use it in other tests, and even some non-tests (which we shouldn't, but we do)."", 'commenter': 'ctubbsii'}, {'comment': 'I was thinking if anyone wants to try to remove more `@Beta` usage in the future, this is one less place to deal with it. I like not introducing new `@Beta` usage intentionally even if there is existing usage. ', 'commenter': 'keith-turner'}, {'comment': ""Makes sense, but I still think it's fine for Test code, since the thing we're concerned about (incompatibility with user-provided version on class path, needed by other components) is not an issue for our tests."", 'commenter': 'ctubbsii'}, {'comment': ""> but I still think it's fine for Test code\r\n\r\nI suppose that its ok, if we would never want to remove `@Beta` usage from test code.  In that case its not creating possible future work.  I will switch it, Guava's Hashing class is very pleasant to use too bad its beta."", 'commenter': 'keith-turner'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {","[{'comment': 'Since this messes with ZK, it might be best to configure this with the MiniClusterOnlyTests Junit category.\r\n\r\n```suggestion\r\n@Category(MiniClusterOnlyTests.class)\r\npublic class ZooMutatorIT extends AccumuloClusterHarness {\r\n```', 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;","[{'comment': '```suggestion\r\n    try (var client = Accumulo.newClient().from(getClientProps()).build();\r\n        var context = (ClientContext) client) {\r\n```', 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;
+      var secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);","[{'comment': ""Some of these uses of `var` are a bit questionable, since they don't make the line shorter or more readable, nor are they used when the variable's type is obvious. I would use the actual type in many of these.\r\n\r\n```suggestion\r\n      String secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);\r\n```"", 'commenter': 'ctubbsii'}, {'comment': ""> Some of these uses of var are a bit questionable, since they don't make the line shorter or more readable\r\n\r\nI agree.  It was faster to use them when writing the code, but it is less readable."", 'commenter': 'keith-turner'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;
+      var secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
+          context.getZooKeepersSessionTimeOut(), secret);
+
+      var execServ = Executors.newFixedThreadPool(16);","[{'comment': '`var` is reasonable here, but only if the variable name makes it clear what type it represents.\r\n\r\n```suggestion\r\n      var executor = Executors.newFixedThreadPool(16);\r\n```', 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;
+      var secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
+          context.getZooKeepersSessionTimeOut(), secret);
+
+      var execServ = Executors.newFixedThreadPool(16);
+
+      String initialData = DigestUtils.sha1Hex(""Accumulo Zookeeper Mutator test 1/4/21"") + "" 0"";","[{'comment': 'A hard-coded date could of `1/4/21` could generate data that is very confusing when run at some point in the future.', 'commenter': 'ctubbsii'}, {'comment': 'You mean if the test were to ever print/log this data?', 'commenter': 'keith-turner'}, {'comment': ""Or if it's stored in ZK and manually inspected while troubleshooting... or even just editing the code later, it could create confusion and raise questions over whether that date is important to preserve or not. It doesn't matter, though... I see you've already changed it."", 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;
+      var secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
+          context.getZooKeepersSessionTimeOut(), secret);
+
+      var execServ = Executors.newFixedThreadPool(16);
+
+      String initialData = DigestUtils.sha1Hex(""Accumulo Zookeeper Mutator test 1/4/21"") + "" 0"";
+
+      // This map is used to ensure multiple threads do not successfully write the same value and no
+      // values are skipped. The hash in the value also verifies similar things in a different way.
+      ConcurrentHashMap<Integer,Integer> countCounts = new ConcurrentHashMap<>();
+
+      for (int i = 0; i < 16; i++) {
+        execServ.execute(() -> {
+          try {
+
+            int count = 0;
+            while (count < 200) {
+              byte[] val =
+                  zk.mutateOrCreate(""/test-zm"", initialData.getBytes(UTF_8), this::nextValue);
+              var nextCount = getCount(val);","[{'comment': '```suggestion\r\n              int nextCount = getCount(val);\r\n```', 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;
+      var secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
+          context.getZooKeepersSessionTimeOut(), secret);
+
+      var execServ = Executors.newFixedThreadPool(16);
+
+      String initialData = DigestUtils.sha1Hex(""Accumulo Zookeeper Mutator test 1/4/21"") + "" 0"";
+
+      // This map is used to ensure multiple threads do not successfully write the same value and no
+      // values are skipped. The hash in the value also verifies similar things in a different way.
+      ConcurrentHashMap<Integer,Integer> countCounts = new ConcurrentHashMap<>();
+
+      for (int i = 0; i < 16; i++) {
+        execServ.execute(() -> {
+          try {
+
+            int count = 0;
+            while (count < 200) {
+              byte[] val =
+                  zk.mutateOrCreate(""/test-zm"", initialData.getBytes(UTF_8), this::nextValue);
+              var nextCount = getCount(val);
+              assertTrue(nextCount > count);","[{'comment': 'This assertion can fail in threads, but since the executor has no uncaught exception handler, they just cause the threads to die, but the test passes anyway (with a messy stack trace printed to STDERR).', 'commenter': 'ctubbsii'}, {'comment': 'When switching to futures, I found this assertion was throwing an exception. Turns out the sanity checks in the test were not properly handling the initial insert into ZK and it was being silently ignored.', 'commenter': 'keith-turner'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;
+      var secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
+          context.getZooKeepersSessionTimeOut(), secret);
+
+      var execServ = Executors.newFixedThreadPool(16);
+
+      String initialData = DigestUtils.sha1Hex(""Accumulo Zookeeper Mutator test 1/4/21"") + "" 0"";
+
+      // This map is used to ensure multiple threads do not successfully write the same value and no
+      // values are skipped. The hash in the value also verifies similar things in a different way.
+      ConcurrentHashMap<Integer,Integer> countCounts = new ConcurrentHashMap<>();
+
+      for (int i = 0; i < 16; i++) {
+        execServ.execute(() -> {
+          try {
+
+            int count = 0;
+            while (count < 200) {
+              byte[] val =
+                  zk.mutateOrCreate(""/test-zm"", initialData.getBytes(UTF_8), this::nextValue);
+              var nextCount = getCount(val);
+              assertTrue(nextCount > count);
+              count = nextCount;
+              countCounts.merge(nextCount, 1, Integer::sum);
+            }
+
+          } catch (Exception e) {
+            throw new RuntimeException(e);","[{'comment': ""RTE won't stop the test from passing."", 'commenter': 'ctubbsii'}, {'comment': 'Good catch.  I was planning to use Futures when writing this, but then later did not.  Will change the code to use futures.', 'commenter': 'keith-turner'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;
+      var secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
+          context.getZooKeepersSessionTimeOut(), secret);
+
+      var execServ = Executors.newFixedThreadPool(16);
+
+      String initialData = DigestUtils.sha1Hex(""Accumulo Zookeeper Mutator test 1/4/21"") + "" 0"";
+
+      // This map is used to ensure multiple threads do not successfully write the same value and no
+      // values are skipped. The hash in the value also verifies similar things in a different way.
+      ConcurrentHashMap<Integer,Integer> countCounts = new ConcurrentHashMap<>();
+
+      for (int i = 0; i < 16; i++) {
+        execServ.execute(() -> {
+          try {
+
+            int count = 0;
+            while (count < 200) {
+              byte[] val =
+                  zk.mutateOrCreate(""/test-zm"", initialData.getBytes(UTF_8), this::nextValue);
+              var nextCount = getCount(val);
+              assertTrue(nextCount > count);
+              count = nextCount;
+              countCounts.merge(nextCount, 1, Integer::sum);
+            }
+
+          } catch (Exception e) {
+            throw new RuntimeException(e);
+          }
+        });
+      }
+
+      execServ.shutdown();
+
+      while (!execServ.awaitTermination(1, TimeUnit.SECONDS)) {
+
+      }
+
+      var actual = zk.getData(""/test-zm"");
+      int settledCount = getCount(actual);
+
+      assertTrue(settledCount >= 200);
+
+      String expected = initialData;
+
+      for (int i = 1; i <= settledCount; i++) {
+        assertEquals(1, (int) countCounts.get(i));
+        expected = nextValue(expected);
+      }
+
+      assertEquals(settledCount, countCounts.size());
+      assertEquals(expected, new String(actual, UTF_8));
+    }
+  }
+
+  private String nextValue(String currString) {
+    var tokens = currString.split("" "");
+    var currHash = tokens[0];
+    var count = Integer.parseInt(tokens[1]);","[{'comment': '```suggestion\r\n    String[] tokens = currString.split("" "");\r\n    String currHash = tokens[0];\r\n    int count = Integer.parseInt(tokens[1]);\r\n```', 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.commons.codec.digest.DigestUtils;
+import org.junit.Test;
+
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      @SuppressWarnings(""resource"")
+      var context = (ClientContext) client;
+      var secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
+          context.getZooKeepersSessionTimeOut(), secret);
+
+      var execServ = Executors.newFixedThreadPool(16);
+
+      String initialData = DigestUtils.sha1Hex(""Accumulo Zookeeper Mutator test 1/4/21"") + "" 0"";
+
+      // This map is used to ensure multiple threads do not successfully write the same value and no
+      // values are skipped. The hash in the value also verifies similar things in a different way.
+      ConcurrentHashMap<Integer,Integer> countCounts = new ConcurrentHashMap<>();
+
+      for (int i = 0; i < 16; i++) {
+        execServ.execute(() -> {
+          try {
+
+            int count = 0;
+            while (count < 200) {
+              byte[] val =
+                  zk.mutateOrCreate(""/test-zm"", initialData.getBytes(UTF_8), this::nextValue);
+              var nextCount = getCount(val);
+              assertTrue(nextCount > count);
+              count = nextCount;
+              countCounts.merge(nextCount, 1, Integer::sum);
+            }
+
+          } catch (Exception e) {
+            throw new RuntimeException(e);
+          }
+        });
+      }
+
+      execServ.shutdown();
+
+      while (!execServ.awaitTermination(1, TimeUnit.SECONDS)) {
+
+      }
+
+      var actual = zk.getData(""/test-zm"");","[{'comment': '```suggestion\r\n      byte[] actual = zk.getData(""/test-zm"");\r\n```', 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.accumulo.test.categories.MiniClusterOnlyTests;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import com.google.common.io.BaseEncoding;
+
+@Category(MiniClusterOnlyTests.class)
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * A simple stress test that looks for race conditions in
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   */
+  @Test
+  public void concurrentMutatorTest() throws Exception {
+    try (var client = Accumulo.newClient().from(getClientProps()).build();
+        var context = (ClientContext) client) {
+      String secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
+          context.getZooKeepersSessionTimeOut(), secret);
+
+      var executor = Executors.newFixedThreadPool(16);
+
+      String initialData = hash(""Accumulo Zookeeper Mutator test data"") + "" 0"";
+
+      List<Future<?>> futures = new ArrayList<>();
+
+      // This map is used to ensure multiple threads do not successfully write the same value and no
+      // values are skipped. The hash in the value also verifies similar things in a different way.
+      ConcurrentHashMap<Integer,Integer> countCounts = new ConcurrentHashMap<>();
+
+      for (int i = 0; i < 16; i++) {
+        futures.add(executor.submit(() -> {
+          try {
+
+            int count = -1;
+            while (count < 200) {
+              byte[] val =
+                  zk.mutateOrCreate(""/test-zm"", initialData.getBytes(UTF_8), this::nextValue);
+              int nextCount = getCount(val);
+              assertTrue(""nextCount <= count "" + nextCount + "" "" + count, nextCount > count);
+              count = nextCount;
+              countCounts.merge(count, 1, Integer::sum);
+            }
+
+          } catch (Exception e) {
+            throw new RuntimeException(e);
+          }
+        }));
+      }
+
+      // wait and check for errors in background threads
+      for (Future<?> future : futures) {
+        future.get();
+      }","[{'comment': ""You could probably get a one-liner out of this:\r\n\r\n```suggestion\r\n      futures.forEach(f -> f.get());\r\n```\r\n\r\nI'm still uncertain about how the Futures handle RTEs and/or AssertionErrors. Will those be caught and preserved in the Future object for later, when `.get()` is called?"", 'commenter': 'ctubbsii'}, {'comment': 'Any exception occurring in the background thread will be wrapped in a ExecutionException and thrown to the foreground thread when calling get(), so the RTEs and AssertionErrors in the background would show up that way. The one liner probably would not work because get() throws ExecutionException', 'commenter': 'keith-turner'}, {'comment': 'Ah, okay, that makes sense. Thanks for the explanation.', 'commenter': 'ctubbsii'}]"
1851,test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java,"@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.accumulo.test.categories.MiniClusterOnlyTests;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import com.google.common.io.BaseEncoding;
+
+@Category(MiniClusterOnlyTests.class)
+public class ZooMutatorIT extends AccumuloClusterHarness {
+  /**
+   * This test uses multiple threads to update the data in a single zookeeper node using
+   * {@link ZooReaderWriter#mutateOrCreate(String, byte[], org.apache.accumulo.fate.zookeeper.ZooReaderWriter.Mutator)}
+   * and tries to detect errors and race conditions in that code. Each thread uses
+   * {@link #nextValue(String)} to compute a new value for the ZK node based on the current value,
+   * producing a new unique value. The test has sanity checks for the following conditions:
+   *
+   * <UL>
+   * <LI>All updates in the chain of updates were made, none were skipped.
+   * <LI>No update in the chain of updates is made twice. For example if two threads wrote the exact
+   * same value to the node this should be detected by the test.
+   * <LI>The updates in the chain of updates were made in the proper order.
+   * </UL>","[{'comment': 'Thanks for including the description of how the test works. It helps a lot. I\'m still unclear on the term ""chain of updates"", and a few other things though. ""chain of updates"" is mentioned in the ""sanity checks"", but is not explained. It looks like you have 16 threads, each writing 200 times. Is a group of 200 in a single thread considered a ""chain of updates"" or something else? What ensures each write in a chain is uniquely identifiable (within a chain, or across threads)? And how do updates in a ""chain"" relate to one another? Are there certain transitions that are valid to mutate, and others not valid?\r\n\r\nAlso, what does countCounts actually store a mapping of? Both the key and the value are Integer, but it\'s not clear what the key is supposed to represent vs. the value. It\'s also a bit difficult to parse what `countCounts.merge(count, 1, Integer::sum)` is doing. Is this just tracking the frequency of occurrence for whatever the key in the map represents?\r\n\r\nSorry for all the questions. I\'ve been staring at this code for an hour, and while I get the general idea, I also know that in its current state, I don\'t think I\'d be able to fix it if it were broken.\r\n\r\nNit: While HTML5 isn\'t case-sensitive with regard to HTML tags, it\'s [good practice](https://stackoverflow.com/a/19808671) to use lowercase for tag and attribute names. It\'s also more consistent with (most of) the rest of our code.', 'commenter': 'ctubbsii'}, {'comment': 'I updated the test description and I dropped the term ""chain of updates"".  Let me know if the updates are helpful.  \r\n\r\n> It\'s also a bit difficult to parse what countCounts.merge(count, 1, Integer::sum) is doing.\r\n\r\nThe goal of this is to detect duplicate or missing updates to zookeeper.  Each update increments a counter in the data, all the counter values updated are recorded in the map and each counter value should be seen once.  The chain of hashes could detect missing and out of order updates, but not duplicate updates.  That is why the stronger check was added for duplicate updates using the map.  There is weak ordering check on the counts using an assertion, I think the chain of hashes provided a much stronger ordering check.\r\n\r\n', 'commenter': 'keith-turner'}, {'comment': 'Thanks. I think the description you added in the above comment helps bring it all together for me. The changes look good to me.', 'commenter': 'ctubbsii'}, {'comment': 'I added one more paragraph to the docs pulling some information from this discussion.  Will merge after the builds complete.', 'commenter': 'keith-turner'}]"
1871,server/manager/src/main/java/org/apache/accumulo/master/ManagerExecutable.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.master;
+
+import org.apache.accumulo.start.spi.KeywordExecutable;
+
+import com.google.auto.service.AutoService;
+
+@AutoService(KeywordExecutable.class)
+public class ManagerExecutable implements KeywordExecutable {","[{'comment': 'Should add this to `KeywordStartIT`, which I think is the IT to test all the keywords.', 'commenter': 'milleruntime'}, {'comment': ""I would expect that `KeywordStartIT.testExpectedClasses` should be failing with this change, until it is added. I believe that's the only test that should be updated."", 'commenter': 'ctubbsii'}]"
1871,assemble/bin/accumulo-cluster,"@@ -340,10 +340,16 @@ function main() {
   accumulo_cmd=""${bin}/accumulo""
   SSH='ssh -qnf -o ConnectTimeout=2'
 
+  manager_file=""managers""
+  if [ ! -f ""$conf/$manager_file"" -a -f ""$conf/masters"" ]; then","[{'comment': 'Prefer bash double square brackets for consistency and to avoid use (or apparent use) of external process `/usr/bin/\\[`.\r\n\r\n```suggestion\r\n  if [[ ! -f ""$conf/$manager_file"" -a -f ""$conf/masters"" ]]; then\r\n```', 'commenter': 'ctubbsii'}]"
1871,assemble/bin/accumulo-cluster,"@@ -340,10 +340,16 @@ function main() {
   accumulo_cmd=""${bin}/accumulo""
   SSH='ssh -qnf -o ConnectTimeout=2'
 
+  manager_file=""managers""
+  if [ ! -f ""$conf/$manager_file"" -a -f ""$conf/masters"" ]; then
+    echo ""WARN : Use of masters files is deprecated.  Use managers files instead""","[{'comment': '```suggestion\r\n    echo ""WARN : Use of \'masters\' file is deprecated; use \'managers\' file instead.""\r\n```', 'commenter': 'ctubbsii'}]"
1871,assemble/bin/accumulo-service,"@@ -25,7 +25,8 @@ Usage: accumulo-service <service> <command>
 Services:
   gc          Accumulo garbage collector
   monitor     Accumulo monitor
-  master      Accumulo master
+  manager     Accumulo manager
+  master      Accumulo master (Deprecated)","[{'comment': ""This isn't related to your change, but: should this list be organized alphabetically? It's roughly, but not quite, alphabetical."", 'commenter': 'ctubbsii'}]"
1871,assemble/bin/accumulo-service,"@@ -137,9 +138,15 @@ function main() {
     host=$(ip addr | grep 'state UP' -A2 | tail -n1 | awk '{print $2}' | cut -f1  -d'/')
   fi 
   service=""$1""
+
+  if [[ $service == ""master"" ]]; then
+    echo ""WARN : Use of master service name is deprecated, use manager instead.""","[{'comment': '```suggestion\r\n    echo ""WARN : Use of \'master\' service name is deprecated; use \'manager\' instead.""\r\n```', 'commenter': 'ctubbsii'}]"
1873,core/src/test/java/org/apache/accumulo/core/conf/DeprecatedPropertyUtilTest.java,"@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.conf;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertThrows;
+
+import org.apache.commons.configuration2.CompositeConfiguration;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class DeprecatedPropertyUtilTest {","[{'comment': 'Seems like it would be useful to also have some test in this class that rename master props to manager props.', 'commenter': 'keith-turner'}, {'comment': ""I was torn about that since the master to manager prop renaming is deprecated, and would therefore eventually need to be removed from this test. I was thinking what I added was sufficient to test the general functionality of the class, but am happy to add master to manager rename checks as well if you think it's worth it even though they'll be removed eventually."", 'commenter': 'brianloss'}, {'comment': '> I was torn about that since the master to manager prop renaming is deprecated, and would therefore eventually need to be removed from this test.\r\n\r\nThat makes sense.  It would be nice to test the code that users will run.  When its removed in the future, if the test fails cleanly in such a way that the cause is obvious, I think it will be easy for someone to clean up the test.  ', 'commenter': 'keith-turner'}, {'comment': 'Ok, added.', 'commenter': 'brianloss'}]"
1873,core/src/test/java/org/apache/accumulo/core/conf/DeprecatedPropertyUtilTest.java,"@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.conf;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertThrows;
+
+import org.apache.commons.configuration2.CompositeConfiguration;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class DeprecatedPropertyUtilTest {
+  private static class TestPropertyUtil extends DeprecatedPropertyUtil {
+    public static void registerTestRenamer() {
+      renamers.add(new PropertyRenamer() {
+        private static final String OLD_PREFIX = ""old."";
+
+        @Override
+        public boolean matches(String property) {
+          return property.startsWith(OLD_PREFIX);
+        }
+
+        @Override
+        public String rename(String property) {
+          return ""new."" + property.substring(OLD_PREFIX.length());
+        }
+      });
+    }
+  }
+
+  @BeforeClass
+  public static void setup() {
+    TestPropertyUtil.registerTestRenamer();
+  }
+
+  @Test
+  public void testNonDeprecatedPropertyRename() {
+    String oldProp = ""some_property_name"";
+    String newProp = DeprecatedPropertyUtil.renameDeprecatedProperty(oldProp);
+    assertEquals(oldProp, newProp);
+    assertSame(oldProp, newProp);
+  }
+
+  @Test
+  public void testDeprecatedPropertyRename() {
+    String newProp = DeprecatedPropertyUtil.renameDeprecatedProperty(""old.test"", false);
+    assertEquals(""new.test"", newProp);
+  }
+
+  @Test
+  public void testSanityCheckWithOldProp() {
+    CompositeConfiguration config = new CompositeConfiguration();
+    config.setProperty(""old.prop"", ""3"");
+    DeprecatedPropertyUtil.sanityCheck(config);
+  }
+
+  @Test
+  public void testSanityCheckWithNewProp() {
+    CompositeConfiguration config = new CompositeConfiguration();
+    config.setProperty(""new.prop"", ""4"");
+    DeprecatedPropertyUtil.sanityCheck(config);
+  }
+","[{'comment': 'Could also have a sanity check test w/ new and old prefix that do not resolve to same, like ""old.prop1"" and ""new.prop2"".', 'commenter': 'keith-turner'}, {'comment': 'Added in next commit.', 'commenter': 'brianloss'}]"
1873,core/src/test/java/org/apache/accumulo/core/conf/DeprecatedPropertyUtilTest.java,"@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.conf;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertThrows;
+
+import org.apache.commons.configuration2.CompositeConfiguration;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class DeprecatedPropertyUtilTest {
+  private static class TestPropertyUtil extends DeprecatedPropertyUtil {
+    public static void registerTestRenamer() {
+      renamers.add(new PropertyRenamer() {
+        private static final String OLD_PREFIX = ""old."";
+
+        @Override
+        public boolean matches(String property) {
+          return property.startsWith(OLD_PREFIX);
+        }
+
+        @Override
+        public String rename(String property) {
+          return ""new."" + property.substring(OLD_PREFIX.length());
+        }
+      });
+    }
+  }
+
+  @BeforeClass
+  public static void setup() {
+    TestPropertyUtil.registerTestRenamer();
+  }
+
+  @Test
+  public void testNonDeprecatedPropertyRename() {
+    String oldProp = ""some_property_name"";
+    String newProp = DeprecatedPropertyUtil.renameDeprecatedProperty(oldProp);
+    assertEquals(oldProp, newProp);
+    assertSame(oldProp, newProp);
+  }
+
+  @Test
+  public void testDeprecatedPropertyRename() {
+    String newProp = DeprecatedPropertyUtil.renameDeprecatedProperty(""old.test"", false);
+    assertEquals(""new.test"", newProp);
+  }
+
+  @Test
+  public void testSanityCheckWithOldProp() {
+    CompositeConfiguration config = new CompositeConfiguration();
+    config.setProperty(""old.prop"", ""3"");
+    DeprecatedPropertyUtil.sanityCheck(config);
+  }
+
+  @Test
+  public void testSanityCheckWithNewProp() {
+    CompositeConfiguration config = new CompositeConfiguration();
+    config.setProperty(""new.prop"", ""4"");
+    DeprecatedPropertyUtil.sanityCheck(config);
+  }
+
+  @Test
+  public void testSanityCheckFailure() {
+    CompositeConfiguration config = new CompositeConfiguration();
+    config.setProperty(""old.prop"", ""3"");
+    config.setProperty(""new.prop"", ""4"");
+    IllegalStateException e =
+        assertThrows(IllegalStateException.class, () -> DeprecatedPropertyUtil.sanityCheck(config));
+    assertEquals(""new.prop and deprecated old.prop cannot both be site in the configuration."",","[{'comment': '`cannot both be site in the` does not seem like what you intended to write.', 'commenter': 'keith-turner'}, {'comment': 'Oops, good catch. That was supposed to be set not site. Fixed in the next commit.', 'commenter': 'brianloss'}]"
1873,server/manager/src/main/java/org/apache/accumulo/master/upgrade/Upgrader9to10.java,"@@ -201,6 +204,39 @@ private void upgradeRootTabletMetadata(ServerContext ctx) {
     delete(ctx, ZROOT_TABLET_PATH);
   }
 
+  @SuppressWarnings(""removal"")
+  @Deprecated(since = ""2.1.0"", forRemoval = true)
+  private void renameMasterProps(ServerContext ctx) {
+    try {
+      // Figure out which props with the ""master."" prefix are set only in Zookeeper. If
+      // a property exists in the config and is either not set in the site config, or has
+      // a different value from the site config, then the property is set in zookeeper
+      // and we want to upgrade.
+      HashMap<String,String> configProps = new HashMap<>();
+      HashMap<String,String> siteProps = new HashMap<>();
+      ctx.getConfiguration().getProperties(configProps,
+          p -> p.startsWith(DeprecatedPropertyUtil.MasterPropertyRenamer.MASTER_PREFIX));
+      ctx.getSiteConfiguration().getProperties(siteProps,
+          p -> p.startsWith(DeprecatedPropertyUtil.MasterPropertyRenamer.MASTER_PREFIX));
+      configProps.entrySet().removeIf(
+          e -> siteProps.containsKey(e.getKey()) && e.getValue().equals(siteProps.get(e.getKey())));
+
+      for (Entry<String,String> entry : configProps.entrySet()) {
+        // Set the property under the new name
+        SystemPropUtil.setSystemProperty(ctx,
+            DeprecatedPropertyUtil.renameDeprecatedProperty(entry.getKey(), false),","[{'comment': 'Seems `DeprecatedPropertyUtil.renameDeprecatedProperty` is called explicitly here to avoid logging.   That is nice because the log message does not fit this situation. Made me think a log message geared towards upgrade may be useful.  Like `During upgrade property X was renamed to Y`', 'commenter': 'keith-turner'}, {'comment': ""Sure, good idea. I'll add in the next commit."", 'commenter': 'brianloss'}]"
1873,server/manager/src/main/java/org/apache/accumulo/master/upgrade/Upgrader9to10.java,"@@ -201,6 +204,39 @@ private void upgradeRootTabletMetadata(ServerContext ctx) {
     delete(ctx, ZROOT_TABLET_PATH);
   }
 
+  @SuppressWarnings(""removal"")
+  @Deprecated(since = ""2.1.0"", forRemoval = true)
+  private void renameMasterProps(ServerContext ctx) {
+    try {
+      // Figure out which props with the ""master."" prefix are set only in Zookeeper. If
+      // a property exists in the config and is either not set in the site config, or has
+      // a different value from the site config, then the property is set in zookeeper
+      // and we want to upgrade.
+      HashMap<String,String> configProps = new HashMap<>();
+      HashMap<String,String> siteProps = new HashMap<>();
+      ctx.getConfiguration().getProperties(configProps,
+          p -> p.startsWith(DeprecatedPropertyUtil.MasterPropertyRenamer.MASTER_PREFIX));
+      ctx.getSiteConfiguration().getProperties(siteProps,
+          p -> p.startsWith(DeprecatedPropertyUtil.MasterPropertyRenamer.MASTER_PREFIX));
+      configProps.entrySet().removeIf(
+          e -> siteProps.containsKey(e.getKey()) && e.getValue().equals(siteProps.get(e.getKey())));","[{'comment': 'Seems like if `master.prop1=v1` is set in site and ZK, then it will not be renamed in ZK?  If so, why not rename it in ZK?', 'commenter': 'keith-turner'}, {'comment': ""I was trying to list only properties that are set in ZK. I couldn't find a better way to determine that, and modeled this after code in [ConfigCommand](https://github.com/apache/accumulo/blob/main/shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java).\r\nIn the case you mention, the ZK property is superfluous and really should be removed rather than renamed if it matches what's in site already, but I'm not doing that either.\r\nIs there another utility for retrieving ZK properties while ignoring the contents of the site file, command-line and env properties? Maybe I need to write something if not."", 'commenter': 'brianloss'}, {'comment': 'I missed that configProps was a union of the site config and ZK config, I see what you are trying to do now.\r\n\r\n> Is there another utility for retrieving ZK properties while ignoring the contents of the site file\r\n\r\nLooking around in the code, one way might be to create a ZooConfiguration object directly with an empty parent. ', 'commenter': 'keith-turner'}, {'comment': 'I tried that, and it looks a little cleaner (and catches the edge case). In the next commit...', 'commenter': 'brianloss'}]"
1873,server/base/src/main/java/org/apache/accumulo/server/util/SystemPropUtil.java,"@@ -70,6 +81,20 @@ public static void setSystemProperty(ServerContext context, String property, Str
 
   public static void removeSystemProperty(ServerContext context, String property)
       throws InterruptedException, KeeperException {
+    AtomicBoolean shouldRemove = new AtomicBoolean(true);
+    DeprecatedPropertyUtil.getReplacementName(property, (log, replacement) -> {
+      log.warn(""{} was deprecated and will be removed in a future release;""
+          + "" no action was taken because it is not set here;""
+          + "" did you mean to remove its replacment {} instead?"", property, replacement);
+      shouldRemove.set(false);
+    });
+    if (shouldRemove.get()) {
+      removePropWithoutDeprecationWarning(context, property);
+    }","[{'comment': ""@keith-turner I'm wondering if you have any opinion on this change in particular. This change means a user would not be able to use the old property name when removing a property from zookeeper--they could only do that using the new property name. You had mentioned some concern about automation updating zookeeper properties, though setting a property using the old name is supported. Do you have any concerns here? The thinking is for a property removal it is more dangerous to assume the user meant to remove the new property name and used the old name."", 'commenter': 'brianloss'}, {'comment': 'What is the rationale for this asymmetrical behavior?  Set and delete both mutate a property.  One could think of delete as special case of set that sets the value to NULL. ', 'commenter': 'keith-turner'}, {'comment': ""There's a discussion @ctubbsii and I had [here](https://github.com/brianloss/accumulo/pull/2#discussion_r562689513)."", 'commenter': 'brianloss'}, {'comment': '@keith-turner The rationale is that when setting a deprecated property, the intent is clear that the user is trying to change the behavior of Accumulo by setting the property to some specific value. We can handle that intent by translating to the new name when we store it. However, when a user is removing a property, the intent is much less clear, and could be *very* unintuitive. It would be very strange, for example, to ask to remove ""X"" (which does not exist), and then to have ""Y"" deleted instead. Rather than rely on this unintuitive behavior, I think it would be better to just do the safest thing and make no changes, relying instead on the very verbose warning, so that the user can be more explicit about what they intended to do.\r\n\r\nFurthermore, removing a property already had the asymmetrical behavior of not triggering any warning or error when removing a property that isn\'t a valid zoo property, while setting the same property would have triggered an error. The behavior in this PR is an extension of that existing NOOP behavior when an invalid property is requested to be removed. This preserves the previous behavior of the remove method, only adding a new warning.\r\n\r\nHonestly, I preferred the solution where we just prevent the API from being able to set the old `master.*` property names entirely. That way, it would just get an error, and we wouldn\'t have to try to discern and accommodate user intent. Yes, in the edge case where there might be some automation setting/removing properties, that automation may fail as a result of no longer being able to set/remove these properties, since they aren\'t recognized. But, I think that would be far more preferable than trying to guess user intent and try to communicate with users via warnings, introducing the possibility for more mistakes (mistakes due to not expecting the automatic translation, or mistakes due to expecting an automatic translation that doesn\'t occur). At least with an error, the situation is clearly communicated to the caller.', 'commenter': 'ctubbsii'}, {'comment': 'I think a nice transitional experience is that each manager property has a deprecated alias master property that can be used interchangeably(with a warning).  This is being achieved for set, but not delete and I think the semantics are very confusing. \r\n\r\n> Honestly, I preferred the solution where we just prevent the API from being able to set the old master.* property names entirely. \r\n\r\nWhen I put myself in the shoes of a user, that is not the experience I would want.  An experience where code that worked in 2.0 (code written following Accumulo documentation) blows up by design in 2.1 (and its only detectable at runtime).  As a user I would prefer the experience where it continues to work in 2.1 (with warnings) and blows up at runtime in 2.2, 3.0, etc.\r\n\r\nIf master aliases are only supported for set and not delete, I think it may be a less confusing user experience to not support aliases for either set or delete and throw errors for both instead.  Errors are not my personal preference, but its consistent at least and not confusing.', 'commenter': 'keith-turner'}, {'comment': '> As a user I would prefer the experience where it continues to work in 2.1 (with warnings) and blows up at runtime in 2.2, 3.0, etc.\r\n\r\n""it continues to work"" is something I would prefer also, but it\'s not that simple, because there isn\'t a single behavior that is expected. There are multiple distinct behaviors, and the phrase ""it continues to work"" is ambiguous about which ones should be preserved and which ones are acceptable to be changed.\r\n\r\nHere are some of the behaviors of 2.0 that people might expect:\r\n\r\nA. Configurability: `{ } -> set X -> { X }`\r\nB. Independence: `{ X } -> set Y -> { X, Y } -> remove X -> { Y }`\r\nC. Reversibility: `{ } -> set X -> { X } -> remove X -> { }`\r\nD. NOOP on removing missing: `{ Y } -> remove X -> { Y }`\r\n\r\nWith the automatic translation of of `X -> Y`, no matter what, some of these expectations are going to be broken with any of the proposed behaviors for 2.1:\r\n\r\n1. `{ } -> set X -> { Y } (with WARN)` - breaks independence (the intent of this PR, so this break is acceptable when limited to the properties being upgraded)\r\n2. `{ Y } -> remove X -> { } (with WARN)` - your proposal; preserves reversibility, but breaks NOOP on removing missing\r\n3. `{ Y } -> remove X -> { Y } (with WARN)` - my proposal; preserves NOOP on removing missing, but breaks reversibility\r\n4. `{ Y } -> remove X -> ERROR` - breaks NOOP on removing missing\r\n5. `{ } -> set X -> ERROR` - breaks configurability of the old properties, but at least could be consistent with option 4 if used in conjunction with that\r\n\r\n> Errors are not my personal preference, but its consistent at least and not confusing.\r\n\r\nIf we accept that there are different user expectations and we can\'t reliably infer user intent, then at least we can be unambiguous and perfectly clear. However, I\'m not certain if it makes more sense to be clear with only ERROR messages, or also with thrown exceptions, because adding new thrown exceptions on the remove case is breaking existing behavior (NOOP on removing missing). For the set case, it wouldn\'t be that unusual, since we already have some properties which cannot be set in ZooKeeper, which generate an exception. These same properties do not generate a corresponding exception when requested to be removed from ZooKeeper, so there\'s already an asymmetry there.\r\n\r\n> If master aliases are only supported for set and not delete ...\r\n\r\nI don\'t think it has ever been a matter of *if* they are supported, but one of *how* (as in, which specific behaviors are preserved, and which are broken).', 'commenter': 'ctubbsii'}, {'comment': 'I\'m in the camp of removing the replacement property if the deprecated property name is sent to remove. I\'ve been looking at this change though we\'re telling users: ""Property X has been replaced by property Y, but in order to be less disruptive we are going to continue (for a short time) to allow you to use X and behind the scenes we\'ll replace it with Y. If you use X, it\'s as though you had used Y instead.""\r\n\r\nGiven that we will have documented that using X is a substitute for using Y, I\'m struggling to come up with a good example where it is confusing or unexpected for the user if they use the deprecated property name with remove and we remove the replacement property (I\'d say it\'s confusing not to do it). The only case I can come with is if the user chooses to immediately start using the new property names and then thinks they need to clean up zookeeper themselves and remove the old properties so they call remove on the old names. That usage is inconsistent with the documentation, and they\'d figure it out quickly with the warning that we\'re removing the replacement property names instead. Are there other ways it could be confusing/unexpected for the user if we remove the replacement property when they attempt to remove the deprecated property?', 'commenter': 'brianloss'}, {'comment': 'I think we can very reliably infer user intent for code like the following written against past versions of Accumulo.\r\n\r\n```java\r\nclient.instanceOperations().removeProperty(""master.status.threadpool.size"");\r\nclient.instanceOperations().setProperty(""master.fate.threadpool.size"",""9"");\r\n```\r\n\r\nAs for judging intent of code that will be written in the future against 2.1, that code  will be written with knowledge of how manager and master properties work in 2.1.  Users can write new code based on this understanding, giving them the opportunity to write code that is correct based on how Accumulo 2.1 actually works. \r\n', 'commenter': 'keith-turner'}, {'comment': ""Okay. That's fine. I'm not entirely convinced, but I've made my case, and I am in the minority. I can submit another update to Brian's code to apply the behavior advocated for by Keith and Brian. I appreciate both of your patience in letting me advocate for my perspective, but I'm content to let it go at this point, and proceed with the majority viewpoint."", 'commenter': 'ctubbsii'}, {'comment': '@keith-turner and @brianloss - I pushed the changes based on this discussion directly to this PR in fa0db45e8c59ec566c5f9f25582bd01ae234f8bc', 'commenter': 'ctubbsii'}]"
1891,core/src/main/java/org/apache/accumulo/core/client/PluginEnvironment.java,"@@ -33,10 +35,7 @@
  */
 public interface PluginEnvironment {
 
-  /**
-   * @since 2.1.0
-   */","[{'comment': ""Should preserve the since tag, since it's an inner-interface, and has its own apidoc page as a separate type."", 'commenter': 'ctubbsii'}, {'comment': 'Thanks, updated for the next commit.', 'commenter': 'brianloss'}]"
1891,core/src/main/java/org/apache/accumulo/core/client/PluginEnvironment.java,"@@ -122,6 +121,16 @@
      */
     @Override
     Iterator<Entry<String,String>> iterator();
+
+    /**
+     * Returns a derived value from this Configuration. The returned value supplier is thread-safe
+     * and attempts to avoid re-computation of the response. The intended use for a derived value is
+     * to ensure that configuration changes that may be made in Zookeeper, for example, are always
+     * reflected in the returned value.
+     *
+     * @since 2.1.0","[{'comment': ""This actually doesn't need the since tag, since the entire class is new in 2.1."", 'commenter': 'ctubbsii'}, {'comment': 'Thanks, updated for the next commit.', 'commenter': 'brianloss'}]"
1891,core/src/main/java/org/apache/accumulo/core/client/PluginEnvironment.java,"@@ -158,6 +167,18 @@
    */
   <T> T instantiate(String className, Class<T> base) throws Exception;
 
+  /**
+   * Loads a class using Accumulo's system classloader.
+   *
+   * @param className
+   *          Fully qualified name of the class.
+   * @param base
+   *          The expected super type of the class.
+   *
+   * @since 2.1.0
+   */
+  <T> Class<? extends T> loadClass(String className, Class<T> base) throws ClassNotFoundException;","[{'comment': ""What's the use case for this being in the public API?\r\nNote: we're transitioning how we do class loading, and phasing out some internal class loading stuffs. We should avoid exposing stuff like this to the public API. The user should be able to just load the class themselves, using their own object's classloader, if they need to."", 'commenter': 'ctubbsii'}, {'comment': 'Fair enough. I really just needed the context-loading variant for TableLoadBalancer. But that really just needs a method to get the context name for the table configuration, so I added that method to BalancerEnvironment and removed the loadClass methods from here.', 'commenter': 'brianloss'}]"
1891,core/src/main/java/org/apache/accumulo/core/manager/balancer/TServerStatusImpl.java,"@@ -0,0 +1,148 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.manager.balancer;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Objects;
+
+import org.apache.accumulo.core.master.thrift.TabletServerStatus;
+import org.apache.accumulo.core.spi.balancer.data.TServerStatus;
+import org.apache.accumulo.core.spi.balancer.data.TableStatistics;
+
+public class TServerStatusImpl implements TServerStatus {
+  private final TabletServerStatus thriftStatus;
+  private Map<String,TableStatistics> tableInfoMap;
+
+  public static TServerStatusImpl fromThrift(TabletServerStatus tss) {
+    return (tss == null) ? null : new TServerStatusImpl(tss);
+  }
+
+  public TServerStatusImpl(TabletServerStatus thriftStatus) {
+    Objects.requireNonNull(thriftStatus);
+
+    this.thriftStatus = thriftStatus;","[{'comment': 'This is cleanest with a static import of `requireNonNull`:\r\n```suggestion\r\n    this.thriftStatus = requireNonNull(thriftStatus);\r\n```\r\nBut, even without a static import, you can still one-line this:\r\n```suggestion\r\n    this.thriftStatus = Objects.requireNonNull(thriftStatus);\r\n```\r\n', 'commenter': 'ctubbsii'}, {'comment': 'Thanks, updated for the next commit.', 'commenter': 'brianloss'}]"
1891,core/src/main/java/org/apache/accumulo/core/spi/balancer/data/TabletMigration.java,"@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.balancer.data;
+
+import static java.util.Objects.requireNonNull;
+
+import java.util.Objects;
+
+import org.apache.accumulo.core.data.TabletId;
+
+/**
+ * @since 2.1.0
+ */
+public class TabletMigration {
+  private final TabletId tabletId;
+  private final TabletServerId oldTabletServer;
+  private final TabletServerId newTabletServer;
+
+  public TabletMigration(TabletId tabletId, TabletServerId oldTabletServer,
+      TabletServerId newTabletServer) {
+    requireNonNull(tabletId);
+    requireNonNull(oldTabletServer);
+    requireNonNull(newTabletServer);
+
+    this.tabletId = tabletId;
+    this.oldTabletServer = oldTabletServer;
+    this.newTabletServer = newTabletServer;","[{'comment': '```suggestion\r\n    this.tabletId = requireNonNull(tabletId);\r\n    this.oldTabletServer = requireNonNull(oldTabletServer);\r\n    this.newTabletServer = requireNonNull(newTabletServer);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Thanks, updated for the next commit.', 'commenter': 'brianloss'}]"
1891,server/base/src/main/java/org/apache/accumulo/server/master/balancer/TabletBalancer.java,"@@ -56,7 +56,11 @@
  * Implementations may wish to store configuration in Accumulo's system configuration using the
  * {@link Property#GENERAL_ARBITRARY_PROP_PREFIX}. They may also benefit from using per-table
  * configuration using {@link Property#TABLE_ARBITRARY_PROP_PREFIX}.
+ *
+ * @deprecated since 2.1.0. Use {@link org.apache.accumulo.core.spi.balancer.TabletBalancer}
+ *             instead.
  */
+@Deprecated(since = ""2.1.0"")
 public abstract class TabletBalancer {","[{'comment': 'Is there any way we can make this implement or extend the SPI version, so we automatically support compatibility with any implementations of the old interface, without two code paths in the manager?', 'commenter': 'ctubbsii'}, {'comment': ""I think that could be done. The main downside I can think of is that, when using the deprecated balancer, the params to getAssignments and balance would get converted to the new types in the master and then converted back to the thrift types in the implementation of the SPI TabletBalancer methods that then delegate to the deprecated TabletBalancer methods. I don't think the balancer methods are called often enough that we really need to worry about the performance hit, though."", 'commenter': 'brianloss'}, {'comment': 'Done in latest commit.', 'commenter': 'brianloss'}, {'comment': '> The main downside I can think of is that, when using the deprecated balancer, the params to getAssignments and balance would get converted to the new types in the master and then converted back to the thrift types\r\n\r\n@brianloss  I was thinking that AssignmentParamsImpl could be created w/ refs to the new and old types and have methods to get the old types.  The could avoid the conversion from new to old.  I am not sure but it seems like the method that creates AssignmentParamsImpl has refs to the old types.', 'commenter': 'keith-turner'}, {'comment': '> @brianloss I was thinking that AssignmentParamsImpl could be created w/ refs to the new and old types and have methods to get the old types. The could avoid the conversion from new to old. I am not sure but it seems like the method that creates AssignmentParamsImpl has refs to the old types.\r\n\r\n@keith-turner I pushed some changes that I think take care of this.', 'commenter': 'brianloss'}, {'comment': '@brianloss  I just took a quick look at, I like how you pulled code out of Master,TabletBalancer and moved it into AssignmentParamsImpl and BalancerParamsImpl.  Making those classes own the responsibility for converting from the SPI types to the internal thrift types is nice.', 'commenter': 'keith-turner'}]"
1891,server/manager/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1686,4 +1741,56 @@ public boolean isActiveService() {
     return masterInitialized.get();
   }
 
+  @SuppressWarnings(""deprecation"")
+  void initializeBalancer() {
+
+    // Try to initialize the defined balancer as the updated TabletBalancer class first. If that
+    // fails with a ClassCastException, it means the property is specified as the deprecated
+    // balancer type, so initialize it instead. If we still end up with no balancer, then use
+    // DefaultLoadBalancer.
+    try {
+      tabletBalancer = Property.createInstanceFromPropertyName(getConfiguration(),
+          Property.TABLE_LOAD_BALANCER, TabletBalancer.class, null);
+    } catch (ClassCastException e) {
+      // ignore -- this means that the deprecated balancer type was used
+      deprecatedTabletBalancer = Property.createInstanceFromPropertyName(getConfiguration(),
+          Property.MANAGER_TABLET_BALANCER,
+          org.apache.accumulo.server.master.balancer.TabletBalancer.class, null);
+    }","[{'comment': ""If the old were made to implement or extend the new SPI, then we wouldn't need to have two code paths here."", 'commenter': 'ctubbsii'}, {'comment': 'I was curious about this and tried experimenting with it locally with the following approach.  Assuming we can make assumptions about the concrete types within Accumulo code and add needed methods to the concrete types.\r\n\r\n```java\r\n@Deprecated(since = ""2.1.0"")\r\npublic abstract class TabletBalancer implements org.apache.accumulo.core.spi.balancer.TabletBalancer {\r\n\r\n  @Override\r\n  public void init(BalancerEnvironment balancerEnvironment) {\r\n    var bei = (BalancerEnvironmentImpl)balancerEnvironment;\r\n    init(bei.getContext());\r\n  }\r\n\r\n  @Override\r\n  public void getAssignments(AssignmentParameters params) {\r\n    var api = (AssignmentParamsImpl)params; \r\n    Map<KeyExtent,TServerInstance> assignments = new HashMap<>();\r\n    getAssignments(api.currentStatusOld(), api.unassignedTabletsOld(), assignments);\r\n    assignments.forEach((ke, tsi) -> {\r\n      params.assignmentsOut().put(new TabletIdImpl(ke), TabletServerIdImpl.fromThrift(tsi));\r\n    });\r\n  }\r\n\r\n  @Override\r\n  public long balance(BalanceParameters params) {\r\n    var bpi = (BalanceParamsImpl) params;\r\n    List<TabletMigration> migrationsOut = new ArrayList<>();\r\n    balance(bpi.currentStatusOld(), bpi.currentMigrationsOld(), migrationsOut);\r\n    migrationsOut.forEach(mo -> {\r\n      params.migrationsOut()\r\n          .add(new org.apache.accumulo.core.spi.balancer.data.TabletMigration(\r\n              new TabletIdImpl(mo.tablet), TabletServerIdImpl.fromThrift(mo.oldServer),\r\n              TabletServerIdImpl.fromThrift(mo.newServer)));\r\n    });\r\n  }\r\n```', 'commenter': 'keith-turner'}, {'comment': ""I was just about to start working on this, but looks like you beat me to it. :) Thanks. I'll add this code change in."", 'commenter': 'brianloss'}, {'comment': 'This is done in the latest commit.', 'commenter': 'brianloss'}]"
1891,server/base/src/main/java/org/apache/accumulo/server/manager/balancer/BalancerEnvironmentImpl.java,"@@ -0,0 +1,132 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.manager.balancer;
+
+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOCATION;
+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;
+
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.clientImpl.Tables;
+import org.apache.accumulo.core.clientImpl.thrift.ThriftSecurityException;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.data.TabletId;
+import org.apache.accumulo.core.dataImpl.TabletIdImpl;
+import org.apache.accumulo.core.manager.balancer.TabletServerIdImpl;
+import org.apache.accumulo.core.manager.balancer.TabletStatisticsImpl;
+import org.apache.accumulo.core.master.state.tables.TableState;
+import org.apache.accumulo.core.metadata.TServerInstance;
+import org.apache.accumulo.core.metadata.schema.TabletsMetadata;
+import org.apache.accumulo.core.rpc.ThriftUtil;
+import org.apache.accumulo.core.spi.balancer.BalancerEnvironment;
+import org.apache.accumulo.core.spi.balancer.data.TabletMigration;
+import org.apache.accumulo.core.spi.balancer.data.TabletServerId;
+import org.apache.accumulo.core.spi.balancer.data.TabletStatistics;
+import org.apache.accumulo.core.tabletserver.thrift.TabletClientService;
+import org.apache.accumulo.core.trace.TraceUtil;
+import org.apache.accumulo.core.util.HostAndPort;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.ServiceEnvironmentImpl;
+import org.apache.thrift.TException;
+import org.apache.thrift.transport.TTransportException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class BalancerEnvironmentImpl extends ServiceEnvironmentImpl implements BalancerEnvironment {
+  private static final Logger log = LoggerFactory.getLogger(BalancerEnvironmentImpl.class);
+
+  public BalancerEnvironmentImpl(ServerContext ctx) {
+    super(ctx);
+  }
+
+  @Override
+  public Map<String,TableId> getTableIdMap() {
+    return Tables.getNameToIdMap(getContext());
+  }
+
+  @Override
+  public boolean isTableOnline(TableId tableId) {
+    return TableState.ONLINE.equals(Tables.getTableState(getContext(), tableId));
+  }
+
+  @Override
+  public Map<TabletId,TabletServerId> listTabletLocations(TableId tableId) {
+    Map<TabletId,TabletServerId> tablets = new LinkedHashMap<>();
+    for (var tm : TabletsMetadata.builder().forTable(tableId).fetch(LOCATION, PREV_ROW)
+        .build(getContext())) {
+      TServerInstance inst = tm.getLocation();
+      tablets.put(new TabletIdImpl(tm.getExtent()), new TabletServerIdImpl(inst.getHost(),
+          inst.getHostAndPort().getPort(), inst.getSession()));
+    }
+    return tablets;
+  }
+
+  @Override
+  public List<TabletStatistics> listOnlineTabletsForTable(TabletServerId tabletServerId,
+      TableId tableId) throws AccumuloException, AccumuloSecurityException {
+    log.debug(""Scanning tablet server {} for table {}"", tabletServerId, tableId);
+    try {
+      TabletClientService.Client client = ThriftUtil.getClient(
+          new TabletClientService.Client.Factory(),
+          HostAndPort.fromParts(tabletServerId.getHost(), tabletServerId.getPort()), getContext());
+      try {
+        return client
+            .getTabletStats(TraceUtil.traceInfo(), getContext().rpcCreds(), tableId.canonical())
+            .stream().map(TabletStatisticsImpl::new).collect(Collectors.toList());
+      } catch (TTransportException e) {
+        log.error(""Unable to connect to {}: "", tabletServerId, e);
+      } finally {
+        ThriftUtil.returnClient(client);
+      }
+    } catch (ThriftSecurityException e) {
+      throw new AccumuloSecurityException(e);
+    } catch (TException e) {
+      throw new AccumuloException(e);
+    }
+    return null;
+  }
+
+  @Override
+  public List<TabletMigration> checkMigrationSanity(Set<TabletServerId> current,","[{'comment': ""Although this method is public in the old Balancer, I don't think it was intended for user use.  I think it was just meant to be an internal sanity check code.  So maybe it does not need to exists in the new SPI interfaces.  It could be placed somewhere outside SPI for internal use."", 'commenter': 'keith-turner'}, {'comment': ""Yeah, that makes sense. I'll look into it."", 'commenter': 'brianloss'}, {'comment': 'This is done in the latest commit.', 'commenter': 'brianloss'}]"
1891,server/manager/src/main/java/org/apache/accumulo/master/Master.java,"@@ -1686,4 +1741,56 @@ public boolean isActiveService() {
     return masterInitialized.get();
   }
 
+  @SuppressWarnings(""deprecation"")
+  void initializeBalancer() {
+
+    // Try to initialize the defined balancer as the updated TabletBalancer class first. If that
+    // fails with a ClassCastException, it means the property is specified as the deprecated
+    // balancer type, so initialize it instead. If we still end up with no balancer, then use
+    // DefaultLoadBalancer.
+    try {
+      tabletBalancer = Property.createInstanceFromPropertyName(getConfiguration(),
+          Property.TABLE_LOAD_BALANCER, TabletBalancer.class, null);
+    } catch (ClassCastException e) {
+      // ignore -- this means that the deprecated balancer type was used","[{'comment': 'It would be nice to log the exception, maybe at debug.  Could help a user figure out a problem.', 'commenter': 'keith-turner'}, {'comment': 'The try/catch will go away when I finish changing the old TabletBalancer to implement the new SPI balancer interface.', 'commenter': 'brianloss'}]"
1891,core/src/main/java/org/apache/accumulo/core/spi/balancer/TabletBalancer.java,"@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.balancer;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TabletId;
+import org.apache.accumulo.core.spi.balancer.data.TServerStatus;
+import org.apache.accumulo.core.spi.balancer.data.TabletMigration;
+import org.apache.accumulo.core.spi.balancer.data.TabletServerId;
+
+/**
+ * This class is responsible for managing the distribution of tablets throughout an Accumulo
+ * cluster. In most cases, users will want a balancer implementation which ensures a uniform
+ * distribution of tablets, so that no individual tablet server is handling significantly more work
+ * than any other.
+ *
+ * <p>
+ * Implementations may wish to store configuration in Accumulo's system configuration using the
+ * {@link Property#GENERAL_ARBITRARY_PROP_PREFIX}. They may also benefit from using per-table
+ * configuration using {@link Property#TABLE_ARBITRARY_PROP_PREFIX}.
+ *
+ * @since 2.1.0
+ */
+public interface TabletBalancer {
+
+  /**
+   * An interface for grouping parameters required for the balancer to assign unassigned tablets.
+   * This interface allows for evolution of the parameter set without changing the balancer's method
+   * signature.
+   *
+   * @since 2.1.0
+   */
+  interface AssignmentParameters {
+    /**
+     * @return the current status for all tablet servers (read-only)
+     */
+    SortedMap<TabletServerId,TServerStatus> currentStatus();
+
+    /**
+     * @return the tablets that need to be assigned, mapped to their previous known location
+     *         (read-only)
+     */
+    Map<TabletId,TabletServerId> unassignedTablets();
+
+    /**
+     * @return a write-only map for storing new assignments
+     */
+    Map<TabletId,TabletServerId> assignmentsOut();
+  }
+
+  /**
+   * An interface for grouping parameters required for the balancer to balance tablets. This
+   * interface allows for evolution of the parameter set without changing the balancer's method
+   * signature.
+   *
+   * @since 2.1.0
+   */
+  interface BalanceParameters {
+    /**
+     * @return the current status for all tablet servers (read-only)
+     */
+    SortedMap<TabletServerId,TServerStatus> currentStatus();
+
+    /**
+     * @return the migrations that are currently in progress (read-only)
+     */
+    Set<TabletId> currentMigrations();
+
+    /**
+     * @return a write-only map for storing new assignments made by the balancer. It is important
+     *         that any tablets found in {@link #currentMigrations()} are not included in the output
+     *         migrations.
+     */
+    List<TabletMigration> migrationsOut();","[{'comment': 'Could possibly do the following instead. Maybe its better to just make the new SPI interfaces follow the patterns of the old ones for this PR and then have a second PR that focuses on improving the new balancer SPI.\r\n\r\n```suggestion\r\n    addMigration(TabletMigration);\r\n```', 'commenter': 'keith-turner'}, {'comment': 'This is done in the latest commit.', 'commenter': 'brianloss'}]"
1891,core/src/main/java/org/apache/accumulo/core/manager/balancer/TabletServerIdImpl.java,"@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.manager.balancer;
+
+import static java.util.Objects.requireNonNull;
+
+import org.apache.accumulo.core.metadata.TServerInstance;
+import org.apache.accumulo.core.spi.balancer.data.TabletServerId;
+import org.apache.accumulo.core.util.HostAndPort;
+
+/**
+ * @since 2.1.0
+ */
+public class TabletServerIdImpl implements TabletServerId {
+  private final TServerInstance tServerInstance;
+
+  public static TabletServerIdImpl fromThrift(TServerInstance tsi) {
+    return (tsi == null) ? null : new TabletServerIdImpl(tsi);
+  }
+
+  public TabletServerIdImpl(String host, int port, String session) {
+    requireNonNull(host);
+    this.tServerInstance = new TServerInstance(HostAndPort.fromParts(host, port), session);
+  }
+
+  public TabletServerIdImpl(TServerInstance tServerInstance) {
+    this.tServerInstance = requireNonNull(tServerInstance);
+  }
+
+  public String getHost() {","[{'comment': '```suggestion\r\n  @Override\r\n  public String getHost() {\r\n```', 'commenter': 'keith-turner'}]"
1891,core/src/main/java/org/apache/accumulo/core/manager/balancer/TabletServerIdImpl.java,"@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.manager.balancer;
+
+import static java.util.Objects.requireNonNull;
+
+import org.apache.accumulo.core.metadata.TServerInstance;
+import org.apache.accumulo.core.spi.balancer.data.TabletServerId;
+import org.apache.accumulo.core.util.HostAndPort;
+
+/**
+ * @since 2.1.0
+ */
+public class TabletServerIdImpl implements TabletServerId {
+  private final TServerInstance tServerInstance;
+
+  public static TabletServerIdImpl fromThrift(TServerInstance tsi) {
+    return (tsi == null) ? null : new TabletServerIdImpl(tsi);
+  }
+
+  public TabletServerIdImpl(String host, int port, String session) {
+    requireNonNull(host);
+    this.tServerInstance = new TServerInstance(HostAndPort.fromParts(host, port), session);
+  }
+
+  public TabletServerIdImpl(TServerInstance tServerInstance) {
+    this.tServerInstance = requireNonNull(tServerInstance);
+  }
+
+  public String getHost() {
+    return tServerInstance.getHostAndPort().getHost();
+  }
+
+  public int getPort() {","[{'comment': '```suggestion\r\n  @Override\r\n  public int getPort() {\r\n```', 'commenter': 'keith-turner'}]"
1891,core/src/main/java/org/apache/accumulo/core/manager/balancer/TabletServerIdImpl.java,"@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.manager.balancer;
+
+import static java.util.Objects.requireNonNull;
+
+import org.apache.accumulo.core.metadata.TServerInstance;
+import org.apache.accumulo.core.spi.balancer.data.TabletServerId;
+import org.apache.accumulo.core.util.HostAndPort;
+
+/**
+ * @since 2.1.0
+ */
+public class TabletServerIdImpl implements TabletServerId {
+  private final TServerInstance tServerInstance;
+
+  public static TabletServerIdImpl fromThrift(TServerInstance tsi) {
+    return (tsi == null) ? null : new TabletServerIdImpl(tsi);
+  }
+
+  public TabletServerIdImpl(String host, int port, String session) {
+    requireNonNull(host);
+    this.tServerInstance = new TServerInstance(HostAndPort.fromParts(host, port), session);
+  }
+
+  public TabletServerIdImpl(TServerInstance tServerInstance) {
+    this.tServerInstance = requireNonNull(tServerInstance);
+  }
+
+  public String getHost() {
+    return tServerInstance.getHostAndPort().getHost();
+  }
+
+  public int getPort() {
+    return tServerInstance.getHostAndPort().getPort();
+  }
+
+  public String getSession() {","[{'comment': '```suggestion\r\n  @Override\r\n  public String getSession() {\r\n```', 'commenter': 'keith-turner'}]"
1899,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1899,7 +1906,8 @@ public void checkIfMinorCompactionNeededForLogs(List<DfsLogger> closedLogs) {
 
     if (reason != null) {
       // initiate and log outside of tablet lock
-      initiateMinorCompaction(MinorCompactionReason.SYSTEM);
+      TableState tableState = context.getTableManager().getTableState(extent.tableId());
+      initiateMinorCompaction(MinorCompactionReason.SYSTEM, tableState);
       log.debug(""Initiating minor compaction for {} because {}"", getExtent(), reason);","[{'comment': ""It could get confusing that this debug statement says it's initiating a flush and occurs immediately after the one inside `initiateMinorCompaction()` that says not to flush."", 'commenter': 'ctubbsii'}, {'comment': ""Good catch. I'll move the debug before the method call."", 'commenter': 'milleruntime'}]"
1899,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java,"@@ -556,14 +557,15 @@ private void manageMemory() {
                 continue;
               }
               Tablet tablet = tabletReport.getTablet();
-              if (!tablet.initiateMinorCompaction(MinorCompactionReason.SYSTEM)) {
-                if (tablet.isClosed()) {
+              var state = context.getTableManager().getTableState(tablet.getExtent().tableId());
+              if (!tablet.initiateMinorCompaction(MinorCompactionReason.SYSTEM, state)) {
+                if (tablet.isClosed() || state.equals(TableState.DELETING)) {","[{'comment': 'If a tablet is in the deleteting state for a long time and holding lots of memory, it seems like it could starve other minor compactions from ever running.  Does this deleting of reports help mitigate that?', 'commenter': 'keith-turner'}, {'comment': ""Yeah. I haven't seen the MinC max threads (tserver.compaction.minor.concurrent.max=4) consumed by 4 deleted tablets after removing the report. But I think it is still possible.\r\n\r\nI am wondering now if I should move the check to inside of `LargestFirstMemoryManager.tabletsToMinorCompact()` so that the deleted tablets will be skipped and not returned in `tabletsToMinorCompact`. This would allow flushes up to `tserver.compaction.minor.concurrent.max` without having to make another pass. Right now the `tabletsToMinorCompact` won't utilize all of the MinC threads if some tablets are being deleted. If all large tablets are being deleted, then no MinC will run during that pass."", 'commenter': 'milleruntime'}, {'comment': 'I added this change in my latest commit dfc579b. Let me know what you think.', 'commenter': 'milleruntime'}, {'comment': 'Enums should always be compared using `==` instead of `.equals()`:\r\n\r\n```suggestion\r\n                if (tablet.isClosed() || state == TableState.DELETING) {\r\n```', 'commenter': 'ctubbsii'}]"
1899,server/tserver/src/main/java/org/apache/accumulo/tserver/memory/LargestFirstMemoryManager.java,"@@ -147,6 +148,11 @@ protected boolean tableExists(TableId tableId) {
     return context.getTableConfiguration(tableId) != null;
   }
 
+  private boolean tableBeingDeleted(TableId tableId) {
+    var state = context.getTableManager().getTableState(tableId);
+    return state.equals(TableState.DELETING);","[{'comment': 'Should use `==` to compare enum.\r\n\r\n```suggestion\r\n    return context.getTableManager().getTableState(tableId) == TableState.DELETING;\r\n```', 'commenter': 'ctubbsii'}]"
1899,server/tserver/src/test/java/org/apache/accumulo/tserver/memory/LargestFirstMemoryManagerTest.java,"@@ -204,22 +212,35 @@ protected long getMinCIdleThreshold(KeyExtent extent) {
     protected boolean tableExists(TableId tableId) {
       return true;
     }
+
+    @Override
+    protected boolean tableBeingDeleted(TableId tableId) {
+      return false;
+    }
   }
 
   private static class LargestFirstMemoryManagerWithExistenceCheck
       extends LargestFirstMemoryManagerUnderTest {
 
     Function<TableId,Boolean> existenceCheck;
+    Function<TableId,Boolean> deletingCheck;
 
-    public LargestFirstMemoryManagerWithExistenceCheck(Function<TableId,Boolean> existenceCheck) {
+    public LargestFirstMemoryManagerWithExistenceCheck(Function<TableId,Boolean> existenceCheck,
+        Function<TableId,Boolean> deletingCheck) {
       super();
       this.existenceCheck = existenceCheck;
+      this.deletingCheck = deletingCheck;","[{'comment': ""It'd be better to use the primitive functional interface that maps to a boolean: `Predicate<TableId>` instead of `Function<TableId,Boolean>`."", 'commenter': 'ctubbsii'}]"
1905,core/src/main/java/org/apache/accumulo/core/spi/fs/VolumeChooserEnvironment.java,"@@ -0,0 +1,29 @@
+package org.apache.accumulo.core.spi.fs;
+
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.spi.common.ServiceEnvironment;
+import org.apache.hadoop.io.Text;
+
+/**
+ * @since 2.1.0
+ */
+public interface VolumeChooserEnvironment {
+  /**
+   * A scope the volume chooser environment; a TABLE scope should be accompanied by a tableId.
+   *
+   * @since 2.1.0
+   */
+  public static enum Scope {
+    DEFAULT, TABLE, INIT, LOGGER
+  }
+
+  public Text getEndRow();
+
+  public boolean hasTableId();
+
+  public TableId getTableId();","[{'comment': 'Should these just be combined into:\r\n\r\n```suggestion\r\n  public Optional<TableId> getTableId();\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'I made that change in 34bd97f.', 'commenter': 'keith-turner'}]"
1905,core/src/main/java/org/apache/accumulo/core/spi/fs/VolumeChooser.java,"@@ -0,0 +1,65 @@
+package org.apache.accumulo.core.spi.fs;
+
+import java.util.Set;
+
+import org.apache.accumulo.core.conf.Property;
+
+/**
+ * Helper used to select from a set of Volume URIs. N.B. implementations must be threadsafe.
+ * VolumeChooser.equals will be used for internal caching.
+ *
+ * <p>
+ * Implementations may wish to store configuration in Accumulo's system configuration using the
+ * {@link Property#GENERAL_ARBITRARY_PROP_PREFIX}. They may also benefit from using per-table
+ * configuration using {@link Property#TABLE_ARBITRARY_PROP_PREFIX}.
+ *
+ * @since 2.1.0
+ */
+public interface VolumeChooser {
+
+  /**
+   * Choose a volume from the provided options.
+   *
+   * @param env
+   *          the server environment provided by the calling framework
+   * @param options
+   *          the list of volumes to choose from
+   * @return one of the options
+   * @throws VolumeChooserException
+   *           if there is an error choosing (this is a RuntimeException); this does not preclude
+   *           other RuntimeExceptions from occurring
+   */
+  String choose(VolumeChooserEnvironment env, Set<String> options) throws VolumeChooserException;
+
+  /**
+   * Return the subset of volumes that could possibly be chosen by this chooser across all
+   * invocations of {@link #choose(VolumeChooserEnvironment, Set)}.
+   *
+   * @param env
+   *          the server environment provided by the calling framework
+   * @param options
+   *          the subset of volumes to choose from
+   * @return array of valid options
+   * @throws VolumeChooserException
+   *           if there is an error choosing (this is a RuntimeException); this does not preclude
+   *           other RuntimeExceptions from occurring
+   *
+   */
+  Set<String> choosable(VolumeChooserEnvironment env, Set<String> options)
+      throws VolumeChooserException;
+
+  class VolumeChooserException extends RuntimeException {","[{'comment': 'Should this exception extend a more narrow type of RuntimeException to give it some category? Would this be an IllegalStateException, for example?', 'commenter': 'ctubbsii'}, {'comment': 'Maybe it would be best to get rid of it. It does not serve any well defined purpose in the code over run time exception.', 'commenter': 'keith-turner'}, {'comment': ':+1: to removing it. Implementers can decide which specific RTE they want to throw.', 'commenter': 'ctubbsii'}, {'comment': 'Its gone', 'commenter': 'keith-turner'}]"
1905,core/src/main/java/org/apache/accumulo/core/spi/fs/VolumeChooser.java,"@@ -0,0 +1,65 @@
+package org.apache.accumulo.core.spi.fs;
+
+import java.util.Set;
+
+import org.apache.accumulo.core.conf.Property;
+
+/**
+ * Helper used to select from a set of Volume URIs. N.B. implementations must be threadsafe.
+ * VolumeChooser.equals will be used for internal caching.
+ *
+ * <p>
+ * Implementations may wish to store configuration in Accumulo's system configuration using the
+ * {@link Property#GENERAL_ARBITRARY_PROP_PREFIX}. They may also benefit from using per-table
+ * configuration using {@link Property#TABLE_ARBITRARY_PROP_PREFIX}.
+ *
+ * @since 2.1.0
+ */
+public interface VolumeChooser {
+
+  /**
+   * Choose a volume from the provided options.
+   *
+   * @param env
+   *          the server environment provided by the calling framework
+   * @param options
+   *          the list of volumes to choose from
+   * @return one of the options
+   * @throws VolumeChooserException
+   *           if there is an error choosing (this is a RuntimeException); this does not preclude
+   *           other RuntimeExceptions from occurring
+   */
+  String choose(VolumeChooserEnvironment env, Set<String> options) throws VolumeChooserException;
+
+  /**
+   * Return the subset of volumes that could possibly be chosen by this chooser across all
+   * invocations of {@link #choose(VolumeChooserEnvironment, Set)}.
+   *
+   * @param env
+   *          the server environment provided by the calling framework
+   * @param options
+   *          the subset of volumes to choose from
+   * @return array of valid options
+   * @throws VolumeChooserException
+   *           if there is an error choosing (this is a RuntimeException); this does not preclude
+   *           other RuntimeExceptions from occurring
+   *
+   */
+  Set<String> choosable(VolumeChooserEnvironment env, Set<String> options)
+      throws VolumeChooserException;","[{'comment': 'This javadoc adequately explains what it does, but not what its purpose is. In the SPI, I think it might be important to add a note about why this method exists in its javadoc (a hint about when it might be called).', 'commenter': 'ctubbsii'}, {'comment': 'It is a good time to improve it, I can take a pass at it but I will need to do some research to figure out the specifics.  It was added in #1413 I think.', 'commenter': 'keith-turner'}, {'comment': ""That's what I was thinking. I didn't know the specifics."", 'commenter': 'ctubbsii'}, {'comment': 'There is only one use of this functionality in the code.  I updated the javadocs in 8e9c7e0e6526cef89dd9161360de118cb702fbf7', 'commenter': 'keith-turner'}]"
1910,assemble/conf/create-jshell.sh,"@@ -0,0 +1,134 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addLicenseHeader(){
+  export header=""$1""
+  export jPath=""$2""
+  
+  # Does an auto-generated JShell config file exists?
+  if [[ -e ""$jPath"" ]]; then
+    printf ""%s\n"" ""/*"" >> ""$jPath""
+    
+    # Load in license header
+    while IFS= read -r line; do
+      echo ""$line"" >> ""$jPath""
+    done < ""$header""
+    
+    printf ""%s\n"" ""*/"" >> ""$jPath"" 
+    echo "" "" >> ""$jPath""
+  else
+    echo ""Cannot add license header in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi","[{'comment': ""The generated file doesn't hold any meaningful/copyrightable content. It's just generated import statements, so it's not strictly necessary to add the license header. The rat check error from the missing license header can be avoided by generating the file in the build tree, rather than the source tree. For maven, that means generating it in `assemble/target/` directory rather than `assemble/conf/`\r\n\r\nThat should simplify things a bit."", 'commenter': 'ctubbsii'}, {'comment': ""@ctubbsii Thank you for reviewing my code submission. I'll make the proposed updates and test out the code before pushing a revised copy later."", 'commenter': 'slackwinner'}]"
1910,assemble/pom.xml,"@@ -520,6 +520,16 @@
               </arguments>
             </configuration>
           </execution>
+          <execution>
+            <id>create-jshell</id>
+            <goals>
+              <goal>exec</goal>
+            </goals>
+            <phase>compile</phase>
+            <configuration>
+              <executable>${basedir}/conf/create-jshell.sh</executable>","[{'comment': 'Since this file should not be included as part of the distribution tarball, but is instead part of the build tools, this `create-jshell.sh` file should live in `assemble/src/main/scripts/`\r\n\r\n```suggestion\r\n              <executable>src/main/scripts/create-jshell.sh</executable>\r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/bin/accumulo,"@@ -58,7 +58,30 @@ function main() {
     echo ""$CLASSPATH""
     exit 0
   fi
+  
+  # Set up path variable for default import config file
+  export jShellPath=""$conf/jshell-init.jsh""
+ 
+  # Verify what file(s) is loaded into JShell
+  if [[ $cmd == ""jshell"" ]] && [[ ""$#"" -eq 1 ]]; then
+    # Load in default Accumulo APIs into JShell
+    CLASSPATH=""$CLASSPATH"" ""$@"" ""$jShellPath"" 
+    exit 0
 
+  elif [[ $cmd == ""jshell"" ]] && [[ ""$#"" -ne 1 ]]; then
+    cmdArr=( ""$@"" )
+    # Traverse through user bash command and verify -startup argument exists
+    for arg in ""${!cmdArr[@]}""; do
+       if [[ ""${cmdArr[arg]}"" =~ ""-startup"" ]]; then
+         # Insert ""DEFAULT"" JShell script after -startup argument
+         cmdArr=( ""${cmdArr[@]:0:arg}"" ""DEFAULT"" ""${cmdArr[@]:arg+1}"" )
+       fi
+    done
+    # Load in default java libraries first followed by custom file(s)
+    CLASSPATH=""$CLASSPATH"" ""${cmdArr[@]}""
+    exit 0
+  fi","[{'comment': 'CLASSPATH is probably already exported, so you might not need that part. If you do, then `export CLASSPATH` on the preceding line before the exec might be more clear.\r\n\r\nAlso, you can probably inject the startup file (if it exists) before the user\'s own arguments, as in:\r\n\r\n```suggestion\r\n  jShellPath=""$conf/jshell-init.jsh""\r\n  if [[ $cmd == ""jshell"" ]]; then\r\n    shift\r\n    if [[ -f ""$jShellPath"" ]]; then\r\n      exec ""$cmd"" --startup ""$jShellPath"" ""$@""\r\n    else\r\n      exec ""$cmd"" ""$@""\r\n    fi\r\n  fi\r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/conf/create-jshell.sh,"@@ -0,0 +1,134 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addLicenseHeader(){
+  export header=""$1""
+  export jPath=""$2""
+  
+  # Does an auto-generated JShell config file exists?
+  if [[ -e ""$jPath"" ]]; then
+    printf ""%s\n"" ""/*"" >> ""$jPath""
+    
+    # Load in license header
+    while IFS= read -r line; do
+      echo ""$line"" >> ""$jPath""
+    done < ""$header""
+    
+    printf ""%s\n"" ""*/"" >> ""$jPath"" 
+    echo "" "" >> ""$jPath""
+  else
+    echo ""Cannot add license header in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+}
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""
+    
+  # Does an auto-generated JShell config file exists?
+  if [[ ! -e ""$jPath"" ]]; then
+    echo ""Cannot add APIs in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+  
+  # Is the source directory valid?
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    rm ""$jPath""
+    exit 1
+  fi
+  
+  # Does a valid JShell path and source directory exists?
+  if [[ -e ""$jPath"" ]] && [[ -d ""$srcDir"" ]]; then
+    # Add API category designator in jshell-init.jsh 
+    case ""$srcDir"" in
+        *""$client""*) echo ""// Accumulo Client API"" >> ""$jPath"";;
+        *""$data""*) echo ""// Accumulo Data API"" >> ""$jPath"";;
+        *""$security""*) echo ""// Accumulo Security API"" >> ""$jPath"";;
+        *""$mini""*) echo ""// Accumulo Minicluster API"" >> ""$jPath"";;
+        *""$hadoop""*) echo ""// Accumulo Hadoop API"" >> ""$jPath"";;
+        *) echo ""// Other API"" >> ""$jPath"";;
+    esac
+   
+    # Extract API info from provided source directory
+    mapfile -t api < <(find ""$srcDir"" -type f -name '*.java'| 
+                       xargs -n1 dirname| sort -u)
+   
+    # Load in API and format source directory into Java import statements
+    for apiPath in ""${api[@]}""; do
+       printf ""%s\n"" ""import ${apiPath##*/java/}.*"" >> ""$jPath""
+    done
+    sed -i '/^ *import / s#/#.#g' ""$jPath""
+    echo "" "" >> ""$jPath""
+  fi
+}
+
+function main(){
+  # Establish Accumulo's main base directory
+  SOURCE=""${BASH_SOURCE[0]}""
+  while [[ -h ""${SOURCE}"" ]]; do
+    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+    SOURCE=""$(readlink ""${SOURCE}"")""
+    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""
+  done
+    
+  # Establish file and folder paths for JShell config
+  export conf=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""; 
+  export jPath=""$conf/jshell-init.jsh""","[{'comment': 'You should generate this in the `assemble/target` directory during the build, and then update the `assemble/src/main/assemblies/component.xml` file to ensure it is copied to the appropriate `conf/` directory by the `maven-assembly-plugin` for the distribution tarball.', 'commenter': 'ctubbsii'}]"
1910,assemble/conf/create-jshell.sh,"@@ -0,0 +1,134 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addLicenseHeader(){
+  export header=""$1""
+  export jPath=""$2""
+  
+  # Does an auto-generated JShell config file exists?
+  if [[ -e ""$jPath"" ]]; then
+    printf ""%s\n"" ""/*"" >> ""$jPath""
+    
+    # Load in license header
+    while IFS= read -r line; do
+      echo ""$line"" >> ""$jPath""
+    done < ""$header""
+    
+    printf ""%s\n"" ""*/"" >> ""$jPath"" 
+    echo "" "" >> ""$jPath""
+  else
+    echo ""Cannot add license header in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+}
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""
+    
+  # Does an auto-generated JShell config file exists?
+  if [[ ! -e ""$jPath"" ]]; then
+    echo ""Cannot add APIs in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+  
+  # Is the source directory valid?
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    rm ""$jPath""
+    exit 1
+  fi
+  
+  # Does a valid JShell path and source directory exists?
+  if [[ -e ""$jPath"" ]] && [[ -d ""$srcDir"" ]]; then
+    # Add API category designator in jshell-init.jsh 
+    case ""$srcDir"" in
+        *""$client""*) echo ""// Accumulo Client API"" >> ""$jPath"";;
+        *""$data""*) echo ""// Accumulo Data API"" >> ""$jPath"";;
+        *""$security""*) echo ""// Accumulo Security API"" >> ""$jPath"";;
+        *""$mini""*) echo ""// Accumulo Minicluster API"" >> ""$jPath"";;
+        *""$hadoop""*) echo ""// Accumulo Hadoop API"" >> ""$jPath"";;","[{'comment': 'I like how you added section headers to the generated file!', 'commenter': 'ctubbsii'}]"
1910,assemble/bin/accumulo,"@@ -58,6 +58,18 @@ function main() {
     echo ""$CLASSPATH""
     exit 0
   fi
+  
+  # Set up path variable for default import config file
+  export jShellPath=""$conf/jshell-init.jsh""
+  
+  if [[ $cmd == ""jshell"" ]]; then
+    shift
+    if [[ -f ""$jShellPath"" ]]; then
+      exec ""$cmd"" --startup DEFAULT ""$jShellPath"" ""$@""","[{'comment': 'I think `--startup DEFAULT` is a redundant option, since it is the default behavior.\r\n\r\nI was thinking `--startup ""$jShellPath""` could be used instead of `--startup DEFAULT`, as its replacement. So, the choice we have is to use the default startup or to replace it.\r\n\r\nThis really comes down to [the difference between an \'option\' and a \'parameter\'](https://stackoverflow.com/a/36495940/196405), and the fact that the man page says `jshell [options] [load-files]`, implying options should precede file parameters, as is typical for command-line programs.\r\n\r\nTo explain why this matters, consider the following. In my explanation, I replace the `--startup DEFAULT` option with the `--no-startup` option, to show the differences between options and file parameters, while ignoring the behavioral differences between including the `DEFAULT` startup or not.\r\n\r\nFunctionally, the following are the same:\r\n```\r\njshell --startup    conf/jshell-init.jsh <-- replaces the DEFAULT startup with this file\r\njshell --no-startup conf/jshell-init.jsh <-- skips the DEFAULT startup and only executes this file\r\n```\r\n\r\nHowever, if we\'re also passing in user options in `$@`, this becomes a problem. To see why, imagine the user executes `bin/accumulo jshell -v userFile.jsh`, then the resulting commands look like:\r\n```\r\njshell --startup    conf/jshell-init.jsh -v userFile.jsh <-- replaces the DEFAULT startup, also sets the verbose feedback mode, and then executes the user file\r\njshell --no-startup conf/jshell-init.jsh -v userFile.jsh <-- may work, but mixes [options] with [load-files]\r\n```\r\n\r\nI actually tried these both, and jshell did work for me, even though it is not correct according to the manual. It seems jshell is a bit forgiving, at least in the version I have installed. However, we shouldn\'t count on it working.\r\n\r\nAnd, there is still a difference. If you run `/list -all`, you can see that anything executed as a startup script is listed with the letter `s` followed by a number, and anything executed after, including all commands from `[load-files]` parameters, just gets a number. This matters for user experience, because if you want to reset the state of the jshell using `/reset`, all the startup scripts will re-execute, but any `[load-files]` ones will not (`/reload` would be used instead of `/reset` if you want to re-run the other commands after resetting).\r\n\r\nSo, this is all to say that I think that we should make sure that when we execute `jshell-init.jsh`, we should ensure that it is done as a `--startup` option, and not as a `[load-files]` parameter. That way, `/reset` and `/reload` behave as expected, and we don\'t mix and match `[options]` and `[load-files]` arguments when users specify additional options to the command, which may not work in future.\r\n\r\nThe only downside to this is that if we want any `java.*` imports automatically loaded, we need to include them in the generated file. However, this isn\'t really a downside, as much as it is an opportunity to tailor our initial imports to avoid any potential conflicts (such as that between `java.util.Scanner` and `org.apache.accumulo.core.client.Scanner`).\r\n\r\n```suggestion\r\n      exec ""$cmd"" --startup ""$jShellPath"" ""$@""\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'I just learned that we can actually specify more than one `--startup` option, so if we don\'t want to customize the `java.*` imports for now, and just want the defaults, we can do:\r\n\r\n```suggestion\r\n      exec ""$cmd"" --startup DEFAULT --startup ""$jShellPath"" ""$@""\r\n```\r\n\r\nThis preserves proper `/reset` and `/reload` behavior without mixing and matching `[options]` and `[load-files]` parameter, but saves us the work of generating `java.*` imports ourselves. However, it does leave the Scanner conflict in place, which is probably acceptable for now.', 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii For the DEFAULT script, I originally left that as a start up parameter to ensure the user can see the default import library and the Accumulo API when they first boot up JShell. I believe having two start-up statements should do the trick and I do see the results you are getting from my end. Also I believe I found a work around regarding the Scanner. Whenever a Scanner is defined, we need to explicitly specify which Scanner we are referring to. For example: \r\n\r\n**Referencing Accumulo Scanner:**\r\norg.apache.accumulo.core.client.Scanner scan = client.createScanner(""GothamPD"", Authorizations.EMPTY);\r\n\r\n**Referencing Java Scanner:**\r\n java.util.Scanner myObj = new java.util.Scanner(System.in);\r\n\r\nI created a Java test code where one function uses the Accumulo Scanner and another function uses the Java Scanner. I then executed the code in JShell with Accumulo and both functions where able to execute with no issues. \r\n', 'commenter': 'slackwinner'}, {'comment': ""I was aware of the workaround, but it's still annoying to specify the fully-qualified class name. Since the java.util.Scanner is very unlikely to be used by Accumulo devs using this, I was thinking we could try to exclude it by replacing `java.util.*` from the defaults with a narrower set of useful imports from `java.util`, but I'm not sure what that set would look like. It would probably include a bunch of Collection classes, but I'm not sure what else.\r\n\r\nThe alternative is that we just settle for the workaround for now, and consider a new name for Scanner in Accumulo's public API in the future to make it easier to coexist with java.util.Scanner."", 'commenter': 'ctubbsii'}, {'comment': 'Would you like me to modify the create-jshell.sh script to import all the classes from java.util.* into the jshell-init.jsh file or just go with the alternative solution?', 'commenter': 'slackwinner'}, {'comment': 'Let\'s just go with the double-startup option for now. `--startup DEFAULT --startup ""$jShellPath""`', 'commenter': 'ctubbsii'}, {'comment': 'Cool. I just pushed a commit to apply your double-startup option idea into the accumulo script file. Thanks for explaining the reasoning behind the --startup option. I did not consider the /reset case up until now. ', 'commenter': 'slackwinner'}]"
1910,assemble/bin/accumulo,"@@ -58,6 +58,18 @@ function main() {
     echo ""$CLASSPATH""
     exit 0
   fi
+  
+  # Set up path variable for default import config file
+  export jShellPath=""$conf/jshell-init.jsh""
+  ","[{'comment': 'Remove trailing whitespace on empty lines. Also, I don\'t think this variable needs to be exported, since it\'s only ever used within this same script, and not used by any subcommands. However, it is used inside a function (called \'main\'), so you can make it a local variable, so its assignment doesn\'t leak outside the function. Annoyingly, the default in bash is that assignments create global variables.\r\n\r\n```suggestion\r\n\r\n  # Set up path variable for default import config file\r\n  local jShellPath=""$conf/jshell-init.jsh""\r\n\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""@ctubbsii Thank you for the helpful improvements/suggestions. This code review is helping me understand bash scripting in general. I'll apply these improvements and push a revised copy soon. "", 'commenter': 'slackwinner'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,110 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""","[{'comment': ""I don't think any of the variables in this entire script need to be exported, but they can all be declared as local variables. A few other variables don't have any exports, but can also be declared as local variables if they are inside a function."", 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,110 @@
+#! /usr/bin/env bash","[{'comment': ""This file has a lot of lines that have trailing whitespace. It'd be good to remove any trailing spaces on all lines in the file. If you're using VIM, you can do this with the command:\r\n```\r\n:1,$ s/  *$//c\r\n```\r\nThis goes through lines 1, through $ (marker for last line in file), and replaces any single space followed by an optional sequence of multiple spaces at the end of each line with an empty string, and the `c` at the end is to confirm each one, so you can make sure it's working correctly. Drop the `c` if you just want to run without checking."", 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,110 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""
+    
+  # Does an auto-generated JShell config file exists?
+  if [[ ! -e ""$jPath"" ]]; then
+    echo ""Cannot add APIs in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+  
+  # Is the source directory valid?
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    rm ""$jPath""
+    exit 1
+  fi
+  
+  # Does a valid JShell path and source directory exists?
+  if [[ -e ""$jPath"" ]] && [[ -d ""$srcDir"" ]]; then
+    # Add API category designator in jshell-init.jsh 
+    case ""$srcDir"" in
+        *""$client""*) echo ""// Accumulo Client API"" >> ""$jPath"";;
+        *""$data""*) echo ""// Accumulo Data API"" >> ""$jPath"";;
+        *""$security""*) echo ""// Accumulo Security API"" >> ""$jPath"";;
+        *""$mini""*) echo ""// Accumulo Minicluster API"" >> ""$jPath"";;
+        *""$hadoop""*) echo ""// Accumulo Hadoop API"" >> ""$jPath"";;
+        *) echo ""// Other API"" >> ""$jPath"";;
+    esac
+   
+    # Extract API info from provided source directory
+    mapfile -t api < <(find ""$srcDir"" -type f -name '*.java'| 
+                       xargs -n1 dirname| sort -u)
+   
+    # Load in API and format source directory into Java import statements
+    for apiPath in ""${api[@]}""; do
+       printf ""%s\n"" ""import ${apiPath##*/java/}.*"" >> ""$jPath""
+    done
+    sed -i '/^ *import / s#/#.#g' ""$jPath""
+    echo "" "" >> ""$jPath""","[{'comment': 'You could just use echo here instead of printf, since the only thing printf is doing is adding the newline, which echo would do by default. `tr` is probably also faster than `sed`, and the extra blank line doesn\'t need a space:\r\n\r\n```suggestion\r\n       echo ""import ${apiPath##*/java/}.*"" | tr / . >> ""$jPath""\r\n    done\r\n    echo >> ""$jPath""\r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,110 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""
+    
+  # Does an auto-generated JShell config file exists?
+  if [[ ! -e ""$jPath"" ]]; then
+    echo ""Cannot add APIs in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+  
+  # Is the source directory valid?
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    rm ""$jPath""
+    exit 1
+  fi
+  
+  # Does a valid JShell path and source directory exists?
+  if [[ -e ""$jPath"" ]] && [[ -d ""$srcDir"" ]]; then
+    # Add API category designator in jshell-init.jsh 
+    case ""$srcDir"" in
+        *""$client""*) echo ""// Accumulo Client API"" >> ""$jPath"";;
+        *""$data""*) echo ""// Accumulo Data API"" >> ""$jPath"";;
+        *""$security""*) echo ""// Accumulo Security API"" >> ""$jPath"";;
+        *""$mini""*) echo ""// Accumulo Minicluster API"" >> ""$jPath"";;
+        *""$hadoop""*) echo ""// Accumulo Hadoop API"" >> ""$jPath"";;
+        *) echo ""// Other API"" >> ""$jPath"";;
+    esac
+   
+    # Extract API info from provided source directory
+    mapfile -t api < <(find ""$srcDir"" -type f -name '*.java'| 
+                       xargs -n1 dirname| sort -u)","[{'comment': 'When I ran shellcheck on this script, it warned of files containing special characters that might propose a problem. Making the output of `find` a null `\\0` character and the input of `xargs` look split on null characters, the risk goes away:\r\n\r\n```suggestion\r\n    # Extract API info from provided source directory\r\n    mapfile -t api < <(find ""$srcDir"" -type f -name \'*.java\' -print0 | \r\n                       xargs -0 -n1 dirname| sort -u)\r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,110 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""
+    
+  # Does an auto-generated JShell config file exists?
+  if [[ ! -e ""$jPath"" ]]; then
+    echo ""Cannot add APIs in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+  
+  # Is the source directory valid?
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    rm ""$jPath""
+    exit 1
+  fi
+  
+  # Does a valid JShell path and source directory exists?
+  if [[ -e ""$jPath"" ]] && [[ -d ""$srcDir"" ]]; then
+    # Add API category designator in jshell-init.jsh 
+    case ""$srcDir"" in
+        *""$client""*) echo ""// Accumulo Client API"" >> ""$jPath"";;
+        *""$data""*) echo ""// Accumulo Data API"" >> ""$jPath"";;
+        *""$security""*) echo ""// Accumulo Security API"" >> ""$jPath"";;
+        *""$mini""*) echo ""// Accumulo Minicluster API"" >> ""$jPath"";;
+        *""$hadoop""*) echo ""// Accumulo Hadoop API"" >> ""$jPath"";;
+        *) echo ""// Other API"" >> ""$jPath"";;
+    esac
+   
+    # Extract API info from provided source directory
+    mapfile -t api < <(find ""$srcDir"" -type f -name '*.java'| 
+                       xargs -n1 dirname| sort -u)
+   
+    # Load in API and format source directory into Java import statements
+    for apiPath in ""${api[@]}""; do
+       printf ""%s\n"" ""import ${apiPath##*/java/}.*"" >> ""$jPath""
+    done
+    sed -i '/^ *import / s#/#.#g' ""$jPath""
+    echo "" "" >> ""$jPath""
+  fi
+}
+
+function main(){
+  # Establish Accumulo's main base directory
+  SOURCE=""${BASH_SOURCE[0]}""
+  while [[ -h ""${SOURCE}"" ]]; do
+    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+    SOURCE=""$(readlink ""${SOURCE}"")""
+    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""
+  done
+    
+  # Establish file and folder paths for JShell config
+  export scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""; 
+  export mainBase=$( cd -P ""${scriptPath}""/../../../.. && pwd );
+  export jPath=""$mainBase/assemble/target/jshell-init.jsh""
+  export corePath=""core/src/main/java/org/apache/accumulo/core""
+  export miniPath=""minicluster/src/main/java/org/apache/accumulo""
+  export hadoopPath=""hadoop-mapreduce/src/main/java/org/apache/accumulo""
+    
+  # Create path to Accumulo Public API Source Directories
+  export CLIENT=""$mainBase/$corePath/client""
+  export DATA=""$mainBase/$corePath/data""
+  export SECURITY=""$mainBase/$corePath/security""
+  export MINI=""$mainBase/$miniPath/minicluster""
+  export HADOOP=""$mainBase/$hadoopPath/hadoop/mapreduce"" ","[{'comment': 'shellcheck warns of return code suppression when assigning and exporting at the same time. It would also warn after these are converted to local variables, which can also be done, since these don\'t need to be exported. This only applies when assigning a variable to the result of a command, but not to simple assignments.\r\n\r\nI strongly recommend running shellcheck to catch any potential errors on any scripts you write.\r\n\r\n```suggestion\r\n  local scriptPath; scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""; \r\n  local mainBase; mainBase=$( cd -P ""${scriptPath}""/../../../.. && pwd );\r\n  local jPath=""$mainBase/assemble/target/jshell-init.jsh""\r\n  local corePath=""core/src/main/java/org/apache/accumulo/core""\r\n  local miniPath=""minicluster/src/main/java/org/apache/accumulo""\r\n  local hadoopPath=""hadoop-mapreduce/src/main/java/org/apache/accumulo""\r\n    \r\n  # Create path to Accumulo Public API Source Directories\r\n  local CLIENT=""$mainBase/$corePath/client""\r\n  local DATA=""$mainBase/$corePath/data""\r\n  local SECURITY=""$mainBase/$corePath/security""\r\n  local MINI=""$mainBase/$miniPath/minicluster""\r\n  local HADOOP=""$mainBase/$hadoopPath/hadoop/mapreduce"" \r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,110 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""
+    
+  # Does an auto-generated JShell config file exists?
+  if [[ ! -e ""$jPath"" ]]; then
+    echo ""Cannot add APIs in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+  
+  # Is the source directory valid?
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    rm ""$jPath""
+    exit 1
+  fi
+  
+  # Does a valid JShell path and source directory exists?
+  if [[ -e ""$jPath"" ]] && [[ -d ""$srcDir"" ]]; then
+    # Add API category designator in jshell-init.jsh 
+    case ""$srcDir"" in
+        *""$client""*) echo ""// Accumulo Client API"" >> ""$jPath"";;
+        *""$data""*) echo ""// Accumulo Data API"" >> ""$jPath"";;
+        *""$security""*) echo ""// Accumulo Security API"" >> ""$jPath"";;
+        *""$mini""*) echo ""// Accumulo Minicluster API"" >> ""$jPath"";;
+        *""$hadoop""*) echo ""// Accumulo Hadoop API"" >> ""$jPath"";;
+        *) echo ""// Other API"" >> ""$jPath"";;
+    esac
+   
+    # Extract API info from provided source directory
+    mapfile -t api < <(find ""$srcDir"" -type f -name '*.java'| 
+                       xargs -n1 dirname| sort -u)
+   
+    # Load in API and format source directory into Java import statements
+    for apiPath in ""${api[@]}""; do
+       printf ""%s\n"" ""import ${apiPath##*/java/}.*"" >> ""$jPath""
+    done
+    sed -i '/^ *import / s#/#.#g' ""$jPath""
+    echo "" "" >> ""$jPath""
+  fi
+}
+
+function main(){
+  # Establish Accumulo's main base directory
+  SOURCE=""${BASH_SOURCE[0]}""
+  while [[ -h ""${SOURCE}"" ]]; do
+    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+    SOURCE=""$(readlink ""${SOURCE}"")""
+    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""
+  done
+    
+  # Establish file and folder paths for JShell config
+  export scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""; 
+  export mainBase=$( cd -P ""${scriptPath}""/../../../.. && pwd );
+  export jPath=""$mainBase/assemble/target/jshell-init.jsh""
+  export corePath=""core/src/main/java/org/apache/accumulo/core""
+  export miniPath=""minicluster/src/main/java/org/apache/accumulo""
+  export hadoopPath=""hadoop-mapreduce/src/main/java/org/apache/accumulo""
+    
+  # Create path to Accumulo Public API Source Directories
+  export CLIENT=""$mainBase/$corePath/client""
+  export DATA=""$mainBase/$corePath/data""
+  export SECURITY=""$mainBase/$corePath/security""
+  export MINI=""$mainBase/$miniPath/minicluster""
+  export HADOOP=""$mainBase/$hadoopPath/hadoop/mapreduce"" 
+   
+  # Does an auto-generated JShell config file exists?
+  if [[ -e ""$jPath"" ]]; then
+     rm ""$jPath""
+  fi
+    
+  # Create new jshell-init file and load in license header   
+  touch ""$jPath""","[{'comment': 'This file can be erased and created at the same time. Also, you can make sure its directory is created also, in case the script runs before other parts of Maven that normally create the target directory:\r\n\r\n```suggestion\r\n  # Create new jshell-init file\r\n  mkdir -p ""$mainBase/assemble/target""\r\n  :> ""$jPath""\r\n```\r\n', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,110 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""
+    
+  # Does an auto-generated JShell config file exists?
+  if [[ ! -e ""$jPath"" ]]; then
+    echo ""Cannot add APIs in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+  
+  # Is the source directory valid?
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    rm ""$jPath""
+    exit 1
+  fi
+  
+  # Does a valid JShell path and source directory exists?
+  if [[ -e ""$jPath"" ]] && [[ -d ""$srcDir"" ]]; then
+    # Add API category designator in jshell-init.jsh 
+    case ""$srcDir"" in
+        *""$client""*) echo ""// Accumulo Client API"" >> ""$jPath"";;
+        *""$data""*) echo ""// Accumulo Data API"" >> ""$jPath"";;
+        *""$security""*) echo ""// Accumulo Security API"" >> ""$jPath"";;
+        *""$mini""*) echo ""// Accumulo Minicluster API"" >> ""$jPath"";;
+        *""$hadoop""*) echo ""// Accumulo Hadoop API"" >> ""$jPath"";;
+        *) echo ""// Other API"" >> ""$jPath"";;
+    esac
+   
+    # Extract API info from provided source directory
+    mapfile -t api < <(find ""$srcDir"" -type f -name '*.java'| 
+                       xargs -n1 dirname| sort -u)
+   
+    # Load in API and format source directory into Java import statements
+    for apiPath in ""${api[@]}""; do
+       printf ""%s\n"" ""import ${apiPath##*/java/}.*"" >> ""$jPath""
+    done
+    sed -i '/^ *import / s#/#.#g' ""$jPath""
+    echo "" "" >> ""$jPath""
+  fi
+}
+
+function main(){
+  # Establish Accumulo's main base directory
+  SOURCE=""${BASH_SOURCE[0]}""
+  while [[ -h ""${SOURCE}"" ]]; do
+    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+    SOURCE=""$(readlink ""${SOURCE}"")""
+    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""
+  done
+    
+  # Establish file and folder paths for JShell config
+  export scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""; 
+  export mainBase=$( cd -P ""${scriptPath}""/../../../.. && pwd );
+  export jPath=""$mainBase/assemble/target/jshell-init.jsh""
+  export corePath=""core/src/main/java/org/apache/accumulo/core""
+  export miniPath=""minicluster/src/main/java/org/apache/accumulo""
+  export hadoopPath=""hadoop-mapreduce/src/main/java/org/apache/accumulo""
+    
+  # Create path to Accumulo Public API Source Directories
+  export CLIENT=""$mainBase/$corePath/client""
+  export DATA=""$mainBase/$corePath/data""
+  export SECURITY=""$mainBase/$corePath/security""
+  export MINI=""$mainBase/$miniPath/minicluster""
+  export HADOOP=""$mainBase/$hadoopPath/hadoop/mapreduce"" 
+   
+  # Does an auto-generated JShell config file exists?
+  if [[ -e ""$jPath"" ]]; then
+     rm ""$jPath""
+  fi
+    
+  # Create new jshell-init file and load in license header   
+  touch ""$jPath""
+ 
+  # Create and add Accumulo APIs into API storage 
+  apiStorage=(""$CLIENT"" ""$DATA"" ""$SECURITY"" ""$MINI"" ""$HADOOP"")
+   
+  # Traverse through each source directory and load in Accumulo APIs
+  for srcDir in ""${apiStorage[@]}""; do","[{'comment': 'These local variables, like arrays and loop variables are easy to overlook and accidentally create global variables. Global variables aren\'t necessarily a problem in a small single-threaded script, but it\'s good practice to keep variables as local as possible.\r\n\r\n```suggestion\r\n  local apiStorage=(""$CLIENT"" ""$DATA"" ""$SECURITY"" ""$MINI"" ""$HADOOP"")\r\n   \r\n  # Traverse through each source directory and load in Accumulo APIs\r\n  local srcDir\r\n  for srcDir in ""${apiStorage[@]}""; do\r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,110 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI(){
+  export srcDir=""$1""
+  export jPath=""$2""
+  export client=""accumulo/core/client""
+  export data=""accumulo/core/data""
+  export security=""accumulo/core/security""
+  export mini=""accumulo/minicluster""
+  export hadoop=""accumulo/hadoop""
+    
+  # Does an auto-generated JShell config file exists?
+  if [[ ! -e ""$jPath"" ]]; then
+    echo ""Cannot add APIs in $jPath""
+    echo ""Please ensure jshell-init.jsh exists""
+    exit 1
+  fi
+  
+  # Is the source directory valid?
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    rm ""$jPath""
+    exit 1
+  fi
+  
+  # Does a valid JShell path and source directory exists?
+  if [[ -e ""$jPath"" ]] && [[ -d ""$srcDir"" ]]; then
+    # Add API category designator in jshell-init.jsh 
+    case ""$srcDir"" in
+        *""$client""*) echo ""// Accumulo Client API"" >> ""$jPath"";;
+        *""$data""*) echo ""// Accumulo Data API"" >> ""$jPath"";;
+        *""$security""*) echo ""// Accumulo Security API"" >> ""$jPath"";;
+        *""$mini""*) echo ""// Accumulo Minicluster API"" >> ""$jPath"";;
+        *""$hadoop""*) echo ""// Accumulo Hadoop API"" >> ""$jPath"";;
+        *) echo ""// Other API"" >> ""$jPath"";;
+    esac
+   
+    # Extract API info from provided source directory
+    mapfile -t api < <(find ""$srcDir"" -type f -name '*.java'| 
+                       xargs -n1 dirname| sort -u)
+   
+    # Load in API and format source directory into Java import statements
+    for apiPath in ""${api[@]}""; do
+       printf ""%s\n"" ""import ${apiPath##*/java/}.*"" >> ""$jPath""
+    done
+    sed -i '/^ *import / s#/#.#g' ""$jPath""
+    echo "" "" >> ""$jPath""
+  fi
+}
+
+function main(){
+  # Establish Accumulo's main base directory
+  SOURCE=""${BASH_SOURCE[0]}""
+  while [[ -h ""${SOURCE}"" ]]; do
+    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+    SOURCE=""$(readlink ""${SOURCE}"")""
+    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""
+  done
+    
+  # Establish file and folder paths for JShell config
+  export scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""; 
+  export mainBase=$( cd -P ""${scriptPath}""/../../../.. && pwd );
+  export jPath=""$mainBase/assemble/target/jshell-init.jsh""
+  export corePath=""core/src/main/java/org/apache/accumulo/core""
+  export miniPath=""minicluster/src/main/java/org/apache/accumulo""
+  export hadoopPath=""hadoop-mapreduce/src/main/java/org/apache/accumulo""
+    
+  # Create path to Accumulo Public API Source Directories
+  export CLIENT=""$mainBase/$corePath/client""
+  export DATA=""$mainBase/$corePath/data""
+  export SECURITY=""$mainBase/$corePath/security""
+  export MINI=""$mainBase/$miniPath/minicluster""
+  export HADOOP=""$mainBase/$hadoopPath/hadoop/mapreduce"" 
+   
+  # Does an auto-generated JShell config file exists?
+  if [[ -e ""$jPath"" ]]; then
+     rm ""$jPath""
+  fi
+    
+  # Create new jshell-init file and load in license header   
+  touch ""$jPath""
+ 
+  # Create and add Accumulo APIs into API storage 
+  apiStorage=(""$CLIENT"" ""$DATA"" ""$SECURITY"" ""$MINI"" ""$HADOOP"")
+   
+  # Traverse through each source directory and load in Accumulo APIs
+  for srcDir in ""${apiStorage[@]}""; do
+    addAccumuloAPI ""$srcDir"" ""$jPath""
+  done
+  exit 0","[{'comment': 'Normally at the end of a function, you\'d want to let the return code of the last command propagate outside of the function. If this loop terminates abnormally, we shouldn\'t mask its return code with our own using `exit 0`, which forces it to terminate successfully, even if there was an error.\r\n\r\nAlso, I was thinking, you don\'t need to pass in the filename to the function or create it ahead of time. Instead, you can just create the file right here at the output of the whole for loop. That would simplify the implementation of the addAccumuloAPI command a lot.\r\n\r\n```suggestion\r\n    addAccumuloAPI ""$srcDir""\r\n  done > ""$jPath""\r\n```\r\nOne other suggestion I had was that you could move the printing of the section headers to outside that method, that way you don\'t need to create so many variables in that method to do the section header checks. You could either move the if statements to here, inside this for loop, before calling `addAccumuloAPI`, or you remove the loop, and do something like the following. You wouldn\'t even need all the extra variables this way, because you could just inline them:\r\n\r\n```bash\r\n  {\r\n    echo ""// section header here for client""\r\n    addAccumuloAPI ""$CLIENT""\r\n    echo ""// section header here for data""\r\n    addAccumuloAPI ""$DATA""\r\n    # etc...\r\n  } > ""$jPath""\r\n```\r\n', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,92 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI(){
+  local srcDir=""$1""
+  # Extract API info from provided source directory
+  mapfile -t api < <(find ""$srcDir"" -type f -name '*.java' -print0|
+                     xargs -0 -n1 dirname| sort -u)
+
+  # Load in API and format source directory into Java import statements
+  for apiPath in ""${api[@]}""; do
+     echo ""import ${apiPath##*/java/}.*"" | tr / .
+  done
+  echo
+}
+
+function main(){
+  # Establish Accumulo's main base directory
+  SOURCE=""${BASH_SOURCE[0]}""
+  while [[ -h ""${SOURCE}"" ]]; do
+    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+    SOURCE=""$(readlink ""${SOURCE}"")""
+    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""
+  done
+
+  # Establish file and folder paths for JShell config
+  local scriptPath
+  scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+  local mainBase
+  mainBase=""$( cd -P ""${scriptPath}""/../../../.. && pwd )""
+  local jPath=""$mainBase/assemble/target/jshell-init.jsh""
+  local corePath=""core/src/main/java/org/apache/accumulo/core""
+  local miniPath=""minicluster/src/main/java/org/apache/accumulo""
+  local hadoopPath=""hadoop-mapreduce/src/main/java/org/apache/accumulo""
+
+  # Create path to Accumulo Public API Source Directories
+  local CLIENT=""$mainBase/$corePath/client""
+  local DATA=""$mainBase/$corePath/data""
+  local SECURITY=""$mainBase/$corePath/security""
+  local MINI=""$mainBase/$miniPath/minicluster""
+  local HADOOP=""$mainBase/$hadoopPath/hadoop/mapreduce""
+
+  # Create new jshell-init file
+  mkdir -p ""$mainBase/assemble/target""
+  :> ""$jPath""
+
+  # Create and add Accumulo APIs into API storage
+  local apiStorage=(""$CLIENT"" ""$DATA"" ""$SECURITY"" ""$MINI"" ""$HADOOP"")
+  local srcDir
+
+  # Validate each source directory before populating JShell-Init file
+  for srcDir in ""${apiStorage[@]}""; do
+    if [[ ! -d ""$srcDir"" ]]; then
+      echo ""Could not auto-generate jshell-init.jsh""
+      echo ""$srcDir is not a valid directory. Please make sure it exists.""
+      rm ""$jPath""
+      exit 1
+    fi
+  done
+  echo ""Generating JShell-Init file""
+  {
+    echo ""System.out.println(\""Preparing JShell for Apache Accumulo\"")""
+    echo ""// Accumulo Client API""
+    addAccumuloAPI ""${apiStorage[0]}""
+    echo ""// Accumulo Data API""
+    addAccumuloAPI ""${apiStorage[1]}""
+    echo ""// Accumulo Security API""
+    addAccumuloAPI ""${apiStorage[2]}""
+    echo ""// Accumulo MiniCluster API""
+    addAccumuloAPI ""${apiStorage[3]}""
+    echo ""// Accumulo Hadoop API""
+    addAccumuloAPI ""${apiStorage[4]}""
+  } > ""$jPath""
+}
+main ""$@""","[{'comment': 'I had a few other minor tips for this script, but probably not worth going back and forth over them, so here they are as one lump suggestion:\r\n\r\n* Minor style improvements (indentation using 2 spaces instead of 3,\r\n  space before opening brace in function declaration and around pipes\r\n* Declare additional variables as local (SOURCE, bin) and place in\r\n  single line at top of each function\r\n* Do validation of srcDirs in called method, rather than in a separate\r\n  loop, to avoid needing to use an array for the srcDirs\r\n* Reduce number of variables needed by inlining several\r\n* Remove redundant clearing of destination file\r\n* Use single quote for strings that don\'t have embedded variables\r\n* Move System.out.println to after the imports\r\n\r\n```suggestion\r\nfunction addAccumuloAPI() {\r\n  local srcDir=""$1"" api apiPath\r\n  # Validate each source directory before populating JShell-Init file\r\n  if [[ ! -d ""$srcDir"" ]]; then\r\n    echo ""$srcDir is not a valid directory. Please make sure it exists.""\r\n    exit 1\r\n  fi\r\n\r\n  # Extract API info from provided source directory\r\n  mapfile -t api < <(find ""$srcDir"" -type f -name \'*.java\' -print0 | xargs -0 -n1 dirname | sort -u)\r\n\r\n  # Load in API and format source directory into Java import statements\r\n  for apiPath in ""${api[@]}""; do\r\n    echo ""import ${apiPath##*/java/}.*"" | tr / .\r\n  done\r\n  echo\r\n}\r\n\r\nfunction main() {\r\n  local SOURCE bin scriptPath mainBase corePath\r\n  # Establish Accumulo\'s main base directory\r\n  SOURCE=""${BASH_SOURCE[0]}""\r\n  while [[ -h ""${SOURCE}"" ]]; do\r\n    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""\r\n    SOURCE=""$(readlink ""${SOURCE}"")""\r\n    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""\r\n  done\r\n\r\n  # Establish file and folder paths for JShell config\r\n  scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""\r\n  mainBase=""$( cd -P ""${scriptPath}""/../../../.. && pwd )""\r\n  corePath=""$mainBase/core/src/main/java/org/apache/accumulo/core""\r\n\r\n  # Create new jshell-init file\r\n  mkdir -p ""$mainBase/assemble/target""\r\n  echo \'Generating JShell-Init file\'\r\n  {\r\n    echo \'// Accumulo Client API\'\r\n    addAccumuloAPI ""$corePath/client""\r\n    echo \'// Accumulo Data API\'\r\n    addAccumuloAPI ""$corePath/data""\r\n    echo \'// Accumulo Security API\'\r\n    addAccumuloAPI ""$corePath/security""\r\n    echo \'// Accumulo MiniCluster API\'\r\n    addAccumuloAPI ""$mainBase/minicluster/src/main/java/org/apache/accumulo/minicluster""\r\n    echo \'// Accumulo Hadoop API\'\r\n    addAccumuloAPI ""$mainBase/hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoop/mapreduce""\r\n    echo\r\n    echo \'System.out.println(""Preparing JShell for Apache Accumulo"")\'\r\n    echo\r\n  } > ""$mainBase/assemble/target/jshell-init.jsh""\r\n}\r\n\r\nmain ""$@""\r\n\r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,74 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI() {
+  local srcDir=""$1"" api apiPath
+  # Validate each source directory before populating JShell-Init file
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    exit 1
+  fi
+
+  # Extract API info from provided source directory
+  mapfile -t api < <(find ""$srcDir"" -type f -name '*.java' -print0 | xargs -0 -n1 dirname | sort -u)
+
+  # Load in API and format source directory into Java import statements
+  for apiPath in ""${api[@]}""; do
+    echo ""import ${apiPath##*/java/}.*"" | tr / .","[{'comment': 'Trailing semicolon isn\'t strictly necessary for JShell, but it\'s nice to have.\r\n\r\n```suggestion\r\n    echo ""import ${apiPath##*/java/}.*;"" | tr / .\r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,74 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI() {
+  local srcDir=""$1"" api apiPath
+  # Validate each source directory before populating JShell-Init file
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    exit 1
+  fi
+
+  # Extract API info from provided source directory
+  mapfile -t api < <(find ""$srcDir"" -type f -name '*.java' -print0 | xargs -0 -n1 dirname | sort -u)
+
+  # Load in API and format source directory into Java import statements
+  for apiPath in ""${api[@]}""; do
+    echo ""import ${apiPath##*/java/}.*"" | tr / .
+  done
+  echo
+}
+
+function main() {
+  local SOURCE bin scriptPath mainBase corePath
+  # Establish Accumulo's main base directory
+  SOURCE=""${BASH_SOURCE[0]}""
+  while [[ -h ""${SOURCE}"" ]]; do
+    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+    SOURCE=""$(readlink ""${SOURCE}"")""
+    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""
+  done
+
+  # Establish file and folder paths for JShell config
+  scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+  mainBase=""$( cd -P ""${scriptPath}""/../../../.. && pwd )""
+  corePath=""$mainBase/core/src/main/java/org/apache/accumulo/core""
+
+  # Create new jshell-init file
+  mkdir -p ""$mainBase/assemble/target""
+  echo 'Generating JShell-Init file'
+  {
+    echo '// Accumulo Client API'
+    addAccumuloAPI ""$corePath/client""
+    echo '// Accumulo Data API'
+    addAccumuloAPI ""$corePath/data""
+    echo '// Accumulo Security API'
+    addAccumuloAPI ""$corePath/security""
+    echo '// Accumulo MiniCluster API'
+    addAccumuloAPI ""$mainBase/minicluster/src/main/java/org/apache/accumulo/minicluster""
+    echo '// Accumulo Hadoop API'
+    addAccumuloAPI ""$mainBase/hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoop/mapreduce""
+    echo
+    echo 'System.out.println(""Preparing JShell for Apache Accumulo"")'","[{'comment': 'While playing with these changes, I noticed that `Text` from Hadoop would be nice to have by default, as it is frequently used in our own APIs.\r\n\r\n```suggestion\r\n    echo \'// Essential Hadoop API\r\n    echo \'import org.apache.hadoop.io.Text;\'\r\n    echo\r\n    echo \'// Initialization code\'\r\n    echo \'System.out.println(""Preparing JShell for Apache Accumulo"");\'\r\n```', 'commenter': 'ctubbsii'}]"
1910,assemble/src/main/scripts/create-jshell.sh,"@@ -0,0 +1,77 @@
+#! /usr/bin/env bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+function addAccumuloAPI() {
+  local srcDir=""$1"" api apiPath
+  # Validate each source directory before populating JShell-Init file
+  if [[ ! -d ""$srcDir"" ]]; then
+    echo ""$srcDir is not a valid directory. Please make sure it exists.""
+    exit 1
+  fi
+
+  # Extract API info from provided source directory
+  mapfile -t api < <(find ""$srcDir"" -type f -name '*.java' -print0 | xargs -0 -n1 dirname | sort -u)
+
+  # Load in API and format source directory into Java import statements
+  for apiPath in ""${api[@]}""; do
+    echo ""import ${apiPath##*/java/}.*;"" | tr / .
+  done
+  echo
+}
+
+function main() {
+  local SOURCE bin scriptPath mainBase corePath
+  # Establish Accumulo's main base directory
+  SOURCE=""${BASH_SOURCE[0]}""
+  while [[ -h ""${SOURCE}"" ]]; do
+    bin=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+    SOURCE=""$(readlink ""${SOURCE}"")""
+    [[ ""${SOURCE}"" != /* ]] && SOURCE=""${bin}/${SOURCE}""
+  done
+
+  # Establish file and folder paths for JShell config
+  scriptPath=""$( cd -P ""$( dirname ""${SOURCE}"" )"" && pwd )""
+  mainBase=""$( cd -P ""${scriptPath}""/../../../.. && pwd )""
+  corePath=""$mainBase/core/src/main/java/org/apache/accumulo/core""
+
+  # Create new jshell-init file
+  mkdir -p ""$mainBase/assemble/target""
+  echo 'Generating JShell-Init file'
+  {
+    echo '// Accumulo Client API'
+    addAccumuloAPI ""$corePath/client""
+    echo '// Accumulo Data API'
+    addAccumuloAPI ""$corePath/data""
+    echo '// Accumulo Security API'
+    addAccumuloAPI ""$corePath/security""
+    echo '// Accumulo MiniCluster API'
+    addAccumuloAPI ""$mainBase/minicluster/src/main/java/org/apache/accumulo/minicluster""
+    echo '// Accumulo Hadoop API'
+    addAccumuloAPI ""$mainBase/hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoop/mapreduce""
+    echo '// Essential Hadoop API","[{'comment': ""Oops. I broke this with the missing quote\r\n```suggestion\r\n    echo '// Essential Hadoop API'\r\n```"", 'commenter': 'ctubbsii'}]"
1943,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/server.ftl,"@@ -141,7 +141,7 @@
             <thead>
               <tr>
                 <th>Table&nbsp;</th>
-                <th>Tablet&nbsp;</th>
+                <th>Tablet&nbsp;(run shell cmd: getsplits -v)</th>","[{'comment': 'This should be in the mouseover/tooltip, instead of in the column.', 'commenter': 'ctubbsii'}, {'comment': ""I had thought of that but didn't think that it was obvious enough. Also, none of the columns on that table currently have tooltips. "", 'commenter': 'milleruntime'}, {'comment': 'I just think that leaving it in the column header makes the interface messy, since it\'s instructions, and not part of the actual name/description of the field/column.\r\n\r\nIt\'s also a bit weird to specifically call out a separate tool without a more complete sentence, which we could do in a tooltip. The monitor and shell are largely independent tools that users could mix-and-match and replace with other tools, and different users might interface with each of them. So the monitor\'s explicit assumption that the user viewing the monitor has access to the Accumulo-provided shell and that they will understand ""run shell cmd"" as short for ""run the Accumulo Shell client utility provided in the default Accumulo binary distribution tarball"" are a little bit of a stretch for me.\r\n\r\nMaybe the availability of a tooltip could be made more obvious with a little information symbol 🛈 or similar. I was thinking a longer tip, like `To associate the tablets listed here with their entries in the metadata table, use the <code>getsplits -v</code> command in the Accumulo Shell utility.`', 'commenter': 'ctubbsii'}, {'comment': ""I like the information symbol idea. I will look and see if our web deps already have an icon included. The unicode you used isn't showing up for me"", 'commenter': 'milleruntime'}, {'comment': 'From my comments on issue #1940 - would it be more useful if the column displayed the table-id and assigned server? Otherwise I agree with the tool-tip suggestion.', 'commenter': 'EdColeman'}, {'comment': ""I responded on #1940 but the short answer is no, it wouldn't make sense."", 'commenter': 'milleruntime'}]"
1943,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/server.ftl,"@@ -141,7 +141,8 @@
             <thead>
               <tr>
                 <th>Table&nbsp;</th>
-                <th>Tablet&nbsp;</th>
+                <th title=""Run 'getsplits -v' in the Accumulo Shell to associate the encoded tablets with their actual splits."">Tablet&nbsp;
+                  <img height=""10em"" width=""10em"" alt=""information"" src=""/resources/images/circled-information-icon.png""></th>","[{'comment': 'The non-breaking space is moot if it is followed by a breaking space (newline, indentation). \r\n\r\nAlso, the image tag isn\'t proper XML. I know HTML5 is pretty forgiving, but it\'s nice to have the tags be balanced with either a closing tag element or the terminal `/>` at the end of the start tag.\r\n\r\n```suggestion\r\n                <th title=""Run \'getsplits -v\' in the Accumulo Shell to associate the encoded tablets with their actual splits."">Tablet&nbsp;<img height=""10em"" width=""10em"" alt=""information"" src=""/resources/images/circled-information-icon.png"" />&nbsp;</th>\r\n```\r\n\r\nI\'m more concerned, however, that the new image file added to the PR seems to be showing up as an ""empty file"". Not sure what\'s going on with that. Bug in GitHub? Did you add an empty file? There might be an existing bootstrap icon that can be used, that is already in the monitor\'s resources.', 'commenter': 'ctubbsii'}, {'comment': 'Maybe `.glyphicon-info-sign` will work?\r\n\r\n```suggestion\r\n                <th title=""Run \'getsplits -v\' in the Accumulo Shell to associate the encoded tablets with their actual splits."">Tablet&nbsp;<span class=""glyphicon glyphicon-info-sign"" aria-hidden=""true""></span>&nbsp;</th>\r\n```', 'commenter': 'ctubbsii'}, {'comment': '> The non-breaking space is moot if it is followed by a breaking space (newline, indentation).\r\n\r\nI thought the template will ignore whitespace/format when generating the page (hence our use of the `&nbsp;` encodings)? \r\n\r\n> I\'m more concerned, however, that the new image file added to the PR seems to be showing up as an ""empty file"". Not sure what\'s going on with that. Bug in GitHub? Did you add an empty file? There might be an existing bootstrap icon that can be used, that is already in the monitor\'s resources.\r\n\r\nThe file I committed is the image and not empty so I don\'t know what is going on with github.\r\n<pre>\r\n08:13:02 (tablet-details-monitor) ~/workspace/accumulo$ file server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/images/circled-information-icon.png\r\nserver/monitor/src/main/resources/org/apache/accumulo/monitor/resources/images/circled-information-icon.png: PNG image data, 128 x 128, 8-bit/color RGBA, non-interlaced\r\n08:13:07 (tablet-details-monitor) ~/workspace/accumulo$ du -sh server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/images/circled-information-icon.png\r\n16K\tserver/monitor/src/main/resources/org/apache/accumulo/monitor/resources/images/circled-information-icon.png\r\n</pre>', 'commenter': 'milleruntime'}, {'comment': ""> > The non-breaking space is moot if it is followed by a breaking space (newline, indentation).\r\n> \r\n> I thought the template will ignore whitespace/format when generating the page (hence our use of the `&nbsp;` encodings)?\r\n> \r\n\r\nThe non-breaking space is there because datatables appends the sort icon to the right of it, and we want to make sure there's a gap between the title and that icon that it adds, but we also want to make sure that the icon doesn't wrap to the next line when the screen is resized. Hence a non-breaking space instead of a regular space for that gap. In HTML, any amount of non-breaking space between two elements permits a line break to be inserted. So, if we want to avoid a line break, elements must be immediately adjacent. In this case, datatables should append its sorting icon to be immediately adjacent to the non-breaking space, and we should have only non-breaking spaces in our own title.\r\n\r\n> The file I committed is the image and not empty so I don't know what is going on with github.\r\n\r\nNot sure why GitHub says it's empty. Where did the icon come from? Any copyright issues with it? I think the bootstrap built-in is best, if we can get it working, rather than try to track copyright license issues from an additional source."", 'commenter': 'ctubbsii'}, {'comment': 'That is a possibility. I would have to track down where I got it from. I will try the bootstrap icon.', 'commenter': 'milleruntime'}]"
1944,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -1352,6 +1352,17 @@ public static boolean isValidZooPropertyKey(String key) {
         || key.startsWith(REPLICATION_PREFIX.getKey());
   }
 
+  /**
+   * Checks if the given sampler option is valid
+   *
+   * @param option
+   *          property option
+   * @return true if option is a valid sampler option
+   */
+  public static boolean isValidSamplerOption(String option) {
+    return option.endsWith(""hasher"") || option.endsWith(""modules"");
+  }
+","[{'comment': ""Sampler options are arbitrary, and implementation-specific. The available options depend on the user's provided sampler implementation. They do not have to only be one of these options. These are merely example properties placed in the docs that an implementation might have."", 'commenter': 'ctubbsii'}, {'comment': ""That is what I was afraid of. Guess there isn't an easy way to guarantee validity. "", 'commenter': 'Manno15'}]"
1944,core/src/main/java/org/apache/accumulo/core/client/sample/Sampler.java,"@@ -61,4 +61,11 @@
    *         Return false if it should not be included.
    */
   boolean accept(Key k);
+
+  /**
+   * @param option
+   *          Option to validate.
+   * @return True if option is a valid sampler option. False otherwise.
+   */
+  boolean isValidOption(String option);","[{'comment': ""Since this is public API and changes the interface, it should be a default method, so that way it won't break custom implementations users have written against the interface that don't have the new method.\r\n\r\nThe original version of this method was previously only used to throw an exception for unrecognized options, but didn't actually do any validation of the option's values. With the addition of this public API, we can improve that situation. This new method should be able to see the keys as well as their corresponding values. To validate all options' values and detect missing required values, this new method should accept all options at the same time (probably as a `Map<String,String>`)."", 'commenter': 'ctubbsii'}, {'comment': 'Good idea, I will look into adding that improvement.', 'commenter': 'Manno15'}, {'comment': 'Is there anything we should do for the default case? I currently have it verified to not be null. Not sure what else to add to the interface itself. ', 'commenter': 'Manno15'}, {'comment': ""The default implementation should probably be a no-op. It is assumed that the map of all the options will be non-null, because Accumulo would be responsible for preparing that map itself, but it's fine if you want to check for that, to be sure."", 'commenter': 'ctubbsii'}, {'comment': 'I went ahead and kept the check. If the user forgets to even mention the option before compacting, it will cause the shell to hang.', 'commenter': 'Manno15'}, {'comment': 'Nevermind. You are correct. I just realized the null gets thrown before reaching my validation check.', 'commenter': 'Manno15'}, {'comment': 'Made these changes in my recent commit. Still need to add a value check for `modulus` in `AbstractHashSampler`', 'commenter': 'Manno15'}]"
1944,core/src/main/java/org/apache/accumulo/core/client/sample/AbstractHashSampler.java,"@@ -77,13 +74,6 @@ public void init(SamplerConfiguration config) {
     String hasherOpt = config.getOptions().get(""hasher"");
     String modulusOpt = config.getOptions().get(""modulus"");
 
-    requireNonNull(hasherOpt, ""Hasher not specified"");
-    requireNonNull(modulusOpt, ""Modulus not specified"");","[{'comment': 'This was removed, but was not replaced with any kind of validation that the required options were set with actual values.', 'commenter': 'ctubbsii'}, {'comment': 'My recent commit moved the null check to where the other argument is checked. ', 'commenter': 'Manno15'}, {'comment': 'Not quite. What you added was a check that the key was not null, which might be redundant, since the fact that it exists implies it is not null. This code here was checking that the values (not the keys) that correspond to these option keys was not null.', 'commenter': 'ctubbsii'}, {'comment': 'Ah, you are correct. I misread that. My future changes should handle this.', 'commenter': 'Manno15'}]"
1944,core/src/main/java/org/apache/accumulo/core/sample/impl/SamplerFactory.java,"@@ -38,13 +42,16 @@ public static Sampler newSampler(SamplerConfigurationImpl config, AccumuloConfig
         clazz = ClassLoaderUtil.loadClass(context, config.getClassName(), Sampler.class);
 
       Sampler sampler = clazz.getDeclaredConstructor().newInstance();
-
+      sampler.validateOptions(config.getOptions());
       sampler.init(config.toSamplerConfiguration());
 
       return sampler;
 
     } catch (ReflectiveOperationException e) {
       throw new RuntimeException(e);
+    } catch (IllegalArgumentException | NullPointerException e) {
+      log.error(""Cannot init sampler, invalid configuration: {}"", e.getMessage(), e);
+      return null;","[{'comment': 'It\'s a little weird that if the class can\'t be loaded, then it throws a runtime exception, but if it can\'t be started because of invalid config or some other problem, then it logs and returns null. I\'m thinking these should probably be consistent. I would do something like:\r\n\r\n```suggestion\r\n    } catch (ReflectiveOperationException | RuntimeException e) {\r\n      log.error(""Cannot initialize sampler {}: {}"", config.getClassName(), e.getMessage(), e);\r\n      return null;\r\n```\r\n', 'commenter': 'ctubbsii'}, {'comment': 'Good idea, I like this change.', 'commenter': 'Manno15'}]"
1944,core/src/main/java/org/apache/accumulo/core/client/sample/RowColumnSampler.java,"@@ -80,8 +83,11 @@ private boolean hashField(SamplerConfiguration config, String field) {
   }
 
   @Override
-  protected boolean isValidOption(String option) {
-    return super.isValidOption(option) || VALID_OPTIONS.contains(option);
+  public void validateOptions(Map<String,String> config) {
+    super.validateOptions(config);
+    for (Map.Entry<String,String> entry : config.entrySet()) {","[{'comment': ""Since you're only using the key, and not validating any values in this implementation, you could just loop over the `keySet()` rather than the `entrySet()`."", 'commenter': 'ctubbsii'}, {'comment': 'Good catch', 'commenter': 'Manno15'}]"
1944,core/src/main/java/org/apache/accumulo/core/client/sample/AbstractHashSampler.java,"@@ -57,14 +58,28 @@
   private int modulus;
 
   private static final Set<String> VALID_OPTIONS = Set.of(""hasher"", ""modulus"");
+  private static final Set<String> VALID_VALUES_HASHER = Set.of(""murmur3_32"", ""md5"", ""sha1"");
 
   /**
    * Subclasses with options should override this method and return true if the option is valid for
    * the subclass or if {@code super.isValidOption(opt)} returns true.
    */
+  @Override
+  public void validateOptions(Map<String,String> config) {
+    for (Map.Entry<String,String> entry : config.entrySet()) {
+      checkArgument(isValid(entry.getKey()), ""Unknown option: %s"", entry.getKey());
+
+      if (entry.getKey().equals(""hasher""))
+        checkArgument(isValid(entry.getValue()), ""Unknown value for hasher: %s"", entry.getValue());
 
-  protected boolean isValidOption(String option) {
-    return VALID_OPTIONS.contains(option);
+      if (entry.getKey().equals(""modulus""))
+        checkArgument(Integer.parseInt(entry.getValue()) > 0,
+            ""Improper Integer value for modulus: %s"", entry.getValue());
+    }
+  }
+
+  protected boolean isValid(String entry) {
+    return VALID_OPTIONS.contains(entry) || VALID_VALUES_HASHER.contains(entry);","[{'comment': ""Each half of this method's implementation is checking for different things. The left side of the `||` checks keys, whereas the right side checks values. Each case should probably just be inlined into their respective areas of the code, and this method can be deleted."", 'commenter': 'ctubbsii'}, {'comment': 'Or, since this method is protected (public API), it should be deprecated, and its original implementation left intact.', 'commenter': 'ctubbsii'}, {'comment': 'My lasted commit includes these changes. I did revert that function and deprecated it. Let me know if I need to do something else for it or not. Will add comments above to explain the deprecation (might not be necessary). ', 'commenter': 'Manno15'}]"
1944,core/src/main/java/org/apache/accumulo/core/client/sample/AbstractHashSampler.java,"@@ -57,12 +58,33 @@
   private int modulus;
 
   private static final Set<String> VALID_OPTIONS = Set.of(""hasher"", ""modulus"");
+  private static final Set<String> VALID_VALUES_HASHER = Set.of(""murmur3_32"", ""md5"", ""sha1"");
+
+  /**
+   * Subclasses with options should override this method to validate subclass options while also
+   * calling {@code super.validateOptions(config)} to validate base class options.
+   */
+  @Override
+  public void validateOptions(Map<String,String> config) {
+    for (Map.Entry<String,String> entry : config.entrySet()) {
+      checkArgument(VALID_OPTIONS.contains(entry.getKey()), ""Unknown option: %s"", entry.getKey());
+
+      if (entry.getKey().equals(""hasher""))
+        checkArgument(VALID_VALUES_HASHER.contains(entry.getValue()),
+            ""Unknown value for hasher: %s"", entry.getValue());
+
+      if (entry.getKey().equals(""modulus""))
+        checkArgument(Integer.parseInt(entry.getValue()) > 0,
+            ""Improper Integer value for modulus: %s"", entry.getValue());
+    }
+  }
 
   /**
    * Subclasses with options should override this method and return true if the option is valid for
    * the subclass or if {@code super.isValidOption(opt)} returns true.
+   * @deprecated since 2.1.0, replaced by {@link #validateOptions(Map)}","[{'comment': '```suggestion\r\n   *\r\n   * @deprecated since 2.1.0, replaced by {@link #validateOptions(Map)}\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""That is what caused the other build error. I couldn't win with either."", 'commenter': 'Manno15'}, {'comment': 'I guess it was something on my end to create extra white space. I had what you committed before and the build failed.', 'commenter': 'Manno15'}, {'comment': ""When the formatter automatically adds a line, it leaves a space after the `*` to indent the section. That causes checkstyle to complain. However, if you delete that extra space, the formatter is fine with it and will not reformat it to add it again. My change added the line back without the extra space that caused it to fail previously. It's just an annoying quirk of the formatter."", 'commenter': 'ctubbsii'}, {'comment': ""I actually think it might be useful to add the trailing space removal as a feature to https://github.com/revelc/formatter-maven-plugin since the Eclipse formatter library it uses doesn't have a feature to remove the trailing whitespace itself."", 'commenter': 'ctubbsii'}, {'comment': ""> When the formatter automatically adds a line, it leaves a space after the * to indent the section. That causes checkstyle to complain. However, if you delete that extra space, the formatter is fine with it and will not reformat it to add it again. My change added the line back without the extra space that caused it to fail previously. It's just an annoying quirk of the formatted.\r\n\r\nah, that makes sense. Thanks for fixing it."", 'commenter': 'Manno15'}]"
1950,core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionDirectives.java,"@@ -30,19 +30,4 @@
    * @return The service where a compaction should run.
    */
   CompactionServiceId getService();
-
-  /**
-   * @since 2.1.0
-   */
-  public static interface Builder {","[{'comment': 'This builder was intended to be used by someone writing a CompactionDispatcher.  If someone were to write a compaction dispatcher now they would need to implement CompactionDirective.', 'commenter': 'keith-turner'}, {'comment': ""If CompactionDirective is for returning information to the user, than it shouldn't need a builder (similar to how we return Environment objects). If the user needs to set objects in it for their implementation, perhaps a separate utility method would be better."", 'commenter': 'milleruntime'}]"
1950,core/src/main/java/org/apache/accumulo/core/spi/compaction/SimpleCompactionDispatcher.java,"@@ -72,19 +72,17 @@
   public void init(InitParameters params) {
     services = new EnumMap<>(CompactionKind.class);
 
-    var defaultService = CompactionDirectives.builder().build();
+    CompactionsDirectiveImpl defaultService = CompactionsDirectiveImpl.DEFAULT;
 
     if (params.getOptions().containsKey(""service"")) {
-      defaultService =
-          CompactionDirectives.builder().setService(params.getOptions().get(""service"")).build();
+      defaultService.setService(params.getOptions().get(""service""));
     }
 
     for (CompactionKind ctype : CompactionKind.values()) {
       String service = params.getOptions().get(""service."" + ctype.name().toLowerCase());
-      if (service == null)
-        services.put(ctype, defaultService);
-      else
-        services.put(ctype, CompactionDirectives.builder().setService(service).build());
+      if (service != null)
+        defaultService.setService(service);","[{'comment': 'This is changing a static object used by many different dispatcher instance.  This would cause a config change on one table to affect all other tables.  Also the builder used to create immutable objects.', 'commenter': 'keith-turner'}, {'comment': 'It was not clear to me that this class was immutable, since it contained multiple `setter` methods. I think a POJO with final members set in the constructor would be a lot more clear.', 'commenter': 'milleruntime'}, {'comment': ""Changing CompactionDirectives from an interface w/ a builder to a POJO with a single constructor and final instance vars would be an alternative.  I intentionally chose not to take that route because I have seen too many things in Accumulo end up with lots over overloaded constructors and methods over time.  The builder pattern is a good way to avoid this.  If nothing is ever added the POJO would be better overall, but I like I mentioned with the ScanDirectives I never could have imagined adding the caching directives in 2.1.0 beforehand.     Also I don't think there is much complexity currently from the perspective of someone using the current incarnation of CompactionDirectives.  There is certainly a lot of complexity in the impl class and some of that comes from trying to avoid object allocations."", 'commenter': 'keith-turner'}, {'comment': 'Another alternative to the POJO would be an interface w/ static methods that create an instance.  This has the same potential problem as constructors, there could be lots of overloaded methods in the future.  I think one nice thing about an interface w/ a static method is that if in the future multiple things are going to be added then a static builder() method could be added.  \r\n\r\nSomething like the following in 2.1.0\r\n\r\n```java\r\npublic interface CompactionDirectives {\r\n\r\n  /**\r\n   * @return The service where a compaction should run.\r\n   */\r\n  CompactionServiceId getService();\r\n \r\n  static CompactionDirectives from(CompactionServiceId){...}\r\n}\r\n```\r\n\r\nThen in 2.2.0 we think of something else we want to return.\r\n\r\n```java\r\npublic interface CompactionDirectives {\r\n\r\n  /**\r\n   * @return The service where a compaction should run.\r\n   */\r\n  CompactionServiceId getService();\r\n \r\n  static CompactionDirectives from(CompactionServiceId){}\r\n /**\r\n  * @since 2.2.0\r\n  */\r\n  TypeX getX();\r\n\r\n /**\r\n  * @since 2.2.0\r\n  */\r\n static CompactionDirectives from(CompactionServiceId csid, TypeX x){}\r\n}\r\n```\r\n\r\nThen in 2.3.0 we think of 5 things we want to add and at that point just add a builder.\r\n\r\n```java\r\npublic interface CompactionDirectives {\r\n\r\n  /**\r\n   * @return The service where a compaction should run.\r\n   */\r\n  CompactionServiceId getService();\r\n \r\n  static CompactionDirectives from(CompactionServiceId){}\r\n /**\r\n  * @since 2.2.0\r\n  */\r\n  TypeX getX();\r\n\r\n /**\r\n  * @since 2.2.0\r\n  */\r\n static CompactionDirectives from(CompactionServiceId csid, TypeX x){}\r\n\r\n /**\r\n  * @since 2.3.0\r\n  */\r\n  interface Builder {...}\r\n\r\n /**\r\n  * @since 2.3.0\r\n  */\r\n  static Builder builder(); \r\n}\r\n```', 'commenter': 'keith-turner'}, {'comment': ""I am just trying to figure out if there is a way to make your highly evolved API code easier to understand for new developers. After playing around with it all afternoon, I still don't have a way to do that. I just worry because I _know_ that I have looked at the ScanDispatcher code before and yet, it still took me a few minutes to comprehend it again. Oh well, I digress."", 'commenter': 'milleruntime'}, {'comment': '> I am just trying to figure out if there is a way to make your highly evolved API code easier to understand for new developers.\r\n\r\nEasier to understand for an Accumulo user writing a compaction dispatcher or easier to understand for an Accumulo developer working on internal code?  For the latter, CompactionDirectivesImpl is complicated.. I may have gotten carried away trying to avoid object allocations which I think is at the root of the complexity.  For the former, if we want to clean up the user facing SPI it would good to do that before 2.1 is released.', 'commenter': 'keith-turner'}, {'comment': 'To simplify CompactionDirectivesImpl w/o changing the current SPI, could create two classes like the followin.\r\n\r\n```java\r\nclass CompactionDirectiveBuilder implements Builder {\r\n\r\n  private CompactionServiceId service;\r\n  \r\n  @Override\r\n  public Builder setService(CompactionServiceId service) {\r\n    this.service = service;\r\n    return this;\r\n  }\r\n\r\n  @Override\r\n  public Builder setService(String compactionServiceId) {\r\n    this.service = CompactionServiceId.of(compactionServiceId);\r\n    return this;\r\n  }\r\n\r\n  @Override\r\n  public CompactionDirectives build() {\r\n    return new CompactionsDirectiveImpl(service);\r\n  }\r\n  \r\n}\r\n\r\nclass CompactionsDirectiveImpl implements CompactionDirectives {\r\n\r\n  private final CompactionServiceId service;\r\n  \r\n  public CompactionsDirectiveImpl(CompactionServiceId service) {\r\n    this.service = service;\r\n  }\r\n\r\n  @Override\r\n  public CompactionServiceId getService() {\r\n    return service;\r\n  }\r\n\r\n  @Override\r\n  public String toString() {\r\n    return ""service="" + service;\r\n  }\r\n}\r\n```', 'commenter': 'keith-turner'}, {'comment': '> Easier to understand for an Accumulo user writing a compaction dispatcher or easier to understand for an Accumulo developer working on internal code?\r\n\r\nI think both would be good goals. But I was first targeting the latter. \r\n\r\nI think splitting the two implementations is a big help. I will replace the changes on this PR with the two separate classes.', 'commenter': 'milleruntime'}]"
1950,core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionsDirectiveImpl.java,"@@ -18,64 +18,28 @@
  */
 package org.apache.accumulo.core.spi.compaction;
 
-import java.util.Objects;
-
-import org.apache.accumulo.core.spi.compaction.CompactionDirectives.Builder;
-
-import com.google.common.base.Preconditions;
-
 /**
- * This class intentionally package private. This implementation is odd because it supports zero
- * object allocations for {@code CompactionDirectives.builder().build()}.
+ * This class intentionally package private. It supports one object allocation for
+ * {@code CompactionDirectives}.","[{'comment': ""```suggestion\r\n * This class intentionally package private.\r\n```\r\n\r\nThe old code implemented a copy on write strategy to avoid object allocation when building the default.  If that is to be removed I don't think a comment is needed saying objects are allocated."", 'commenter': 'keith-turner'}, {'comment': ""I am going to rename this class to `CompactionDirectivesImpl`. This follows the interface name. It was also the only class with an 's' on Compaction for some reason, possibly a typo."", 'commenter': 'milleruntime'}]"
1950,core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionDirectives.java,"@@ -43,6 +43,6 @@
   }
 
   public static Builder builder() {
-    return CompactionsDirectiveImpl.DEFAULT_BUILDER;
+    return CompactionDirectivesBuilder.DEFAULT_BUILDER;","[{'comment': 'If there are two concurrent threads they would both be using the same object.\r\n\r\n```suggestion\r\n    return new CompactionDirectivesBuilder();\r\n```', 'commenter': 'keith-turner'}]"
1950,core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionDirectivesBuilder.java,"@@ -0,0 +1,44 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.compaction;
+
+class CompactionDirectivesBuilder implements CompactionDirectives.Builder {
+
+  static final CompactionDirectives.Builder DEFAULT_BUILDER = new CompactionDirectivesBuilder();","[{'comment': '```suggestion\r\n```', 'commenter': 'keith-turner'}]"
1950,core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionDirectivesBuilder.java,"@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.compaction;
+
+class CompactionDirectivesBuilder implements CompactionDirectives.Builder {
+
+  private CompactionServiceId service;","[{'comment': '```suggestion\r\n  private static final CompactionServiceId DEFAULT_SERVICE_ID=CompactionServiceId.of(""default"");\r\n  private CompactionServiceId service = DEFAULT_SERVICE_ID;\r\n```', 'commenter': 'keith-turner'}, {'comment': ""I don't see why we need to have a hard-coded default name at all. Why can't this just always be a required parameter provided by the dispatcher?"", 'commenter': 'ctubbsii'}, {'comment': 'That is kinda what I was going for by putting the name of ""default"" set in the `SimpleCompactionDispatcher`', 'commenter': 'milleruntime'}, {'comment': ""> I don't see why we need to have a hard-coded default name at all. Why can't this just always be a required parameter provided by the dispatcher?\r\n\r\nMy thinking was that when the builder builds a default obj that it reflects the defaults in config."", 'commenter': 'keith-turner'}]"
1950,core/src/main/java/org/apache/accumulo/core/spi/compaction/SimpleCompactionDispatcher.java,"@@ -72,7 +72,7 @@
   public void init(InitParameters params) {
     services = new EnumMap<>(CompactionKind.class);
 
-    var defaultService = CompactionDirectives.builder().build();
+    var defaultService = CompactionDirectives.builder().setService(""default"").build();","[{'comment': '```suggestion\r\n    var defaultService = CompactionDirectives.builder().build();\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Or, could use the name of the `SimpleCompactionDispatcher`?', 'commenter': 'ctubbsii'}, {'comment': 'The service name is used to get compaction service config and uniquely identify the service within the tserver.\r\n\r\nCompaction service properties have the following prefix.\r\n```\r\ntserver.compaction.major.service.<service id>\r\n```\r\n\r\nWith the default service currently having multiple default props w/ the following prefix.\r\n\r\n```\r\ntserver.compaction.major.service.default\r\n```\r\n\r\nUsing the classname  would not result in very nice default props.', 'commenter': 'keith-turner'}, {'comment': 'I am fine with the name. I just thought it made more sense setting the value of ""default"" in the `SimpleCompactionDispatcher` class because this _is_ the default implementation. Your suggestion puts this logic into the builder, which I don\'t think should have any of the default business logic.', 'commenter': 'milleruntime'}, {'comment': ""I agree my name change suggestion doesn't make sense, and that `default` makes more sense here, since that's the name of the service that `SimpleCompactionDispatcher` is dispatching *to*. So, I agree with @milleruntime more than @keith-turner here, and think that setting it in `SimpleCompactionDispatcher` is more intuitive. I would rather the builder simply have it as a required parameter.\r\n\r\nAlso, I think a lot of this would be more clear if the Directive were called a Dispatch. Then, it would be more clear that what is being built by the builder is the responsibility of the Dispatcher, because it is the Dispatcher's job to set the name of the service that the Dispatch is being dispatched *to*."", 'commenter': 'ctubbsii'}, {'comment': ""> Also, I think a lot of this would be more clear if the Directive were called a Dispatch. Then, it would be more clear that what is being built by the builder is the responsibility of the Dispatcher, because it is the Dispatcher's job to set the name of the service that the Dispatch is being dispatched to.\r\n\r\nThis seems reasonable to me. I agree with @keith-turner though that the name should be consistent with the `ScanDispatcher`, since they follow similar patterns. I didn't want to touch any of the Scan* classes in this PR though, as it was intended to clarify `CompactionDirectives` so I think renaming them both could be done separately."", 'commenter': 'milleruntime'}, {'comment': 'There are three additional cases to consider.\r\n\r\n * Future additions of fields to CompactionDirectives would require that the builder be able to default those or require the user to specify them breaking existing code written against the SPI. ScanDirectives currently has three fields and one can do ScanDirectives.builder().build() to create a directives with defaults for all three fields.\r\n * Users writing their own CompactionDispatcher who want to fall back to the default service would need to use the string `default` in their own code.\r\n * A user can write   CompactionDirectives.builder().build() and it would compile, but I think would result in an NPE at runtime.  If its going to be required, then it would probably be best to structure the code such that its impossible to produce code that compiles w/o specifying it.\r\n\r\nThe three cases lead me to support the user not having to specify the default when building a CompactionDirectives', 'commenter': 'keith-turner'}, {'comment': 'I see what you are saying that a call to `CompactionDirectives.builder().build()` should work properly and set the default values. Typically this is fine if the values set in there will work for most purposes. This could be my limited understanding of the new compaction code but I think the problem here is that the default values for `CompactionDirectives` are tied directly to the implementation in `SimpleCompactionDispatcher`. \r\n\r\nYes, if I wrote my own CustomCompactionDispatcher, it should keep working with future updates to CompactionDirectives interface. But why would my CustomCompactionDispatcher call CompactionDirectives.builder().build() ? That would set the values for SimpleCompactionDispatcher. Wouldn\'t CustomCompactionDispatcher call CompactionDirectives.builder().setService(""customCD"").build() ?', 'commenter': 'milleruntime'}, {'comment': ""> his could be my limited understanding of the new compaction code but I think the problem here is that the default values for CompactionDirectives are tied directly to the implementation in SimpleCompactionDispatcher\r\n\r\nThe default values in this case are more tied to the default in Accumulo's Property.java\r\n\r\n> But why would my CustomCompactionDispatcher call CompactionDirectives.builder().build() ?\r\n\r\nIf the javadoc states that one should not do that, then no one should.  Its just an API/SPI design principal that if an API/SPI can be designed such that user errors are caught at compile time instead of runtime, then compile time would be better if it can be done in a reasonable way."", 'commenter': 'keith-turner'}, {'comment': '> API/SPI can be designed such that user errors are caught at compile time instead of runtime\r\n\r\nFor sure. I can do that in this PR.', 'commenter': 'milleruntime'}, {'comment': ""> The default values in this case are more tied to the default in Accumulo's Property.java\r\n\r\nI had a question about this that perhaps you can clarify: am I using the terminology correctly if I say that there is a **major compaction service** named *default*, and that this **service** named *default*, by default, has 3 **executors** named *small*, *medium*, and *large*? If I have the terminology correct so far, is it correct to say that the only default value from the CompactionDirectives builder that ties to what's in Property.java is the name *default* to refer to the named **service**?\r\n\r\nMy understanding is that the CompactionDirectives is basically just assignment information to send the Directive (which I would call Dispatch) to the specified service. Is that correct?\r\n\r\nIf I am understanding all this correctly, then I think we should just make `setService()` a required method call before one is capable of calling `build()` (possibly calling it `toService()` since it's dispatching *to* a service). Since the responsibility of assigning the dispatch to a specific service is pretty much the only thing that the Dispatcher is responsible for doing, then I don't think it should rely on any defaults in the builder... but declare it explicitly."", 'commenter': 'ctubbsii'}, {'comment': ""> that this service named default, by default, has 3 executors named small, medium, and large?\r\n\r\nThat is correct and the service also has a planner configured in Property.java\r\n\r\n> only default value from the CompactionDirectives builder that ties to what's in Property.java is the name default to refer to the named service?\r\n\r\nThat is correct.\r\n\r\n> If I am understanding all this correctly, then I think we should just make setService() a required method call before one is capable of calling build() (possibly calling it toService()\r\n\r\nIf it's required at compile time then I would be ok w/ that change.  Also I agree setService is not a good name for a fluent builder, something else would be better like toService()."", 'commenter': 'keith-turner'}]"
1952,test/src/main/java/org/apache/accumulo/test/metrics/fate/ZooLockTest.java,"@@ -57,16 +57,19 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class ZooLockIT extends SharedMiniClusterBase {
+public class ZooLockTest {
+
+  private static ZooKeeperTestingServer szk = null;
 
   @BeforeClass
   public static void setup() throws Exception {
-    SharedMiniClusterBase.startMiniCluster();
+    szk = new ZooKeeperTestingServer();
+    szk.initPaths(""/accumulo/1234"");","[{'comment': 'You may want to consider using UUID.randomUUID().toString() - and if the UUID is used elsewhere, create / initialize it as a static final variable for the class.  This more closely emulates the Accumulo instance id / paths used by Accumulo. \r\n\r\nWith a quick glance I think because you are using the `super.create(path, data, acl, createMode);` later - will also create a path that is `/accumulo/[instance-id]`\r\n', 'commenter': 'EdColeman'}, {'comment': 'Good idea. Will include this in my next commit.', 'commenter': 'DomGarguilo'}]"
1952,test/src/main/java/org/apache/accumulo/test/metrics/fate/ZooLockTest.java,"@@ -16,7 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.accumulo.test.fate.zookeeper;
+package org.apache.accumulo.test.metrics.fate;","[{'comment': 'This should not be relocated to a metrics subpackage. It is not related to metrics.', 'commenter': 'ctubbsii'}]"
1952,test/src/main/java/org/apache/accumulo/test/metrics/fate/ZooLockTest.java,"@@ -57,16 +57,19 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class ZooLockIT extends SharedMiniClusterBase {
+public class ZooLockTest {","[{'comment': 'While it may not need to extend SharedMiniClusterBase anymore, it is still an integration test, as it is specifically testing the integration between Accumulo code and a test instance of ZooKeeper that is started as an extra process. It should keep its original name, `ZooLockIT`.\r\n\r\nThe existing `FateMetricsTest` should probably also be called `FateMetricsIT` for the same reason, if you want to rename it for consistency, as part of this PR.', 'commenter': 'ctubbsii'}, {'comment': 'This rule in our `pom.xml` will cause the build to fail if we keep the IT nomenclature and do not extend one of the classes (or add it to the test itself) that includes the import specified below: \r\n```\r\n<classAnnotationPattern>org[.]junit[.]experimental[.]categories[.]Category</classAnnotationPattern>\r\n```\r\nhttps://github.com/apache/accumulo/blob/b12d1103d788473168616f28ae87a65bb76aa880/pom.xml#L1176-L1183', 'commenter': 'Manno15'}, {'comment': ""I agree that this should be an IT. I was running into the issue that @Manno15 has outlined. I'm not sure what the best way to overcome this issue might be and am open to suggestions."", 'commenter': 'DomGarguilo'}, {'comment': 'The simplest solution is to just create a new annotation to use as a category of test. Perhaps `start/src/main/java/org/apache/accumulo/test/categories/ZooKeeperTests`', 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii, the most recent commit should include the remaining changes you have mentioned.', 'commenter': 'DomGarguilo'}]"
1952,test/src/main/java/org/apache/accumulo/test/metrics/fate/ZooMutatorTest.java,"@@ -16,31 +16,26 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.accumulo.test.functional;
+package org.apache.accumulo.test.metrics.fate;","[{'comment': 'Not a metrics test.', 'commenter': 'ctubbsii'}]"
1952,test/src/main/java/org/apache/accumulo/test/metrics/fate/ZooMutatorTest.java,"@@ -16,31 +16,26 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.accumulo.test.functional;
+package org.apache.accumulo.test.metrics.fate;
 
 import static java.nio.charset.StandardCharsets.UTF_8;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.UUID;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
 
-import org.apache.accumulo.core.client.Accumulo;
-import org.apache.accumulo.core.clientImpl.ClientContext;
-import org.apache.accumulo.core.conf.Property;
 import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
-import org.apache.accumulo.harness.AccumuloClusterHarness;
-import org.apache.accumulo.test.categories.MiniClusterOnlyTests;
+import org.apache.accumulo.test.zookeeper.ZooKeeperTestingServer;
 import org.junit.Test;
-import org.junit.experimental.categories.Category;
 
 import com.google.common.hash.Hashing;
 
-@Category(MiniClusterOnlyTests.class)
-public class ZooMutatorIT extends AccumuloClusterHarness {
+public class ZooMutatorTest {","[{'comment': 'Still an IT.', 'commenter': 'ctubbsii'}]"
1952,test/src/main/java/org/apache/accumulo/test/metrics/fate/ZooMutatorTest.java,"@@ -75,66 +70,65 @@
    */
   @Test
   public void concurrentMutatorTest() throws Exception {
-    try (var client = Accumulo.newClient().from(getClientProps()).build();
-        var context = (ClientContext) client) {
-      String secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
 
-      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
-          context.getZooKeepersSessionTimeOut(), secret);
+    ZooKeeperTestingServer szk = new ZooKeeperTestingServer();
+    szk.initPaths(""/accumulo/"" + UUID.randomUUID().toString());
+    ZooReaderWriter zk = new ZooReaderWriter(szk.getConn(), 10_0000, ""aPasswd"");
 
-      var executor = Executors.newFixedThreadPool(16);
+    var executor = Executors.newFixedThreadPool(16);
 
-      String initialData = hash(""Accumulo Zookeeper Mutator test data"") + "" 0"";
+    String initialData = hash(""Accumulo Zookeeper Mutator test data"") + "" 0"";
 
-      List<Future<?>> futures = new ArrayList<>();
+    List<Future<?>> futures = new ArrayList<>();
 
-      // This map is used to ensure multiple threads do not successfully write the same value and no
-      // values are skipped. The hash in the value also verifies similar things in a different way.
-      ConcurrentHashMap<Integer,Integer> countCounts = new ConcurrentHashMap<>();
+    // This map is used to ensure multiple threads do not successfully write the same value and no
+    // values are skipped. The hash in the value also verifies similar things in a different way.
+    ConcurrentHashMap<Integer,Integer> countCounts = new ConcurrentHashMap<>();
 
-      for (int i = 0; i < 16; i++) {
-        futures.add(executor.submit(() -> {
-          try {
+    for (int i = 0; i < 16; i++) {
+      futures.add(executor.submit(() -> {
+        try {
 
-            int count = -1;
-            while (count < 200) {
-              byte[] val =
-                  zk.mutateOrCreate(""/test-zm"", initialData.getBytes(UTF_8), this::nextValue);
-              int nextCount = getCount(val);
-              assertTrue(""nextCount <= count "" + nextCount + "" "" + count, nextCount > count);
-              count = nextCount;
-              countCounts.merge(count, 1, Integer::sum);
-            }
-
-          } catch (Exception e) {
-            throw new RuntimeException(e);
+          int count = -1;
+          while (count < 200) {
+            byte[] val =
+                zk.mutateOrCreate(""/test-zm"", initialData.getBytes(UTF_8), this::nextValue);
+            int nextCount = getCount(val);
+            assertTrue(""nextCount <= count "" + nextCount + "" "" + count, nextCount > count);
+            count = nextCount;
+            countCounts.merge(count, 1, Integer::sum);
           }
-        }));
-      }
 
-      // wait and check for errors in background threads
-      for (Future<?> future : futures) {
-        future.get();
-      }
-      executor.shutdown();
+        } catch (Exception e) {
+          throw new RuntimeException(e);
+        }
+      }));
+    }
 
-      byte[] actual = zk.getData(""/test-zm"");
-      int settledCount = getCount(actual);
+    // wait and check for errors in background threads
+    for (Future<?> future : futures) {
+      future.get();
+    }
+    executor.shutdown();
 
-      assertTrue(settledCount >= 200);
+    byte[] actual = zk.getData(""/test-zm"");
+    int settledCount = getCount(actual);
 
-      String expected = initialData;
+    assertTrue(settledCount >= 200);
 
-      assertEquals(1, (int) countCounts.get(0));
+    String expected = initialData;
 
-      for (int i = 1; i <= settledCount; i++) {
-        assertEquals(1, (int) countCounts.get(i));
-        expected = nextValue(expected);
-      }
+    assertEquals(1, (int) countCounts.get(0));
 
-      assertEquals(settledCount + 1, countCounts.size());
-      assertEquals(expected, new String(actual, UTF_8));
+    for (int i = 1; i <= settledCount; i++) {
+      assertEquals(1, (int) countCounts.get(i));
+      expected = nextValue(expected);
     }
+
+    assertEquals(settledCount + 1, countCounts.size());
+    assertEquals(expected, new String(actual, UTF_8));
+
+    szk.close();","[{'comment': 'Instead of a close here, the szk should be declared as a resource in a try-with-resources block, so it gets closed, even if the other code throws an exception.', 'commenter': 'ctubbsii'}, {'comment': 'ZooKeeperTestingServer does not implement auto-closable so it wont work in a regular try-with-resources block. I could either make the ZooKeeperTestingServer a member variable and use a try-finally to close it or, have a `@BeforeClass` and `@AfterClass` to initialize and then close it. Not sure which would be best in this case.', 'commenter': 'DomGarguilo'}, {'comment': ""It has a close method, so it might as well implement the AutoCloseable interface. Whether you use the before/after class methods or try-with-resources or try-finally pattern, might depend on case-by-case basis, but there's no reason not to add the interface to the class, since it already has an implementing method."", 'commenter': 'ctubbsii'}, {'comment': 'These changes are included in the most recent commits.', 'commenter': 'DomGarguilo'}]"
1952,test/src/main/java/org/apache/accumulo/test/zookeeper/fate/ZooLockIT.java,"@@ -16,7 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.accumulo.test.fate.zookeeper;
+package org.apache.accumulo.test.zookeeper.fate;","[{'comment': ""`fate.zookeeper` makes slightly more sense than `zookeeper.fate` because 1) it avoids one relocation, which is always nicer on git archeology, and 2) it more closely matches the package name of the corresponding code it is testing.\r\n\r\nThere's technically no reason why it should have `fate` in the package name at all. That's leftover from this code being in a separate jar. It's not fate-specific code. But, until the code itself is moved, the test code should probably stay in a package that resembles the code it is testing."", 'commenter': 'ctubbsii'}, {'comment': 'I have included this change in the most recent commit.', 'commenter': 'DomGarguilo'}]"
1952,test/src/main/java/org/apache/accumulo/test/zookeeper/fate/ZooMutatorIT.java,"@@ -75,12 +73,10 @@
    */
   @Test
   public void concurrentMutatorTest() throws Exception {
-    try (var client = Accumulo.newClient().from(getClientProps()).build();
-        var context = (ClientContext) client) {
-      String secret = cluster.getSiteConfiguration().get(Property.INSTANCE_SECRET);
 
-      ZooReaderWriter zk = new ZooReaderWriter(context.getZooKeepers(),
-          context.getZooKeepersSessionTimeOut(), secret);
+    try (ZooKeeperTestingServer szk = new ZooKeeperTestingServer();) {","[{'comment': 'nit: extra semicolon\r\n\r\n```suggestion\r\n    try (ZooKeeperTestingServer szk = new ZooKeeperTestingServer()) {\r\n```', 'commenter': 'ctubbsii'}]"
1958,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooQueueLock.java,"@@ -32,7 +33,7 @@
 
 public class ZooQueueLock implements QueueLock {
 
-  private static final String PREFIX = ""lock-"";
+  private static final String PREFIX = ""zlock#"" + UUID.randomUUID() + ""#"";","[{'comment': ""If it is required that this PREFIX here and the ZLOCK_PREFIX in ZooLock match, then maybe the definition should be placed in Constants and shared.  (Not saying that the should be the same, that's a different discussion)  "", 'commenter': 'EdColeman'}, {'comment': 'In general I think we should look for another solution - if something is named like a zoo lock, then it should be managed by ZooLock - if it is not a zoo lock, then it should be clear that its usage / management in zookeeper is handled by code other than ZooLock.', 'commenter': 'EdColeman'}, {'comment': 'I think the parts that have to match is the ""zlock#"" and then also end in a ""#"". The first part could definitely be placed in Constants. \r\n\r\nI guess the issue is the `CreateTable` command uses `ZooQueueLock` for creation and the `FateCommand` uses `ZooLock` for validation. ', 'commenter': 'Manno15'}, {'comment': 'Constants.java has slowly been in the process of being phased out, as constants move closer to the code that is responsible for them, under the premise that fewer global constants increase modularity and separation of responsibilities.\r\n\r\nIf the schema is defined by ZooLock and ZooQueueLock is trying to match it, then the constant should live in ZooLock.', 'commenter': 'ctubbsii'}]"
1958,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooQueueLock.java,"@@ -111,4 +115,50 @@ public void removeEntry(long entry) {
       throw new RuntimeException(ex);
     }
   }
+
+  public static List<String> validateAndSortChildrenByLockPrefix(String path,
+      List<String> children) {
+    log.trace(""validating and sorting children at path {}"", path);
+    List<String> validChildren = new ArrayList<>();
+    if (children == null || children.isEmpty()) {
+      return validChildren;
+    }
+    children.forEach(c -> {
+      log.trace(""Validating {}"", c);
+      if (c.startsWith(PREFIX)) {
+        int idx = c.indexOf('#');
+        String sequenceNum = c.substring(idx + 1);
+        if (sequenceNum.length() == 10) {
+          try {
+            log.trace(""Testing number format of {}"", sequenceNum);
+            Integer.parseInt(sequenceNum);
+            validChildren.add(c);
+          } catch (NumberFormatException e) {
+            log.warn(""Child found with invalid sequence format: {} (not a number)"", c);","[{'comment': 'It may be more descriptive if, instead of Child... if it was FateLock (or flock) or something that is more descriptive than generic child - this applies in multiple places.', 'commenter': 'EdColeman'}, {'comment': 'I made some log changes in my latest commit', 'commenter': 'Manno15'}, {'comment': '@EdColeman Thoughts on my recent log changes? Is that more descriptive and do you think I should make those same log changes in the validation function inside `ZooLock`? If you approve then I will get this merged in today. ', 'commenter': 'Manno15'}, {'comment': ""I'll look at zoo lock.  That should not hold up this request.  I think this can be merged unless others have issues."", 'commenter': 'EdColeman'}]"
1959,minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java,"@@ -109,6 +109,14 @@ public void stop() throws IOException, InterruptedException {
     impl.stop();
   }
 
+  /**
+   * @since 2.0.1
+   */
+  @Override
+  public void close() throws IOException, InterruptedException {
+    this.stop();
+  }
+","[{'comment': ""It would probably be better to not throw InterruptedException, but to catch it, and reset the current thread's interrupt status. I found this good explanation of how to handle these: https://dzone.com/articles/how-to-handle-the-interruptedexception"", 'commenter': 'ctubbsii'}, {'comment': 'The most recent commit contains my attempt to include this suggestion. @ctubbsii ', 'commenter': 'DomGarguilo'}]"
1959,minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java,"@@ -98,6 +98,21 @@ public MiniAccumuloConfig setZooKeeperPort(int zooKeeperPort) {
     return this;
   }
 
+  /**
+   * Configure an existing ZooKeeper instance to use. Calling this method is optional. If not set, a
+   * new ZooKeeper instance is created.
+   *
+   * @param existingZooKeepers
+   *          Connection string for a already-running ZooKeeper instance. A null value will turn off
+   *          this feature.
+   *
+   * @since 2.0.1","[{'comment': '```suggestion\r\n   * @since 2.1.0\r\n```', 'commenter': 'ctubbsii'}]"
1965,core/src/main/java/org/apache/accumulo/core/util/ratelimit/SharedRateLimiterFactory.java,"@@ -112,9 +112,9 @@ protected void update() {
     }
     for (Map.Entry<String,WeakReference<SharedRateLimiter>> entry : limitersCopy.entrySet()) {
       try {
-        if (Objects.nonNull(entry.getValue().get())) {
-          entry.getValue().get().update();
-        }
+        Objects.requireNonNull(entry.getValue().get()).update();
+      } catch (NullPointerException npex) {","[{'comment': 'These changes were added to avoid spotbugs throwing  a ""Possible null pointer dereference"" error. It seems like adding the extra catch block for NPE is redundant and a single catch for `Exception` should be sufficient, however without it spotbugs will throw the mentioned error.\r\n\r\nI am wondering if it would be best to suppress spotbugs in this case and let the singular `Exception` catch block handle the NPE. I am open to suggestions on this.', 'commenter': 'DomGarguilo'}, {'comment': ""Spotbugs is signaling that WeakReferences stored in the map may actually be a reference to null, if the referenced object had been garbage collected, but not yet removed from the map. It would be better to avoid inserting any weak references into this copy, since the whole point is to get strong references to call update on them. It'd probably be enough to simply have an `if` statement to check to see if they are null before calling `update` on them after the copy.\r\n\r\nAlternatively, you could map the values by calling `WeakReference.get()` on the values, and filtering out the null ones. And, since we don't use the keys, it's even easier. Something like:\r\n\r\n```java\r\n  List<SharedRateLimiter> limitersCopy = new ArrayList<>();\r\n  synchronized(activeLimiters) {\r\n    limitersCopy = activeLImiters.values().stream().filter(Objects::nonNull).collect(Collectors.toList());\r\n  }\r\n  limitersCopy.forEach(SharedRateLimiter::update);\r\n```\r\n\r\nOf course, it might not be exactly that simple, because I just realized we *do* use the keys for the log message if there's an exception. But hopefully you get the point... filter out or check for nulls from the copy before calling update, because WeakReference.get() can be null."", 'commenter': 'ctubbsii'}]"
1965,core/src/main/java/org/apache/accumulo/core/util/ratelimit/SharedRateLimiterFactory.java,"@@ -87,12 +90,12 @@ public static synchronized SharedRateLimiterFactory getInstance(AccumuloConfigur
   public RateLimiter create(String name, RateProvider rateProvider) {
     synchronized (activeLimiters) {
       if (activeLimiters.containsKey(name)) {
-        return activeLimiters.get(name);
+        return activeLimiters.get(name).get();
       } else {
         long initialRate;
         initialRate = rateProvider.getDesiredRate();
         SharedRateLimiter limiter = new SharedRateLimiter(name, rateProvider, initialRate);
-        activeLimiters.put(name, limiter);
+        activeLimiters.put(name, new WeakReference<>(limiter));
         return limiter;","[{'comment': ""The typical pattern with WeakReferences in a WeakHashMap should be to check if the `weakReference.get()` is null. If it's null, it shouldn't be returned, but instead recomputed, as in the `else` clause. That is because map entries in the WeakHashMap may not be cleaned up yet, even if the WeakReference is already null.\r\n\r\nAlso, this whole if/else block may be more easily implemented inside a lambda passed to `activeLimiters.compute()`."", 'commenter': 'ctubbsii'}, {'comment': ""5c274bf should address your suggestions @ctubbsii. I'm not sure if I understand your suggestion on using a lambda: \r\n \r\n> Also, this whole if/else block may be more easily implemented inside a lambda passed to `activeLimiters.compute()`.\r\n\r\n"", 'commenter': 'DomGarguilo'}, {'comment': ""What I was referring to was the ability to use something like:\r\n\r\n```java\r\n  var storedValue = map.compute(key, (key, oldValue) -> {\r\n    if (oldValue != null) {\r\n      return oldValue; // no change\r\n    }\r\n    return generateNewValueForKey(key);\r\n  });\r\n```\r\n\r\nIt won't work in this case, because we'd be storing a WeakReference in the value, and we'd need to keep a strong reference around before returning from the lambda inside the compute method. We could do that by setting a final AtomicReference from within the compute method... but it's more complicated than just using get and put instead of compute, so it's not worth it. See my PR against your PR for a better suggestion."", 'commenter': 'ctubbsii'}]"
1965,core/src/main/java/org/apache/accumulo/core/util/ratelimit/SharedRateLimiterFactory.java,"@@ -149,7 +161,7 @@ protected void report() {
     }
 
     @Override
-    public void acquire(long permits) {
+    public synchronized void acquire(long permits) {","[{'comment': ""This synchronization seems unrelated. What's this for?"", 'commenter': 'ctubbsii'}, {'comment': 'Oh yes, I forgot to make a comment about that but intellij was displaying a warning ""Non-atomic operation on volatile field \'permitsAcquired\'"". Adding `synchronized` seems to fix this issue. ', 'commenter': 'DomGarguilo'}, {'comment': ""I suspect this warning is because of `+=` effectively being two operations on `permitsAcquired`. That warning might possibly be more appropriately addressed by making `permitsAcquired` an `AtomicLong` and using that type's increment methods. That would probably be better than synchronizing on the entire object as this keyword would do."", 'commenter': 'ctubbsii'}, {'comment': 'Good idea, I will look into doing that instead.', 'commenter': 'DomGarguilo'}, {'comment': 'I found an explanation of two reads/writes on the variable at https://stackoverflow.com/a/19744523/196405', 'commenter': 'ctubbsii'}, {'comment': 'The most recent commit converts to AtomicLong and its equivalent operations.', 'commenter': 'DomGarguilo'}]"
2020,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/modals.ftl,"@@ -45,6 +45,10 @@
                 <div class=""col-sm-4 text-right"">Instance&nbsp;Id</div>
                 <div class=""col-sm-6 text-left"">${instance_id}</div>
               </div>
+              <div class=""row"">
+                <div class=""col-sm-4 text-right"">Zookeeper&nbsp;Hosts</div>","[{'comment': 'Trying to keep the capitalization consistent with how the ZooKeeper project presents it on their website, the `K` should be uppercase.\r\n\r\n```suggestion\r\n                <div class=""col-sm-4 text-right"">ZooKeeper&nbsp;Hosts</div>\r\n```', 'commenter': 'ctubbsii'}]"
2020,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/overview.ftl,"@@ -24,7 +24,7 @@
         </div>
       </div>
       <div class=""row"">
-        <div class=""col-sm-6"" id=""manager"">
+        <div class=""col-sm-6 col-sm-offset-3"" id=""manager"">","[{'comment': 'I think the information in this table can probably be ""prettified"" a bit, by moving it out of the HTML table format, and into some other format for presentation, but centering it is an acceptable interim solution.', 'commenter': 'ctubbsii'}, {'comment': ""Hi @ctubbsii, If you have any thoughts/ideas on the format let me know I'm happy to work on it. Thanks"", 'commenter': 'karthick-rn'}, {'comment': 'I was thinking something like these boxes in this bootstrap example: https://getbootstrap.com/docs/4.0/examples/pricing/\r\nBut, I would defer to somebody more experienced with site design. This comment was mostly just a passing thought about how we could probably get rid of the table format for the information at the top of the overview page.', 'commenter': 'ctubbsii'}]"
2055,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java,"@@ -98,7 +98,7 @@
   private final ExecutorService assignmentPool;
   private final ExecutorService assignMetaDataPool;
   private final ExecutorService summaryRetrievalPool;
-  private final ExecutorService summaryParitionPool;
+  private final ExecutorService summaryParititionPool;","[{'comment': 'This should be summaryPartitionPool', 'commenter': 'EdColeman'}, {'comment': 'done', 'commenter': 'himanshu007-creator'}]"
2065,core/src/test/java/org/apache/accumulo/core/util/NumUtilTest.java,"@@ -22,12 +22,15 @@
 import static org.apache.accumulo.core.util.NumUtil.bigNumberForSize;
 import static org.junit.Assert.assertEquals;
 
+import java.util.Locale;
+
 import org.junit.Test;
 
 public class NumUtilTest {
 
   @Test
   public void testBigNumberForSize() {
+    Locale.setDefault(Locale.US);","[{'comment': ""This change seems unrelated. Is it necessary? If it's an issue, it could be a separate PR."", 'commenter': 'ctubbsii'}, {'comment': 'I created https://github.com/apache/accumulo/pull/2072 - that will separate this issue into its own commit. Once that change is merged, this can be re-based and merged. ', 'commenter': 'EdColeman'}]"
2068,.github/CODE_OF_CONDUCT.md,"@@ -0,0 +1,84 @@
+# Contributor Code of Conduct","[{'comment': 'Is this a copy from a different source? Is it from https://www.apache.org/foundation/policies/conduct ? Just curious in case we want to keep it up-to-date with the source material, if that gets updated.', 'commenter': 'ctubbsii'}, {'comment': 'It is the same copy from: https://www.apache.org/foundation/policies/conduct. Do you say it by the title? Confusion of mine ...', 'commenter': 'nicolasalarconrapela'}, {'comment': ""I just wanted to understand where it came from. You could mention it in the git commit log message. However, I think it'd be better to follow my proposal to create this in a central place for all ASF projects, instead of adding it to the Accumulo repository. See https://github.com/apache/accumulo/pull/2068#issuecomment-830654865"", 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -15,42 +15,97 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-[![Apache Accumulo][logo]][accumulo]
---
-[![Build Status][ti]][tl] [![Maven Central][mi]][ml] [![Javadoc][ji]][jl] [![Apache License][li]][ll]
-
-[Apache Accumulo][accumulo] is a sorted, distributed key/value store that provides robust,
-scalable data storage and retrieval. With Apache Accumulo, users can store and manage large
+<!-- LOGO -->","[{'comment': ""I don't see any reason to change the the markdown formatting to raw HTML, or change the layout/style of badges or the logo. And, there definitely shouldn't be a change to link to vuejs's site. I assume that was copied/pasted from another project and is a mistake, but I don't think any of this should be changed at all."", 'commenter': 'ctubbsii'}, {'comment': ""> I don't see any reason to change the the markdown formatting to raw HTML, or change the layout/style of badges or the logo. \r\n \r\nThe main idea is to make it look like this:\r\n![image](https://user-images.githubusercontent.com/23703592/116788862-aa202280-aab4-11eb-8e12-c4f93739098e.png)\r\n\r\nThe only thing that has been modified is to make it look centered (and with that it would become an HTML format) the rest remains unchanged\r\n\r\n---\r\n> And, there definitely shouldn't be a change to link to vuejs's site. I assume that was copied/pasted from another project and is a mistake, but I don't think any of this should be changed at all.\r\n\r\nAt the moment VueJs does not sponsor Apache ... Although I never know ... \r\n_This kind of 'slip' is my signature, my apologies!_\r\nIt would be a matter of modifying the link .. :)"", 'commenter': 'nicolasalarconrapela'}, {'comment': ""I think it's better to stick with the Markdown instead of the HTML. I don't think centering is necessarily better or worth it. The Markdown is easier to maintain, and is the convention for many projects on GitHub, especially for the badges."", 'commenter': 'ctubbsii'}, {'comment': ""> I don't think centering is necessarily better or worth it.\r\n\r\nHonestly, I see it as more minimalist, detailed ... _Hahaha I appeal to democracy ... would there be a way to vote?_\r\n\r\n> The Markdown is easier to maintain, .(...) , especially for the badges.\r\n\r\nIn my humble opinion regarding maintenance (in this case) there would be no problem because the images and badges are referenced in an external link and therefore it does not matter to use markdown or html. Another thing is that there is behind a link automation or something similar.\r\n\r\n> and is the convention for many projects on GitHub\r\n\r\nThere are projects like those of [Facebook](https://github.com/facebook/react-native), [Vue Js](https://github.com/vuejs/vue) that use html in their README"", 'commenter': 'nicolasalarconrapela'}, {'comment': ""@nicolasalarconrapela We don't just use Markdown because it's shorthand for HTML. We also use it because it's more human-readable. The less HTML, the better. We don't need HTML. This README is also included in the tarball we generate. So, it should be readable in a user's text editor, and the reader should not have to parse HTML/XML tags to read it. Centering the image, in my opinion, does not justify adding HTML that is harder for users to view using a plain text viewer."", 'commenter': 'ctubbsii'}, {'comment': ""Perfect !!! I think it's great !!"", 'commenter': 'nicolasalarconrapela'}]"
2068,README.md,"@@ -15,42 +15,97 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-[![Apache Accumulo][logo]][accumulo]
---
-[![Build Status][ti]][tl] [![Maven Central][mi]][ml] [![Javadoc][ji]][jl] [![Apache License][li]][ll]
-
-[Apache Accumulo][accumulo] is a sorted, distributed key/value store that provides robust,
-scalable data storage and retrieval. With Apache Accumulo, users can store and manage large
+<!-- LOGO -->
+<p align=""center"">
+    <a href=""https://vuejs.org"" target=""_blank"" rel=""noopener noreferrer"">
+        <img src=""contrib/accumulo-logo.png"" alt=""Vue logo"">
+    </a>
+</p>
+
+<!-- BADGES -->
+<p align=""center"">
+    <!-- Build Status -->
+    <a href=""https://github.com/apache/accumulo/actions"">
+        <img src=""https://github.com/apache/accumulo/workflows/QA/badge.svg"" alt=""Build Status"">
+    </a>
+    <!-- Maven Central -->
+    <a href=""https://maven-badges.herokuapp.com/maven-central/org.apache.accumulo/accumulo-core"">
+        <img src=""https://maven-badges.herokuapp.com/maven-central/org.apache.accumulo/accumulo-core/badge.svg""
+            alt=""Maven Central"">
+    </a>
+    <!-- JavaDocs -->
+    <a href=""https://www.javadoc.io/doc/org.apache.accumulo/accumulo-core"">
+        <img src=""https://www.javadoc.io/badge/org.apache.accumulo/accumulo-core.svg"" alt=""JavaDocs"">
+    </a>
+    <!-- Apache License -->
+    <a href=""https://www.apache.org/licenses/LICENSE-2.0"">
+        <img src=""https://img.shields.io/badge/license-ASL-blue.svg"" alt=""Apache License"">
+    </a>
+</p>
+
+# About","[{'comment': ""I'm opposed to expanding the README to duplicate information on the website. We previously took efforts to substantially minimize the README to make it easier to read, and keep more comprehensive information up-to-date on the website instead. The README really should be minimal, so we don't have to maintain the same information in multiple places. A link to the website should be fine.\r\n\r\nKeep in mind that the README is also included in the release tarball, so links to files/pages not in the tarball won't work. That includes everything in `.github/`.\r\n\r\nAlso, links to specific pages on the website should be minimized. Since this file is included in the release tarball at the time of release, it should endeavor to show information that will not be invalidated by website redesigns or reorganizations."", 'commenter': 'ctubbsii'}, {'comment': ""> I'm opposed to expanding the README to duplicate information on the website. We previously took efforts to substantially minimize the README to make it easier to read, and keep more comprehensive information up-to-date on the website instead. The README really should be minimal, so we don't have to maintain the same information in multiple places. A link to the website should be fine.\r\n\r\nI see well that the sections are maintained ... Opinions ... :)\r\n\r\n> Keep in mind that the README is also included in the release tarball, so links to files/pages not in the tarball won't work. That includes everything in .github/.\r\n> Also, links to specific pages on the website should be minimized. Since this file is included in the release tarball at the time of release, it should endeavor to show information that will not be invalidated by website redesigns or reorganizations.\r\n\r\nIt seems good to me I will check the indexing of url's and that they comply with this commitment"", 'commenter': 'nicolasalarconrapela'}]"
2068,CHANGELOG.md,"@@ -0,0 +1,35 @@
+# CHANGELOG","[{'comment': ""Please remove this file. As a community, we decided a long time ago to eliminate maintaining a separate CHANGELOG. This concept predates development using version control systems, and is substantially less useful than the curated release notes that we create to highlight the changes we think users should be aware of. A separate CHANGELOG file creates extra maintenance burden on us, and adds no value that isn't already available from the git history and the release notes."", 'commenter': 'ctubbsii'}, {'comment': 'Whoops !!! This change was mine in theory it should never have gotten on ...', 'commenter': 'nicolasalarconrapela'}]"
2068,README.md,"@@ -15,42 +15,89 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-[![Apache Accumulo][logo]][accumulo]
---
-[![Build Status][ti]][tl] [![Maven Central][mi]][ml] [![Javadoc][ji]][jl] [![Apache License][li]][ll]
-
-[Apache Accumulo][accumulo] is a sorted, distributed key/value store that provides robust,
-scalable data storage and retrieval. With Apache Accumulo, users can store and manage large
+<!-- LOGO -->
+<p align=""center"">
+    <a href=""https://accumulo.apache.org/"" target=""_blank"" rel=""noopener noreferrer"">
+        <img src=""contrib/accumulo-logo.png"" alt=""Accumulo logo"">
+    </a>
+</p>
+
+<!-- BADGES -->
+<p align=""center"">
+    <!-- Build Status -->
+    <a href=""https://github.com/apache/accumulo/actions"">
+        <img src=""https://github.com/apache/accumulo/workflows/QA/badge.svg"" alt=""Build Status"">
+    </a>
+    <!-- Maven Central -->
+    <a href=""https://maven-badges.herokuapp.com/maven-central/org.apache.accumulo/accumulo-core"">
+        <img src=""https://maven-badges.herokuapp.com/maven-central/org.apache.accumulo/accumulo-core/badge.svg""
+            alt=""Maven Central"">
+    </a>
+    <!-- JavaDocs -->
+    <a href=""https://www.javadoc.io/doc/org.apache.accumulo/accumulo-core"">
+        <img src=""https://www.javadoc.io/badge/org.apache.accumulo/accumulo-core.svg"" alt=""JavaDocs"">
+    </a>
+    <!-- Apache License -->
+    <a href=""https://www.apache.org/licenses/LICENSE-2.0"">
+        <img src=""https://img.shields.io/badge/license-ASL-blue.svg"" alt=""Apache License"">
+    </a>
+</p>
+
+# About
+
+[Apache Accumulo®][accumulo] is a sorted, distributed key/value store that provides robust, scalable data storage and retrieval.
+
+With Apache Accumulo, users can store and manage large
 data sets across a cluster. Accumulo uses [Apache Hadoop]'s HDFS to store its data and
-[Apache Zookeeper] for consensus. Check out the [Accumulo project website][accumulo] for
-news and general information.
+[Apache Zookeeper] for consensus.
+
+Visit our 🌐 [project website][accumulo] for news and general information","[{'comment': ""Emoji characters may not show up in a user's system if they don't have a font with a visible glyph for it. Please avoid them."", 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -79,9 +126,14 @@ more details on bouncycastle's cryptography features.
 
 </details>
 
+<br/>
+
+# License
+
+[Apache License, Version 2.0](./LICENSE).","[{'comment': ""This is what the LICENSE file is for. It's not necessary to include it here as well."", 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -79,9 +126,14 @@ more details on bouncycastle's cryptography features.
 
 </details>
 
+<br/>
+
+# License","[{'comment': ""Do all of these sections really deserve to be `h1` headers? I don't think we need all these sections."", 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -15,42 +15,89 @@ See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-[![Apache Accumulo][logo]][accumulo]
---
-[![Build Status][ti]][tl] [![Maven Central][mi]][ml] [![Javadoc][ji]][jl] [![Apache License][li]][ll]
-
-[Apache Accumulo][accumulo] is a sorted, distributed key/value store that provides robust,
-scalable data storage and retrieval. With Apache Accumulo, users can store and manage large
+<!-- LOGO -->
+<p align=""center"">
+    <a href=""https://accumulo.apache.org/"" target=""_blank"" rel=""noopener noreferrer"">
+        <img src=""contrib/accumulo-logo.png"" alt=""Accumulo logo"">
+    </a>
+</p>
+
+<!-- BADGES -->
+<p align=""center"">
+    <!-- Build Status -->
+    <a href=""https://github.com/apache/accumulo/actions"">
+        <img src=""https://github.com/apache/accumulo/workflows/QA/badge.svg"" alt=""Build Status"">
+    </a>
+    <!-- Maven Central -->
+    <a href=""https://maven-badges.herokuapp.com/maven-central/org.apache.accumulo/accumulo-core"">
+        <img src=""https://maven-badges.herokuapp.com/maven-central/org.apache.accumulo/accumulo-core/badge.svg""
+            alt=""Maven Central"">
+    </a>
+    <!-- JavaDocs -->
+    <a href=""https://www.javadoc.io/doc/org.apache.accumulo/accumulo-core"">
+        <img src=""https://www.javadoc.io/badge/org.apache.accumulo/accumulo-core.svg"" alt=""JavaDocs"">
+    </a>
+    <!-- Apache License -->
+    <a href=""https://www.apache.org/licenses/LICENSE-2.0"">
+        <img src=""https://img.shields.io/badge/license-ASL-blue.svg"" alt=""Apache License"">
+    </a>
+</p>
+
+# About
+
+[Apache Accumulo®][accumulo] is a sorted, distributed key/value store that provides robust, scalable data storage and retrieval.
+
+With Apache Accumulo, users can store and manage large
 data sets across a cluster. Accumulo uses [Apache Hadoop]'s HDFS to store its data and
-[Apache Zookeeper] for consensus. Check out the [Accumulo project website][accumulo] for
-news and general information.
+[Apache Zookeeper] for consensus.
+
+Visit our 🌐 [project website][accumulo] for news and general information
+
+Download the latest version of Apache Accumulo® [here](https://accumulo.apache.org/downloads/).
+
+# Getting Started
+
+Follow the [quick start] to install and run Accumulo
+
+# Documentation
 
-## Getting Started
+Read the [Accumulo documentation][docs]
+
+# Examples
 
-* Follow the [quick start] to install and run Accumulo
-* Read the [Accumulo documentation][docs]
 * Run the [Accumulo examples][examples] to learn how to write Accumulo clients
 * View the [Javadocs][javadocs] to learn the [Accumulo API][api]
 
 More resources can be found on the [project website][accumulo].
 
-## Building
+# Building
+
+Accumulo uses [Maven] to compile, [test], and package its source.
 
-Accumulo uses [Maven] to compile, [test], and package its source. The following
-command will build the binary tar.gz from source. Add `-DskipTests` to build without
-waiting for the tests to run.
+The following
+command :
 
     mvn package
 
-This command produces `assemble/target/accumulo-<version>-bin.tar.gz`
+will build the binary `tar.gz` (`assemble/target/accumulo-<version>-bin.tar.gz`) from source.
+> _Add `-DskipTests` to build without waiting for the tests to run._","[{'comment': ""This shouldn't be a blockquote. It's not quoting anything/anyone."", 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -20,11 +20,14 @@ limitations under the License.
 [![Build Status][ti]][tl] [![Maven Central][mi]][ml] [![Javadoc][ji]][jl] [![Apache License][li]][ll]
 
 [Apache Accumulo][accumulo] is a sorted, distributed key/value store that provides robust,
-scalable data storage and retrieval. With Apache Accumulo, users can store and manage large
-data sets across a cluster. Accumulo uses [Apache Hadoop]'s HDFS to store its data and
+scalable data storage and retrieval. With Apache Accumulo, users can store and manage large data sets across a cluster. Accumulo uses [Apache Hadoop]'s HDFS to store its data and","[{'comment': ""Since this section doesn't change any wording, we should keep the existing 2 lines, rather than merge them into 1."", 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -20,11 +20,14 @@ limitations under the License.
 [![Build Status][ti]][tl] [![Maven Central][mi]][ml] [![Javadoc][ji]][jl] [![Apache License][li]][ll]
 
 [Apache Accumulo][accumulo] is a sorted, distributed key/value store that provides robust,
-scalable data storage and retrieval. With Apache Accumulo, users can store and manage large
-data sets across a cluster. Accumulo uses [Apache Hadoop]'s HDFS to store its data and
+scalable data storage and retrieval. With Apache Accumulo, users can store and manage large data sets across a cluster. Accumulo uses [Apache Hadoop]'s HDFS to store its data and
 [Apache Zookeeper] for consensus. Check out the [Accumulo project website][accumulo] for
 news and general information.
 
+[Apache Zookeeper] for consensus.
+
+Download the latest version of Apache Accumulo® [here][dl].
+","[{'comment': '```suggestion\r\n[Apache Zookeeper] for consensus.\r\n\r\nDownload the latest version of Apache Accumulo on the [project website][dl].\r\n\r\n```', 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -44,13 +47,18 @@ waiting for the tests to run.
 
 This command produces `assemble/target/accumulo-<version>-bin.tar.gz`
 
+## Contributing
+
+Contributions are welcome to all Apache Accumulo repositories
+
+If you want to contribute, go through our guide [How to contribute](https://accumulo.apache.org/how-to-contribute/)
+
 ## Export Control
 
 <details>
 <summary>Click here to show/hide details</summary>
 
----
-
+<br>","[{'comment': 'This change can be undone.', 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -100,3 +108,4 @@ more details on bouncycastle's cryptography features.
 [tl]: https://github.com/apache/accumulo/actions
 [java-export]: https://www.oracle.com/us/products/export/export-regulations-345813.html
 [bouncy-site]: https://bouncycastle.org
+[dl]: https://accumulo.apache.org/downloads/","[{'comment': '```suggestion\r\n[dl]: https://accumulo.apache.org/downloads\r\n[contribute]: https://accumulo.apache.org/how-to-contribute\r\n```', 'commenter': 'ctubbsii'}]"
2068,README.md,"@@ -44,13 +47,18 @@ waiting for the tests to run.
 
 This command produces `assemble/target/accumulo-<version>-bin.tar.gz`
 
+## Contributing
+
+Contributions are welcome to all Apache Accumulo repositories
+
+If you want to contribute, go through our guide [How to contribute](https://accumulo.apache.org/how-to-contribute/)","[{'comment': '```suggestion\r\nContributions are welcome to all Apache Accumulo repositories.\r\n\r\nIf you want to contribute, read [our guide on our website][contribute].\r\n```\r\n\r\nCan put `[contribute]: https://accumulo.apache.org/how-to-contribute` at the end.', 'commenter': 'ctubbsii'}]"
2073,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -340,16 +341,17 @@ public void setupShell() throws Exception {
   }
 
   @AfterClass
-  public static void tearDownAfterClass() {
+  public static void tearDownAfterClass() throws Exception {
     if (traceProcess != null) {
       traceProcess.destroy();
     }
 
+    deleteTables();
+
     SharedMiniClusterBase.stopMiniCluster();
   }
 
-  @After
-  public void deleteTables() throws Exception {
+  public static void deleteTables() throws Exception {","[{'comment': ""I'm not sure it's worth bothering deleting tables at this point if we're killing mini anyway.\r\n\r\nI think this change would only matter if we were using the `parallel` options in `maven-failsafe-plugin`, which we aren't, and shouldn't because that would likely cause all sorts of other problems. The test methods that are run inside the class are always run serially. This delete tables method merely cleans up between different test methods in case they use the same table names or makes the system use too many resources. I don't think your change here accomplishes anything, except leaving tables around longer, possibly causing some methods to collide if they use the same table names."", 'commenter': 'ctubbsii'}]"
2073,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -513,7 +515,7 @@ public void du() throws Exception {
     String o = ts.output.get();
     // for some reason, there's a bit of fluctuation
     assertTrue(""Output did not match regex: '"" + o + ""'"",
-        o.matches("".*[1-9][0-9][0-9]\\s\\["" + table + ""\\]\\n""));
+        o.matches("".*[1-9][0-9][0-9]\\s\\["" + table + ""]\\n""));","[{'comment': 'Was this an unnecessary escape?', 'commenter': 'ctubbsii'}, {'comment': 'Yes', 'commenter': 'EdColeman'}]"
2073,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -1717,7 +1719,7 @@ public void testPertableClasspath() throws Exception {
     fooFilterJar.deleteOnExit();
 
     File fooConstraintJar = File.createTempFile(""FooConstraint"", "".jar"", new File(rootPath));
-    FileUtils.copyInputStreamToFile(this.getClass().getResourceAsStream(""/FooConstraint.jar""),
+    FileUtils.copyInputStreamToFile(this.getClass().getResourceAsStream(""/FooConstraint_2_1.jar""),","[{'comment': ""I assume this new jar has constraints using the new public API package? I'm not sure it matters for this test. We should support older constraints as well, so this test should pass whether or not the jar contains classes that implement the new or old constraint interface."", 'commenter': 'ctubbsii'}, {'comment': 'Yes - this uses the new interface.  When I tested with something built with the older interface it failed.  I think one way to handle this is to add a tests that tests a jar built with the each version - step one was just to see if this would get the test to pass (which it does now) in case that would help point a way forward - this was the motivation to be able to build jars for multiple versions.', 'commenter': 'EdColeman'}]"
2073,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -2687,13 +2705,15 @@ public void testCreateTableWithBinarySplitsFile1()
     try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {
       splitsFile = System.getProperty(""user.dir"") + ""/target/splitFile"";
       generateSplitsFile(splitsFile, 200, 12, true, true, true, false, false);
-      SortedSet<Text> expectedSplits = readSplitsFromFile(splitsFile, false);
+      SortedSet<Text> expectedSplits = readSplitsFromFile(splitsFile);
       final String tableName = name.getMethodName() + ""_table"";
       ts.exec(""createtable "" + tableName + "" -sf "" + splitsFile, true);
       Collection<Text> createdSplits = client.tableOperations().listSplits(tableName);
       assertEquals(expectedSplits, new TreeSet<>(createdSplits));
     } finally {
-      Files.delete(Paths.get(splitsFile));
+      if (Objects.nonNull(splitsFile)) {","[{'comment': '`Objects.nonNull` exists for use as a lambda when a Predicate is required (`stream.filter(Objects::nonNull)`, for example). For plain old if statements, it\'s much more clear to just write:\r\n\r\n```suggestion\r\n      if (splitsFile != null) {\r\n```\r\n\r\nHowever, there\'s no reason for it to ever be null or for us to check for it being null. We can just assign it to `System.getProperty(""user.dir"") + ""/target/splitFile"";` in the first place. There\'s no reason that needs to be done inside the try block.', 'commenter': 'ctubbsii'}, {'comment': 'I\'ll change these - the ""reason"" is the code analysis flags these as possibility returning null and didn\'t want to add suppression(s)', 'commenter': 'EdColeman'}, {'comment': ""Right, but they are only possibly returning null because null is assigned to the variable first. However, that's an unnecessary assignment. The assignment inside each try block can be moved to just before the try block, so they are immediately assigned to the value from reading the system property, and never assign null in the first place. Then, they can never be null, and you don't need the checks to see if they are null in the finally blocks."", 'commenter': 'ctubbsii'}]"
2073,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -2900,9 +2931,7 @@ private void generateSplitsFile(final String splitsFile, final int numItems, fin
   }
 
   private Text getRandomText(final int len) {
-    int desiredLen = len;
-    if (len > 32)
-      desiredLen = 32;
+    var desiredLen = Math.min(len, 32);","[{'comment': ""This isn't a great use of `var`. `int` would be better here, since the type isn't obvious from the variable name or its assignment."", 'commenter': 'ctubbsii'}]"
2073,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -345,21 +345,33 @@ public static void tearDownAfterClass() {
       traceProcess.destroy();
     }
 
+    logUndeletedTables();
     SharedMiniClusterBase.stopMiniCluster();
   }
 
-  @After
-  public void deleteTables() throws Exception {
+  /**
+   * Tests should be cleaning up tables as they complete - this checks at the end of this test and
+   * prints diagnostic messages if a table remains.
+   */
+  private static void logUndeletedTables() {
     try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {
       for (String table : c.tableOperations().list()) {
-        if (!table.startsWith(Namespace.ACCUMULO.name() + ""."") && !table.equals(""trace""))
-          try {
-            c.tableOperations().delete(table);
-          } catch (TableNotFoundException e) {
-            // don't care
-          }
+        if (!table.startsWith(Namespace.ACCUMULO.name() + ""."") && !table.equals(""trace"")) {
+          log.warn(""Clean up of Table left after test: {}"", table);
+        }
       }","[{'comment': ""If the `getUniqueNames()` method is used for the table names and namespaces for each test, then this `@After` method could be modified to clean up only the most recently run method's tables, rather than clean up each test separately in each test method body. Doing it that way would keep the cleanup code minimal, while not deleting tables that might be in use by another test."", 'commenter': 'ctubbsii'}]"
2075,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientInfoImpl.java,"@@ -115,6 +120,18 @@ public static Properties toProperties(Path propertiesFile) {
     return properties;
   }
 
+  @SuppressFBWarnings(value = ""URLCONNECTION_SSRF_FD"",
+      justification = ""code runs in same security context as user who provided propertiesURL"")
+  public static Properties toProperties(URL propertiesURL) {
+    Properties properties = new Properties();
+    try (InputStream is = propertiesURL.openStream()) {","[{'comment': 'You could add some additional validation to make sure this is just a properties file and not something malicious.', 'commenter': 'milleruntime'}, {'comment': ""Good idea. I'll include a validation mechanism in my next commit.  "", 'commenter': 'slackwinner'}, {'comment': ""@milleruntime What kind of validation were you thinking? It's just getting the data from the data source. One would think that the Properties object would validate that the contents are appropriate when calling `load()`."", 'commenter': 'ctubbsii'}, {'comment': ""That's a good point. I was just thinking he could do something simple, like check to see if the URL ends with a valid extension but that will probably get vetted on `load()`. I told @slackwinner that it would be a good exercise to figure out how to sanitize a user parameter to prevent malicious code execution."", 'commenter': 'milleruntime'}, {'comment': ""@milleruntime wrote:\r\n> That's a good point. I was just thinking he could do something simple, like check to see if the URL ends with a valid extension but that will probably get vetted on `load()`.\r\n\r\nThere's no requirement that the file or URL have any particular naming convention.\r\n\r\n> I told @slackwinner that it would be a good exercise to figure out how to sanitize a user parameter to prevent malicious code execution.\r\n\r\nIn general, that's good advice, but I don't think it applies here. What's the potential attack vector?\r\n\r\nAre you thinking the file itself could contain something malicious? I don't think the properties parser executes any code it finds in the file... if it does, then that vulnerability would be in the parser, and would apply no matter where the file is read from, whether from a URL, a Path, a File, or even a String object.\r\n\r\nOr, are you thinking that parsing the URL itself in order to locate the stream to read from could itself cause code to be executed? Because, then the vulnerability would be in Java's URL parsing code, and in any case that doesn't seem like a thing that can happen.\r\n\r\nI think the spotbugs sec-bugs error highlights the biggest risk: since the user decides the URL to read from, they could instruct the code to read a file that the user themselves don't have permission to read, and then leak that data back to the user. However, that doesn't apply here, since the code executes in the same security context as the user. So, the code can only read files the user already has permission to read. This would be a valid concern if the URL was coming from some outside process or remote user, but the responsibility to sanitize that would be with the service that is built around Accumulo's API and receives that remote input. It's not a concern for our API.\r\n\r\nI don't think there's anything to sanitize. What could the URL contain or link to that would necessitate sanitization?\r\n"", 'commenter': 'ctubbsii'}, {'comment': 'Well I was thinking we can ensure the URL contains a properties extension before executing load() to achieve fast failure in case a user accidentally passes in the wrong URL (e.g. The user sends in a URL that leads to a text file). However if load() already provides the fast failure feature, we could forgo the parameter sanitation all together. ', 'commenter': 'slackwinner'}, {'comment': '> Or, are you thinking that parsing the URL itself in order to locate the stream to read from could itself cause code to be executed?\r\n\r\nYes. But a URL _could_ be a lot of things and there is nothing preventing the user from opening a stream to ""http://www.badguys.com/maliciousFile.exe"". This is what I thought the sec-bugs error was highlighting. ', 'commenter': 'milleruntime'}, {'comment': '@slackwinner wrote:\r\n> Well I was thinking we can ensure the URL contains a properties extension before executing load() to achieve fast failure in case a user accidentally passes in the wrong URL (e.g. The user sends in a URL that leads to a text file). However if load() already provides the fast failure feature, we could forgo the parameter sanitation all together.\r\n\r\nWe have no requirement on the file\'s name. It could be `*.conf`, `*.properties`, `*.props`, `*.properties.backup`, `nofileextension`, or anything else. Such a check would only restrict user flexibility, but not actually achieve anything in terms of security.\r\n\r\n@milleruntime wrote:\r\n> Yes. But a URL _could_ be a lot of things and there is nothing preventing the user from opening a stream to ""http://www.badguys.com/maliciousFile.exe"".\r\n\r\nYeah, it could open a stream to that. But, a method that takes a Path or File could also open `/home/user/maliciousFile.exe`. It matters what we do with it. Merely reading from the stream won\'t automatically execute it. A well-crafted file could be used to exploit a parser vulnerability and execute code, if there were such a vulnerability, but that would apply whether it\'s a File or a URL, and would be the responsibility of the parser code to prevent. However, since this is client code, it executes in the same security context as the calling client user who provided the URL or File name. So, it would be nonsensical for such a user to employ Accumulo to execute this code, since the user could execute the malicious file directly if they wanted.\r\n\r\n>  This is what I thought the sec-bugs error was highlighting.\r\n\r\nIt wasn\'t. https://find-sec-bugs.github.io/bugs.htm#URLCONNECTION_SSRF_FD\r\n\r\nThis class of bugs requires the user and the code to be operating in different security contexts. Commonly, user input on a web form or REST endpoint causes a web server to read or execute code on the server-side that it shouldn\'t, or in some other security context that is separate from the user (such as within a ""same origin"" sandbox in a browser). None of those apply here.', 'commenter': 'ctubbsii'}]"
2086,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -1237,4 +1238,37 @@ protected SERIALIZED_FORMAT getSerializedFormat() {
     return this.useOldDeserialize ? SERIALIZED_FORMAT.VERSION1 : SERIALIZED_FORMAT.VERSION2;
   }
 
+  /**
+   * Creates a multi-lined, human-readable String for this mutation.
+   *
+   * This method creates many intermediate Strings and should not be used for large volumes of
+   * Mutations.
+   *
+   * @return A multi-lined, human-readable String for this mutation.
+   */
+  public static String prettyPrint(Mutation mutation) {
+    StringBuilder sb = new StringBuilder();
+
+    sb.append(""mutation: "").append(utf8String(mutation.row)).append('\n');
+    for (ColumnUpdate update : mutation.getUpdates()) {
+      sb.append("" update: "");
+      sb.append(utf8String(update.getColumnFamily()));
+      sb.append(':');
+      sb.append(utf8String(update.getColumnQualifier()));
+      sb.append("" value "");
+
+      if (update.isDeleted()) {
+        sb.append(""[delete]"");
+      } else {
+        sb.append(utf8String(update.getValue()));
+      }
+      sb.append('\n');
+    }
+
+    return sb.toString();
+  }
+
+  private static String utf8String(byte[] bytes) {
+    return new String(bytes, StandardCharsets.UTF_8);","[{'comment': ""If you just do a static import of `StandardCharsets.UTF_8`, the lines where this is used become short enough that you don't need this separate private helper method."", 'commenter': 'ctubbsii'}]"
2086,core/src/main/java/org/apache/accumulo/core/data/Mutation.java,"@@ -1237,4 +1238,37 @@ protected SERIALIZED_FORMAT getSerializedFormat() {
     return this.useOldDeserialize ? SERIALIZED_FORMAT.VERSION1 : SERIALIZED_FORMAT.VERSION2;
   }
 
+  /**
+   * Creates a multi-lined, human-readable String for this mutation.
+   *
+   * This method creates many intermediate Strings and should not be used for large volumes of
+   * Mutations.
+   *
+   * @return A multi-lined, human-readable String for this mutation.
+   */
+  public static String prettyPrint(Mutation mutation) {","[{'comment': ""This is public API, so it would need a `@since` javadoc tag.\r\nAlso, why make this static?\r\n\r\nAlso, according to semantic versioning, we don't make API additions in bugfix releases, so this should not get added to 1.10, but could be added to 2.1, our next minor release planned."", 'commenter': 'ctubbsii'}, {'comment': 'Added since tag, rebased to main, and made the method an instance method.', 'commenter': 'jschmidt10'}]"
2086,server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java,"@@ -34,7 +34,6 @@
 
 import org.apache.accumulo.core.client.admin.TableOperations;
 import org.apache.accumulo.core.conf.AccumuloConfiguration;
-import org.apache.accumulo.core.conf.ConfigurationObserver;","[{'comment': 'This seems unrelated. Is this an existing unused import?', 'commenter': 'ctubbsii'}, {'comment': 'Yes, the build removed that for me I think. Since I re-targeted main, this is gone (from this PR). ', 'commenter': 'jschmidt10'}]"
2086,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -148,22 +148,24 @@ public static void update(ClientContext context, ZooLock zooLock, Mutation m, Ke
   }
 
   public static void update(Writer t, ZooLock zooLock, Mutation m) {
+    update(t, zooLock, m, null);
+  }
+
+  public static void update(Writer t, ZooLock zooLock, Mutation m, KeyExtent extent) {
     if (zooLock != null)
       putLockID(zooLock, m);
     while (true) {
       try {
         t.update(m);
         return;
-      } catch (AccumuloException e) {
-        log.error(""{}"", e.getMessage(), e);
-      } catch (AccumuloSecurityException e) {
-        log.error(""{}"", e.getMessage(), e);
+      } catch (AccumuloException | TableNotFoundException | AccumuloSecurityException e) {
+        String extentMsg = extent == null ? """" : "" for extent: "" + extent;
+        log.error(""Failed to write metadata updates {} {}"", extentMsg, Mutation.prettyPrint(m), e);
       } catch (ConstraintViolationException e) {
-        log.error(""{}"", e.getMessage(), e);
+        String extentMsg = extent == null ? """" : "" for extent: "" + extent;
+        log.error(""Failed to write metadata updates {} {}"", extentMsg, Mutation.prettyPrint(m), e);","[{'comment': 'This code is duplicated. It could be moved to a private static method.', 'commenter': 'ctubbsii'}]"
2086,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -148,22 +148,24 @@ public static void update(ClientContext context, ZooLock zooLock, Mutation m, Ke
   }
 
   public static void update(Writer t, ZooLock zooLock, Mutation m) {
+    update(t, zooLock, m, null);
+  }
+
+  public static void update(Writer t, ZooLock zooLock, Mutation m, KeyExtent extent) {","[{'comment': ""I see the new method here, but I don't see any code that actually uses it, where KeyExtent is non-null."", 'commenter': 'ctubbsii'}, {'comment': 'Good catch, I missed an update. ', 'commenter': 'jschmidt10'}]"
2086,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -144,27 +144,37 @@ private static void update(ServerContext context, Mutation m, KeyExtent extent)
   public static void update(ServerContext context, ServiceLock zooLock, Mutation m,
       KeyExtent extent) {
     Writer t = extent.isMeta() ? getRootTable(context) : getMetadataTable(context);
-    update(context, t, zooLock, m);
+    update(context, t, zooLock, m, extent);
   }
 
   public static void update(ServerContext context, Writer t, ServiceLock zooLock, Mutation m) {
+    update(context, t, zooLock, m, null);
+  }
+
+  public static void update(ServerContext context, Writer t, ServiceLock zooLock, Mutation m,
+      KeyExtent extent) {
     if (zooLock != null)
       putLockID(context, zooLock, m);
     while (true) {
       try {
         t.update(m);
         return;
       } catch (AccumuloException | TableNotFoundException | AccumuloSecurityException e) {
-        log.error(""{}"", e.getMessage(), e);
+        logUpdateFailure(m, extent, e);
       } catch (ConstraintViolationException e) {
-        log.error(""{}"", e.getMessage(), e);
+        logUpdateFailure(m, extent, e);
         // retrying when a CVE occurs is probably futile and can cause problems, see ACCUMULO-3096
         throw new RuntimeException(e);
       }
       sleepUninterruptibly(1, TimeUnit.SECONDS);
     }
   }
 
+  private static void logUpdateFailure(Mutation m, KeyExtent extent, Exception e) {
+    String extentMsg = extent == null ? """" : "" for extent: "" + extent;
+    log.error(""Failed to write metadata updates {} {}"", extentMsg, m.prettyPrint(), e);","[{'comment': 'Removes the second space after the word `updates`:\r\n\r\n```suggestion\r\n    String extentMsg = extent == null ? """" : "" for extent: "" + extent;\r\n    log.error(""Failed to write metadata updates{} {}"", extentMsg, m.prettyPrint(), e);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'This suggestion is superseded by my subsequent suggestion to remove this ternary logic, since extent never needs to be null.', 'commenter': 'ctubbsii'}]"
2086,server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java,"@@ -144,27 +144,37 @@ private static void update(ServerContext context, Mutation m, KeyExtent extent)
   public static void update(ServerContext context, ServiceLock zooLock, Mutation m,
       KeyExtent extent) {
     Writer t = extent.isMeta() ? getRootTable(context) : getMetadataTable(context);
-    update(context, t, zooLock, m);
+    update(context, t, zooLock, m, extent);
   }
 
   public static void update(ServerContext context, Writer t, ServiceLock zooLock, Mutation m) {
+    update(context, t, zooLock, m, null);
+  }
+
+  public static void update(ServerContext context, Writer t, ServiceLock zooLock, Mutation m,
+      KeyExtent extent) {","[{'comment': 'I see there\'s only ever two places in the code that called the old method without the extent. The first is the one you changed above to call the new method, and where extent is never null. The second is in MetaConstraintRetryIT, which also has an extent that is non-null. I think there\'s no reason to have both update methods. You can just add extent to the existing one, without preserving the old behavior. You only have to update two references, and there\'s never a need to pass null into the method for extent, or to check for null in the log message where you have the ternary operator to change the method if it\'s null. If, for some reason it does end up being null, it\'s not a problem for it to say ""for extent null"".', 'commenter': 'ctubbsii'}, {'comment': 'Cool. Fixed.', 'commenter': 'jschmidt10'}]"
2098,test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java,"@@ -68,6 +116,54 @@ protected int defaultTimeoutSeconds() {
     return 4 * 60;
   }
 
+  @Test
+  public void testBadSelector() throws Exception {
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {
+      final String tableName = getUniqueNames(1)[0];
+      c.tableOperations().create(tableName);
+      // Ensure compactions don't kick off
+      c.tableOperations().setProperty(tableName, Property.TABLE_MAJC_RATIO.getKey(), ""10.0"");
+      // Create multiple RFiles
+      BatchWriter bw = c.createBatchWriter(tableName);
+      Mutation m = new Mutation(""1"");
+      m.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m2 = new Mutation(""2"");
+      m2.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m2);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m3 = new Mutation(""3"");
+      m3.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m3);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m4 = new Mutation(""4"");
+      m4.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m4);
+      bw.close();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+
+      List<String> files = FunctionalTestUtils.getRFileNames(c, tableName);
+      Assert.assertEquals(4, files.size());
+
+      String subset = files.get(0).substring(files.get(0).lastIndexOf('/') + 1) + "",""
+          + files.get(3).substring(files.get(3).lastIndexOf('/') + 1);
+
+      CompactionConfig config = new CompactionConfig()
+          .setSelector(new PluginConfig(RandomErrorThrowingSelector.class.getName(),
+              Map.of(RandomErrorThrowingSelector.FILE_LIST_PARAM, subset)))
+          .setWait(true);
+      c.tableOperations().compact(tableName, config);
+
+      List<String> files2 = FunctionalTestUtils.getRFileNames(c, tableName);
+      Assert.assertFalse(files2.contains(files.get(0)));
+      Assert.assertFalse(files2.contains(files.get(3)));","[{'comment': '```suggestion\r\n      Assert.assertFalse(files2.contains(files.get(0)));\r\n      Assert.assertTrue(files2.contains(files.get(1)));\r\n      Assert.assertTrue(files2.contains(files.get(2)));\r\n      Assert.assertFalse(files2.contains(files.get(3)));\r\n```', 'commenter': 'keith-turner'}]"
2098,test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java,"@@ -68,6 +116,54 @@ protected int defaultTimeoutSeconds() {
     return 4 * 60;
   }
 
+  @Test
+  public void testBadSelector() throws Exception {
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {
+      final String tableName = getUniqueNames(1)[0];
+      c.tableOperations().create(tableName);
+      // Ensure compactions don't kick off
+      c.tableOperations().setProperty(tableName, Property.TABLE_MAJC_RATIO.getKey(), ""10.0"");","[{'comment': 'setting these props are eventually consistent, so not sure when tserver will see the config update.  Could set the prop at table creation time instead, avoiding any possible race conditions in the test. ', 'commenter': 'keith-turner'}]"
2098,test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java,"@@ -68,6 +116,54 @@ protected int defaultTimeoutSeconds() {
     return 4 * 60;
   }
 
+  @Test
+  public void testBadSelector() throws Exception {
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {
+      final String tableName = getUniqueNames(1)[0];
+      c.tableOperations().create(tableName);
+      // Ensure compactions don't kick off
+      c.tableOperations().setProperty(tableName, Property.TABLE_MAJC_RATIO.getKey(), ""10.0"");
+      // Create multiple RFiles
+      BatchWriter bw = c.createBatchWriter(tableName);
+      Mutation m = new Mutation(""1"");
+      m.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m2 = new Mutation(""2"");
+      m2.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m2);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m3 = new Mutation(""3"");
+      m3.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m3);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m4 = new Mutation(""4"");
+      m4.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m4);
+      bw.close();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+
+      List<String> files = FunctionalTestUtils.getRFileNames(c, tableName);
+      Assert.assertEquals(4, files.size());
+
+      String subset = files.get(0).substring(files.get(0).lastIndexOf('/') + 1) + "",""
+          + files.get(3).substring(files.get(3).lastIndexOf('/') + 1);
+
+      CompactionConfig config = new CompactionConfig()
+          .setSelector(new PluginConfig(RandomErrorThrowingSelector.class.getName(),
+              Map.of(RandomErrorThrowingSelector.FILE_LIST_PARAM, subset)))
+          .setWait(true);
+      c.tableOperations().compact(tableName, config);
+
+      List<String> files2 = FunctionalTestUtils.getRFileNames(c, tableName);
+      Assert.assertFalse(files2.contains(files.get(0)));
+      Assert.assertFalse(files2.contains(files.get(3)));
+    }","[{'comment': 'It would be nice to scan table and verify data is as expected.', 'commenter': 'keith-turner'}]"
2098,test/src/main/java/org/apache/accumulo/test/functional/FunctionalTestUtils.java,"@@ -81,6 +83,19 @@ public static int countRFiles(AccumuloClient c, String tableName) throws Excepti
     }
   }
 
+  public static List<String> getRFileNames(AccumuloClient c, String tableName) throws Exception {","[{'comment': 'This method name seems off.  Seems like its returning paths or URIs and not names.  Also could possibly use Ample for the implementation, but not quite sure. It may not be easy to use Ample unless you have a ServerContext.', 'commenter': 'keith-turner'}]"
2098,test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java,"@@ -68,6 +115,67 @@ protected int defaultTimeoutSeconds() {
     return 4 * 60;
   }
 
+  @Test
+  public void testBadSelector() throws Exception {
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {
+      final String tableName = getUniqueNames(1)[0];
+      NewTableConfiguration tc = new NewTableConfiguration();
+      // Ensure compactions don't kick off
+      tc.setProperties(Map.of(Property.TABLE_MAJC_RATIO.getKey(), ""10.0""));
+      c.tableOperations().create(tableName, tc);
+      // Create multiple RFiles
+      BatchWriter bw = c.createBatchWriter(tableName);
+      Mutation m = new Mutation(""1"");
+      m.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m2 = new Mutation(""2"");
+      m2.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m2);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m3 = new Mutation(""3"");
+      m3.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m3);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m4 = new Mutation(""4"");
+      m4.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m4);
+      bw.close();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+
+      List<String> files = FunctionalTestUtils.getRFilePaths(c, tableName);
+      assertEquals(4, files.size());
+
+      String subset = files.get(0).substring(files.get(0).lastIndexOf('/') + 1) + "",""
+          + files.get(3).substring(files.get(3).lastIndexOf('/') + 1);
+
+      CompactionConfig config = new CompactionConfig()
+          .setSelector(new PluginConfig(RandomErrorThrowingSelector.class.getName(),
+              Map.of(RandomErrorThrowingSelector.FILE_LIST_PARAM, subset)))
+          .setWait(true);
+      c.tableOperations().compact(tableName, config);
+
+      List<String> files2 = FunctionalTestUtils.getRFilePaths(c, tableName);
+      assertFalse(files2.contains(files.get(0)));
+      assertTrue(files2.contains(files.get(1)));
+      assertTrue(files2.contains(files.get(2)));
+      assertFalse(files2.contains(files.get(3)));","[{'comment': '```suggestion\r\n      // check that the subset of files selected are compacted, but the others remain untouched\r\n      List<String> filesAfterCompact = FunctionalTestUtils.getRFilePaths(c, tableName);\r\n      assertFalse(filesAfterCompact.contains(files.get(0)));\r\n      assertTrue(filesAfterCompact.contains(files.get(1)));\r\n      assertTrue(filesAfterCompact.contains(files.get(2)));\r\n      assertFalse(filesAfterCompact.contains(files.get(3)));\r\n```', 'commenter': 'ctubbsii'}]"
2098,test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java,"@@ -68,6 +115,67 @@ protected int defaultTimeoutSeconds() {
     return 4 * 60;
   }
 
+  @Test
+  public void testBadSelector() throws Exception {
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {
+      final String tableName = getUniqueNames(1)[0];
+      NewTableConfiguration tc = new NewTableConfiguration();
+      // Ensure compactions don't kick off
+      tc.setProperties(Map.of(Property.TABLE_MAJC_RATIO.getKey(), ""10.0""));
+      c.tableOperations().create(tableName, tc);
+      // Create multiple RFiles
+      BatchWriter bw = c.createBatchWriter(tableName);
+      Mutation m = new Mutation(""1"");
+      m.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m2 = new Mutation(""2"");
+      m2.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m2);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m3 = new Mutation(""3"");
+      m3.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m3);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m4 = new Mutation(""4"");
+      m4.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m4);
+      bw.close();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);","[{'comment': '```suggestion\r\n      try (BatchWriter bw = c.createBatchWriter(tableName)) {\r\n        for (int i = 1; i <= 4; i++) {\r\n          Mutation m = new Mutation(Integer.toString(i));\r\n          m.put(""cf"", ""cq"", new Value());\r\n          bw.addMutation(m);\r\n          bw.flush();\r\n          c.tableOperations().flush(tableName, null, null, true);\r\n        }\r\n      }\r\n```', 'commenter': 'ctubbsii'}]"
2098,test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java,"@@ -68,6 +115,67 @@ protected int defaultTimeoutSeconds() {
     return 4 * 60;
   }
 
+  @Test
+  public void testBadSelector() throws Exception {
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {
+      final String tableName = getUniqueNames(1)[0];
+      NewTableConfiguration tc = new NewTableConfiguration();
+      // Ensure compactions don't kick off
+      tc.setProperties(Map.of(Property.TABLE_MAJC_RATIO.getKey(), ""10.0""));
+      c.tableOperations().create(tableName, tc);
+      // Create multiple RFiles
+      BatchWriter bw = c.createBatchWriter(tableName);
+      Mutation m = new Mutation(""1"");
+      m.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m2 = new Mutation(""2"");
+      m2.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m2);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m3 = new Mutation(""3"");
+      m3.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m3);
+      bw.flush();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+      Mutation m4 = new Mutation(""4"");
+      m4.put(""cf"", ""cq"", new Value(new byte[0]));
+      bw.addMutation(m4);
+      bw.close();
+      c.tableOperations().flush(tableName, new Text(""0""), new Text(""9""), true);
+
+      List<String> files = FunctionalTestUtils.getRFilePaths(c, tableName);
+      assertEquals(4, files.size());
+
+      String subset = files.get(0).substring(files.get(0).lastIndexOf('/') + 1) + "",""
+          + files.get(3).substring(files.get(3).lastIndexOf('/') + 1);
+
+      CompactionConfig config = new CompactionConfig()
+          .setSelector(new PluginConfig(RandomErrorThrowingSelector.class.getName(),
+              Map.of(RandomErrorThrowingSelector.FILE_LIST_PARAM, subset)))
+          .setWait(true);
+      c.tableOperations().compact(tableName, config);
+
+      List<String> files2 = FunctionalTestUtils.getRFilePaths(c, tableName);
+      assertFalse(files2.contains(files.get(0)));
+      assertTrue(files2.contains(files.get(1)));
+      assertTrue(files2.contains(files.get(2)));
+      assertFalse(files2.contains(files.get(3)));
+
+      List<Text> rows = new ArrayList<>();
+      rows.add(new Text(""1""));
+      rows.add(new Text(""2""));
+      rows.add(new Text(""3""));
+      rows.add(new Text(""4""));
+      c.createScanner(tableName).forEach((k, v) -> {
+        assertTrue(rows.remove(k.getRow()));
+      });
+      assertEquals(0, rows.size());","[{'comment': '```suggestion\r\n      List<String> rows = new ArrayList<>();\r\n      c.createScanner(tableName).forEach((k, v) -> rows.add(k.getRow().toString()));\r\n      assertEquals(List.of(""1"", ""2"", ""3"", ""4""), actualRows);\r\n```', 'commenter': 'ctubbsii'}]"
2098,test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java,"@@ -68,6 +115,50 @@ protected int defaultTimeoutSeconds() {
     return 4 * 60;
   }
 
+  @Test
+  public void testBadSelector() throws Exception {
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {
+      final String tableName = getUniqueNames(1)[0];
+      NewTableConfiguration tc = new NewTableConfiguration();
+      // Ensure compactions don't kick off
+      tc.setProperties(Map.of(Property.TABLE_MAJC_RATIO.getKey(), ""10.0""));
+      c.tableOperations().create(tableName, tc);
+      // Create multiple RFiles
+      try (BatchWriter bw = c.createBatchWriter(tableName)) {
+        for (int i = 1; i <= 4; i++) {
+          Mutation m = new Mutation(Integer.toString(i));
+          m.put(""cf"", ""cq"", new Value());
+          bw.addMutation(m);
+          bw.flush();
+          c.tableOperations().flush(tableName, null, null, true);
+        }
+      }
+
+      List<String> files = FunctionalTestUtils.getRFilePaths(c, tableName);
+      assertEquals(4, files.size());
+
+      String subset = files.get(0).substring(files.get(0).lastIndexOf('/') + 1) + "",""
+          + files.get(3).substring(files.get(3).lastIndexOf('/') + 1);
+
+      CompactionConfig config = new CompactionConfig()
+          .setSelector(new PluginConfig(RandomErrorThrowingSelector.class.getName(),
+              Map.of(RandomErrorThrowingSelector.FILE_LIST_PARAM, subset)))
+          .setWait(true);
+      c.tableOperations().compact(tableName, config);
+
+      // check that the subset of files selected are compacted, but the others remain untouched
+      List<String> filesAfterCompact = FunctionalTestUtils.getRFilePaths(c, tableName);
+      assertFalse(filesAfterCompact.contains(files.get(0)));
+      assertTrue(filesAfterCompact.contains(files.get(1)));
+      assertTrue(filesAfterCompact.contains(files.get(2)));
+      assertFalse(filesAfterCompact.contains(files.get(3)));
+
+      List<String> rows = new ArrayList<>();
+      c.createScanner(tableName).forEach((k, v) -> rows.add(k.getRow().toString()));
+      assertEquals(List.of(""1"", ""2"", ""3"", ""4""), actualRows);","[{'comment': 'Oops, my initial suggestion renamed this variable.\r\n\r\n```suggestion\r\n      assertEquals(List.of(""1"", ""2"", ""3"", ""4""), rows);\r\n```', 'commenter': 'ctubbsii'}]"
2122,core/src/main/java/org/apache/accumulo/core/client/admin/NamespaceOperations.java,"@@ -202,11 +202,32 @@ void removeProperty(String namespace, String property)
    *           if the user does not have permission
    * @throws NamespaceNotFoundException
    *           if the specified namespace doesn't exist
-   * @since 1.6.0
+   * @deprecated since 2.1.0; use {@link #getPropertiesMap(String)} instead.
    */
+  @Deprecated(since = ""2.1.0"")","[{'comment': ""I am wondering if we _need_ to mark this method deprecated. I can't think of a risky/harmful reason why a user shouldn't use this method. Yes, a map is more convenient, which makes the new method nice but I don't know if we need to mark this as deprecated. If a user is using it today there is no reason why they should change their code unless they want to."", 'commenter': 'milleruntime'}, {'comment': ""I think the initial reasoning that was detailed in the JIRA ticket was bloating the API. In this case, it isn't much bloat but that was the reasoning given in that discussion."", 'commenter': 'Manno15'}, {'comment': 'The `@since` tag shouldn\'t be removed here. I\'d prefer the old method be deprecated. There are two ways to think about this:\r\n\r\n1. Thinking about reducing churn, based on what exists now; the right question is: ""do we need to remove it?""\r\n2. Thinking about what the API *should* be, in order to have a clean API; the right question: ""do we need to have it?""\r\n\r\nI tend to think in terms of the clean API, but I understand we need to be careful of churn. However, that\'s what the deprecation cycle is *for*. So, I\'d prefer to deprecate it, because we don\'t need both. We don\'t have to remove it any time soon, though. That decision can be made later.', 'commenter': 'ctubbsii'}, {'comment': ""I marked it on the PR since I think it warranted discussion. But after some more thought, I think it should be deprecated as well. It does feel like a mistake that the API didn't originally return the more flexible object."", 'commenter': 'milleruntime'}, {'comment': 'My preference would be to not deprecate or remove the method for the following reasons.\r\n\r\n- The method is useful for the case of looping through the properties to find props matching a pattern.  This use case is not uncommon and it seems like the existing method supports slightly shorter code.\r\n- For table props, it seems like the existing method has been the only way to get table props in the API since Accumulo was released. So for the past decade any code written would have had to use this method.  To me, removing the method when it has no problems seems needlessly disruptive to this existing code.\r\n\r\nMy preference would be a `@see` javadoc tag pointing to the other method instead of deprecating. ', 'commenter': 'keith-turner'}, {'comment': ""> It does feel like a mistake that the API didn't originally return the more flexible object.\r\n\r\nFollowing that line of thought, maybe Map is not flexible enough either.  For example [a specific configuration type was created for plugins](https://github.com/apache/accumulo/blob/8a636a3ba91f5dae1d8b09b095178889a7d79c1d/core/src/main/java/org/apache/accumulo/core/client/PluginEnvironment.java#L41) and it supports an operation like `isSet()`."", 'commenter': 'keith-turner'}, {'comment': ""I think we'd have to add a lot more plumbing changes to make something like `isSet` or anything else in a dedicated type useful. Personally, I think Map is more than sufficient for now."", 'commenter': 'ctubbsii'}, {'comment': 'I just noticed that the RFile API has two different builder methods that take properties as a parameter (one that takes a map and one that takes an iterator). https://github.com/apache/accumulo/blob/2ff2618b1c49d01a3ded44173b522c5670cfdeac/core/src/main/java/org/apache/accumulo/core/client/rfile/RFile.java#L185', 'commenter': 'milleruntime'}, {'comment': ""Since there are two use cases for each type, it might be better to have both methods. Also, the method that returns the iterable is used quite a lot across the Accumulo code (a quick search showed me 51 places). I guess today I am back to not deprecating the method. If the implementation uses the same underlying method and without something better (like a new type that supports `isSet()`) it doesn't seem necessary to deprecate. Sorry, API design is difficult!"", 'commenter': 'milleruntime'}, {'comment': ""I'm still in favor of deprecating. Some of the objections so far have been against removing, not against deprecating. The bar for deprecating should be much lower. I think API bloat is a harm that should be considered, and I think the SemVer process mitigates against the concerns over the worst effects of disruption. I would like to clean up API bloat over time, to streamline and simplify our API for ease of use in future, as well as accommodate existing users. I think the deprecation process with SemVer helps balance those competing goals, and think that some of the objections would have our API bloat over time. But, I think API evolution should also include the loss of vestigial APIs.\r\n\r\nHowever, as it stands, I'm outvoted here. So, unless others agree with me, that we should slowly phase out the old vestigial API, using deprecation as a start, we should keep both and not deprecate at this time."", 'commenter': 'ctubbsii'}, {'comment': 'My thoughts on deprecating are not super well defined, but my thinking on deprecation of API is that it should be related to functionality that will be removed or is fundamentally broken.  For this particular case the functionality behind the existing API in questions works fine and will not be removed.  Also the new API being introduced is functionally equivalent to the existing API (each could be fully implemented using only the other).  We could introduce an infinite number of functionally equivalent APIs and deprecate existing APIs.\r\n\r\nUnrelated to deprecation, I also tend to think that new APIs should normally be motivated by new functionality.  However its not that well defined for me, because I can think of exceptions to this guide.   In this case there is no new functionality, but the new API sill makes sense to me for other reasons.  Thinking about this a bit more, it makes me wonder if there are any new functional changes related to configuration on the horizon.  If so could those require new APIs?  If that were the case it may be prudent to wait on this change and consider it w/ that new functionality.  ', 'commenter': 'keith-turner'}]"
2122,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -617,10 +617,28 @@ void removeProperty(String tableName, String property)
    *         recently changed properties may not be visible immediately.
    * @throws TableNotFoundException
    *           if the table does not exist
+   * @deprecated since 2.1.0; use {@link #getPropertiesMap(String)} instead.
    */
+  @Deprecated(since = ""2.1.0"")
   Iterable<Entry<String,String>> getProperties(String tableName)
       throws AccumuloException, TableNotFoundException;
 
+  /**
+   * Gets properties of a table. This operation is asynchronous and eventually consistent. It is not
+   * guaranteed that all tablets in a table will return the same values. Within a few seconds
+   * without another change, all tablets in a table should be consistent. The clone table feature
+   * can be used if consistency is required. This new method returns a Map instead of an Iterable.
+   *
+   * @param tableName
+   *          the name of the table
+   * @return all properties visible by this table (system and per-table properties). Note that
+   *         recently changed properties may not be visible immediately.
+   * @throws TableNotFoundException
+   *           if the table does not exist
+   */
+  Map<String,String> getPropertiesMap(String tableName)","[{'comment': 'Would the name ""getConfiguration"" make more sense here? We always talk in terms of ""per-table configuration"", not ""per-table properties"".', 'commenter': 'ctubbsii'}, {'comment': 'When thinking about the name, may want to consider [the methods in InstanceOperations](https://github.com/apache/accumulo/blob/8a636a3ba91f5dae1d8b09b095178889a7d79c1d/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java#L69-L77)', 'commenter': 'keith-turner'}, {'comment': ""To be consistent with those, getTableConfiguration and getNamespaceConfiguration might be most appropriate, but those are quite long, so I'm not 100% sold on those."", 'commenter': 'ctubbsii'}, {'comment': '> getTableConfiguration and getNamespaceConfiguration\r\n\r\nYeah those are long.  The suggestion of `getConfiguration` is more similar to the ones in InstanceOperations.  ', 'commenter': 'keith-turner'}, {'comment': '> Yeah those are long. The suggestion of `getConfiguration` is more similar to the ones in InstanceOperations.\r\n\r\nI have gone with `getConfiguration` . I think the consensus was this was the most appropriate in this situation.', 'commenter': 'foster33'}]"
2122,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsImpl.java,"@@ -219,7 +220,28 @@ public void removeProperty(final String namespace, final String property)
     } catch (Exception e) {
       throw new AccumuloException(e);
     }
+  }
 
+  @Override
+  public Map<String,String> getPropertiesMap(final String namespace)
+      throws AccumuloException, NamespaceNotFoundException {
+    checkArgument(namespace != null, ""namespace is null"");
+    try {
+      return ServerClient.executeRaw(context, client -> client
+          .getNamespaceConfiguration(TraceUtil.traceInfo(), context.rpcCreds(), namespace));
+    } catch (ThriftTableOperationException e) {
+      switch (e.getType()) {
+        case NAMESPACE_NOTFOUND:
+          throw new NamespaceNotFoundException(e);
+        case OTHER:
+        default:
+          throw new AccumuloException(e.description, e);
+      }
+    } catch (AccumuloException e) {
+      throw e;
+    } catch (Exception e) {
+      throw new AccumuloException(e);
+    }","[{'comment': ""Instead of having duplicate code here and in the `getProperties` methods, the `getProperties` methods should be changed to call this method and then call `.entrySet()` on the map, since that's the only difference between these two method implementations anyway."", 'commenter': 'ctubbsii'}]"
2122,core/src/main/java/org/apache/accumulo/core/clientImpl/OfflineIterator.java,"@@ -284,7 +284,7 @@ private TabletMetadata getTabletFiles(Range nextRange) {
     // possible race condition here, if table is renamed
     String tableName = Tables.getTableName(context, tableId);
     AccumuloConfiguration acuTableConf =
-        new ConfigurationCopy(context.tableOperations().getProperties(tableName));
+        new ConfigurationCopy(context.tableOperations().getPropertiesMap(tableName));","[{'comment': ""Do we still need the old ConfigurationCopy constructor now that these have been changed to use the constructor that uses a Map? If not, I would delete the constructor that takes an Iterable. It's safe to make that change, since it's internal only, and not public API."", 'commenter': 'ctubbsii'}, {'comment': 'I have not made any changes to this part. I felt as though it might be better for someone with more experience with the use and nature of the ConfigurationCopy constructor to make the decision on this.', 'commenter': 'foster33'}]"
2122,server/base/src/main/java/org/apache/accumulo/server/util/Admin.java,"@@ -529,7 +529,8 @@ private void printNameSpaceConfiguration(AccumuloClient accumuloClient, String n
     try (BufferedWriter nsWriter = new BufferedWriter(new FileWriter(namespaceScript, UTF_8))) {
       nsWriter.write(createNsFormat.format(new String[] {namespace}));
       TreeMap<String,String> props = new TreeMap<>();
-      for (Entry<String,String> p : accumuloClient.namespaceOperations().getProperties(namespace)) {
+      for (Entry<String,String> p : accumuloClient.namespaceOperations().getPropertiesMap(namespace)
+          .entrySet()) {
         props.put(p.getKey(), p.getValue());
       }","[{'comment': ""Many of these can be simplified with `Map.forEach`:\r\n\r\n```suggestion\r\n      accumuloClient.namespaceOperations().getPropertiesMap(namespace).forEach(props::put);\r\n```\r\n\r\nBasically, if the body of the loop doesn't throw a checked exception, this is possible, and almost always better."", 'commenter': 'ctubbsii'}]"
2122,shell/src/main/java/org/apache/accumulo/shell/commands/CreateTableCommand.java,"@@ -146,8 +146,9 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
     // Copy options if flag was set
     if (cl.hasOption(createTableOptCopyConfig.getOpt())) {
       if (shellState.getAccumuloClient().tableOperations().exists(tableName)) {
-        final Iterable<Entry<String,String>> configuration = shellState.getAccumuloClient()
-            .tableOperations().getProperties(cl.getOptionValue(createTableOptCopyConfig.getOpt()));
+        final Iterable<Entry<String,String>> configuration =
+            shellState.getAccumuloClient().tableOperations()
+                .getPropertiesMap(cl.getOptionValue(createTableOptCopyConfig.getOpt())).entrySet();","[{'comment': ""Instead of calling the new method, only to call entrySet and assign it to an Iterable, many of these would make more sense being assigned to a variable that is of the Map type instead. The assignments to Iterable types are very clunky, and many of them probably only were written that way because that's the type that the old method returned, not because we wanted them to be an Iterable."", 'commenter': 'ctubbsii'}]"
2122,shell/src/main/java/org/apache/accumulo/shell/commands/ShellPluginConfigurationCommand.java,"@@ -99,7 +99,8 @@ protected void removePlugin(final CommandLine cl, final Shell shellState, final
       final Shell shellState, final Class<T> clazz, final Property pluginProp) {
     Iterator<Entry<String,String>> props;
     try {
-      props = shellState.getAccumuloClient().tableOperations().getProperties(tableName).iterator();
+      props = shellState.getAccumuloClient().tableOperations().getPropertiesMap(tableName)
+          .entrySet().iterator();
     } catch (AccumuloException | TableNotFoundException e) {","[{'comment': 'This code definitely should assign it to a Map type, and instead of iterating over the iterator, it should loop over the entrySet or call Map.forEach in the loop below this assignment.', 'commenter': 'ctubbsii'}]"
2122,test/src/main/java/org/apache/accumulo/test/functional/DurabilityIT.java,"@@ -165,10 +165,10 @@ public void testMetaDurability() throws Exception {
     try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
       String tableName = getUniqueNames(1)[0];
       c.instanceOperations().setProperty(Property.TABLE_DURABILITY.getKey(), ""none"");
-      Map<String,String> props = map(c.tableOperations().getProperties(MetadataTable.NAME));","[{'comment': ""Not sure if the map method is still needed after this change. If it isn't needed, it should be deleted."", 'commenter': 'ctubbsii'}]"
2122,test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java,"@@ -862,7 +861,8 @@ private void testArbitraryProperty(AccumuloClient c, String tableName, boolean h
 
       // Loop through properties to make sure the new property is added to the list
       int count = 0;
-      for (Entry<String,String> property : c.tableOperations().getProperties(tableName)) {
+      for (Entry<String,String> property : c.tableOperations().getPropertiesMap(tableName)
+          .entrySet()) {
         if (property.getKey().equals(propertyName) && property.getValue().equals(description1))
           count++;
       }","[{'comment': 'This is a place that could be cleaned up quite a bit with a stream:\r\n\r\n```java\r\nlong count = c.tableOperations().getPropertiesMap(tableName).entrySet().stream().filter(e -> e.getKey().equals(propertyName) && e.getValue().equals(description1)).count();\r\n```', 'commenter': 'ctubbsii'}]"
2122,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsHelper.java,"@@ -72,9 +72,7 @@ public void removeIterator(String namespace, String name, EnumSet<IteratorScope>
     if (!exists(namespace))
       throw new NamespaceNotFoundException(null, namespace, null);
     Map<String,String> copy = new TreeMap<>();
-    for (Entry<String,String> property : this.getProperties(namespace)) {
-      copy.put(property.getKey(), property.getValue());
-    }
+    this.getConfiguration(namespace).forEach(copy::put);","[{'comment': ""This could be:\r\n\r\n```java\r\nMap<String,String> copy = Map.copyOf(this.getConfiguration(namespace));\r\n```\r\n\r\n(so long as we aren't making changes to the copy, because this would make an immutable copy)\r\n\r\nThere are a few such occurrences, if you want to change them."", 'commenter': 'ctubbsii'}]"
2122,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsHelper.java,"@@ -217,7 +215,7 @@ public void removeConstraint(String namespace, int number)
   public Map<String,Integer> listConstraints(String namespace)
       throws AccumuloException, NamespaceNotFoundException, AccumuloSecurityException {
     Map<String,Integer> constraints = new TreeMap<>();
-    for (Entry<String,String> property : this.getProperties(namespace)) {
+    for (Entry<String,String> property : this.getConfiguration(namespace).entrySet()) {","[{'comment': ""Since we're not deprecating, these that are longer/less convenient than the existing code can be reverted back to their former `getProperties`. Sorry for the churn in your PR!"", 'commenter': 'ctubbsii'}]"
2122,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -617,10 +617,30 @@ void removeProperty(String tableName, String property)
    *         recently changed properties may not be visible immediately.
    * @throws TableNotFoundException
    *           if the table does not exist
+   * @since 1.6.0
+   * @deprecated since 2.1.0; use {@link #getConfiguration(String)} (String)} instead.
    */
+  @Deprecated(since = ""2.1.0"")
   Iterable<Entry<String,String>> getProperties(String tableName)
       throws AccumuloException, TableNotFoundException;","[{'comment': 'If a default method is used in the interface, most, if not all, of the implementing classes and subclasses can simply remove their implementing method:\r\n\r\n```suggestion\r\n   * @since 1.6.0\r\n   */\r\n  default Iterable<Entry<String,String>> getProperties(String tableName)\r\n    throws AccumuloException, TableNotFoundException {\r\n    return getConfiguration(tableName).entrySet();\r\n  }\r\n```\r\n\r\n(and the same can be done in NamespaceOperations and its implementing classes and subclasses)', 'commenter': 'ctubbsii'}]"
2122,test/src/main/java/org/apache/accumulo/test/replication/StatusCombinerMacIT.java,"@@ -86,11 +86,9 @@ public void testCombinerSetOnMetadata() throws Exception {
       assertTrue(scopes.contains(IteratorScope.minc));
       assertTrue(scopes.contains(IteratorScope.majc));
 
-      Iterable<Entry<String,String>> propIter = tops.getProperties(MetadataTable.NAME);
+      Map<String,String> config = tops.getConfiguration(MetadataTable.NAME);
       HashMap<String,String> properties = new HashMap<>();
-      for (Entry<String,String> entry : propIter) {
-        properties.put(entry.getKey(), entry.getValue());
-      }
+      config.forEach(properties::put);","[{'comment': 'Another opportunity for `Map.copyOf`', 'commenter': 'ctubbsii'}]"
2122,core/src/test/java/org/apache/accumulo/core/clientImpl/TableOperationsHelperTest.java,"@@ -144,10 +144,15 @@ public void removeProperty(String tableName, String property) {
 
     @Override
     public Iterable<Entry<String,String>> getProperties(String tableName) {
+      return getConfiguration(tableName).entrySet();
+    }","[{'comment': 'This method can just be deleted now, since the default method exists in the interface.', 'commenter': 'ctubbsii'}, {'comment': 'I had deleted this at first but then had put it back because I was unsure. I will fix this.', 'commenter': 'foster33'}]"
2133,assemble/bin/accumulo-cluster,"@@ -147,17 +147,17 @@ function start_all() {
     start_tservers
   fi
 
-  for host in $(egrep -v '(^#|^\s*$)' ""${conf}/$manager_file""); do
-    start_service ""$host"" manager
-  done
+ grep -E -v '(^#|^\s*$)' < ""${conf}/$manager_file"" |  while read -r host; do
+  start_service ""$host"" manager
+ done
 
-  for host in $(egrep -v '(^#|^\s*$)' ""${conf}/gc""); do
-    start_service ""$host"" gc
-  done
+  grep -E -v '(^#|^\s*$)' < ""${conf}/gc"" |  while read -r host; do
+  start_service ""$host"" gc
+ done
 
-  for host in $(egrep -v '(^#|^\s*$)' ""${conf}/tracers""); do
-    start_service ""$host"" tracer
-  done
+  grep -E -v '(^#|^\s*$)' < ""${conf}/tracers"" |  while read -r host; do
+  start_service ""$host"" tracer
+ done","[{'comment': ""The indentation changes here don't look right."", 'commenter': 'ctubbsii'}]"
2133,assemble/bin/accumulo-cluster,"@@ -128,7 +128,7 @@ function start_service() {
 function start_tservers() {
   echo -n ""Starting tablet servers ...""
   count=1
-  for server in $(egrep -v '(^#|^\s*$)' ""${conf}/tservers""); do
+ grep -E -v '(^#|^\s*$)' < ""${conf}/tservers"" |  while read -r server; do","[{'comment': 'The indentation here doesn\'t look right. Also, you don\'t need to read in the file with `<` to read it in as STDIN, because grep itself takes file names as arguments. So, you can just do:\r\n\r\n```suggestion\r\n  grep -Ev \'(^#|^\\s*$)\' ""${conf}/tservers"" | while read -r server; do\r\n```\r\n', 'commenter': 'ctubbsii'}]"
2133,assemble/bin/accumulo-cluster,"@@ -299,9 +299,9 @@ function stop_all() {
 
 function stop_here() {
   # Determine hostname without errors to user
-  hosts_to_check=($(hostname -a 2> /dev/null | head -1) $(hostname -f))
+  hosts_to_check=""$(hostname -a 2> /dev/null | head -1) $(hostname -f)""","[{'comment': 'This is wrong. It should be assigning as an array, not a flat string. To properly quote the elements of the array, do:\r\n\r\n```suggestion\r\n  hosts_to_check=(""$(hostname -a 2> /dev/null | head -1)"" ""$(hostname -f)"")\r\n```', 'commenter': 'ctubbsii'}]"
2133,assemble/bin/accumulo-cluster,"@@ -299,9 +299,9 @@ function stop_all() {
 
 function stop_here() {
   # Determine hostname without errors to user
-  hosts_to_check=($(hostname -a 2> /dev/null | head -1) $(hostname -f))
+  hosts_to_check=""$(hostname -a 2> /dev/null | head -1) $(hostname -f)""
 
-  if egrep -q localhost\|127.0.0.1 ""${conf}/tservers""; then
+  if grep -E -q localhost\|127.0.0.1 ""${conf}/tservers""; then","[{'comment': 'The pattern here would make more sense quoted, rather than escaped, and it looks like the dots aren\'t being matched properly. Here\'s a fix:\r\n\r\n```suggestion\r\n  if grep -Eq \'localhost|127[.]0[.]0[.]1\' ""${conf}/tservers""; then\r\n```', 'commenter': 'ctubbsii'}]"
2133,assemble/bin/accumulo-cluster,"@@ -59,16 +59,16 @@ function verify_config {
 
   unset manager1
   if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(egrep -v '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+    manager1=$(grep -E -v '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)","[{'comment': 'Single-character options can be combined. This doesn\'t really matter, but the end result of converting from `egrep -v` to `grep -Ev` is that it is the same number of characters, rather than `grep -E -v`, which is 2 more.\r\n\r\n```suggestion\r\n    manager1=$(grep -Ev \'(^#|^\\s*$)\' ""${conf}/$manager_file"" | head -1)\r\n```', 'commenter': 'ctubbsii'}]"
2133,assemble/bin/accumulo-cluster,"@@ -235,21 +235,21 @@ function stop_tservers() {
 function kill_all() {
   echo ""Killing Accumulo cluster...""
 
-  for manager in $(grep -v '^#' ""${conf}/$manager_file""); do
+  grep -v '^#' ""${conf}/$manager_file"" | while read -r manager; do
     kill_service ""$manager"" manager
   done
 
-  for gc in $(grep -v '^#' ""${conf}/gc""); do
+  grep -v '^#' ""${conf}/gc"" | while read -r gc; do
     kill_service ""$gc"" gc
   done
 
   kill_service ""$monitor"" monitor
 
-  for tracer in $(egrep -v '(^#|^\s*$)' ""${conf}/tracers""); do
+  grep -Ev '(^#|^\s*$)' ""${conf}/tracers"" | while read -r tracer; do
     kill_service ""$tracer"" tracer
   done
 
-  for host in $(egrep -v '(^#|^\s*$)' ""${conf}/tservers""); do
+  grep -Ev '(^#|^\s*$)' ""${conf}/tservers"" | while read -r server; do","[{'comment': 'Wrong variable name:\r\n\r\n```suggestion\r\n  grep -Ev \'(^#|^\\s*$)\' ""${conf}/tservers"" | while read -r host; do\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Good catch. Thanks', 'commenter': 'Manno15'}]"
2150,core/src/test/java/org/apache/accumulo/core/clientImpl/TableOperationsHelperTest.java,"@@ -267,7 +269,7 @@ void check(TableOperationsHelper t, String tablename, String[] values) throws Ex
       String[] parts = value.split(""="", 2);
       expected.put(parts[0], parts[1]);
     }
-    Map<String,String> actual = Map.copyOf(t.getConfiguration(tablename));","[{'comment': ""This one doesn't matter. Map equality doesn't depend on order, only equivalent mappings. It'd be better to avoid the use of Guava's method here."", 'commenter': 'ctubbsii'}]"
2160,core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java,"@@ -347,7 +347,7 @@ public SamplerConfiguration getSamplerConfiguration() {
       for (int i = 0; i < sources.length; i++) {
         // TODO may have been a bug with multiple files and caching in older version...
         FSDataInputStream inputStream = (FSDataInputStream) sources[i].getInputStream();
-        CachableBuilder cb = new CachableBuilder().cacheId(""source-"" + i).input(inputStream)
+        CachableBuilder cb = new CachableBuilder().cacheId(opts.in.getPaths()[i]).input(inputStream)","[{'comment': ""While this is more informative than `source-1,source-2,...`, I'm not sure this will work in all cases. The Opts are constructed using RFileScannerBuilder from either a set of RFileSource objects, or a set of String filenames. We only have the path information if the builder used the String filenames. But, the RFileSource version only has InputStreams.\r\n\r\nThe `getSources()` method will translate any provided String filenames into RFileSources, but any that were passed in as RFileSource won't materialize path information in the reverse, in your new `getPaths()` method. Rather, that new method you added will only show paths if it was constructed with paths. It won't show anything that was constructed with RFileSources."", 'commenter': 'ctubbsii'}, {'comment': 'What about starting cacheId with ""cache-"" and then use Random int for the second part? With something like 1000 max. So we always set it to ""cache-####"". We could just create one random integer per scanner and then add ""i"".', 'commenter': 'milleruntime'}, {'comment': ""Found `SplittableRandom` which is supposed to be better and faster than `Random`. Whether we need a random or not, it is nice that it doesn't get dinged by sec-bugs."", 'commenter': 'milleruntime'}]"
2160,core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java,"@@ -339,6 +340,7 @@ public SamplerConfiguration getSamplerConfiguration() {
   @Override
   public Iterator<Entry<Key,Value>> iterator() {
     try {
+      int rand = new SplittableRandom().nextInt(1_000_000);","[{'comment': ""(Could also have used ThreadLocalRandom instead)\r\n\r\nI think you're trying to use random numbers to get a unique value for the cache keys. However, since the cache only exists for the lifetime of the process, a 1-up `final static AtomicLong` would be better than random for guaranteeing uniqueness for the lifetime of the process. Could even make it local to the current class, as in:\r\n\r\n```java\r\nprivate static final AtomicLong counter = new AtomicLong(0);\r\n...\r\n  .cacheId(this.getClass().getName() + counter.incrementAndGet())\r\n...\r\n```\r\n\r\nAlso (and this shows my lack of understanding of the caching code), wouldn't we want these cache keys to be predictable so we can use them to look up cached blocks later? What value are they for efficient subsequent scans if they contain random elements (or even unpredictable 1-up elements) that prevent looking up the cached content?"", 'commenter': 'ctubbsii'}, {'comment': 'Reverted changing the cacheId 04920fb ', 'commenter': 'milleruntime'}]"
2160,core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java,"@@ -347,7 +349,7 @@ public SamplerConfiguration getSamplerConfiguration() {
       for (int i = 0; i < sources.length; i++) {
         // TODO may have been a bug with multiple files and caching in older version...
         FSDataInputStream inputStream = (FSDataInputStream) sources[i].getInputStream();
-        CachableBuilder cb = new CachableBuilder().cacheId(""source-"" + i).input(inputStream)
+        CachableBuilder cb = new CachableBuilder().input(inputStream, ""cache-"" + rand + i)","[{'comment': 'I like the idea of specifying the cache id as a required field alongside the input stream. However, I wonder if we need the cache key to be more strongly related to the input stream, like how Object.toString() shows unique names for objects.', 'commenter': 'ctubbsii'}, {'comment': 'Reverted changing the cacheId 04920fb and just specified what was there in the builder method.', 'commenter': 'milleruntime'}]"
2160,core/src/test/java/org/apache/accumulo/core/file/rfile/MultiLevelIndexTest.java,"@@ -18,6 +18,7 @@
  */
 package org.apache.accumulo.core.file.rfile;
 
+import static org.apache.accumulo.core.crypto.CryptoServiceFactory.newInstance;","[{'comment': ""I like static imports for some things, but `newInstance` seems too generic and prone to error, since it looks like `Class.newInstance` and it isn't obvious it's using the CryptoServiceFactory."", 'commenter': 'ctubbsii'}]"
2160,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/CachableBlockFile.java,"@@ -94,7 +89,8 @@ public CachableBuilder fsPath(FileSystem fs, Path dataFile) {
       return this;
     }
 
-    public CachableBuilder input(InputStream is) {
+    public CachableBuilder input(InputStream is, String cacheId) {","[{'comment': 'This is a nice change.', 'commenter': 'keith-turner'}]"
2171,core/src/test/resources/accumulo2.properties,"@@ -24,6 +24,6 @@ instance.volumes=hdfs://localhost:8020/accumulo123
 instance.zookeeper.host=myhost123:2181
 table.durability=flush
 tserver.memory.maps.native.enabled=false
-tserver.walog.max.size=256M
+tserver.wal.max.size=256M","[{'comment': 'Good catch.', 'commenter': 'milleruntime'}]"
2171,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -360,25 +365,59 @@
           + "" When there are more RFiles than this setting multiple passes must be""
           + "" made, which is slower. However opening too many RFiles at once can cause""
           + "" problems.""),
+  TSERV_WAL_MAX_REFERENCED(""tserver.wal.max.referenced"", ""3"", PropertyType.COUNT,
+      ""When a tablet server has more than this many write ahead logs, any tablet referencing older ""
+          + ""logs over this threshold is minor compacted.  Also any tablet referencing this many ""
+          + ""logs or more will be compacted.""),
+  @Deprecated(since = ""2.1.0"")
+  @ReplacedBy(property = Property.TSERV_WAL_MAX_REFERENCED)
   TSERV_WALOG_MAX_REFERENCED(""tserver.walog.max.referenced"", ""3"", PropertyType.COUNT,
       ""When a tablet server has more than this many write ahead logs, any tablet referencing older ""
           + ""logs over this threshold is minor compacted.  Also any tablet referencing this many ""
           + ""logs or more will be compacted.""),
+  TSERV_WAL_MAX_SIZE(""tserver.wal.max.size"", ""1G"", PropertyType.BYTES,
+      ""The maximum size for each write-ahead log. See comment for property""
+          + "" tserver.memory.maps.max""),
+  @Deprecated(since = ""2.1.0"")
+  @ReplacedBy(property = Property.TSERV_WAL_MAX_SIZE)
   TSERV_WALOG_MAX_SIZE(""tserver.walog.max.size"", ""1G"", PropertyType.BYTES,
       ""The maximum size for each write-ahead log. See comment for property""
           + "" tserver.memory.maps.max""),
+  TSERV_WAL_MAX_AGE(""tserver.wal.max.age"", ""24h"", PropertyType.TIMEDURATION,
+      ""The maximum age for each write-ahead log.""),
+  @Deprecated(since = ""2.1.0"")
+  @ReplacedBy(property = Property.TSERV_WAL_MAX_AGE)
   TSERV_WALOG_MAX_AGE(""tserver.walog.max.age"", ""24h"", PropertyType.TIMEDURATION,
       ""The maximum age for each write-ahead log.""),
+  TSERV_WAL_TOLERATED_CREATION_FAILURES(""tserver.wal.tolerated.creation.failures"", ""50"",
+      PropertyType.COUNT,
+      ""The maximum number of failures tolerated when creating a new write-ahead""
+          + "" log. Negative values will allow unlimited creation failures. Exceeding this""
+          + "" number of failures consecutively trying to create a new write-ahead log""
+          + "" causes the TabletServer to exit.""),
+  @Deprecated(since = ""2.1.0"")
+  @ReplacedBy(property = Property.TSERV_WAL_TOLERATED_CREATION_FAILURES)
   TSERV_WALOG_TOLERATED_CREATION_FAILURES(""tserver.walog.tolerated.creation.failures"", ""50"",
       PropertyType.COUNT,
       ""The maximum number of failures tolerated when creating a new write-ahead""
           + "" log. Negative values will allow unlimited creation failures. Exceeding this""
           + "" number of failures consecutively trying to create a new write-ahead log""
           + "" causes the TabletServer to exit.""),
+  TSERV_WAL_TOLERATED_WAIT_INCREMENT(""tserver.wal.tolerated.wait.increment"", ""1000ms"",
+      PropertyType.TIMEDURATION,
+      ""The amount of time to wait between failures to create or write a write-ahead log.""),
+  @Deprecated(since = ""2.1.0"")
+  @ReplacedBy(property = Property.TSERV_WAL_TOLERATED_WAIT_INCREMENT)
   TSERV_WALOG_TOLERATED_WAIT_INCREMENT(""tserver.walog.tolerated.wait.increment"", ""1000ms"",
       PropertyType.TIMEDURATION,
       ""The amount of time to wait between failures to create or write a write-ahead log.""),
   // Never wait longer than 5 mins for a retry
+  TSERV_WAL_TOLERATED_MAXIMUM_WAIT_DURATION(""tserver.wal.maximum.wait.duration"", ""5m"",
+      PropertyType.TIMEDURATION,
+      ""The maximum amount of time to wait after a failure to create or write a write-ahead log.""),
+  // Never wait longer than 5 mins for a retry
+  @Deprecated(since = ""2.1.0"")
+  @ReplacedBy(property = Property.TSERV_WAL_TOLERATED_MAXIMUM_WAIT_DURATION)","[{'comment': 'Although the Property names are not sorted, we usually try to group common ones together. You could put the new tserver ones all together with the other ""tserver.wal"" properties. But I don\'t think this is a big deal since they can be sorted in an IDE.', 'commenter': 'milleruntime'}, {'comment': ""@milleruntime I'm not sure I understand your comment. It looks like these are already grouped together. Each new one looks like it was added directly next to the one it replaced, which is a nice, minimal change."", 'commenter': 'ctubbsii'}]"
2171,server/base/src/main/java/org/apache/accumulo/server/master/recovery/HadoopLogCloser.java,"@@ -36,7 +36,7 @@
 
   public HadoopLogCloser() {
     log.warn(""{} has been deprecated. Please update property {} to {} instead."",
-        getClass().getName(), Property.MANAGER_WALOG_CLOSER_IMPLEMETATION.getKey(),
+        getClass().getName(), Property.MANAGER_WAL_CLOSER_IMPLEMENTATION.getKey(),","[{'comment': ""I don't think you need to worry about resolving here. What you have is fine since this whole class was just deprecated in main. You just replaced the new property with a newer one."", 'commenter': 'milleruntime'}, {'comment': 'This is one of those props that got changed multiple times in the main branch. This warning was originally added to help facilitate the migration from `master.` to `manager.`. We can keep the`manager.walog.` version around to help facilitate that transition, but the log message should really only refer to the new `manager.wal.` name. So, I think this change is good here.', 'commenter': 'ctubbsii'}]"
2171,core/src/test/java/org/apache/accumulo/core/conf/SiteConfigurationTest.java,"@@ -61,7 +61,9 @@ public void testDefault() {
     assertEquals(""DEFAULT"", conf.get(Property.INSTANCE_SECRET));
     assertEquals("""", conf.get(Property.INSTANCE_VOLUMES));
     assertEquals(""120s"", conf.get(Property.GENERAL_RPC_TIMEOUT));
-    assertEquals(""1G"", conf.get(Property.TSERV_WALOG_MAX_SIZE));
+    @SuppressWarnings(""deprecation"")
+    Property deprecatedProp = Property.TSERV_WALOG_MAX_SIZE;
+    assertEquals(""1G"", conf.get(conf.resolve(Property.TSERV_WAL_MAX_SIZE, deprecatedProp)));","[{'comment': ""Since this test is just testing the default values for a bunch of props not set in a config file, it'd be fine to just use the new prop, without calling `conf.resolve`."", 'commenter': 'ctubbsii'}]"
2171,core/src/test/java/org/apache/accumulo/core/conf/SiteConfigurationTest.java,"@@ -75,7 +77,9 @@ public void testFile() {
     assertEquals(""mysecret"", conf.get(Property.INSTANCE_SECRET));
     assertEquals(""hdfs://localhost:8020/accumulo123"", conf.get(Property.INSTANCE_VOLUMES));
     assertEquals(""123s"", conf.get(Property.GENERAL_RPC_TIMEOUT));
-    assertEquals(""256M"", conf.get(Property.TSERV_WALOG_MAX_SIZE));
+    @SuppressWarnings(""deprecation"")
+    Property deprecatedProp = Property.TSERV_WALOG_MAX_SIZE;
+    assertEquals(""256M"", conf.get(conf.resolve(Property.TSERV_WAL_MAX_SIZE, deprecatedProp)));","[{'comment': ""Since this test is just testing that we can read the file correctly, we don't need to check the deprecated prop, just the new one, so there's no need to do `conf.resolve` here. I see that you already updated the config file to use the new prop, anyway, so just checking the new prop should suffice.\r\n\r\nThat is, unless you wanted this to serve as a test for `conf.resolve` itself... but I assume that's already unit tested elsewhere."", 'commenter': 'ctubbsii'}]"
2171,server/manager/src/main/java/org/apache/accumulo/manager/recovery/RecoveryManager.java,"@@ -197,9 +197,11 @@ public boolean recoverLogs(KeyExtent extent, Collection<Collection<String>> walo
         synchronized (this) {
           if (!closeTasksQueued.contains(sortId) && !sortsQueued.contains(sortId)) {
             AccumuloConfiguration aconf = manager.getConfiguration();
+            @SuppressWarnings(""deprecation"")
             LogCloser closer = Property.createInstanceFromPropertyName(aconf,
-                Property.MANAGER_WALOG_CLOSER_IMPLEMETATION, LogCloser.class,
-                new HadoopLogCloser());
+                aconf.resolve(Property.MANAGER_WAL_CLOSER_IMPLEMENTATION,
+                    Property.MANAGER_WALOG_CLOSER_IMPLEMETATION),","[{'comment': ""I think @milleruntime had suggested elsewhere in his review that it might not be necessary to resolve between these two, because the one being deprecated is entirely new. However, I think it probably still is necessary, because the upgrade code will read in `master.walog.closer.implementation` as `manager.walog.closer.implementation`, so it could still be in use inside the AccumuloConfiguration object. So, I think this is fine. If we want the upgrade code to do some additional translations directly to this new version, to eliminate the intermediate `manager.walog.` version, we can do that, but I don't think we should, because having the intermediate version helps users easily transition their old config files with simple `sed` commands, and that will break if we don't have a direct mapping of all `master.` props to `manager.` ones."", 'commenter': 'ctubbsii'}, {'comment': ""> having the intermediate version helps users easily transition their old config files with simple sed commands, and that will break if we don't have a direct mapping of all master. props to manager. ones.\r\n\r\nThat is a good point. It is unfortunate that we will change a user's property to a deprecated one but having the intermediate property makes the changes more flexible. I think this is OK as long as the version that eventually drops the `walog` properties, does the replace in the upgrade code. We have become better at resolving things in the upgrade code but this is another example of why you should never skip Major or Minor versions when upgrading."", 'commenter': 'milleruntime'}]"
2171,server/manager/src/test/resources/conf/accumulo-site.xml,"@@ -82,7 +82,7 @@
   </property>
 
   <property>
-    <name>tserver.walog.max.size</name>
+    <name>tserver.wal.max.size</name>","[{'comment': ""Eek. Do we still support XML files in 2.x? I thought we got rid of them. I don't see a reference to this file anywhere. I had thought maybe as a test case somewhere, but I didn't find it. This file might be able to be just deleted outright."", 'commenter': 'ctubbsii'}, {'comment': 'Would this be best to do in a future PR?', 'commenter': 'foster33'}, {'comment': 'Yeah this file can be removed in a separate PR.', 'commenter': 'milleruntime'}]"
2171,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java,"@@ -300,23 +300,35 @@ public void run() {
           }
         }), 5000, 5000, TimeUnit.MILLISECONDS);
 
-    final long walogMaxSize = aconf.getAsBytes(Property.TSERV_WALOG_MAX_SIZE);
-    final long walogMaxAge = aconf.getTimeInMillis(Property.TSERV_WALOG_MAX_AGE);
+    @SuppressWarnings(""deprecation"")
+    final long walMaxSize =
+        aconf.getAsBytes(aconf.resolve(Property.TSERV_WAL_MAX_SIZE, Property.TSERV_WALOG_MAX_SIZE));
+    @SuppressWarnings(""deprecation"")
+    final long walMaxAge = aconf
+        .getTimeInMillis(aconf.resolve(Property.TSERV_WAL_MAX_AGE, Property.TSERV_WALOG_MAX_AGE));
     final long minBlockSize =
         context.getHadoopConf().getLong(""dfs.namenode.fs-limits.min-block-size"", 0);
-    if (minBlockSize != 0 && minBlockSize > walogMaxSize) {
+    if (minBlockSize != 0 && minBlockSize > walMaxSize) {
+      @SuppressWarnings(""deprecation"")
+      Property deprecatedProp = Property.TSERV_WALOG_MAX_SIZE;
       throw new RuntimeException(""Unable to start TabletServer. Logger is set to use blocksize ""
-          + walogMaxSize + "" but hdfs minimum block size is "" + minBlockSize
-          + "". Either increase the "" + Property.TSERV_WALOG_MAX_SIZE
+          + walMaxSize + "" but hdfs minimum block size is "" + minBlockSize
+          + "". Either increase the "" + aconf.resolve(Property.TSERV_WAL_MAX_SIZE, deprecatedProp)","[{'comment': ""This message can be changed to only refer to the new property. We don't need to recommend the old one just because they were previously using it."", 'commenter': 'ctubbsii'}]"
2171,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1926,7 +1926,7 @@ public void checkIfMinorCompactionNeededForLogs(List<DfsLogger> closedLogs) {
     // grab this outside of tablet lock.
     @SuppressWarnings(""deprecation"")
     int maxLogs = tableConfiguration.getCount(tableConfiguration
-        .resolve(Property.TSERV_WALOG_MAX_REFERENCED, Property.TABLE_MINC_LOGS_MAX));
+        .resolve(Property.TSERV_WAL_MAX_REFERENCED, Property.TSERV_WALOG_MAX_REFERENCED));","[{'comment': ""It looks like `TABLE_MINC_LOGS_MAX` was deprecated in 2.0.0. There should be two resolves here to ensure we use the right one:\r\n\r\n```suggestion\r\n        .resolve(Property.TSERV_WAL_MAX_REFERENCED, tableConfiguration.resolve(Property.TSERV_WALOG_MAX_REFERENCED, Property.TABLE_MINC_LOGS_MAX));\r\n```\r\n(this suggestion isn't formatted)\r\n\r\nIf we have to do this a lot, it might make sense to modify the `resolve` command to take more than one argument, but probably not worth it if there's only one or two."", 'commenter': 'ctubbsii'}]"
2171,test/src/main/java/org/apache/accumulo/test/functional/ManyWriteAheadLogsIT.java,"@@ -87,11 +88,13 @@ public void alterConfig() throws Exception {
     try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {
       InstanceOperations iops = client.instanceOperations();
       Map<String,String> conf = iops.getSystemConfiguration();
+      AccumuloConfiguration aconf = getCluster().getSiteConfiguration();
+      @SuppressWarnings(""deprecation"")
+      Property deprecatedProp = Property.TSERV_WALOG_MAX_SIZE;
       majcDelay = conf.get(Property.TSERV_MAJC_DELAY.getKey());
-      walogSize = conf.get(Property.TSERV_WALOG_MAX_SIZE.getKey());
-
+      walSize = conf.get(aconf.resolve(Property.TSERV_WAL_MAX_SIZE, deprecatedProp).getKey());
       iops.setProperty(Property.TSERV_MAJC_DELAY.getKey(), ""1"");
-      iops.setProperty(Property.TSERV_WALOG_MAX_SIZE.getKey(), ""1M"");
+      iops.setProperty(aconf.resolve(Property.TSERV_WAL_MAX_SIZE, deprecatedProp).getKey(), ""1M"");","[{'comment': ""These can just set the new property, and don't need to make a decision based on what is set already in the config. First, we control the config for the test cluster, and you've already set the cluster to use the new property, and second, even if a cluster did use the old property in its existing config, setting only the new property is sufficient, since the new property will overrule the old one."", 'commenter': 'ctubbsii'}]"
2171,test/src/main/java/org/apache/accumulo/test/functional/ManyWriteAheadLogsIT.java,"@@ -103,8 +106,12 @@ public void resetConfig() throws Exception {
     if (majcDelay != null) {
       try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {
         InstanceOperations iops = client.instanceOperations();
+        AccumuloConfiguration conf = getServerContext().getConfiguration();
+        @SuppressWarnings(""deprecation"")
+        Property deprecatedProp = Property.TSERV_WALOG_MAX_SIZE;
         iops.setProperty(Property.TSERV_MAJC_DELAY.getKey(), majcDelay);
-        iops.setProperty(Property.TSERV_WALOG_MAX_SIZE.getKey(), walogSize);
+        iops.setProperty(conf.resolve(Property.TSERV_WAL_MAX_SIZE, deprecatedProp).getKey(),
+            walSize);","[{'comment': 'Same here. This only needs to use the new property.', 'commenter': 'ctubbsii'}]"
2180,server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java,"@@ -90,19 +92,20 @@ private static void checkTable(TableId tableId, TreeSet<KeyExtent> tablets) {
       lastEndRow = tabke.endRow();
     }
     if (everythingLooksGood)
-      System.out.println(""All is well for table "" + tableId);
+      System.out.println(""...All is well for table "" + tableName + "" ("" + tableId + "")"");
     else
       sawProblems = true;
   }
 
   private static void checkMetadataAndRootTableEntries(String tableNameToCheck, ServerUtilOpts opts)
       throws Exception {
-    System.out.println(""Checking table: "" + tableNameToCheck);
+    TableId tableCheckId = Tables.getTableId(opts.getServerContext(), tableNameToCheck);
+    System.out.println(""Checking tables whos metadata is found in: "" + tableNameToCheck + "" (""","[{'comment': '```suggestion\r\n    System.out.println(""Checking tables whose metadata is found in: "" + tableNameToCheck + "" (""\r\n```', 'commenter': 'ctubbsii'}]"
2180,server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java,"@@ -125,7 +128,12 @@ private static void checkMetadataAndRootTableEntries(String tableNameToCheck, Se
         TreeSet<KeyExtent> tablets = tables.get(tableId);
         if (tablets == null) {
 
-          tables.forEach(CheckForMetadataProblems::checkTable);
+          for (Entry<TableId,TreeSet<KeyExtent>> e : tables.entrySet()) {
+            TableId key = e.getKey();
+            TreeSet<KeyExtent> value = e.getValue();
+            String tableName = Tables.getTableName(opts.getServerContext(), key);
+            checkTable(key, value, tableName);
+          }","[{'comment': ""I think this line could have stayed the same and the name lookup could have occurred in the method. Also, need to handle the case that looking up the name fails, in case the table is deleted, but there is still metadata left in the metadata tablet. That wasn't an issue before because we only used the table ID before."", 'commenter': 'ctubbsii'}, {'comment': '>Also, need to handle the case that looking up the name fails, in case the table is deleted, but there is still metadata left in the metadata tablet\r\n\r\nIs there any way that I would easily be able to recreate this error in testing?', 'commenter': 'foster33'}, {'comment': ""> Is there any way that I would easily be able to recreate this error in testing?\r\n\r\nYou could put the `Tables.getTableName` in a different method, and then do a partial mock or subclass this utility to provide a different response. However, it's not critical. Just wrapping `Tables.getTableName` with a try-catch block and setting tableName to null if there's an exception should be sufficient, as long as none of the rest of the code minds it being null."", 'commenter': 'ctubbsii'}, {'comment': 'This should be addressed in my latest commit.', 'commenter': 'foster33'}]"
2180,server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java,"@@ -90,19 +92,20 @@ private static void checkTable(TableId tableId, TreeSet<KeyExtent> tablets) {
       lastEndRow = tabke.endRow();
     }
     if (everythingLooksGood)
-      System.out.println(""All is well for table "" + tableId);
+      System.out.println(""...All is well for table "" + tableName + "" ("" + tableId + "")"");","[{'comment': 'If these ""All is well"" messages are indented with `...`, then the error messages should also be.', 'commenter': 'ctubbsii'}]"
2180,server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java,"@@ -75,12 +86,12 @@ private static void checkTable(TableId tableId, TreeSet<KeyExtent> tablets) {
       KeyExtent tabke = tabIter.next();
       boolean broke = false;
       if (tabke.prevEndRow() == null) {
-        System.out
-            .println(""Table "" + tableId + "" has null prev end row in middle of table "" + tabke);
+        System.out.println(""Table "" + tableName + "" ("" + tableId","[{'comment': '```suggestion\r\n        System.out.println(""...Table "" + tableName + "" ("" + tableId\r\n```', 'commenter': 'ctubbsii'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/log/RecoveryLogsIterator.java,"@@ -56,42 +56,57 @@
   private final List<Scanner> scanners;
   private final Iterator<Entry<Key,Value>> iter;
 
+  public RecoveryLogsIterator(ServerContext context, List<Path> recoveryLogDirs,
+      boolean checkFirstKEy) throws IOException {
+    this(context, recoveryLogDirs, null, null, checkFirstKEy);
+  }
+
   /**
    * Scans the files in each recoveryLogDir over the range [start,end].
    */
-  RecoveryLogsIterator(ServerContext context, List<Path> recoveryLogDirs, LogFileKey start,
+  public RecoveryLogsIterator(ServerContext context, List<Path> recoveryLogDirs, LogFileKey start,
       LogFileKey end, boolean checkFirstKey) throws IOException {
 
     List<Iterator<Entry<Key,Value>>> iterators = new ArrayList<>(recoveryLogDirs.size());
     scanners = new ArrayList<>();
-    Range range = LogFileKey.toRange(start, end);
+    Range range = start != null ? LogFileKey.toRange(start, end) : null;
     var vm = context.getVolumeManager();
 
     for (Path logDir : recoveryLogDirs) {
       LOG.debug(""Opening recovery log dir {}"", logDir.getName());
-      List<Path> logFiles = getFiles(vm, logDir);
       var fs = vm.getFileSystemByPath(logDir);
 
-      // only check the first key once to prevent extra iterator creation and seeking
-      if (checkFirstKey) {
-        validateFirstKey(context, fs, logFiles, logDir);
-      }
-
-      for (Path log : logFiles) {
-        var scanner = RFile.newScanner().from(log.toString()).withFileSystem(fs)
-            .withTableProperties(context.getConfiguration()).build();
-
-        scanner.setRange(range);
+      // if path passed in is actually a file, read a single RFile
+      if (vm.getFileStatus(logDir).isFile()) {","[{'comment': ""This should be in LogReader. I don't think we want this check in `RecoveryLogsIterator` since it is used in very specific ways in the recovery code."", 'commenter': 'milleruntime'}, {'comment': 'I will look for alternatives. Have to work around a few inconsistencies with the old stuff and new. ', 'commenter': 'Manno15'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/logger/LogReader.java,"@@ -123,33 +130,44 @@ public void execute(String[] args) throws Exception {
       Set<Integer> tabletIds = new HashSet<>();
 
       for (String file : opts.files) {
-
         Path path = new Path(file);
         LogFileKey key = new LogFileKey();
         LogFileValue value = new LogFileValue();
 
-        if (fs.getFileStatus(path).isFile()) {
-          // read log entries from a simple hdfs file
-          try (final FSDataInputStream fsinput = fs.open(path);
-              DataInputStream input = DfsLogger.getDecryptingStream(fsinput, siteConfig)) {
-            while (true) {
-              try {
-                key.readFields(input);
-                value.readFields(input);
-              } catch (EOFException ex) {
-                break;
+        // read old style WALs
+        if (containsMapFile(fs, path)) {","[{'comment': ""I think this check should happen in the else below for the temporary sorted WALs. The first `isFile()` check takes care of reading from the main WALs (the files in /accumulo/wal), which hasn't changed. I think the else below should read the files in /accumulo/recovery. "", 'commenter': 'milleruntime'}, {'comment': ""That's how it was, but I ran into issues while trying to read the RFile with accumulo wal-info. It was simpler to figure out which version to use before figuring out if the user passed in a file or a directory. "", 'commenter': 'Manno15'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/logger/LogReader.java,"@@ -123,36 +122,17 @@ public void execute(String[] args) throws Exception {
       Set<Integer> tabletIds = new HashSet<>();
 
       for (String file : opts.files) {
-
         Path path = new Path(file);
-        LogFileKey key = new LogFileKey();
-        LogFileValue value = new LogFileValue();
-
-        if (fs.getFileStatus(path).isFile()) {
-          // read log entries from a simple hdfs file
-          try (final FSDataInputStream fsinput = fs.open(path);
-              DataInputStream input = DfsLogger.getDecryptingStream(fsinput, siteConfig)) {
-            while (true) {
-              try {
-                key.readFields(input);
-                value.readFields(input);
-              } catch (EOFException ex) {
-                break;
-              }
-              printLogEvent(key, value, row, rowMatcher, ke, tabletIds, opts.maxMutations);
-            }
-          } catch (LogHeaderIncompleteException e) {
-            log.warn(""Could not read header for {} . Ignoring..."", path);
-            continue;
-          }
-        } else {
-          // read the log entries sorted in a map file
-          try (RecoveryLogReader input = new RecoveryLogReader(fs, path)) {
-            while (input.hasNext()) {
-              Entry<LogFileKey,LogFileValue> entry = input.next();
-              printLogEvent(entry.getKey(), entry.getValue(), row, rowMatcher, ke, tabletIds,
-                  opts.maxMutations);
-            }
+        if (!fs.getFileStatus(path).isDirectory()) {
+          log.error(""No directory was given. Please pass in a recovery directory"");
+          continue;
+        }
+        // read the log entries sorted in a RFile
+        try (var rli = new RecoveryLogsIterator(context, Collections.singletonList(path), true)) {
+          while (rli.hasNext()) {
+            Entry<LogFileKey,LogFileValue> entry = rli.next();
+            printLogEvent(entry.getKey(), entry.getValue(), row, rowMatcher, ke, tabletIds,
+                opts.maxMutations);","[{'comment': 'This looks good for reading the sorted logs but we still want the section under `if (fs.getFileStatus(path).isFile())` for reading regular write ahead logs using `DfsLogger`.', 'commenter': 'milleruntime'}, {'comment': 'My mistake, I thought we previously discussed removing it. I will add it back. ', 'commenter': 'Manno15'}, {'comment': 'Also, if a user specifies a single sorted WAL to read, it will error out (the second part of your issue mentions this). I will look into adding a check to prevent this (something like if file.endsWith("",rf""), though a more descriptive error message could suffice. ', 'commenter': 'Manno15'}, {'comment': 'OK cool. If user passes in an rfile I think you could just create a singleton list to send as the param for `recoveryLogDirs`.', 'commenter': 'milleruntime'}, {'comment': ""A single rfile will not work with RLI currently. It doesn't contain the finish marker which is looked for in rli.getFiles (that seems to be the only issue with it not working). Also, since LogReader usually deals with one directory/file at a time since it's looped through what the user passes in, I already am using a singleton list to read in that path. I could change the passed in files (opts.files in LogReader) to be a List \\<path> but it doesn't seem necessary. "", 'commenter': 'Manno15'}, {'comment': ""Gotcha. I think making the user pass in the directory is fine. It should print the error if it doesn't have the finished file, correct? I think reading a partially sorted directory is undesirable so that should work."", 'commenter': 'milleruntime'}, {'comment': ""My latest commit adds an extra error message if it detects a file but the file ends with .rf. Otherwise, yes, it will error out if it gets past that but doesn't contain the finished file."", 'commenter': 'Manno15'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/log/RecoveryLogsIterator.java,"@@ -56,15 +56,20 @@
   private final List<Scanner> scanners;
   private final Iterator<Entry<Key,Value>> iter;
 
+  public RecoveryLogsIterator(ServerContext context, List<Path> recoveryLogDirs,
+      boolean checkFirstKEy) throws IOException {
+    this(context, recoveryLogDirs, null, null, checkFirstKEy);
+  }
+
   /**
    * Scans the files in each recoveryLogDir over the range [start,end].
    */
-  RecoveryLogsIterator(ServerContext context, List<Path> recoveryLogDirs, LogFileKey start,
+  public RecoveryLogsIterator(ServerContext context, List<Path> recoveryLogDirs, LogFileKey start,
       LogFileKey end, boolean checkFirstKey) throws IOException {
 
     List<Iterator<Entry<Key,Value>>> iterators = new ArrayList<>(recoveryLogDirs.size());
     scanners = new ArrayList<>();
-    Range range = LogFileKey.toRange(start, end);
+    Range range = start != null ? LogFileKey.toRange(start, end) : null;","[{'comment': 'This is more readable without the negations:\r\n\r\n```suggestion\r\n    Range range = start == null ? null : LogFileKey.toRange(start, end);\r\n```', 'commenter': 'ctubbsii'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/log/RecoveryLogsIterator.java,"@@ -56,15 +56,20 @@
   private final List<Scanner> scanners;
   private final Iterator<Entry<Key,Value>> iter;
 
+  public RecoveryLogsIterator(ServerContext context, List<Path> recoveryLogDirs,
+      boolean checkFirstKEy) throws IOException {
+    this(context, recoveryLogDirs, null, null, checkFirstKEy);
+  }","[{'comment': ""It seems like this new constructor was added for convenience, but it doesn't have a javadoc to explain how it differs from the other constructor. If your goal is to make it crystal clear when you would use these additional parameters and when you don't need them, instead of using constructors here you could use static methods whose names are descriptive in the API.\r\n\r\nSo, instead of the following in the calling code:\r\n\r\n```java\r\nvar iter = new RecoveryLogsIterator(context, dirs, start, stop, false);\r\nvar iter2 = new RecoveryLogsIterator(context, dirs, true);\r\n```\r\n\r\nit could look like:\r\n\r\n```java\r\nvar iter = RecoveryLogsIterator.scanRange(context, dirs, start, stop, false);\r\nvar iter2 = RecoveryLogsIterator.scanAll(context, dirs); // don't think you need the boolean for this case\r\n```\r\n\r\nThen, you can make the constructor private (or package-private, if you need it visible for testing), and the implementing static methods can do some additional parameter validation to ensure that when you specify a range, both arguments aren't null.\r\n\r\nThese are just general design tips, though. For this, since the new constructor is only ever used once, I'd probably just remove it and pass in nulls on the old constructor for the one time it is called. Minimally, if you're keeping the new constructor, it should have a javadoc (and you should double check to see if you actually need to pass the boolean, since I don't think it matters when the first key is null)."", 'commenter': 'ctubbsii'}, {'comment': 'It is probably best to delete it and use passed in null here. The only reason I added it was to keep it consistent with how `RecoveryLogReader` did a rangeless read. ', 'commenter': 'Manno15'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/logger/LogReader.java,"@@ -35,11 +36,13 @@
 import org.apache.accumulo.core.data.Mutation;
 import org.apache.accumulo.core.data.TableId;
 import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.fs.VolumeManager;
 import org.apache.accumulo.server.fs.VolumeManagerImpl;
 import org.apache.accumulo.start.spi.KeywordExecutable;
 import org.apache.accumulo.tserver.log.DfsLogger;
 import org.apache.accumulo.tserver.log.DfsLogger.LogHeaderIncompleteException;
-import org.apache.accumulo.tserver.log.RecoveryLogReader;","[{'comment': 'Is the old RecoveryLogReader not used after this? Can it be deleted?', 'commenter': 'ctubbsii'}, {'comment': 'It does appear that RecoveryLogReader is only used in its test now. Should I remove it in this PR or a new one? \r\n\r\n(Sorry on the original comment to this, I misunderstood what you meant)', 'commenter': 'Manno15'}, {'comment': 'I went ahead and added it to this PR but I can remove it. ', 'commenter': 'Manno15'}, {'comment': 'The subject line and ultimately the commit message when it is merged will provide the narrative to frame the scope of the work that was done in the commit. Currently, the subject line says ""Update LogReader to utilize RecoveryLogsIterator"". If there were room for a longer description, it could just as well have also said ""... instead of RecoveryLogReader"", because that\'s what the code ended up doing. So, I would consider removing the unused code to be part of this same change set and same scope of work. I don\'t see any reason to defer to a future PR, because that would be like leaving commented out or unused code in place. But, you gotta use your best judgment. Sometimes it\'s not clear whether its better to do it as follow on or not. In this case, I think removing the unused code should be included in the change.', 'commenter': 'ctubbsii'}, {'comment': 'That makes sense. I will keep in it this change and will add detail in the commit message once merged. ', 'commenter': 'Manno15'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/logger/LogReader.java,"@@ -123,12 +127,16 @@ public void execute(String[] args) throws Exception {
       Set<Integer> tabletIds = new HashSet<>();
 
       for (String file : opts.files) {
-
         Path path = new Path(file);
         LogFileKey key = new LogFileKey();
         LogFileValue value = new LogFileValue();
 
         if (fs.getFileStatus(path).isFile()) {
+          if (file.endsWith("".rf"")) {
+            log.error(
+                ""Can not read from a single rfile. Please pass in a directory for recovery logs."");
+            continue;
+          }
           // read log entries from a simple hdfs file
           try (final FSDataInputStream fsinput = fs.open(path);
               DataInputStream input = DfsLogger.getDecryptingStream(fsinput, siteConfig)) {","[{'comment': 'Given this error message and new behavior, it\'s not clear what this condition (single file, but not ending in "".rf"") is supposed to do. Is this for reading the older sequence files? A comment could go a long way here. The only comment it has is ""read log entries from a simple hdfs file"". Clearly, that can\'t be read from a simple RFile in HDFS, so that comment could be updated to make it clear what files it *can* read (i.e. what does ""simple hdfs file"" mean?).', 'commenter': 'ctubbsii'}, {'comment': 'This area of the code is still used for wal-info to read in a single wal file (just tested to confirm). I will add additional comments because I agree, it is confusing. ', 'commenter': 'Manno15'}, {'comment': 'I moved the comment up and tried to be more explicit in my latest commit. Let me know if you think I should add more information to it.', 'commenter': 'Manno15'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/logger/LogReader.java,"@@ -123,12 +127,16 @@ public void execute(String[] args) throws Exception {
       Set<Integer> tabletIds = new HashSet<>();
 
       for (String file : opts.files) {
-
         Path path = new Path(file);
         LogFileKey key = new LogFileKey();
         LogFileValue value = new LogFileValue();
 
         if (fs.getFileStatus(path).isFile()) {
+          if (file.endsWith("".rf"")) {","[{'comment': ""There's probably a constant for the RFile extension somewhere that could be used... not that it would change."", 'commenter': 'ctubbsii'}, {'comment': ""I couldn't find one and I see other parts of the code check against the extension like I did. "", 'commenter': 'Manno15'}, {'comment': ""It's fine. It's not a problem."", 'commenter': 'ctubbsii'}, {'comment': 'Based on the updated comments, I can see that the first case is for reading regular non-sorted WAL files, and not the sorted ones, whereas the `else` block is for reading sorted WAL files from a single directory. I think that could be made even more clear:\r\n\r\n```suggestion\r\n          // make sure it\'s a regular non-sorted WAL file, and not a single sorted WAL in RFile format\r\n          if (file.endsWith("".rf"")) {\r\n```\r\n\r\nThinking about this a bit, it\'s a little weird that we can\'t read a single sorted WAL file. We can read a directory containing one, but we can\'t read it by itself? That doesn\'t seem to be a useful restriction for this utility, which is primarily intended to be a troubleshooting utility. If we pass in a single RFile, I feel like this tool should still be able to read it. That could be done here or in a separate PR, unless there\'s a reason to not support that at all.\r\n', 'commenter': 'ctubbsii'}]"
2181,server/tserver/src/main/java/org/apache/accumulo/tserver/logger/LogReader.java,"@@ -123,13 +127,18 @@ public void execute(String[] args) throws Exception {
       Set<Integer> tabletIds = new HashSet<>();
 
       for (String file : opts.files) {
-
         Path path = new Path(file);
         LogFileKey key = new LogFileKey();
         LogFileValue value = new LogFileValue();
 
+        // read log entries from a single WAL file.
         if (fs.getFileStatus(path).isFile()) {
-          // read log entries from a simple hdfs file
+          if (file.endsWith("".rf"")) {
+            log.error(
+                ""Can not read from a single rfile. Please pass in a directory for recovery logs."");","[{'comment': '```suggestion\r\n                ""Unable to read from a single RFile. A non-sorted WAL file was expected. To read sorted WALs, please pass in a directory containing the sorted recovery logs."");\r\n```', 'commenter': 'ctubbsii'}]"
2181,server/tserver/src/test/java/org/apache/accumulo/tserver/log/RecoveryLogsIteratorTest.java,"@@ -0,0 +1,267 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.tserver.log;
+
+import static org.apache.accumulo.tserver.logger.LogEvents.DEFINE_TABLET;
+import static org.apache.accumulo.tserver.logger.LogEvents.OPEN;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Objects;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.conf.DefaultConfiguration;
+import org.apache.accumulo.core.crypto.CryptoServiceFactory;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.core.util.Pair;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.fs.VolumeManager;
+import org.apache.accumulo.server.fs.VolumeManagerImpl;
+import org.apache.accumulo.server.log.SortedLogState;
+import org.apache.accumulo.tserver.logger.LogFileKey;
+import org.apache.accumulo.tserver.logger.LogFileValue;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_IN"", justification = ""paths not set by user input"")
+public class RecoveryLogsIteratorTest {
+
+  private VolumeManager fs;
+  private File workDir;
+  static final KeyExtent extent = new KeyExtent(TableId.of(""table""), null, null);
+  static ServerContext context;
+  static LogSorter logSorter;
+
+  @Rule
+  public TemporaryFolder tempFolder =
+      new TemporaryFolder(new File(System.getProperty(""user.dir"") + ""/target""));
+
+  @Before
+  public void setUp() throws Exception {
+    context = createMock(ServerContext.class);
+    logSorter = new LogSorter(context, DefaultConfiguration.getInstance());
+
+    workDir = tempFolder.newFolder();
+    String path = workDir.getAbsolutePath();
+    assertTrue(workDir.delete());
+    fs = VolumeManagerImpl.getLocalForTesting(path);
+    expect(context.getVolumeManager()).andReturn(fs).anyTimes();
+    expect(context.getCryptoService()).andReturn(CryptoServiceFactory.newDefaultInstance())
+        .anyTimes();
+    expect(context.getConfiguration()).andReturn(DefaultConfiguration.getInstance()).anyTimes();
+    replay(context);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    fs.close();
+  }
+
+  static class KeyValue implements Comparable<KeyValue> {
+    public final LogFileKey key;
+    public final LogFileValue value;
+
+    KeyValue() {
+      key = new LogFileKey();
+      value = new LogFileValue();
+    }
+
+    @Override
+    public int hashCode() {
+      return Objects.hashCode(key) + Objects.hashCode(value);
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+      return this == obj || (obj instanceof KeyValue && 0 == compareTo((KeyValue) obj));
+    }
+
+    @Override
+    public int compareTo(KeyValue o) {
+      return key.compareTo(o.key);
+    }
+  }
+
+  @Test
+  public void testSimpleRLI() throws IOException {
+    KeyValue keyValue = new KeyValue();
+    keyValue.key.event = DEFINE_TABLET;
+    keyValue.key.seq = 0;
+    keyValue.key.tabletId = 1;
+    keyValue.key.tablet = extent;
+
+    KeyValue[] keyValues = {keyValue};
+
+    Map<String,KeyValue[]> logs = new TreeMap<>();
+    logs.put(""keyValues"", keyValues);
+
+    ArrayList<Path> dirs = new ArrayList<>();
+
+    createRecoveryDir(logs, dirs, true);
+
+    try (RecoveryLogsIterator rli = new RecoveryLogsIterator(context, dirs, null, null, false)) {
+      while (rli.hasNext()) {
+        Entry<LogFileKey,LogFileValue> entry = rli.next();
+        assertEquals(""TabletId does not match"", 1, entry.getKey().tabletId);
+        assertEquals(""Event does not match"", DEFINE_TABLET, entry.getKey().event);
+      }
+    }
+  }
+
+  @Test
+  public void testFinishMarker() throws IOException {
+    KeyValue keyValue = new KeyValue();
+    keyValue.key.event = DEFINE_TABLET;
+    keyValue.key.seq = 0;
+    keyValue.key.tabletId = 1;
+    keyValue.key.tablet = extent;
+
+    KeyValue[] keyValues = {keyValue};
+
+    Map<String,KeyValue[]> logs = new TreeMap<>();
+    logs.put(""keyValues"", keyValues);
+
+    ArrayList<Path> dirs = new ArrayList<>();
+
+    createRecoveryDir(logs, dirs, false);
+
+    try (RecoveryLogsIterator rli = new RecoveryLogsIterator(context, dirs, null, null, false)) {
+      while (rli.hasNext()) {
+        fail(""Finish marker should not be found. Exception should have been thrown."");
+      }
+    } catch (IOException e) {
+      // Expected exception
+    }
+  }","[{'comment': 'It may be possible to use assertThrows here, something like:\r\n```suggestion\r\n    assertThrows(""Finish marker should not be found"", IOException.class, () -> new RecoveryLogsIterator(context, dirs, null, null, false));\r\n  }\r\n```\r\nThis code would be slightly different if the expected exception is thrown in the `hasNext()` method instead of in the constructor, but the idea is the same.\r\n\r\n(same comment for the other occurrences of `fail()` and `try-catch` to handle expected exceptions elsewhere in this class)', 'commenter': 'ctubbsii'}, {'comment': 'Made this change in me latest commit. ', 'commenter': 'Manno15'}]"
2181,server/tserver/src/test/java/org/apache/accumulo/tserver/log/RecoveryLogsIteratorTest.java,"@@ -161,24 +161,20 @@ public void testFinishMarker() throws IOException {
 
     createRecoveryDir(logs, dirs, false);
 
-    assertThrows(""Finish marker should not be found"", IOException.class, () -> new RecoveryLogsIterator(context, dirs, null, null, false));
+    assertThrows(""Finish marker should not be found"", IOException.class,
+        () -> new RecoveryLogsIterator(context, dirs, null, null, false));
   }
 
   @Test
   public void testSingleFile() throws IOException {
     String destPath = workDir + ""/test.rf"";
     fs.create(new Path(destPath));
 
-    try (RecoveryLogsIterator rli = new RecoveryLogsIterator(context,
-        Collections.singletonList(new Path(destPath)), null, null, false)) {
-      while (rli.hasNext()) {
-        fail(
-            ""Finish marker should not be found for a single file. Exception should have been thrown."");
-      }
-    } catch (IOException e) {
-      fs.delete(new Path(destPath));
-      // Expected exception
-    }
+    assertThrows(""Finish marker should not be found for a single file."", IOException.class,
+        () -> new RecoveryLogsIterator(context, Collections.singletonList(new Path(destPath)), null,
+            null, false));
+
+    fs.delete(new Path(destPath));","[{'comment': ""You don't even need to delete the file, since the folder is temporary."", 'commenter': 'ctubbsii'}]"
2215,core/src/main/thrift/client.thrift,"@@ -331,6 +338,18 @@ service ClientService {
     1:ThriftTableOperationException tope
   )
 
+  string executeAdminOperation(
+      3:trace.TInfo tinfo
+      4:security.TCredentials credentials
+      2:AdminOperation op
+      5:list<string> arguments
+      6:set<i64> filtertxids
+      7:list<string> filterStatues
+      8:string secretOption","[{'comment': ""For a new method that doesn't need to preserve backwards compatibility, the parameters can be numbered in order starting at 1."", 'commenter': 'ctubbsii'}]"
2215,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -149,6 +152,59 @@ boolean testClassLoad(final String className, final String asTypeName)
    */
   void waitForBalance() throws AccumuloException;
 
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param args
+   *          Command line arguments passed in from user command.
+   * @param secretOption
+   *          Specified instance secret to use for commands.
+   *
+   * @since 2.1.0
+   */
+  boolean fateFail(List<String> args, String secretOption) throws AccumuloException;","[{'comment': 'I think I agree with @EdColeman \'s observation that you can probably just require these to be used with an authenticated user with Admin permission, rather than provide the instance secret. The server side can use the instance secret from its config file, without needing to pass it.\r\n\r\nNot sure from this what values are appropriate for ""args"". Are these just a list of FaTE transaction IDs to fail? The parameters to these methods could be more specific.', 'commenter': 'ctubbsii'}, {'comment': '> Not sure from this what values are appropriate for ""args"". Are these just a list of FaTE transaction IDs to fail? The parameters to these methods could be more specific.\r\n\r\nYes, Args will be the TiDs. Initially I was going to have it be more broad and include other things inside it. I can see how it is confusing now. Since I have parameters specifically for FilterTiDs for the print statement I could possibly combine the two parameters. ', 'commenter': 'Manno15'}, {'comment': 'Done in my latest commit', 'commenter': 'Manno15'}]"
2215,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -149,6 +152,59 @@ boolean testClassLoad(final String className, final String asTypeName)
    */
   void waitForBalance() throws AccumuloException;
 
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param args
+   *          Command line arguments passed in from user command.
+   * @param secretOption
+   *          Specified instance secret to use for commands.
+   *
+   * @since 2.1.0
+   */
+  boolean fateFail(List<String> args, String secretOption) throws AccumuloException;
+
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param args
+   *          Command line arguments passed in from user command.
+   * @param secretOption
+   *          Specified instance secret to use for commands.
+   *
+   * @since 2.1.0
+   */
+  boolean fateDelete(List<String> args, String secretOption) throws AccumuloException;
+
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param args
+   *          Command line arguments passed in from user command.
+   * @param filterTxid
+   *          Parsed transaction IDs for print filter.
+   * @param filterStatus
+   *          Parsed TStatus for print filter.
+   * @param secretOption
+   *          Specified instance secret to use for commands.
+   *
+   * @since 2.1.0
+   */
+  String fatePrint(List<String> args, Set<Long> filterTxid, EnumSet<TStatus> filterStatus,","[{'comment': 'We wouldn\'t want TStatus in the public API, but this could be a set of strings for the various statuses. The implementation can convert to the TStatus type, if necessary.\r\n\r\nInstead of returning ""String"" with ""Print"" in the method name, this could return a map of fate IDs to statuses, or some similar data structure.', 'commenter': 'ctubbsii'}, {'comment': '> Instead of returning ""String"" with ""Print"" in the method name, this could return a map of fate IDs to statuses, or some similar data structure.\r\n\r\nIs there a benefit you had in mind with this possible change? Currently, the returning string is the entire output from `Admin.print` that then gets printed to the shell. Admin print handles all the formatting, the only thing FateCommand has to do is print it to the shell. \r\n\r\nFor reference, the current output looks something like this :\r\n<details>\r\n\r\n![image](https://user-images.githubusercontent.com/29436247/127521080-6ad9b69d-dd76-4f36-8e98-85f52e49dfa7.png)\r\n\r\n\r\n</details>', 'commenter': 'Manno15'}, {'comment': 'Instead of \'Print\', I could change the name to ""fateStatus"" as I think that more accurately describes the intended use of the function. It shows the status of either all the txid\'s in operation or the specific one the user requests. ', 'commenter': 'Manno15'}, {'comment': '> > Instead of returning ""String"" with ""Print"" in the method name, this could return a map of fate IDs to statuses, or some similar data structure.\r\n> \r\n> Is there a benefit you had in mind with this possible change?\r\n\r\nWell, mainly just that APIs should return objects that can be manipulated/read/transformed by calling code. It\'s not a console REPL where we have only STDIN/STDOUT and have to parse STDOUT from a command. We can have actual typed data. So, there\'s no reason to resort to ""String"" types to represent complex information in the API.\r\n\r\n> Currently, the returning string is the entire output from `Admin.print` that then gets printed to the shell. Admin print handles all the formatting, the only thing FateCommand has to do is print it to the shell.\r\n\r\nRight, that\'s all that the shell needs to do... but if we\'re going through the effort of creating a public API endpoint, we\'re not doing it just for the shell. The shell can take a complex object returned from the public API and format it into a string for printing. But other callers to this API endpoint might want to do something different with the data. When creating a new public API endpoint, we should try to consider the value it has to a generic caller of the public API, and not just the single use case that inspired its creation.\r\n\r\n> Instead of \'Print\', I could change the name to ""fateStatus"" as I think that more accurately describes the intended use of the function. It shows the status of either all the txid\'s in operation or the specific one the user requests.\r\n\r\nThe name is fine. It could return something that looks like `Set<FateStatus>` or `Map<Txid,Status>` or similar.', 'commenter': 'ctubbsii'}, {'comment': 'Okay, that makes a lot of sense. My viewpoint was too narrowly focused. ', 'commenter': 'Manno15'}]"
2215,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -149,6 +152,59 @@ boolean testClassLoad(final String className, final String asTypeName)
    */
   void waitForBalance() throws AccumuloException;
 
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param args
+   *          Command line arguments passed in from user command.
+   * @param secretOption
+   *          Specified instance secret to use for commands.
+   *
+   * @since 2.1.0
+   */
+  boolean fateFail(List<String> args, String secretOption) throws AccumuloException;
+
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param args
+   *          Command line arguments passed in from user command.
+   * @param secretOption
+   *          Specified instance secret to use for commands.
+   *
+   * @since 2.1.0
+   */
+  boolean fateDelete(List<String> args, String secretOption) throws AccumuloException;
+
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param args
+   *          Command line arguments passed in from user command.
+   * @param filterTxid
+   *          Parsed transaction IDs for print filter.
+   * @param filterStatus
+   *          Parsed TStatus for print filter.
+   * @param secretOption
+   *          Specified instance secret to use for commands.
+   *
+   * @since 2.1.0
+   */
+  String fatePrint(List<String> args, Set<Long> filterTxid, EnumSet<TStatus> filterStatus,
+      String secretOption) throws AccumuloException;
+
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param args
+   *          Command line arguments passed in from user command.
+   * @param secretOption
+   *          Specified instance secret to use for commands.
+   *
+   * @since 2.1.0
+   */
+  String fateDump(List<String> args, String secretOption) throws AccumuloException;","[{'comment': 'Not sure if we need both ""Dump"" and ""Print"" methods. They seem to be doing similar things.', 'commenter': 'ctubbsii'}, {'comment': 'Dump provides a lot more information, I will post a snippet below\r\n\r\n<details>\r\n\r\n<pre>\r\nroot@uno> fate dump\r\n[\r\n  {\r\n    ""txid"": ""58136bf11a1bc7d5"",\r\n    ""stack"": [\r\n      {\r\n        ""org.apache.accumulo.manager.tableOps.TraceRepo"": {\r\n          ""traceId"": 0,\r\n          ""parentId"": 0,\r\n          ""repo"": {\r\n            ""org.apache.accumulo.manager.tableOps.create.CreateTable"": {\r\n              ""tableInfo"": {\r\n                ""tableName"": ""test"",\r\n                ""namespaceId"": {\r\n                  ""canonical"": ""+default""\r\n                },\r\n                ""timeType"": ""MILLIS"",\r\n                ""user"": ""root"",\r\n                ""initialTableState"": ""ONLINE"",\r\n                ""initialSplitSize"": 19,\r\n                ""splitFile"": ""hdfs://groot:8020/accumulo/tmp/fate-58136bf11a1bc7d5/splits"",\r\n                ""splitDirsFile"": ""hdfs://groot:8020/accumulo/tmp/fate-58136bf11a1bc7d5/splitsDirs"",\r\n                ""props"": {\r\n                  ""table.majc.compaction.strategy.opts.large.compress.threshold"": ""100M"",\r\n                  ""table.iterator.majc.vers"": ""20,org.apache.accumulo.core.iterators.user.VersioningIterator"",\r\n                  ""table.constraint.1"": ""org.apache.accumulo.core.data.constraints.DefaultKeySizeConstraint"",\r\n                  ""table.iterator.scan.vers.opt.maxVersions"": ""1"",\r\n                  ""table.iterator.minc.vers"": ""20,org.apache.accumulo.core.iterators.user.VersioningIterator"",\r\n                  ""table.majc.compaction.strategy.opts.large.compress.type"": ""gz"",\r\n                  ""table.iterator.majc.vers.opt.maxVersions"": ""1"",\r\n                  ""table.majc.compaction.strategy"": ""org.apache.accumulo.tserver.compaction.strategies.BasicCompactionStrategy"",\r\n                  ""table.iterator.minc.vers.opt.maxVersions"": ""1"",\r\n                  ""table.iterator.scan.vers"": ""20,org.apache.accumulo.core.iterators.user.VersioningIterator"",\r\n                  ""table.majc.compaction.strategy.opts.filter.size"": ""250M""\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  \r\n</pre>\r\n\r\n</details>\r\n', 'commenter': 'Manno15'}, {'comment': 'Whether or not we want all that information I guess is up for debate.', 'commenter': 'Manno15'}, {'comment': 'If we had one method that returned the set of transaction IDs, their states, and their blobs like this, code that calls this method can decide what pieces they care about.', 'commenter': 'ctubbsii'}]"
2215,core/src/main/java/org/apache/accumulo/fate/AdminUtil.java,"@@ -533,26 +533,29 @@ public void deleteLocks(ZooReaderWriter zk, String path, String txidStr)
     }
   }
 
+  // Follow on ticket to rework exception handling to
+  // coincide with changes to FateCommand. At the moment, user has no feedback of why something
+  // failed without having to go check server logs.
   @SuppressFBWarnings(value = ""DM_EXIT"",
       justification = ""TODO - should probably avoid System.exit here; ""
           + ""this code is used by the fate admin shell command"")
   public boolean checkGlobalLock(ZooReaderWriter zk, ServiceLockPath path) {
     try {
       if (ServiceLock.getLockData(zk.getZooKeeper(), path) != null) {
-        System.err.println(""ERROR: Manager lock is held, not running"");
+        log.error(""ERROR: Manager lock is held, not running"");","[{'comment': 'If changing these to log messages, including ""ERROR"" in the message is redundant.', 'commenter': 'ctubbsii'}]"
2215,core/src/main/java/org/apache/accumulo/fate/AdminUtil.java,"@@ -533,26 +533,29 @@ public void deleteLocks(ZooReaderWriter zk, String path, String txidStr)
     }
   }
 
+  // Follow on ticket to rework exception handling to
+  // coincide with changes to FateCommand. At the moment, user has no feedback of why something
+  // failed without having to go check server logs.
   @SuppressFBWarnings(value = ""DM_EXIT"",
       justification = ""TODO - should probably avoid System.exit here; ""
           + ""this code is used by the fate admin shell command"")
   public boolean checkGlobalLock(ZooReaderWriter zk, ServiceLockPath path) {
     try {
       if (ServiceLock.getLockData(zk.getZooKeeper(), path) != null) {
-        System.err.println(""ERROR: Manager lock is held, not running"");
+        log.error(""ERROR: Manager lock is held, not running"");
         if (this.exitOnError)
           System.exit(1);
         else
           return false;
       }
     } catch (KeeperException e) {
-      System.err.println(""ERROR: Could not read manager lock, not running "" + e.getMessage());
+      log.error(""ERROR: Could not read manager lock, not running "" + e.getMessage());
       if (this.exitOnError)
         System.exit(1);
       else
         return false;
     } catch (InterruptedException e) {
-      System.err.println(""ERROR: Could not read manager lock, not running"" + e.getMessage());
+      log.error(""ERROR: Could not read manager lock, not running"" + e.getMessage());","[{'comment': 'To include the stack trace in the log message:\r\n\r\n```suggestion\r\n      log.error(""Could not read manager lock, not running"", e);\r\n```', 'commenter': 'ctubbsii'}]"
2215,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -149,6 +150,46 @@ boolean testClassLoad(final String className, final String asTypeName)
    */
   void waitForBalance() throws AccumuloException;
 
+  /**
+   * Throws an exception if a tablet server can not be contacted.","[{'comment': 'These javadocs should say what the method does normally. The explanation for when an exception is thrown can be in a javadoc `@throws SomeException when <explanation here>` line.', 'commenter': 'ctubbsii'}, {'comment': 'This was just a mistake by me. I pushed those up without editing the description of them. ', 'commenter': 'Manno15'}]"
2215,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -149,6 +150,46 @@ boolean testClassLoad(final String className, final String asTypeName)
    */
   void waitForBalance() throws AccumuloException;
 
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param txids
+   *          Transaction IDs.
+   * @since 2.1.0
+   */
+  void fateFail(List<String> txids) throws AccumuloException;
+
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param txids
+   *          Transaction IDs.
+   * @since 2.1.0
+   */
+  void fateDelete(List<String> txids) throws AccumuloException;
+
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param txids
+   *          Transaction IDs.
+   * @param tStatus
+   *          Parsed TStatus for print filter.
+   * @return String containing the output to print to the shell.
+   * @since 2.1.0
+   */
+  String fatePrint(List<String> txids, List<String> tStatus) throws AccumuloException;
+
+  /**
+   * Throws an exception if a tablet server can not be contacted.
+   *
+   * @param txids
+   *          Transaction IDs.
+   * @return String containing the output to print to the shell.
+   * @since 2.1.0
+   */
+  String fateDump(List<String> txids) throws AccumuloException;","[{'comment': 'See previous review conversation for discussion about consolidating these, and having a meaningful return type other than String. These APIs should not be written exclusively for use with the Shell, but should have meaningful return types of their own, and not just formatted strings for the shell.', 'commenter': 'ctubbsii'}]"
2215,core/src/main/thrift/client.thrift,"@@ -331,6 +338,16 @@ service ClientService {
     1:ThriftTableOperationException tope
   )
 
+  string executeAdminOperation(
+      2:trace.TInfo tinfo","[{'comment': ""This can be numbered starting at 1. The numbers for the throws exceptions can also start at 1. They are a separate context, and it doesn't matter if they reuse numbers across contexts, as long as the numbers aren't reused within the same parameter/exception context."", 'commenter': 'ctubbsii'}]"
2215,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -150,6 +150,37 @@ boolean testClassLoad(final String className, final String asTypeName)
    */
   void waitForBalance() throws AccumuloException;
 
+  /**
+   * Fails a fate transaction based on the given txID. At least one txID must be provided.
+   *
+   * @param txids
+   *          Transaction IDs to fail.
+   * @since 2.1.0
+   */
+  void fateFail(List<String> txids) throws AccumuloException;
+
+  /**
+   * Deletes a fate transaction based on the given txID. At least one txID must be provided.
+   *
+   * @param txids
+   *          Transaction IDs to delete.
+   * @since 2.1.0
+   */
+  void fateDelete(List<String> txids) throws AccumuloException;
+
+  /**
+   * Gathers Transaction status information for either all fate transactions or requested txIDs.
+   *
+   * @param txids
+   *          Transaction IDs to use as a filter. Optional.
+   * @param tStatus
+   *          Parsed TStatus for print filter. Optional.
+   * @return A list of TransactionStatues for corresponding txids
+   * @since 2.1.0
+   */
+  List<TransactionStatus> fateStatus(List<String> txids, List<String> tStatus)
+      throws AccumuloException;
+","[{'comment': 'I think these are good so far but we need a way to get the `TxIds`. We need a method like this on InstanceOperations:\r\n`List<TransactionStatus> fatePrint() throws AccumuloException;`\r\n\r\nHere is an example of how it could be used:\r\n<pre>\r\nvar list = client.instanceOperations().fatePrint();\r\nfor (var ts : list) {\r\n  client.instanceOperations().fateFail(List.of(ts.getTxid()));\r\n}\r\n</pre>\r\n\r\nI also think this would be a good opportunity to use a stronger type for Txid. This was discussed recently here: https://github.com/apache/accumulo/issues/2473#issuecomment-1039473784', 'commenter': 'milleruntime'}, {'comment': ""If this would benefit from some of the more aggressive FaTE refactoring you're working on that might go in 3.0, I'd be okay with pushing this PR off until then. But, otherwise, I'd generally agree with your comment above that this would be good to get into 2.1 (based on the intended outcome, not the code itself, since I haven't yet had a chance to review the code)."", 'commenter': 'ctubbsii'}]"
2215,core/src/main/java/org/apache/accumulo/fate/AdminUtil.java,"@@ -360,6 +285,10 @@ private FateStatus getTransactionStatus(ReadOnlyTStore<T> zs, Set<Long> filterTx
 
     List<Long> transactions = zs.list();
     List<TransactionStatus> statuses = new ArrayList<>(transactions.size());
+    Gson gson = new GsonBuilder()
+        .registerTypeAdapter(ReadOnlyRepo.class, new InterfaceSerializer<>())
+        .registerTypeAdapter(Repo.class, new InterfaceSerializer<>())
+        .registerTypeAdapter(byte[].class, new ByteArraySerializer()).setPrettyPrinting().create();","[{'comment': 'What are these type adapters doing? An inline comment could help.', 'commenter': 'ctubbsii'}]"
2215,core/src/main/java/org/apache/accumulo/fate/AdminUtil.java,"@@ -478,32 +411,32 @@ public boolean prepFail(TStore<T> zs, ZooReaderWriter zk, ServiceLockPath zLockM
     try {
       txid = Long.parseLong(txidStr, 16);
     } catch (NumberFormatException nfe) {
-      System.out.printf(""Invalid transaction ID format: %s%n"", txidStr);
+      log.error(""Invalid transaction ID format: {}"", txidStr);
       return false;
     }
     boolean state = false;
     zs.reserve(txid);
     TStatus ts = zs.getStatus(txid);
     switch (ts) {
       case UNKNOWN:
-        System.out.printf(""Invalid transaction ID: %016x%n"", txid);
+        log.error(""Invalid transaction ID: {}"", txid);","[{'comment': 'These are a bit of a change in behavior for the shell, which I think currently uses AdminUtil. I think this behavior is better, but might want to include some release notes that mention the change, so if anybody was relying on scripting the shell, and capturing STDOUT, it will work differently for them, depending on their log configuration.', 'commenter': 'ctubbsii'}]"
2215,core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java,"@@ -284,6 +287,42 @@ public void waitForBalance() throws AccumuloException {
 
   }
 
+  @Override
+  public void fateFail(List<String> txids) throws AccumuloException {
+    checkArgument(txids != null, ""txids is null"");
+    executeAdminOperation(AdminOperation.FAIL, txids, null);
+  }
+
+  @Override
+  public void fateDelete(List<String> txids) throws AccumuloException {
+    checkArgument(txids != null, ""txids is null"");
+    executeAdminOperation(AdminOperation.DELETE, txids, null);
+  }
+
+  @Override
+  public List<TransactionStatus> fateStatus(List<String> txids, List<String> tStatus)
+      throws AccumuloException {
+    checkArgument(txids != null, ""txids is null"");
+    List<TransactionStatus> txStatus = new ArrayList<>();
+    for (var tx : executeAdminOperation(AdminOperation.PRINT, txids, tStatus)) {
+      txStatus.add(new TransactionStatus(tx.getTxid(), tx.getTstatus(), tx.getDebug(),
+          tx.getHlocks(), tx.getWlocks(), tx.getTop(), tx.getTimecreated(), tx.getStackInfo()));
+    }
+    return txStatus;
+  }
+
+  private List<FateTransaction> executeAdminOperation(AdminOperation op, List<String> txids,
+      List<String> filterStatuses) throws AccumuloException {
+    try {
+      return ThriftClientTypes.CLIENT.execute(context,
+          client -> client.executeAdminOperation(TraceUtil.traceInfo(), context.rpcCreds(), op,
+              txids, filterStatuses));
+    } catch (AccumuloSecurityException e) {
+      throw new RuntimeException(""Unexpected exception thrown"", e);","[{'comment': 'It seems like AccumuloSecurityException should propagate up.', 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii, how far up do you think it should propagate? Should it be caught and handled where `executeAdminOperation` is called, or propagate up even further than that?', 'commenter': 'DomGarguilo'}]"
2215,server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java,"@@ -447,6 +460,83 @@ public List<TDiskUsage> getDiskUsage(Set<String> tables, TCredentials credential
     }
   }
 
+  @Override
+  public List<FateTransaction> executeAdminOperation(TInfo tInfo, TCredentials credentials,
+      AdminOperation op, List<String> txids, List<String> filterStatuses)
+      throws ThriftSecurityException, TException {
+    try {
+      authenticate(tInfo, credentials);","[{'comment': 'In addition to authenticating, we probably want to restrict this to users with admin permissions, right?', 'commenter': 'ctubbsii'}, {'comment': 'What do you mean by ""admin permissions"" here? It looks like this checks that the credentials contain `SYSTEM` permissions.', 'commenter': 'DomGarguilo'}, {'comment': '> What do you mean by ""admin permissions"" here? It looks like this checks that the credentials contain `SYSTEM` permissions.\r\n\r\nWhat line do you see that?', 'commenter': 'ctubbsii'}, {'comment': 'Sorry I should have quote replied. I was referring to your comment here @ctubbsii:\r\n\r\n> In addition to authenticating, we probably want to restrict this to users with admin permissions, right?\r\n\r\n', 'commenter': 'DomGarguilo'}, {'comment': 'I mean `SystemPermission.SYSTEM`, or `security.canPerformSystemActions(credentials)`', 'commenter': 'ctubbsii'}]"
2220,core/src/main/java/org/apache/accumulo/core/file/rfile/PrintInfo.java,"@@ -106,10 +106,11 @@ public void add(int size) {
     }
 
     public void print(String indent) {
-      System.out.println(indent + ""Up to size      count      %-age"");
+      System.out.println(indent + ""Up to size      Count      %-age"");
       for (int i = 1; i < countBuckets.length; i++) {
-        System.out.println(String.format(""%s%11.0f : %10d %6.2f%%"", indent, Math.pow(10, i),
-            countBuckets[i], sizeBuckets[i] * 100. / totalSize));
+        System.out.println(String.format(""%s%11s : %10d %6.2f%%"", indent,
+            bigNumberForSize(Double.valueOf(Math.pow(10, i)).longValue()), countBuckets[i],","[{'comment': '```suggestion\r\n            NumUtil.bigNumberForSize(Double.valueOf(Math.pow(10, i)).longValue()), countBuckets[i],\r\n```', 'commenter': 'Manno15'}]"
2220,core/src/main/java/org/apache/accumulo/core/file/rfile/PrintInfo.java,"@@ -106,10 +107,11 @@ public void add(int size) {
     }
 
     public void print(String indent) {
-      System.out.println(indent + ""Up to size      count      %-age"");
+      System.out.println(indent + ""Up to size      Count      %-age"");
       for (int i = 1; i < countBuckets.length; i++) {
-        System.out.println(String.format(""%s%11.0f : %10d %6.2f%%"", indent, Math.pow(10, i),
-            countBuckets[i], sizeBuckets[i] * 100. / totalSize));
+        System.out.println(String.format(""%s%11s : %10d %6.2f%%"", indent,
+            NumUtil.bigNumberForSize(Double.valueOf(Math.pow(10, i)).longValue()), countBuckets[i],
+            sizeBuckets[i] * 100. / totalSize));","[{'comment': '```suggestion\r\n        System.out.println(""%s%11s : %10d %6.2f%%"", indent,\r\n            NumUtil.bigNumberForSize(Double.valueOf(Math.pow(10, i)).longValue()), countBuckets[i],\r\n            sizeBuckets[i] * 100. / totalSize);\r\n```\r\nBased from my testing, the String.format isn\'t necessary. ', 'commenter': 'Manno15'}]"
2220,core/src/main/java/org/apache/accumulo/core/file/rfile/PrintInfo.java,"@@ -106,10 +107,11 @@ public void add(int size) {
     }
 
     public void print(String indent) {
-      System.out.println(indent + ""Up to size      count      %-age"");
+      System.out.println(indent + ""Up to size      Count      %-age"");
       for (int i = 1; i < countBuckets.length; i++) {
-        System.out.println(String.format(""%s%11.0f : %10d %6.2f%%"", indent, Math.pow(10, i),
-            countBuckets[i], sizeBuckets[i] * 100. / totalSize));
+        System.out.println(""%s%11s : %10d %6.2f%%"", indent,","[{'comment': '```suggestion\r\n        System.out.printf(""%s%11s : %10d %6.2f%%"", indent,\r\n```\r\n\r\nThat is my mistake, I forgot to add this change. ', 'commenter': 'Manno15'}]"
2220,core/src/main/java/org/apache/accumulo/core/file/rfile/PrintInfo.java,"@@ -106,10 +107,11 @@ public void add(int size) {
     }
 
     public void print(String indent) {
-      System.out.println(indent + ""Up to size      count      %-age"");
+      System.out.println(indent + ""Up to size      Count      %-age"");
       for (int i = 1; i < countBuckets.length; i++) {
-        System.out.println(String.format(""%s%11.0f : %10d %6.2f%%"", indent, Math.pow(10, i),
-            countBuckets[i], sizeBuckets[i] * 100. / totalSize));
+        System.out.printf(""%s%11s : %10d %6.2f%%"", indent,
+            NumUtil.bigNumberForSize(Double.valueOf(Math.pow(10, i)).longValue()), countBuckets[i],
+            sizeBuckets[i] * 100. / totalSize);","[{'comment': '```suggestion\r\n        System.out.println(String.format(""%s%11s : %10d %6.2f%%"", indent,\r\n            NumUtil.bigNumberForQuantity((long) Math.pow(10, i)), countBuckets[i],\r\n            sizeBuckets[i] * 100. / totalSize));\r\n```', 'commenter': 'Manno15'}]"
2221,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -231,14 +246,26 @@ public OutputStream createCompressionStream(OutputStream downStream, Compressor
        */
       private static final int DEFAULT_BUFFER_SIZE = 32 * 1024;
 
+      private final Boolean USE_QAT =
+          Boolean.valueOf(System.getProperty(USE_QAT_PROPERTY, ""false""));","[{'comment': ""Since this will return something non-null, you can assign it to a primitive `boolean`. Also, to support `-Duse.qat` in addition to `-Duse.qat=true`, this logic will need to be adjusted a bit. Could see how Maven does it, because it supports similar flags like `-DskipTests` where you don't have to specify `-DskipTests=true`."", 'commenter': 'ctubbsii'}]"
2221,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -250,33 +277,71 @@ public void initializeDefaultCodec() {
        */
       @Override
       protected CompressionCodec createNewCodec(final int bufferSize) {
-        DefaultCodec myCodec = new DefaultCodec();
-        Configuration myConf = new Configuration(conf);
-        // only use the buffersize if > 0, otherwise we'll use
-        // the default defined within the codec
-        if (bufferSize > 0)
-          myConf.setInt(BUFFER_SIZE_OPT, bufferSize);
-        myCodec.setConf(myConf);
-        return myCodec;
+        if (USE_QAT) {
+          String extClazz =
+              (conf.get(CONF_QAT_CLASS) == null ? System.getProperty(CONF_QAT_CLASS) : null);
+          String clazz = (extClazz != null) ? extClazz : DEFAULT_QAT_CLASS;
+          try {
+            LOG.info(""Trying to load qat codec class: "" + clazz);
+
+            Configuration myConf = new Configuration(conf);
+            // only use the buffersize if > 0, otherwise we'll use
+            // the default defined within the codec
+            if (bufferSize > 0)
+              myConf.setInt(QAT_BUFFER_SIZE_OPT, bufferSize);
+
+            CompressionCodec c =
+                (CompressionCodec) ReflectionUtils.newInstance(Class.forName(clazz), myConf);
+            if (c instanceof Configurable) {
+              ((Configurable) c).setConf(myConf);
+            }
+            return c;
+          } catch (ClassNotFoundException e) {
+            LOG.error(""Unable to create QAT codec"", e);
+          }
+          return null;","[{'comment': 'If this is null, should we fall back to the default impl? This seems like it would result in NPEs and break things in weird ways. The QuickAssist impl should probably be an optional optimization that works when available, but is ignored when not available.', 'commenter': 'ctubbsii'}, {'comment': 'I was thinking that if someone specified `-Duse.qat`, then they were intending on using the device and expected it to work. The fact that it was not working in this case would be an error as maybe they have a configuration issue. Falling back to the default implementation and logging it is not honoring the users intent.', 'commenter': 'dlmarion'}, {'comment': 'Seems reasonable. Perhaps we can throw an exception instead of `return null`, so this error is explicit? Would want to make sure it is thrown early.', 'commenter': 'ctubbsii'}, {'comment': ""On the other hand, we should weigh the user's intent to use GZ, whether or not the optimization is available. The user has two intents, to use GZ, and to use QAT. The current behavior says if QAT isn't available, then GZ isn't available at all. I'm not sure that's a good experience for users either."", 'commenter': 'ctubbsii'}, {'comment': ""I'd rather fail fast, rather than fail silently. From a deployment perspective, if I configure something to run a specific way, then I want it to do that or fail so I can fix it. It's not fun or easy to diagnose why something isn't working the way you expect it to. If you specify `-Xmx8g` for the JVM, you expect it to use up to that much memory. If the JVM can't allocate that much memory it doesn't fall back to allocating whatever it can. I'd rather throw an error here and move on."", 'commenter': 'dlmarion'}, {'comment': 'Yeah, I understand the desire to fail fast instead of diagnose when things aren\'t working as expected. I\'m just thinking we have to infer what the user is expecting. I\'m sure some users might expect ""I want QAT\'s impl of GZ only"". But other users might expect ""I want GZ, QAT to make it faster, if available"". Personally, I\'d be somebody in the latter camp.\r\n\r\nCould support both sets of user expectations with values of `.enabled=[true,false,required/force]`. However, considering the forwards-compatible requirement of 1.10.2 with 1.10.1/1.10.0, it\'s impossible for a user of ""1.10.x"" versions to expect that it will always fail if QAT isn\'t available. The user might set \'.enabled=force\', but then need to downgrade to 1.10.1 for whatever reason, and the option is completely ignored. So, at best, the user can only reasonably expect 1.10 versions to treat this as an optimization, and never a requirement, unless they know for sure they are a sufficiently recent 1.10 version.', 'commenter': 'ctubbsii'}]"
2221,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -250,33 +277,71 @@ public void initializeDefaultCodec() {
        */
       @Override
       protected CompressionCodec createNewCodec(final int bufferSize) {
-        DefaultCodec myCodec = new DefaultCodec();
-        Configuration myConf = new Configuration(conf);
-        // only use the buffersize if > 0, otherwise we'll use
-        // the default defined within the codec
-        if (bufferSize > 0)
-          myConf.setInt(BUFFER_SIZE_OPT, bufferSize);
-        myCodec.setConf(myConf);
-        return myCodec;
+        if (USE_QAT) {
+          String extClazz =
+              (conf.get(CONF_QAT_CLASS) == null ? System.getProperty(CONF_QAT_CLASS) : null);
+          String clazz = (extClazz != null) ? extClazz : DEFAULT_QAT_CLASS;
+          try {
+            LOG.info(""Trying to load qat codec class: "" + clazz);
+
+            Configuration myConf = new Configuration(conf);
+            // only use the buffersize if > 0, otherwise we'll use
+            // the default defined within the codec
+            if (bufferSize > 0)
+              myConf.setInt(QAT_BUFFER_SIZE_OPT, bufferSize);
+
+            CompressionCodec c =
+                (CompressionCodec) ReflectionUtils.newInstance(Class.forName(clazz), myConf);","[{'comment': 'Is `Class.forName` the right way to load this class for the VFS Classloader in Accumulo in 1.x? What about 2.1 with the new classloader changes there?', 'commenter': 'ctubbsii'}, {'comment': 'I just copied what the other implementations were doing. For Hadoop to be able to use the Codec it has to be on the CLASSPATH.', 'commenter': 'dlmarion'}, {'comment': 'That\'s fine. I just wasn\'t sure what the ""right way"" to load a class anymore is. :smiley_cat: I haven\'t thought about classloaders in a bit. 2.1 code looks similar.', 'commenter': 'ctubbsii'}]"
2221,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -250,33 +277,71 @@ public void initializeDefaultCodec() {
        */
       @Override
       protected CompressionCodec createNewCodec(final int bufferSize) {
-        DefaultCodec myCodec = new DefaultCodec();
-        Configuration myConf = new Configuration(conf);
-        // only use the buffersize if > 0, otherwise we'll use
-        // the default defined within the codec
-        if (bufferSize > 0)
-          myConf.setInt(BUFFER_SIZE_OPT, bufferSize);
-        myCodec.setConf(myConf);
-        return myCodec;
+        if (USE_QAT) {
+          String extClazz =
+              (conf.get(CONF_QAT_CLASS) == null ? System.getProperty(CONF_QAT_CLASS) : null);
+          String clazz = (extClazz != null) ? extClazz : DEFAULT_QAT_CLASS;","[{'comment': ""I'm a little confused by this logic. This reads:\r\n\r\nIf `io.compression.codec.qat.class` is not set in the Hadoop config file, then use the class name defined by `-Dorg.apache.hadoop.io.compress.QatCodec.class=????`. This part makes sense, although I don't know why we'd want to be able to override the QAT codec class name, since it seems like it would be a well-known value.\r\n\r\nIf it **is** set in the Hadoop config file, then just set the class name to `null`, which will result in us using the default class name we've hard-coded. This part doesn't make sense to me. Why wouldn't we use the class name from the Hadoop config file in this case?\r\n\r\nAlso, shouldn't we prioritize our explicit `-D` override instead of only using it when the Hadoop config doesn't set it? The implementation here will prioritize what's in the the default value if the Hadoop config is set, and ignore the override.\r\n\r\nI think the logic should be:\r\n\r\n1. Check if system property is set to non-null. Use it if set, otherwise\r\n2. check if Hadoop config is set to non-null. Use it if set, otherwise\r\n3. use the default class name that we've hard-coded.\r\n\r\nYou could also use the first one in that order where the class isn't just non-null, but also the class loads.\r\n\r\nAlso, minor nit: avoiding unnecessary negations in ternary statements helps readability:\r\n```java\r\nString clazz = extClazz == null ? DEFAULT_QAT_CLASS : extClazz;\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'I was just doing what the others do here (mostly copy/paste). LZO does it [here](https://github.com/apache/accumulo/blob/1.10/core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java#L159) and Snappy does it [here](https://github.com/apache/accumulo/blob/1.10/core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java#L387)', 'commenter': 'dlmarion'}, {'comment': ""Oh wow. That existing code seems very broken. I'm fine with following suit with what the rest of the code does and fixing it later. It's especially concerning that we completely ignore the value in the Hadoop config when it's set there, and use our property when it's not set there. I'm not sure why we even bothered to read the value from the Hadoop config at all in that case, if it has no bearing on what we're going to do."", 'commenter': 'ctubbsii'}]"
2221,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -250,33 +277,71 @@ public void initializeDefaultCodec() {
        */
       @Override
       protected CompressionCodec createNewCodec(final int bufferSize) {
-        DefaultCodec myCodec = new DefaultCodec();
-        Configuration myConf = new Configuration(conf);
-        // only use the buffersize if > 0, otherwise we'll use
-        // the default defined within the codec
-        if (bufferSize > 0)
-          myConf.setInt(BUFFER_SIZE_OPT, bufferSize);
-        myCodec.setConf(myConf);
-        return myCodec;
+        if (USE_QAT) {
+          String extClazz =
+              (conf.get(CONF_QAT_CLASS) == null ? System.getProperty(CONF_QAT_CLASS) : null);
+          String clazz = (extClazz != null) ? extClazz : DEFAULT_QAT_CLASS;
+          try {
+            LOG.info(""Trying to load qat codec class: "" + clazz);
+
+            Configuration myConf = new Configuration(conf);
+            // only use the buffersize if > 0, otherwise we'll use
+            // the default defined within the codec
+            if (bufferSize > 0)
+              myConf.setInt(QAT_BUFFER_SIZE_OPT, bufferSize);
+
+            CompressionCodec c =
+                (CompressionCodec) ReflectionUtils.newInstance(Class.forName(clazz), myConf);
+            if (c instanceof Configurable) {
+              ((Configurable) c).setConf(myConf);
+            }
+            return c;
+          } catch (ClassNotFoundException e) {","[{'comment': ""Is CNFE is the only ReflectiveOperationException that can be thrown here? I'm surprised if that's the case, but perhaps this is because the rest are caught in Hadoop's ReflectionUtils? (Is that really public API for Hadoop? That seems like an internal utility class that we should probably avoid using ourselves... but it's outside the scope of this PR.)"", 'commenter': 'ctubbsii'}, {'comment': 'ReflectionUtils is public. The newInstance method actually catches all Exceptions and throws a RuntimeException. The CNFE exception here is being thrown from the Class.forName() call.\r\n\r\n```\r\n@InterfaceAudience.Public\r\n@InterfaceStability.Evolving\r\npublic class ReflectionUtils\r\n```', 'commenter': 'dlmarion'}]"
2221,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -219,7 +220,21 @@ public OutputStream createCompressionStream(OutputStream downStream, Compressor
 
     GZ(COMPRESSION_GZ) {
 
-      private transient DefaultCodec codec = null;
+      private static final String USE_QAT_PROPERTY = ""use.qat"";","[{'comment': '`use.qat` is succinct, but it might be better to have a more explicit name in the `accumulo.` namespace because this is for affecting Accumulo behavior, something like:\r\n\r\n```suggestion\r\n      private static final String USE_QAT_PROPERTY = ""accumulo.rfile.compression.quickassist.enabled"";\r\n```', 'commenter': 'ctubbsii'}]"
2221,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -250,33 +277,71 @@ public void initializeDefaultCodec() {
        */
       @Override
       protected CompressionCodec createNewCodec(final int bufferSize) {
-        DefaultCodec myCodec = new DefaultCodec();
-        Configuration myConf = new Configuration(conf);
-        // only use the buffersize if > 0, otherwise we'll use
-        // the default defined within the codec
-        if (bufferSize > 0)
-          myConf.setInt(BUFFER_SIZE_OPT, bufferSize);
-        myCodec.setConf(myConf);
-        return myCodec;
+        if (USE_QAT) {
+          String extClazz =
+              (conf.get(CONF_QAT_CLASS) == null ? System.getProperty(CONF_QAT_CLASS) : null);
+          String clazz = (extClazz != null) ? extClazz : DEFAULT_QAT_CLASS;
+          try {
+            LOG.info(""Trying to load qat codec class: "" + clazz);
+
+            Configuration myConf = new Configuration(conf);
+            // only use the buffersize if > 0, otherwise we'll use
+            // the default defined within the codec
+            if (bufferSize > 0)
+              myConf.setInt(QAT_BUFFER_SIZE_OPT, bufferSize);
+
+            CompressionCodec c =
+                (CompressionCodec) ReflectionUtils.newInstance(Class.forName(clazz), myConf);
+            if (c instanceof Configurable) {
+              ((Configurable) c).setConf(myConf);
+            }
+            return c;
+          } catch (ClassNotFoundException e) {
+            LOG.error(""Unable to create QAT codec"", e);
+          }
+          return null;
+        } else {
+          DefaultCodec myCodec = new DefaultCodec();
+          Configuration myConf = new Configuration(conf);
+          // only use the buffersize if > 0, otherwise we'll use
+          // the default defined within the codec
+          if (bufferSize > 0)
+            myConf.setInt(BUFFER_SIZE_OPT, bufferSize);
+          myCodec.setConf(myConf);
+          return myCodec;
+        }
       }
 
       @Override
       public InputStream createDecompressionStream(InputStream downStream,
           Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        // Set the internal buffer size to read from down stream.
-        CompressionCodec decomCodec = codec;
-        // if we're not using the default, let's pull from the loading cache
-        if (DEFAULT_BUFFER_SIZE != downStreamBufferSize) {
-          Entry<Algorithm,Integer> sizeOpt = Maps.immutableEntry(GZ, downStreamBufferSize);
-          try {
-            decomCodec = codecCache.get(sizeOpt);
-          } catch (ExecutionException e) {
-            throw new IOException(e);
+
+        if (USE_QAT) {","[{'comment': ""This conditional is checking that 'use.qat' system property is set to true, but not whether the QAT codec successfully loaded. The conditional should probably depend on the success of loading the QAT library, especially if we fall back to the regular codec if it fails to load, as I suggested elsewhere."", 'commenter': 'ctubbsii'}]"
2221,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -297,7 +362,7 @@ public OutputStream createCompressionStream(OutputStream downStream, Compressor
 
       @Override
       public boolean isSupported() {
-        return true;
+        return codec != null;","[{'comment': 'We should always support GZ, even if the QAT library fails to load.', 'commenter': 'ctubbsii'}]"
2228,server/base/pom.xml,"@@ -120,6 +116,11 @@
       <artifactId>easymock</artifactId>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.jline</groupId>
+      <artifactId>jline</artifactId>
+      <scope>test</scope>
+    </dependency>","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'Manno15'}]"
2228,server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java,"@@ -279,9 +267,8 @@ static boolean checkInit(VolumeManager fs, SiteConfiguration sconf, Configuratio
       return false;
     }
     if (sconf.get(Property.INSTANCE_SECRET).equals(Property.INSTANCE_SECRET.getDefaultValue())) {
-      LineReader c = getLineReader();
-      var w = c.getTerminal().writer();
-      c.getTerminal().puts(InfoCmp.Capability.bell);
+      Console c = System.console();","[{'comment': 'I was more thinking of something like this: \r\n\r\n```java\r\n      System.out.println();\r\n      System.out.println();\r\n      System.out.println(""Warning!!! Your instance secret is still set to the default,""\r\n          + "" this is not secure. We highly recommend you change it."");\r\n      System.out.println();\r\n      System.out.println();\r\n      System.out.println(""You can change the instance secret in accumulo by using:"");\r\n      System.out.println(""   bin/accumulo "" + org.apache.accumulo.server.util.ChangeSecret.class.getName());\r\n      System.out.println(""You will also need to edit your secret in your configuration""\r\n          + "" file by adding the property instance.secret to your""\r\n          + "" accumulo.properties. Without this accumulo will not operate"" + "" correctly"");\r\n\r\n\r\n```\r\n\r\nsince all this is trying to do is print those messages to screen. ', 'commenter': 'Manno15'}, {'comment': 'Would we also be able to do `System.out.println(""\\n"");` instead of two seprate println\'s? Or even appending two newlines (\\n) to the preceding statements?', 'commenter': 'DomGarguilo'}, {'comment': 'Probably could do some cleanup here, yes. ', 'commenter': 'Manno15'}, {'comment': '\\n is unix if we do it the way  it is, its compatible with any other systems you run on. I suggest we live it like that', 'commenter': 'harjitdotsingh'}]"
2228,server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java,"@@ -748,24 +735,27 @@ private String getRootUserName(SiteConfiguration siteConfig, Opts opts) throws I
     if (opts.cliPassword != null) {
       return opts.cliPassword.getBytes(UTF_8);
     }
-    String rootpass;
-    String confirmpass;
+    char[] rootpass;
+    char[] confirmpass;
+    String strrootpass;
+    String strconfirmpass;
     do {
-      rootpass = getLineReader().readLine(
-          ""Enter initial password for "" + rootUser + getInitialPasswordWarning(siteConfig), '*');
+      rootpass = System.console().readPassword(
+          ""Enter initial password for "" + rootUser + getInitialPasswordWarning(siteConfig));
       if (rootpass == null) {
         System.exit(0);
       }
-      confirmpass =
-          getLineReader().readLine(""Confirm initial password for "" + rootUser + "": "", '*');
+      confirmpass = System.console().readPassword(""Confirm initial password for "" + rootUser + "":"");
       if (confirmpass == null) {
         System.exit(0);
       }
-      if (!rootpass.equals(confirmpass)) {
+      strrootpass = new String(rootpass);
+      strconfirmpass = new String(confirmpass);
+      if (!strrootpass.equals(strconfirmpass)) {
         log.error(""Passwords do not match"");
       }
     } while (!rootpass.equals(confirmpass));","[{'comment': 'This should probably be the comparison of the string version otherwise, you would need Arrays.,equals', 'commenter': 'Manno15'}, {'comment': 'I have already fixed that', 'commenter': 'harjitdotsingh'}]"
2228,server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java,"@@ -144,17 +141,8 @@
   private static final String DEFAULT_ROOT_USER = ""root"";
   private static final String TABLE_TABLETS_TABLET_DIR = ""table_info"";
 
-  private static LineReader reader = null;
-  private static Terminal terminal = null;
   private static ZooReaderWriter zoo = null;
-
-  private static LineReader getLineReader() throws IOException {
-    if (terminal == null)
-      terminal = TerminalBuilder.builder().jansi(false).build();
-    if (reader == null)
-      reader = LineReaderBuilder.builder().terminal(terminal).build();
-    return reader;
-  }
+  private static Console reader = null;","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'Manno15'}]"
2228,server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java,"@@ -748,24 +735,27 @@ private String getRootUserName(SiteConfiguration siteConfig, Opts opts) throws I
     if (opts.cliPassword != null) {
       return opts.cliPassword.getBytes(UTF_8);
     }
-    String rootpass;
-    String confirmpass;
+    char[] rootpass;
+    char[] confirmpass;
+    String strrootpass;
+    String strconfirmpass;
     do {
-      rootpass = getLineReader().readLine(
-          ""Enter initial password for "" + rootUser + getInitialPasswordWarning(siteConfig), '*');
+      rootpass = System.console().readPassword(
+          ""Enter initial password for "" + rootUser + getInitialPasswordWarning(siteConfig));
       if (rootpass == null) {
         System.exit(0);
       }
-      confirmpass =
-          getLineReader().readLine(""Confirm initial password for "" + rootUser + "": "", '*');
+      confirmpass = System.console().readPassword(""Confirm initial password for "" + rootUser + "":"");
       if (confirmpass == null) {
         System.exit(0);
       }
-      if (!rootpass.equals(confirmpass)) {
+      strrootpass = new String(rootpass);
+      strconfirmpass = new String(confirmpass);
+      if (!strrootpass.equals(strconfirmpass)) {
         log.error(""Passwords do not match"");
       }
     } while (!rootpass.equals(confirmpass));
-    return rootpass.getBytes(UTF_8);
+    return strrootpass.getBytes(Charset.forName(""UTF-8""));","[{'comment': '```suggestion\r\n    return strrootpass.getBytes(UTF_8);\r\n```', 'commenter': 'Manno15'}, {'comment': 'If I do strrootpass.getBytes(UTF_8) It fails the quality check. Recommend way is the way I have it  strrootpass.getBytes(Charset.forName(""UTF-8""));', 'commenter': 'harjitdotsingh'}, {'comment': 'The tests pass for me with this change included, perhaps something else caused the test failures.', 'commenter': 'DomGarguilo'}, {'comment': 'I recommended this change since that was there previously and IntelliJ actually gives a warning with the way you have it. We are already importing the StandardCharset for UTF-8 and using it elsewhere so this part should follow the precedent already set. ', 'commenter': 'Manno15'}]"
2228,server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java,"@@ -724,8 +709,8 @@ private String getRootUserName(SiteConfiguration siteConfig, Opts opts) throws I
       return DEFAULT_ROOT_USER;
     }
 
-    LineReader c = getLineReader();
-    c.getTerminal().writer().println(""Running against secured HDFS"");
+    Console c = System.console();","[{'comment': '```suggestion\r\n    \r\n```\r\n\r\nCan drop this and instead do \r\n\r\n```\r\nSystem.console().readLine(""Principal (user) to grant administrative privileges to : ""); \r\n```\r\nbelow where c is used (it is used only once).', 'commenter': 'Manno15'}, {'comment': 'Cleaned it up. ', 'commenter': 'harjitdotsingh'}]"
2228,server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java,"@@ -748,24 +733,27 @@ private String getRootUserName(SiteConfiguration siteConfig, Opts opts) throws I
     if (opts.cliPassword != null) {
       return opts.cliPassword.getBytes(UTF_8);
     }
-    String rootpass;
-    String confirmpass;
+    char[] rootpass;
+    char[] confirmpass;
+    String strrootpass;
+    String strconfirmpass;
     do {
-      rootpass = getLineReader().readLine(
-          ""Enter initial password for "" + rootUser + getInitialPasswordWarning(siteConfig), '*');
+      rootpass = System.console().readPassword(
+          ""Enter initial password for "" + rootUser + getInitialPasswordWarning(siteConfig));","[{'comment': '```suggestion\r\n     var rootpass = System.console().readPassword(\r\n          ""Enter initial password for "" + rootUser + getInitialPasswordWarning(siteConfig));\r\n```\r\n\r\nCould do something like this for both passwords and drop the char[] objects above.\r\n', 'commenter': 'Manno15'}, {'comment': 'Done. ', 'commenter': 'harjitdotsingh'}]"
2235,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/CompactableImpl.java,"@@ -1388,11 +1388,12 @@ public synchronized void close() {
 
       closed = true;
 
-      // wait while internal jobs are running or external compactions are committing, but do not
-      // wait on external compactions that are running
+      // wait while internal jobs are running, external compactions are committing or the status of
+      // chops is active, but do not wait on external compactions that are running
       while (runningJobs.stream()
           .anyMatch(job -> !((CompactionExecutorIdImpl) job.getExecutor()).isExternalId())
-          || !externalCompactionsCommitting.isEmpty()) {
+          || !externalCompactionsCommitting.isEmpty()
+          || fileMgr.chopStatus != FileSelectionStatus.NOT_ACTIVE) {","[{'comment': 'I think in the current code when a chop is actually marking the state will actually be NOT_ACTIVE.  Thinking we should introduce a new explicit state for MARKING and then we could do something like the following.\r\n\r\n```suggestion\r\n          || fileMgr.chopStatus == FileSelectionStatus.MARKING) {\r\n```\r\n\r\nTo go with the above change could do something like the following in checkIfChopComplete()\r\n\r\n```java\r\n  private void checkifChopComplete(Set<StoredTabletFile> allFiles) {\r\n\r\n    boolean completed = false;\r\n\r\n    synchronized (this) {\r\n      if (closed) {\r\n       // if closed do not attempt transition to marking state\r\n        return;\r\n     }\r\n      // when this returns true it means we transitioned to the MARKING state\r\n      completed = fileMgr.finishChop(allFiles);\r\n    }\r\n\r\n    if (completed) {\r\n      try {\r\n        markChopped();\r\n      } finally {\r\n        synchronized (this) {\r\n          // this will transition from MARKING to NOT_ACTIVE\r\n          fileMgr.finishMarkingChop();\r\n          this.notifyAll();\r\n        }\r\n      }\r\n\r\n      TabletLogger.selected(getExtent(), CompactionKind.CHOP, Set.of());\r\n    }\r\n  }\r\n```\r\n\r\nIn a [branch](https://github.com/keith-turner/accumulo/commits/accumulo-2199) I created for #2199 I have created a separate enum for chop status since chop only uses a subset of the file status states.  Adding an extra state that is only used for chop would make this separate enum even more useful.  For this PR could add the new state to the existing enum and I can fix it up later.\r\n', 'commenter': 'keith-turner'}, {'comment': ""> For this PR could add the new state to the existing enum and I can fix it up later.\r\n\r\nOkay I'll go ahead and do that here"", 'commenter': 'DomGarguilo'}, {'comment': ""> > For this PR could add the new state to the existing enum and I can fix it up later.\r\n> \r\n> Okay I'll go ahead and do that here\r\n\r\n@keith-turner, Actually do you think that the new state should just be added with the changes that will use it instead of adding it here before it is used anywhere? It seems like this PR could just be closed if there are plans to account for this later on."", 'commenter': 'DomGarguilo'}, {'comment': 'For the work on #2199 I would not be adding a new MARKING state, only a new enum specific to chop states.  Still need to make these changes. These changes could wait until the new enum is added and add the new state to that enum as that will be cleaner.  ', 'commenter': 'keith-turner'}, {'comment': '@keith-turner, I tried incorporating your suggestions in a9c0f9b. Not too sure if they are what you had in mind.', 'commenter': 'DomGarguilo'}, {'comment': 'The changes look good. Need to see if the new state affects existing code.', 'commenter': 'keith-turner'}, {'comment': ""I wasn't too sure how to best test to see if these changes affect existing code so I ended up running all the ITs on this branch. Everything looks good (except a failing ConcurrentDeleteTableIT which is known flaky and also fails on main so is probably unrelated. Might create a ticket for this.).\r\n\r\nI think these changes are ready for review @keith-turner "", 'commenter': 'DomGarguilo'}]"
2235,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/CompactableImpl.java,"@@ -1447,11 +1470,12 @@ public synchronized void close() {
 
       closed = true;
 
-      // wait while internal jobs are running or external compactions are committing, but do not
-      // wait on external compactions that are running
+      // wait while internal jobs are running, external compactions are committing or the status of
+      // chops is active, but do not wait on external compactions that are running
       while (runningJobs.stream()
           .anyMatch(job -> !((CompactionExecutorIdImpl) job.getExecutor()).isExternalId())
-          || !externalCompactionsCommitting.isEmpty()) {
+          || !externalCompactionsCommitting.isEmpty()
+          || fileMgr.chopStatus != ChopSelectionStatus.NOT_ACTIVE) {","[{'comment': 'Need to make this check more narrow.  After closed is set to true above, its not possible to enter the marking state. So we only want to wait on things that were in the marking state before closed was set to true.\r\n  \r\n```suggestion\r\n          || fileMgr.chopStatus == ChopSelectionStatus.MARKING) {\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Oops, I had thought I included this change earlier. Just included in 2e3d97e', 'commenter': 'DomGarguilo'}]"
2235,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/CompactableImpl.java,"@@ -739,11 +749,25 @@ private void checkifChopComplete(Set<StoredTabletFile> allFiles) {
     boolean completed;
 
     synchronized (this) {
+      if (closed) {
+        // if closed, do not attempt to transition to the MARKING state
+        return;
+      }
+      // when this returns true it means we transitioned to the MARKING state
       completed = fileMgr.finishChop(allFiles);
     }
 
     if (completed) {
-      markChopped();
+      try {
+        markChopped();
+      } finally {
+        synchronized (this) {
+          // transition the state from MARKING to NOT_ACTIVE
+          fileMgr.finishMarkingChop();","[{'comment': 'Could you do something with the boolean that is returned from this method? It might not matter but since you are checking the state and returning an outcome, maybe a false should be handled.', 'commenter': 'milleruntime'}, {'comment': 'Could remove the boolean and change the method to have a precondition that expects the current chop state to be MARKING.  I made a suggested change for the method.  The way the code is currently written, it would be unexpected for the chop state to be anything else when this method is called.', 'commenter': 'keith-turner'}]"
2235,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/CompactableImpl.java,"@@ -111,7 +111,7 @@
   private Set<CompactionServiceId> servicesUsed = new ConcurrentSkipListSet<>();
 
   enum ChopSelectionStatus {
-    SELECTING, SELECTED, NOT_ACTIVE
+    SELECTING, SELECTED, NOT_ACTIVE, MARKING","[{'comment': ""It would be nice to have a description of the different states. Or if that isn't helpful, at least a description of the state to state transitions."", 'commenter': 'milleruntime'}]"
2235,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/CompactableImpl.java,"@@ -347,6 +347,15 @@ boolean finishChop(Set<StoredTabletFile> allFiles) {
       return completed;
     }
 
+    boolean finishMarkingChop() {
+      if (chopStatus == ChopSelectionStatus.MARKING) {
+        chopStatus = ChopSelectionStatus.NOT_ACTIVE;
+        return true;
+      } else {
+        return false;
+      }
+    }","[{'comment': '```suggestion\r\n    void finishMarkingChop() {\r\n        Preconditions.checkState(chopStatus == ChopSelectionStatus.MARKING);\r\n        chopStatus = ChopSelectionStatus.NOT_ACTIVE;\r\n    }\r\n```', 'commenter': 'keith-turner'}]"
2235,server/tserver/src/test/java/org/apache/accumulo/tserver/tablet/CompactableImplFileManagerTest.java,"@@ -379,11 +379,15 @@ public void testChop() {
     tabletFiles = newFiles(""C00004.rf"", ""C00006.rf"", ""F00003.rf"", ""F00004.rf"");
     assertFalse(fileMgr.finishChop(tabletFiles));
     assertEquals(ChopSelectionStatus.SELECTED, fileMgr.getChopStatus());
-    assertFalse(fileMgr.finishMarkingChop());
+    try {
+      fileMgr.finishMarkingChop();
+    } catch (IllegalStateException e) {
+      // expected
+    }","[{'comment': 'Could use the assertThrows functions from junit here', 'commenter': 'keith-turner'}, {'comment': 'Good idea. Addressed in 76e6942', 'commenter': 'DomGarguilo'}]"
2235,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/CompactableImpl.java,"@@ -739,11 +745,25 @@ private void checkifChopComplete(Set<StoredTabletFile> allFiles) {
     boolean completed;
 
     synchronized (this) {
+      if (closed) {
+        // if closed, do not attempt to transition to the MARKING state
+        return;
+      }
+      // when this returns true it means we transitioned to the MARKING state","[{'comment': 'This comment is outdated now.', 'commenter': 'milleruntime'}, {'comment': 'Sorry, nevermind this is calling the other method.', 'commenter': 'milleruntime'}]"
2238,pom.xml,"@@ -566,6 +566,11 @@
         <artifactId>slf4j-api</artifactId>
         <version>${slf4j.version}</version>
       </dependency>
+      <dependency>
+        <groupId>org.yaml</groupId>
+        <artifactId>snakeyaml</artifactId>
+        <version>1.29</version>
+      </dependency>","[{'comment': ""This looks too good to be true... please correct me if I'm wrong, but at first glance, this appears to be a pure Java yaml implementation licensed as Apache 2.0 without any new transitive dependencies to conflict with.\r\n\r\nI'd probably be surprised, however, if commons-configuration2, which is already on our class path, didn't already support YAML configs."", 'commenter': 'ctubbsii'}, {'comment': ""> This looks too good to be true... please correct me if I'm wrong, but at first glance, this appears to be a pure Java yaml implementation licensed as Apache 2.0 without any new transitive dependencies to conflict with.\r\n\r\nYes, that appears to be true: https://bitbucket.org/asomov/snakeyaml/src/snakeyaml-1.29/pom.xml.\r\n\r\n> I'd probably be surprised, however, if commons-configuration2, which is already on our class path, didn't already support YAML configs.\r\n\r\nI have seen snakeyaml get pulled into my other projects usually as a dependency. In fact, commons-configuration2 uses it in [YamlConfiguration](https://github.com/apache/commons-configuration/blob/rel/commons-configuration-2.7/src/main/java/org/apache/commons/configuration2/YAMLConfiguration.java). See also the [pom](https://github.com/apache/commons-configuration/blob/rel/commons-configuration-2.7/pom.xml#L402). "", 'commenter': 'dlmarion'}]"
2238,core/src/test/resources/cluster.yml,"@@ -0,0 +1,53 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+managers:
+  - localhost1
+  - localhost2
+
+monitors:
+  - localhost1
+  - localhost2
+
+tracer:
+  - localhost
+
+gc:","[{'comment': 'This can also be plural, since we can have standby accumulo-gc services, just like the manager or monitor. If you make `gc` plural, it looks like `gcs`, which reads as ""jee - see - ess"" rather than the intended ""jee - sees"". To normalize, could just make them all singular. In English, instead of reading the `managers:` section as ""list of `managers`"", we read the `manager:` section as ""list of hosts with `manager` role"".', 'commenter': 'ctubbsii'}, {'comment': 'made key names singular', 'commenter': 'dlmarion'}]"
2238,assemble/bin/accumulo-service,"@@ -77,7 +79,7 @@ function start_service() {
   rotate_log ""$outfile""
   rotate_log ""$errfile""
 
-  nohup ""${bin}/accumulo"" ""$service"" >""$outfile"" 2>""$errfile"" < /dev/null &
+  nohup ""${bin}/accumulo"" ""$service"" ""${@:2}"" >""$outfile"" 2>""$errfile"" < /dev/null &","[{'comment': ""Hmm, was this related to something needed in this PR, or just a nice add you found? I'm okay either way, just curious what motivated this."", 'commenter': 'ctubbsii'}, {'comment': 'This was needed to be able to pass the `-q queueName` arguments to the Compactor.', 'commenter': 'dlmarion'}]"
2238,assemble/bin/accumulo-cluster,"@@ -340,57 +458,74 @@ function main() {
   accumulo_cmd=""${bin}/accumulo""
   SSH='ssh -qnf -o ConnectTimeout=2'
 
-  manager_file=""managers""
-  if [[ ! -f ""$conf/$manager_file"" && -f ""$conf/masters"" ]]; then
-    echo ""WARN : Use of 'masters' file is deprecated; use 'managers' file instead.""
-    manager_file=""masters""
-  fi
-
   case ""$1"" in
     create-config)
-      echo ""localhost"" > ""$conf/gc""
-      echo ""localhost"" > ""$conf/managers""
-      echo ""localhost"" > ""$conf/monitor""
-      echo ""localhost"" > ""$conf/tracers""
-      echo ""localhost"" > ""$conf/tservers""
+      cat <<EOF > $conf/cluster.yml
+managers:
+  - localhost
+
+monitors:
+  - localhost
+
+tracer:
+  - localhost
+
+gc:
+  - localhost
+
+tservers:
+  - localhost
+
+#compaction:
+#  coordinators:
+#    - localhost
+#  compactors:
+#    - queues:
+#      - q1
+#      - q2
+#    - q1:
+#        - localhost
+#    - q2:
+#        - localhost
+      EOF
       ;;
     restart)
-      verify_config","[{'comment': 'Nice how we already had a method that could be swapped out to do something more useful.', 'commenter': 'ctubbsii'}]"
2238,server/base/src/main/java/org/apache/accumulo/server/util/ZooZap.java,"@@ -61,6 +61,10 @@ private static void message(String msg, Opts opts) {
     boolean zapTservers = false;
     @Parameter(names = ""-tracers"", description = ""remove tracer locks"")
     boolean zapTracers = false;
+    @Parameter(names = ""-coordinators"", description = ""remove compaction coordinator locks"")","[{'comment': ""I wish `compaction-coordinators` wasn't so lengthy. `-coordinators` is just so generic, when we're referring to a specific coordinator."", 'commenter': 'ctubbsii'}, {'comment': 'What about calling it `coach` instead of `coordinator`. A Compaction Coach or compaction-coaches. It is a new term, its a lot shorter and has a similar meaning. We could even shorten them to `comp-coaches` or `co-coaches`.', 'commenter': 'milleruntime'}, {'comment': ""> I wish compaction-coordinators wasn't so lengthy. -coordinators is just so generic, when we're referring to a specific coordinator.\r\n\r\nIn the context of ZooZap, it's designed to clean up things in ZooKeeper for all servers of a given type. I don't think you can use ZooZap to clean up entries for a specific tserver. Regarding the parameter name, we don't have anything else called a coordinator today. I'm not sure where the confusion would come to play."", 'commenter': 'dlmarion'}]"
2238,core/src/test/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParserTest.java,"@@ -0,0 +1,94 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.conf.cluster;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+
+import java.io.File;
+import java.net.URL;
+import java.util.Map;
+
+import org.junit.Test;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_IN"", justification = ""paths provided by test"")
+public class ClusterConfigParserTest {
+
+  @Test
+  public void testParse() throws Exception {
+    URL configFile = ClusterConfigParserTest.class.getResource(""/cluster.yml"");
+    assertNotNull(configFile);
+
+    Map<String,String> contents =
+        ClusterConfigParser.parseConfiguration(new File(configFile.toURI()).getAbsolutePath());
+    assertEquals(5, contents.size());
+    assertTrue(contents.containsKey(""managers""));
+    assertEquals(""localhost1,localhost2"", contents.get(""managers""));
+    assertTrue(contents.containsKey(""monitors""));
+    assertEquals(""localhost1,localhost2"", contents.get(""monitors""));
+    assertTrue(contents.containsKey(""tracer""));
+    assertEquals(""localhost"", contents.get(""tracer""));
+    assertTrue(contents.containsKey(""gc""));
+    assertEquals(""localhost"", contents.get(""gc""));
+    assertTrue(contents.containsKey(""tservers""));
+    assertEquals(""localhost1,localhost2,localhost3,localhost4"", contents.get(""tservers""));
+    assertFalse(contents.containsKey(""compaction""));
+    assertFalse(contents.containsKey(""compaction.coordinators""));
+    assertFalse(contents.containsKey(""compaction.compactors""));
+    assertFalse(contents.containsKey(""compaction.compactors.queues""));
+    assertFalse(contents.containsKey(""compaction.compactors.q1""));
+    assertFalse(contents.containsKey(""compaction.compactors.q2""));
+  }
+
+  @Test
+  public void testParseWithExternalCompactions() throws Exception {
+    URL configFile =
+        ClusterConfigParserTest.class.getResource(""/cluster-with-external-compactions.yml"");","[{'comment': ""We have a lot of top-level resources. Resources can go in package folders like classes. This makes their location on the class path less likely to collide. Granted, this file name isn't likely to collide, but I still think it's best practice to put resources in a package. Could put this file in `src/test/resources/org/apache/accumulo/core/conf/cluster`, and then I think you could drop the leading `/` when loading and just load it by file name (but I'm not 100% sure I remember rules for locating resources)."", 'commenter': 'ctubbsii'}]"
2238,core/src/test/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParserTest.java,"@@ -0,0 +1,94 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.conf.cluster;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+
+import java.io.File;
+import java.net.URL;
+import java.util.Map;
+
+import org.junit.Test;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_IN"", justification = ""paths provided by test"")
+public class ClusterConfigParserTest {
+
+  @Test
+  public void testParse() throws Exception {
+    URL configFile = ClusterConfigParserTest.class.getResource(""/cluster.yml"");","[{'comment': 'See other comment about top-level resources that are likely to collide.', 'commenter': 'ctubbsii'}]"
2238,core/src/main/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParser.java,"@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.conf.cluster;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.nio.file.StandardOpenOption;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.yaml.snakeyaml.Yaml;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+public class ClusterConfigParser {","[{'comment': ""This class looks about right. I'm not a big fan of when libraries force you to work with `Object` types as generic params, but it looks like you don't have a lot of choice there."", 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -340,57 +458,74 @@ function main() {
   accumulo_cmd=""${bin}/accumulo""
   SSH='ssh -qnf -o ConnectTimeout=2'
 
-  manager_file=""managers""
-  if [[ ! -f ""$conf/$manager_file"" && -f ""$conf/masters"" ]]; then
-    echo ""WARN : Use of 'masters' file is deprecated; use 'managers' file instead.""
-    manager_file=""masters""
-  fi
-
   case ""$1"" in
     create-config)
-      echo ""localhost"" > ""$conf/gc""
-      echo ""localhost"" > ""$conf/managers""
-      echo ""localhost"" > ""$conf/monitor""
-      echo ""localhost"" > ""$conf/tracers""
-      echo ""localhost"" > ""$conf/tservers""
+      cat <<EOF > $conf/cluster.yml
+managers:
+  - localhost
+
+monitors:
+  - localhost
+
+tracer:
+  - localhost
+
+gc:
+  - localhost
+
+tservers:
+  - localhost
+
+#compaction:
+#  coordinators:
+#    - localhost
+#  compactors:
+#    - queues:
+#      - q1
+#      - q2
+#    - q1:
+#        - localhost
+#    - q2:
+#        - localhost
+      EOF","[{'comment': 'Nice. But, the EOF needs to be all the way to the left for this to work properly. It needs to be the only thing on the line.\r\n\r\n```suggestion\r\n      cat <<EOF > $conf/cluster.yml\r\nmanagers:\r\n  - localhost\r\n\r\nmonitors:\r\n  - localhost\r\n\r\ntracer:\r\n  - localhost\r\n\r\ngc:\r\n  - localhost\r\n\r\ntservers:\r\n  - localhost\r\n\r\n#compaction:\r\n#  coordinators:\r\n#    - localhost\r\n#  compactors:\r\n#    - queues:\r\n#      - q1\r\n#      - q2\r\n#    - q1:\r\n#        - localhost\r\n#    - q2:\r\n#        - localhost\r\nEOF\r\n```', 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -304,16 +417,21 @@ function stop_here() {
   if grep -Eq 'localhost|127[.]0[.]0[.]1' ""${conf}/tservers""; then
     ${accumulo_cmd} admin stop localhost
   else
+    IFS_ORIG=$IFS
+    IFS="",""
     for host in ""${hosts_to_check[@]}""; do
-      if grep -q ""$host"" ""${conf}/tservers""; then
-        ${accumulo_cmd} admin stop ""$host""
-      fi
+      for tserver in ${HOSTS[""TSERVERS""]}; do
+        if grep -q ""$host"" ""$tserver""; then
+          ${accumulo_cmd} admin stop ""$host""
+        fi
+      done
     done
+    IFS=$IFS_ORIG","[{'comment': ""This looks like the correct use of IFS... but messing with IFS can have unexpected consequences, especially when a future dev comes in later and changes something without understanding the implications.\r\n\r\nInstead of having these comma-separated, just make them space separated or use an array to begin with, as in the outer loop. It'd be much more reliable."", 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,74 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""
     exit 1
   fi
 
-  unset manager1
-  if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+  config=$(${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml)
+
+  declare -A HOSTS
+
+  HOSTS[""MANAGERS""]=$(grep managers $config | awk -F':' '{ print $2 }')
+  HOSTS[""MONITORS""]=$(grep monitors $config | awk -F':' '{ print $2 }')
+  HOSTS[""GC""]=$(grep gc $config | awk -F':' '{ print $2 }')
+  HOSTS[""TRACER""]=$(grep tracer $config | awk -F':' '{ print $2 }')
+  HOSTS[""TSERVERS""]=$(grep tservers $config | awk -F':' '{ print $2 }')","[{'comment': 'Don\'t rely on bash 4.0 associative arrays. They won\'t work on a Mac. It\'s also unnecessary, because you can just as easily do `MANAGER_HOSTS=...` or similar. You don\'t actually need all hosts to be stored in the same variable. I\'m also a bit confused by the grepping over a variable instead of a file handle. I don\'t think that will work.\r\n\r\nIf the output was something like:\r\n\r\n```\r\nmanager:host1,host2,host3\r\ngc:host4\r\n```\r\n\r\nThen `$config` would look like (a space separated array):\r\n```\r\nmanager:host1,host2,host3 gc:host4\r\n```\r\n\r\nthen you could either loop through the array until you found an item starting with `manager:`, or just grep for the manager chunk at word boundaries:\r\n\r\n```bash\r\nMANAGER_HOSTS=$(echo ""$config"" | grep \'\\<managers:[^ ]*\' | cut -f2- -d: | tr \',\' \' \')\r\n```\r\n\r\nAlso be careful about using colon as a delimiter for hosts, because hosts could be specified as IPv6, or we may wish to add port numbers at some point. In my example above, it\'s okay, because it only removes the first colon with the trailing dash in the field specifier, `-f2-` (interpret as fields 2 and later; the delimiter was specified as a colon with `-d:`).', 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,74 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""
     exit 1
   fi
 
-  unset manager1
-  if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+  config=$(${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml)
+
+  declare -A HOSTS
+
+  HOSTS[""MANAGERS""]=$(grep managers $config | awk -F':' '{ print $2 }')
+  HOSTS[""MONITORS""]=$(grep monitors $config | awk -F':' '{ print $2 }')
+  HOSTS[""GC""]=$(grep gc $config | awk -F':' '{ print $2 }')
+  HOSTS[""TRACER""]=$(grep tracer $config | awk -F':' '{ print $2 }')
+  HOSTS[""TSERVERS""]=$(grep tservers $config | awk -F':' '{ print $2 }')
+
+  grep ""compaction"" $config $2>1 > /dev/null
+  compaction_in_config=$?
+
+  if [[ $compaction_in_config -eq 0 ]]; then","[{'comment': 'You don\'t need to check `$?` when you can put the command directly into the if statement. Here, `grep -q` is useful for matching without displaying the output:\r\n\r\n```suggestion\r\n  if echo ""$config"" | grep -q compaction; then\r\n```\r\n', 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,74 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""
     exit 1
   fi
 
-  unset manager1
-  if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+  config=$(${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml)
+
+  declare -A HOSTS
+
+  HOSTS[""MANAGERS""]=$(grep managers $config | awk -F':' '{ print $2 }')
+  HOSTS[""MONITORS""]=$(grep monitors $config | awk -F':' '{ print $2 }')
+  HOSTS[""GC""]=$(grep gc $config | awk -F':' '{ print $2 }')
+  HOSTS[""TRACER""]=$(grep tracer $config | awk -F':' '{ print $2 }')
+  HOSTS[""TSERVERS""]=$(grep tservers $config | awk -F':' '{ print $2 }')
+
+  grep ""compaction"" $config $2>1 > /dev/null
+  compaction_in_config=$?
+
+  if [[ $compaction_in_config -eq 0 ]]; then
+    HOSTS[""COMPACTION_COORDINATORS""]=$(grep ""compaction.coordinators"" $file | awk -F':' '{ print $2 }')
+    HOSTS[""COMPACTION_QUEUES""]=$(grep ""compaction.compactors.queues"" $file | awk -F':' '{ print $2 }')","[{'comment': ""Probably better to use `cut -f2- -d:` rather than the heavyweight `awk -F':' '{ print $2 }'`"", 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,74 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""
     exit 1
   fi
 
-  unset manager1
-  if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+  config=$(${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml)
+
+  declare -A HOSTS
+
+  HOSTS[""MANAGERS""]=$(grep managers $config | awk -F':' '{ print $2 }')
+  HOSTS[""MONITORS""]=$(grep monitors $config | awk -F':' '{ print $2 }')
+  HOSTS[""GC""]=$(grep gc $config | awk -F':' '{ print $2 }')
+  HOSTS[""TRACER""]=$(grep tracer $config | awk -F':' '{ print $2 }')
+  HOSTS[""TSERVERS""]=$(grep tservers $config | awk -F':' '{ print $2 }')
+
+  grep ""compaction"" $config $2>1 > /dev/null
+  compaction_in_config=$?
+
+  if [[ $compaction_in_config -eq 0 ]]; then
+    HOSTS[""COMPACTION_COORDINATORS""]=$(grep ""compaction.coordinators"" $file | awk -F':' '{ print $2 }')
+    HOSTS[""COMPACTION_QUEUES""]=$(grep ""compaction.compactors.queues"" $file | awk -F':' '{ print $2 }')
+
+    IFS_ORIG=$IFS
+    IFS="",""
+    for queue in ${HOSTS[""COMPACTION_QUEUES""]}; do
+      HOSTS[""COMPACTORS_$queue""]=$(grep ""compaction.compactors.$queue"" $file | awk -F':' '{ print $2 }')","[{'comment': 'could do `eval COMPACTORS_$queue=...` instead of an associative array for these. Or, put the queue name at the front, in a regular bash array, as in:\r\n\r\n```bash\r\nCOMPACTOR_QUEUES=($queue:\'compactors,here\')\r\n```\r\nYou can use += on bash 3.2 on Macs, I\'ve read. But, another way to append to an array is:\r\n```bash\r\nCOMPACTOR_QUEUES=(""${COMPACTOR_QUEUES[@]}"" $queue:\'compactors,here\')\r\n```', 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,74 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""
     exit 1
   fi
 
-  unset manager1
-  if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+  config=$(${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml)
+
+  declare -A HOSTS
+
+  HOSTS[""MANAGERS""]=$(grep managers $config | awk -F':' '{ print $2 }')
+  HOSTS[""MONITORS""]=$(grep monitors $config | awk -F':' '{ print $2 }')
+  HOSTS[""GC""]=$(grep gc $config | awk -F':' '{ print $2 }')
+  HOSTS[""TRACER""]=$(grep tracer $config | awk -F':' '{ print $2 }')
+  HOSTS[""TSERVERS""]=$(grep tservers $config | awk -F':' '{ print $2 }')
+
+  grep ""compaction"" $config $2>1 > /dev/null
+  compaction_in_config=$?
+
+  if [[ $compaction_in_config -eq 0 ]]; then
+    HOSTS[""COMPACTION_COORDINATORS""]=$(grep ""compaction.coordinators"" $file | awk -F':' '{ print $2 }')
+    HOSTS[""COMPACTION_QUEUES""]=$(grep ""compaction.compactors.queues"" $file | awk -F':' '{ print $2 }')
+
+    IFS_ORIG=$IFS
+    IFS="",""
+    for queue in ${HOSTS[""COMPACTION_QUEUES""]}; do
+      HOSTS[""COMPACTORS_$queue""]=$(grep ""compaction.compactors.$queue"" $file | awk -F':' '{ print $2 }')
+    done
+    IFS=$IFS_ORIG
   fi
 
-  if [[ -z ""${monitor}"" ]] ; then
-    monitor=$manager1
-    if [[ -f ""${conf}/monitor"" ]]; then
-      monitor=$(grep -Ev '(^#|^\s*$)' ""${conf}/monitor"" | head -1)
-    fi
-    if [[ -z ""${monitor}"" ]] ; then
-      echo ""Could not infer a Monitor role. You need to either define \""${conf}/monitor\"",""
-      echo ""or make sure \""${conf}/$manager_file\"" is non-empty.""
-      exit 1
-    fi
+  if [[ ${#HOSTS[""MANAGERS""]} -eq 0 ]]; then
+    echo ""ERROR: managers not found in ${conf}/cluster.yml""
+    exit 1
   fi
-  if [[ ! -f ""${conf}/tracers"" ]]; then
-    if [[ -z ""${manager1}"" ]] ; then
-      echo ""Could not find a manager node to use as a default for the tracer role.""
-      echo ""Either set up \""${conf}/tracers\"" or make sure \""${conf}/$manager_file\"" is non-empty.""
-      exit 1
-    else
-      echo ""$manager1"" > ""${conf}/tracers""
-    fi
+
+  if [[ ${#HOSTS[""TSERVERS""]} -eq 0 ]]; then
+    echo ""ERROR: tservers not found in ${conf}/cluster.yml""
+    exit 1
   fi
-  if [[ ! -f ""${conf}/gc"" ]]; then
-    if [[ -z ""${manager1}"" ]] ; then
-      echo ""Could not infer a GC role. You need to either set up \""${conf}/gc\"" or make sure \""${conf}/$manager_file\"" is non-empty.""
-      exit 1
-    else
-      echo ""$manager1"" > ""${conf}/gc""
-    fi
+
+  unset manager1
+  manager1=$(echo ""${HOSTS[""MANAGERS""]}"" | cut -d"","" -f1)
+
+  if [[ ${#HOSTS[""MONITORS""]} -eq 0 ]]; then
+    echo ""WARN: monitors not found in ${conf}/cluster.yml, using first manager host $manager1""
+    HOSTS[""MONITORS""]=$manager1
+    exit 1
+  fi","[{'comment': ""You probably don't want to exit with an error if it's just a warning and you're mitigating it by assigning the first manager. Also, the Java code could probably do that logic, so that its output is always valid, and there's less checking for whether a server type is set or falls back to the default."", 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,74 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""
     exit 1
   fi
 
-  unset manager1
-  if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+  config=$(${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml)","[{'comment': 'Might want to catch a bad return code from Java here, as in:\r\n\r\n```suggestion\r\n  config=$(${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml) || call_some_die_method_here\r\n```', 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -116,19 +138,21 @@ function control_service() {
     if [[ $host == localhost || $host == ""$(hostname -s)"" || $host == ""$(hostname -f)"" || $host == $(get_ip) ]] ; then
       ACCUMULO_SERVICE_INSTANCE=""${ACCUMULO_SERVICE_INSTANCE}"" ""${bin}/accumulo-service"" ""$service"" ""$control_cmd""
     else
-      $SSH ""$host"" ""bash -c 'ACCUMULO_SERVICE_INSTANCE=${ACCUMULO_SERVICE_INSTANCE} ${bin}/accumulo-service \""$service\"" \""$control_cmd\""'""
+      $SSH ""$host"" ""bash -c 'ACCUMULO_SERVICE_INSTANCE=${ACCUMULO_SERVICE_INSTANCE} ${bin}/accumulo-service \""$service\"" \""$control_cmd\"" \""{$@:4}'""","[{'comment': ""This looks incorrect.  There's no trailing escaped double-quote. Also, the quoting is extremely hard to reason about here... not sure it's a great idea to pass along additional arbitrary arguments to bash within ssh within bash when they could have their own quoting issues."", 'commenter': 'ctubbsii'}]"
2238,assemble/bin/accumulo-cluster,"@@ -147,86 +172,141 @@ function start_all() {
     start_tservers
   fi
 
-  grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | while read -r host; do
-    start_service ""$host"" manager
+  IFS_ORIG=$IFS
+  IFS="",""
+
+  for manager in ${HOSTS[""MANAGERS""]}; do
+    start_service ""$manager"" manager
+  done
+
+  for gc in ""${HOSTS[""GC""]}""; do
+    start_service ""$gc"" gc
+  done
+
+  for monitor in ${HOSTS[""MONITORS""]}; do
+    start_service ""$monitor"" monitor
   done
 
-  grep -Ev '(^#|^\s*$)' ""${conf}/gc"" | while read -r host; do
-    start_service ""$host"" gc
+  for tracer in ${HOSTS[""TRACER""]}; do
+    start_service ""$tracer"" tracer
   done
 
-  grep -Ev '(^#|^\s*$)' ""${conf}/tracers"" | while read -r host; do
-    start_service ""$host"" tracer
+  for coordinator in ${HOSTS[""COMPACTION_COORDINATORS""]}; do
+    start_service ""$coordinator"" compaction-coordinator
   done
 
-  start_service ""$monitor"" monitor
+  for queue in ${HOSTS[""COMPACTION_QUEUES""]}; do
+    for compactor in ${HOSTS[""COMPACTORS_$queue""]}; do
+      start_service ""$compactor"" compactor ""-q $queue""","[{'comment': 'These are two arguments, not one, so should be quoted as two:\r\n\r\n```suggestion\r\n      start_service ""$compactor"" compactor -q ""$queue""\r\n```', 'commenter': 'ctubbsii'}]"
2238,core/src/main/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParser.java,"@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.conf.cluster;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.nio.file.StandardOpenOption;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.yaml.snakeyaml.Yaml;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+public class ClusterConfigParser {
+
+  @SuppressFBWarnings(value = ""PATH_TRAVERSAL_IN"", justification = ""paths not set by user input"")
+  public static Map<String,String> parseConfiguration(String configFile) throws IOException {
+    Map<String,String> results = new HashMap<>();
+    try (InputStream fis = Files.newInputStream(Paths.get(configFile), StandardOpenOption.READ)) {
+      Yaml y = new Yaml();
+      Map<String,Object> config = y.load(fis);
+      config.forEach((k, v) -> flatten("""", k, v, results));
+    }
+    return results;
+  }
+
+  private static String addTheDot(String key) {
+    return (key.endsWith(""."")) ? """" : ""."";
+  }
+
+  @SuppressWarnings(""unchecked"")
+  private static void flatten(String parentKey, String key, Object value,
+      Map<String,String> results) {
+    String parent =
+        (parentKey == null || parentKey.equals("""")) ? """" : parentKey + addTheDot(parentKey);
+    if (value instanceof String) {
+      results.put(parent + key, (String) value);
+      return;
+    } else if (value instanceof List) {
+      ((List<?>) value).forEach(l -> {
+        if (l instanceof String) {
+          // remove the [] at the ends of toString()
+          String val = value.toString();
+          results.put(parent + key, val.substring(1, val.length() - 1).replace("", "", "",""));
+          return;
+        } else {
+          flatten(parent, key, l, results);
+        }
+      });
+    } else if (value instanceof Map) {
+      ((Map<String,Object>) value).forEach((k, v) -> flatten(parent + key, k, v, results));
+    } else {
+      throw new RuntimeException(""Unhandled object type: "" + value.getClass());
+    }
+  }
+
+  public static void main(String[] args) throws IOException {
+    if (args == null || args.length != 1) {
+      System.err.println(""Usage: ClusterConfigParser <configFile>"");
+      System.exit(1);
+    }
+    parseConfiguration(args[0]).forEach((k, v) -> System.out.println(k + "":"" + v));","[{'comment': '@ctubbsii  - what are your thoughts here if instead of outputting the keys and values of the map to STDOUT, if instead I wrote out something that can be written to a file and then sourced by the script. For example:\r\n\r\n```\r\nMANAGER_HOSTS=localhost1,localhost2\r\n...\r\n```', 'commenter': 'dlmarion'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,68 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_fail {
+  echo ""Failed to parse ${conf}/cluster.yml""
+  exit 1
+}
+
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then","[{'comment': 'This could check for  existence of `slave` or `tservers` files now.', 'commenter': 'keith-turner'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,68 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_fail {
+  echo ""Failed to parse ${conf}/cluster.yml""
+  exit 1
+}
+
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""","[{'comment': 'This message is no longer correct.', 'commenter': 'keith-turner'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,68 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_fail {
+  echo ""Failed to parse ${conf}/cluster.yml""
+  exit 1
+}
+
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""","[{'comment': '```suggestion\r\n    echo ""Please make sure it exists and is configured with the host information. Run \'accumulo-cluster create-config\' to create an example configuration.""\r\n```', 'commenter': 'keith-turner'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,68 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_fail {
+  echo ""Failed to parse ${conf}/cluster.yml""
+  exit 1
+}
+
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""
     exit 1
   fi
 
-  unset manager1
-  if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+  trap 'rm -f ""$CONFIG_FILE""' EXIT
+  CONFIG_FILE=$(mktemp) || exit 1
+  ${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml > $CONFIG_FILE || parse_fail
+  . $CONFIG_FILE","[{'comment': 'This is really neat how the java code creates bash code that can be sourced.', 'commenter': 'keith-turner'}]"
2238,assemble/bin/accumulo-cluster,"@@ -147,84 +164,128 @@ function start_all() {
     start_tservers
   fi
 
-  grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | while read -r host; do
-    start_service ""$host"" manager
+  for manager in $MANAGER_HOSTS; do
+    start_service ""$manager"" manager
+  done
+
+  for gc in $GC_HOSTS; do
+    start_service ""$gc"" gc
   done
 
-  grep -Ev '(^#|^\s*$)' ""${conf}/gc"" | while read -r host; do
-    start_service ""$host"" gc
+  for monitor in $MONITOR_HOSTS; do
+    start_service ""$monitor"" monitor
   done
 
-  grep -Ev '(^#|^\s*$)' ""${conf}/tracers"" | while read -r host; do
-    start_service ""$host"" tracer
+  for tracer in $TRACER_HOSTS; do
+    start_service ""$tracer"" tracer
+  done
+
+  for coordinator in $COORDINATOR_HOSTS; do
+    start_service ""$coordinator"" compaction-coordinator
+  done
+
+  for queue in $COMPACTION_QUEUES; do
+    Q=""COMPACTOR_HOSTS_${queue}""
+    for compactor in ""${!Q}""; do
+      start_service ""$compactor"" compactor ""-q"" ""$queue""
+    done
   done
 
-  start_service ""$monitor"" monitor
 }
 
 function start_here() {
 
   local_hosts=""$(hostname -a 2> /dev/null) $(hostname) localhost 127.0.0.1 $(get_ip)""
+
   for host in $local_hosts; do
-    if grep -q ""^${host}\$"" ""${conf}/tservers""; then
-      start_service ""$host"" tserver
-      break
-    fi
+    for tserver in $TSERVER_HOSTS; do
+      if echo ""$tserver"" | grep -q ""^${host}\$""; then
+        start_service ""$host"" tserver
+        break
+      fi
+    done
   done
 
   for host in $local_hosts; do
-    if grep -q ""^${host}\$"" ""${conf}/$manager_file""; then
-      start_service ""$host"" manager
-      break
-    fi
+    for manager in $MANAGER_HOSTS; do
+      if echo ""$manager"" | grep -q ""^${host}\$""; then
+        start_service ""$host"" manager
+        break
+      fi
+    done
   done
 
   for host in $local_hosts; do
-    if grep -q ""^${host}\$"" ""${conf}/gc""; then
-      start_service ""$host"" gc
-      break
-    fi
+    for gc in $GC_HOSTS; do
+      if echo ""$gc"" | grep -q ""^${host}\$""; then
+        start_service ""$host"" gc
+        break
+      fi
+    done
   done
 
   for host in $local_hosts; do
-    if [[ ""$host"" == ""$monitor"" ]]; then
-      start_service ""$host"" monitor
-      break
-    fi
+    for monitor in $MONITOR_HOSTS; do
+      if echo ""$monitor"" | grep -q ""^${host}\$""; then
+        start_service ""$host"" monitor
+        break
+      fi
+    done
   done
 
   for host in $local_hosts; do
-    if grep -q ""^${host}\$"" ""${conf}/tracers""; then
-      start_service ""$host"" tracer
-      break
-    fi
+    for tracer in $TRACER_HOSTS; do
+      if echo ""$tracer"" | grep -q ""^${host}\$""; then
+        start_service ""$host"" tracer
+        break
+      fi
+    done
   done
+
+  for host in $local_hosts; do
+    for coordinator in $COORDINATOR_HOSTS; do
+      if echo ""$coordinator"" | grep -q ""^${host}\$""; then
+        start_service ""$coordinator"" compaction-coordinator
+      fi
+    done
+  done
+
+  for queue in $COMPACTION_QUEUES; do
+    for host in $local_hosts; do
+      Q=""COMPACTOR_HOSTS_${queue}""
+      for compactor in ""${!Q}""; do","[{'comment': 'What are these two lines doing?  I don\'t understand the `""${!Q}""` bash syntax.  If not easy/quick to explain, don\'t worry about it.', 'commenter': 'keith-turner'}]"
2238,assemble/bin/accumulo-cluster,"@@ -43,52 +43,68 @@ function invalid_args {
   exit 1
 }
 
-function verify_config {
+function parse_fail {
+  echo ""Failed to parse ${conf}/cluster.yml""
+  exit 1
+}
+
+function parse_config {
   if [[ -f ${conf}/slaves ]]; then
     echo ""ERROR: A 'slaves' file was found in ${conf}/""
-    echo ""Accumulo now reads tablet server hosts from 'tservers' and requires that the 'slaves' file not be present to reduce confusion.""
+    echo ""Accumulo now uses cluster host configuration information from 'cluster.yml' and requires that the 'slaves' file not be present to reduce confusion.""
     echo ""Please rename the 'slaves' file to 'tservers' or remove it if both exist.""
     exit 1
   fi
 
-  if [[ ! -f ${conf}/tservers ]]; then
-    echo ""ERROR: A 'tservers' file was not found at ${conf}/tservers""
-    echo ""Please make sure it exists and is configured with tablet server hosts.""
+  if [[ ! -f ${conf}/cluster.yml ]]; then
+    echo ""ERROR: A 'cluster.yml' file was not found at ${conf}/cluster.yml""
+    echo ""Please make sure it exists and is configured with the host information. Run 'create-config' to create an example configuration.""
     exit 1
   fi
 
-  unset manager1
-  if [[ -f ""${conf}/$manager_file"" ]]; then
-    manager1=$(grep -Ev '(^#|^\s*$)' ""${conf}/$manager_file"" | head -1)
+  trap 'rm -f ""$CONFIG_FILE""' EXIT
+  CONFIG_FILE=$(mktemp) || exit 1
+  ${accumulo_cmd} org.apache.accumulo.core.conf.cluster.ClusterConfigParser ${conf}/cluster.yml > $CONFIG_FILE || parse_fail","[{'comment': 'For the case where the tmp dir is full, the error message in parse_fail could be slightly misleading.  Someone may look for non-existent errors in the yaml.  However I am not sure its worthwhile improving the error message for this case.', 'commenter': 'keith-turner'}]"
2255,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -1939,6 +1939,17 @@ public void scans() throws Exception {
     ts.exec(""deletetable -f t"");
   }
 
+  @Test
+  public void scanswithcolon() throws Exception {","[{'comment': '```suggestion\r\n  public void scansWithColon() throws Exception {\r\n```', 'commenter': 'Manno15'}, {'comment': 'Done', 'commenter': 'harjitdotsingh'}]"
2255,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -1939,6 +1939,17 @@ public void scans() throws Exception {
     ts.exec(""deletetable -f t"");
   }
 
+  @Test
+  public void scanswithcolon() throws Exception {
+    ts.exec(""createtable twithcolontest"");
+    make10withcolon();
+    String result = ts.exec(""scan -b row0 -c c\\:f:col0 -e row0"");
+    assertEquals(1, result.split(""\n"").length);
+    result = ts.exec(""scan -r row0 -c c\\:f:col0"");
+    assertEquals(1, result.split(""\n"").length);","[{'comment': ""This part does not seem to be working as expected. The excepted value of 1 here is actually the number of lines in the test shell. There will always be 1 here since the inputted command counts as one line. So in theory, the expected output should be two which isn't the case. "", 'commenter': 'Manno15'}, {'comment': 'You could also do something like the code below to confirm the output includes the expected value for each row. This is just a suggestion though. \r\n\r\n```java\r\nts.exec(""scan -r row0 -c c\\\\:f:col0"", true, ""value"");\r\n```\r\n\r\n', 'commenter': 'Manno15'}]"
2255,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -1939,6 +1939,17 @@ public void scans() throws Exception {
     ts.exec(""deletetable -f t"");
   }
 
+  @Test
+  public void scanswithcolon() throws Exception {
+    ts.exec(""createtable twithcolontest"");
+    make10withcolon();","[{'comment': 'With this specific change, I think it is safe to insert a couple of rows inside the test instead of creating a separate function to create 10. Not a big deal either way. ', 'commenter': 'Manno15'}]"
2255,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -1939,6 +1939,19 @@ public void scans() throws Exception {
     ts.exec(""deletetable -f t"");
   }
 
+  @Test
+  public void scansWithColon() throws Exception {
+    ts.exec(""createtable twithcolontest"");
+    make10WithColon();
+    ts.exec(""scan -r row0 -c c\\:f:col0"", true, ""value"");
+    String result = ts.exec(""scan -b row1 -c c\\:f:col1 -e row1"");
+    assertEquals(2, result.split(""\n"").length);
+    result = ts.exec(""scan -r row0 -c c:f"");
+    assertEquals(1, result.split(""\n"").length);","[{'comment': ""These changes look good so far, just a few things I am noticing with the latest changes It seems the use of an '\\\\' character is unnecessary now. The reason your second check doesn't pass is because it does not contain the :col1 (which isn't a requirement for scan, just a quirk of the changes you added) at the end."", 'commenter': 'Manno15'}, {'comment': 'So `scan -b row1 -c c\\:f:col1 -e row1` will return the row, `scan -b row1 -c c:f:col1 -e row1` will as well, but `scan -b row1 -c c\\:f -e row1` and `scan -b row1 -c c:f -e row1` will not', 'commenter': 'Manno15'}, {'comment': ""For Some reasons the '\\\\' doesn't make it, the string is not escaped anymore and hence the RE was un-necessary. I made it more simpler. The last test is a negative test I deliberately left out to make sure that things are working. Hope that makes sense. "", 'commenter': 'harjitdotsingh'}, {'comment': ""> The last test is a negative test I deliberately left out to make sure that things are working. Hope that makes sense.\r\n\r\nI understood that, I was using it to show the inconsistent behavior. My first comment was just poorly worded on my part. I was stating that if the :col1 was added, it would return the row even though the intent was that it shouldn't. "", 'commenter': 'Manno15'}]"
2255,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -285,6 +288,19 @@ protected void fetchColumns(final CommandLine cl, final ScannerBase scanner,
     }
   }
 
+  protected String[] extractColumnFamily(final String columnString) {","[{'comment': ""This might be able to be simplified by just using the substring method. Also it doesn't seem like this accounts for strings where there are more than 2 ':' characters (not sure if that will ever be the case), for example `a:c:d:e` would be sent to the `else` branch and possibly handled incorrectly. "", 'commenter': 'DomGarguilo'}, {'comment': 'Are referring to that we just take a substring from the last index of : and that would do it?  If we have more than 2 that will be a problem, I can change the condition because anything  &gt; 2 should be split into two strings cf and qualifier. ', 'commenter': 'harjitdotsingh'}, {'comment': 'The colFam and the colQual values are user defined so Accumulo really should be able to handle whatever they wish to do.  Unless there are specific characters that are prohibited, then they should be allowed.\r\n\r\nOne example could be an ipv6 address say colFam was IP and colQual was port - you may want to scan for a specific IP and specific port, just the IP, or even maybe IP that starts with and either with or without a port.\r\n\r\nUsing the IPv6 example from [wikipedia](https://en.wikipedia.org/wiki/IPv6_address) ```2001:0db8:85a3:0000:0000:8a2e:0370:7334``` If that was a colFam, then it would be desirable to scan for all or parts of that address plus (optionally) any colQual value that a user might want.\r\n\r\n', 'commenter': 'EdColeman'}, {'comment': 'makes sense, we should be able to fix it easily\r\n', 'commenter': 'harjitdotsingh'}, {'comment': 'If that is the case, then having a separate opt for cf and cq could be easier. Take the entire string after the -cf opt instead of trying to parse everything accordingly. ', 'commenter': 'Manno15'}, {'comment': 'If we are taking input in this form 2001:0db8:85a3:0000:0000:8a2e:0370:7334 its hard to know what is part of cf and what is cq. I think if we would be seeing this then we should move to using options  to specify the column family and column qualifier using cf and cq. ', 'commenter': 'harjitdotsingh'}, {'comment': ""> Are referring to that we just take a substring from the last index of : and that would do it?\r\n\r\nYea what I'm saying is, the substring method you have will correctly handle all cases so you could remove what is in the else block and allow all cases to be handled by what you have in the if block. That way there doesnt need to be a check for how many colons are in the string. @harjitdotsingh "", 'commenter': 'DomGarguilo'}, {'comment': 'so if we go the cf and cq route do we get rid of -c or keep it ?', 'commenter': 'harjitdotsingh'}, {'comment': ""I'd vote to get rid of it since cf and cq covers its functionality."", 'commenter': 'Manno15'}, {'comment': 'Here are the comments from @ctubbsii  \r\n\r\n`Alternatively, we can add -cf and -cq options to the scan command which is mutually exclusive with -c.`\r\n\r\nGoing by that  it would mean that we still want to keep -c but  IMHO its redundant to have  -c , -cf and -cq.  Would like to see what @ctubbsii  has to say about it? Do we want to keep -c with -cf and -cq. I think it will be confusing for people to have 2 options for a operation. Thoughts ?\r\n\r\n', 'commenter': 'harjitdotsingh'}, {'comment': 'It is likely that people have scripts that use -c. Removing that as an option will break those scripts.  People using -c currently are limited to not having : in the colFam, but they are coping.\r\n\r\nAdding -cq and -cf provide additional capability when (the current -c) is insufficient for their needs.  ', 'commenter': 'EdColeman'}]"
2255,shell/src/main/java/org/apache/accumulo/shell/commands/QuotedStringTokenizer.java,"@@ -72,7 +72,7 @@ private void createTokens() throws BadArgumentException, UnsupportedEncodingExce
         inEscapeSequence = false;
         if (ch == 'x') {
           hexChars = """";
-        } else if (ch == ' ' || ch == '\'' || ch == '""' || ch == '\\') {
+        } else if (ch == ' ' || ch == '\'' || ch == '""' || ch == '\\' || ch == ':') {","[{'comment': ""Not sure if this change is necessary for regards to this PR. If you think it still is then some of the comments should be updated to include the ':' being added. "", 'commenter': 'Manno15'}, {'comment': ""We don't need this. We can revert this back. Let me do that"", 'commenter': 'harjitdotsingh'}]"
2255,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -59,7 +59,8 @@
 public class ScanCommand extends Command {
 
   private Option scanOptAuths, scanOptRow, scanOptColumns, disablePaginationOpt, showFewOpt,
-      formatterOpt, interpreterOpt, formatterInterpeterOpt, outputFileOpt;
+      formatterOpt, interpreterOpt, formatterInterpeterOpt, outputFileOpt, scanOptCfOptions,
+      scanOptColQualifier;","[{'comment': ""```suggestion\r\n      formatterOpt, interpreterOpt, formatterInterpeterOpt, outputFileOpt, scanOptColFam,\r\n      scanOptColQualifier;\r\n```\r\n\r\nDoesn't have to be this specific wording but the two added opts should have a similar naming convention. "", 'commenter': 'Manno15'}]"
2255,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -270,6 +272,16 @@ protected ScanInterpreter getInterpreter(final CommandLine cl, final String tabl
 
   protected void fetchColumns(final CommandLine cl, final ScannerBase scanner,
       final ScanInterpreter formatter) throws UnsupportedEncodingException {
+
+    if ((cl.hasOption(scanOptCfOptions.getOpt()) || cl.hasOption(scanOptColQualifier.getOpt()))
+        && cl.hasOption(scanOptColumns.getOpt())) {
+
+      String formattedString =
+          String.format(""Options - %s AND (- %s"" + ""OR - %s are mutually exclusive "",","[{'comment': 'The spacing is a little off here. This is how it looks in the shell: `Options - c AND (- cfOR - cq are mutually exclusive`', 'commenter': 'Manno15'}]"
2255,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -285,6 +297,33 @@ protected void fetchColumns(final CommandLine cl, final ScannerBase scanner,
     }
   }
 
+  private void fetchColumsWithCFAndCQ(CommandLine cl, Scanner scanner, ScanInterpreter interpeter) {
+    String cf = """";
+    String cq = """";
+    if (cl.hasOption(scanOptCfOptions.getOpt())) {
+      cf = cl.getOptionValue(scanOptCfOptions.getOpt());
+    }
+    if (cl.hasOption(scanOptColQualifier.getOpt())) {
+      cq = cl.getOptionValue(scanOptColQualifier.getOpt());
+    }
+
+    if (cf.isEmpty() && !cq.isEmpty()) {
+      String formattedString = String.format(
+          ""Option - %s when used with (- %s"" + ""cannot be empty "", scanOptCfOptions.getOpt(),","[{'comment': 'Like above, the spacing is also a little off here. This is how it looks in the shell: ` Option - cf when used with (- cqcannot be empty`', 'commenter': 'Manno15'}]"
2255,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -348,6 +387,9 @@ public Options getOptions() {
     optEndRowExclusive.setArgName(""end-exclusive"");
     scanOptRow = new Option(""r"", ""row"", true, ""row to scan"");
     scanOptColumns = new Option(""c"", ""columns"", true, ""comma-separated columns"");
+    scanOptCfOptions = new Option(""cf"", ""column-family"", true, ""Column Family"");
+    scanOptColQualifier = new Option(""cq"", ""column-qualifier"", true, ""Column Qualifier"");","[{'comment': '```suggestion\r\n    scanOptCfOptions = new Option(""cf"", ""column-family"", true, ""column family"");\r\n    scanOptColQualifier = new Option(""cq"", ""column-qualifier"", true, ""column qualifier"");\r\n```\r\nTo match the rest of the options. Could also add more detail such as `column family to scan`.\r\n', 'commenter': 'Manno15'}]"
2255,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -1939,6 +1939,16 @@ public void scans() throws Exception {
     ts.exec(""deletetable -f t"");
   }
 
+  @Test
+  public void scansWithColon() throws Exception {
+    ts.exec(""createtable twithcolontest"");
+    make10WithColon();
+    ts.exec(""scan -r row0 -cf c:f"", true, ""value"");
+    String result = ts.exec(""scan -b row1 -cf c:f  -cq col1 -e row1"");
+    assertEquals(2, result.split(""\n"").length);","[{'comment': 'Could add a test case for if a user adds the `-cq` option without adding a `-cf` and one where they also add the `-c` option.', 'commenter': 'Manno15'}, {'comment': 'sure', 'commenter': 'harjitdotsingh'}]"
2255,test/src/main/java/org/apache/accumulo/test/ShellServerIT.java,"@@ -1939,6 +1939,16 @@ public void scans() throws Exception {
     ts.exec(""deletetable -f t"");
   }
 
+  @Test
+  public void scansWithColon() throws Exception {
+    ts.exec(""createtable twithcolontest"");
+    make10WithColon();","[{'comment': '```suggestion\r\n    ts.exec(""insert row c:f cq value"")\r\n```\r\nThis test really only needs one row in the table.', 'commenter': 'Manno15'}]"
2255,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -348,6 +387,9 @@ public Options getOptions() {
     optEndRowExclusive.setArgName(""end-exclusive"");
     scanOptRow = new Option(""r"", ""row"", true, ""row to scan"");
     scanOptColumns = new Option(""c"", ""columns"", true, ""comma-separated columns"");","[{'comment': 'Could mention that -c is mutually exclusive here so users will avoid the thrown error above. One idea I had is if both options are passed in, we could just log a warning and then choose to ignore one of the arguments (either -c or -cf/cq). Thoughts on that? ', 'commenter': 'Manno15'}]"
2279,server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java,"@@ -578,15 +577,6 @@ private void fetchScans() throws Exception {
         ThriftUtil.returnClient(tserver);
       }
     }
-    // Age off old scan information
-    Iterator<Entry<HostAndPort,ScanStats>> entryIter = allScans.entrySet().iterator();
-    long now = System.currentTimeMillis();
-    while (entryIter.hasNext()) {
-      Entry<HostAndPort,ScanStats> entry = entryIter.next();
-      if (now - entry.getValue().fetched > 5 * 60 * 1000) {
-        entryIter.remove();
-      }
-    }","[{'comment': ""I don't think this change will result in desired behavior. There is no other code that will ever remove anything from `allScans` other than this code. So, that object will continue to grow indefinitely, keeping every scan ever fetched, until the monitor falls over due to memory problems."", 'commenter': 'ctubbsii'}, {'comment': 'The map will only grow per tserver. For each tserver, we get the active scans and do a straight put in the map:\r\n<pre>\r\nallScans.put(parsedServer, new ScanStats(scans));\r\n</pre>\r\n\r\nSo yes it could grow as tservers are added and removed from the cluster but not by that much. Either way, this could be done better so it only stores the relevant data.\r\n\r\nI am trying to simulate a slow running scan in Uno using `SlowIterator`. If I can get that to work, I will make this better.', 'commenter': 'milleruntime'}]"
2293,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -172,18 +173,22 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
 
     }
 
+    Set<String> tableIdsBefore = gce.getTableIDs();
+    Set<String> tableIdsSeen = new HashSet<>();
     Iterator<Entry<Key,Value>> iter = gce.getReferenceIterator();
     while (iter.hasNext()) {
       Entry<Key,Value> entry = iter.next();
       Key key = entry.getKey();
       Text cft = key.getColumnFamily();
+      tableIdsSeen.add(new String(KeyExtent.tableOfMetadataRow(key.getRow())));
 
       if (cft.equals(DataFileColumnFamily.NAME) || cft.equals(ScanFileColumnFamily.NAME)) {
         String cq = key.getColumnQualifier().toString();
 
         String reference = cq;
         if (cq.startsWith(""/"")) {
           String tableID = new String(KeyExtent.tableOfMetadataRow(key.getRow()));
+          tableIdsSeen.add(tableID);","[{'comment': 'Seems like the code is deriving the table id from the row multiple times and adding it (or is there some little diff I am missing?).  May be able to do the following.  \r\n\r\n```suggestion\r\n      String tableID = new String(KeyExtent.tableOfMetadataRow(key.getRow()));\r\n      tableIdsSeen.add(tableID);\r\n\r\n      if (cft.equals(DataFileColumnFamily.NAME) || cft.equals(ScanFileColumnFamily.NAME)) {\r\n        String cq = key.getColumnQualifier().toString();\r\n\r\n        String reference = cq;\r\n        if (cq.startsWith(""/"")) {\r\n```', 'commenter': 'keith-turner'}, {'comment': ""This is a good change, I'll implement"", 'commenter': 'mjwall'}]"
2293,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -217,11 +222,30 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
         throw new RuntimeException(
             ""Scanner over metadata table returned unexpected column : "" + entry.getKey());
     }
+    Set<String> tableIdsAfter = gce.getTableIDs();
+    ensureAllTablesChecked(tableIdsBefore, tableIdsSeen, tableIdsAfter);
 
     confirmDeletesFromReplication(gce.getReplicationNeededIterator(),
         candidateMap.entrySet().iterator());
   }
 
+  @VisibleForTesting
+  protected void ensureAllTablesChecked(Set<String> tableIdsBefore, Set<String> tableIdsSeen,","[{'comment': 'This is protected and marked vis for testing, but does not seem to be used in a test.  Is there a test change that was not checked in?', 'commenter': 'keith-turner'}, {'comment': 'Right, I need to add tests.', 'commenter': 'mjwall'}]"
2293,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -217,11 +222,30 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
         throw new RuntimeException(
             ""Scanner over metadata table returned unexpected column : "" + entry.getKey());
     }
+    Set<String> tableIdsAfter = gce.getTableIDs();
+    ensureAllTablesChecked(tableIdsBefore, tableIdsSeen, tableIdsAfter);
 
     confirmDeletesFromReplication(gce.getReplicationNeededIterator(),
         candidateMap.entrySet().iterator());
   }
 
+  @VisibleForTesting
+  protected void ensureAllTablesChecked(Set<String> tableIdsBefore, Set<String> tableIdsSeen,
+      Set<String> tableIdsAfter) {
+
+    // if a table was added or deleted during this run, it is acceptable to not
+    // have seen those tables ids when scanning the metadata table. So get the intersection
+    Set<String> tableIdsMustHaveSeen = new HashSet<>(tableIdsBefore);
+    tableIdsMustHaveSeen.retainAll(tableIdsAfter);
+    // From that intersection, remove all the table ids that were seen.
+    tableIdsMustHaveSeen.removeAll(tableIdsSeen);
+    // If anything is left then we missed a table and may not have removed rfiles still in use
+    if (tableIdsMustHaveSeen.size() > 0) {","[{'comment': '```suggestion\r\n    if (!tableIdsMustHaveSeen.isEmpty()) {\r\n```', 'commenter': 'keith-turner'}, {'comment': '`removeAll` returns a boolean too...', 'commenter': 'dlmarion'}]"
2293,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -217,11 +222,30 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
         throw new RuntimeException(
             ""Scanner over metadata table returned unexpected column : "" + entry.getKey());
     }
+    Set<String> tableIdsAfter = gce.getTableIDs();
+    ensureAllTablesChecked(tableIdsBefore, tableIdsSeen, tableIdsAfter);
 
     confirmDeletesFromReplication(gce.getReplicationNeededIterator(),
         candidateMap.entrySet().iterator());
   }
 
+  @VisibleForTesting
+  protected void ensureAllTablesChecked(Set<String> tableIdsBefore, Set<String> tableIdsSeen,
+      Set<String> tableIdsAfter) {
+
+    // if a table was added or deleted during this run, it is acceptable to not
+    // have seen those tables ids when scanning the metadata table. So get the intersection
+    Set<String> tableIdsMustHaveSeen = new HashSet<>(tableIdsBefore);
+    tableIdsMustHaveSeen.retainAll(tableIdsAfter);","[{'comment': 'The following is a suggestion based on discussion we had at the top level of the PR.  This may be another useful sanity check, but I have not fully convinced myself it will not cause troublesome false positives. \r\n\r\n```suggestion\r\n    tableIdsMustHaveSeen.retainAll(tableIdsAfter);\r\n \r\n    if(tableIdsMustHaveSeen.isEmpty() && !tableIdsSeen.isEmpty()) {\r\n    \t// we saw no table ids in ZK but did in the metadata table.  This is unexpected.\r\n    \tthrow new RuntimeException(""Saw no table ids in ZK but did see table ids in metadata table: "" + tableIdsSeen);\r\n    }\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Will apply this change, thanks', 'commenter': 'mjwall'}, {'comment': '`retainAll` returns a boolean to signify that the collection changed. You could add it to the if statement.\r\n\r\n```suggestion\r\n    if (tableIdsMustHaveSeen.retainAll(tableIdsAfter) && tableIdsMustHaveSeen.isEmpty() && !tableIdsSeen.isEmpty()) {\r\n```', 'commenter': 'dlmarion'}, {'comment': 'When `tableIdsMustHaveSeen` is empty it seems like `retainAll()` would always return false. We want to check that it is empty, so adding it to the if may mean `tableIdsMustHaveSeen.retainAll(tableIdsAfter) && tableIdsMustHaveSeen.isEmpty()` can never be true.', 'commenter': 'keith-turner'}, {'comment': '> The following is a suggestion based on discussion we had at the top level of the PR. This may be another useful sanity check, but I have not fully convinced myself it will not cause troublesome false positives.\r\n\r\nAdding this check but it breaks existing test lol.  Will fix those', 'commenter': 'mjwall'}]"
2293,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -217,11 +222,30 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
         throw new RuntimeException(
             ""Scanner over metadata table returned unexpected column : "" + entry.getKey());
     }
+    Set<String> tableIdsAfter = gce.getTableIDs();
+    ensureAllTablesChecked(tableIdsBefore, tableIdsSeen, tableIdsAfter);
 
     confirmDeletesFromReplication(gce.getReplicationNeededIterator(),
         candidateMap.entrySet().iterator());
   }
 
+  @VisibleForTesting
+  protected void ensureAllTablesChecked(Set<String> tableIdsBefore, Set<String> tableIdsSeen,
+      Set<String> tableIdsAfter) {
+
+    // if a table was added or deleted during this run, it is acceptable to not
+    // have seen those tables ids when scanning the metadata table. So get the intersection
+    Set<String> tableIdsMustHaveSeen = new HashSet<>(tableIdsBefore);
+    tableIdsMustHaveSeen.retainAll(tableIdsAfter);
+    // From that intersection, remove all the table ids that were seen.
+    tableIdsMustHaveSeen.removeAll(tableIdsSeen);
+    // If anything is left then we missed a table and may not have removed rfiles still in use
+    if (tableIdsMustHaveSeen.size() > 0) {
+      throw new RuntimeException(""Missed checking tables: "" + tableIdsMustHaveSeen);","[{'comment': '```suggestion\r\n      throw new RuntimeException(""Saw table IDs in ZK that were not in metadata table: "" + tableIdsMustHaveSeen);\r\n```', 'commenter': 'keith-turner'}]"
2293,server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectionTest.java,"@@ -644,4 +699,132 @@ public void bulkImportReplicationRecordsPreventDeletion() throws Exception {
     assertEquals(1, gce.deletes.size());
     assertEquals(""hdfs://foo.com:6000/accumulo/tables/2/t-00002/A000002.rf"", gce.deletes.get(0));
   }
+
+  @Test
+  public void testMissingTableIds() throws Exception {
+    GarbageCollectionAlgorithm gca = new GarbageCollectionAlgorithm();
+
+    TestGCE gce = new TestGCE();
+
+    gce.candidates.add(""hdfs://foo.com:6000/user/foo/tables/a/t-0/F00.rf"");
+
+    gce.addFileReference(""a"", null, ""hdfs://foo.com:6000/user/foo/tables/a/t-0/F00.rf"");
+    gce.addFileReference(""c"", null, ""hdfs://foo.com:6000/user/foo/tables/c/t-0/F00.rf"");
+
+    // the following table ids must be seen in the references
+    gce.tableIds.add(""a"");
+    gce.tableIds.add(""b"");
+    gce.tableIds.add(""c"");
+    gce.tableIds.add(""d"");
+
+    String msg = Assert.assertThrows(RuntimeException.class, () -> gca.collect(gce)).getMessage();
+    Assert.assertTrue(msg, (msg.contains(""[b, d]"") || msg.contains(""[d, b]""))
+        && msg.contains(""Saw table IDs in ZK that were not in metadata table:""));
+  }
+
+  // below are tests for potential failure conditions of the GC process. Some of these cases were
+  // observed on clusters. Some were hypothesis based on observations. The result was that
+  // candidate entries were not removed when they should have been and therefore files were
+  // removed from HDFS that were actually still in use
+
+  private Set<String> makeUnmodifiableSet(String... args) {
+    return Collections.unmodifiableSet(new HashSet<>(Arrays.asList(args)));
+  }
+
+  @Test
+  public void testNormalGCRun() {
+    // happy path, no tables added or removed during this portion and all the tables checked
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableAddedInMiddle() {
+    // table was added during this portion and we don't see it, should be fine
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");","[{'comment': '```suggestion\r\n    Set<String> tablesBefore = Set.of(""1"", ""2"", ""3"");\r\n```\r\nWould this work instead? If so you could do away with `makeUnmodifiableSet()`. Set.of() creates an immutable set.', 'commenter': 'DomGarguilo'}, {'comment': 'It would work for Java 1.9 and above.  I know I have to backport this to 1.9 which is still 1.8.  I can make the change here to take advantage of the new API and then use my hacky method when I backport.  Thanks @DomGarguilo ', 'commenter': 'mjwall'}, {'comment': ""Actually that would require changing the pom to Java 9 compatibility.  I'll try to remember to use Set.of when I forward port this to main."", 'commenter': 'mjwall'}]"
2293,server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectionTest.java,"@@ -644,4 +699,132 @@ public void bulkImportReplicationRecordsPreventDeletion() throws Exception {
     assertEquals(1, gce.deletes.size());
     assertEquals(""hdfs://foo.com:6000/accumulo/tables/2/t-00002/A000002.rf"", gce.deletes.get(0));
   }
+
+  @Test
+  public void testMissingTableIds() throws Exception {
+    GarbageCollectionAlgorithm gca = new GarbageCollectionAlgorithm();
+
+    TestGCE gce = new TestGCE();
+
+    gce.candidates.add(""hdfs://foo.com:6000/user/foo/tables/a/t-0/F00.rf"");
+
+    gce.addFileReference(""a"", null, ""hdfs://foo.com:6000/user/foo/tables/a/t-0/F00.rf"");
+    gce.addFileReference(""c"", null, ""hdfs://foo.com:6000/user/foo/tables/c/t-0/F00.rf"");
+
+    // the following table ids must be seen in the references
+    gce.tableIds.add(""a"");
+    gce.tableIds.add(""b"");
+    gce.tableIds.add(""c"");
+    gce.tableIds.add(""d"");
+
+    String msg = Assert.assertThrows(RuntimeException.class, () -> gca.collect(gce)).getMessage();
+    Assert.assertTrue(msg, (msg.contains(""[b, d]"") || msg.contains(""[d, b]""))
+        && msg.contains(""Saw table IDs in ZK that were not in metadata table:""));
+  }
+
+  // below are tests for potential failure conditions of the GC process. Some of these cases were
+  // observed on clusters. Some were hypothesis based on observations. The result was that
+  // candidate entries were not removed when they should have been and therefore files were
+  // removed from HDFS that were actually still in use
+
+  private Set<String> makeUnmodifiableSet(String... args) {
+    return Collections.unmodifiableSet(new HashSet<>(Arrays.asList(args)));
+  }
+
+  @Test
+  public void testNormalGCRun() {
+    // happy path, no tables added or removed during this portion and all the tables checked
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableAddedInMiddle() {
+    // table was added during this portion and we don't see it, should be fine
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableAddedInMiddleTwo() {
+    // table was added during this portion and we DO see it
+    // Means table was added after candidates were grabbed, so there should be nothing to remove
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableDeletedInMiddle() {
+    // table was deleted during this portion and we don't see it
+    // this mean any candidates from the deleted table wil stay on the candidate list
+    // and during the delete step they will try to removed
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableDeletedInMiddleTwo() {
+    // table was deleted during this portion and we DO see it
+    // this mean candidates from the deleted table may get removed from the candidate list
+    // which should be ok, as the delete table function should be responsible for removing those
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""4"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testMissEntireTable() {
+    // this test simulates missing an entire table when looking for what files are in use
+    // if you add custom splits to the metadata at able boundaries, this can happen with a failed
+    // scan
+    // recall the ~tab:~pr for this first entry of a new table is empty, so there is now way to
+    // check the prior row. If you split a couple of tables in the metadata the table boundary
+    // , say table ids 2,3,4, and then miss scanning table 3 but get 4, it is possible other
+    // consistency checks will miss this
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""1"", ""2"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+
+    try {
+      new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen,
+          tablesAfter);
+      fail(""should have failed"");
+    } catch (RuntimeException e) {","[{'comment': 'Can this be replaced with assertThrows from JUnit?', 'commenter': 'DomGarguilo'}]"
2293,server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectionTest.java,"@@ -644,4 +699,132 @@ public void bulkImportReplicationRecordsPreventDeletion() throws Exception {
     assertEquals(1, gce.deletes.size());
     assertEquals(""hdfs://foo.com:6000/accumulo/tables/2/t-00002/A000002.rf"", gce.deletes.get(0));
   }
+
+  @Test
+  public void testMissingTableIds() throws Exception {
+    GarbageCollectionAlgorithm gca = new GarbageCollectionAlgorithm();
+
+    TestGCE gce = new TestGCE();
+
+    gce.candidates.add(""hdfs://foo.com:6000/user/foo/tables/a/t-0/F00.rf"");
+
+    gce.addFileReference(""a"", null, ""hdfs://foo.com:6000/user/foo/tables/a/t-0/F00.rf"");
+    gce.addFileReference(""c"", null, ""hdfs://foo.com:6000/user/foo/tables/c/t-0/F00.rf"");
+
+    // the following table ids must be seen in the references
+    gce.tableIds.add(""a"");
+    gce.tableIds.add(""b"");
+    gce.tableIds.add(""c"");
+    gce.tableIds.add(""d"");
+
+    String msg = Assert.assertThrows(RuntimeException.class, () -> gca.collect(gce)).getMessage();
+    Assert.assertTrue(msg, (msg.contains(""[b, d]"") || msg.contains(""[d, b]""))
+        && msg.contains(""Saw table IDs in ZK that were not in metadata table:""));
+  }
+
+  // below are tests for potential failure conditions of the GC process. Some of these cases were
+  // observed on clusters. Some were hypothesis based on observations. The result was that
+  // candidate entries were not removed when they should have been and therefore files were
+  // removed from HDFS that were actually still in use
+
+  private Set<String> makeUnmodifiableSet(String... args) {
+    return Collections.unmodifiableSet(new HashSet<>(Arrays.asList(args)));
+  }
+
+  @Test
+  public void testNormalGCRun() {
+    // happy path, no tables added or removed during this portion and all the tables checked
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableAddedInMiddle() {
+    // table was added during this portion and we don't see it, should be fine
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableAddedInMiddleTwo() {
+    // table was added during this portion and we DO see it
+    // Means table was added after candidates were grabbed, so there should be nothing to remove
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableDeletedInMiddle() {
+    // table was deleted during this portion and we don't see it
+    // this mean any candidates from the deleted table wil stay on the candidate list
+    // and during the delete step they will try to removed
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableDeletedInMiddleTwo() {
+    // table was deleted during this portion and we DO see it
+    // this mean candidates from the deleted table may get removed from the candidate list
+    // which should be ok, as the delete table function should be responsible for removing those
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""4"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testMissEntireTable() {
+    // this test simulates missing an entire table when looking for what files are in use
+    // if you add custom splits to the metadata at able boundaries, this can happen with a failed
+    // scan
+    // recall the ~tab:~pr for this first entry of a new table is empty, so there is now way to
+    // check the prior row. If you split a couple of tables in the metadata the table boundary
+    // , say table ids 2,3,4, and then miss scanning table 3 but get 4, it is possible other
+    // consistency checks will miss this
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""1"", ""2"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+
+    try {
+      new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen,
+          tablesAfter);
+      fail(""should have failed"");
+    } catch (RuntimeException e) {
+      assertTrue(e.getMessage().startsWith(""Saw table IDs in ZK that were not in metadata table:""));
+    }
+
+  }
+
+  @Test
+  public void testZKHadNoTables() {
+    // this test simulates getting nothing from ZK for table ids, which should not happen, but just
+    // in case
+    Set<String> tablesBefore = makeUnmodifiableSet();
+    Set<String> tablesSeen = makeUnmodifiableSet(""1"", ""2"");
+    Set<String> tablesAfter = makeUnmodifiableSet();
+
+    try {
+      new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen,
+          tablesAfter);
+      fail(""should have failed"");
+    } catch (RuntimeException e) {","[{'comment': 'Can this be replaced with assertThrows from JUnit?', 'commenter': 'DomGarguilo'}]"
2293,server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectionTest.java,"@@ -130,7 +136,31 @@ public Value addDirReference(String tableId, String endRow, String dir) {
     }
 
     public Value removeDirReference(String tableId, String endRow) {
-      return references.remove(newDirReferenceKey(tableId, endRow));
+      Value retVal = references.remove(newDirReferenceKey(tableId, endRow));
+      removeDanglingTableIds(tableId);
+      return retVal;
+    }
+
+    /*
+     * this is to be called from removeDirReference or removeFileReference.
+     *
+     * If you just removed the last reference to a table, we need to remove it from
+     * the tableIds in zookeeper
+     */
+    private void removeDanglingTableIds(String tableId) {
+      for(Entry<Key, Value> entry : references.entrySet()) {
+        // check table id from row
+        // TODO: check table ids referenced in dir in || clause
+        if (entry.getKey().getRow().toString().equals(tableId)) {","[{'comment': 'The row contains more than just the tableID,  Seems like this should do something like `String tableID = new String(KeyExtent.tableOfMetadataRow(key.getRow()));` to get the id. Copied that from elsewhere in this PR.\r\n\r\nNot completely sure I understand the intent of this method.  I think its meant to remove a tableId if there are no refs to it, I am not sure it does that though.  Could possibly do the the following if that is the intent.\r\n\r\n```java\r\n  bool inUse = references.keySet().stream().map(k -> new String(KeyExtent.tableOfMetadataRow(key.getRow()))).anyMatch(tid -> tableId.equals(tid));\r\n  if(!inUse) {\r\n     assertTrue(tableIds.remove(tableId));\r\n  }\r\n```\r\n', 'commenter': 'keith-turner'}, {'comment': 'the intent is to remove tableids from the list of tablesIDs in the TestGCE because existing tests are now breaking with the added checks.  going to rework this, will look at your method.\r\n', 'commenter': 'mjwall'}]"
2293,server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectionTest.java,"@@ -644,4 +692,132 @@ public void bulkImportReplicationRecordsPreventDeletion() throws Exception {
     assertEquals(1, gce.deletes.size());
     assertEquals(""hdfs://foo.com:6000/accumulo/tables/2/t-00002/A000002.rf"", gce.deletes.get(0));
   }
+
+  @Test
+  public void testMissingTableIds() throws Exception {
+    GarbageCollectionAlgorithm gca = new GarbageCollectionAlgorithm();
+
+    TestGCE gce = new TestGCE();
+
+    gce.candidates.add(""hdfs://foo.com:6000/user/foo/tables/a/t-0/F00.rf"");
+
+    gce.addFileReference(""a"", null, ""hdfs://foo.com:6000/user/foo/tables/a/t-0/F00.rf"");
+    gce.addFileReference(""c"", null, ""hdfs://foo.com:6000/user/foo/tables/c/t-0/F00.rf"");
+
+    // the following table ids must be seen in the references
+    gce.tableIds.add(""a"");
+    gce.tableIds.add(""b"");
+    gce.tableIds.add(""c"");
+    gce.tableIds.add(""d"");
+
+    String msg = assertThrows(RuntimeException.class, () -> gca.collect(gce)).getMessage();
+    assertTrue(msg, (msg.contains(""[b, d]"") || msg.contains(""[d, b]""))
+        && msg.contains(""Saw table IDs in ZK that were not in metadata table:""));
+  }
+
+  // below are tests for potential failure conditions of the GC process. Some of these cases were
+  // observed on clusters. Some were hypothesis based on observations. The result was that
+  // candidate entries were not removed when they should have been and therefore files were
+  // removed from HDFS that were actually still in use
+
+  private Set<String> makeUnmodifiableSet(String... args) {
+    return Collections.unmodifiableSet(new HashSet<>(Arrays.asList(args)));
+  }
+
+  @Test
+  public void testNormalGCRun() {
+    // happy path, no tables added or removed during this portion and all the tables checked
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableAddedInMiddle() {
+    // table was added during this portion and we don't see it, should be fine
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableAddedInMiddleTwo() {
+    // table was added during this portion and we DO see it
+    // Means table was added after candidates were grabbed, so there should be nothing to remove
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""3"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""3"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableDeletedInMiddle() {
+    // table was deleted during this portion and we don't see it
+    // this mean any candidates from the deleted table wil stay on the candidate list
+    // and during the delete step they will try to removed
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testTableDeletedInMiddleTwo() {
+    // table was deleted during this portion and we DO see it
+    // this mean candidates from the deleted table may get removed from the candidate list
+    // which should be ok, as the delete table function should be responsible for removing those
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""2"", ""1"", ""4"", ""3"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""4"");
+
+    new GarbageCollectionAlgorithm().ensureAllTablesChecked(tablesBefore, tablesSeen, tablesAfter);
+  }
+
+  @Test
+  public void testMissEntireTable() {
+    // this test simulates missing an entire table when looking for what files are in use
+    // if you add custom splits to the metadata at able boundaries, this can happen with a failed
+    // scan
+    // recall the ~tab:~pr for this first entry of a new table is empty, so there is now way to
+    // check the prior row. If you split a couple of tables in the metadata the table boundary
+    // , say table ids 2,3,4, and then miss scanning table 3 but get 4, it is possible other
+    // consistency checks will miss this
+    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+    Set<String> tablesSeen = makeUnmodifiableSet(""1"", ""2"", ""4"");
+    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");
+","[{'comment': 'In addition to this test method it would be nice to have a few more test methods w/ variations for this scenario.  \r\n\r\nLike the following is a table that is missing and a table being added.\r\n\r\n```\r\n    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"");\r\n    Set<String> tablesSeen = makeUnmodifiableSet(""1"", ""2"", ""4"");\r\n    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""3"", ""4"");\r\n```\r\n\r\nCould also have a table missing and a table being removed\r\n\r\n```\r\n    Set<String> tablesBefore = makeUnmodifiableSet(""1"", ""2"", ""3"",""4"");\r\n    Set<String> tablesSeen = makeUnmodifiableSet(""1"", ""2"", ""4"");\r\n    Set<String> tablesAfter = makeUnmodifiableSet(""1"", ""2"", ""3"");\r\n```', 'commenter': 'keith-turner'}, {'comment': 'adding these tests, thanks @keith-turner ', 'commenter': 'mjwall'}]"
2293,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -217,11 +221,43 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
         throw new RuntimeException(
             ""Scanner over metadata table returned unexpected column : "" + entry.getKey());
     }
+    Set<String> tableIdsAfter = gce.getTableIDs();
+    ensureAllTablesChecked(Collections.unmodifiableSet(tableIdsBefore),
+        Collections.unmodifiableSet(tableIdsSeen), Collections.unmodifiableSet(tableIdsAfter));
 
     confirmDeletesFromReplication(gce.getReplicationNeededIterator(),
         candidateMap.entrySet().iterator());
   }
 
+  @VisibleForTesting
+  /**
+   *
+   */
+  protected void ensureAllTablesChecked(Set<String> tableIdsBefore, Set<String> tableIdsSeen,
+      Set<String> tableIdsAfter) {
+
+    // if a table was added or deleted during this run, it is acceptable to not
+    // have seen those tables ids when scanning the metadata table. So get the intersection
+    Set<String> tableIdsMustHaveSeen = new HashSet<>(tableIdsBefore);
+    tableIdsMustHaveSeen.retainAll(tableIdsAfter);
+
+    if (tableIdsMustHaveSeen.isEmpty() && !tableIdsSeen.isEmpty()) {
+      // we saw no table ids in ZK but did in the metadata table. This is unexpected.
+      throw new RuntimeException(
+          ""Saw no table ids in ZK but did see table ids in metadata table: "" + tableIdsSeen);
+    }
+
+    // From that intersection, remove all the table ids that were seen.
+    tableIdsMustHaveSeen.removeAll(tableIdsSeen);
+
+    // If anything is left then we missed a table and may not have removed rfiles still in use","[{'comment': 'and may not have removed -> and might remove', 'commenter': 'ivakegg'}, {'comment': 'should say ""may not have removed rfile references from the list of candidates that are actually still in use"", will update', 'commenter': 'mjwall'}]"
2293,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -217,11 +221,43 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
         throw new RuntimeException(
             ""Scanner over metadata table returned unexpected column : "" + entry.getKey());
     }
+    Set<String> tableIdsAfter = gce.getTableIDs();
+    ensureAllTablesChecked(Collections.unmodifiableSet(tableIdsBefore),
+        Collections.unmodifiableSet(tableIdsSeen), Collections.unmodifiableSet(tableIdsAfter));
 
     confirmDeletesFromReplication(gce.getReplicationNeededIterator(),
         candidateMap.entrySet().iterator());
   }
 
+  @VisibleForTesting
+  /**
+   *
+   */
+  protected void ensureAllTablesChecked(Set<String> tableIdsBefore, Set<String> tableIdsSeen,
+      Set<String> tableIdsAfter) {
+
+    // if a table was added or deleted during this run, it is acceptable to not
+    // have seen those tables ids when scanning the metadata table. So get the intersection
+    Set<String> tableIdsMustHaveSeen = new HashSet<>(tableIdsBefore);
+    tableIdsMustHaveSeen.retainAll(tableIdsAfter);
+
+    if (tableIdsMustHaveSeen.isEmpty() && !tableIdsSeen.isEmpty()) {
+      // we saw no table ids in ZK but did in the metadata table. This is unexpected.","[{'comment': 'if all tables are deleted since the scan, then could this not be a valid situation?', 'commenter': 'ivakegg'}, {'comment': '> if all tables are deleted since the scan, then could this not be a valid situation?\r\n\r\nThat could happen.  This is a situation that could be caused by :\r\n\r\n 1. Tables being deleted and added concurrently w/ this code (going to or from zero tables).\r\n 2. Something going wrong w/ ZK where it temporarily returns no tables ids, even though there are tables ids.\r\n\r\nThis check is an attempt to defends against case 2 w/ the thinking that its ok to ignore case 1 temporarily and let subsequent GC runs take care of things.  However I suppose the error message for the case 1 race condition could be lead to false concerns.', 'commenter': 'keith-turner'}]"
2293,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -217,11 +221,43 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
         throw new RuntimeException(
             ""Scanner over metadata table returned unexpected column : "" + entry.getKey());
     }
+    Set<String> tableIdsAfter = gce.getTableIDs();
+    ensureAllTablesChecked(Collections.unmodifiableSet(tableIdsBefore),
+        Collections.unmodifiableSet(tableIdsSeen), Collections.unmodifiableSet(tableIdsAfter));
 
     confirmDeletesFromReplication(gce.getReplicationNeededIterator(),
         candidateMap.entrySet().iterator());
   }
 
+  @VisibleForTesting
+  /**
+   *
+   */
+  protected void ensureAllTablesChecked(Set<String> tableIdsBefore, Set<String> tableIdsSeen,
+      Set<String> tableIdsAfter) {
+
+    // if a table was added or deleted during this run, it is acceptable to not
+    // have seen those tables ids when scanning the metadata table. So get the intersection
+    Set<String> tableIdsMustHaveSeen = new HashSet<>(tableIdsBefore);
+    tableIdsMustHaveSeen.retainAll(tableIdsAfter);
+
+    if (tableIdsMustHaveSeen.isEmpty() && !tableIdsSeen.isEmpty()) {
+      // we saw no table ids in ZK but did in the metadata table. This is unexpected.
+      throw new RuntimeException(
+          ""Saw no table ids in ZK but did see table ids in metadata table: "" + tableIdsSeen);
+    }
+
+    // From that intersection, remove all the table ids that were seen.
+    tableIdsMustHaveSeen.removeAll(tableIdsSeen);","[{'comment': 'What I think we want is that tableIdsMustHaveSeem is equivalent to tableIdsSeen.  This just tests if tableIdsSeen is a superset of tableIdsMustHaveSeen.', 'commenter': 'ivakegg'}, {'comment': '> What I think we want is that tableIdsMustHaveSeem is equivalent to tableIdsSeen.\r\n\r\nThis would mean GC would not run if tables were added or delete during the GC scan process.  If we did this it would make the check much stronger, however it could lead to starvation of GC where it never runs if tables are constantly added and deleted.', 'commenter': 'keith-turner'}, {'comment': ""Maybe in 2.x we need a configurable mode for the GC to handle both use cases. One mode could be what @ivakegg desires, where the GC is more risk adverse on a cluster that doesn't delete tables very often but is very active with splitting and removing data. The second mode could be more aggressive garbage collection for a cluster that is constantly adding and deleting tables."", 'commenter': 'milleruntime'}, {'comment': '> Maybe in 2.x we need a configurable mode for the GC to handle both use cases.\r\n\r\nWe could also explore reducing/removing the ambiguity when trying to discern between tables being added/removed and some kind of silent error when reading from ZK.  For example maybe for deleting tables we could make the GC remove table ids (with a certain table state like DELETED) from ZK instead of the manager.  This would allow the GC to positively identify a table that was deleted while it was scanning the metadata table and know that its ok to see or not see that table in the metadata table.   ', 'commenter': 'keith-turner'}]"
2293,server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectionTest.java,"@@ -532,11 +557,13 @@ public void test() throws Exception {
     assertRemoved(gce);
 
     gce = new TestGCE();
+    // adding this causes a failure, what do we need to do with blips?
+    // gce.tableIds.add(""1636"");","[{'comment': '@mjwall the only other thing I noticed was this, but have not dug into it.  Wanted to understand why this needed to be commented out a bit better.', 'commenter': 'keith-turner'}]"
2303,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -915,4 +919,12 @@ public void setProperty(ClientProperty property, Integer value) {
       setProperty(property, Integer.toString(value));
     }
   }
+
+  public ThriftTransportPool getTransportPool() {","[{'comment': ""We should probably make this synchronized, so a second thread can't come in, create a new thread pool, overwrite the reference, and leave the previous thread pool running in the background eating up resources."", 'commenter': 'ctubbsii'}]"
2303,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java,"@@ -773,8 +773,9 @@ public int stopProcessWithTimeout(final Process proc, long timeout, TimeUnit uni
   public ManagerMonitorInfo getManagerMonitorInfo()
       throws AccumuloException, AccumuloSecurityException {
     ManagerClientService.Iface client = null;
+    AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build();
     while (true) {
-      try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
+      try {","[{'comment': ""Why not just put the try-with-resources outside the loop, so you don't have to explicitly remember to close() in a finally block?"", 'commenter': 'ctubbsii'}, {'comment': ""I don't think that will work. The loop needs to be around the exceptions so it can retry in the event of a connection issue."", 'commenter': 'milleruntime'}, {'comment': 'This seems like one place the `RetryableThriftFunction` for external compactions @dlmarion added would be useful.', 'commenter': 'milleruntime'}, {'comment': 'It works with an additional try-with-resources, which I guess is better then the explicit close.', 'commenter': 'milleruntime'}]"
2303,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -915,4 +919,12 @@ public void setProperty(ClientProperty property, Integer value) {
       setProperty(property, Integer.toString(value));
     }
   }
+
+  public synchronized ThriftTransportPool getTransportPool() {
+    if (thriftTransportPool == null) {","[{'comment': '```suggestion\r\n   ensureOpen();\r\n    if (thriftTransportPool == null) {\r\n```', 'commenter': 'keith-turner'}]"
2303,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java,"@@ -787,8 +788,9 @@ public ManagerMonitorInfo getManagerMonitorInfo()
         throw new AccumuloException(exception);
       } finally {
         if (client != null) {
-          ManagerClient.close(client);
+          ManagerClient.close(client, (ClientContext) c);
         }
+        c.close();","[{'comment': 'Is this close inside the loop?  If so, does another client need to be opened for the next loop iteration?  Or maybe have pattern like the following :\r\n\r\n```java\r\ntry(AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()){\r\n  while(true){\r\n      try {...} ...\r\n   }\r\n}\r\n```', 'commenter': 'keith-turner'}]"
2321,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1205,8 +1212,9 @@ boolean canSuspendTablets() {
     // checking stored user hashes if any of them uses an outdated algorithm
     security.validateStoredUserCreditentials();
 
-    // The manager is fully initialized. Clients are allowed to connect now.
+    // The manager is fully initialized and upgraded. Clients are allowed to connect now.
     managerInitialized.set(true);
+    managerUpgrading.set(false);","[{'comment': 'Maybe set `managerUpgrading` to false first? I think we are saying here that the upgrade is part of the initialize process so I take that as the upgrade is completed and then it is initialized.', 'commenter': 'milleruntime'}, {'comment': 'Good point. ', 'commenter': 'Manno15'}]"
2321,server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceInvocationHandler.java,"@@ -53,7 +53,7 @@ public Object invoke(Object proxy, Method method, Object[] args) throws Throwabl
         LOG.trace(""Denying access to RPC service as this instance is not the active instance."");
         throw new ThriftNotActiveServiceException();
       }
-      // What to do when upgrading?
+      LOG.warn(""Cannot access service as it is in the process of upgrading."");","[{'comment': '```suggestion\r\n      LOG.warn(""Cannot access service while it is upgrading."");\r\n```', 'commenter': 'ctubbsii'}]"
2321,server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceInvocationHandler.java,"@@ -49,8 +49,12 @@ public HighlyAvailableServiceInvocationHandler(I instance, HighlyAvailableServic
   public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
     // If the service is not active, throw an exception
     if (!service.isActiveService()) {
-      LOG.trace(""Denying access to RPC service as this instance is not the active instance."");
-      throw new ThriftNotActiveServiceException();
+      if (!service.isUpgrading()) {
+        LOG.trace(""Denying access to RPC service as this instance is not the active instance."");
+        throw new ThriftNotActiveServiceException();
+      }
+      LOG.warn(""Cannot access service while it is upgrading."");
+      return null;","[{'comment': ""This puts the warning about upgrading in the body of the if block where it is *not* the active service. Isn't the main problem when it *is* the active service?"", 'commenter': 'ctubbsii'}, {'comment': ""The issue is when the Manager is not an active service (initialization hasn't been completed) because it is still in the process of upgrading. Not due to an error of some kind. The isActiveService only checks for a boolean inside manager that gets set to true when it is determined that it has completed initialization. "", 'commenter': 'Manno15'}, {'comment': 'Hmm, are you sure this isn\'t when it is marked as ""active"". ""Active"" here simply means it holds the number one position in the ZooKeeper lock queue, not that it is ""actively"" serving RPC clients.', 'commenter': 'ctubbsii'}, {'comment': 'Based on the function inside manager and the comments above it along with my testing, yes. ', 'commenter': 'Manno15'}, {'comment': 'Okay, I think I understand now. Let me look back over this again.', 'commenter': 'ctubbsii'}]"
2321,server/base/src/main/java/org/apache/accumulo/server/HighlyAvailableService.java,"@@ -31,4 +31,11 @@
    */
   boolean isActiveService();
 
+  /**
+   * Is this service instance is currently in the process of upgrading","[{'comment': '```suggestion\r\n   * Is this service instance currently in the process of upgrading?\r\n```', 'commenter': 'DomGarguilo'}, {'comment': 'Fixed in https://github.com/apache/accumulo/pull/2321/commits/1df9f2d5c79e171594b0fc785f7f5571857856b3', 'commenter': 'Manno15'}]"
2321,server/base/src/main/java/org/apache/accumulo/server/HighlyAvailableService.java,"@@ -31,4 +31,11 @@
    */
   boolean isActiveService();
 
+  /**
+   * Is this service instance is currently in the process of upgrading
+   *
+   * @return True if the service is the service is upgrading, false otherwise.","[{'comment': '```suggestion\r\n   * @return True if the service is upgrading, false otherwise.\r\n```', 'commenter': 'DomGarguilo'}, {'comment': 'Fixed in https://github.com/apache/accumulo/pull/2321/commits/1df9f2d5c79e171594b0fc785f7f5571857856b3', 'commenter': 'Manno15'}]"
2321,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1714,6 +1723,11 @@ public boolean isActiveService() {
     return managerInitialized.get();
   }
 
+  @Override
+  public boolean isUpgrading() {
+    return managerUpgrading.get();
+  }","[{'comment': 'I think you could simplify this:\r\n\r\n```suggestion\r\n    if (!managerInitialized.get()) {\r\n      return false;\r\n    }\r\n    if (managerUpgrading.get()) {\r\n      log.warn(""Cannot access service while it is upgrading."");\r\n      return false;\r\n    }\r\n    return true;\r\n  }\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'I like the idea. I will test it out locally.', 'commenter': 'Manno15'}, {'comment': 'This will still have the exception be thrown inside the invocation handler. One of the original goals was to avoid throwing that exception if the issue was merely due to the upgrade since it will retry regardless. ', 'commenter': 'Manno15'}, {'comment': ""Is the exception a problem? At the very least, it seems safer than returning `null` in that case. It's not clear what the full implications are for returning `null`, because doing that makes it appear as though the method that is being proxied actually got executed and just returned `null`. I think the exception is fine, so long as there's an explanation for it, and the log message adds that explanation. But, if there's a problem with the exception... then maybe we can come up with a different solution."", 'commenter': 'ctubbsii'}, {'comment': ""There isn't necessarily a problem with the exception but it can be misleading, especially at first glance. Even with the additional warning, it is not clear that they are connected in any way. I do agree that returning `null` isn't the best solution which is part of the reason I wanted a review to get more ideas on what the ideal solution would be. "", 'commenter': 'Manno15'}, {'comment': ""You could throw a more specific exception for the upgrade case, so there's not a redundant exception logged, doesn't return null, and has a more informative message."", 'commenter': 'ctubbsii'}, {'comment': 'That is something I am looking into now. ', 'commenter': 'Manno15'}]"
2321,server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceInvocationHandler.java,"@@ -47,10 +47,17 @@ public HighlyAvailableServiceInvocationHandler(I instance, HighlyAvailableServic
 
   @Override
   public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
+
+    // If the service is upgrading, throw an exception
+    if (service.isUpgrading()) {
+      throw new ThriftNotActiveServiceException(service.toString(),
+          ""Service can not be accessed while it is upgrading"");
+    }
+
     // If the service is not active, throw an exception
     if (!service.isActiveService()) {
-      LOG.trace(""Denying access to RPC service as this instance is not the active instance."");
-      throw new ThriftNotActiveServiceException();
+      throw new ThriftNotActiveServiceException(service.toString(),
+          ""Denying access to RPC service as this instance is not the active instance"");
     }","[{'comment': ""Throwing the thrift exception back over the wire will make sure the calling RPC client gets the message. However, it would still be useful to log these messages at trace in this server's own log."", 'commenter': 'ctubbsii'}, {'comment': 'Done in https://github.com/apache/accumulo/pull/2321/commits/80aae5a075bf00a6ba67ef68cd3640140ed018a3', 'commenter': 'Manno15'}]"
2321,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1042,6 +1043,12 @@ public void run() {
       throw new IllegalStateException(""Exception getting manager lock"", e);
     }
 
+    // If UpgradeStatus is not at complete by this moment, then things are currently
+    // upgrading.
+    if (!upgradeCoordinator.getStatus().equals(UpgradeCoordinator.UpgradeStatus.COMPLETE)) {","[{'comment': 'Avoid `.equals` when comparing enums\r\n\r\n```suggestion\r\n    if (upgradeCoordinator.getStatus() != UpgradeCoordinator.UpgradeStatus.COMPLETE) {\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Done in https://github.com/apache/accumulo/pull/2321/commits/80aae5a075bf00a6ba67ef68cd3640140ed018a3', 'commenter': 'Manno15'}]"
2333,test/src/main/java/org/apache/accumulo/test/functional/ManagerApiIT.java,"@@ -210,12 +210,19 @@ public void testPermissions_shutdownTabletServer() throws Exception {
   // see the junit annotation to control test ordering at the top of this class
   @Test
   public void z99_testPermissions_shutdown() throws Exception {
+    // grab connections before shutting down
+    AccumuloClient rootUserClient = Accumulo.newClient().from(getClientProps())
+        .as(rootUser.getPrincipal(), rootUser.getToken()).build();
+    AccumuloClient privilegedUserClient = Accumulo.newClient().from(getClientProps())
+        .as(privilegedUser.getPrincipal(), privilegedUser.getToken()).build();
     // To shutdown, user needs SystemPermission.SYSTEM
     op = user -> client -> client.shutdown(null, user, false);
     expectPermissionDenied(op, regularUser);
     // We should be able to do both of the following RPC calls before it actually shuts down
-    expectPermissionSuccess(op, rootUser);
-    expectPermissionSuccess(op, privilegedUser);
+    expectPermissionSuccess(op, (ClientContext) rootUserClient);
+    expectPermissionSuccess(op, (ClientContext) privilegedUserClient);
+    rootUserClient.close();
+    privilegedUserClient.close();","[{'comment': 'These clients should be in try-with-resources blocks, rather than have explicit calls to `close()`', 'commenter': 'ctubbsii'}, {'comment': 'I spent way too much time getting the formatting to not be hideous so I hope you appreciate the double try-with-resources in 1f0bcd1 :stuck_out_tongue_winking_eye: ', 'commenter': 'milleruntime'}]"
2340,core/src/main/java/org/apache/accumulo/core/client/AccumuloClient.java,"@@ -338,6 +338,13 @@ ConditionalWriter createConditionalWriter(String tableName, ConditionalWriterCon
   @Override
   void close();
 
+  /**
+   * Sets a user-defined ClientThreadPools implementation
+   *
+   * @param impl","[{'comment': 'javadoc quality: remove tag if no description, or add description', 'commenter': 'ctubbsii'}, {'comment': 'Fixed in 94a6163, I should have waited to have you review until the checks were done.', 'commenter': 'dlmarion'}]"
2340,core/src/main/java/org/apache/accumulo/core/client/ClientThreadPools.java,"@@ -0,0 +1,136 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client;
+
+import java.util.concurrent.ScheduledThreadPoolExecutor;
+import java.util.concurrent.ThreadPoolExecutor;
+
+import org.apache.accumulo.core.clientImpl.ClientContext;
+
+public interface ClientThreadPools {
+
+  /**
+   * return a shared scheduled executor for trivial tasks
+   *
+   * @param ctx
+   * @return ScheduledThreadPoolExecutor
+   */
+  ScheduledThreadPoolExecutor getSharedScheduledExecutor(ClientContext context);","[{'comment': 'ClientContext is not public API, and should not be', 'commenter': 'ctubbsii'}, {'comment': 'Fixed in 94a6163, I should have waited to have you review until the checks were done.', 'commenter': 'dlmarion'}]"
2340,core/src/main/java/org/apache/accumulo/core/client/ClientThreadPools.java,"@@ -0,0 +1,136 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client;
+
+import java.util.concurrent.ScheduledThreadPoolExecutor;
+import java.util.concurrent.ThreadPoolExecutor;
+
+import org.apache.accumulo.core.clientImpl.ClientContext;
+
+public interface ClientThreadPools {
+
+  /**
+   * return a shared scheduled executor for trivial tasks
+   *
+   * @param ctx
+   * @return ScheduledThreadPoolExecutor
+   */
+  ScheduledThreadPoolExecutor getSharedScheduledExecutor(ClientContext context);
+
+  /**
+   * ThreadPoolExecutor that runs bulk import tasks
+   *
+   * @param ctx
+   * @param numThreads
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getBulkImportThreadPool(ClientContext ctx, int numThreads);
+
+  /**
+   * ThreadPoolExecutor that runs tasks to contact Compactors to get running compaction information
+   *
+   * @param numThreads
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getExternalCompactionActiveCompactionsPool(ClientContext ctx, int numThreads);
+
+  /**
+   * ThreadPoolExecutor used for fetching data from the TabletServers
+   *
+   * @param ctx
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getScannerReadAheadPool(ClientContext ctx);
+
+  /**
+   * ThreadPoolExecutor used for adding splits to a table
+   *
+   * @param ctx
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getAddSplitsThreadPool(ClientContext ctx);
+
+  /**
+   * ThreadPoolExecutor used for fetching data from the TabletServers
+   *
+   * @param ctx
+   * @param numQueryThreads
+   * @param batchReaderInstance
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getBatchReaderThreadPool(ClientContext ctx, int numQueryThreads,
+      int batchReaderInstance);
+
+  /**
+   * ScheduledThreadPoolExecutor that runs tasks for the BatchWriter to meet the users latency
+   * goals.
+   * 
+   * @param ctx
+   *          client context object
+   * @return ScheduledThreadPoolExecutor
+   */
+  ScheduledThreadPoolExecutor getBatchWriterLatencyTasksThreadPool(ClientContext ctx);
+
+  /**
+   * ThreadPoolExecutor that runs the tasks of binning mutations
+   * 
+   * @param ctx
+   *          client context object
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getBatchWriterBinningThreadPool(ClientContext ctx);
+
+  /**
+   * ThreadPoolExecutor that runs the tasks of sending mutations to TabletServers
+   *
+   * @param ctx
+   *          client context object
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getBatchWriterSendThreadPool(ClientContext ctx, int numSendThreads);
+
+  /**
+   * ThreadPoolExecutor that runs clean up tasks when close is called on the ConditionalWriter
+   *
+   * @param ctx
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getConditionalWriterCleanupTaskThreadPool(ClientContext ctx);","[{'comment': 'Having one central location that entagles itself with so many disparate public API methods seems less than ideal. I think it would be better if each entry point (batch scanner, batch writer, conditional writer, etc.) added an option to their existing builders/config to set a thread pool... or more narrowly, just an uncaught exception handler for the thread pools we manage internally.', 'commenter': 'ctubbsii'}, {'comment': ""I was trying to reduce the number of places users would have to modify their pre-2.1.0 code. This way would allow them to create their own implementation when they create the client instead of specifying for each scanner/writer/etc. Additionally, by creating their own ThreadPool's, they can set their own uncaught exception handler."", 'commenter': 'dlmarion'}, {'comment': ""We did try to make those builders/config extensible, so they wouldn't have to modify anything unless they were taking advantage of the new feature... However, I think I have a better idea anyway.\r\n\r\nI noticed that this class is essentially a ThreadPoolExecutorFactory for either `ThreadPoolExecutor` or `ScheduledThreadPoolExecutor`. Instead of having so many disparate methods, we can make the interface substantially simpler, and still support all the different uses, by passing in some kind of context/scope, as in:\r\n\r\n```java\r\n    // this name is bad, but illustrates the idea; this could even be a String, if we don't want to constrain it\r\n    enum TheadPoolScope {\r\n      BATCH_WRITER, BATCH_SCANNER, CONDITIONAL_WRITER;\r\n    }\r\n    ThreadPoolExecutor getExecutor(ThreadPoolScope scope, ConfigSource conf);\r\n    ScheduledThreadPoolExecutor getScheduledThreadPoolExecutor(ThreadPoolScope scope, ConfigSource conf);\r\n```\r\n\r\nThe main idea here is to keep the API as free of bloat as possible, but still be extensible and applicable to all our use cases."", 'commenter': 'ctubbsii'}, {'comment': 'see 2effcce.', 'commenter': 'dlmarion'}, {'comment': '> The main idea here is to keep the API as free of bloat as possible\r\n\r\nIt would be nice to trim this down. Not sure how, but all of the enums do seem like a bit much.', 'commenter': 'keith-turner'}, {'comment': ""I'm not sure how we do that. We need to pass enough information to determine which pool to return and how to configure it. I'm open to suggestions."", 'commenter': 'dlmarion'}, {'comment': 'The enums do not properly capture all of the complexity.  For example I see for one enum a SynchronousQueue is used and for another enum 0 to 3 threads are used.  It seems like the enum could be dropped and everything that is needed for creation pushed into the config object, maybe that becomes to complex for anyone to ever use properly though.\r\n\r\n', 'commenter': 'keith-turner'}, {'comment': ""I have some changes locally where I'm cleaning up potentially unclosed thread pools (for example, if someone did not call close). This means that clients might be able to specify their own implementation, but they would not be able to share the thread pool within the client or with the clients application. I'm starting to think that the only thing a user would be able to configure is the number of threads in the pools. Currently the number of threads is either hardcoded or based on a property."", 'commenter': 'dlmarion'}]"
2340,core/src/main/java/org/apache/accumulo/core/util/threads/AccumuloUncaughtExceptionHandler.java,"@@ -44,7 +46,10 @@ public void uncaughtException(Thread t, Throwable e) {
         // If e == OutOfMemoryError, then it's probably that another Error might be
         // thrown when trying to print to System.err.
       } finally {
-        Runtime.getRuntime().halt(-1);
+        Mode m = SingletonManager.getMode();
+        if (m != null && m.equals(Mode.SERVER)) {
+          Runtime.getRuntime().halt(-1);
+        }","[{'comment': 'Rather than making this handler behave differently in some circumstances, why not make it so a different handler can be configured instead?', 'commenter': 'ctubbsii'}, {'comment': 'Never mind on this. I think the current idea of providing the executor factory is better.', 'commenter': 'ctubbsii'}]"
2340,core/src/main/java/org/apache/accumulo/core/clientImpl/bulk/BulkImport.java,"@@ -229,6 +228,7 @@ private Path checkPath(FileSystem fs, String dir) throws IOException, AccumuloEx
   }
 
   @Override
+  @Deprecated(since = ""2.1.0"")
   public ImportMappingOptions executor(Executor service) {","[{'comment': ""It's not clear. Why is this option being deprecated in the bulk import builder?"", 'commenter': 'ctubbsii'}, {'comment': ""It's available in the ClientThreadPools implementation."", 'commenter': 'dlmarion'}, {'comment': 'Okay, a javadoc deprecated note to point to the new API would be helpful here', 'commenter': 'ctubbsii'}, {'comment': 'Resolved in 6c996df', 'commenter': 'dlmarion'}]"
2340,core/src/main/java/org/apache/accumulo/core/client/ClientThreadPools.java,"@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client;
+
+import java.util.Map.Entry;
+import java.util.Optional;
+import java.util.concurrent.ScheduledThreadPoolExecutor;
+import java.util.concurrent.ThreadPoolExecutor;
+
+public interface ClientThreadPools {
+
+  class ThreadPoolConfig {
+
+    public static final ThreadPoolConfig EMPTY_CONFIG =
+        new ThreadPoolConfig(Optional.empty(), Optional.empty(), Optional.empty());
+
+    private final Optional<Iterable<Entry<String,String>>> configuration;
+    private final Optional<Integer> numThreads;
+    private final Optional<String> threadName;
+
+    public ThreadPoolConfig(Iterable<Entry<String,String>> configuration) {
+      this(Optional.of(configuration), Optional.empty(), Optional.empty());
+    }
+
+    public ThreadPoolConfig(Iterable<Entry<String,String>> configuration, int numThreads) {
+      this(Optional.of(configuration), Optional.of(numThreads), Optional.empty());
+    }
+
+    public ThreadPoolConfig(Iterable<Entry<String,String>> configuration, int numThreads,
+        String threadName) {
+      this(Optional.of(configuration), Optional.of(numThreads), Optional.of(threadName));
+    }
+
+    private ThreadPoolConfig(Optional<Iterable<Entry<String,String>>> configuration,
+        Optional<Integer> numThreads, Optional<String> threadName) {
+      this.configuration = configuration;
+      this.numThreads = numThreads;
+      this.threadName = threadName;
+    }
+
+    public Optional<Iterable<Entry<String,String>>> getConfiguration() {
+      return configuration;
+    }
+
+    public Optional<Integer> getNumThreads() {
+      return numThreads;
+    }
+
+    public Optional<String> getThreadName() {
+      return threadName;
+    }
+  }
+
+  enum ThreadPoolUsage {
+    /**
+     * ThreadPoolExecutor that runs bulk import tasks
+     */
+    BULK_IMPORT_POOL,
+    /**
+     * ThreadPoolExecutor that runs tasks to contact Compactors to get running compaction
+     * information
+     */
+    ACTIVE_EXTERNAL_COMPACTION_POOL,
+    /**
+     * ThreadPoolExecutor used for fetching data from the TabletServers
+     */
+    SCANNER_READ_AHEAD_POOL,
+    /**
+     * ThreadPoolExecutor used for adding splits to a table
+     */
+    ADD_SPLITS_THREAD_POOL,
+    /**
+     * ThreadPoolExecutor used for fetching data from the TabletServers
+     */
+    BATCH_SCANNER_READ_AHEAD_POOL,
+    /**
+     * ThreadPoolExecutor that runs the tasks of binning mutations
+     */
+    BATCH_WRITER_BINNING_POOL,
+    /**
+     * ThreadPoolExecutor that runs the tasks of sending mutations to TabletServers
+     */
+    BATCH_WRITER_SEND_POOL,
+    /**
+     * ThreadPoolExecutor that runs clean up tasks when close is called on the ConditionalWriter
+     */
+    CONDITIONAL_WRITER_CLEANUP_TASK_POOL,
+    /**
+     * ThreadPoolExecutor responsible for loading bloom filters
+     */
+    BLOOM_FILTER_LAYER_LOADER_POOL
+  }
+
+  enum ScheduledThreadPoolUsage {
+    /**
+     * shared scheduled executor for trivial tasks
+     */
+    SHARED_GENERAL_SCHEDULED_TASK_POOL,
+    /**
+     * ScheduledThreadPoolExecutor that runs tasks for the BatchWriter to meet the users latency
+     * goals.
+     */
+    BATCH_WRITER_LATENCY_TASK_POOL,
+    /**
+     * ScheduledThreadPoolExecutor that periodically runs tasks to handle failed write mutations and
+     * send mutations to TabletServers
+     */
+    CONDITIONAL_WRITER_RETRY_POOL
+  }
+
+  /**
+   * return a ThreadPoolExecutor configured for the specified usage
+   *
+   * @param usage
+   *          thread pool usage
+   * @param config
+   *          thread pool configuration
+   * @return ThreadPoolExecutor
+   */
+  ThreadPoolExecutor getThreadPool(ThreadPoolUsage usage, ThreadPoolConfig config);","[{'comment': 'Could an ExecutorService type be returned instead of ThreadPoolExecutor?  The javadocs need to clearly state that Accumulo will shut these thread pools down as needed (if that is what will happen).  Also seem like the name `newExecutorService` or `createExecutorService` would be better than get.\r\n\r\n```suggestion\r\n  ExecutorService createExecutorService(ThreadPoolUsage usage, ThreadPoolConfig config);\r\n```', 'commenter': 'keith-turner'}, {'comment': ""> Could an ExecutorService type be returned instead of ThreadPoolExecutor?\r\n\r\nThere are some methods on the ScheduledThreadPoolExecutor, like remove(Runnable), that aren't  on the interface."", 'commenter': 'dlmarion'}]"
2374,server/base/src/main/java/org/apache/accumulo/server/ServerInfo.java,"@@ -176,7 +177,10 @@ public String getInstanceName() {
   }
 
   public Credentials getCredentials() {
-    return SystemCredentials.get(getInstanceID(), getSiteConfiguration());
+    if (credentials == null) {
+      credentials = SystemCredentials.get(getInstanceID(), getSiteConfiguration());","[{'comment': 'This probably needs sync, volatile, or an AtomicReference to be thread safe.\r\n\r\nThis comment may be moot because of the other comment @ctubbsii  made about moving this to the constructor which would make it thread safe.', 'commenter': 'keith-turner'}]"
2374,server/base/src/main/java/org/apache/accumulo/server/ServerInfo.java,"@@ -175,10 +179,14 @@ public String getInstanceName() {
     return instanceName;
   }
 
-  public Credentials getCredentials() {
+  private Credentials loadCredentials() {
     return SystemCredentials.get(getInstanceID(), getSiteConfiguration());
   }","[{'comment': ""This `loadCredentials` method should be inline'd into the constructor, and use the instanceId and siteConfig variables available in those constructors, to avoid calling instance methods on a half-constructed object. Since that change is trivial, I can go ahead and do it and merge this in."", 'commenter': 'ctubbsii'}]"
2380,assemble/bin/accumulo-cluster,"@@ -129,9 +129,21 @@ function control_service() {
     [[ ""$service"" == ""compactor"" ]] && ACCUMULO_SERVICE_INSTANCE=""${inst_id}_${5}""
 
     if [[ $host == localhost || $host == ""$(hostname -s)"" || $host == ""$(hostname -f)"" || $host == $(get_ip) ]] ; then
-      ACCUMULO_SERVICE_INSTANCE=""${ACCUMULO_SERVICE_INSTANCE}"" ""${bin}/accumulo-service"" ""$service"" ""$control_cmd"" ""${@:4}""
+      #
+      # The server processes take arguments (e.g. -a, -p, -o, -q [in the case of the Compactor]).
+      #
+      if [[ $# -gt 3 ]]; then
+        ACCUMULO_SERVICE_INSTANCE=""${ACCUMULO_SERVICE_INSTANCE}"" ""${bin}/accumulo-service"" ""$service"" ""$control_cmd"" ""${@:4}""
+      else
+        ACCUMULO_SERVICE_INSTANCE=""${ACCUMULO_SERVICE_INSTANCE}"" ""${bin}/accumulo-service"" ""$service"" ""$control_cmd""
+      fi
     else
-      $SSH ""$host"" ""bash -c 'ACCUMULO_SERVICE_INSTANCE=${ACCUMULO_SERVICE_INSTANCE} ${bin}/accumulo-service \""$service\"" \""$control_cmd\"" \""${@:4}\"" '""
+      if [[ $# -gt 3 ]]; then
+        EXTRA_ARGS=""${@:4}""
+        $SSH ""$host"" ""bash -c 'ACCUMULO_SERVICE_INSTANCE=${ACCUMULO_SERVICE_INSTANCE} ${bin}/accumulo-service \""$service\"" \""$control_cmd\"" $EXTRA_ARGS '""","[{'comment': 'This is flattening the array into a single string, which can mess with our ability to reason about the quoting. It\'s probably better to be consistent, and use the array form (or leave the original, which was already an array):\r\n```suggestion\r\n        EXTRA_ARGS=(""${@:4}"")\r\n        $SSH ""$host"" ""bash -c \'ACCUMULO_SERVICE_INSTANCE=${ACCUMULO_SERVICE_INSTANCE} ${bin}/accumulo-service \\""$service\\"" \\""$control_cmd\\"" \\""${EXTRA_ARGS[@]}\\"" \'""\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'I can test this out and see if it works.', 'commenter': 'dlmarion'}, {'comment': ""So, the args come over as one string in the way that you suggested and does not work. Error in the compactor.err file:\r\n\r\n```\r\nWas passed main parameter '-q q2' but no main parameter was defined in your arg class\r\n```"", 'commenter': 'dlmarion'}, {'comment': ""Okay, that's fine then. It looks like it's flattening them already on the local end. So, I guess we need to pass them unquoted so they get treated as separate args... or we could do something crazy like build the command using an array, but that seems like overkill. I think your solution that worked without the quoting is fine."", 'commenter': 'ctubbsii'}]"
2384,test/src/main/java/org/apache/accumulo/test/functional/YieldingIterator.java,"@@ -95,8 +95,8 @@ public void next() throws IOException {
    */
   @Override
   public Value getTopValue() {
-    String value = Integer.toString(yieldNexts.get()) + ',' + Integer.toString(yieldSeeks.get())
-        + ',' + Integer.toString(rebuilds.get());
+    String value =
+        Integer.toString(yieldNexts.get()) + ',' + yieldSeeks.get() + ',' + rebuilds.get();","[{'comment': '```suggestion\r\n        yieldNexts.get() + "","" + yieldSeeks.get() + "","" + rebuilds.get();\r\n```\r\nCould do this if we wanted to avoid using the first `Integer.toString` as well. Either way, LGTM.', 'commenter': 'DomGarguilo'}, {'comment': 'Fixed in 4df7b130fc', 'commenter': 'EdColeman'}]"
2384,server/base/src/main/java/org/apache/accumulo/server/util/DumpZookeeper.java,"@@ -129,7 +129,7 @@ private static void write(PrintStream out, int indent, String fmt, Object... arg
     for (int i = 0; i < indent; i++) {
       out.print(""  "");
     }
-    out.println(String.format(fmt, args));
+    out.printf((fmt) + ""%n"", args);","[{'comment': '```suggestion\r\n    out.printf(fmt + ""%n"", args);\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""I just accepted the IDE suggested changes - looking at your suggestion, I can't find where adding the %n is necessary to add to the calling fmt string."", 'commenter': 'EdColeman'}, {'comment': 'Updated with proper newlines added in 76fa72c862', 'commenter': 'EdColeman'}]"
2384,test/src/main/java/org/apache/accumulo/test/GetManagerStats.java,"@@ -148,11 +148,11 @@ public static void main(String[] args) throws Exception {
     }
   }
 
-  private static void out(int indent, String string, Object... args) {
+  private static void out(int indent, String fmtString, Object... args) {
     for (int i = 0; i < indent; i++) {
       System.out.print("" "");
     }
-    System.out.println(String.format(string, args));
+    System.out.printf((fmtString) + ""%n"", args);","[{'comment': '```suggestion\r\n    System.out.printf(fmtString + ""%n"", args);\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""I just accepted the IDE suggested changes - looking at your suggestion, I can't find where adding the %n is necessary to add to the calling fmt string."", 'commenter': 'EdColeman'}, {'comment': 'Updated with proper newlines added in 76fa72c862', 'commenter': 'EdColeman'}]"
2384,core/src/main/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParser.java,"@@ -83,8 +83,7 @@ private static void flatten(String parentKey, String key, Object value,
   public static void outputShellVariables(Map<String,String> config, PrintStream out) {
     for (String section : SECTIONS) {
       if (config.containsKey(section)) {
-        out.println(
-            String.format(PROPERTY_FORMAT, section.toUpperCase() + ""_HOSTS"", config.get(section)));
+        out.printf((PROPERTY_FORMAT) + ""%n"", section.toUpperCase() + ""_HOSTS"", config.get(section));","[{'comment': '```suggestion\r\n        out.printf(PROPERTY_FORMAT + ""%n"", section.toUpperCase() + ""_HOSTS"", config.get(section));\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Not sure why, but I missed this and following comments - will address in next PR.', 'commenter': 'EdColeman'}]"
2384,core/src/main/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParser.java,"@@ -94,20 +93,20 @@ public static void outputShellVariables(Map<String,String> config, PrintStream o
     }
 
     if (config.containsKey(""compaction.coordinator"")) {
-      out.println(String.format(PROPERTY_FORMAT, ""COORDINATOR_HOSTS"",
-          config.get(""compaction.coordinator"")));
+      out.printf((PROPERTY_FORMAT) + ""%n"", ""COORDINATOR_HOSTS"",","[{'comment': '```suggestion\r\n      out.printf(PROPERTY_FORMAT + ""%n"", ""COORDINATOR_HOSTS"",\r\n```', 'commenter': 'ctubbsii'}]"
2384,core/src/main/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParser.java,"@@ -94,20 +93,20 @@ public static void outputShellVariables(Map<String,String> config, PrintStream o
     }
 
     if (config.containsKey(""compaction.coordinator"")) {
-      out.println(String.format(PROPERTY_FORMAT, ""COORDINATOR_HOSTS"",
-          config.get(""compaction.coordinator"")));
+      out.printf((PROPERTY_FORMAT) + ""%n"", ""COORDINATOR_HOSTS"",
+          config.get(""compaction.coordinator""));
     }
     if (config.containsKey(""compaction.compactor.queue"")) {
-      out.println(String.format(PROPERTY_FORMAT, ""COMPACTION_QUEUES"",
-          config.get(""compaction.compactor.queue"")));
+      out.printf((PROPERTY_FORMAT) + ""%n"", ""COMPACTION_QUEUES"",","[{'comment': '```suggestion\r\n      out.printf(PROPERTY_FORMAT + ""%n"", ""COMPACTION_QUEUES"",\r\n```', 'commenter': 'ctubbsii'}]"
2384,core/src/main/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParser.java,"@@ -94,20 +93,20 @@ public static void outputShellVariables(Map<String,String> config, PrintStream o
     }
 
     if (config.containsKey(""compaction.coordinator"")) {
-      out.println(String.format(PROPERTY_FORMAT, ""COORDINATOR_HOSTS"",
-          config.get(""compaction.coordinator"")));
+      out.printf((PROPERTY_FORMAT) + ""%n"", ""COORDINATOR_HOSTS"",
+          config.get(""compaction.coordinator""));
     }
     if (config.containsKey(""compaction.compactor.queue"")) {
-      out.println(String.format(PROPERTY_FORMAT, ""COMPACTION_QUEUES"",
-          config.get(""compaction.compactor.queue"")));
+      out.printf((PROPERTY_FORMAT) + ""%n"", ""COMPACTION_QUEUES"",
+          config.get(""compaction.compactor.queue""));
     }
     String queues = config.get(""compaction.compactor.queue"");
     if (StringUtils.isNotEmpty(queues)) {
       String[] q = queues.split("" "");
       for (int i = 0; i < q.length; i++) {
         if (config.containsKey(""compaction.compactor."" + q[i])) {
-          out.println(String.format(PROPERTY_FORMAT, ""COMPACTOR_HOSTS_"" + q[i],
-              config.get(""compaction.compactor."" + q[i])));
+          out.printf((PROPERTY_FORMAT) + ""%n"", ""COMPACTOR_HOSTS_"" + q[i],","[{'comment': '```suggestion\r\n          out.printf(PROPERTY_FORMAT + ""%n"", ""COMPACTOR_HOSTS_"" + q[i],\r\n```', 'commenter': 'ctubbsii'}]"
2384,minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java,"@@ -288,22 +288,22 @@ private static void setMemoryOnConfig(MiniAccumuloConfig config, String memorySt
 
   private static void printInfo(MiniAccumuloCluster accumulo, int shutdownPort) {
     System.out.println(""Mini Accumulo Cluster\n"");
-    System.out.println(String.format(FORMAT_STRING, ""Directory:"",
-        accumulo.getConfig().getDir().getAbsoluteFile()));
-    System.out.println(String.format(FORMAT_STRING, ""Logs:"",
-        accumulo.getConfig().getImpl().getLogDir().getAbsoluteFile()));
-    System.out.println(
-        String.format(FORMAT_STRING, ""Instance Name:"", accumulo.getConfig().getInstanceName()));
-    System.out.println(
-        String.format(FORMAT_STRING, ""Root Password:"", accumulo.getConfig().getRootPassword()));
-    System.out.println(String.format(FORMAT_STRING, ""ZooKeeper:"", accumulo.getZooKeepers()));
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Directory:"",
+        accumulo.getConfig().getDir().getAbsoluteFile());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Logs:"",
+        accumulo.getConfig().getImpl().getLogDir().getAbsoluteFile());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Instance Name:"",
+        accumulo.getConfig().getInstanceName());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Root Password:"",
+        accumulo.getConfig().getRootPassword());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""ZooKeeper:"", accumulo.getZooKeepers());","[{'comment': '```suggestion\r\n    System.out.printf(FORMAT_STRING + ""%n"", ""Directory:"",\r\n        accumulo.getConfig().getDir().getAbsoluteFile());\r\n    System.out.printf(FORMAT_STRING + ""%n"", ""Logs:"",\r\n        accumulo.getConfig().getImpl().getLogDir().getAbsoluteFile());\r\n    System.out.printf(FORMAT_STRING + ""%n"", ""Instance Name:"",\r\n        accumulo.getConfig().getInstanceName());\r\n    System.out.printf(FORMAT_STRING + ""%n"", ""Root Password:"",\r\n        accumulo.getConfig().getRootPassword());\r\n    System.out.printf(FORMAT_STRING + ""%n"", ""ZooKeeper:"", accumulo.getZooKeepers());\r\n```', 'commenter': 'ctubbsii'}]"
2384,minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java,"@@ -288,22 +288,22 @@ private static void setMemoryOnConfig(MiniAccumuloConfig config, String memorySt
 
   private static void printInfo(MiniAccumuloCluster accumulo, int shutdownPort) {
     System.out.println(""Mini Accumulo Cluster\n"");
-    System.out.println(String.format(FORMAT_STRING, ""Directory:"",
-        accumulo.getConfig().getDir().getAbsoluteFile()));
-    System.out.println(String.format(FORMAT_STRING, ""Logs:"",
-        accumulo.getConfig().getImpl().getLogDir().getAbsoluteFile()));
-    System.out.println(
-        String.format(FORMAT_STRING, ""Instance Name:"", accumulo.getConfig().getInstanceName()));
-    System.out.println(
-        String.format(FORMAT_STRING, ""Root Password:"", accumulo.getConfig().getRootPassword()));
-    System.out.println(String.format(FORMAT_STRING, ""ZooKeeper:"", accumulo.getZooKeepers()));
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Directory:"",
+        accumulo.getConfig().getDir().getAbsoluteFile());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Logs:"",
+        accumulo.getConfig().getImpl().getLogDir().getAbsoluteFile());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Instance Name:"",
+        accumulo.getConfig().getInstanceName());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Root Password:"",
+        accumulo.getConfig().getRootPassword());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""ZooKeeper:"", accumulo.getZooKeepers());
 
     for (Pair<ServerType,Integer> pair : accumulo.getDebugPorts()) {
-      System.out.println(String.format(FORMAT_STRING, pair.getFirst().prettyPrint() + "" JDWP Host:"",
-          ""localhost:"" + pair.getSecond()));
+      System.out.printf((FORMAT_STRING) + ""%n"", pair.getFirst().prettyPrint() + "" JDWP Host:"",","[{'comment': '```suggestion\r\n      System.out.printf(FORMAT_STRING + ""%n"", pair.getFirst().prettyPrint() + "" JDWP Host:"",\r\n```', 'commenter': 'ctubbsii'}]"
2384,minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java,"@@ -288,22 +288,22 @@ private static void setMemoryOnConfig(MiniAccumuloConfig config, String memorySt
 
   private static void printInfo(MiniAccumuloCluster accumulo, int shutdownPort) {
     System.out.println(""Mini Accumulo Cluster\n"");
-    System.out.println(String.format(FORMAT_STRING, ""Directory:"",
-        accumulo.getConfig().getDir().getAbsoluteFile()));
-    System.out.println(String.format(FORMAT_STRING, ""Logs:"",
-        accumulo.getConfig().getImpl().getLogDir().getAbsoluteFile()));
-    System.out.println(
-        String.format(FORMAT_STRING, ""Instance Name:"", accumulo.getConfig().getInstanceName()));
-    System.out.println(
-        String.format(FORMAT_STRING, ""Root Password:"", accumulo.getConfig().getRootPassword()));
-    System.out.println(String.format(FORMAT_STRING, ""ZooKeeper:"", accumulo.getZooKeepers()));
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Directory:"",
+        accumulo.getConfig().getDir().getAbsoluteFile());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Logs:"",
+        accumulo.getConfig().getImpl().getLogDir().getAbsoluteFile());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Instance Name:"",
+        accumulo.getConfig().getInstanceName());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Root Password:"",
+        accumulo.getConfig().getRootPassword());
+    System.out.printf((FORMAT_STRING) + ""%n"", ""ZooKeeper:"", accumulo.getZooKeepers());
 
     for (Pair<ServerType,Integer> pair : accumulo.getDebugPorts()) {
-      System.out.println(String.format(FORMAT_STRING, pair.getFirst().prettyPrint() + "" JDWP Host:"",
-          ""localhost:"" + pair.getSecond()));
+      System.out.printf((FORMAT_STRING) + ""%n"", pair.getFirst().prettyPrint() + "" JDWP Host:"",
+          ""localhost:"" + pair.getSecond());
     }
 
-    System.out.println(String.format(FORMAT_STRING, ""Shutdown Port:"", shutdownPort));
+    System.out.printf((FORMAT_STRING) + ""%n"", ""Shutdown Port:"", shutdownPort);","[{'comment': '```suggestion\r\n    System.out.printf(FORMAT_STRING + ""%n"", ""Shutdown Port:"", shutdownPort);\r\n```', 'commenter': 'ctubbsii'}]"
2384,server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java,"@@ -83,8 +83,8 @@ public int run(String[] args) throws Exception {
         for (Entry<String,Long> entry : totalBlocks.entrySet()) {
           final String host = entry.getKey();
           final Long blocksForHost = entry.getValue();
-          System.out.println(String.format(""%15s %5.1f %8d"", host,
-              (localBlocks.get(host) * 100.) / blocksForHost, blocksForHost));
+          System.out.printf(""%15s %5.1f %8d%n"", host,
+              (localBlocks.get(host) * 100.) / blocksForHost, blocksForHost);","[{'comment': '```suggestion\r\n              localBlocks.get(host) * 100. / blocksForHost, blocksForHost);\r\n```', 'commenter': 'ctubbsii'}]"
2384,test/src/main/java/org/apache/accumulo/test/TestMultiTableIngest.java,"@@ -95,8 +95,7 @@ public static void main(String[] args) throws Exception {
         // populate
         for (int i = 0; i < opts.count; i++) {
           Mutation m = new Mutation(new Text(String.format(""%06d"", i)));
-          m.put(new Text(""col"" + Integer.toString((i % 3) + 1)), new Text(""qual""),
-              new Value(""junk""));
+          m.put(new Text(""col"" + ((i % 3) + 1)), new Text(""qual""), new Value(""junk""));","[{'comment': 'I think there\'s a CharSequence version of this:\r\n\r\n```suggestion\r\n          m.put(""col"" + ((i % 3) + 1), ""qual"", ""junk"");\r\n```', 'commenter': 'ctubbsii'}]"
2384,test/src/main/java/org/apache/accumulo/test/functional/TabletIT.java,"@@ -85,8 +85,7 @@ public void createTableTest(AccumuloClient accumuloClient, String tableName, boo
         // populate
         for (int i = 0; i < N; i++) {
           Mutation m = new Mutation(new Text(String.format(""%05d"", i)));
-          m.put(new Text(""col"" + Integer.toString((i % 3) + 1)), new Text(""qual""),
-              new Value(""junk""));
+          m.put(new Text(""col"" + ((i % 3) + 1)), new Text(""qual""), new Value(""junk""));","[{'comment': '```suggestion\r\n          m.put(""col"" + ((i % 3) + 1), ""qual"", ""junk"");\r\n```', 'commenter': 'ctubbsii'}]"
2384,server/base/src/main/java/org/apache/accumulo/server/util/DumpZookeeper.java,"@@ -129,12 +129,12 @@ private static void write(PrintStream out, int indent, String fmt, Object... arg
     for (int i = 0; i < indent; i++) {
       out.print(""  "");
     }
-    out.println(String.format(fmt, args));
+    out.printf(fmt, args);","[{'comment': 'In this instance, it might actually make sense to change `fmt` to `fmt + %n` rather than update each caller separately. The idea here is to minimize the potential for error and to ensure consistency by doing the work in fewer places:\r\n\r\n```suggestion\r\n    out.printf(fmt + ""%n"", args);\r\n```\r\n\r\nOf course, changing this would mean reverting the changes to the other lines in this file.', 'commenter': 'ctubbsii'}]"
2384,test/src/main/java/org/apache/accumulo/harness/SharedMiniClusterBase.java,"@@ -124,7 +124,7 @@ private static String getTestClassName() {
     Optional<String> callerClassName =
         StackWalker.getInstance(RETAIN_CLASS_REFERENCE).walk(findCallerITClass).map(Class::getName);
     // use the calling class name, or default to a unique name if IT class can't be found
-    return callerClassName.orElse(String.format(""UnknownITClass-{}-{}"", System.currentTimeMillis(),
+    return callerClassName.orElse(String.format(""UnknownITClass-%d-%s"", System.currentTimeMillis(),","[{'comment': 'Should both of these be `%d`?\r\n\r\n```suggestion\r\n    return callerClassName.orElse(String.format(""UnknownITClass-%d-%d"", System.currentTimeMillis(),\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""@EdColeman Just pinging you for this one, because I'm not sure if you saw it. My guess is that it probably works fine either way."", 'commenter': 'ctubbsii'}, {'comment': 'I saw this too but it does work either way since `%s` accepts any type even though we are expecting a long.  Ran some tests in jshell to verify this.', 'commenter': 'DomGarguilo'}, {'comment': 'I did miss it - corrected in #2390. ', 'commenter': 'EdColeman'}]"
2384,test/src/main/java/org/apache/accumulo/test/GetManagerStats.java,"@@ -58,101 +58,103 @@ public static void main(String[] args) throws Exception {
         }
       }
     }
-    out(0, ""State: "" + stats.state.name());
-    out(0, ""Goal State: "" + stats.goalState.name());
+    out(0, ""State: %s%n"", stats.state.name());
+    out(0, ""Goal State: %s%n"", stats.goalState.name());
     if (stats.serversShuttingDown != null && !stats.serversShuttingDown.isEmpty()) {
-      out(0, ""Servers to shutdown"");
+      out(0, ""Servers to shutdown%n"");
       for (String server : stats.serversShuttingDown) {
-        out(1, ""%s"", server);
+        out(1, ""%s%n"", server);
       }
     }
-    out(0, ""Unassigned tablets: %d"", stats.unassignedTablets);
+    out(0, ""Unassigned tablets: %d%n"", stats.unassignedTablets);
     if (stats.badTServers != null && !stats.badTServers.isEmpty()) {
-      out(0, ""Bad servers"");
+      out(0, ""Bad servers%n"");
 
       for (Entry<String,Byte> entry : stats.badTServers.entrySet()) {
-        out(1, ""%s: %d"", entry.getKey(), (int) entry.getValue());
+        out(1, ""%s: %d%n"", entry.getKey(), (int) entry.getValue());
       }
     }
-    out(0, ""Dead tablet servers count: %s"", stats.deadTabletServers.size());
+    out(0, ""Dead tablet servers count: %s%n"", stats.deadTabletServers.size());
     for (DeadServer dead : stats.deadTabletServers) {
-      out(1, ""Dead tablet server: %s"", dead.server);
-      out(2, ""Last report: %s"", new SimpleDateFormat().format(new Date(dead.lastStatus)));
-      out(2, ""Cause: %s"", dead.status);
+      out(1, ""Dead tablet server: %s%n"", dead.server);
+      out(2, ""Last report: %s%n"", new SimpleDateFormat().format(new Date(dead.lastStatus)));
+      out(2, ""Cause: %s%n"", dead.status);
     }
-    out(0, ""Bulk imports: %s"", stats.bulkImports.size());
+    out(0, ""Bulk imports: %s%n"", stats.bulkImports.size());
     for (BulkImportStatus bulk : stats.bulkImports) {
-      out(1, ""Import directory: %s"", bulk.filename);
-      out(2, ""Bulk state %s"", bulk.state);
-      out(2, ""Bulk start %s"", bulk.startTime);
+      out(1, ""Import directory: %s%n"", bulk.filename);
+      out(2, ""Bulk state %s%n"", bulk.state);
+      out(2, ""Bulk start %s%n"", bulk.startTime);
     }
     if (stats.tableMap != null && !stats.tableMap.isEmpty()) {
-      out(0, ""Tables"");
+      out(0, ""Tables%n"");
       for (Entry<String,TableInfo> entry : stats.tableMap.entrySet()) {
         TableInfo v = entry.getValue();
-        out(1, ""%s"", entry.getKey());
-        out(2, ""Records: %d"", v.recs);
-        out(2, ""Records in Memory: %d"", v.recsInMemory);
-        out(2, ""Tablets: %d"", v.tablets);
-        out(2, ""Online Tablets: %d"", v.onlineTablets);
-        out(2, ""Ingest Rate: %.2f"", v.ingestRate);
-        out(2, ""Query Rate: %.2f"", v.queryRate);
+        out(1, ""%s%n"", entry.getKey());
+        out(2, ""Records: %d%n"", v.recs);
+        out(2, ""Records in Memory: %d%n"", v.recsInMemory);
+        out(2, ""Tablets: %d%n"", v.tablets);
+        out(2, ""Online Tablets: %d%n"", v.onlineTablets);
+        out(2, ""Ingest Rate: %.2f%n"", v.ingestRate);
+        out(2, ""Query Rate: %.2f%n"", v.queryRate);
       }
     }
     if (stats.tServerInfo != null && !stats.tServerInfo.isEmpty()) {
-      out(0, ""Tablet Servers"");
+      out(0, ""Tablet Servers%n"");
       long now = System.currentTimeMillis();
       for (TabletServerStatus server : stats.tServerInfo) {
         TableInfo summary = TableInfoUtil.summarizeTableStats(server);
-        out(1, ""Name: %s"", server.name);
-        out(2, ""Ingest: %.2f"", summary.ingestRate);
-        out(2, ""Last Contact: %s"", server.lastContact);
-        out(2, ""OS Load Average: %.2f"", server.osLoad);
-        out(2, ""Queries: %.2f"", summary.queryRate);
-        out(2, ""Time Difference: %.1f"", ((now - server.lastContact) / 1000.));
-        out(2, ""Total Records: %d"", summary.recs);
-        out(2, ""Lookups: %d"", server.lookups);
+        out(1, ""Name: %s%n"", server.name);
+        out(2, ""Ingest: %.2f%n"", summary.ingestRate);
+        out(2, ""Last Contact: %s%n"", server.lastContact);
+        out(2, ""OS Load Average: %.2f%n"", server.osLoad);
+        out(2, ""Queries: %.2f%n"", summary.queryRate);
+        out(2, ""Time Difference: %.1f%n"", ((now - server.lastContact) / 1000.));
+        out(2, ""Total Records: %d%n"", summary.recs);
+        out(2, ""Lookups: %d%n"", server.lookups);
         if (server.holdTime > 0) {
-          out(2, ""Hold Time: %d"", server.holdTime);
+          out(2, ""Hold Time: %d%n"", server.holdTime);
         }
         if (server.tableMap != null && !server.tableMap.isEmpty()) {
-          out(2, ""Tables"");
+          out(2, ""Tables%n"");
           for (Entry<String,TableInfo> status : server.tableMap.entrySet()) {
             TableInfo info = status.getValue();
-            out(3, ""Table: %s"", status.getKey());
-            out(4, ""Tablets: %d"", info.onlineTablets);
-            out(4, ""Records: %d"", info.recs);
-            out(4, ""Records in Memory: %d"", info.recsInMemory);
-            out(4, ""Ingest: %.2f"", info.ingestRate);
-            out(4, ""Queries: %.2f"", info.queryRate);
-            out(4, ""Major Compacting: %d"", info.majors == null ? 0 : info.majors.running);
-            out(4, ""Queued for Major Compaction: %d"", info.majors == null ? 0 : info.majors.queued);
-            out(4, ""Minor Compacting: %d"", info.minors == null ? 0 : info.minors.running);
-            out(4, ""Queued for Minor Compaction: %d"", info.minors == null ? 0 : info.minors.queued);
+            out(3, ""Table: %s%n"", status.getKey());
+            out(4, ""Tablets: %d%n"", info.onlineTablets);
+            out(4, ""Records: %d%n"", info.recs);
+            out(4, ""Records in Memory: %d%n"", info.recsInMemory);
+            out(4, ""Ingest: %.2f%n"", info.ingestRate);
+            out(4, ""Queries: %.2f%n"", info.queryRate);
+            out(4, ""Major Compacting: %d%n"", info.majors == null ? 0 : info.majors.running);
+            out(4, ""Queued for Major Compaction: %d%n"",
+                info.majors == null ? 0 : info.majors.queued);
+            out(4, ""Minor Compacting: %d%n"", info.minors == null ? 0 : info.minors.running);
+            out(4, ""Queued for Minor Compaction: %d%n"",
+                info.minors == null ? 0 : info.minors.queued);
           }
         }
-        out(2, ""Recoveries: %d"", server.logSorts.size());
+        out(2, ""Recoveries: %d%n"", server.logSorts.size());
         for (RecoveryStatus sort : server.logSorts) {
-          out(3, ""File: %s"", sort.name);
-          out(3, ""Progress: %.2f%%"", sort.progress * 100);
-          out(3, ""Time running: %s"", sort.runtime / 1000.);
+          out(3, ""File: %s%n"", sort.name);
+          out(3, ""Progress: %.2f%%%n"", sort.progress * 100);
+          out(3, ""Time running: %s%n"", sort.runtime / 1000.);
         }
-        out(3, ""Bulk imports: %s"", stats.bulkImports.size());
+        out(3, ""Bulk imports: %s%n"", stats.bulkImports.size());
         for (BulkImportStatus bulk : stats.bulkImports) {
-          out(4, ""Import file: %s"", bulk.filename);
-          out(5, ""Bulk state %s"", bulk.state);
-          out(5, ""Bulk start %s"", bulk.startTime);
+          out(4, ""Import file: %s%n"", bulk.filename);
+          out(5, ""Bulk state %s%n"", bulk.state);
+          out(5, ""Bulk start %s%n"", bulk.startTime);
         }
 
       }
     }
   }
 
-  private static void out(int indent, String string, Object... args) {
+  private static void out(int indent, String fmtString, Object... args) {
     for (int i = 0; i < indent; i++) {
       System.out.print("" "");
     }
-    System.out.println(String.format(string, args));
+    System.out.printf(fmtString, args);","[{'comment': 'I think this makes more sense, so the rest of the file doesn\'t need to change.\r\n\r\n```suggestion\r\n    System.out.printf(fmtString + ""%n"", args);\r\n```', 'commenter': 'ctubbsii'}]"
2399,core/src/main/java/org/apache/accumulo/core/file/rfile/SplitLarge.java,"@@ -53,10 +56,25 @@
   }
 
   public static void main(String[] args) throws Exception {
+    new SplitLarge().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""split-large"";
+  }
+
+  @Override
+  public String description() {
+    return ""Splits an RFile into large and small key/value files"";
+  }
+
+  @Override
+  public void execute(String[] args) throws Exception {
     Configuration conf = new Configuration();
     FileSystem fs = FileSystem.get(conf);
     Opts opts = new Opts();
-    opts.parseArgs(SplitLarge.class.getName(), args);
+    opts.parseArgs(""accumulo split-large"", args);","[{'comment': 'I don\'t think prefixing this with ""accumulo"" is appropriate here, because starts including information about the script that launched this Java code, and I don\'t think the code should make such assumptions.\r\n\r\nI suggest:\r\n```suggestion\r\n    opts.parseArgs(keyword(), args);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Do you think it is ever appropriate? Taking a quick look it appears the following commands are prefixed with ""accumulo"" in the same way as this one: `convert-config`, `file-info`, `create-token`, `init`, `wal-info`.', 'commenter': 'DomGarguilo'}, {'comment': 'I just followed what was done in CreateToken 😀', 'commenter': 'dlmarion'}, {'comment': ""Seems to me that those are wrong, too, but it's fine if it's consistent. As far as I can tell, it's only used for printing the help/usage."", 'commenter': 'ctubbsii'}, {'comment': '> Seems to me that those are wrong, too, but it\'s fine if it\'s consistent.\r\n\r\nIt doesn\'t look like it is consistent. Looks like in some places the command is hardcoded (e.g., `opts.parseArgs(""accumulo split-large""...`) while other places use `.class.getName()` (e.g., `opts.parseArgs(SplitLarge.class.getName()...`). It might be easy to make these consistent and I could go ahead and do that if it seems like it would be worth it.', 'commenter': 'DomGarguilo'}, {'comment': ""The class name is appropriate for those with `main` methods that aren't `KeywordExecutable`. I think `keyword()` is probably most appropriate for those that are `KeywordExecutable`. But, making these consistent can be done as a follow-up so it doesn't hold this one up."", 'commenter': 'ctubbsii'}]"
2400,core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java,"@@ -544,7 +544,7 @@ public static void main(String[] args) throws IOException {
 
     t2 = System.currentTimeMillis();
 
-    out.printf(""existant lookup rate %6.2f%n"", 500 / ((t2 - t1) / 1000.0));
+    out.printf(""existent lookup rate %6.2f%n"", 500 / ((t2 - t1) / 1000.0));","[{'comment': 'Maybe this should be existing?\r\n``` suggest\r\n    out.printf(""existing lookup rate %6.2f%n"", 500 / ((t2 - t1) / 1000.0));\r\n```', 'commenter': 'EdColeman'}]"
2400,core/src/main/java/org/apache/accumulo/core/util/AddressUtil.java,"@@ -61,7 +61,7 @@ public static int getAddressCacheNegativeTtl(UnknownHostException originalExcept
     try {
       negativeTtl = Integer.parseInt(Security.getProperty(""networkaddress.cache.negative.ttl""));
     } catch (NumberFormatException exception) {
-      log.warn(""Failed to get JVM negative DNS respones cache TTL due to format problem ""
+      log.warn(""Failed to get JVM negative DNS responses cache TTL due to format problem ""","[{'comment': 'Maybe response?\r\n``` suggest\r\n      log.warn(""Failed to get JVM negative DNS response cache TTL due to format problem ""\r\n```', 'commenter': 'EdColeman'}, {'comment': '```suggestion\r\n      log.warn(""Failed to get JVM negative DNS response cache TTL due to format problem ""\r\n```', 'commenter': 'DomGarguilo'}]"
2414,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java,"@@ -327,6 +334,13 @@ public MiniAccumuloClusterImpl(MiniAccumuloConfigImpl config) throws IOException
 
     this.config = config.initialize();
 
+    if (Boolean.TRUE.equals(Boolean
+        .valueOf(this.config.getSiteConfig().get(Property.TSERV_NATIVEMAP_ENABLED.name())))) {
+      if (!NativeMap.isLoaded())
+        throw new RuntimeException(
+            ""MAC configured to use native maps, but unable to load the library."");
+    }
+","[{'comment': ""I wouldn't expect this to work. The native maps are loaded by adding LD_LIBRARY_PATH environment to the tserver Process before it is launched. There's no reason to expect the native map to be loaded inside the MiniAccumuloClusterImpl... only for the processes it launches.\r\n\r\n(Also, no need to do `if (Boolean.TRUE.equals(someBoolExpr))` when you can just do `if (someBoolExpr)`)"", 'commenter': 'ctubbsii'}, {'comment': '`ConfigurableMacBase.createMiniAccumulo` sets `TSERV_NATIVEMAP_ENABLED` to true and then starts MAC. This checks that NativeMap will be able to load the library in this test thread before starting up the processes.', 'commenter': 'dlmarion'}, {'comment': 'Addressed logic comment in 3f7968f', 'commenter': 'dlmarion'}, {'comment': ""> This checks that NativeMap will be able to load the library in this test thread before starting up the processes.\r\n\r\nThere's no reason to believe that this test thread should be able to load the library. It doesn't have the native maps on its linker path. It's ability, or inability, to load native maps has no bearing on whether the tserver process that mini launches will be able to do so. They have different environments.\r\n\r\n> `ConfigurableMacBase.createMiniAccumulo` sets `TSERV_NATIVEMAP_ENABLED` to true and then starts MAC.\r\n\r\nYes, and the way this works is that `MiniAccumuloClusterImpl` sets the `LD_LIBRARY_PATH` in the environment for the tserver's `Process`. Nothing does that for the test thread's environment. There's no reason to expect it to be able to load native maps."", 'commenter': 'ctubbsii'}, {'comment': ""After fixing the bug where the config was being checked to see if the native maps were enabled (using `Property.name()` instead of `Property.getKey()`), I was able to confirm that this does, in fact, fail because the native maps can't be loaded in the test thread. `mvn clean verify -Dtest=blah -Dit.test=WALSunnyDayIT` is an easy one to use to reproduce this, since it extends `ConfigurableMacBase`, and doesn't disable the native maps.\r\n\r\nSo, this native map loading check in the test thread really needs to be removed for this to have any chance of verifyUp working properly."", 'commenter': 'ctubbsii'}, {'comment': 'I will remove this morning. I was waiting on the results of an IT run after my changes in 3f7968f. In an IT run from the commit, only one test failed and it was unrelated to verifyUp.', 'commenter': 'dlmarion'}, {'comment': 'This inspired me to create #2425 , which will accomplish what you were trying to do inside Mini, but does it inside ConfigurableMacBase, among other improvements. I think there\'s still something that can be done here, though.\r\n\r\nThis could become something like:\r\n\r\n```java\r\n    if (Boolean.valueOf(config.getSiteConfig().get(Property.TSERV_NATIVEMAP_ENABLED.getKey()))\r\n        && config.getNativeLibPaths().length == 0\r\n        && !config.getSystemProperties().containsKey(""accumulo.native.lib.path"")) {\r\n      throw new RuntimeException(\r\n          ""MAC configured to use native maps, but native library path was not provided."");\r\n    }\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Removed in 0ffb6fc', 'commenter': 'dlmarion'}, {'comment': 'Regarding #2425, when I was looking at NativeMap, I was wondering why we first looked in `accumulo.native.lib.path` and then tried LD_LIBRARY_PATH as a fallback. It seems that this is another case where we are doing something non-standard. Why not just use LD_LIBRARY_PATH ?', 'commenter': 'dlmarion'}, {'comment': 'tl;dr there\'s good reasons behind it, but it\'s complicated.\r\n\r\nMessing with `LD_LIBRARY_PATH` isn\'t good practice. Ideally, the native library is installed by a package manager into `/usr/lib{64}/whatever` and can be loaded without messing with environment variables. If this is the case, then it should just be picked up automatically by `System.loadLibrary(""accumulo"")`. Messing with `LD_LIBRARY_PATH` directly is just a hack to get `System.loadLibrary()` to find it from a standalone ""tarball"" packaging.\r\n\r\nHowever, `System.loadLibrary()` isn\'t always recommended. In some systems, the file naming conventions won\'t match up to the name of the files. For example, on Macs, it can be a problem if the filename is `libaccumulo.jnilib` instead of `libaccumulo.dylib` or vice-versa. In those cases, it\'s not enough to specify the path. You have to specify the exact file. This is also the way it is supposed to be done for all JNI libs according to the Fedora/EPEL/RHEL packaging guidelines for Java. In such cases, you have to have some mechanism to specify the files directly, so you can use `System.load(filename)` instead of `System.loadLibrary(libname)`. Hence the property. We can inject specific filenames (but we also allow directories that we\'ll search) using the value of the Java system property `accumulo.native.lib.path`.', 'commenter': 'ctubbsii'}]"
2414,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java,"@@ -624,53 +640,80 @@ private void verifyUp() throws InterruptedException, IOException {
           ""Error starting TabletServer "" + tsExpectedCount + ""- instance not started"");
     }
 
-    var zrw = getServerContext().getZooReaderWriter();
-    String rootPath = getServerContext().getZooKeeperRoot();
+    Watcher w = new Watcher() {
+      public void process(WatchedEvent event) {
+        if (event.getState() == KeeperState.Expired) {
+          log.debug(""Session expired, state of current session : {}"", event.getState());
+        }
+      }
+    };","[{'comment': ""I don't think we need to register a watcher. This complicates things."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 3f7968f', 'commenter': 'dlmarion'}]"
2414,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java,"@@ -624,53 +640,80 @@ private void verifyUp() throws InterruptedException, IOException {
           ""Error starting TabletServer "" + tsExpectedCount + ""- instance not started"");
     }
 
-    var zrw = getServerContext().getZooReaderWriter();
-    String rootPath = getServerContext().getZooKeeperRoot();
+    Watcher w = new Watcher() {
+      public void process(WatchedEvent event) {
+        if (event.getState() == KeeperState.Expired) {
+          log.debug(""Session expired, state of current session : {}"", event.getState());
+        }
+      }
+    };
+    try (ZooKeeper zk = new ZooKeeper(getZooKeepers(), 60000, w)) {
 
-    int tsActualCount = 0;
-    int tryCount = 0;
-    try {
-      while (tsActualCount != tsExpectedCount) {
-        tryCount++;
-        tsActualCount = 0;
-        for (String child : zrw.getChildren(rootPath + Constants.ZTSERVERS)) {
-          tsActualCount++;
-          if (zrw.getChildren(rootPath + Constants.ZTSERVERS + ""/"" + child).isEmpty())
-            log.info(""TServer "" + tsActualCount + "" not yet present in ZooKeeper"");
+      String secret = getSiteConfiguration().get(Property.INSTANCE_SECRET);
+
+      for (int i = 0; i < numTries; i++) {
+        if (zk.getState().equals(States.CONNECTED)) {
+          zk.addAuthInfo(""digest"", (""accumulo"" + "":"" + secret).getBytes(UTF_8));","[{'comment': ""We should have a static utility method for constructing the auth info, so we don't have to do this more than one place."", 'commenter': 'ctubbsii'}, {'comment': ""Are you suggesting that something already exists for this, or that I should create it? I see where this is done in other tests. I don't see evidence of a utility."", 'commenter': 'dlmarion'}, {'comment': ""I'm suggesting we should make one, if we don't have one already. It can be done separately from this PR, though."", 'commenter': 'ctubbsii'}, {'comment': 'Did this in #2424 ', 'commenter': 'ctubbsii'}, {'comment': '```suggestion\r\n          ZooUtil.digestAuth(zk, secret);\r\n```', 'commenter': 'ctubbsii'}]"
2414,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java,"@@ -327,6 +331,12 @@ public MiniAccumuloClusterImpl(MiniAccumuloConfigImpl config) throws IOException
 
     this.config = config.initialize();
 
+    if (Boolean.valueOf(this.config.getSiteConfig().get(Property.TSERV_NATIVEMAP_ENABLED.name()))) {","[{'comment': ""Oh, I just realized. This is also buggy because you're using `.name()` instead of `.getKey()`. So, this will always return false. That's probably why you've never seen the native map failing to load that I mentioned in my other comment.\r\n\r\n```suggestion\r\n    if (Boolean.valueOf(config.getSiteConfig().get(Property.TSERV_NATIVEMAP_ENABLED.getKey()))) {\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Except that the code does work. I tested NativeMapIT in the IDE after running `mvn clean` on the filesystem and it failed.', 'commenter': 'dlmarion'}, {'comment': 'NativeMapIT is a special test. It sets a system property to load the native maps. It works differently than other mini tests.', 'commenter': 'ctubbsii'}]"
2414,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java,"@@ -608,6 +618,8 @@ public synchronized void start() throws IOException, InterruptedException {
 
   private void verifyUp() throws InterruptedException, IOException {
 
+    int numTries = 10;
+
     requireNonNull(getClusterControl().managerProcess, ""Error starting Manager - no process"");
     requireNonNull(getClusterControl().managerProcess.info().startInstant().get(),","[{'comment': 'I am not sure this error message will get thrown. If you look at the impl of `Optional.get()` it will throw its own exception:\r\n<pre>\r\n public T get() {\r\n        if (value == null) {\r\n            throw new NoSuchElementException(""No value present"");\r\n        }\r\n        return value;\r\n    }\r\n</pre>', 'commenter': 'milleruntime'}, {'comment': ""Yeah, it's a redundant check with optional."", 'commenter': 'ctubbsii'}, {'comment': 'We should change these to just check `Optional.isEmpty()` and throw an exception. The message ""No value present"" is not very helpful.', 'commenter': 'milleruntime'}, {'comment': 'I have a fix incoming that will also address #2431 ', 'commenter': 'ctubbsii'}]"
2417,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -761,16 +761,20 @@ CompressionCodec initCodec(final AtomicBoolean checked, final int bufferSize,
      */
     CompressionCodec createNewCodec(final String codecClazzProp, final String defaultClazz,
         final int bufferSize, final String bufferSizeConfigOpt) {
-      String extClazz =
-          (conf.get(codecClazzProp) == null ? System.getProperty(codecClazzProp) : null);
+      String extClazz = conf.get(codecClazzProp);
+      if (extClazz == null) {
+        extClazz = System.getProperty(codecClazzProp);
+      }
       String clazz = (extClazz != null) ? extClazz : defaultClazz;","[{'comment': ""I don't think this behaves the way it should. This would get the class from the config file, and only use the system property if it's not set in the config file. I think it should work the other way around. Setting the system property on the command-line, for example, should be able to override what is set in the config file.\r\n\r\n```suggestion\r\n      String clazz = System.getProperty(codecClazzProp, conf.get(codecClazzProp, defaultClazz));\r\n```\r\n\r\nAlso, this syntax is much more readable: try system props, fall back to config, fall back to default."", 'commenter': 'ctubbsii'}, {'comment': ""The behavior comes from the existing Accumulo code which didn't work and the Hadoop TFile example linked here: https://github.com/apache/hadoop/blob/e103c83765898f756f88c27b2243c8dd3098a989/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/Compression.java#L88 - It is a bit backwards I agree and may be more Hadoop semantics rather than Accumulo.  I would agree with your approach."", 'commenter': 'trietopsoft'}, {'comment': 'Yeah, the simpler version of that implementation would look like:\r\n\r\n```java\r\n      String clazz = conf.get(codecClazzProp, System.getProperty(codecClazzProp, defaultClazz));\r\n```\r\n\r\nBut, that would make the value in the config file override a java property on the command-line. And, I think it should go the other way around. The command-line is should take precedence.', 'commenter': 'ctubbsii'}]"
2417,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -761,16 +761,20 @@ CompressionCodec initCodec(final AtomicBoolean checked, final int bufferSize,
      */
     CompressionCodec createNewCodec(final String codecClazzProp, final String defaultClazz,
         final int bufferSize, final String bufferSizeConfigOpt) {
-      String extClazz =
-          (conf.get(codecClazzProp) == null ? System.getProperty(codecClazzProp) : null);
+      String extClazz = conf.get(codecClazzProp);
+      if (extClazz == null) {
+        extClazz = System.getProperty(codecClazzProp);
+      }
       String clazz = (extClazz != null) ? extClazz : defaultClazz;
       try {
         log.info(""Trying to load codec class {} for {}"", clazz, codecClazzProp);
         Configuration config = new Configuration(conf);
         updateBuffer(config, bufferSizeConfigOpt, bufferSize);
         return (CompressionCodec) ReflectionUtils.newInstance(Class.forName(clazz), config);
       } catch (ClassNotFoundException e) {
-        // This is okay.
+        // This is not okay.
+        log.warn(""Unable to load codec class {} for {}, reason: {}"", clazz, codecClazzProp,
+            e.getMessage());","[{'comment': '```suggestion\r\n        log.warn(""Unable to load codec class {} for {}"", clazz, codecClazzProp, e);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'This was discussed here: https://github.com/apache/accumulo/pull/2417#issuecomment-1013312101 The exception stacktrace is not required since having the message is sufficient.', 'commenter': 'trietopsoft'}, {'comment': ""Varargs doesn't capture the exception object and it is unbound in your requested change."", 'commenter': 'trietopsoft'}, {'comment': ""I don't see harm in having a stack trace in the logs with a warning. I would leave it unbound so it does the stack trace, primarily because I think the code looks cleaner, but could go either way if somebody felt strongly about it."", 'commenter': 'ctubbsii'}]"
2417,core/src/test/java/org/apache/accumulo/core/file/rfile/bcfile/CompressionTest.java,"@@ -119,7 +126,17 @@ public void testSupport() {
     } catch (ClassNotFoundException e) {
       // that is okay
     }
+  }
 
+  @After
+  public void restoreSysProps() {
+    // Restore system property for next test
+    Algorithm.conf.clear();
+    if (extLz4 != null) {
+      System.setProperty(Compression.Algorithm.CONF_LZ4_CLASS, extLz4);
+    } else {
+      System.clearProperty(Compression.Algorithm.CONF_LZ4_CLASS);
+    }","[{'comment': ""We run our unit tests in parallel and reuse JVM forks to do so. Messing with system properties like this could make these tests not thread safe. But, I'm not sure."", 'commenter': 'ctubbsii'}, {'comment': 'Parallel tests may cause issues here.  I can back these tests out.', 'commenter': 'trietopsoft'}]"
2417,core/src/test/java/org/apache/accumulo/core/file/rfile/bcfile/CompressionTest.java,"@@ -282,4 +299,27 @@ public void testThereCanBeOnlyOne() throws InterruptedException, ExecutionExcept
     }
   }
 
+  @Test
+  public void testHadoopCodecOverride() {
+    Algorithm.conf.set(Compression.Algorithm.CONF_LZ4_CLASS, DummyCodec.class.getName());
+    CompressionCodec dummyCodec = Compression.Algorithm.LZ4.createNewCodec(4096);
+    assertEquals(""Hadoop override DummyCodec not loaded"", DummyCodec.class, dummyCodec.getClass());","[{'comment': ""These tests seem a bit heavyweight for what they are effectively testing. They are effectively testing that we can read the class name from the config files and/or system property. We don't actually need to create the codec to test that. These tests can probably be made more lightweight. However, given the triviality of my proposed one-liner above to get the class name, I'm not sure there's much to test here at all."", 'commenter': 'ctubbsii'}, {'comment': 'Eh, for someone who had to chase down this bug, I would typically argue for more test cases and code coverage, but I get your point.', 'commenter': 'trietopsoft'}, {'comment': ""Given the potential problems with JVM reuse and setting system properties, I'd probably just remove the test in this PR, but I agree with you in the general case about more unit testing for greater code coverage.\r\nYeah, I just don't want us to get to the point in our testing eagerness that we're doing things like: `assertTrue(Set.of(x).contains(x));`\r\nAt a certain point, we're not even testing Accumulo code, but that `conf.get()` and `System.getProperty()` work correctly."", 'commenter': 'ctubbsii'}]"
2426,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -111,6 +111,31 @@
  * {@link #dropSortedMapWALFiles(ServerContext)}. For more information see the following issues:
  * <a href=""https://github.com/apache/accumulo/issues/2117"">#2117</a> and
  * <a href=""https://github.com/apache/accumulo/issues/2179"">#2179</a>
+ *
+ * The following methods were created for External compactions:
+ * {@link #createExternalCompactionNodes(ServerContext)}, {@link #setMetaTableProps(ServerContext)}
+ *
+ * Improvements to the metadata and root tables were made in this version. See pull request
+ * <a href=""https://github.com/apache/accumulo/pull/1174"">#1174</a> for more details. The
+ * {@link #upgradeRootTabletMetadata(ServerContext)} method was added for this change. This change
+ * allowed other changes to how volumes were stored in the metadata and have Accumulo always call
+ * the volume chooser for new tablet files. This change was facilitated with the
+ * {@link #upgradeDirColumns(ServerContext, Ample.DataLevel)} method and done in
+ * <a href=""https://github.com/apache/accumulo/pull/1389"">#1389</a>
+ *
+ * The method {@link #upgradeRelativePaths(ServerContext, Ample.DataLevel)} was added for resolving
+ * and replacing all relative tablet file paths found in metadata tables with absolute paths during
+ * upgrade. Absolute paths are resolved by prefixing relative paths with a volume configured by the
+ * user in a the instance.volumes.upgrade.relative property, which is only used during an upgrade.","[{'comment': '```suggestion\r\n * user in the instance.volumes.upgrade.relative property, which is only used during an upgrade.\r\n```', 'commenter': 'DomGarguilo'}]"
2426,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -111,6 +111,31 @@
  * {@link #dropSortedMapWALFiles(ServerContext)}. For more information see the following issues:
  * <a href=""https://github.com/apache/accumulo/issues/2117"">#2117</a> and
  * <a href=""https://github.com/apache/accumulo/issues/2179"">#2179</a>
+ *","[{'comment': 'If these blank lines are supposed to represent new paragraphs, then they need to be done as HTML.\r\n', 'commenter': 'ctubbsii'}]"
2426,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -111,6 +111,31 @@
  * {@link #dropSortedMapWALFiles(ServerContext)}. For more information see the following issues:
  * <a href=""https://github.com/apache/accumulo/issues/2117"">#2117</a> and
  * <a href=""https://github.com/apache/accumulo/issues/2179"">#2179</a>
+ *
+ * The following methods were created for External compactions:
+ * {@link #createExternalCompactionNodes(ServerContext)}, {@link #setMetaTableProps(ServerContext)}","[{'comment': ""A lot of this is very verbose and could be eliminated to streamlined. Some of the verbosity that explains how things changed over time can be removed. The javadoc should document the current code, not the previous code or the changes. Changes are documented in the change log / git history. In this case, the above comment is explaining the reason why these methods were added, but the purpose of those methods should be in the javadoc for those specific methods. They don't need to be here at the class level."", 'commenter': 'ctubbsii'}, {'comment': 'I disagree. The upgrade code is specifically about past changes to code and is not clear what methods are doing what. But this documentation was more meant to be a bridge for to assist admins performing upgrades. It is a lot easier for an admin to find the line in the code an exception refers to (something they can even do in the browser) then to track git history. Do you have any specific suggestions?', 'commenter': 'milleruntime'}]"
2426,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -111,6 +111,31 @@
  * {@link #dropSortedMapWALFiles(ServerContext)}. For more information see the following issues:
  * <a href=""https://github.com/apache/accumulo/issues/2117"">#2117</a> and
  * <a href=""https://github.com/apache/accumulo/issues/2179"">#2179</a>
+ *
+ * The following methods were created for External compactions:
+ * {@link #createExternalCompactionNodes(ServerContext)}, {@link #setMetaTableProps(ServerContext)}
+ *
+ * Improvements to the metadata and root tables were made in this version. See pull request
+ * <a href=""https://github.com/apache/accumulo/pull/1174"">#1174</a> for more details. The","[{'comment': ""If the details don't matter for understanding the current code, we probably don't need this here at all. The git history should suffice."", 'commenter': 'ctubbsii'}, {'comment': 'I disagree. See my other comment.', 'commenter': 'milleruntime'}]"
2426,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -111,6 +111,31 @@
  * {@link #dropSortedMapWALFiles(ServerContext)}. For more information see the following issues:
  * <a href=""https://github.com/apache/accumulo/issues/2117"">#2117</a> and
  * <a href=""https://github.com/apache/accumulo/issues/2179"">#2179</a>
+ *
+ * The following methods were created for External compactions:
+ * {@link #createExternalCompactionNodes(ServerContext)}, {@link #setMetaTableProps(ServerContext)}
+ *
+ * Improvements to the metadata and root tables were made in this version. See pull request
+ * <a href=""https://github.com/apache/accumulo/pull/1174"">#1174</a> for more details. The
+ * {@link #upgradeRootTabletMetadata(ServerContext)} method was added for this change. This change
+ * allowed other changes to how volumes were stored in the metadata and have Accumulo always call
+ * the volume chooser for new tablet files. This change was facilitated with the
+ * {@link #upgradeDirColumns(ServerContext, Ample.DataLevel)} method and done in
+ * <a href=""https://github.com/apache/accumulo/pull/1389"">#1389</a>","[{'comment': ""Again, we don't need to document the changes here. If anything here about these methods is useful, it would be more useful on the actual methods, instead of at the class level."", 'commenter': 'ctubbsii'}, {'comment': 'I can move javadoc to the methods. But I was trying to link the methods at a higher level and some of them have multiple methods and called over the different upgrade steps.', 'commenter': 'milleruntime'}]"
2426,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -105,12 +105,6 @@
  * <a href=""https://github.com/apache/accumulo/issues/1642"">#1642</a>, and
  * <a href=""https://github.com/apache/accumulo/issues/1643"">#1643</a> as well.</li>
  * </ul>
- *
- * Sorted recovery was updated to use RFiles instead of map files. So to prevent issues during
- * tablet recovery, remove the old temporary map files and resort using RFiles. This is done in
- * {@link #dropSortedMapWALFiles(ServerContext)}. For more information see the following issues:
- * <a href=""https://github.com/apache/accumulo/issues/2117"">#2117</a> and
- * <a href=""https://github.com/apache/accumulo/issues/2179"">#2179</a>","[{'comment': 'This was probably fine to leave here, because it described the overall behavior of the Upgrader class.', 'commenter': 'ctubbsii'}]"
2427,pom.xml,"@@ -610,6 +605,16 @@
         <artifactId>jline</artifactId>
         <version>3.20.0</version>
       </dependency>
+      <dependency>
+        <groupId>org.junit.jupiter</groupId>
+        <artifactId>junit-jupiter-api</artifactId>
+        <version>5.8.2</version>
+      </dependency>
+      <dependency>
+        <groupId>org.junit.vintage</groupId>
+        <artifactId>junit-vintage-engine</artifactId>
+        <version>5.8.2</version>
+      </dependency>","[{'comment': 'It might be possible to manage all the junit dependencies with a single bom:\r\n\r\n```suggestion\r\n      <dependency>\r\n        <groupId>org.junit</groupId>\r\n        <artifactId>junit-bom</artifactId>\r\n        <version>5.8.2</version>\r\n        <type>pom</type>\r\n        <scope>import</scope>\r\n      </dependency>\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'addressed in [466348b](https://github.com/apache/accumulo/pull/2427/commits/466348b9a2e23f776be47c54b236545d2b62805c)', 'commenter': 'DomGarguilo'}]"
2427,core/src/test/java/org/apache/accumulo/core/client/ZooKeeperInstanceTest.java,"@@ -128,33 +129,33 @@ public void testGetInstanceID_Direct() {
     assertEquals(IID_STRING, zki.getInstanceID());
   }
 
-  @Test(expected = RuntimeException.class)
+  @Test
   public void testGetInstanceID_NoMapping() {
     ClientConfiguration config = createMock(ClientConfiguration.class);
     expect(zc.get(Constants.ZROOT + Constants.ZINSTANCES + ""/instance"")).andReturn(null);
     replay(zc);
     EasyMock.reset(config, zcf);
-    new ZooKeeperInstance(config, zcf);
+    assertThrows(RuntimeException.class, () -> new ZooKeeperInstance(config, zcf));","[{'comment': 'FWIW, the current version of JUnit supports this. This is something that we can easily apply to all our code today, and it would make this migration go more smoothly, at least for the code review portion.', 'commenter': 'ctubbsii'}, {'comment': 'Do you think it would be a good idea to make a separate PR and take care of all of these cases first? Might make sense since even if we decide to not upgrade to JUnit 5 for whatever reason, that could still be a working improvement for our JUnit 4 tests.', 'commenter': 'DomGarguilo'}, {'comment': 'Yes. These would be good to have first. Then, you can rebase this onto that, in order to make this migration smaller and easier to review.', 'commenter': 'ctubbsii'}, {'comment': 'addressed in #2435', 'commenter': 'DomGarguilo'}]"
2427,core/src/test/java/org/apache/accumulo/core/client/ZooKeeperInstanceTest.java,"@@ -22,7 +22,8 @@
 import static org.easymock.EasyMock.createMock;
 import static org.easymock.EasyMock.expect;
 import static org.easymock.EasyMock.replay;
-import static org.junit.Assert.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertThrows;","[{'comment': 'It would be very useful if all of our uses of `assert*()` methods were already using static imports. If that were done already in our existing code, it would make this migration go more smoothly, because only the import statement would change, which makes reviewing easier. We may already be using static imports for these everywhere, though.', 'commenter': 'ctubbsii'}, {'comment': 'I think it may already be the case that all `assert*()` methods are static imports', 'commenter': 'DomGarguilo'}, {'comment': 'You are right. It looks like I added some checkstyle rules for that awhile back. These rules will need to be updated at some point before this migration is fully done:\r\n\r\nhttps://github.com/apache/accumulo/blob/912292d2131dbaf0f9cc28aff464608b478bf362/pom.xml#L1084-L1091', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/client/mapred/AccumuloMultiTableInputFormatTest.java,"@@ -29,24 +29,20 @@
 import org.apache.accumulo.core.util.Pair;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.JobConf;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.TestInfo;
 
 @Deprecated(since = ""2.0.0"")
 public class AccumuloMultiTableInputFormatTest {
 
-  @Rule
-  public TestName testName = new TestName();
-
   /**
    * Verify {@link org.apache.accumulo.core.client.mapreduce.InputTableConfig} objects get correctly
    * serialized in the JobContext.
    */
   @Test
-  public void testTableQueryConfigSerialization() {
-    String table1Name = testName.getMethodName() + ""1"";
-    String table2Name = testName.getMethodName() + ""2"";
+  public void testTableQueryConfigSerialization(TestInfo testInfo) {
+    String table1Name = testInfo.getDisplayName() + ""1"";","[{'comment': ""The display name can contain invalid characters (commas) that aren't suitable for table names. It's better to use the `.getTestMethod().get()` (or some other variant that ensures it's not null, like `.getTestMethod().orElseThrow(IllegalStateException::new)`)."", 'commenter': 'ctubbsii'}, {'comment': 'Good point. I was looking for a good replacement for `.getMethodName()` and from some quick testing after your suggestions, it looks like `testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName()` might be a suitable replacement.', 'commenter': 'DomGarguilo'}, {'comment': 'That\'s a bit bulky to replace everywhere. I\'d put a static method somewhere. Maybe in `AccumuloITBase`:\r\n\r\n```java\r\n  public static String testMethodName(TestInfo testInfo) {\r\n    return testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName();\r\n  }\r\n```\r\n\r\nThat way, you can do:\r\n\r\n```java\r\nimport static org.apache.accumulo.harness.AccumuloITBase.testMethodName;\r\n// ...\r\n@Test\r\npublic void someTestName(TestInfo testInfo) {\r\n  String table1Name = testMethodName(testInfo) + ""1"";\r\n  // ...\r\n}\r\n\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""I like this idea. I think I'll forgo creating that method until I work on that module (accumulo-test, where AccumuloITBase resides) but I have left a note about creating the method and using it in the TODO section on ticket #2441"", 'commenter': 'DomGarguilo'}]"
2427,iterator-test-harness/pom.xml,"@@ -47,6 +42,11 @@
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-client-api</artifactId>
     </dependency>
+    <!--TODO Don't force downstream users to have JUnit -->","[{'comment': ""Can just remove this comment. There's no problem having JUnit as a dependency."", 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/fate/util/RetryTest.java,"@@ -276,8 +276,8 @@ public void testMaxWait() {
     builder.maxWait(15, MILLISECONDS);
     builder.maxWait(16, MILLISECONDS);
 
-    assertThrows(""Max wait time should be greater than or equal to initial wait time"",
-        IllegalArgumentException.class, () -> builder.maxWait(14, MILLISECONDS));
+    assertThrows(IllegalArgumentException.class, () -> builder.maxWait(14, MILLISECONDS),
+        ""Max wait time "" + ""should be greater than or equal to initial wait time"");","[{'comment': '```suggestion\r\n        ""Max wait time should be greater than or equal to initial wait time"");\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/fate/util/RetryTest.java,"@@ -96,8 +96,8 @@ public void usingNonExistentRetryFails() {
       retry.useRetry();
     }
     assertFalse(retry.canRetry());
-    assertThrows(""Calling useRetry when canRetry returns false throws an exception"",
-        IllegalStateException.class, () -> retry.useRetry());
+    assertThrows(IllegalStateException.class, () -> retry.useRetry(),
+        ""Calling useRetry when canRetry "" + ""returns false throws an exception"");","[{'comment': '```suggestion\r\n        ""Calling useRetry when canRetry returns false throws an exception"");\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/spi/balancer/HostRegexTableLoadBalancerTest.java,"@@ -79,9 +79,10 @@ public void init(Map<String,String> tableProperties) {
   @Test
   public void testInit() {
     init(DEFAULT_TABLE_PROPERTIES);
-    assertEquals(""OOB check interval value is incorrect"", 7000, this.getOobCheckMillis());
-    assertEquals(""Max migrations is incorrect"", 4, this.getMaxMigrations());
-    assertEquals(""Max outstanding migrations is incorrect"", 10, this.getMaxOutstandingMigrations());
+    assertEquals(7000, this.getOobCheckMillis(), ""OOB check interval value is incorrect"");
+    assertEquals(4, this.getMaxMigrations(), ""Max migrations is incorrect"");
+    assertEquals(10, this.getMaxOutstandingMigrations(),
+        ""Max outstanding migrations is "" + ""incorrect"");","[{'comment': '```suggestion\r\n    assertEquals(10, this.getMaxOutstandingMigrations(), ""Max outstanding migrations is incorrect"");\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/security/CredentialsTest.java,"@@ -35,20 +36,17 @@
 import org.apache.accumulo.core.clientImpl.Credentials;
 import org.apache.accumulo.core.data.InstanceId;
 import org.apache.accumulo.core.securityImpl.thrift.TCredentials;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.TestInfo;
 
 public class CredentialsTest {
 
-  @Rule
-  public TestName test = new TestName();
-
   private InstanceId instanceID;","[{'comment': ""Could get rid of this member variable and replace it with a function, to make it easier to create the local variables in each test:\r\n\r\n(don't need to `orElseThrow`, because it will already throw `NoSuchElementException`)\r\n```suggestion\r\n  private Function<TestInfo, InstanceId> testInfoToInstanceId = testInfo -> InstanceId.of(testInfo.getTestMethod().get().getName());\r\n```"", 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/security/CredentialsTest.java,"@@ -35,20 +36,17 @@
 import org.apache.accumulo.core.clientImpl.Credentials;
 import org.apache.accumulo.core.data.InstanceId;
 import org.apache.accumulo.core.securityImpl.thrift.TCredentials;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.TestInfo;
 
 public class CredentialsTest {
 
-  @Rule
-  public TestName test = new TestName();
-
   private InstanceId instanceID;
 
   @Test
-  public void testToThrift() throws DestroyFailedException {
-    instanceID = InstanceId.of(test.getMethodName());
+  public void testToThrift(TestInfo testInfo) throws DestroyFailedException {
+    instanceID =
+        InstanceId.of(testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName());","[{'comment': 'To use the aforementioned Function inside each of these methods:\r\n\r\n```suggestion\r\n    var instanceID = testInfoToInstanceId.apply(testInfo);\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/file/rfile/bcfile/CompressionTest.java,"@@ -267,16 +271,16 @@ public void testThereCanBeOnlyOne() throws InterruptedException, ExecutionExcept
 
         results.addAll(service.invokeAll(list));
         // ensure that we
-        assertEquals(al + "" created too many codecs"", 1, testSet.size());
+        assertEquals(1, testSet.size(), al + "" created too many codecs"");
         service.shutdown();
 
         while (!service.awaitTermination(1, TimeUnit.SECONDS)) {
           // wait
         }
 
         for (Future<Boolean> result : results) {
-          assertTrue(al + "" resulted in a failed call to getcodec within the thread pool"",
-              result.get());
+          assertTrue(result.get(),
+              al + "" resulted in a failed call to getcodec within the thread "" + ""pool"");","[{'comment': '```suggestion\r\n              al + "" resulted in a failed call to getcodec within the thread pool"");\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/file/BloomFilterLayerLookupTest.java,"@@ -84,7 +80,9 @@ public void test() throws IOException {
 
     // get output file name
     String suffix = FileOperations.getNewFileExtension(acuconf);
-    String fname = new File(tempDir.getRoot(), testName + ""."" + suffix).getAbsolutePath();
+    String fname = new File(tempDir,
+        testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName() + ""."" + suffix)","[{'comment': 'Already throws NoSuchElementException on .get if empty:\r\n```suggestion\r\n    String fname = new File(tempDir, testInfo.getTestMethod().get().getName() + ""."" + suffix)\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/data/TableIdTest.java,"@@ -83,15 +81,17 @@ public void testCacheNoDuplicates() {
     assertSame(table1, table2);
   }
 
-  @Test(timeout = 30_000)
-  public void testCacheIncreasesAndDecreasesAfterGC() {
+  @Test
+  @Timeout(30_000)
+  public void testCacheIncreasesAndDecreasesAfterGC(TestInfo testInfo) {
     long initialSize = cacheCount();
     assertTrue(initialSize < 20); // verify initial amount is reasonably low
     LOG.info(""Initial cache size: {}"", initialSize);
     LOG.info(TableId.cache.asMap().toString());
 
     // add one and check increase
-    String tableString = ""table-"" + name.getMethodName();
+    String tableString =
+        ""table-"" + testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName();","[{'comment': '```suggestion\r\n    String tableString = ""table-"" + testInfo.getTestMethod().get().getName();\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/data/TableIdTest.java,"@@ -61,7 +58,8 @@ public void testCacheNoDuplicates() {
     assertNotSame(RootTable.ID, MetadataTable.ID);
     assertNotSame(RootTable.ID, REPL_TABLE_ID);
 
-    String tableString = ""table-"" + name.getMethodName();
+    String tableString =
+        ""table-"" + testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName();","[{'comment': '```suggestion\r\n    String tableString = ""table-"" + testInfo.getTestMethod().get().getName();\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/data/NamespaceIdTest.java,"@@ -71,15 +69,17 @@ public void testCacheNoDuplicates() {
     assertSame(nsId, nsId2);
   }
 
-  @Test(timeout = 30_000)
-  public void testCacheIncreasesAndDecreasesAfterGC() {
+  @Test
+  @Timeout(30_000)
+  public void testCacheIncreasesAndDecreasesAfterGC(TestInfo testInfo) {
     long initialSize = cacheCount();
     assertTrue(initialSize < 20); // verify initial amount is reasonably low
     LOG.info(""Initial cache size: {}"", initialSize);
     LOG.info(NamespaceId.cache.asMap().toString());
 
     // add one and check increase
-    String namespaceString = ""namespace-"" + name.getMethodName();
+    String namespaceString =
+        ""namespace-"" + testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName();","[{'comment': '```suggestion\r\n    String namespaceString = ""namespace-"" + testInfo.getTestMethod().get().getName();\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/data/NamespaceIdTest.java,"@@ -37,22 +37,20 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(NamespaceIdTest.class);
 
-  @Rule
-  public TestName name = new TestName();
-
   private static long cacheCount() {
     // guava cache size() is approximate, and can include garbage-collected entries
     // so we iterate to get the actual cache size
     return NamespaceId.cache.asMap().entrySet().stream().count();
   }
 
   @Test
-  public void testCacheNoDuplicates() {
+  public void testCacheNoDuplicates(TestInfo testInfo) {
     // the next line just preloads the built-ins, since they now exist in a separate class from
     // NamespaceId, and aren't preloaded when the NamespaceId class is referenced
     assertNotSame(Namespace.ACCUMULO.id(), Namespace.DEFAULT.id());
 
-    String namespaceString = ""namespace-"" + name.getMethodName();
+    String namespaceString =
+        ""namespace-"" + testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName();","[{'comment': '```suggestion\r\n    String namespaceString = ""namespace-"" + testInfo.getTestMethod().get().getName();\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/conf/PropertyTypeTest.java,"@@ -18,38 +18,39 @@
  */
 package org.apache.accumulo.core.conf;
 
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
 import java.lang.reflect.Method;
 import java.util.Arrays;
 import java.util.List;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.DisplayNameGeneration;
+import org.junit.jupiter.api.DisplayNameGenerator;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.TestInfo;
 
 import com.google.common.base.Joiner;
 
+@DisplayNameGeneration(DisplayNameGenerator.Simple.class)
 public class PropertyTypeTest {
 
-  @Rule
-  public TestName testName = new TestName();
-  private PropertyType type = null;
+  private PropertyType type;
 
-  @Before
-  public void getPropertyTypeForTest() {
-    String tn = testName.getMethodName();
-    if (tn.startsWith(""testType"")) {
+  @BeforeEach
+  public void getPropertyTypeForTest(TestInfo testInfo) {
+    String displayName = testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName();","[{'comment': '```suggestion\r\n    String displayName = testInfo.getTestMethod().get().getName();\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/conf/PropertyTypeTest.java,"@@ -18,38 +18,39 @@
  */
 package org.apache.accumulo.core.conf;
 
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
 import java.lang.reflect.Method;
 import java.util.Arrays;
 import java.util.List;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.DisplayNameGeneration;
+import org.junit.jupiter.api.DisplayNameGenerator;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.TestInfo;
 
 import com.google.common.base.Joiner;
 
+@DisplayNameGeneration(DisplayNameGenerator.Simple.class)","[{'comment': ""Do we need to use this display name generator? What's wrong with the default?"", 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/conf/DeprecatedPropertyUtilTest.java,"@@ -89,9 +89,9 @@ public void testSanityCheckManagerProperties() {
     DeprecatedPropertyUtil.sanityCheckManagerProperties(config); // should succeed
     config.setProperty(""manager.replacementProp"", ""value"");
     assertEquals(4, config.size());
-    assertThrows(""Sanity check should fail when 'master.*' and 'manager.*' appear in same config"",
-        IllegalStateException.class,
-        () -> DeprecatedPropertyUtil.sanityCheckManagerProperties(config));
+    assertThrows(IllegalStateException.class,
+        () -> DeprecatedPropertyUtil.sanityCheckManagerProperties(config),
+        ""Sanity check should "" + ""fail when 'master.*' and 'manager.*' appear in same config"");","[{'comment': '```suggestion\r\n        ""Sanity check should fail when \'master.*\' and \'manager.*\' appear in same config"");\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/client/mapreduce/AccumuloMultiTableInputFormatTest.java,"@@ -30,23 +30,21 @@
 import org.apache.accumulo.core.util.Pair;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Job;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.TestInfo;
 
 @Deprecated(since = ""2.0.0"")
 public class AccumuloMultiTableInputFormatTest {
 
-  @Rule
-  public TestName testName = new TestName();
-
   /**
    * Verify {@link InputTableConfig} objects get correctly serialized in the JobContext.
    */
   @Test
-  public void testInputTableConfigSerialization() throws IOException {
-    String table1 = testName.getMethodName() + ""1"";
-    String table2 = testName.getMethodName() + ""2"";
+  public void testInputTableConfigSerialization(TestInfo testInfo) throws IOException {
+    String table1 =
+        testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName() + ""1"";
+    String table2 =
+        testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName() + ""2"";","[{'comment': '```suggestion\r\n    String table1 = testInfo.getTestMethod().get().getName() + ""1"";\r\n    String table2 = testInfo.getTestMethod().get().getName() + ""2"";\r\n```', 'commenter': 'ctubbsii'}]"
2427,core/src/test/java/org/apache/accumulo/core/client/mapred/AccumuloMultiTableInputFormatTest.java,"@@ -29,24 +29,22 @@
 import org.apache.accumulo.core.util.Pair;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.JobConf;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.TestInfo;
 
 @Deprecated(since = ""2.0.0"")
 public class AccumuloMultiTableInputFormatTest {
 
-  @Rule
-  public TestName testName = new TestName();
-
   /**
    * Verify {@link org.apache.accumulo.core.client.mapreduce.InputTableConfig} objects get correctly
    * serialized in the JobContext.
    */
   @Test
-  public void testTableQueryConfigSerialization() {
-    String table1Name = testName.getMethodName() + ""1"";
-    String table2Name = testName.getMethodName() + ""2"";
+  public void testTableQueryConfigSerialization(TestInfo testInfo) {
+    String table1Name =
+        testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName() + ""1"";
+    String table2Name =
+        testInfo.getTestMethod().orElseThrow(IllegalStateException::new).getName() + ""2"";","[{'comment': '```suggestion\r\n    String table1Name = testInfo.getTestMethod().get().getName() + ""1"";\r\n    String table2Name = testInfo.getTestMethod().get().getName() + ""2"";\r\n```', 'commenter': 'ctubbsii'}]"
2445,core/src/main/java/org/apache/accumulo/core/util/HostAndPort.java,"@@ -143,7 +143,7 @@ public static HostAndPort fromParts(String host, int port) {
    *           if nothing meaningful could be parsed.
    */
   public static HostAndPort fromString(String hostPortString) {
-    hostPortString = java.util.Objects.requireNonNull(hostPortString);
+    java.util.Objects.requireNonNull(hostPortString);","[{'comment': 'Maybe add the optional message stating that hostPortString was null?  The way that the NPE shows up in the logs, adding the message seems to help figuring things out.', 'commenter': 'EdColeman'}, {'comment': 'Do we really need to avoid the import statement and use the fully-qualified class name here?\r\n```suggestion\r\n    Objects.requireNonNull(hostPortString);\r\n```', 'commenter': 'ctubbsii'}]"
2447,pom.xml,"@@ -1622,5 +1623,62 @@
         <surefire.reuseForks>${reuseForks}</surefire.reuseForks>
       </properties>
     </profile>
+    <profile>
+      <!-- This profile uses the Google errorprone tool to perform static code analysis at
+      compile time. Auto-generated code is not checked.
+      See: https://errorprone.info/bugpatterns for list of available bug patterns.-->
+      <id>errorprone</id>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-compiler-plugin</artifactId>
+            <configuration>
+              <showWarnings>true</showWarnings>
+              <source>${maven.compiler.source}</source>
+              <source>${maven.compiler.target}}</source>
+              <encoding>UTF-8</encoding>","[{'comment': ""These are already set in the main config, so they don't need to be set here. The rest of the config here should be merged with that.\r\n```suggestion\r\n```"", 'commenter': 'ctubbsii'}]"
2447,pom.xml,"@@ -1622,5 +1623,62 @@
         <surefire.reuseForks>${reuseForks}</surefire.reuseForks>
       </properties>
     </profile>
+    <profile>
+      <!-- This profile uses the Google errorprone tool to perform static code analysis at
+      compile time. Auto-generated code is not checked.
+      See: https://errorprone.info/bugpatterns for list of available bug patterns.-->
+      <id>errorprone</id>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-compiler-plugin</artifactId>
+            <configuration>
+              <showWarnings>true</showWarnings>
+              <source>${maven.compiler.source}</source>
+              <source>${maven.compiler.target}}</source>
+              <encoding>UTF-8</encoding>
+              <compilerArgs>
+                <arg>-XDcompilePolicy=simple</arg>
+                <arg>
+                  -Xplugin:ErrorProne \
+                  -XepExcludedPaths:.*/(proto|thrift|generated-sources)/.* \
+                  -XepDisableWarningsInGeneratedCode \
+                  -XepDisableAllWarnings \
+                  <!-- error/warning patterns to ignore -->
+                  -Xep:Incomparable:OFF \
+                  -Xep:CheckReturnValue:OFF \
+                  -Xep:MustBeClosedChecker:OFF \
+                  -Xep:ReturnValueIgnored:OFF \
+                  <!-- error/warning patterns to specifically check -->
+                  -Xep:ExpectedExceptionChecker \
+                  -Xep:MissingOverride \
+                  <!-- Items containing 'OFF' are currently flagged by errorprone. The 'OFF'
+                  can be removed and the project recompiled to discover lcation of errors for
+                  further analysis. @SuppressWarnings can be used to ignore errors if desired. -->
+                </arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED</arg>
+                <arg>-J--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED</arg>
+                <arg>-J--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED</arg>
+              </compilerArgs>
+              <annotationProcessorPaths>
+                <path>
+                  <groupId>com.google.errorprone</groupId>
+                  <artifactId>error_prone_core</artifactId>
+                  <version>${errorprone.version}</version>
+                </path>
+              </annotationProcessorPaths>","[{'comment': 'When you set the annotation processor paths explicitly, does it prevent the automatic annotation processors from being picked up for the auto-service?', 'commenter': 'ctubbsii'}, {'comment': ""After reading the docs a little more, it looks like this would cause issues if other annotations processor are used. It would no longer scan the compile classpath for annotation processors. They would have to be added using the  annotationProcessorPaths argument. \r\n\r\nGiven that could be an issue, I'm ok closing this ticket without merging as it would be easy enough for me to run the processor every month or so on my own to see if any uncaught issues have been introduced into the codebase."", 'commenter': 'jmark99'}, {'comment': ""Couldn't you just add this artifact as an `<optional>true</optional>` dependency in this profile, so it gets picked up alongside the other annotation processors, rather than being explicitly set and overriding the others?"", 'commenter': 'ctubbsii'}, {'comment': 'Or, you could modify our other annotation processors to be explicitly run like you did with this one, rather than used as a dependency. Then these configs could be merged when the profile is activated.', 'commenter': 'ctubbsii'}, {'comment': ""Does it look like this is feasible to include? I'm finding this tool helpful."", 'commenter': 'DomGarguilo'}, {'comment': ""@DomGarguilo I've upgraded the version from 1.11 to 1.12. I added an additional exception so that non-ASCII Unicode in method names is not treated as an error. I also added the spotbugs annotation processor into the profile. If @ctubbsii can take a look and see if the POM is viable I think it can be included. \r\n\r\nFor the moment, I would prefer to keep it as a profile rather than directly adding into the default build until I can better understand if it is using a separate java compiler. The documentation does make that distinction clear - at least as far as I can find so far. Also, adding to the default build will take some work to address various warnings so that the profile will not become overly large due to its aggresive nature. As it is set up now, errors are found unless specifically turned off. But warnings are off by default unless specifically activated. We can add specific warnings of interest as we go forward. "", 'commenter': 'jmark99'}]"
2447,pom.xml,"@@ -1622,5 +1623,58 @@
         <surefire.reuseForks>${reuseForks}</surefire.reuseForks>
       </properties>
     </profile>
+    <profile>
+      <!-- This profile uses the Google errorprone tool to perform static code analysis at
+      compile time. Auto-generated code is not checked.
+      See: https://errorprone.info/bugpatterns for list of available bug patterns.-->
+      <id>errorprone</id>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-compiler-plugin</artifactId>
+            <configuration>
+              <compilerArgs>
+                <arg>-XDcompilePolicy=simple</arg>
+                <arg>
+                  -Xplugin:ErrorProne \
+                  -XepExcludedPaths:.*/(proto|thrift|generated-sources)/.* \
+                  -XepDisableWarningsInGeneratedCode \
+                  -XepDisableAllWarnings \
+                  <!-- error/warning patterns to ignore -->
+                  -Xep:Incomparable:OFF \
+                  -Xep:CheckReturnValue:OFF \
+                  -Xep:MustBeClosedChecker:OFF \
+                  -Xep:ReturnValueIgnored:OFF \
+                  <!-- error/warning patterns to specifically check -->
+                  -Xep:ExpectedExceptionChecker \
+                  -Xep:MissingOverride \
+                  <!-- Items containing 'OFF' are currently flagged by errorprone. The 'OFF'
+                  can be removed and the project recompiled to discover lcation of errors for","[{'comment': ""```suggestion\r\n                  <!-- Items containing 'OFF' are currently flagged by ErrorProne. The 'OFF'\r\n                  can be removed and the project recompiled to discover location of errors for\r\n```"", 'commenter': 'DomGarguilo'}]"
2447,pom.xml,"@@ -1622,5 +1623,58 @@
         <surefire.reuseForks>${reuseForks}</surefire.reuseForks>
       </properties>
     </profile>
+    <profile>
+      <!-- This profile uses the Google errorprone tool to perform static code analysis at","[{'comment': '```suggestion\r\n      <!-- This profile uses the Google ErrorProne tool to perform static code analysis at\r\n```', 'commenter': 'DomGarguilo'}]"
2447,pom.xml,"@@ -1623,5 +1624,64 @@
         <surefire.reuseForks>${reuseForks}</surefire.reuseForks>
       </properties>
     </profile>
+    <profile>
+      <!-- This profile uses the Google ErrorProne tool to perform static code analysis at
+      compile time. Auto-generated code is not checked.
+      See: https://errorprone.info/bugpatterns for list of available bug patterns.-->
+      <id>errorprone</id>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-compiler-plugin</artifactId>
+            <configuration>
+              <compilerArgs>
+                <arg>-XDcompilePolicy=simple</arg>
+                <arg>
+                  -Xplugin:ErrorProne \
+                  -XepExcludedPaths:.*/(proto|thrift|generated-sources)/.* \
+                  -XepDisableWarningsInGeneratedCode \
+                  -XepDisableAllWarnings \
+                  <!-- error/warning patterns to ignore -->
+                  -Xep:Incomparable:OFF \
+                  -Xep:CheckReturnValue:OFF \
+                  -Xep:MustBeClosedChecker:OFF \
+                  -Xep:ReturnValueIgnored:OFF \
+                  -Xep:UnicodeInCode:OFF \
+                  <!-- error/warning patterns to specifically check -->
+                  -Xep:ExpectedExceptionChecker \
+                  -Xep:MissingOverride \
+                  <!-- Items containing 'OFF' are currently flagged by ErrorProne. The 'OFF'
+                  can be removed and the project recompiled to discover location of errors for
+                  further analysis. @SuppressWarnings can be used to ignore errors if desired. -->
+                </arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED</arg>
+                <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED</arg>
+                <arg>-J--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED</arg>
+                <arg>-J--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED</arg>
+              </compilerArgs>
+              <annotationProcessorPaths>
+                <path>
+                  <groupId>com.google.errorprone</groupId>
+                  <artifactId>error_prone_core</artifactId>
+                  <version>${errorprone.version}</version>
+                </path>
+                <path>
+                  <groupId>com.github.spotbugs</groupId>
+                  <artifactId>spotbugs-annotations</artifactId>
+                  <version>4.4.2</version>
+                </path>","[{'comment': ""I don't think spotbugs-annotations is a compile-time annotation-processor, just a regular dependency to import the annotations used by spotbugs when it does its static code analysis. I do know that we have an annotation processor in use in the mail build, but it's not this. It's the auto-service one."", 'commenter': 'ctubbsii'}]"
2462,core/src/main/java/org/apache/accumulo/fate/Fate.java,"@@ -81,6 +81,9 @@ public void run() {
 
               if (deferTime == 0) {
                 prevOp = op;
+                if (status.equals(TStatus.SUBMITTED)) {
+                  store.setStatus(tid, TStatus.IN_PROGRESS);
+                }","[{'comment': 'enums should always be compared with `==` and not `.equals`\r\n\r\n```suggestion\r\n                if (status == TStatus.SUBMITTED) {\r\n                  store.setStatus(tid, TStatus.IN_PROGRESS);\r\n                }\r\n```', 'commenter': 'ctubbsii'}]"
2462,core/src/main/java/org/apache/accumulo/fate/ReadOnlyTStore.java,"@@ -36,7 +36,9 @@
   enum TStatus {
     /** Unseeded transaction */
     NEW,
-    /** Transaction is eligible to be executing */
+    /** Transaction that is eligible to be executed */
+    SUBMITTED,
+    /** Transaction that is executing */","[{'comment': ""New enums should be appended to the list rather than inserted into the middle, so it doesn't affect their ordinal, which could be important for serialization issues."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 46a503ace773ab8f3c1f35a91691c78014e43f9b', 'commenter': 'dlmarion'}]"
2462,core/src/main/java/org/apache/accumulo/fate/ZooStore.java,"@@ -174,7 +174,8 @@ public long reserve() {
 
           try {
             TStatus status = TStatus.valueOf(new String(zk.getData(path + ""/"" + txdir), UTF_8));
-            if (status == TStatus.IN_PROGRESS || status == TStatus.FAILED_IN_PROGRESS) {
+            if (status.equals(TStatus.SUBMITTED) || status.equals(TStatus.IN_PROGRESS)
+                || status.equals(TStatus.FAILED_IN_PROGRESS)) {","[{'comment': '```suggestion\r\n            if (status == TStatus.SUBMITTED || status == TStatus.IN_PROGRESS\r\n                || status == TStatus.FAILED_IN_PROGRESS) {\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 46a503ace773ab8f3c1f35a91691c78014e43f9b', 'commenter': 'dlmarion'}]"
2462,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/FateIT.java,"@@ -0,0 +1,193 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.fate.zookeeper;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+import java.util.UUID;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.clientImpl.thrift.TableOperation;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.AgeOffStore;
+import org.apache.accumulo.fate.Fate;
+import org.apache.accumulo.fate.ReadOnlyTStore.TStatus;
+import org.apache.accumulo.fate.Repo;
+import org.apache.accumulo.fate.ZooStore;
+import org.apache.accumulo.fate.util.UtilWaitThread;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.manager.Manager;
+import org.apache.accumulo.manager.tableOps.ManagerRepo;
+import org.apache.accumulo.manager.tableOps.TraceRepo;
+import org.apache.accumulo.manager.tableOps.Utils;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.test.categories.ZooKeeperTestingServerTests;
+import org.apache.accumulo.test.zookeeper.ZooKeeperTestingServer;
+import org.apache.zookeeper.KeeperException;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category({ZooKeeperTestingServerTests.class})
+public class FateIT {
+
+  public static class TestOperation extends ManagerRepo {
+
+    private static final long serialVersionUID = 1L;
+
+    private final TableId tableId;
+    private final NamespaceId namespaceId;
+
+    public TestOperation(NamespaceId namespaceId, TableId tableId) {
+      this.namespaceId = namespaceId;
+      this.tableId = tableId;
+    }
+
+    @Override
+    public long isReady(long tid, Manager manager) throws Exception {
+      return Utils.reserveNamespace(manager, namespaceId, tid, false, true, TableOperation.RENAME)
+          + Utils.reserveTable(manager, tableId, tid, true, true, TableOperation.RENAME);
+    }
+
+    @Override
+    public void undo(long tid, Manager manager) throws Exception {
+      Utils.unreserveNamespace(manager, namespaceId, tid, false);
+      Utils.unreserveTable(manager, tableId, tid, true);
+    }
+
+    @Override
+    public Repo<Manager> call(long tid, Manager environment) throws Exception {
+      FateIT.inCall();
+      return null;
+    }
+
+  }
+
+  private static ZooKeeperTestingServer szk = null;
+  private static final String ZK_ROOT = ""/accumulo/"" + UUID.randomUUID().toString();
+  private static final NamespaceId NS = NamespaceId.of(""testNameSpace"");
+  private static final TableId TID = TableId.of(""testTable"");
+
+  private static CountDownLatch callStarted;
+  private static CountDownLatch finishCall;
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    szk = new ZooKeeperTestingServer();
+  }
+
+  @AfterClass
+  public static void teardown() throws Exception {
+    szk.close();
+  }
+
+  @Test(timeout = 30000)
+  public void testTransactionStatus() throws Exception {
+    ZooReaderWriter zk = new ZooReaderWriter(szk.getConn(), 30000, ""secret"");
+
+    zk.mkdirs(ZK_ROOT + Constants.ZFATE);
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_LOCKS);
+    zk.mkdirs(ZK_ROOT + Constants.ZNAMESPACES + ""/"" + NS.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_STATE + ""/"" + TID.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLES + ""/"" + TID.canonical());
+
+    ZooStore<Manager> zooStore = new ZooStore<Manager>(ZK_ROOT + Constants.ZFATE, zk);
+    final AgeOffStore<Manager> store = new AgeOffStore<Manager>(zooStore, 1000 * 60 * 60 * 8);
+
+    Manager manager = createMock(Manager.class);
+    ServerContext sctx = createMock(ServerContext.class);
+    expect(manager.getContext()).andReturn(sctx).anyTimes();
+    expect(sctx.getZooKeeperRoot()).andReturn(ZK_ROOT).anyTimes();
+    expect(sctx.getZooReaderWriter()).andReturn(zk).anyTimes();
+    replay(manager, sctx);
+
+    Fate<Manager> fate = new Fate<Manager>(manager, store, TraceRepo::toLogString);
+    try {
+      ConfigurationCopy config = new ConfigurationCopy();
+      config.set(Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE, ""2"");
+      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, ""1"");
+      fate.startTransactionRunners(config);
+
+      // Wait for the transaction runner to be scheduled.
+      UtilWaitThread.sleep(3000);
+
+      callStarted = new CountDownLatch(1);
+      finishCall = new CountDownLatch(1);
+
+      long txid = fate.startTransaction();
+      assertEquals(TStatus.NEW, getTxStatus(zk, txid));
+      fate.seedTransaction(txid, new TestOperation(NS, TID), true);
+      assertEquals(TStatus.SUBMITTED, getTxStatus(zk, txid));
+      // wait for call() to be called
+      callStarted.await();
+      assertEquals(TStatus.IN_PROGRESS, getTxStatus(zk, txid));
+      // tell the op to exit the method
+      finishCall.countDown();
+      // Check that it transitions to SUCCESSFUL
+      TStatus s = getTxStatus(zk, txid);
+      while (!s.equals(TStatus.SUCCESSFUL)) {
+        s = getTxStatus(zk, txid);
+      }
+      // Check that it gets removed
+      boolean errorSeen = false;
+      while (!errorSeen) {
+        try {
+          s = getTxStatus(zk, txid);
+        } catch (KeeperException e) {
+          if (e.code() == KeeperException.Code.NONODE) {
+            errorSeen = true;
+          } else {
+            fail(""Unexpected error thrown: "" + e.getMessage());
+          }
+        }
+      }
+
+    } finally {
+      fate.shutdown();
+    }
+  }
+
+  public static void inCall() throws InterruptedException {
+    // signal that call started
+    callStarted.countDown();
+    // wait for the signal to exit the method
+    finishCall.await();
+  }
+
+  /*
+   * Get the status of the TX from ZK directly. Unable to call ZooStore.getStatus because this test
+   * thread does not have the reservation (the FaTE thread does)
+   */
+  private static TStatus getTxStatus(ZooReaderWriter zrw, long txid)
+      throws KeeperException, InterruptedException {
+    String txdir = String.format(""%s/tx_%016x"", ZK_ROOT + Constants.ZFATE, txid);","[{'comment': 'This is already using a format string. So, you don\'t need to do the concatenation as well:\r\n```suggestion\r\n    String txdir = String.format(""%s%s/tx_%016x"", ZK_ROOT, Constants.ZFATE, txid);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 46a503ace773ab8f3c1f35a91691c78014e43f9b', 'commenter': 'dlmarion'}]"
2462,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/FateIT.java,"@@ -0,0 +1,193 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.fate.zookeeper;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+import java.util.UUID;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.clientImpl.thrift.TableOperation;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.AgeOffStore;
+import org.apache.accumulo.fate.Fate;
+import org.apache.accumulo.fate.ReadOnlyTStore.TStatus;
+import org.apache.accumulo.fate.Repo;
+import org.apache.accumulo.fate.ZooStore;
+import org.apache.accumulo.fate.util.UtilWaitThread;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.manager.Manager;
+import org.apache.accumulo.manager.tableOps.ManagerRepo;
+import org.apache.accumulo.manager.tableOps.TraceRepo;
+import org.apache.accumulo.manager.tableOps.Utils;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.test.categories.ZooKeeperTestingServerTests;
+import org.apache.accumulo.test.zookeeper.ZooKeeperTestingServer;
+import org.apache.zookeeper.KeeperException;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category({ZooKeeperTestingServerTests.class})
+public class FateIT {
+
+  public static class TestOperation extends ManagerRepo {
+
+    private static final long serialVersionUID = 1L;
+
+    private final TableId tableId;
+    private final NamespaceId namespaceId;
+
+    public TestOperation(NamespaceId namespaceId, TableId tableId) {
+      this.namespaceId = namespaceId;
+      this.tableId = tableId;
+    }
+
+    @Override
+    public long isReady(long tid, Manager manager) throws Exception {
+      return Utils.reserveNamespace(manager, namespaceId, tid, false, true, TableOperation.RENAME)
+          + Utils.reserveTable(manager, tableId, tid, true, true, TableOperation.RENAME);
+    }
+
+    @Override
+    public void undo(long tid, Manager manager) throws Exception {
+      Utils.unreserveNamespace(manager, namespaceId, tid, false);
+      Utils.unreserveTable(manager, tableId, tid, true);
+    }
+
+    @Override
+    public Repo<Manager> call(long tid, Manager environment) throws Exception {
+      FateIT.inCall();
+      return null;
+    }
+
+  }
+
+  private static ZooKeeperTestingServer szk = null;
+  private static final String ZK_ROOT = ""/accumulo/"" + UUID.randomUUID().toString();
+  private static final NamespaceId NS = NamespaceId.of(""testNameSpace"");
+  private static final TableId TID = TableId.of(""testTable"");
+
+  private static CountDownLatch callStarted;
+  private static CountDownLatch finishCall;
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    szk = new ZooKeeperTestingServer();
+  }
+
+  @AfterClass
+  public static void teardown() throws Exception {
+    szk.close();
+  }
+
+  @Test(timeout = 30000)
+  public void testTransactionStatus() throws Exception {
+    ZooReaderWriter zk = new ZooReaderWriter(szk.getConn(), 30000, ""secret"");
+
+    zk.mkdirs(ZK_ROOT + Constants.ZFATE);
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_LOCKS);
+    zk.mkdirs(ZK_ROOT + Constants.ZNAMESPACES + ""/"" + NS.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_STATE + ""/"" + TID.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLES + ""/"" + TID.canonical());
+
+    ZooStore<Manager> zooStore = new ZooStore<Manager>(ZK_ROOT + Constants.ZFATE, zk);
+    final AgeOffStore<Manager> store = new AgeOffStore<Manager>(zooStore, 1000 * 60 * 60 * 8);
+
+    Manager manager = createMock(Manager.class);
+    ServerContext sctx = createMock(ServerContext.class);
+    expect(manager.getContext()).andReturn(sctx).anyTimes();
+    expect(sctx.getZooKeeperRoot()).andReturn(ZK_ROOT).anyTimes();
+    expect(sctx.getZooReaderWriter()).andReturn(zk).anyTimes();
+    replay(manager, sctx);
+
+    Fate<Manager> fate = new Fate<Manager>(manager, store, TraceRepo::toLogString);
+    try {
+      ConfigurationCopy config = new ConfigurationCopy();
+      config.set(Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE, ""2"");
+      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, ""1"");
+      fate.startTransactionRunners(config);
+
+      // Wait for the transaction runner to be scheduled.
+      UtilWaitThread.sleep(3000);
+
+      callStarted = new CountDownLatch(1);
+      finishCall = new CountDownLatch(1);
+
+      long txid = fate.startTransaction();
+      assertEquals(TStatus.NEW, getTxStatus(zk, txid));
+      fate.seedTransaction(txid, new TestOperation(NS, TID), true);
+      assertEquals(TStatus.SUBMITTED, getTxStatus(zk, txid));
+      // wait for call() to be called
+      callStarted.await();
+      assertEquals(TStatus.IN_PROGRESS, getTxStatus(zk, txid));
+      // tell the op to exit the method
+      finishCall.countDown();
+      // Check that it transitions to SUCCESSFUL
+      TStatus s = getTxStatus(zk, txid);
+      while (!s.equals(TStatus.SUCCESSFUL)) {
+        s = getTxStatus(zk, txid);
+      }
+      // Check that it gets removed
+      boolean errorSeen = false;
+      while (!errorSeen) {
+        try {
+          s = getTxStatus(zk, txid);
+        } catch (KeeperException e) {
+          if (e.code() == KeeperException.Code.NONODE) {
+            errorSeen = true;
+          } else {
+            fail(""Unexpected error thrown: "" + e.getMessage());
+          }
+        }
+      }
+
+    } finally {
+      fate.shutdown();
+    }
+  }
+
+  public static void inCall() throws InterruptedException {","[{'comment': '```suggestion\r\n  private static void inCall() throws InterruptedException {\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 46a503ace773ab8f3c1f35a91691c78014e43f9b', 'commenter': 'dlmarion'}]"
2462,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/FateIT.java,"@@ -0,0 +1,193 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.fate.zookeeper;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+import java.util.UUID;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.clientImpl.thrift.TableOperation;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.AgeOffStore;
+import org.apache.accumulo.fate.Fate;
+import org.apache.accumulo.fate.ReadOnlyTStore.TStatus;
+import org.apache.accumulo.fate.Repo;
+import org.apache.accumulo.fate.ZooStore;
+import org.apache.accumulo.fate.util.UtilWaitThread;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.manager.Manager;
+import org.apache.accumulo.manager.tableOps.ManagerRepo;
+import org.apache.accumulo.manager.tableOps.TraceRepo;
+import org.apache.accumulo.manager.tableOps.Utils;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.test.categories.ZooKeeperTestingServerTests;
+import org.apache.accumulo.test.zookeeper.ZooKeeperTestingServer;
+import org.apache.zookeeper.KeeperException;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category({ZooKeeperTestingServerTests.class})
+public class FateIT {
+
+  public static class TestOperation extends ManagerRepo {
+
+    private static final long serialVersionUID = 1L;
+
+    private final TableId tableId;
+    private final NamespaceId namespaceId;
+
+    public TestOperation(NamespaceId namespaceId, TableId tableId) {
+      this.namespaceId = namespaceId;
+      this.tableId = tableId;
+    }
+
+    @Override
+    public long isReady(long tid, Manager manager) throws Exception {
+      return Utils.reserveNamespace(manager, namespaceId, tid, false, true, TableOperation.RENAME)
+          + Utils.reserveTable(manager, tableId, tid, true, true, TableOperation.RENAME);
+    }
+
+    @Override
+    public void undo(long tid, Manager manager) throws Exception {
+      Utils.unreserveNamespace(manager, namespaceId, tid, false);
+      Utils.unreserveTable(manager, tableId, tid, true);
+    }
+
+    @Override
+    public Repo<Manager> call(long tid, Manager environment) throws Exception {
+      FateIT.inCall();
+      return null;
+    }
+
+  }
+
+  private static ZooKeeperTestingServer szk = null;
+  private static final String ZK_ROOT = ""/accumulo/"" + UUID.randomUUID().toString();
+  private static final NamespaceId NS = NamespaceId.of(""testNameSpace"");
+  private static final TableId TID = TableId.of(""testTable"");
+
+  private static CountDownLatch callStarted;
+  private static CountDownLatch finishCall;
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    szk = new ZooKeeperTestingServer();
+  }
+
+  @AfterClass
+  public static void teardown() throws Exception {
+    szk.close();
+  }
+
+  @Test(timeout = 30000)
+  public void testTransactionStatus() throws Exception {
+    ZooReaderWriter zk = new ZooReaderWriter(szk.getConn(), 30000, ""secret"");
+
+    zk.mkdirs(ZK_ROOT + Constants.ZFATE);
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_LOCKS);
+    zk.mkdirs(ZK_ROOT + Constants.ZNAMESPACES + ""/"" + NS.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_STATE + ""/"" + TID.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLES + ""/"" + TID.canonical());
+
+    ZooStore<Manager> zooStore = new ZooStore<Manager>(ZK_ROOT + Constants.ZFATE, zk);
+    final AgeOffStore<Manager> store = new AgeOffStore<Manager>(zooStore, 1000 * 60 * 60 * 8);
+
+    Manager manager = createMock(Manager.class);
+    ServerContext sctx = createMock(ServerContext.class);
+    expect(manager.getContext()).andReturn(sctx).anyTimes();
+    expect(sctx.getZooKeeperRoot()).andReturn(ZK_ROOT).anyTimes();
+    expect(sctx.getZooReaderWriter()).andReturn(zk).anyTimes();
+    replay(manager, sctx);
+
+    Fate<Manager> fate = new Fate<Manager>(manager, store, TraceRepo::toLogString);
+    try {
+      ConfigurationCopy config = new ConfigurationCopy();
+      config.set(Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE, ""2"");
+      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, ""1"");
+      fate.startTransactionRunners(config);
+
+      // Wait for the transaction runner to be scheduled.
+      UtilWaitThread.sleep(3000);
+
+      callStarted = new CountDownLatch(1);
+      finishCall = new CountDownLatch(1);
+
+      long txid = fate.startTransaction();
+      assertEquals(TStatus.NEW, getTxStatus(zk, txid));
+      fate.seedTransaction(txid, new TestOperation(NS, TID), true);
+      assertEquals(TStatus.SUBMITTED, getTxStatus(zk, txid));
+      // wait for call() to be called
+      callStarted.await();
+      assertEquals(TStatus.IN_PROGRESS, getTxStatus(zk, txid));
+      // tell the op to exit the method
+      finishCall.countDown();
+      // Check that it transitions to SUCCESSFUL
+      TStatus s = getTxStatus(zk, txid);
+      while (!s.equals(TStatus.SUCCESSFUL)) {
+        s = getTxStatus(zk, txid);
+      }
+      // Check that it gets removed
+      boolean errorSeen = false;
+      while (!errorSeen) {
+        try {
+          s = getTxStatus(zk, txid);","[{'comment': ""Should there be a few millis of sleep in here so it doesn't rapid cycle?\r\n```suggestion\r\n          s = getTxStatus(zk, txid);\r\n          UtilWaitThread.sleep(50);\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 46a503ace773ab8f3c1f35a91691c78014e43f9b', 'commenter': 'dlmarion'}]"
2462,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/FateIT.java,"@@ -0,0 +1,193 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.fate.zookeeper;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+import java.util.UUID;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.clientImpl.thrift.TableOperation;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.AgeOffStore;
+import org.apache.accumulo.fate.Fate;
+import org.apache.accumulo.fate.ReadOnlyTStore.TStatus;
+import org.apache.accumulo.fate.Repo;
+import org.apache.accumulo.fate.ZooStore;
+import org.apache.accumulo.fate.util.UtilWaitThread;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.manager.Manager;
+import org.apache.accumulo.manager.tableOps.ManagerRepo;
+import org.apache.accumulo.manager.tableOps.TraceRepo;
+import org.apache.accumulo.manager.tableOps.Utils;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.test.categories.ZooKeeperTestingServerTests;
+import org.apache.accumulo.test.zookeeper.ZooKeeperTestingServer;
+import org.apache.zookeeper.KeeperException;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category({ZooKeeperTestingServerTests.class})
+public class FateIT {
+
+  public static class TestOperation extends ManagerRepo {
+
+    private static final long serialVersionUID = 1L;
+
+    private final TableId tableId;
+    private final NamespaceId namespaceId;
+
+    public TestOperation(NamespaceId namespaceId, TableId tableId) {
+      this.namespaceId = namespaceId;
+      this.tableId = tableId;
+    }
+
+    @Override
+    public long isReady(long tid, Manager manager) throws Exception {
+      return Utils.reserveNamespace(manager, namespaceId, tid, false, true, TableOperation.RENAME)
+          + Utils.reserveTable(manager, tableId, tid, true, true, TableOperation.RENAME);
+    }
+
+    @Override
+    public void undo(long tid, Manager manager) throws Exception {
+      Utils.unreserveNamespace(manager, namespaceId, tid, false);
+      Utils.unreserveTable(manager, tableId, tid, true);
+    }
+
+    @Override
+    public Repo<Manager> call(long tid, Manager environment) throws Exception {
+      FateIT.inCall();
+      return null;
+    }
+
+  }
+
+  private static ZooKeeperTestingServer szk = null;
+  private static final String ZK_ROOT = ""/accumulo/"" + UUID.randomUUID().toString();
+  private static final NamespaceId NS = NamespaceId.of(""testNameSpace"");
+  private static final TableId TID = TableId.of(""testTable"");
+
+  private static CountDownLatch callStarted;
+  private static CountDownLatch finishCall;
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    szk = new ZooKeeperTestingServer();
+  }
+
+  @AfterClass
+  public static void teardown() throws Exception {
+    szk.close();
+  }
+
+  @Test(timeout = 30000)
+  public void testTransactionStatus() throws Exception {
+    ZooReaderWriter zk = new ZooReaderWriter(szk.getConn(), 30000, ""secret"");
+
+    zk.mkdirs(ZK_ROOT + Constants.ZFATE);
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_LOCKS);
+    zk.mkdirs(ZK_ROOT + Constants.ZNAMESPACES + ""/"" + NS.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_STATE + ""/"" + TID.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLES + ""/"" + TID.canonical());
+
+    ZooStore<Manager> zooStore = new ZooStore<Manager>(ZK_ROOT + Constants.ZFATE, zk);
+    final AgeOffStore<Manager> store = new AgeOffStore<Manager>(zooStore, 1000 * 60 * 60 * 8);
+
+    Manager manager = createMock(Manager.class);
+    ServerContext sctx = createMock(ServerContext.class);
+    expect(manager.getContext()).andReturn(sctx).anyTimes();
+    expect(sctx.getZooKeeperRoot()).andReturn(ZK_ROOT).anyTimes();
+    expect(sctx.getZooReaderWriter()).andReturn(zk).anyTimes();
+    replay(manager, sctx);
+
+    Fate<Manager> fate = new Fate<Manager>(manager, store, TraceRepo::toLogString);
+    try {
+      ConfigurationCopy config = new ConfigurationCopy();
+      config.set(Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE, ""2"");
+      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, ""1"");
+      fate.startTransactionRunners(config);
+
+      // Wait for the transaction runner to be scheduled.
+      UtilWaitThread.sleep(3000);
+
+      callStarted = new CountDownLatch(1);
+      finishCall = new CountDownLatch(1);
+
+      long txid = fate.startTransaction();
+      assertEquals(TStatus.NEW, getTxStatus(zk, txid));
+      fate.seedTransaction(txid, new TestOperation(NS, TID), true);
+      assertEquals(TStatus.SUBMITTED, getTxStatus(zk, txid));
+      // wait for call() to be called
+      callStarted.await();
+      assertEquals(TStatus.IN_PROGRESS, getTxStatus(zk, txid));
+      // tell the op to exit the method
+      finishCall.countDown();
+      // Check that it transitions to SUCCESSFUL
+      TStatus s = getTxStatus(zk, txid);
+      while (!s.equals(TStatus.SUCCESSFUL)) {
+        s = getTxStatus(zk, txid);
+      }","[{'comment': '```suggestion\r\n      while (!s.equals(TStatus.SUCCESSFUL)) {\r\n        s = getTxStatus(zk, txid);\r\n        UtilWaitThread.sleep(50);\r\n      }\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 46a503ace773ab8f3c1f35a91691c78014e43f9b', 'commenter': 'dlmarion'}]"
2462,core/src/main/java/org/apache/accumulo/fate/AgeOffStore.java,"@@ -94,6 +94,7 @@ public void ageOff() {
         try {
           switch (store.getStatus(txid)) {
             case NEW:
+            case SUBMITTED:","[{'comment': 'Are we sure we want transactions in the `SUBMITTED` state to be subject to age-off? I think `NEW` makes sense because they are probably leftover from a crash and the transaction ID is just not used. However, `SUBMITTED` seems more similar to `IN_PROGRESS` here. It could still run. Right?\r\n(Also, same question in `AgeOffStore` constructor below)', 'commenter': 'ctubbsii'}, {'comment': ""Good catch, I was in the mode of making sure I added the enum to the switch statements. I'll update."", 'commenter': 'dlmarion'}, {'comment': 'Modified in f228aab', 'commenter': 'dlmarion'}]"
2462,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/FateIT.java,"@@ -0,0 +1,195 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.fate.zookeeper;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+import java.util.UUID;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.clientImpl.thrift.TableOperation;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.AgeOffStore;
+import org.apache.accumulo.fate.Fate;
+import org.apache.accumulo.fate.ReadOnlyTStore.TStatus;
+import org.apache.accumulo.fate.Repo;
+import org.apache.accumulo.fate.ZooStore;
+import org.apache.accumulo.fate.util.UtilWaitThread;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.manager.Manager;
+import org.apache.accumulo.manager.tableOps.ManagerRepo;
+import org.apache.accumulo.manager.tableOps.TraceRepo;
+import org.apache.accumulo.manager.tableOps.Utils;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.test.categories.ZooKeeperTestingServerTests;
+import org.apache.accumulo.test.zookeeper.ZooKeeperTestingServer;
+import org.apache.zookeeper.KeeperException;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category({ZooKeeperTestingServerTests.class})
+public class FateIT {
+
+  public static class TestOperation extends ManagerRepo {
+
+    private static final long serialVersionUID = 1L;
+
+    private final TableId tableId;
+    private final NamespaceId namespaceId;
+
+    public TestOperation(NamespaceId namespaceId, TableId tableId) {
+      this.namespaceId = namespaceId;
+      this.tableId = tableId;
+    }
+
+    @Override
+    public long isReady(long tid, Manager manager) throws Exception {
+      return Utils.reserveNamespace(manager, namespaceId, tid, false, true, TableOperation.RENAME)
+          + Utils.reserveTable(manager, tableId, tid, true, true, TableOperation.RENAME);
+    }
+
+    @Override
+    public void undo(long tid, Manager manager) throws Exception {
+      Utils.unreserveNamespace(manager, namespaceId, tid, false);
+      Utils.unreserveTable(manager, tableId, tid, true);
+    }
+
+    @Override
+    public Repo<Manager> call(long tid, Manager environment) throws Exception {
+      FateIT.inCall();
+      return null;
+    }
+
+  }
+
+  private static ZooKeeperTestingServer szk = null;
+  private static final String ZK_ROOT = ""/accumulo/"" + UUID.randomUUID().toString();
+  private static final NamespaceId NS = NamespaceId.of(""testNameSpace"");
+  private static final TableId TID = TableId.of(""testTable"");
+
+  private static CountDownLatch callStarted;
+  private static CountDownLatch finishCall;
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    szk = new ZooKeeperTestingServer();
+  }
+
+  @AfterClass
+  public static void teardown() throws Exception {
+    szk.close();
+  }
+
+  @Test(timeout = 30000)
+  public void testTransactionStatus() throws Exception {
+    ZooReaderWriter zk = new ZooReaderWriter(szk.getConn(), 30000, ""secret"");
+
+    zk.mkdirs(ZK_ROOT + Constants.ZFATE);
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_LOCKS);
+    zk.mkdirs(ZK_ROOT + Constants.ZNAMESPACES + ""/"" + NS.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLE_STATE + ""/"" + TID.canonical());
+    zk.mkdirs(ZK_ROOT + Constants.ZTABLES + ""/"" + TID.canonical());
+
+    ZooStore<Manager> zooStore = new ZooStore<Manager>(ZK_ROOT + Constants.ZFATE, zk);
+    final AgeOffStore<Manager> store = new AgeOffStore<Manager>(zooStore, 1000 * 60 * 60 * 8);
+
+    Manager manager = createMock(Manager.class);
+    ServerContext sctx = createMock(ServerContext.class);
+    expect(manager.getContext()).andReturn(sctx).anyTimes();
+    expect(sctx.getZooKeeperRoot()).andReturn(ZK_ROOT).anyTimes();
+    expect(sctx.getZooReaderWriter()).andReturn(zk).anyTimes();
+    replay(manager, sctx);
+
+    Fate<Manager> fate = new Fate<Manager>(manager, store, TraceRepo::toLogString);
+    try {
+      ConfigurationCopy config = new ConfigurationCopy();
+      config.set(Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE, ""2"");
+      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, ""1"");
+      fate.startTransactionRunners(config);
+
+      // Wait for the transaction runner to be scheduled.
+      UtilWaitThread.sleep(3000);
+
+      callStarted = new CountDownLatch(1);
+      finishCall = new CountDownLatch(1);
+
+      long txid = fate.startTransaction();
+      assertEquals(TStatus.NEW, getTxStatus(zk, txid));
+      fate.seedTransaction(txid, new TestOperation(NS, TID), true);
+      assertEquals(TStatus.SUBMITTED, getTxStatus(zk, txid));
+      // wait for call() to be called
+      callStarted.await();
+      assertEquals(TStatus.IN_PROGRESS, getTxStatus(zk, txid));
+      // tell the op to exit the method
+      finishCall.countDown();
+      // Check that it transitions to SUCCESSFUL
+      TStatus s = getTxStatus(zk, txid);
+      while (!s.equals(TStatus.SUCCESSFUL)) {","[{'comment': '```suggestion\r\n      while (s != TStatus.SUCCESSFUL) {\r\n```', 'commenter': 'ctubbsii'}]"
2462,core/src/main/java/org/apache/accumulo/fate/Fate.java,"@@ -284,7 +287,7 @@ public void seedTransaction(long tid, Repo<T> repo, boolean autoCleanUp) {
 
         store.setProperty(tid, DEBUG_PROP, repo.getDescription());
 
-        store.setStatus(tid, TStatus.IN_PROGRESS);
+        store.setStatus(tid, TStatus.SUBMITTED);","[{'comment': 'I think there are some OPs that do work in the `isReady()` function so I wonder if changing when it gets set to IN_PROGRESS will affect them. I will see if I can find one.', 'commenter': 'milleruntime'}, {'comment': ""The BulkImport and Compact operations do things in the `isReady()` function. I don't know if it matters if the status is IN_PROGRESS or SUBMITTED though."", 'commenter': 'milleruntime'}, {'comment': 'Also the `ShutdownTServer` operation', 'commenter': 'milleruntime'}]"
2467,core/src/main/java/org/apache/accumulo/fate/Fate.java,"@@ -300,6 +328,56 @@ public TStatus waitForCompletion(long tid) {
     return store.waitForStatusChange(tid, FINISHED_STATES);
   }
 
+  /**
+   * Attempts to cancel a running Fate transaction
+   *
+   * @param tid
+   *          transaction id
+   * @return true if transaction transitioned to a failed state or already in a completed state,
+   *         false otherwise
+   */
+  public boolean cancel(long tid) {
+    String tidStr = Long.toHexString(tid);
+    for (int retries = 0; retries < 5; retries++) {
+      if (store.tryReserve(tid)) {
+        try {
+          TStatus status = store.getStatus(tid);
+          if (status == TStatus.NEW || status == TStatus.SUBMITTED
+              || status == TStatus.IN_PROGRESS) {
+            store.setProperty(tid, EXCEPTION_PROP, new TApplicationException(","[{'comment': ""What is `EXCEPTION_PROP`? I can't seem to find it."", 'commenter': 'milleruntime'}, {'comment': 'See https://github.com/apache/accumulo/blob/main/core/src/main/java/org/apache/accumulo/fate/Fate.java#L48', 'commenter': 'dlmarion'}]"
2467,shell/src/main/java/org/apache/accumulo/shell/commands/FateCommand.java,"@@ -136,7 +140,29 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
         getZooReaderWriter(context, siteConfig, cl.getOptionValue(secretOption.getOpt()));
     ZooStore<FateCommand> zs = new ZooStore<>(path, zk);
 
-    if (""fail"".equals(cmd)) {
+    if (""fail-live"".equals(cmd)) {","[{'comment': 'Why not just call it cancel?', 'commenter': 'milleruntime'}, {'comment': ""I'm not tied to the name, we can change it to whatever... Doesn't the currently named `fail` method actually cancel ? I was trying to stick with the current naming convention."", 'commenter': 'dlmarion'}, {'comment': 'I do not know if there is a nuance with cancel vs. fail.  Cancel seems (to me) a more normal operation - I changed my mind. Fail seems more of an error condition.  The documentation in the FATE section states that  ""Failing an operation is not a normal procedure"" Does canceling a compaction raise to that level?  So either way fail / or cancel, but it maybe worth considering the context and usage.   \r\n\r\nOne one hand, not needing to remember when its fail vs. cancel (follow existing naming) is a positive - unless cancel is more ""normal"" and does not take a lot of insight for an operator to execute.', 'commenter': 'EdColeman'}, {'comment': 'SInce I equated `fail` with `cancel` I tried to use a name that implied the user could cancel/fail the operation with the manager up. Hence `fail-live`. That sounded better than `fail-up`, lol.', 'commenter': 'dlmarion'}, {'comment': 'To Ed\'s question, ""Does canceling a compaction raise to that level?"" ... I think both operations are pretty abnormal... to the point that I\'m still wondering whether we really need to be able to do this live when taking the manager offline is probably fine, and safest, under abnormal conditions. I worry that people will normalize it, and their systems will become unstable over time by abusing this.', 'commenter': 'ctubbsii'}, {'comment': ""So, we give users the ability to create fate transactions easily. I think in only one case, we give them the ability to cancel them (cancel compactions). Taking the manager down to fail a fate transaction is very disruptive (and doesn't it terminate the FaTE transaction runners in the middle of their operations?). Certainly the changes I made in #2462 will let users identify which transactions have never been started, those should be able to be cancelled with no issue."", 'commenter': 'dlmarion'}]"
2467,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/FateIT.java,"@@ -71,35 +76,63 @@ public TestOperation(NamespaceId namespaceId, TableId tableId) {
 
     @Override
     public long isReady(long tid, Manager manager) throws Exception {
-      return Utils.reserveNamespace(manager, namespaceId, tid, false, true, TableOperation.RENAME)
-          + Utils.reserveTable(manager, tableId, tid, true, true, TableOperation.RENAME);
+      LOG.info(""Entering isReady {}"", Long.toHexString(tid));
+      try {
+        FateIT.inReady();
+        return Utils.reserveNamespace(manager, namespaceId, tid, false, true, TableOperation.RENAME)
+            + Utils.reserveTable(manager, tableId, tid, true, true, TableOperation.RENAME);
+      } finally {
+        LOG.info(""Leaving isReady {}"", Long.toHexString(tid));
+      }
     }
 
     @Override
     public void undo(long tid, Manager manager) throws Exception {
-      Utils.unreserveNamespace(manager, namespaceId, tid, false);
-      Utils.unreserveTable(manager, tableId, tid, true);
+      LOG.info(""Entering undo {}"", Long.toHexString(tid));
+      try {
+        Utils.unreserveNamespace(manager, namespaceId, tid, false);
+        Utils.unreserveTable(manager, tableId, tid, true);
+      } finally {
+        LOG.info(""Leaving undo {}"", Long.toHexString(tid));
+      }
     }
 
     @Override
-    public Repo<Manager> call(long tid, Manager environment) throws Exception {
-      FateIT.inCall();
-      return null;
+    public Repo<Manager> call(long tid, Manager manager) throws Exception {
+      LOG.info(""Entering call {}"", Long.toHexString(tid));
+      try {
+        FateIT.inCall();
+        return null;
+      } finally {
+        Utils.unreserveNamespace(manager, namespaceId, tid, false);
+        Utils.unreserveTable(manager, tableId, tid, true);
+        LOG.info(""Leaving call {}"", Long.toHexString(tid));","[{'comment': ""You have a bunch of `info` statements like this. Could probably log them at debug or remove them if you don't need them anymore."", 'commenter': 'milleruntime'}, {'comment': 'Addressed in c9e64f4', 'commenter': 'dlmarion'}]"
2467,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/FateIT.java,"@@ -175,6 +205,170 @@ public void testTransactionStatus() throws Exception {
     }
   }
 
+  @Test
+  public void testCancelWhileNew() throws Exception {
+    final ZooReaderWriter zk = new ZooReaderWriter(szk.getConn(), 30000, ""secret"");
+    final ZooStore<Manager> zooStore = new ZooStore<Manager>(ZK_ROOT + Constants.ZFATE, zk);
+    final AgeOffStore<Manager> store = new AgeOffStore<Manager>(zooStore, 3000);
+
+    Manager manager = createMock(Manager.class);
+    ServerContext sctx = createMock(ServerContext.class);
+    expect(manager.getContext()).andReturn(sctx).anyTimes();
+    expect(sctx.getZooKeeperRoot()).andReturn(ZK_ROOT).anyTimes();
+    expect(sctx.getZooReaderWriter()).andReturn(zk).anyTimes();
+    replay(manager, sctx);
+
+    Fate<Manager> fate = new Fate<Manager>(manager, store, TraceRepo::toLogString);
+    try {
+      ConfigurationCopy config = new ConfigurationCopy();
+      config.set(Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE, ""2"");
+      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, ""1"");
+      fate.startTransactionRunners(config);
+
+      // Wait for the transaction runner to be scheduled.
+      UtilWaitThread.sleep(3000);
+
+      doReady = new CountDownLatch(1);
+      callStarted = new CountDownLatch(1);
+      finishCall = new CountDownLatch(1);
+
+      long txid = fate.startTransaction();
+      LOG.info(""Starting test testCancelWhileNew with {}"", Long.toHexString(txid));
+      assertEquals(TStatus.NEW, getTxStatus(zk, txid));
+      // cancel the transaction
+      assertTrue(fate.cancel(txid));
+      assertTrue(TStatus.FAILED_IN_PROGRESS == getTxStatus(zk, txid)
+          || TStatus.FAILED == getTxStatus(zk, txid));
+      fate.seedTransaction(txid, new TestOperation(NS, TID), true);
+      assertTrue(TStatus.FAILED_IN_PROGRESS == getTxStatus(zk, txid)
+          || TStatus.FAILED == getTxStatus(zk, txid));
+    } finally {
+      fate.shutdown();
+    }
+  }
+
+  @Test
+  public void testCancelWhileSubmitted() throws Exception {
+    final ZooReaderWriter zk = new ZooReaderWriter(szk.getConn(), 30000, ""secret"");
+    final ZooStore<Manager> zooStore = new ZooStore<Manager>(ZK_ROOT + Constants.ZFATE, zk);
+    final AgeOffStore<Manager> store = new AgeOffStore<Manager>(zooStore, 3000);
+
+    Manager manager = createMock(Manager.class);
+    ServerContext sctx = createMock(ServerContext.class);
+    expect(manager.getContext()).andReturn(sctx).anyTimes();
+    expect(sctx.getZooKeeperRoot()).andReturn(ZK_ROOT).anyTimes();
+    expect(sctx.getZooReaderWriter()).andReturn(zk).anyTimes();
+    replay(manager, sctx);
+
+    Fate<Manager> fate = new Fate<Manager>(manager, store, TraceRepo::toLogString);
+    try {
+      ConfigurationCopy config = new ConfigurationCopy();
+      config.set(Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE, ""2"");
+      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, ""1"");
+      fate.startTransactionRunners(config);
+
+      // Wait for the transaction runner to be scheduled.
+      UtilWaitThread.sleep(3000);
+
+      doReady = new CountDownLatch(1);
+      callStarted = new CountDownLatch(1);
+      finishCall = new CountDownLatch(1);
+
+      long txid = fate.startTransaction();
+      LOG.info(""Starting test testCancelWhileSubmitted with {}"", Long.toHexString(txid));
+      assertEquals(TStatus.NEW, getTxStatus(zk, txid));
+      fate.seedTransaction(txid, new TestOperation(NS, TID), true);
+      assertEquals(TStatus.SUBMITTED, getTxStatus(zk, txid));
+      // cancel the transaction
+      assertTrue(fate.cancel(txid));
+      // do the isReady method
+      doReady.countDown();
+      // Check that tx is failing or gets removed
+      boolean nodeRemoved = false;
+      while (!nodeRemoved) {
+        try {
+          TStatus s = getTxStatus(zk, txid);
+          assertTrue(
+              s == TStatus.SUBMITTED || s == TStatus.FAILED_IN_PROGRESS || s == TStatus.FAILED);","[{'comment': 'Static imports could clean up the formatting here.', 'commenter': 'milleruntime'}, {'comment': 'Addressed in b87b8f1', 'commenter': 'dlmarion'}]"
2467,core/src/main/java/org/apache/accumulo/fate/Fate.java,"@@ -71,35 +83,58 @@ public void run() {
         try {
           tid = store.reserve();
           TStatus status = store.getStatus(tid);
+          if (status == FAILED) {
+            runnerLog.info(
+                ""FATE txid "" + Long.toHexString(tid) + "" likely cancelled before it started."");
+            return;
+          }
           Repo<T> op = store.top(tid);
-          if (status == TStatus.FAILED_IN_PROGRESS) {
+          if (status == FAILED_IN_PROGRESS) {
             processFailed(tid, op);
           } else {
             Repo<T> prevOp = null;
             try {
+              synchronized (RUNNING_TRANSACTIONS) {
+                RUNNING_TRANSACTIONS.put(tid, Thread.currentThread());
+              }
               deferTime = op.isReady(tid, environment);
 
               if (deferTime == 0) {
                 prevOp = op;
-                if (status == TStatus.SUBMITTED) {
-                  store.setStatus(tid, TStatus.IN_PROGRESS);
+                if (status == SUBMITTED) {
+                  store.setStatus(tid, IN_PROGRESS);
                 }
                 op = op.call(tid, environment);
               } else
                 continue;
 
             } catch (Exception e) {
+              if (e instanceof InterruptedException) {
+                runnerLog.info(
+                    ""FATE Runner thread interrupted processing txid "" + Long.toHexString(tid));
+                if (prevOp != null) {
+                  processFailed(tid, prevOp);
+                }
+                if (op != prevOp) {
+                  processFailed(tid, op);
+                }","[{'comment': 'This code calls processFailed before transitioning the fate tx to FAILED_IN_PROGRESS.  If the manager process dies in the middle of processFailed() then when it comes back it will start executing the tx in the IN_PROGRESS state which could lead to normal processing, failure processing, followed by normal processing for a FATE tx.   This could lead to strange mutations to ZK and metadata table. \r\n\r\nWhat is the motivation for this special case? ', 'commenter': 'keith-turner'}, {'comment': 'Ok, so you are thinking I should change `processFailed` to `transitionToFailed` on lines 116 and 119?', 'commenter': 'dlmarion'}, {'comment': 'I was thinking of just not calling processFailed here unless there is a compelling reason to do so.  When the code calls transitionToFailed then it will eventually call processFailed.', 'commenter': 'keith-turner'}, {'comment': 'Ok, I see. Yeah, I can remove those.', 'commenter': 'dlmarion'}, {'comment': 'Removed in [6d22ea7](https://github.com/apache/accumulo/pull/2467/commits/6d22ea7708b786a9b9cd2aa4469e332e29e57476)', 'commenter': 'dlmarion'}]"
2467,core/src/main/java/org/apache/accumulo/fate/Fate.java,"@@ -300,6 +335,55 @@ public TStatus waitForCompletion(long tid) {
     return store.waitForStatusChange(tid, FINISHED_STATES);
   }
 
+  /**
+   * Attempts to cancel a running Fate transaction
+   *
+   * @param tid
+   *          transaction id
+   * @return true if transaction transitioned to a failed state or already in a completed state,
+   *         false otherwise
+   */
+  public boolean cancel(long tid) {
+    String tidStr = Long.toHexString(tid);
+    for (int retries = 0; retries < 5; retries++) {
+      if (store.tryReserve(tid)) {
+        try {
+          TStatus status = store.getStatus(tid);
+          if (status == NEW || status == SUBMITTED || status == IN_PROGRESS) {
+            store.setProperty(tid, EXCEPTION_PROP, new TApplicationException(
+                TApplicationException.INTERNAL_ERROR, ""Fate transaction cancelled by user""));
+            store.setStatus(tid, FAILED_IN_PROGRESS);
+            log.info(
+                ""Updated status for Repo {} to FAILED_IN_PROGRESS because it was cancelled by user"",
+                tidStr);
+            return true;
+          } else {
+            log.info(""Repo {} cancelled by user but already in finished state"", tidStr);
+            return true;
+          }
+        } finally {
+          store.unreserve(tid, 0);
+        }
+      } else {
+        // It's possible that the FateOp is being run by the TransactionRunner
+        Thread t = null;
+        synchronized (RUNNING_TRANSACTIONS) {
+          t = RUNNING_TRANSACTIONS.get(tid);
+        }
+        if (t != null) {
+          t.interrupt();","[{'comment': 'There is a race condition here.  The same thread runs many different FATE tx steps.  By the time this code executes, the thread could be running a completely different FATE tx,  so it could interrupt the wrong FATE tx.', 'commenter': 'keith-turner'}, {'comment': 'This problem seems tricky.  One possible way to work around it is to create new thread for running each FATE step.  Then its always safe to interrupt it.', 'commenter': 'keith-turner'}, {'comment': 'What if I moved lines 373 to 381 in the synchronized block? Then, it should be guaranteed that if the transaction is in RUNNING_TRANSACTIONS, that the Thread running it has not moved on to something else. ', 'commenter': 'dlmarion'}, {'comment': 'I might also need to move the RUNNING_TRANSACTIONS.remove(tid) up a little and tighten that up.', 'commenter': 'dlmarion'}, {'comment': 'I re-worked the synchronization in [6d22ea7](https://github.com/apache/accumulo/pull/2467/commits/6d22ea7708b786a9b9cd2aa4469e332e29e57476)', 'commenter': 'dlmarion'}, {'comment': '> I re-worked the synchronization in [6d22ea7](https://github.com/apache/accumulo/pull/2467/commits/6d22ea7708b786a9b9cd2aa4469e332e29e57476)\r\n\r\nThat may be a good solution to the problem.  I suggested some tweaks to the approach.', 'commenter': 'keith-turner'}]"
2467,core/src/main/java/org/apache/accumulo/fate/Fate.java,"@@ -71,35 +83,58 @@ public void run() {
         try {
           tid = store.reserve();
           TStatus status = store.getStatus(tid);
+          if (status == FAILED) {
+            runnerLog.info(
+                ""FATE txid "" + Long.toHexString(tid) + "" likely cancelled before it started."");
+            return;
+          }
           Repo<T> op = store.top(tid);
-          if (status == TStatus.FAILED_IN_PROGRESS) {
+          if (status == FAILED_IN_PROGRESS) {
             processFailed(tid, op);
           } else {
             Repo<T> prevOp = null;
             try {
+              synchronized (RUNNING_TRANSACTIONS) {
+                RUNNING_TRANSACTIONS.put(tid, Thread.currentThread());
+              }
               deferTime = op.isReady(tid, environment);
 
               if (deferTime == 0) {
                 prevOp = op;
-                if (status == TStatus.SUBMITTED) {
-                  store.setStatus(tid, TStatus.IN_PROGRESS);
+                if (status == SUBMITTED) {
+                  store.setStatus(tid, IN_PROGRESS);
                 }
                 op = op.call(tid, environment);
               } else
                 continue;
 
             } catch (Exception e) {
+              if (e instanceof InterruptedException) {
+                runnerLog.info(
+                    ""FATE Runner thread interrupted processing txid "" + Long.toHexString(tid));
+                if (prevOp != null) {
+                  processFailed(tid, prevOp);
+                }
+                if (op != prevOp) {
+                  processFailed(tid, op);
+                }
+                Thread.interrupted();","[{'comment': 'Not sure, but this may prevent the following code from running.', 'commenter': 'keith-turner'}, {'comment': 'How so? I was using this method to clear the interrupted state of the current Thread. https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Thread.html#interrupted()', 'commenter': 'dlmarion'}, {'comment': 'Ah, I thought it was interrupting the currently thread.', 'commenter': 'keith-turner'}]"
2467,core/src/main/java/org/apache/accumulo/fate/Fate.java,"@@ -71,24 +83,43 @@ public void run() {
         try {
           tid = store.reserve();
           TStatus status = store.getStatus(tid);
+          if (status == FAILED) {
+            runnerLog.info(
+                ""FATE txid "" + Long.toHexString(tid) + "" likely cancelled before it started."");
+            return;
+          }
           Repo<T> op = store.top(tid);
-          if (status == TStatus.FAILED_IN_PROGRESS) {
+          if (status == FAILED_IN_PROGRESS) {
             processFailed(tid, op);
           } else {
             Repo<T> prevOp = null;
             try {
-              deferTime = op.isReady(tid, environment);
-
-              if (deferTime == 0) {
-                prevOp = op;
-                if (status == TStatus.SUBMITTED) {
-                  store.setStatus(tid, TStatus.IN_PROGRESS);
+              try {
+                synchronized (RUNNING_TRANSACTIONS) {
+                  RUNNING_TRANSACTIONS.put(tid, Thread.currentThread());
                 }
-                op = op.call(tid, environment);
-              } else
-                continue;
-
+                deferTime = op.isReady(tid, environment);
+                if (deferTime == 0) {
+                  prevOp = op;
+                  if (status == SUBMITTED) {
+                    store.setStatus(tid, IN_PROGRESS);
+                  }
+                  op = op.call(tid, environment);
+                } else
+                  continue;
+              } finally {
+                synchronized (Thread.currentThread()) {
+                  synchronized (RUNNING_TRANSACTIONS) {
+                    RUNNING_TRANSACTIONS.remove(tid);
+                  }
+                }","[{'comment': '```suggestion\r\n                  synchronized (RUNNING_TRANSACTIONS) {\r\n                    RUNNING_TRANSACTIONS.remove(tid);\r\n                  }\r\n                  // TODO maybe we should check/clear for interrupt here... if something \r\n                  // was trying to interrupt right before the sync block then we do not want \r\n                  // that to impact the next thing this thread will work on....\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Incorporated this change into [c49f55e](https://github.com/apache/accumulo/pull/2467/commits/c49f55ec90ed3ea883c9dbfd22dfac740b3e8b3d)', 'commenter': 'dlmarion'}]"
2467,core/src/main/java/org/apache/accumulo/fate/Fate.java,"@@ -300,6 +331,68 @@ public TStatus waitForCompletion(long tid) {
     return store.waitForStatusChange(tid, FINISHED_STATES);
   }
 
+  /**
+   * Attempts to cancel a running Fate transaction
+   *
+   * @param tid
+   *          transaction id
+   * @return true if transaction transitioned to a failed state or already in a completed state,
+   *         false otherwise
+   */
+  public boolean cancel(long tid) {
+    String tidStr = Long.toHexString(tid);
+    for (int retries = 0; retries < 5; retries++) {
+      if (store.tryReserve(tid)) {
+        try {
+          TStatus status = store.getStatus(tid);
+          if (status == NEW || status == SUBMITTED || status == IN_PROGRESS) {
+            store.setProperty(tid, EXCEPTION_PROP, new TApplicationException(
+                TApplicationException.INTERNAL_ERROR, ""Fate transaction cancelled by user""));
+            store.setStatus(tid, FAILED_IN_PROGRESS);
+            log.info(
+                ""Updated status for Repo {} to FAILED_IN_PROGRESS because it was cancelled by user"",
+                tidStr);
+            return true;
+          } else {
+            log.info(""Repo {} cancelled by user but already in finished state"", tidStr);
+            return true;
+          }
+        } finally {
+          store.unreserve(tid, 0);
+        }
+      } else {
+        // It's possible that the FateOp is being run by the TransactionRunner. Look up
+        // the thread that is running this transaction.
+        Thread t = null;
+        synchronized (RUNNING_TRANSACTIONS) {
+          t = RUNNING_TRANSACTIONS.get(tid);
+        }
+        if (t != null) {
+          Thread t2 = null;
+          synchronized (t) {
+            // Make sure that the transaction is still associated
+            // with this thread. If it is, then this thread is blocked
+            // from removing this transaction from the table and we can
+            // interrupt it and cause it to fail.
+            synchronized (RUNNING_TRANSACTIONS) {
+              t2 = RUNNING_TRANSACTIONS.get(tid);
+            }
+            if (t2 != null && t == t2) {
+              t.interrupt();
+            }
+          }
+          return true;
+        } else {
+          // reserved, but not in transaction table. It should transition to
+          // an end state or become unreserved in a short amount of time.
+          // Lets retry.
+          UtilWaitThread.sleep(500);
+        }","[{'comment': '```suggestion\r\n        Thread t = null;\r\n        synchronized (RUNNING_TRANSACTIONS) {\r\n          t = RUNNING_TRANSACTIONS.get(tid);\r\n          if( t != null){\r\n             //must interrupt in sync block to ensure thread is still associated with transaction... \r\n             // however if we interrupt the thread and nothing checks the interrupt status then \r\n             // it could still impact an unrelated FATE tx, added a TODO about that elsewhere\r\n             t.interrupt();\r\n             return true; \r\n          }\r\n        }\r\n        \r\n          // TODO not sure if I got the flow/branch on the following part correct\r\n                    \r\n          // reserved, but not in transaction table. It should transition to\r\n          // an end state or become unreserved in a short amount of time.\r\n          // Lets retry.\r\n          UtilWaitThread.sleep(500);\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Incorporated this change into [c49f55e](https://github.com/apache/accumulo/pull/2467/commits/c49f55ec90ed3ea883c9dbfd22dfac740b3e8b3d)', 'commenter': 'dlmarion'}]"
2476,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/ServiceLockIT.java,"@@ -623,11 +625,7 @@ public void testLockParallel() throws Exception {
       assertEquals(0, zk.getChildren(parent.toString(), false).size());
 
       threads.forEach(t -> {
-        try {
-          t.join();
-        } catch (InterruptedException e) {
-          // ignore
-        }
+        Uninterruptibles.joinUninterruptibly(t);","[{'comment': 'This could be:\r\n```java\r\n    threads.forEach(Uninterruptibles::joinUninterruptibly);\r\n```', 'commenter': 'ctubbsii'}]"
2496,test/src/main/java/org/apache/accumulo/test/zookeeper/ZooKeeperTestingServer.java,"@@ -47,15 +47,15 @@
    * Instantiate a running zookeeper server - this call will block until the server is ready for
    * client connections. It will try three times, with a 5 second pause to connect.
    */
-  public ZooKeeperTestingServer() {
-    this(PortUtils.getRandomFreePort());
+  public ZooKeeperTestingServer(TemporaryFolder tmpDir) {","[{'comment': 'This should not accept the TemporaryFolder type, because that is a JUnit specific Rule type. It should instead accept a normal `File` directory, and `TEMP.newFolder()` be called prior to constructing this.', 'commenter': 'ctubbsii'}, {'comment': ""> This should not accept the TemporaryFolder type\r\n\r\nWhy? ZooKeeperTestingServer is a test class and this enforces that a TemporaryFolder is being used as part of the test.  I don't understand the logic here."", 'commenter': 'dlmarion'}, {'comment': ""Because there's no reason to leak JUnit object types outside of JUnit test cases. It makes it harder to update JUnit test cases when we update JUnit-related things, and harder to reuse the utility code for situations where JUnit isn't on the class path (like an IT, inside a server, or inside accumulo-maven-plugin, etc.). We also try to avoid using JUnit assert methods in test utilities for the same reason (though I'm sure we have a few). It's not a strict rule or anything, just a general best practice to keep the code more maintainable and reusable."", 'commenter': 'ctubbsii'}, {'comment': ""The JUnit5 conversion was something that was on my radar for this, because I am aware that they got rid of TemporaryFolder. So, I was hoping to limit how much needed to be changed for migrating to JUnit5 regarding this code. If it's not done now, it'll need to be done later, so I figured let's avoid the churn and set ourselves up by using File instead of TemporaryFolder now.\r\n\r\n(Also, spotbugs is failing, but I assume you saw that already)"", 'commenter': 'ctubbsii'}, {'comment': 'Removed TemporaryFolder in f9ed9a8', 'commenter': 'dlmarion'}]"
2496,test/src/main/java/org/apache/accumulo/test/zookeeper/ZooKeeperTestingServer.java,"@@ -47,16 +46,14 @@
    * Instantiate a running zookeeper server - this call will block until the server is ready for
    * client connections. It will try three times, with a 5 second pause to connect.
    */
-  public ZooKeeperTestingServer() {
-    this(PortUtils.getRandomFreePort());
+  public ZooKeeperTestingServer(File tmpDir) {
+    this(tmpDir, PortUtils.getRandomFreePort());
   }
 
-  private ZooKeeperTestingServer(int port) {
+  private ZooKeeperTestingServer(File tmpDir, int port) {
 ","[{'comment': 'I wonder if adding a check here that tmpDir is indeed a directory would be good. Not sure if its needed but we could add something like:\r\n```\r\nPreconditions.checkArgument(tmpDir.isDirectory());\r\n```\r\nor\r\n```\r\nPreconditions.checkArgument(tmpDir.isDirectory() || tmpDir.mkdir());\r\n```', 'commenter': 'DomGarguilo'}]"
2512,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/external/flot/jquery.flot.time.js,"@@ -248,25 +248,25 @@ API.txt for details.
         ""microsecond"": 0.001,
         ""millisecond"": 1,
         ""second"": 1000,
-        ""minute"": 60 * 1000,
-        ""hour"": 60 * 60 * 1000,
-        ""day"": 24 * 60 * 60 * 1000,
-        ""month"": 30 * 24 * 60 * 60 * 1000,
-        ""quarter"": 3 * 30 * 24 * 60 * 60 * 1000,
-        ""year"": 365.2425 * 24 * 60 * 60 * 1000
+        ""minute"": 60_000,
+        ""hour"": 60 * 60_000,
+        ""day"": 24 * 60 * 60_000,
+        ""month"": 30 * 24 * 60 * 60_000,
+        ""quarter"": 3 * 30 * 24 * 60 * 60_000,
+        ""year"": 365.2425 * 24 * 60 * 60_000
     };
 
     // map of app. size of time units in microseconds
     var timeUnitSizeMicroseconds = {
         ""microsecond"": 1,
         ""millisecond"": 1000,
         ""second"": 1000000,
-        ""minute"": 60 * 1000000,
-        ""hour"": 60 * 60 * 1000000,
-        ""day"": 24 * 60 * 60 * 1000000,
-        ""month"": 30 * 24 * 60 * 60 * 1000000,
-        ""quarter"": 3 * 30 * 24 * 60 * 60 * 1000000,
-        ""year"": 365.2425 * 24 * 60 * 60 * 1000000
+        ""minute"": 60_000_000,
+        ""hour"": 60 * 60_000_000,
+        ""day"": 24 * 60 * 60_000_000,
+        ""month"": 30 * 24 * 60 * 60_000_000,
+        ""quarter"": 3 * 30 * 24 * 60 * 60_000_000,
+        ""year"": 365.2425 * 24 * 60 * 60_000_000","[{'comment': ""This file shouldn't be changed, because it's a bundled third-party file and not our own code. We don't want it to diverge from the upstream bytes we got it from."", 'commenter': 'ctubbsii'}]"
2512,core/src/test/java/org/apache/accumulo/core/client/BatchWriterConfigTest.java,"@@ -62,8 +62,8 @@ public void testOverridingDefaults() {
     bwConfig.setDurability(Durability.NONE);
 
     assertEquals(1123581321L, bwConfig.getMaxMemory());
-    assertEquals(22 * 60 * 60 * 1000L, bwConfig.getMaxLatency(TimeUnit.MILLISECONDS));
-    assertEquals(33 * 24 * 60 * 60 * 1000L, bwConfig.getTimeout(TimeUnit.MILLISECONDS));
+    assertEquals(22 * 60 * 60_000L, bwConfig.getMaxLatency(TimeUnit.MILLISECONDS));
+    assertEquals(33 * 24 * 60 * 60_000L, bwConfig.getTimeout(TimeUnit.MILLISECONDS));","[{'comment': 'It might improve clarity to use time conversions instead of replacing a portion. \r\n```\r\n    assertEquals(TimeUnit.HOURS.toMillis(22), bwConfig.getMaxLatency(TimeUnit.MILLISECONDS));\r\n    assertEquals(TimeUnit.DAYS.toMillis(33), bwConfig.getTimeout(TimeUnit.MILLISECONDS));\r\n```', 'commenter': 'EdColeman'}]"
2512,core/src/main/java/org/apache/accumulo/core/spi/balancer/HostRegexTableLoadBalancer.java,"@@ -175,7 +175,7 @@
     }
   }
 
-  private static final long ONE_HOUR = 60 * 60 * 1000;
+  private static final long ONE_HOUR = 60 * 60_000;","[{'comment': 'This could be \r\n```\r\n  private static final long ONE_HOUR = TimeUnit.HOURS.toMillis(1);\r\n```', 'commenter': 'EdColeman'}]"
2512,core/src/test/java/org/apache/accumulo/core/rpc/TTimeoutTransportTest.java,"@@ -75,7 +75,7 @@ public void testFailedSocketOpenIsClosed() throws IOException {
 
   @Test
   public void testFailedInputStreamClosesSocket() throws IOException {
-    long timeout = 2 * 60 * 1000; // 2 mins
+    long timeout = 2 * 60_000; // 2 mins","[{'comment': 'Could be\r\n```\r\n    long timeout = TimeUnit.MINUTES.toMillis(2); \r\n```', 'commenter': 'EdColeman'}]"
2512,core/src/test/java/org/apache/accumulo/core/rpc/TTimeoutTransportTest.java,"@@ -105,7 +105,7 @@ public void testFailedInputStreamClosesSocket() throws IOException {
 
   @Test
   public void testFailedOutputStreamClosesSocket() throws IOException {
-    long timeout = 2 * 60 * 1000; // 2 mins
+    long timeout = 2 * 60_000; // 2 mins","[{'comment': 'Could be\r\n```\r\n    long timeout = TimeUnit.MINUTES.toMillis(2); \r\n```\r\n', 'commenter': 'EdColeman'}]"
2512,server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java,"@@ -310,7 +310,7 @@ private synchronized void checkServer(final Set<TServerInstance> updates,
       Long firstSeen = locklessServers.get(zPath);
       if (firstSeen == null) {
         locklessServers.put(zPath, System.currentTimeMillis());
-      } else if (System.currentTimeMillis() - firstSeen > 10 * 60 * 1000) {
+      } else if (System.currentTimeMillis() - firstSeen > 10 * 60_000) {
         deleteServerNode(path + ""/"" + zPath);","[{'comment': 'Could be \r\n```\r\n      } else if (System.currentTimeMillis() - firstSeen > TimeUnit.MINUTES.toMillis(10)) {\r\n```', 'commenter': 'EdColeman'}]"
2512,server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java,"@@ -176,7 +176,7 @@
     }
   }
 
-  private static final long ONE_HOUR = 60 * 60 * 1000;
+  private static final long ONE_HOUR = 60 * 60_000;","[{'comment': 'Could be\r\n```\r\n  private static final long ONE_HOUR = TimeUnit.HOURS.toMillis(1);\r\n```', 'commenter': 'EdColeman'}]"
2512,server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java,"@@ -66,7 +66,7 @@ public static void setupKeyGenerator() throws Exception {
   private ZooReader zk;
   private InstanceId instanceId;
   private String baseNode;
-  private long tokenLifetime = 7 * 24 * 60 * 60 * 1000; // 7days
+  private long tokenLifetime = 7 * 24 * 60 * 60_000; // 7days","[{'comment': 'Could be\r\n```\r\n  private long tokenLifetime = TimeUnit.DAYS.toMillis(7); // 7days\r\n```', 'commenter': 'EdColeman'}]"
2512,server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java,"@@ -134,7 +134,7 @@ public static void main(String[] args) throws Exception {
     return Collections.synchronizedList(new LinkedList<>() {
 
       private static final long serialVersionUID = 1L;
-      private final long maxDelta = 60 * 60 * 1000;
+      private final long maxDelta = 60 * 60_000;","[{'comment': 'Could be\r\n```\r\n      private final long maxDelta = TimeUnit.MINUTES.toMillis(60);\r\n```', 'commenter': 'EdColeman'}]"
2512,server/tserver/src/main/java/org/apache/accumulo/tserver/AssignmentHandler.java,"@@ -217,7 +217,7 @@ public void run() {
       }
       log.warn(""failed to open tablet {} reporting failure to manager"", extent);
       server.enqueueManagerMessage(new TabletStatusMessage(TabletLoadState.LOAD_FAILURE, extent));
-      long reschedule = Math.min((1L << Math.min(32, retryAttempt)) * 1000, 10 * 60 * 1000L);
+      long reschedule = Math.min((1L << Math.min(32, retryAttempt)) * 1000, 10 * 60_000L);","[{'comment': 'Could be\r\n```\r\nlong reschedule = Math.min((1L << Math.min(32, retryAttempt)) * 1000, TimeUnit.MINUTES.toMillis(10));\r\n```', 'commenter': 'EdColeman'}]"
2512,start/src/main/java/org/apache/accumulo/start/classloader/vfs/AccumuloReloadingVFSClassLoader.java,"@@ -52,7 +52,7 @@
 
   // set to 5 mins. The rationale behind this large time is to avoid a gazillion tservers all asking
   // the name node for info too frequently.
-  private static final int DEFAULT_TIMEOUT = 5 * 60 * 1000;
+  private static final int DEFAULT_TIMEOUT = 5 * 60_000;","[{'comment': 'Could be\r\n```\r\n   private static final int DEFAULT_TIMEOUT = TimeUnit.MINUTES.toMillis(5);\r\n```', 'commenter': 'EdColeman'}]"
2512,test/src/main/java/org/apache/accumulo/test/functional/KerberosIT.java,"@@ -622,7 +622,7 @@ public void testDelegationTokenWithReducedLifetime() throws Throwable {
 
     AuthenticationTokenIdentifier identifier = ((DelegationTokenImpl) dt).getIdentifier();
     assertTrue(""Expected identifier to expire in no more than 5 minutes: "" + identifier,
-        identifier.getExpirationDate() - identifier.getIssueDate() <= (5 * 60 * 1000));
+        identifier.getExpirationDate() - identifier.getIssueDate() <= (5 * 60_000));","[{'comment': 'Could be:\r\n```\r\n        identifier.getExpirationDate() - identifier.getIssueDate() <= TimeUnit.MINUTES.toMillis(5));\r\n```', 'commenter': 'EdColeman'}]"
2513,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -385,7 +385,7 @@ public Options getOptions() {
     optEndRowExclusive.setArgName(""end-exclusive"");
     scanOptRow = new Option(""r"", ""row"", true, ""row to scan"");
     scanOptColumns = new Option(""c"", ""columns"", true,
-        ""comma-separated columns.This"" + "" option is mutually exclusive with cf and cq"");
+        ""comma-separated columns.This option is mutually exclusive with cf and cq"");","[{'comment': 'Might as well fix this while in here:\r\n\r\n```suggestion\r\n        ""comma-separated columns. This option is mutually exclusive with cf and cq"");\r\n```', 'commenter': 'ctubbsii'}]"
2513,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/external/flot/jquery.flot.time.js,"@@ -145,8 +145,8 @@ API.txt for details.
                     case 's': c = """" + formatSubSeconds(d.getMilliseconds(), d.getMicroseconds(), decimals); break;
                     case 'y': c = leftPad(d.getFullYear() % 100); break;
                     case 'Y': c = """" + d.getFullYear(); break;
-                    case 'p': c = (isAM) ? ("""" + ""am"") : ("""" + ""pm""); break;
-                    case 'P': c = (isAM) ? ("""" + ""AM"") : ("""" + ""PM""); break;
+                    case 'p': c = (isAM) ? (""am"") : (""pm""); break;
+                    case 'P': c = (isAM) ? (""AM"") : (""PM""); break;","[{'comment': ""This file should not be changed, because it's a third party library file that we've bundled. It should stay byte-for-byte equivalent to the upstream source."", 'commenter': 'ctubbsii'}]"
2513,core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/lru/LruBlockCacheConfiguration.java,"@@ -114,7 +114,7 @@ public LruBlockCacheConfiguration(Configuration conf, CacheType type) {
 
     if (this.getSingleFactor() + this.getMultiFactor() + this.getMemoryFactor() != 1) {
       throw new IllegalArgumentException(
-          ""Single, multi, and memory factors "" + "" should total 1.0"");
+          ""Single, multi, and memory factors  should total 1.0"");","[{'comment': 'Looks like this one revealed an extra space:\r\n```suggestion\r\n          ""Single, multi, and memory factors should total 1.0"");\r\n```', 'commenter': 'ctubbsii'}]"
2513,pom.xml,"@@ -1119,6 +1119,11 @@
                   <property name=""format"" value=""(^[A-Z][0-9]?)$|([A-Z][a-zA-Z0-9]*[T]$)"" />
                 </module>
                 <module name=""NonEmptyAtclauseDescription"" />
+                <module name=""RegexpSinglelineJava"">
+                  <!-- double escape quotes because checkstyle passes these through another xml parser -->
+                  <property name=""format"" value=""&amp;quot; [+] &amp;quot;"" />
+                  <property name=""message"" value=""Unnecessary concatenation of string literals"" />
+                </module>","[{'comment': ""There are several others of this same type in here. This should be relocated to that group of modules. But, don't worry, I'll take care of it before I merge. :smiley_cat: Thanks!"", 'commenter': 'ctubbsii'}]"
2515,core/src/main/java/org/apache/accumulo/core/classloader/DefaultContextClassLoaderFactory.java,"@@ -69,7 +71,7 @@ private static void startCleanupThread(final AccumuloConfiguration conf,
               .collect(Collectors.toSet());
           LOG.trace(""{}-cleanup thread, contexts in use: {}"", className, contextsInUse);
           AccumuloVFSClassLoader.removeUnusedContexts(contextsInUse);
-        }), 60_000, 60_000, TimeUnit.MILLISECONDS);
+        }), MINUTES.toMillis(1), MINUTES.toMillis(1), TimeUnit.MILLISECONDS);","[{'comment': 'You could static import the MILLISECONDS as well, to be consistent. Or, in this case, you could do:\r\n\r\n```suggestion\r\n        }), 1, 1, MINUTES);\r\n```', 'commenter': 'ctubbsii'}]"
2515,core/src/test/java/org/apache/accumulo/core/conf/ConfigurationTypeHelperTest.java,"@@ -80,11 +81,11 @@ public void testGetMemoryAsBytesFailureCases2() {
 
   @Test
   public void testGetTimeInMillis() {
-    assertEquals(42L * 24 * 60 * 60 * 1000, ConfigurationTypeHelper.getTimeInMillis(""42d""));
-    assertEquals(42L * 60 * 60 * 1000, ConfigurationTypeHelper.getTimeInMillis(""42h""));
-    assertEquals(42L * 60 * 1000, ConfigurationTypeHelper.getTimeInMillis(""42m""));
-    assertEquals(42L * 1000, ConfigurationTypeHelper.getTimeInMillis(""42s""));
-    assertEquals(42L * 1000, ConfigurationTypeHelper.getTimeInMillis(""42""));
+    assertEquals(DAYS.toMillis(42), ConfigurationTypeHelper.getTimeInMillis(""42d""));
+    assertEquals(HOURS.toMillis(42), ConfigurationTypeHelper.getTimeInMillis(""42h""));
+    assertEquals(MINUTES.toMillis(42), ConfigurationTypeHelper.getTimeInMillis(""42m""));
+    assertEquals(SECONDS.toMillis(42), ConfigurationTypeHelper.getTimeInMillis(""42s""));
+    assertEquals(42_000L, ConfigurationTypeHelper.getTimeInMillis(""42""));","[{'comment': 'Why not?\r\n\r\n```suggestion\r\n    assertEquals(SECONDS.toMillis(42), ConfigurationTypeHelper.getTimeInMillis(""42""));\r\n```', 'commenter': 'ctubbsii'}, {'comment': '42_000L seemed shorter and cleaner to me :p', 'commenter': 'KikiManjaro'}]"
2515,core/src/test/java/org/apache/accumulo/core/rpc/TTimeoutTransportTest.java,"@@ -75,7 +76,7 @@ public void testFailedSocketOpenIsClosed() throws IOException {
 
   @Test
   public void testFailedInputStreamClosesSocket() throws IOException {
-    long timeout = 2 * 60_000; // 2 mins
+    long timeout = MINUTES.toMillis(2); // 2 mins","[{'comment': '```suggestion\r\n    long timeout = MINUTES.toMillis(2);\r\n```', 'commenter': 'ctubbsii'}]"
2515,core/src/test/java/org/apache/accumulo/core/rpc/TTimeoutTransportTest.java,"@@ -105,7 +106,7 @@ public void testFailedInputStreamClosesSocket() throws IOException {
 
   @Test
   public void testFailedOutputStreamClosesSocket() throws IOException {
-    long timeout = 2 * 60_000; // 2 mins
+    long timeout = MINUTES.toMillis(2); // 2 mins","[{'comment': '```suggestion\r\n    long timeout = MINUTES.toMillis(2);\r\n```', 'commenter': 'ctubbsii'}]"
2515,server/base/src/main/java/org/apache/accumulo/server/ServerContext.java,"@@ -434,7 +435,7 @@ private void monitorSwappiness() {
       } catch (Exception t) {
         log.error("""", t);
       }
-    }, 1000, 10 * 60_000, TimeUnit.MILLISECONDS);
+    }, 1000, MINUTES.toMillis(10), TimeUnit.MILLISECONDS);","[{'comment': '```suggestion\r\n    }, SECONDS.toMillis(1), MINUTES.toMillis(10), TimeUnit.MILLISECONDS);\r\n```', 'commenter': 'ctubbsii'}]"
2515,server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java,"@@ -66,7 +68,7 @@ public static void setupKeyGenerator() throws Exception {
   private ZooReader zk;
   private InstanceId instanceId;
   private String baseNode;
-  private long tokenLifetime = 7 * 24 * 60 * 60_000; // 7days
+  private long tokenLifetime = DAYS.toMillis(7); // 7days","[{'comment': '```suggestion\r\n  private long tokenLifetime = DAYS.toMillis(7);\r\n```', 'commenter': 'ctubbsii'}]"
2515,test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java,"@@ -135,7 +136,7 @@ public void changeTableStateTest() throws Exception {
     OnlineOpTiming timing1 = task.get();
 
     log.trace(""Online 1 in {} ms"",
-        TimeUnit.MILLISECONDS.convert(timing1.runningTime(), TimeUnit.NANOSECONDS));
+        MILLISECONDS.convert(timing1.runningTime(), NANOSECONDS));","[{'comment': ""`.convert()` is a very confusing, because it's not obvious which direction it is converting. The following would be better for these:\r\n\r\n```suggestion\r\n        NANOSECONDS.toMillis(timing1.runningTime()));\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'This note applied to the other occurrences in the same file (and possibly elsewhere).', 'commenter': 'ctubbsii'}]"
2515,test/src/main/java/org/apache/accumulo/test/functional/KerberosIT.java,"@@ -616,13 +617,13 @@ public void testDelegationTokenWithReducedLifetime() throws Throwable {
             assertEquals(rootUser.getPrincipal(), client.whoami());
 
             return client.securityOperations().getDelegationToken(
-                new DelegationTokenConfig().setTokenLifetime(5, TimeUnit.MINUTES));
+                new DelegationTokenConfig().setTokenLifetime(5, MINUTES));
           }
         });
 
     AuthenticationTokenIdentifier identifier = ((DelegationTokenImpl) dt).getIdentifier();
     assertTrue(""Expected identifier to expire in no more than 5 minutes: "" + identifier,
-        identifier.getExpirationDate() - identifier.getIssueDate() <= (5 * 60_000));
+        identifier.getExpirationDate() - identifier.getIssueDate() <= (MINUTES.toMillis(5)));","[{'comment': 'After the conversion, this has some extra parens that could be removed:\r\n```suggestion\r\n        identifier.getExpirationDate() - identifier.getIssueDate() <= MINUTES.toMillis(5));\r\n```', 'commenter': 'ctubbsii'}]"
2518,core/pom.xml,"@@ -275,6 +275,10 @@
               <excludes />
               <allows>
                 <allow>io[.]opentelemetry[.]api[.]OpenTelemetry</allow>
+                <allow>org[.]apache[.]hadoop[.]conf[.]Configuration</allow>
+                <allow>org[.]apache[.]hadoop[.]io[.]compress[.]CompressionCodec</allow>
+                <allow>org[.]apache[.]hadoop[.]io[.]compress[.]Decompressor</allow>
+                <allow>org[.]apache[.]hadoop[.]io[.]compress[.]Compressor</allow>","[{'comment': 'Rather than allowing these types to be exposed in our SPI, why not refactor to avoid exposing the Hadoop types? Do we really need these types? Can we use our own analogous ones? Or can we avoid them entirely, deferring to the plugin implementor?', 'commenter': 'ctubbsii'}, {'comment': ""I removed Configuration in c377cb8. I don't really see an easy way to remove the others since the goal of the plugin is to provide a Codec / Compressor / Decompressor. If we internally were not using Hadoop Codecs, then I think this would be much easier to accomplish. Is that what you are suggesting, that we move the internal implementation away from Hadoop Codecs?"", 'commenter': 'dlmarion'}, {'comment': ""> Is that what you are suggesting, that we move the internal implementation away from Hadoop Codecs?\r\n\r\nWe could, but that's not necessarily what I'm suggesting. I'm just suggesting that it may be possible to use Accumulo-only types in the SPI, and then wrap the or unwrap the configured implementing class to present it as a Hadoop Codec (for the internal code that requires a Codec). That way, we're not bound in our SPI to Hadoop types if we want to change the internal implementation later."", 'commenter': 'ctubbsii'}, {'comment': 'I think the probability of moving from Hadoop Codecs to something else is very low. It would likely occur at the same time that we implement our own FileSystem layer, which would likely have to happen in a major release where we could change the public API anyway.', 'commenter': 'dlmarion'}, {'comment': 'You could just create our own type for SPI and make all the Hadoop types internal private variables', 'commenter': 'milleruntime'}, {'comment': ""My intent in proposing this is to avoid introducing *new* exposures of Hadoop types in our API/SPI. Even if we're not moving away from it soon, the more we expose in our API/SPI, the harder it is to abstract Hadoop out later."", 'commenter': 'ctubbsii'}, {'comment': ""> You could just create our own type for SPI and make all the Hadoop types internal private variables\r\n\r\nIt's not clear to me how a plug-in would create an object and then we convert it to a Codec / Compressor / Decompressor in the internal code. Do you have an example where this is done elsewhere?"", 'commenter': 'dlmarion'}, {'comment': ""If the methods on our own plugin provided all the necessary functionality, it would be trivial to write a pass-through Hadoop Codec that merely called our SPI methods. Then, whatever the user configures can be easily wrapped by that single Codec internally. For implementing classes we provide, we could even make all our SPI methods return `UnsupportedOperationException`, and instead just use its name, as in:\r\n\r\n```java\r\nCodec getCodec() {\r\n  var configuredCompressionPlugin = instantiateConfiguredClass();\r\n  if (configuredCompressionPlugin.getClass().equals(AccumuloGZipCompression.class)) {\r\n    return new GZipCodec(); // return Hadoop's equivalent\r\n  } else if (\r\n    // check other built-in known types\r\n  ) {\r\n    // ....\r\n  } else {\r\n    return new InternalCodecWrapperForAccumuloCompressionPlugin(configuredCompressionPlugin);\r\n  }\r\n}\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'I backed out the changes to core/pom.xml in 91942f7', 'commenter': 'dlmarion'}]"
2518,core/src/main/java/org/apache/accumulo/core/spi/file/rfile/compression/Algorithm.java,"@@ -0,0 +1,326 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.file.rfile.compression;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.FilterOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionInputStream;
+import org.apache.hadoop.io.compress.CompressionOutputStream;
+import org.apache.hadoop.io.compress.Compressor;
+import org.apache.hadoop.io.compress.Decompressor;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.cache.CacheBuilder;
+import com.google.common.cache.CacheLoader;
+import com.google.common.cache.LoadingCache;
+import com.google.common.collect.Maps;
+
+/**
+ * Compression algorithms. There is a static initializer, below the values defined in the
+ * enumeration, that calls the initializer of all defined codecs within {@link Algorithm}. This
+ * promotes a model of the following call graph of initialization by the static initializer,
+ * followed by calls to {@link #getCodec()},
+ * {@link #createCompressionStream(OutputStream, Compressor, int)}, and
+ * {@link #createDecompressionStream(InputStream, Decompressor, int)}. In some cases, the
+ * compression and decompression call methods will include a different buffer size for the stream.
+ * Note that if the compressed buffer size requested in these calls is zero, we will not set the
+ * buffer size for that algorithm. Instead, we will use the default within the codec.
+ * <p>
+ * The buffer size is configured in the Codec by way of a Hadoop {@link Configuration} reference.
+ * One approach may be to use the same Configuration object, but when calls are made to
+ * {@code createCompressionStream} and {@code createDecompressionStream} with non default buffer
+ * sizes, the configuration object must be changed. In this case, concurrent calls to
+ * {@code createCompressionStream} and {@code createDecompressionStream} would mutate the
+ * configuration object beneath each other, requiring synchronization to avoid undesirable activity
+ * via co-modification. To avoid synchronization entirely, we will create Codecs with their own
+ * Configuration object and cache them for re-use. A default codec will be statically created, as
+ * mentioned above to ensure we always have a codec available at loader initialization.
+ * <p>
+ * There is a Guava cache defined within Algorithm that allows us to cache Codecs for re-use. Since
+ * they will have their own configuration object and thus do not need to be mutable, there is no
+ * concern for using them concurrently; however, the Guava cache exists to ensure a maximal size of
+ * the cache and efficient and concurrent read/write access to the cache itself.
+ * <p>
+ * To provide Algorithm specific details and to describe what is in code:
+ * <p>
+ * LZO will always have the default LZO codec because the buffer size is never overridden within it.
+ * <p>
+ * LZ4 will always have the default LZ4 codec because the buffer size is never overridden within it.
+ * <p>
+ * GZ will use the default GZ codec for the compression stream, but can potentially use a different
+ * codec instance for the decompression stream if the requested buffer size does not match the
+ * default GZ buffer size of 32k.
+ * <p>
+ * Snappy will use the default Snappy codec with the default buffer size of 64k for the compression
+ * stream, but will use a cached codec if the buffer size differs from the default.
+ */
+public abstract class Algorithm {","[{'comment': 'So you just moved this file? It looks similar. Did you make any changes?', 'commenter': 'milleruntime'}, {'comment': ""Yes, I just moved it from the enum in Compression to it's own class. I couldn't dynamically add values to the enum, so I had to make it something that could be extended. So, the enum methods are here in this new class, and the enum values (Gz, Bzip, etc) now extend this class and are still located in the Compression class (not in the spi package)"", 'commenter': 'dlmarion'}]"
2518,core/src/main/java/org/apache/accumulo/core/spi/file/rfile/compression/None.java,"@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.file.rfile.compression;
+
+import org.apache.accumulo.core.file.rfile.bcfile.IdentityCodec;
+
+import com.google.auto.service.AutoService;
+
+@AutoService(CompressionAlgorithm.class)
+public class None implements CompressionAlgorithm {","[{'comment': 'I would prefer a more descriptive class name. You could call it `NoCompression`. This would be consistent with how we named the `NoCryptoSerivce`', 'commenter': 'milleruntime'}, {'comment': 'I rename `None` to `NoCompression` in 9b22ead', 'commenter': 'dlmarion'}]"
2518,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/BCFile.java,"@@ -138,7 +137,7 @@ public long getLength() {
       private final SimpleBufferedOutputStream fsBufferedOutput;
       private OutputStream out;
 
-      public WBlockState(Algorithm compressionAlgo, RateLimitedOutputStream fsOut,
+      public WBlockState(DefaultCompressionAlgorithm compressionAlgo, RateLimitedOutputStream fsOut,","[{'comment': 'It looks like you replaced the enum with the default class in multiple places here. Would it be better to pass the interface type `CompressionAlgorithm` in these places? I am not sure if you want to always pass around the default class.', 'commenter': 'milleruntime'}, {'comment': '`CompressionAlgorithm` is really just the config. `DefaultCompressionAlgorithm` is the one, and currently only, implementation. There is no ability for a user to supply their own algorithm implementation, just the parameters/config for the algorithm. Maybe I need to change the class names to make that more clear', 'commenter': 'dlmarion'}, {'comment': 'I renamed `CompressionAlgorithm` to `CompressionAlgorithmConfiguration` in 9b22ead', 'commenter': 'dlmarion'}]"
2518,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/DefaultCompressionAlgorithm.java,"@@ -0,0 +1,354 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.file.rfile.bcfile;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.FilterOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.util.Map.Entry;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.accumulo.core.spi.file.rfile.compression.CompressionAlgorithm;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionInputStream;
+import org.apache.hadoop.io.compress.CompressionOutputStream;
+import org.apache.hadoop.io.compress.Compressor;
+import org.apache.hadoop.io.compress.Decompressor;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.cache.CacheBuilder;
+import com.google.common.cache.CacheLoader;
+import com.google.common.cache.LoadingCache;
+import com.google.common.collect.Maps;
+
+/**
+ * Compression algorithms. There is a static initializer, below the values defined in the
+ * enumeration, that calls the initializer of all defined codecs within
+ * {@link DefaultCompressionAlgorithm}. This promotes a model of the following call graph of
+ * initialization by the static initializer, followed by calls to {@link #getCodec()},
+ * {@link #createCompressionStream(OutputStream, Compressor, int)}, and
+ * {@link #createDecompressionStream(InputStream, Decompressor, int)}. In some cases, the
+ * compression and decompression call methods will include a different buffer size for the stream.
+ * Note that if the compressed buffer size requested in these calls is zero, we will not set the
+ * buffer size for that algorithm. Instead, we will use the default within the codec.
+ * <p>
+ * The buffer size is configured in the Codec by way of a Hadoop {@link Configuration} reference.
+ * One approach may be to use the same Configuration object, but when calls are made to
+ * {@code createCompressionStream} and {@code createDecompressionStream} with non default buffer
+ * sizes, the configuration object must be changed. In this case, concurrent calls to
+ * {@code createCompressionStream} and {@code createDecompressionStream} would mutate the
+ * configuration object beneath each other, requiring synchronization to avoid undesirable activity
+ * via co-modification. To avoid synchronization entirely, we will create Codecs with their own
+ * Configuration object and cache them for re-use. A default codec will be statically created, as
+ * mentioned above to ensure we always have a codec available at loader initialization.
+ * <p>
+ * There is a Guava cache defined within Algorithm that allows us to cache Codecs for re-use. Since
+ * they will have their own configuration object and thus do not need to be mutable, there is no
+ * concern for using them concurrently; however, the Guava cache exists to ensure a maximal size of
+ * the cache and efficient and concurrent read/write access to the cache itself.
+ * <p>
+ * To provide Algorithm specific details and to describe what is in code:
+ * <p>
+ * LZO will always have the default LZO codec because the buffer size is never overridden within it.
+ * <p>
+ * LZ4 will always have the default LZ4 codec because the buffer size is never overridden within it.
+ * <p>
+ * GZ will use the default GZ codec for the compression stream, but can potentially use a different
+ * codec instance for the decompression stream if the requested buffer size does not match the
+ * default GZ buffer size of 32k.
+ * <p>
+ * Snappy will use the default Snappy codec with the default buffer size of 64k for the compression
+ * stream, but will use a cached codec if the buffer size differs from the default.
+ */
+public class DefaultCompressionAlgorithm extends Configured {
+
+  public static class FinishOnFlushCompressionStream extends FilterOutputStream {
+
+    FinishOnFlushCompressionStream(CompressionOutputStream cout) {
+      super(cout);
+    }
+
+    @Override
+    public void write(byte[] b, int off, int len) throws IOException {
+      out.write(b, off, len);
+    }
+
+    @Override
+    public void flush() throws IOException {
+      CompressionOutputStream cout = (CompressionOutputStream) out;
+      cout.finish();
+      cout.flush();
+      cout.resetState();
+    }
+  }
+
+  private static final Logger LOG = LoggerFactory.getLogger(DefaultCompressionAlgorithm.class);
+
+  /**
+   * Guava cache to have a limited factory pattern defined in the Algorithm enum.
+   */
+  private static LoadingCache<Entry<DefaultCompressionAlgorithm,Integer>,
+      CompressionCodec> codecCache =
+          CacheBuilder.newBuilder().maximumSize(25).build(new CacheLoader<>() {
+            @Override
+            public CompressionCodec load(Entry<DefaultCompressionAlgorithm,Integer> key) {
+              return key.getKey().createNewCodec(key.getValue());
+            }
+          });","[{'comment': 'Do we want this cache here? It looks like before we had one cache in Compression class and now you expanded it out to have a cache per type. I am just wondering if this is the best place. We probably still want to cache the codecs but is this something that Hadoop does already?', 'commenter': 'milleruntime'}, {'comment': ""It's static, so it should be one cache. This should be equivalent to what existed before. It's caching codecs with different buffer sizes."", 'commenter': 'dlmarion'}, {'comment': 'Ah I see now.', 'commenter': 'milleruntime'}]"
2518,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -18,843 +18,73 @@
  */
 package org.apache.accumulo.core.file.rfile.bcfile;
 
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.FilterOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
-import java.util.Map.Entry;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.ServiceLoader;
 
+import org.apache.accumulo.core.spi.file.rfile.compression.CompressionAlgorithm;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.compress.CodecPool;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionInputStream;
-import org.apache.hadoop.io.compress.CompressionOutputStream;
-import org.apache.hadoop.io.compress.Compressor;
-import org.apache.hadoop.io.compress.Decompressor;
-import org.apache.hadoop.io.compress.DefaultCodec;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.CacheLoader;
-import com.google.common.cache.LoadingCache;
-import com.google.common.collect.Maps;
 
 /**
  * Compression related stuff.
  */
 public final class Compression {
 
-  private static final Logger log = LoggerFactory.getLogger(Compression.class);
-
   /**
    * Prevent the instantiation of this class.
    */
   private Compression() {
     throw new UnsupportedOperationException();
   }
 
-  static class FinishOnFlushCompressionStream extends FilterOutputStream {
-
-    FinishOnFlushCompressionStream(CompressionOutputStream cout) {
-      super(cout);
-    }
-
-    @Override
-    public void write(byte[] b, int off, int len) throws IOException {
-      out.write(b, off, len);
-    }
-
-    @Override
-    public void flush() throws IOException {
-      CompressionOutputStream cout = (CompressionOutputStream) out;
-      cout.finish();
-      cout.flush();
-      cout.resetState();
-    }
-  }
-
-  /**
-   * Compression: bzip2
-   */
-  public static final String COMPRESSION_BZIP2 = ""bzip2"";
-
-  /**
-   * Compression: zStandard
-   */
-  public static final String COMPRESSION_ZSTD = ""zstd"";
-
-  /**
-   * Compression: snappy
-   **/
-  public static final String COMPRESSION_SNAPPY = ""snappy"";
-
-  /**
-   * Compression: gzip
-   */
-  public static final String COMPRESSION_GZ = ""gz"";
-
-  /**
-   * Compression: lzo
-   */
-  public static final String COMPRESSION_LZO = ""lzo"";
-
-  /**
-   * Compression: lz4
-   */
-  public static final String COMPRESSION_LZ4 = ""lz4"";
-
-  /**
-   * compression: none
-   */
-  public static final String COMPRESSION_NONE = ""none"";
-
-  /**
-   * Compression algorithms. There is a static initializer, below the values defined in the
-   * enumeration, that calls the initializer of all defined codecs within {@link Algorithm}. This
-   * promotes a model of the following call graph of initialization by the static initializer,
-   * followed by calls to {@link #getCodec()},
-   * {@link #createCompressionStream(OutputStream, Compressor, int)}, and
-   * {@link #createDecompressionStream(InputStream, Decompressor, int)}. In some cases, the
-   * compression and decompression call methods will include a different buffer size for the stream.
-   * Note that if the compressed buffer size requested in these calls is zero, we will not set the
-   * buffer size for that algorithm. Instead, we will use the default within the codec.
-   * <p>
-   * The buffer size is configured in the Codec by way of a Hadoop {@link Configuration} reference.
-   * One approach may be to use the same Configuration object, but when calls are made to
-   * {@code createCompressionStream} and {@code createDecompressionStream} with non default buffer
-   * sizes, the configuration object must be changed. In this case, concurrent calls to
-   * {@code createCompressionStream} and {@code createDecompressionStream} would mutate the
-   * configuration object beneath each other, requiring synchronization to avoid undesirable
-   * activity via co-modification. To avoid synchronization entirely, we will create Codecs with
-   * their own Configuration object and cache them for re-use. A default codec will be statically
-   * created, as mentioned above to ensure we always have a codec available at loader
-   * initialization.
-   * <p>
-   * There is a Guava cache defined within Algorithm that allows us to cache Codecs for re-use.
-   * Since they will have their own configuration object and thus do not need to be mutable, there
-   * is no concern for using them concurrently; however, the Guava cache exists to ensure a maximal
-   * size of the cache and efficient and concurrent read/write access to the cache itself.
-   * <p>
-   * To provide Algorithm specific details and to describe what is in code:
-   * <p>
-   * LZO will always have the default LZO codec because the buffer size is never overridden within
-   * it.
-   * <p>
-   * LZ4 will always have the default LZ4 codec because the buffer size is never overridden within
-   * it.
-   * <p>
-   * GZ will use the default GZ codec for the compression stream, but can potentially use a
-   * different codec instance for the decompression stream if the requested buffer size does not
-   * match the default GZ buffer size of 32k.
-   * <p>
-   * Snappy will use the default Snappy codec with the default buffer size of 64k for the
-   * compression stream, but will use a cached codec if the buffer size differs from the default.
-   */
-  public enum Algorithm {
-
-    BZIP2(COMPRESSION_BZIP2) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.BZip2Codec"";
-
-      /**
-       * Configuration option for BZip2 buffer size. Uses the default FS buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.file.buffer.size"";
-
-      /**
-       * Default buffer size. Changed from default of 4096.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_BZIP2_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""BZip2 codec class not specified. Did you forget to set property ""
-              + CONF_BZIP2_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""BZip2 codec class not specified. Did you forget to set property ""
-              + CONF_BZIP2_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    LZO(COMPRESSION_LZO) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.LzoCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.lzo.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_LZO_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZO codec class not specified. Did you forget to set property ""
-              + CONF_LZO_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZO codec class not specified. Did you forget to set property ""
-              + CONF_LZO_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    LZ4(COMPRESSION_LZ4) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.Lz4Codec"";
-
-      /**
-       * Configuration option for LZ4 buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.lz4.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 256 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_LZ4_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZ4 codec class not specified. Did you forget to set property ""
-              + CONF_LZ4_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZ4 codec class not specified. Did you forget to set property ""
-              + CONF_LZ4_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    GZ(COMPRESSION_GZ) {
-
-      private transient DefaultCodec codec = null;
-
-      /**
-       * Configuration option for gz buffer size
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.file.buffer.size"";
-
-      /**
-       * Default buffer size
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 32 * 1024;
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = (DefaultCodec) createNewCodec(DEFAULT_BUFFER_SIZE);
-      }
-
-      /**
-       * Creates a new GZ codec
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        Configuration newConfig = new Configuration(conf);
-        updateBuffer(conf, BUFFER_SIZE_OPT, bufferSize);
-        DefaultCodec newCodec = new DefaultCodec();
-        newCodec.setConf(newConfig);
-        return newCodec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, GZ, codec);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return true;
-      }
-    },
-
-    NONE(COMPRESSION_NONE) {
-      @Override
-      CompressionCodec getCodec() {
-        return null;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) {
-        return bufferStream(downStream, downStreamBufferSize);
-      }
-
-      @Override
-      public void initializeDefaultCodec() {}
-
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return null;
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) {
-        return bufferStream(downStream, downStreamBufferSize);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return true;
-      }
-    },
-
-    SNAPPY(COMPRESSION_SNAPPY) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.SnappyCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.snappy.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      /**
-       * Creates a new snappy codec.
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return createNewCodec(CONF_SNAPPY_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""SNAPPY codec class not specified. Did you forget to set property ""
-              + CONF_SNAPPY_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""SNAPPY codec class not specified. Did you forget to set property ""
-              + CONF_SNAPPY_CLASS + ""?"");
-        }
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, SNAPPY, codec);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-    },
-
-    ZSTANDARD(COMPRESSION_ZSTD) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.ZStandardCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.zstd.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      /**
-       * Creates a new ZStandard codec.
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return createNewCodec(CONF_ZSTD_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              ""ZStandard codec class not specified. Did you forget to set property ""
-                  + CONF_ZSTD_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              ""ZStandard codec class not specified. Did you forget to set property ""
-                  + CONF_ZSTD_CLASS + ""?"");
-        }
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, ZSTANDARD, codec);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-    };
-
-    /**
-     * Guava cache to have a limited factory pattern defined in the Algorithm enum.
-     */
-    private static LoadingCache<Entry<Algorithm,Integer>,CompressionCodec> codecCache =
-        CacheBuilder.newBuilder().maximumSize(25).build(new CacheLoader<>() {
-          @Override
-          public CompressionCodec load(Entry<Algorithm,Integer> key) {
-            return key.getKey().createNewCodec(key.getValue());
-          }
-        });
+  private static final Map<String,DefaultCompressionAlgorithm> CONFIGURED_ALGORITHMS =
+      new HashMap<>();
+
+  private static final ServiceLoader<CompressionAlgorithm> COMPRESSION_ALGORITHMS =
+      ServiceLoader.load(CompressionAlgorithm.class);
+
+  // All compression-related settings are required to be configured statically in the
+  // Configuration object.
+  protected static final Configuration conf;
+
+  // The model defined by the static block below creates a singleton for each defined codec in the
+  // Algorithm enumeration. By creating the codecs, each call to isSupported shall return
+  // true/false depending on if the codec singleton is defined. The static initializer, below,
+  // will ensure this occurs when the Enumeration is loaded. Furthermore, calls to getCodec will
+  // return the singleton, whether it is null or not.
+  //
+  // Calls to createCompressionStream and createDecompressionStream may return a different codec
+  // than getCodec, if the incoming downStreamBufferSize is different than the default. In such a
+  // case, we will place the resulting codec into the codecCache, defined below, to ensure we have
+  // cache codecs.
+  //
+  // Since codecs are immutable, there is no concern about concurrent access to the
+  // CompressionCodec objects within the guava cache.
+  static {
+    conf = new Configuration();","[{'comment': ""I don't know if we still need this static code block. This was probably written before we had ServerContext. I feel like we should just be getting the configuration from ServerContext."", 'commenter': 'milleruntime'}, {'comment': 'The static block is still needed for the other code in it, right?', 'commenter': 'dlmarion'}, {'comment': ""> I feel like we should just be getting the configuration from ServerContext.\r\n\r\nCan't. This is in `core` with the rest of the file-writing code. Maybe ClientContext, though.\r\n\r\n> The static block is still needed for the other code in it, right?\r\n\r\nNo. See my other comment."", 'commenter': 'ctubbsii'}]"
2518,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -18,843 +18,73 @@
  */
 package org.apache.accumulo.core.file.rfile.bcfile;
 
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.FilterOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
-import java.util.Map.Entry;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.ServiceLoader;
 
+import org.apache.accumulo.core.spi.file.rfile.compression.CompressionAlgorithmConfiguration;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.compress.CodecPool;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionInputStream;
-import org.apache.hadoop.io.compress.CompressionOutputStream;
-import org.apache.hadoop.io.compress.Compressor;
-import org.apache.hadoop.io.compress.Decompressor;
-import org.apache.hadoop.io.compress.DefaultCodec;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.CacheLoader;
-import com.google.common.cache.LoadingCache;
-import com.google.common.collect.Maps;
 
 /**
  * Compression related stuff.
  */
 public final class Compression {
 
-  private static final Logger log = LoggerFactory.getLogger(Compression.class);
-
   /**
    * Prevent the instantiation of this class.
    */
   private Compression() {
     throw new UnsupportedOperationException();
   }
 
-  static class FinishOnFlushCompressionStream extends FilterOutputStream {
-
-    FinishOnFlushCompressionStream(CompressionOutputStream cout) {
-      super(cout);
-    }
-
-    @Override
-    public void write(byte[] b, int off, int len) throws IOException {
-      out.write(b, off, len);
-    }
-
-    @Override
-    public void flush() throws IOException {
-      CompressionOutputStream cout = (CompressionOutputStream) out;
-      cout.finish();
-      cout.flush();
-      cout.resetState();
-    }
-  }
-
-  /**
-   * Compression: bzip2
-   */
-  public static final String COMPRESSION_BZIP2 = ""bzip2"";
-
-  /**
-   * Compression: zStandard
-   */
-  public static final String COMPRESSION_ZSTD = ""zstd"";
-
-  /**
-   * Compression: snappy
-   **/
-  public static final String COMPRESSION_SNAPPY = ""snappy"";
-
-  /**
-   * Compression: gzip
-   */
-  public static final String COMPRESSION_GZ = ""gz"";
-
-  /**
-   * Compression: lzo
-   */
-  public static final String COMPRESSION_LZO = ""lzo"";
-
-  /**
-   * Compression: lz4
-   */
-  public static final String COMPRESSION_LZ4 = ""lz4"";
-
-  /**
-   * compression: none
-   */
-  public static final String COMPRESSION_NONE = ""none"";
-
-  /**
-   * Compression algorithms. There is a static initializer, below the values defined in the
-   * enumeration, that calls the initializer of all defined codecs within {@link Algorithm}. This
-   * promotes a model of the following call graph of initialization by the static initializer,
-   * followed by calls to {@link #getCodec()},
-   * {@link #createCompressionStream(OutputStream, Compressor, int)}, and
-   * {@link #createDecompressionStream(InputStream, Decompressor, int)}. In some cases, the
-   * compression and decompression call methods will include a different buffer size for the stream.
-   * Note that if the compressed buffer size requested in these calls is zero, we will not set the
-   * buffer size for that algorithm. Instead, we will use the default within the codec.
-   * <p>
-   * The buffer size is configured in the Codec by way of a Hadoop {@link Configuration} reference.
-   * One approach may be to use the same Configuration object, but when calls are made to
-   * {@code createCompressionStream} and {@code createDecompressionStream} with non default buffer
-   * sizes, the configuration object must be changed. In this case, concurrent calls to
-   * {@code createCompressionStream} and {@code createDecompressionStream} would mutate the
-   * configuration object beneath each other, requiring synchronization to avoid undesirable
-   * activity via co-modification. To avoid synchronization entirely, we will create Codecs with
-   * their own Configuration object and cache them for re-use. A default codec will be statically
-   * created, as mentioned above to ensure we always have a codec available at loader
-   * initialization.
-   * <p>
-   * There is a Guava cache defined within Algorithm that allows us to cache Codecs for re-use.
-   * Since they will have their own configuration object and thus do not need to be mutable, there
-   * is no concern for using them concurrently; however, the Guava cache exists to ensure a maximal
-   * size of the cache and efficient and concurrent read/write access to the cache itself.
-   * <p>
-   * To provide Algorithm specific details and to describe what is in code:
-   * <p>
-   * LZO will always have the default LZO codec because the buffer size is never overridden within
-   * it.
-   * <p>
-   * LZ4 will always have the default LZ4 codec because the buffer size is never overridden within
-   * it.
-   * <p>
-   * GZ will use the default GZ codec for the compression stream, but can potentially use a
-   * different codec instance for the decompression stream if the requested buffer size does not
-   * match the default GZ buffer size of 32k.
-   * <p>
-   * Snappy will use the default Snappy codec with the default buffer size of 64k for the
-   * compression stream, but will use a cached codec if the buffer size differs from the default.
-   */
-  public enum Algorithm {
-
-    BZIP2(COMPRESSION_BZIP2) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.BZip2Codec"";
-
-      /**
-       * Configuration option for BZip2 buffer size. Uses the default FS buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.file.buffer.size"";
-
-      /**
-       * Default buffer size. Changed from default of 4096.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_BZIP2_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""BZip2 codec class not specified. Did you forget to set property ""
-              + CONF_BZIP2_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""BZip2 codec class not specified. Did you forget to set property ""
-              + CONF_BZIP2_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    LZO(COMPRESSION_LZO) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.LzoCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.lzo.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_LZO_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZO codec class not specified. Did you forget to set property ""
-              + CONF_LZO_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZO codec class not specified. Did you forget to set property ""
-              + CONF_LZO_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    LZ4(COMPRESSION_LZ4) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.Lz4Codec"";
-
-      /**
-       * Configuration option for LZ4 buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.lz4.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 256 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_LZ4_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZ4 codec class not specified. Did you forget to set property ""
-              + CONF_LZ4_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZ4 codec class not specified. Did you forget to set property ""
-              + CONF_LZ4_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    GZ(COMPRESSION_GZ) {
-
-      private transient DefaultCodec codec = null;
-
-      /**
-       * Configuration option for gz buffer size
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.file.buffer.size"";
-
-      /**
-       * Default buffer size
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 32 * 1024;
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = (DefaultCodec) createNewCodec(DEFAULT_BUFFER_SIZE);
-      }
-
-      /**
-       * Creates a new GZ codec
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        Configuration newConfig = new Configuration(conf);
-        updateBuffer(conf, BUFFER_SIZE_OPT, bufferSize);
-        DefaultCodec newCodec = new DefaultCodec();
-        newCodec.setConf(newConfig);
-        return newCodec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, GZ, codec);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return true;
-      }
-    },
-
-    NONE(COMPRESSION_NONE) {
-      @Override
-      CompressionCodec getCodec() {
-        return null;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) {
-        return bufferStream(downStream, downStreamBufferSize);
-      }
-
-      @Override
-      public void initializeDefaultCodec() {}
-
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return null;
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) {
-        return bufferStream(downStream, downStreamBufferSize);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return true;
-      }
-    },
-
-    SNAPPY(COMPRESSION_SNAPPY) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.SnappyCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.snappy.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      /**
-       * Creates a new snappy codec.
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return createNewCodec(CONF_SNAPPY_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""SNAPPY codec class not specified. Did you forget to set property ""
-              + CONF_SNAPPY_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""SNAPPY codec class not specified. Did you forget to set property ""
-              + CONF_SNAPPY_CLASS + ""?"");
-        }
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, SNAPPY, codec);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-    },
-
-    ZSTANDARD(COMPRESSION_ZSTD) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.ZStandardCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.zstd.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      /**
-       * Creates a new ZStandard codec.
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return createNewCodec(CONF_ZSTD_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              ""ZStandard codec class not specified. Did you forget to set property ""
-                  + CONF_ZSTD_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              ""ZStandard codec class not specified. Did you forget to set property ""
-                  + CONF_ZSTD_CLASS + ""?"");
-        }
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, ZSTANDARD, codec);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-    };
-
-    /**
-     * Guava cache to have a limited factory pattern defined in the Algorithm enum.
-     */
-    private static LoadingCache<Entry<Algorithm,Integer>,CompressionCodec> codecCache =
-        CacheBuilder.newBuilder().maximumSize(25).build(new CacheLoader<>() {
-          @Override
-          public CompressionCodec load(Entry<Algorithm,Integer> key) {
-            return key.getKey().createNewCodec(key.getValue());
-          }
-        });
+  private static final Map<String,DefaultCompressionAlgorithm> CONFIGURED_ALGORITHMS =
+      new HashMap<>();
+
+  private static final ServiceLoader<CompressionAlgorithmConfiguration> COMPRESSION_ALGORITHMS =
+      ServiceLoader.load(CompressionAlgorithmConfiguration.class);
+
+  // All compression-related settings are required to be configured statically in the
+  // Configuration object.
+  protected static final Configuration conf;","[{'comment': ""This isn't formatted, but the basic idea is there:\r\n\r\n```suggestion\r\n  protected static final Configuration conf = new Configuration();\r\n  private static final ServiceLoader<CompressionAlgorithmConfiguration> COMPRESSION_ALGORITHMS =\r\n      ServiceLoader.load(CompressionAlgorithmConfiguration.class);\r\n  private static final Map<String,DefaultCompressionAlgorithm> CONFIGURED_ALGORITHMS =\r\n      StreamSupport.stream(COMPRESSION_ALGORITHMS.spliterator(), false)\r\n          .map(a -> new DefaultCompressionAlgorithm(a, conf))\r\n          .collect(Collectors.toMap(algo -> algo.getName(), algo -> algo));\r\n```\r\n"", 'commenter': 'ctubbsii'}, {'comment': 'Implemented this change in 362e7bc', 'commenter': 'dlmarion'}]"
2518,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/DefaultCompressionAlgorithm.java,"@@ -0,0 +1,355 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.file.rfile.bcfile;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.FilterOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.util.Map.Entry;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.accumulo.core.spi.file.rfile.compression.CompressionAlgorithmConfiguration;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionInputStream;
+import org.apache.hadoop.io.compress.CompressionOutputStream;
+import org.apache.hadoop.io.compress.Compressor;
+import org.apache.hadoop.io.compress.Decompressor;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.cache.CacheBuilder;
+import com.google.common.cache.CacheLoader;
+import com.google.common.cache.LoadingCache;
+import com.google.common.collect.Maps;
+
+/**
+ * Compression algorithms. There is a static initializer, below the values defined in the
+ * enumeration, that calls the initializer of all defined codecs within
+ * {@link DefaultCompressionAlgorithm}. This promotes a model of the following call graph of
+ * initialization by the static initializer, followed by calls to {@link #getCodec()},
+ * {@link #createCompressionStream(OutputStream, Compressor, int)}, and
+ * {@link #createDecompressionStream(InputStream, Decompressor, int)}. In some cases, the
+ * compression and decompression call methods will include a different buffer size for the stream.
+ * Note that if the compressed buffer size requested in these calls is zero, we will not set the
+ * buffer size for that algorithm. Instead, we will use the default within the codec.
+ * <p>
+ * The buffer size is configured in the Codec by way of a Hadoop {@link Configuration} reference.
+ * One approach may be to use the same Configuration object, but when calls are made to
+ * {@code createCompressionStream} and {@code createDecompressionStream} with non default buffer
+ * sizes, the configuration object must be changed. In this case, concurrent calls to
+ * {@code createCompressionStream} and {@code createDecompressionStream} would mutate the
+ * configuration object beneath each other, requiring synchronization to avoid undesirable activity
+ * via co-modification. To avoid synchronization entirely, we will create Codecs with their own
+ * Configuration object and cache them for re-use. A default codec will be statically created, as
+ * mentioned above to ensure we always have a codec available at loader initialization.
+ * <p>
+ * There is a Guava cache defined within Algorithm that allows us to cache Codecs for re-use. Since
+ * they will have their own configuration object and thus do not need to be mutable, there is no
+ * concern for using them concurrently; however, the Guava cache exists to ensure a maximal size of
+ * the cache and efficient and concurrent read/write access to the cache itself.
+ * <p>
+ * To provide Algorithm specific details and to describe what is in code:
+ * <p>
+ * LZO will always have the default LZO codec because the buffer size is never overridden within it.
+ * <p>
+ * LZ4 will always have the default LZ4 codec because the buffer size is never overridden within it.
+ * <p>
+ * GZ will use the default GZ codec for the compression stream, but can potentially use a different
+ * codec instance for the decompression stream if the requested buffer size does not match the
+ * default GZ buffer size of 32k.
+ * <p>
+ * Snappy will use the default Snappy codec with the default buffer size of 64k for the compression
+ * stream, but will use a cached codec if the buffer size differs from the default.
+ */
+public class DefaultCompressionAlgorithm extends Configured {","[{'comment': 'I think the Javadoc needs to be updated. I am still confused by this class. It doesn\'t really make sense to call it a ""Default"" class since it really isn\'t an implementation of an interface or abstract class. It extends the Hadoop type, which I am not sure if need or not.  Then it just sets up all the different types. Maybe this should be called `CompressionHelper`? It might make sense to split the Hadoop configuration type from the compression classes so you would have a `CompressionHelper` type, which sets up all the compression stuff and then the hadoop configured type to hold what is pulled from the hadoop configuration. The enum was just called `Algorithm` before so maybe `CompressionAlgorithm` makes more sense.\r\n\r\nSorry, I know your changes are just modifications to what was already there but it was confusing before so I am just trying to reduce the confusion if we can.', 'commenter': 'milleruntime'}, {'comment': 'I talked with @dlmarion and he helped clear up some of the confusion. The code was confusing before so I think we can clean up the internal stuff as a follow on.', 'commenter': 'milleruntime'}]"
2518,core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/Compression.java,"@@ -18,843 +18,53 @@
  */
 package org.apache.accumulo.core.file.rfile.bcfile;
 
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.FilterOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
-import java.util.Map.Entry;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.Map;
+import java.util.ServiceLoader;
+import java.util.stream.Collectors;
+import java.util.stream.StreamSupport;
 
+import org.apache.accumulo.core.spi.file.rfile.compression.CompressionAlgorithmConfiguration;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.compress.CodecPool;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionInputStream;
-import org.apache.hadoop.io.compress.CompressionOutputStream;
-import org.apache.hadoop.io.compress.Compressor;
-import org.apache.hadoop.io.compress.Decompressor;
-import org.apache.hadoop.io.compress.DefaultCodec;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.CacheLoader;
-import com.google.common.cache.LoadingCache;
-import com.google.common.collect.Maps;
 
 /**
  * Compression related stuff.
  */
 public final class Compression {
 
-  private static final Logger log = LoggerFactory.getLogger(Compression.class);
-
   /**
    * Prevent the instantiation of this class.
    */
   private Compression() {
     throw new UnsupportedOperationException();
   }
 
-  static class FinishOnFlushCompressionStream extends FilterOutputStream {
-
-    FinishOnFlushCompressionStream(CompressionOutputStream cout) {
-      super(cout);
-    }
-
-    @Override
-    public void write(byte[] b, int off, int len) throws IOException {
-      out.write(b, off, len);
-    }
-
-    @Override
-    public void flush() throws IOException {
-      CompressionOutputStream cout = (CompressionOutputStream) out;
-      cout.finish();
-      cout.flush();
-      cout.resetState();
-    }
-  }
-
-  /**
-   * Compression: bzip2
-   */
-  public static final String COMPRESSION_BZIP2 = ""bzip2"";
-
-  /**
-   * Compression: zStandard
-   */
-  public static final String COMPRESSION_ZSTD = ""zstd"";
-
-  /**
-   * Compression: snappy
-   **/
-  public static final String COMPRESSION_SNAPPY = ""snappy"";
-
-  /**
-   * Compression: gzip
-   */
-  public static final String COMPRESSION_GZ = ""gz"";
-
-  /**
-   * Compression: lzo
-   */
-  public static final String COMPRESSION_LZO = ""lzo"";
-
-  /**
-   * Compression: lz4
-   */
-  public static final String COMPRESSION_LZ4 = ""lz4"";
-
-  /**
-   * compression: none
-   */
-  public static final String COMPRESSION_NONE = ""none"";
-
-  /**
-   * Compression algorithms. There is a static initializer, below the values defined in the
-   * enumeration, that calls the initializer of all defined codecs within {@link Algorithm}. This
-   * promotes a model of the following call graph of initialization by the static initializer,
-   * followed by calls to {@link #getCodec()},
-   * {@link #createCompressionStream(OutputStream, Compressor, int)}, and
-   * {@link #createDecompressionStream(InputStream, Decompressor, int)}. In some cases, the
-   * compression and decompression call methods will include a different buffer size for the stream.
-   * Note that if the compressed buffer size requested in these calls is zero, we will not set the
-   * buffer size for that algorithm. Instead, we will use the default within the codec.
-   * <p>
-   * The buffer size is configured in the Codec by way of a Hadoop {@link Configuration} reference.
-   * One approach may be to use the same Configuration object, but when calls are made to
-   * {@code createCompressionStream} and {@code createDecompressionStream} with non default buffer
-   * sizes, the configuration object must be changed. In this case, concurrent calls to
-   * {@code createCompressionStream} and {@code createDecompressionStream} would mutate the
-   * configuration object beneath each other, requiring synchronization to avoid undesirable
-   * activity via co-modification. To avoid synchronization entirely, we will create Codecs with
-   * their own Configuration object and cache them for re-use. A default codec will be statically
-   * created, as mentioned above to ensure we always have a codec available at loader
-   * initialization.
-   * <p>
-   * There is a Guava cache defined within Algorithm that allows us to cache Codecs for re-use.
-   * Since they will have their own configuration object and thus do not need to be mutable, there
-   * is no concern for using them concurrently; however, the Guava cache exists to ensure a maximal
-   * size of the cache and efficient and concurrent read/write access to the cache itself.
-   * <p>
-   * To provide Algorithm specific details and to describe what is in code:
-   * <p>
-   * LZO will always have the default LZO codec because the buffer size is never overridden within
-   * it.
-   * <p>
-   * LZ4 will always have the default LZ4 codec because the buffer size is never overridden within
-   * it.
-   * <p>
-   * GZ will use the default GZ codec for the compression stream, but can potentially use a
-   * different codec instance for the decompression stream if the requested buffer size does not
-   * match the default GZ buffer size of 32k.
-   * <p>
-   * Snappy will use the default Snappy codec with the default buffer size of 64k for the
-   * compression stream, but will use a cached codec if the buffer size differs from the default.
-   */
-  public enum Algorithm {
-
-    BZIP2(COMPRESSION_BZIP2) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.BZip2Codec"";
-
-      /**
-       * Configuration option for BZip2 buffer size. Uses the default FS buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.file.buffer.size"";
-
-      /**
-       * Default buffer size. Changed from default of 4096.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_BZIP2_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""BZip2 codec class not specified. Did you forget to set property ""
-              + CONF_BZIP2_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""BZip2 codec class not specified. Did you forget to set property ""
-              + CONF_BZIP2_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    LZO(COMPRESSION_LZO) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.LzoCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.lzo.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_LZO_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZO codec class not specified. Did you forget to set property ""
-              + CONF_LZO_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZO codec class not specified. Did you forget to set property ""
-              + CONF_LZO_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    LZ4(COMPRESSION_LZ4) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.Lz4Codec"";
-
-      /**
-       * Configuration option for LZ4 buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.lz4.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 256 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      @Override
-      CompressionCodec createNewCodec(int bufferSize) {
-        return createNewCodec(CONF_LZ4_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZ4 codec class not specified. Did you forget to set property ""
-              + CONF_LZ4_CLASS + ""?"");
-        }
-        InputStream bis = bufferStream(downStream, downStreamBufferSize);
-        CompressionInputStream cis = codec.createInputStream(bis, decompressor);
-        return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""LZ4 codec class not specified. Did you forget to set property ""
-              + CONF_LZ4_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-    },
-
-    GZ(COMPRESSION_GZ) {
-
-      private transient DefaultCodec codec = null;
-
-      /**
-       * Configuration option for gz buffer size
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.file.buffer.size"";
-
-      /**
-       * Default buffer size
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 32 * 1024;
-
-      @Override
-      CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = (DefaultCodec) createNewCodec(DEFAULT_BUFFER_SIZE);
-      }
-
-      /**
-       * Creates a new GZ codec
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        Configuration newConfig = new Configuration(conf);
-        updateBuffer(conf, BUFFER_SIZE_OPT, bufferSize);
-        DefaultCodec newCodec = new DefaultCodec();
-        newCodec.setConf(newConfig);
-        return newCodec;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, GZ, codec);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return true;
-      }
-    },
-
-    NONE(COMPRESSION_NONE) {
-      @Override
-      CompressionCodec getCodec() {
-        return null;
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) {
-        return bufferStream(downStream, downStreamBufferSize);
-      }
-
-      @Override
-      public void initializeDefaultCodec() {}
-
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return null;
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) {
-        return bufferStream(downStream, downStreamBufferSize);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return true;
-      }
-    },
-
-    SNAPPY(COMPRESSION_SNAPPY) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.SnappyCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.snappy.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      /**
-       * Creates a new snappy codec.
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return createNewCodec(CONF_SNAPPY_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""SNAPPY codec class not specified. Did you forget to set property ""
-              + CONF_SNAPPY_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(""SNAPPY codec class not specified. Did you forget to set property ""
-              + CONF_SNAPPY_CLASS + ""?"");
-        }
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, SNAPPY, codec);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-    },
-
-    ZSTANDARD(COMPRESSION_ZSTD) {
-
-      /**
-       * The default codec class.
-       */
-      private static final String DEFAULT_CLAZZ = ""org.apache.hadoop.io.compress.ZStandardCodec"";
-
-      /**
-       * Configuration option for LZO buffer size.
-       */
-      private static final String BUFFER_SIZE_OPT = ""io.compression.codec.zstd.buffersize"";
-
-      /**
-       * Default buffer size.
-       */
-      private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-
-      /**
-       * Whether or not the codec status has been checked. Ensures the default codec is not
-       * recreated.
-       */
-      private final AtomicBoolean checked = new AtomicBoolean(false);
-
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public CompressionCodec getCodec() {
-        return codec;
-      }
-
-      @Override
-      public void initializeDefaultCodec() {
-        codec = initCodec(checked, DEFAULT_BUFFER_SIZE, codec);
-      }
-
-      /**
-       * Creates a new ZStandard codec.
-       */
-      @Override
-      protected CompressionCodec createNewCodec(final int bufferSize) {
-        return createNewCodec(CONF_ZSTD_CLASS, DEFAULT_CLAZZ, bufferSize, BUFFER_SIZE_OPT);
-      }
-
-      @Override
-      public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              ""ZStandard codec class not specified. Did you forget to set property ""
-                  + CONF_ZSTD_CLASS + ""?"");
-        }
-        return createFinishedOnFlushCompressionStream(downStream, compressor, downStreamBufferSize);
-      }
-
-      @Override
-      public InputStream createDecompressionStream(InputStream downStream,
-          Decompressor decompressor, int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              ""ZStandard codec class not specified. Did you forget to set property ""
-                  + CONF_ZSTD_CLASS + ""?"");
-        }
-        return createDecompressionStream(downStream, decompressor, downStreamBufferSize,
-            DEFAULT_BUFFER_SIZE, ZSTANDARD, codec);
-      }
-
-      @Override
-      public boolean isSupported() {
-        return codec != null;
-      }
-    };
-
-    /**
-     * Guava cache to have a limited factory pattern defined in the Algorithm enum.
-     */
-    private static LoadingCache<Entry<Algorithm,Integer>,CompressionCodec> codecCache =
-        CacheBuilder.newBuilder().maximumSize(25).build(new CacheLoader<>() {
-          @Override
-          public CompressionCodec load(Entry<Algorithm,Integer> key) {
-            return key.getKey().createNewCodec(key.getValue());
-          }
-        });
+  // All compression-related settings are required to be configured statically in the
+  // Configuration object.
+  protected static final Configuration conf = new Configuration();
 
-    public static final String CONF_BZIP2_CLASS = ""io.compression.codec.bzip2.class"";
-    public static final String CONF_LZO_CLASS = ""io.compression.codec.lzo.class"";
-    public static final String CONF_LZ4_CLASS = ""io.compression.codec.lz4.class"";
-    public static final String CONF_SNAPPY_CLASS = ""io.compression.codec.snappy.class"";
-    public static final String CONF_ZSTD_CLASS = ""io.compression.codec.zstd.class"";
+  private static final ServiceLoader<CompressionAlgorithmConfiguration> COMPRESSION_ALGORITHMS =
+      ServiceLoader.load(CompressionAlgorithmConfiguration.class);
 
-    // All compression-related settings are required to be configured statically in the
-    // Configuration object.
-    protected static final Configuration conf;
-
-    // The model defined by the static block below creates a singleton for each defined codec in the
-    // Algorithm enumeration. By creating the codecs, each call to isSupported shall return
-    // true/false depending on if the codec singleton is defined. The static initializer, below,
-    // will ensure this occurs when the Enumeration is loaded. Furthermore, calls to getCodec will
-    // return the singleton, whether it is null or not.
-    //
-    // Calls to createCompressionStream and createDecompressionStream may return a different codec
-    // than getCodec, if the incoming downStreamBufferSize is different than the default. In such a
-    // case, we will place the resulting codec into the codecCache, defined below, to ensure we have
-    // cache codecs.
-    //
-    // Since codecs are immutable, there is no concern about concurrent access to the
-    // CompressionCodec objects within the guava cache.
-    static {
-      conf = new Configuration();
-      for (final Algorithm al : Algorithm.values()) {
-        al.initializeDefaultCodec();
-      }
-    }
-
-    // Data input buffer size to absorb small reads from application.
-    private static final int DATA_IBUF_SIZE = 1024;
-
-    // Data output buffer size to absorb small writes from application.
-    private static final int DATA_OBUF_SIZE = 4 * 1024;
-
-    // The name of the compression algorithm.
-    private final String name;
-
-    Algorithm(String name) {
-      this.name = name;
-    }
-
-    public abstract InputStream createDecompressionStream(InputStream downStream,
-        Decompressor decompressor, int downStreamBufferSize) throws IOException;
-
-    public abstract OutputStream createCompressionStream(OutputStream downStream,
-        Compressor compressor, int downStreamBufferSize) throws IOException;
-
-    public abstract boolean isSupported();
-
-    abstract CompressionCodec getCodec();
-
-    /**
-     * Create the default codec object.
-     */
-    abstract void initializeDefaultCodec();
-
-    /**
-     * Shared function to create new codec objects. It is expected that if buffersize is invalid, a
-     * codec will be created with the default buffer size.
-     */
-    abstract CompressionCodec createNewCodec(int bufferSize);
-
-    public Compressor getCompressor() {
-      CompressionCodec codec = getCodec();
-      if (codec != null) {
-        Compressor compressor = CodecPool.getCompressor(codec);
-        if (compressor != null) {
-          if (compressor.finished()) {
-            // Somebody returns the compressor to CodecPool but is still using it.
-            log.warn(""Compressor obtained from CodecPool already finished()"");
-          } else {
-            log.trace(""Got a compressor: {}"", compressor.hashCode());
-          }
-          // The following statement is necessary to get around bugs in 0.18 where a compressor is
-          // referenced after it's
-          // returned back to the codec pool.
-          compressor.reset();
-        }
-        return compressor;
-      }
-      return null;
-    }
-
-    public void returnCompressor(final Compressor compressor) {
-      if (compressor != null) {
-        log.trace(""Return a compressor: {}"", compressor.hashCode());
-        CodecPool.returnCompressor(compressor);
-      }
-    }
-
-    public Decompressor getDecompressor() {
-      CompressionCodec codec = getCodec();
-      if (codec != null) {
-        Decompressor decompressor = CodecPool.getDecompressor(codec);
-        if (decompressor != null) {
-          if (decompressor.finished()) {
-            // Somebody returns the decompressor to CodecPool but is still using it.
-            log.warn(""Decompressor obtained from CodecPool already finished()"");
-          } else {
-            log.trace(""Got a decompressor: {}"", decompressor.hashCode());
-          }
-          // The following statement is necessary to get around bugs in 0.18 where a decompressor is
-          // referenced after
-          // it's returned back to the codec pool.
-          decompressor.reset();
-        }
-        return decompressor;
-      }
-      return null;
-    }
-
-    /**
-     * Returns the specified {@link Decompressor} to the codec cache if it is not null.
-     */
-    public void returnDecompressor(final Decompressor decompressor) {
-      if (decompressor != null) {
-        log.trace(""Returned a decompressor: {}"", decompressor.hashCode());
-        CodecPool.returnDecompressor(decompressor);
-      }
-    }
-
-    /**
-     * Returns the name of the compression algorithm.
-     *
-     * @return the name
-     */
-    public String getName() {
-      return name;
-    }
-
-    /**
-     * Initializes and returns a new codec with the specified buffer size if and only if the
-     * specified {@link AtomicBoolean} has a value of false, or returns the specified original coded
-     * otherwise.
-     */
-    CompressionCodec initCodec(final AtomicBoolean checked, final int bufferSize,
-        final CompressionCodec originalCodec) {
-      if (!checked.get()) {
-        checked.set(true);
-        return createNewCodec(bufferSize);
-      }
-      return originalCodec;
-    }
-
-    /**
-     * Returns a new {@link CompressionCodec} of the specified type, or the default type if no
-     * primary type is specified. If the specified buffer size is greater than 0, the specified
-     * buffer size configuration option will be updated in the codec's configuration with the buffer
-     * size. If the neither the specified codec type or the default codec type can be found, null
-     * will be returned.
-     */
-    CompressionCodec createNewCodec(final String codecClazzProp, final String defaultClazz,
-        final int bufferSize, final String bufferSizeConfigOpt) {
-      String extClazz =
-          (conf.get(codecClazzProp) == null ? System.getProperty(codecClazzProp) : null);
-      String clazz = (extClazz != null) ? extClazz : defaultClazz;
-      try {
-        log.info(""Trying to load codec class {} for {}"", clazz, codecClazzProp);
-        Configuration config = new Configuration(conf);
-        updateBuffer(config, bufferSizeConfigOpt, bufferSize);
-        return (CompressionCodec) ReflectionUtils.newInstance(Class.forName(clazz), config);
-      } catch (ClassNotFoundException e) {
-        // This is okay.
-      }
-      return null;
-    }
-
-    InputStream createDecompressionStream(final InputStream stream, final Decompressor decompressor,
-        final int bufferSize, final int defaultBufferSize, final Algorithm algorithm,
-        CompressionCodec codec) throws IOException {
-      // If the default buffer size is not being used, pull from the loading cache.
-      if (bufferSize != defaultBufferSize) {
-        Entry<Algorithm,Integer> sizeOpt = Maps.immutableEntry(algorithm, bufferSize);
-        try {
-          codec = codecCache.get(sizeOpt);
-        } catch (ExecutionException e) {
-          throw new IOException(e);
-        }
-      }
-      CompressionInputStream cis = codec.createInputStream(stream, decompressor);
-      return new BufferedInputStream(cis, DATA_IBUF_SIZE);
-    }
-
-    /**
-     * Returns a new {@link FinishOnFlushCompressionStream} initialized for the specified output
-     * stream and compressor.
-     */
-    OutputStream createFinishedOnFlushCompressionStream(final OutputStream downStream,
-        final Compressor compressor, final int downStreamBufferSize) throws IOException {
-      OutputStream out = bufferStream(downStream, downStreamBufferSize);
-      CompressionOutputStream cos = getCodec().createOutputStream(out, compressor);
-      return new BufferedOutputStream(new FinishOnFlushCompressionStream(cos), DATA_OBUF_SIZE);
-    }
-
-    /**
-     * Return the given stream wrapped as a {@link BufferedOutputStream} with the given buffer size
-     * if the buffer size is greater than 0, or return the original stream otherwise.
-     */
-    OutputStream bufferStream(final OutputStream stream, final int bufferSize) {
-      if (bufferSize > 0) {
-        return new BufferedOutputStream(stream, bufferSize);
-      }
-      return stream;
-    }
-
-    /**
-     * Return the given stream wrapped as a {@link BufferedInputStream} with the given buffer size
-     * if the buffer size is greater than 0, or return the original stream otherwise.
-     */
-    InputStream bufferStream(final InputStream stream, final int bufferSize) {
-      if (bufferSize > 0) {
-        return new BufferedInputStream(stream, bufferSize);
-      }
-      return stream;
-    }
-
-    /**
-     * Updates the value of the specified buffer size opt in the given {@link Configuration} if the
-     * new buffer size is greater than 0.
-     */
-    void updateBuffer(final Configuration config, final String bufferSizeOpt,
-        final int bufferSize) {
-      // Use the buffersize only if it is greater than 0, otherwise use the default defined within
-      // the codec.
-      if (bufferSize > 0) {
-        config.setInt(bufferSizeOpt, bufferSize);
-      }
-    }
-  }
+  private static final Map<String,CompressionAlgorithm> CONFIGURED_ALGORITHMS =
+      StreamSupport.stream(COMPRESSION_ALGORITHMS.spliterator(), false)
+          .map(a -> new CompressionAlgorithm(a, conf))
+          .collect(Collectors.toMap(algo -> algo.getName(), algo -> algo));
 
   public static String[] getSupportedAlgorithms() {
-    Algorithm[] algorithms = Algorithm.class.getEnumConstants();
     ArrayList<String> supportedAlgorithms = new ArrayList<>();
-    for (Algorithm algorithm : algorithms) {
-      if (algorithm.isSupported()) {
-        supportedAlgorithms.add(algorithm.getName());
+    CONFIGURED_ALGORITHMS.forEach((k, v) -> {
+      if (v.isSupported()) {
+        supportedAlgorithms.add(k);
       }
-    }
+    });","[{'comment': 'This could be shorter without sacrificing comprehensibility:\r\n\r\n```java\r\nCONFIGURED_ALGORITHMS.entrySet().stream().filter(e -> e.getValue().isSupported()).map(Entry::getKey).forEach(supportedAlgorithms::add);\r\n```', 'commenter': 'ctubbsii'}]"
2531,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1401,6 +1401,22 @@ private void closeConsistencyCheck() {
         throw new RuntimeException(msg);
       }
 
+      if (tabletMeta.getFlushId().isPresent()
+          && tabletMeta.getFlushId().orElse(-1) != lastFlushID) {
+        String msg = ""Closed tablet "" + extent + "" lastFlushID is inconsistent with metadata : ""
+            + tabletMeta.getFlushId().orElse(-1) + "" != "" + lastFlushID;
+        log.error(msg);
+        throw new RuntimeException(msg);
+      }
+
+      if (tabletMeta.getCompactId().isPresent()
+          && tabletMeta.getCompactId().orElse(-1) != lastCompactID) {
+        String msg = ""Closed tablet "" + extent + "" lastCompactID is inconsistent with metadata : ""
+            + tabletMeta.getCompactId().orElse(-1) + "" != "" + lastCompactID;
+        log.error(msg);
+        throw new RuntimeException(msg);
+      }","[{'comment': 'Getting the number from the Optional multiple times isn\'t necessary if you use `ifPresent`\r\n\r\n```suggestion\r\n      tabletMeta.getFlushId().ifPresent(flushId -> {\r\n        if (flushId != lastFlushID) {\r\n          String msg = ""Closed tablet "" + extent + "" lastFlushID is inconsistent with metadata : ""\r\n              + flushId + "" != "" + lastFlushID;\r\n          log.error(msg);\r\n          throw new RuntimeException(msg);\r\n        }\r\n      });\r\n\r\n      tabletMeta.getCompactId().ifPresent(compactId -> {\r\n        if (compactId != lastCompactID) {\r\n          String msg = ""Closed tablet "" + extent + "" lastCompactID is inconsistent with metadata : ""\r\n              + compactId + "" != "" + lastCompactID;\r\n          log.error(msg);\r\n          throw new RuntimeException(msg);\r\n        }\r\n      });\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Your changes look good. ', 'commenter': 'Manno15'}]"
2535,core/src/main/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoder.java,"@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.FixedByteArrayOutputStream;
+
+public class BigDecimalLexicoder extends AbstractLexicoder<BigDecimal> {
+
+  BigIntegerLexicoder bigIntegerLexicoder = new BigIntegerLexicoder();","[{'comment': 'Can this be private final?', 'commenter': 'ctubbsii'}]"
2535,core/src/main/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoder.java,"@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.FixedByteArrayOutputStream;
+
+public class BigDecimalLexicoder extends AbstractLexicoder<BigDecimal> {
+
+  BigIntegerLexicoder bigIntegerLexicoder = new BigIntegerLexicoder();
+
+  @Override
+  public BigDecimal decode(byte[] b) {
+    // This concrete implementation is provided for binary compatibility, since the corresponding
+    // superclass method has type-erased return type Object. See ACCUMULO-3789 and #1285.
+    return super.decode(b);
+  }
+
+  @Override
+  public byte[] encode(BigDecimal bd) {
+
+    try {
+      int scale = bd.scale();
+      BigInteger bigInt = bd.unscaledValue();
+      byte[] encodedbigInt = bigIntegerLexicoder.encode(bigInt);
+      byte[] ret = new byte[4 + encodedbigInt.length];
+      int len = ret.length;
+      DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
+      len = len ^ 0x80000000;
+      scale = scale ^ 0x80000000;
+      dos.write(encodedbigInt);
+      dos.writeInt(scale);
+      dos.close();","[{'comment': 'This can be a try-with-resources block.', 'commenter': 'ctubbsii'}, {'comment': 'sure :-). We should do it for other types also. ', 'commenter': 'harjitdotsingh'}]"
2535,core/src/main/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoder.java,"@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.FixedByteArrayOutputStream;
+
+public class BigDecimalLexicoder extends AbstractLexicoder<BigDecimal> {
+
+  BigIntegerLexicoder bigIntegerLexicoder = new BigIntegerLexicoder();
+
+  @Override
+  public BigDecimal decode(byte[] b) {
+    // This concrete implementation is provided for binary compatibility, since the corresponding
+    // superclass method has type-erased return type Object. See ACCUMULO-3789 and #1285.
+    return super.decode(b);
+  }
+
+  @Override
+  public byte[] encode(BigDecimal bd) {
+
+    try {
+      int scale = bd.scale();
+      BigInteger bigInt = bd.unscaledValue();
+      byte[] encodedbigInt = bigIntegerLexicoder.encode(bigInt);
+      byte[] ret = new byte[4 + encodedbigInt.length];
+      int len = ret.length;
+      DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
+      len = len ^ 0x80000000;
+      scale = scale ^ 0x80000000;
+      dos.write(encodedbigInt);
+      dos.writeInt(scale);
+      dos.close();
+      return ret;
+    } catch (IOException e) {
+      throw new RuntimeException(e);","[{'comment': 'There is an UncheckedIOException that would be suitable instead of a generic RTE.', 'commenter': 'ctubbsii'}, {'comment': 'I was just following what was there in the other Codes and hence.Let me fix it', 'commenter': 'harjitdotsingh'}]"
2535,core/src/test/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoderTest.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.math.BigDecimal;
+import java.util.Arrays;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.AbstractLexicoderTest;
+import org.junit.jupiter.api.Test;
+
+public class BigDecimalLexicoderTest extends AbstractLexicoderTest {
+
+  @Test
+  public void testSortOrder() {
+
+    assertSortOrder(new BigDecimalLexicoder(), BigDecimal::compareTo,
+        Arrays.asList(new BigDecimal(""2.0""), new BigDecimal(""2.00""), new BigDecimal(""2.000""),
+            new BigDecimal(""-3.000""), new BigDecimal(""-2.00""), new BigDecimal(""0.0000""),
+            new BigDecimal(""0.1""), new BigDecimal(""0.10""), new BigDecimal(""-65537.000""),
+            new BigDecimal(""-65537.00""), new BigDecimal(""-65537.0"")));
+","[{'comment': 'This might be more readable (formatting might be off):\r\n\r\n```suggestion\r\n        List.of(""2.0"", ""2.00"", ""2.000"", ""-3.000"", ""-2.00"", ""0.0000"", ""0.1"", ""0.10"", ""-65537.000"", ""-65537.00"", ""-65537.0"")\r\n            .stream().map(BigDecimal::new).collect(Collectors.toList()));\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""But also, shouldn't we expect these to be ordered numerically? These are all mixed up. I don't think that's what users are going to want."", 'commenter': 'ctubbsii'}, {'comment': 'I can do that. We are sorting this list and then comparing it with the encoded sorting to get an expected and actual. So how would an ordered list here help ? I think we should create an issue so that I can clean up the other coders also with the same code. ', 'commenter': 'harjitdotsingh'}, {'comment': ""I was under the impression that the values here represented the expected sort order. If the test itself is sorting first, I'm not sure it's giving us the test coverage we want. I'd have to spend more time looking."", 'commenter': 'ctubbsii'}]"
2535,core/src/test/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoderTest.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.math.BigDecimal;
+import java.util.Arrays;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.AbstractLexicoderTest;
+import org.junit.jupiter.api.Test;
+
+public class BigDecimalLexicoderTest extends AbstractLexicoderTest {
+
+  @Test
+  public void testSortOrder() {
+
+    assertSortOrder(new BigDecimalLexicoder(), BigDecimal::compareTo,","[{'comment': ""I'm not sure BigDecimal's compareTo method is a sufficient comparator here. It will treat things as equal if they are equal in magnitude, but with different scale. However, when we serialize, we want to preserve the scale. We should want to keep the order predictable, so the scale must be considered as part of the comparison. This test may pass, but so would other orderings. But, we need to ensure that the test fails if other orderings pass."", 'commenter': 'ctubbsii'}, {'comment': ""If we don't pass the comparator wouldn't it still use the default comparator which would still not give us want we want?  Shouldn't we write our Comparator? Because our encoder and decoder know how data is stored and would compare it correctly.  "", 'commenter': 'harjitdotsingh'}, {'comment': ""The concern I have isn't whether or not we're passing the comparator. The concern I have is that BigDecimal's own comparator does not guarantee a particular ordering of distinct input. It's non-deterministic, because it treats some values as equal when they differ in scale."", 'commenter': 'ctubbsii'}]"
2535,core/src/test/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoderTest.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.math.BigDecimal;
+import java.util.Arrays;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.AbstractLexicoderTest;
+import org.junit.jupiter.api.Test;
+
+public class BigDecimalLexicoderTest extends AbstractLexicoderTest {
+
+  @Test
+  public void testSortOrder() {
+
+    assertSortOrder(new BigDecimalLexicoder(), BigDecimal::compareTo,
+        Arrays.asList(new BigDecimal(""2.0""), new BigDecimal(""2.00""), new BigDecimal(""2.000""),
+            new BigDecimal(""-3.000""), new BigDecimal(""-2.00""), new BigDecimal(""0.0000""),
+            new BigDecimal(""0.1""), new BigDecimal(""0.10""), new BigDecimal(""-65537.000""),
+            new BigDecimal(""-65537.00""), new BigDecimal(""-65537.0"")));
+
+  }
+
+  @Test
+  public void testDecode() {
+    assertDecodes(new BigDecimalLexicoder(), BigDecimal.valueOf(-3.000));","[{'comment': ""BigDecimalLexicoder can be constructed once and reused. It should be stateless, so we don't need more than one for the test."", 'commenter': 'ctubbsii'}]"
2535,core/src/main/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoder.java,"@@ -0,0 +1,83 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.UncheckedIOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.FixedByteArrayOutputStream;
+
+public class BigDecimalLexicoder extends AbstractLexicoder<BigDecimal> {","[{'comment': '```suggestion\r\n/**\r\n * A lexicoder to encode/decode a BigDecimal to/from bytes that maintain its native Java sort order.\r\n *\r\n * @since 2.1.0\r\n */\r\npublic class BigDecimalLexicoder extends AbstractLexicoder<BigDecimal> {\r\n```', 'commenter': 'DomGarguilo'}]"
2535,core/src/test/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoderTest.java,"@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.math.BigDecimal;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.AbstractLexicoderTest;
+import org.junit.jupiter.api.Test;
+
+public class BigDecimalLexicoderTest extends AbstractLexicoderTest {
+
+  @Test
+  public void testSortOrder() {
+
+    List<BigDecimal> list =
+        List.of(""2.0"", ""2.00"", ""2.000"", ""-3.000"", ""-2.00"", ""0.0000"", ""0.1"", ""0.10"", ""-65537.000"",
+            ""-65537.00"", ""-65537.0"").stream().map(BigDecimal::new).collect(Collectors.toList());","[{'comment': 'Looks like this could be simplified even more by using `Stream.of()`. The suggested code should be formatted correctly but changed the imports I think so might fail the checks until they are updated.\r\n```suggestion\r\n    var list = Stream.of(""2.0"", ""2.00"", ""2.000"", ""-3.000"", ""-2.00"", ""0.0000"", ""0.1"", ""0.10"",\r\n        ""-65537.000"", ""-65537.00"", ""-65537.0"").map(BigDecimal::new).collect(Collectors.toList());\r\n```\r\nI changed to `var` here because keeping it as list messed with the formatting making for some less readable code:\r\n```Java\r\n    List<\r\n        BigDecimal> list =\r\n            Stream\r\n                .of(""2.0"", ""2.00"", ""2.000"", ""-3.000"", ""-2.00"", ""0.0000"", ""0.1"", ""0.10"",\r\n                    ""-65537.000"", ""-65537.00"", ""-65537.0"")\r\n                .map(BigDecimal::new).collect(Collectors.toList());\r\n```', 'commenter': 'DomGarguilo'}, {'comment': 'Let me use stream and see what that does. Thanks', 'commenter': 'harjitdotsingh'}]"
2535,core/src/main/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoder.java,"@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.UncheckedIOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.FixedByteArrayOutputStream;
+
+/**
+ * A lexicoder to encode/decode a BigDecimal to/from bytes that maintain its native Java sort order.
+ *","[{'comment': 'I think the biggest problem with the native Java sort order, is that it\'s non-deterministic (for items that differ only in scale). Lexicoder orderings should be deterministic. So, we\'d need to use a comparator that produces the same output, no matter the order of the input, and have the lexicoder be implemented to preserve that ordering. Otherwise, the user experience could be very confusing. For example, the string lexical ordering of ""2.0"" and ""2.00"" would place ""2.0"" before ""2.00"". However, the comparator for BigDecimal is perfectly happy to order ""2.00"" before ""2.0"", because their `compareTo` value is `0`, even though their `equals` value is `false`.', 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii  Shall we have the LexiCoder implement the Comparator interface or write a different Comparator for BigDecimals? Thanks', 'commenter': 'harjitdotsingh'}, {'comment': 'I\'m not sure either are strictly necessary. What matters is that the lexical ordering of the encoded form by this Lexicoder is deterministic, in numerical order first, then in lexical order by precision. So, `new BigDecimal(new BigInteger(""203""), 2)` should sort lexically prior to `new BigDecimal(new BigInteger(""2030""), 3)`, for example.\r\n\r\nThe main reason a Comparator is needed is to test that the ordering is preserved in both directions. So at the very least, a better comparator will be needed for testing. It may be sufficient to use `Comparator.naturalOrder().thenComparingInt(BigDecimal::precision)`, but I don\'t know if there are any edge cases where this breaks.\r\n\r\nAs for the javadoc, rather than stating that we\'re maintaining its native Java sort order, we should specify that we\'re maintaining its numerical order and precision, so that numerically equivalent values, the ones with the least precision are ordered first. When this lexicoder is reversed, the opposite should be true.', 'commenter': 'ctubbsii'}]"
2535,core/src/main/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoder.java,"@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.UncheckedIOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.FixedByteArrayOutputStream;
+
+/**
+ * A lexicoder to encode/decode a BigDecimal to/from bytes that maintain its native Java sort order.
+ *
+ * @since 2.1.0
+ */
+public class BigDecimalLexicoder extends AbstractLexicoder<BigDecimal> {
+
+  private final BigIntegerLexicoder bigIntegerLexicoder = new BigIntegerLexicoder();
+
+  @Override
+  public BigDecimal decode(byte[] b) {
+    // This concrete implementation is provided for binary compatibility, since the corresponding
+    // superclass method has type-erased return type Object. See ACCUMULO-3789 and #1285.
+    return super.decode(b);
+  }
+
+  @Override
+  public byte[] encode(BigDecimal bd) {
+    // To encode we separate out the scale and the unscaled value
+    // serialize each value individually and store them
+    int scale = bd.scale();
+    BigInteger bigInt = bd.unscaledValue();
+    byte[] encodedbigInt = bigIntegerLexicoder.encode(bigInt);
+    // Length is set to size of encoded BigInteger + length of the scale value
+    byte[] ret = new byte[4 + encodedbigInt.length];
+    try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))) {
+      scale = scale ^ 0x80000000;
+      dos.write(encodedbigInt);
+      dos.writeInt(scale);","[{'comment': ""This seems very dependent on DataOutputStream not introducing any additional bytes into the output stream. Since that's an implementation detail, I don't think we should rely on that fact. Why not just use `System.arraycopy` to guarantee you don't overflow the `ret` buffer?"", 'commenter': 'ctubbsii'}, {'comment': ""welcome your comments  😄 .  I would have to change the implementation a little because now I will have to convert  the int to a ByteArray and preserve and or switch the ordering. All our other LexiCoders are using the same approach shouldn't we also change that. Also if it were writing extra bytes our decode tests would have failed or we would have caught that. Let me think what and how we can handle it."", 'commenter': 'harjitdotsingh'}, {'comment': ""I just re-read the javadoc for the DataOutputStream, and while it only guarantees that it can be read back via a DataInputStream, leaving some ambiguity about whether it can write any metadata to the stream, the current implementation doesn't do that and it would break things if it started doing to. So, it's fine."", 'commenter': 'ctubbsii'}, {'comment': 'Wondering if the scale should be added first so that its sorted on first. Thinking of numbers like `4.2E-100`,  `4.2E-10`, `4.2E+10`, `4.2E+100`  \r\n\r\n```suggestion\r\n      dos.writeInt(scale);\r\n      dos.write(encodedbigInt);\r\n```', 'commenter': 'keith-turner'}, {'comment': 'That won\'t work. Consider:\r\n\r\n```java\r\nvar x = new BigDecimal(""2030E-3""); // value == 2.030; x.scale() == 3\r\nvar y = new BigDecimal(""303E-2"");  // value == 3.03;  y.scale() == 2\r\n```\r\n\r\nYou can\'t sort by scale first, because scale is not related to the numeric ordering, only the amount of information about the precision that is stored. I\'m actually not even sure whether we should be using precision or scale.', 'commenter': 'ctubbsii'}, {'comment': ""Also encoding by adding scale first won't have any bearing. When we sort the encoded value it's just a bunch of bytes. We will have to write our comparator which does what we want.  I'm evaluating few other options, will keep posted. "", 'commenter': 'harjitdotsingh'}, {'comment': 'I was thinking the scale was equivalent to an exponent.  The DobuleLexicoder encodes the exponent and then mantissa for proper sorting.  If the scale is not the exponent, then to get proper sorting would need something like an exponent encoded first using a fixed width followed by the mantissa.', 'commenter': 'keith-turner'}, {'comment': 'Here\'s a good example to show that scale is not equivalent to exponent (paste into `jshell PRINTING` to run):\r\n```java\r\nvar y = new BigDecimal(""210"");\r\nIntStream.rangeClosed(-1, 2).forEach(i -> {\r\n  var x = y.setScale(i);\r\n  printf(""unscaledValue() = %s; scale = %s; precision = %s; toString = %s%n"", x.unscaledValue(), x.scale(), x.precision(), x);\r\n});\r\n```\r\n\r\nSince BigDecimal has arbitrary precision, scale is used to track how the unscaled value must be scaled (how many places to move the decimal) to preserve the intended precision.\r\n\r\nHere\'s the output:\r\n\r\n```\r\nunscaledValue() = 21; scale = -1; precision = 2; toString = 2.1E+2\r\nunscaledValue() = 210; scale = 0; precision = 3; toString = 210\r\nunscaledValue() = 2100; scale = 1; precision = 4; toString = 210.0\r\nunscaledValue() = 21000; scale = 2; precision = 5; toString = 210.00\r\n```\r\n\r\nThese all `compareTo` with `0`, but none of them `equals` any of the others.', 'commenter': 'ctubbsii'}, {'comment': 'I tried to rewrite this locally to use an exponent and it mostly works.  Below is what I was experimenting with.\r\n\r\n```java\r\npublic class BigDecimalLexicoder extends AbstractLexicoder<BigDecimal> {\r\n\r\n  private final BigIntegerLexicoder bigIntegerLexicoder = new BigIntegerLexicoder();\r\n\r\n  @Override\r\n  public BigDecimal decode(byte[] b) {\r\n    // This concrete implementation is provided for binary compatibility, since the corresponding\r\n    // superclass method has type-erased return type Object. See ACCUMULO-3789 and #1285.\r\n    return super.decode(b);\r\n  }\r\n\r\n  private static int getExponent(BigDecimal bd){\r\n    return -1*bd.scale()+bd.precision()-1;\r\n  }\r\n\r\n  /**\r\n   * inverse of {@link #getExponent(BigDecimal)}\r\n   */\r\n  private static int getScale(int exponent, int precision) {\r\n    return (exponent+1-precision) * -1;\r\n  }\r\n\r\n  private byte[] encodeBigInt(BigInteger bigInt) {\r\n    byte[] bytes = bigInt.toByteArray();\r\n    // flip the sign bit\r\n    bytes[0] = (byte) (0x80 ^ bytes[0]);\r\n    return bytes;\r\n  }\r\n\r\n  private BigInteger decodeBigInt(byte[] data, int off, int len) {\r\n    // unflip the sign bit\r\n    data[off] = (byte) (0x80 ^ data[off]);\r\n    return new BigInteger(data, off, len);\r\n  }\r\n\r\n  @Override\r\n  public byte[] encode(BigDecimal bd) {\r\n    BigInteger bigInt = bd.unscaledValue();\r\n    byte[] encodedbigInt = encodeBigInt(bigInt);\r\n    // Length is set to size of encoded BigInteger + length of the scale value\r\n    byte[] ret = new byte[5 + encodedbigInt.length];\r\n    try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))) {\r\n      // sort first on the sign, make negative numbers come first\r\n      dos.write((byte)bd.signum()  ^ 0x80);\r\n\r\n      // sort second on the exponent\r\n      int exponent = getExponent(bd);\r\n      if(bd.signum() < 0) {\r\n        // when the number if negative the exponents need to sort in reverse order\r\n        exponent = exponent * -1;\r\n      }\r\n      dos.writeInt(exponent ^ 0x80000000);\r\n\r\n      // sort third on the unscaled value.\r\n      dos.write(encodedbigInt);\r\n      return ret;\r\n\r\n    } catch (IOException e) {\r\n      throw new UncheckedIOException(e);\r\n    }\r\n  }\r\n\r\n  @Override\r\n  protected BigDecimal decodeUnchecked(byte[] b, int offset, int origLen)\r\n      throws IllegalArgumentException {\r\n\r\n    try {\r\n      DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b, offset, origLen));\r\n      int signum = (byte)(dis.read() ^ 0x80);\r\n\r\n      int exponent = dis.readInt() ^ 0x80000000;\r\n      if(signum < 0)\r\n        exponent = exponent * -1;\r\n\r\n      BigInteger bigInt = decodeBigInt(b,5,b.length - 5);\r\n\r\n      // TODO is there a better way to get this\r\n      int precision = bigInt.abs().toString().length();\r\n\r\n      return new BigDecimal(bigInt, getScale(exponent, precision));\r\n    } catch (IOException ioe) {\r\n      throw new UncheckedIOException(ioe);\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI modified the test function to use the following test data.\r\n\r\n\r\n```java\r\n  @Test\r\n  public void testSortOrder() {\r\n\r\n    var list = Stream.of(""4.2E+10"",""4.2E+100"",""4.1E+100"",""4.3E+100"",""4.2E-10"",""4.2E-100"",\r\n        ""-4.2E+10"",""-4.2E+100"",""-4.2E-10"",""-4.2E-100"",""-4.1E-100"",""-4.3E-100"",""4.2"",""-4.2"",""2.1"",""2.2"",\r\n        ""2.0"", ""2.00"", ""2.000"", ""-3.000"", ""-2.00"", ""0.0000"", ""0.1"", ""0.10"", ""-65537.000"", ""-65537.123"",\r\n        ""-65537.00"", ""-65537.0"", ""4.1E-10"",""4.3E-10"").map(BigDecimal::new).collect(Collectors.toList());\r\n\r\n    assertSortOrder(new BigDecimalLexicoder(), list);\r\n\r\n  }\r\n```\r\n\r\nAnd it fails with the following difference in sort order.\r\n\r\n```\r\nExpected :[-4.2E+100, -4.2E+10, -65537.123, -65537.000, -65537.00, -65537.0, -4.2, -3.000, -2.00, -4.2E-10, -4.3E-100, -4.2E-100, -4.1E-100, 0.0000, 4.2E-100, 4.1E-10, 4.2E-10, 4.3E-10, 0.1, 0.10, 2.0, 2.00, 2.000, 2.1, 2.2, 4.2, 4.2E+10, 4.1E+100, 4.2E+100, 4.3E+100]\r\n\r\nActual   :[-4.2E+100, -4.2E+10, -65537.00, -65537.0, -65537.123, -65537.000, -4.2, -3.000, -2.00, -4.2E-10, -4.3E-100, -4.2E-100, -4.1E-100, 0.0000, 4.2E-100, 4.1E-10, 4.2E-10, 4.3E-10, 0.1, 0.10, 2.00, 2.000, 2.0, 2.1, 2.2, 4.2, 4.2E+10, 4.1E+100, 4.2E+100, 4.3E+100]\r\n```\r\n\r\nSo everything in the test data is sorting fine.  The only problem is `-65537.123` is sorting after `-65537.0`.  I understand why its doing this, not sure what to do about it for now though.\r\n\r\n\r\n', 'commenter': 'keith-turner'}, {'comment': 'I came across this paper which I was doing some reading. I think we should to change the encoding to use Bits and hopefully, that would give us what we want.  Here is the link \r\n[Decimal Lexicoder](https://arxiv.org/abs/1506.01598). It has been done in C++ thinking of translating that in Java and see if it gets us what we need. \r\n', 'commenter': 'harjitdotsingh'}]"
2535,core/src/test/java/org/apache/accumulo/core/client/lexicoder/BigDecimalLexicoderTest.java,"@@ -0,0 +1,59 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.lexicoder;
+
+import java.math.BigDecimal;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+import org.apache.accumulo.core.clientImpl.lexicoder.AbstractLexicoderTest;
+import org.junit.jupiter.api.Test;
+
+public class BigDecimalLexicoderTest extends AbstractLexicoderTest {
+
+  @Test
+  public void testSortOrder() {
+
+    var list = Stream.of(""2.0"", ""2.00"", ""2.000"", ""-3.000"", ""-2.00"", ""0.0000"", ""0.1"", ""0.10"",","[{'comment': 'Try adding a number with the same unscaled value that has different exponents.\r\n\r\n```suggestion\r\n    var list = Stream.of(""4.2E+10"",""4.2E+100"",""4.2E-10"",""4.2E-100"",""4.2"",""2.0"", ""2.00"", ""2.000"", ""-3.000"", ""-2.00"", ""0.0000"", ""0.1"", ""0.10"",\r\n```', 'commenter': 'keith-turner'}]"
2545,TESTING.md,"@@ -98,7 +98,7 @@ the tests.
 These tests can be run by providing a system property.  Specific ITs can be run using ""-Dit.test"" or run all tests using:
 
 ```bash
-mvn clean verify -Dtest=foo -Daccumulo.it.properties=/home/user/my_cluster.properties -Dfailsafe.groups=org.apache.accumulo.test.categories.StandaloneCapableClusterTests -Dspotbugs.skip
+mvn clean verify -Dtest=foo -Daccumulo.it.properties=/home/user/my_cluster.properties -Dfailsafe.groups=StandaloneCapableClusterTests -Dspotbugs.skip","[{'comment': 'These tag names can be simplified further. The original names like `SunnyDayTests` was chosen because we had to create a class name that made sense on its own. Now that\'s not necessary, since the class is deleted, and the fact that we\'re in a ""Test"" context is already obvious when using the Tag annotation. So, something like `@Tag(""SunnyDay"")` should suffice instead of `@Tag(""SunnyDayTests"")`.', 'commenter': 'ctubbsii'}, {'comment': ""Good idea. I'm wondering if in that case we could remove the [`-Psunny` profile](https://github.com/apache/accumulo/blob/97af14509d2755de29e2c0196201018f576cab68/pom.xml#L1548-L1555) since to run the same tests now, it would just be `-Dfailsafe.groups=SunnyDay`. The prior is still shorter but just thought I would point it out and see what you/others thought. I assume the profile was created since this group of tests is run often, and before, one would have to type out the full class name (org.apache.accumulo.test.categories.SunnyDayTests)."", 'commenter': 'DomGarguilo'}, {'comment': 'I agree it is superfluous, but it\'s historical, so it\'s probably worth keeping. When we initially added these category annotations, I resisted creating a bunch of new profiles with different combinations of activating them, because you can just use the `-Dfailsafe.groups=` property. I still think it\'s unnecessary to create new profiles, but keeping this one around for historical reasons is probably desirable. If you wanted to simplify it further, you could call it `@Tag(""Sunny"")` in order to get even closer to the profile name.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in [d1fa1f3](https://github.com/apache/accumulo/pull/2545/commits/d1fa1f31dbbf5e8728ceaa9359f2e92b67cd4d75)', 'commenter': 'DomGarguilo'}]"
2545,hadoop-mapreduce/src/test/java/org/apache/accumulo/WithTestNames.java,"@@ -16,10 +16,27 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.accumulo.test.categories;
+package org.apache.accumulo;","[{'comment': ""It looks like this is being used for integration tests. If that's the case, then this needs to be in a unique test directory for this specific test module so no other package collides in any jar, so that it doesn't cause problems with jar sealing.\r\n\r\nThis could be in the the `org.apache.accumulo.hadoop.testing` package for example, so it could not possibly collide with anything in any other jar."", 'commenter': 'ctubbsii'}, {'comment': 'Same comment as before, about this being in a unique package for this module.', 'commenter': 'ctubbsii'}]"
2545,hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/its/mapred/AccumuloOutputFormatIT.java,"@@ -73,7 +73,7 @@ protected void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSit
   public void testMapred() throws Exception {
     try (AccumuloClient client = Accumulo.newClient().from(getClientProperties()).build()) {
       // create a table and put some data in it
-      client.tableOperations().create(testName.getMethodName());
+      client.tableOperations().create(testName());","[{'comment': ""This table name should really be put in its own variable, so `testName()` is only called once, and it's obvious what it's being used for later."", 'commenter': 'ctubbsii'}]"
2545,test/src/main/java/org/apache/accumulo/harness/AccumuloClusterHarness.java,"@@ -60,7 +59,7 @@
  * advanced ITs that do crazy things. For more typical, expected behavior of a cluster see
  * {@link SharedMiniClusterBase}. This instance can be MAC or a standalone instance.
  */
-@Category(StandaloneCapableClusterTests.class)
+@Tag(""StandaloneCapableCluster"")","[{'comment': ""It'd be very easy to have typos in these strings. These string literals could be constants, as in:\r\n\r\n```suggestion\r\n@Tag(ConstantsForTesting.STANDALONE_CAPABLE_CLUSTER)\r\n```\r\n\r\nThat way, there's no chance of mistyping the category, and it not running when you select that category."", 'commenter': 'ctubbsii'}, {'comment': 'I added constants, where possible, to AccumuloITBase.java but for the single `@Tag` that is used outside of accumulo-test module ([here](https://github.com/apache/accumulo/blob/c1507285b4f52785d4c3e4b9fc6ab3ad17b32185/server/monitor/src/test/java/org/apache/accumulo/monitor/it/WebViewsIT.java#L63)), I had to create [a class to store the constant](https://github.com/apache/accumulo/blob/c1507285b4f52785d4c3e4b9fc6ab3ad17b32185/server/monitor/src/test/java/org/apache/accumulo/monitor/TagNameConstants.java). Please let me know what you think of this approach and if you have any alternatives in mind.', 'commenter': 'DomGarguilo'}, {'comment': 'Works for me.', 'commenter': 'ctubbsii'}]"
2545,test/src/main/java/org/apache/accumulo/harness/TestingKdc.java,"@@ -62,8 +62,8 @@ public static File computeKdcDir() {
     File targetDir = new File(System.getProperty(""user.dir""), ""target"");
     if (!targetDir.exists())
       assertTrue(targetDir.mkdirs());
-    assertTrue(""Could not find Maven target directory: "" + targetDir,
-        targetDir.exists() && targetDir.isDirectory());
+    assertTrue(targetDir.exists() && targetDir.isDirectory(),","[{'comment': ""Exists is redundant here. If it's a directory, it exists. I know it's not your change, but could clean it up while in here."", 'commenter': 'ctubbsii'}]"
2545,test/src/main/java/org/apache/accumulo/test/fate/zookeeper/ServiceLockIT.java,"@@ -405,7 +409,8 @@ public void testUnexpectedEvent() throws Exception {
 
   }
 
-  @Test(timeout = 60000)
+  @Test
+  @Timeout(value = 1, unit = MINUTES)","[{'comment': ""These should probably just show 60. For timeouts less than or equal to a minute, it's probably simpler to just show seconds:\r\n\r\n```suggestion\r\n  @Timeout(60)\r\n```"", 'commenter': 'ctubbsii'}]"
2545,test/src/main/java/org/apache/accumulo/test/functional/KerberosRenewalIT.java,"@@ -137,15 +138,16 @@ public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration coreS
     UserGroupInformation.setConfiguration(conf);
   }
 
-  @After
+  @AfterEach
   public void stopMac() throws Exception {
     if (mac != null) {
       mac.stop();
     }
   }
 
   // Intentionally setting the Test annotation timeout. We do not want to scale the timeout.
-  @Test(timeout = TEST_DURATION)
+  @Test
+  @Timeout(value = TEST_DURATION_MINUTES, unit = MINUTES)","[{'comment': 'Does the registered extension still scale these when the timeout is explicitly set this way? If it does, we probably need to modify the extension to skip these that have their timeout explicitly set (if possible).', 'commenter': 'ctubbsii'}, {'comment': 'No, these explicitly set timeouts are not scaled and take precedence over the registered extension.', 'commenter': 'DomGarguilo'}]"
2545,hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/WithTestNames.java,"@@ -16,7 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.accumulo;
+package org.apache.accumulo.hadoop;","[{'comment': ""This isn't unique enough to ensure that this package will only appear in the test classes/test jar. This package name is still one that could conceivable collide with a package in the main jar for this module if it is ever referenced by an integration test.\r\n\r\nIf it's only referenced by unit tests, that's fine, because jar sealing won't come into play... because the maven-surefire-plugin runs before the jars are built. However, integration tests run after the package phase when the jars are built, so the classes used for integration tests must not be in packages that could exist in the main jar."", 'commenter': 'ctubbsii'}, {'comment': ""I'm not sure I fully understand but I think I moved them into a package that is only used for tests. The new package for this class reads `org.apache.accumulo.hadoop.its`"", 'commenter': 'DomGarguilo'}]"
2545,server/monitor/src/test/java/org/apache/accumulo/monitor/TagNameConstants.java,"@@ -0,0 +1,23 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.monitor;","[{'comment': 'Similar comment as above, regarding the uniqueness of the test package name if the class is used for integration tests, due to jar sealing.', 'commenter': 'ctubbsii'}]"
2549,minicluster/pom.xml,"@@ -123,6 +123,12 @@
       <artifactId>curator-test</artifactId>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.curator</groupId>
+      <artifactId>curator-test</artifactId>
+      <version>5.1.0</version>","[{'comment': 'versions should be managed in the pom.xml at the root in the dependencyManagement section', 'commenter': 'ctubbsii'}]"
2549,minicluster/src/test/java/org/apache/accumulo/WithTestNames.java,"@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo;","[{'comment': 'For less confusion, this should be in a package corresponding to the module it is contained in.\r\n\r\n```suggestion\r\npackage org.apache.accumulo.minicluster;\r\n```', 'commenter': 'ctubbsii'}]"
2549,minicluster/src/test/java/org/apache/accumulo/minicluster/MiniAccumuloClusterTest.java,"@@ -98,7 +100,8 @@ public void checkDFSConstants() {
   }
 
   @SuppressWarnings(""deprecation"")
-  @Test(timeout = 30000)
+  @Test
+  @Timeout(value = 30000, unit = TimeUnit.MILLISECONDS)","[{'comment': ""This comment applies throughout this module. We only used millis because we had to. They should all be seconds now that we don't need to use millis.\r\n\r\n```suggestion\r\n  @Timeout(value = 30, unit = TimeUnit.SECONDS)\r\n```"", 'commenter': 'ctubbsii'}]"
2549,minicluster/src/test/java/org/apache/accumulo/minicluster/MiniAccumuloClusterTest.java,"@@ -169,20 +172,21 @@ public void test() throws Exception {
     conn.tableOperations().delete(""table1"");
   }
 
-  @Rule
-  public TemporaryFolder folder =
-      new TemporaryFolder(new File(System.getProperty(""user.dir"") + ""/target""));
+  @TempDir
+  private static File tempDir;
 
   @SuppressWarnings(""deprecation"")
-  @Test(timeout = 60000)
+  @Test
+  @Timeout(value = 60000, unit = TimeUnit.MILLISECONDS)
   public void testPerTableClasspath() throws Exception {
 
     org.apache.accumulo.core.client.Connector conn = accumulo.getConnector(""root"", ""superSecret"");
 
     conn.tableOperations().create(""table2"");
 
-    File jarFile = folder.newFile(""iterator.jar"");
-    FileUtils.copyURLToFile(this.getClass().getResource(""/FooFilter.jar""), jarFile);
+    File jarFile = new File(tempDir, ""iterator.jar"");
+    FileUtils.copyURLToFile(Objects.requireNonNull(this.getClass().getResource(""/FooFilter.jar"")),
+        jarFile);","[{'comment': 'Could static import `requireNonNull` and drop `this.` to make this one line and more readable.\r\n```suggestion\r\n    FileUtils.copyURLToFile(requireNonNull(getClass().getResource(""/FooFilter.jar"")), jarFile);\r\n```', 'commenter': 'ctubbsii'}]"
2549,minicluster/pom.xml,"@@ -138,6 +144,11 @@
       <artifactId>easymock</artifactId>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.junit.jupiter</groupId>
+      <artifactId>junit-jupiter-api</artifactId>
+      <scope>test</scope>
+    </dependency>
     <dependency>
       <groupId>org.junit.vintage</groupId>","[{'comment': 'Vintage engine should be able to be removed once all tests are converted to JUnit5.', 'commenter': 'DomGarguilo'}, {'comment': 'Looks like the vintage engine dependency was commented out instead of removed, possibly by accident.', 'commenter': 'DomGarguilo'}]"
2549,minicluster/src/test/java/org/apache/accumulo/miniclusterImpl/CleanShutdownMacTest.java,"@@ -18,30 +18,32 @@
  */
 package org.apache.accumulo.miniclusterImpl;
 
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
 import java.io.File;
 import java.util.Collections;
 import java.util.concurrent.Callable;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Future;
 
 import org.easymock.EasyMock;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TemporaryFolder;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.io.TempDir;
 
 import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 
 @SuppressFBWarnings(value = ""PATH_TRAVERSAL_IN"", justification = ""paths not set by user input"")
 public class CleanShutdownMacTest {
 
-  @Rule
-  public TemporaryFolder tmpDir =
-      new TemporaryFolder(new File(System.getProperty(""user.dir"") + ""/target""));
+  @TempDir
+  private static File tmpDir;
 
   @SuppressWarnings(""unchecked"")
   @Test
   public void testExecutorServiceShutdown() throws Exception {
-    File tmp = tmpDir.newFolder();
+    // File tmp = tmpDir.newFolder();
+    File tmp = new File(tmpDir, ""folder1/"");","[{'comment': 'Can remove commented out code.\r\n\r\nInstead of ""folder1"", can use `testName()`', 'commenter': 'ctubbsii'}]"
2549,minicluster/pom.xml,"@@ -123,6 +123,12 @@
       <artifactId>curator-test</artifactId>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.curator</groupId>
+      <artifactId>curator-test</artifactId>
+      <version>5.1.0</version>
+      <scope>test</scope>
+    </dependency>","[{'comment': '```suggestion\r\n    <dependency>\r\n      <groupId>org.apache.curator</groupId>\r\n      <artifactId>curator-test</artifactId>\r\n      <version>5.1.0</version>\r\n      <scope>test</scope>\r\n    </dependency>\r\n```\r\nI think this can be removed as a duplicate of the dependency above it.', 'commenter': 'DomGarguilo'}]"
2554,core/src/main/java/org/apache/accumulo/core/client/AccumuloClient.java,"@@ -346,6 +347,16 @@ ConditionalWriter createConditionalWriter(String tableName, ConditionalWriterCon
    */
   interface ClientFactory<T> {
 
+    /**
+     * Override default handling of uncaught exceptions in client threads
+     *
+     * @param ueh
+     *          UncaughtExceptionHandler implementation
+     * @return AccumuloClient or Properties
+     * @since 2.1.0
+     */
+    ClientFactory<T> withUncaughtExceptionHandler(Class<? extends UncaughtExceptionHandler> ueh);","[{'comment': 'Instead of providing a class that we must instantiate (and may not have the expected constructors), why not require the instance of the class?\r\n\r\n```suggestion\r\n    ClientFactory<T> withUncaughtExceptionHandler(UncaughtExceptionHandler ueh);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'With the code the way that it is, would it allow a user to put the class name in the client properties file? If so, then doing it this way would preclude that.', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -164,6 +168,19 @@ public ClientContext(SingletonReservation reservation, ClientInfo info,
     this.singletonReservation = Objects.requireNonNull(reservation);
     this.tableops = new TableOperationsImpl(this);
     this.namespaceops = new NamespaceOperationsImpl(this, tableops);
+    String uehClassName = serverConf.get(ClientProperty.UNCAUGHT_EXCEPTION_HANDLER.getKey());
+    if (!StringUtils.isEmpty(uehClassName)) {
+      try {
+        @SuppressWarnings(""unchecked"")
+        Class<? extends UncaughtExceptionHandler> clazz =
+            (Class<? extends UncaughtExceptionHandler>) Class.forName(uehClassName);
+        Threads.setUncaughtExceptionHandler(clazz.getDeclaredConstructor().newInstance());
+      } catch (ClassNotFoundException | InstantiationException | IllegalAccessException
+          | IllegalArgumentException | InvocationTargetException | NoSuchMethodException
+          | SecurityException e) {","[{'comment': 'Several of these extend `ReflectiveOperationException`. Catching that should replace several others.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 16bc28c', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -164,6 +168,19 @@ public ClientContext(SingletonReservation reservation, ClientInfo info,
     this.singletonReservation = Objects.requireNonNull(reservation);
     this.tableops = new TableOperationsImpl(this);
     this.namespaceops = new NamespaceOperationsImpl(this, tableops);
+    String uehClassName = serverConf.get(ClientProperty.UNCAUGHT_EXCEPTION_HANDLER.getKey());
+    if (!StringUtils.isEmpty(uehClassName)) {
+      try {
+        @SuppressWarnings(""unchecked"")
+        Class<? extends UncaughtExceptionHandler> clazz =
+            (Class<? extends UncaughtExceptionHandler>) Class.forName(uehClassName);","[{'comment': '```suggestion\r\n        var clazz = Class.forName(uehClassName).asSubclass(UncaughtExceptionHandler.class);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 16bc28c', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -164,6 +168,19 @@ public ClientContext(SingletonReservation reservation, ClientInfo info,
     this.singletonReservation = Objects.requireNonNull(reservation);
     this.tableops = new TableOperationsImpl(this);
     this.namespaceops = new NamespaceOperationsImpl(this, tableops);
+    String uehClassName = serverConf.get(ClientProperty.UNCAUGHT_EXCEPTION_HANDLER.getKey());
+    if (!StringUtils.isEmpty(uehClassName)) {
+      try {
+        @SuppressWarnings(""unchecked"")
+        Class<? extends UncaughtExceptionHandler> clazz =
+            (Class<? extends UncaughtExceptionHandler>) Class.forName(uehClassName);
+        Threads.setUncaughtExceptionHandler(clazz.getDeclaredConstructor().newInstance());
+      } catch (ClassNotFoundException | InstantiationException | IllegalAccessException
+          | IllegalArgumentException | InvocationTargetException | NoSuchMethodException
+          | SecurityException e) {
+        throw new RuntimeException(""Error setting uncaughtExceptionHandler"", e);","[{'comment': ""Could choose a more specific RTE. IAE seems appropriate, since it is instantiated by a value provided by the user. It's a bit late to be throwing that, since it's not directly in the builder, but it's probably still better than ISE. Could throw this earlier, in the builder."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 16bc28c', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/util/threads/Threads.java,"@@ -25,6 +25,8 @@
 
 public class Threads {
 
+  private static UncaughtExceptionHandler UEH = new AccumuloUncaughtExceptionHandler();","[{'comment': 'We should be taking care to avoid using JVM-wide static state. What if the user creates two clients with different exception handlers in the same JVM?', 'commenter': 'ctubbsii'}, {'comment': 'Looking into this further, I think I want to just support a property in the client properties file and not on the AccumuloClient object. Everything in ThreadPools and Threads is static and I would need to change everything to be not-static and supply either a ClientContext or ServerContext object (which means I would have to change a ton more code). If we want to support a per-client UEH, then it might be easier to close this and update #2346 .', 'commenter': 'dlmarion'}, {'comment': ""I think that other PR was trying to do a lot more than what I'm suggesting here, which is just the minimal amount of work to make Threads non-static and dangling off of ClientContext, so it can be have a per-client UEH. Yes, it would be larger than this PR. Maybe it could be done as a prerequisite, or even as a follow-on, to this one. But, I think the limitation of having one client control the UEH for all clients is substantial. Other than that limitation, I really like the direction this change took."", 'commenter': 'ctubbsii'}, {'comment': 'I was able to pull some of the ideas from #2346 into this PR. 16bc28c should address the issue that you raised here.', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/classloader/DefaultContextClassLoaderFactory.java,"@@ -63,7 +64,12 @@ public DefaultContextClassLoaderFactory(final AccumuloConfiguration accConf) {
 
   private static void startCleanupThread(final AccumuloConfiguration conf,
       final Supplier<Map<String,String>> contextConfigSupplier) {
-    ScheduledFuture<?> future = ThreadPools.createGeneralScheduledExecutorService(conf)
+    ScheduledFuture<?> future = ThreadPools.getClientThreadPools(new UncaughtExceptionHandler() {
+      @Override
+      public void uncaughtException(Thread arg0, Throwable arg1) {
+        LOG.error(""context classloader cleanup thread has failed."", arg1);
+      }
+    }).createGeneralScheduledExecutorService(conf)","[{'comment': 'Formatting is probably bad, but should be possible to use a lambda here.\r\n\r\n```suggestion\r\n    ScheduledFuture<?> future = ThreadPools.getClientThreadPools((t, e) ->\r\n        LOG.error(""context classloader cleanup thread has failed."", e)).createGeneralScheduledExecutorService(conf)\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in ec0e7cf', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java,"@@ -365,8 +371,12 @@ private void reschedule(SendTask task) {
     this.context = context;
     this.auths = config.getAuthorizations();
     this.ve = new VisibilityEvaluator(config.getAuthorizations());
-    this.threadPool = ThreadPools.createScheduledExecutorService(config.getMaxWriteThreads(),
-        this.getClass().getSimpleName(), false);
+    this.threadPool = context.getClientThreadPools().createScheduledExecutorService(
+        config.getMaxWriteThreads(), this.getClass().getSimpleName(), false);
+    this.cleanupThreadPool = context.getClientThreadPools().createFixedThreadPool(1, 3, SECONDS,
+        ""Conditional Writer Cleanup Thread"", true);
+    this.cleaner = CleanerUtil.shutdownThreadPoolExecutor(cleanupThreadPool, closed,
+        LoggerFactory.getLogger(ConditionalWriterImpl.class));","[{'comment': ""This cleanupThreadPool would probably be better left in ClientContext, and shut down when that is closed, rather than store it as an instance member here, where it's only ever used in the close method of this instance. It's very weird to *start* using resources for an object in that object's close method, which should be cleaning up resources. Putting it in ClientContext makes this effectively a singleton again (per Client, not per JVM, though, but that's still better than per-ConditionalWriter) and its own cleanup will be a bit more sane there, I think."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in ec0e7cf', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/clientImpl/ScannerIterator.java,"@@ -65,11 +67,10 @@
 
   private ScannerImpl.Reporter reporter;
 
-  private static ThreadPoolExecutor readaheadPool =
-      ThreadPools.createThreadPool(0, Integer.MAX_VALUE, 3L, SECONDS,
-          ""Accumulo scanner read ahead thread"", new SynchronousQueue<>(), true);
+  private final ThreadPoolExecutor readaheadPool;","[{'comment': 'Same as my previous comment, this could be moved to ClientContext and cleaned up on `client.close()`. No need for the Cleaner.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in ec0e7cf', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java,"@@ -65,8 +66,18 @@ public ExecutionError(String message, Throwable cause) {
   // the number of seconds before we allow a thread to terminate with non-use.
   public static final long DEFAULT_TIMEOUT_MILLISECS = 180000L;
 
+  private static final ThreadPools SERVER_INSTANCE = new ThreadPools(Threads.UEH);
+
+  public static final ThreadPools getServerThreadPools() {
+    return SERVER_INSTANCE;
+  }","[{'comment': ""I don't know if it's easy to do in all cases, but I was thinking that instead of this, it can share the same sort of code as for the client-side cases.\r\n\r\nEssentially, ClientContext would have a `.threadPools()` method that returns an instance of `ThreadPools` constructed using the UEH provided when AccumuloClient was built (default if not provided). When ServerContext is constructed, it just passes the built-in UEH for the server-side code to ClientContext's constructor in `super()`.\r\n\r\nSo, instead of calling `ThreadPools.getServerThreadPools().whatever`, you'd just use `context.threadPools().whatever` everywhere, regardless of client or server context. That way, there's no static instance of ThreadPools anywhere, but everything still works as expected. (`context.threadPools()` is also shorter than `ThreadPools.getServerThreadPools()`)"", 'commenter': 'ctubbsii'}, {'comment': ""I don't know that in all cases server-side that the ServerContext is available to the code. I would have to wire it up in all cases where it's not reachable."", 'commenter': 'dlmarion'}, {'comment': ""To be clear, only ClientContext would be necessary to wire up. ServerContext isn't available in the core jar. However, it is a subclass of ClientContext, so it would still work if only a ClientContext. But even then, ClientContext might not be available everywhere, and might need to be wired up... that's one of the things we've been fighting against to avoid static state in the JVM and the problems that come with it, so it's not a new issue. But, I was just hoping we could try to avoid a few additional occurrences of it that we'd have to fix later.\r\n\r\nIn any case, that could be treated as a follow-on. I think the main things to address before this is merged in is to move the static thread pools that were converted to instance members for cleanup into ClientContext, so they exist in a Closeable object and don't need a separate Cleaner, and so they can be reused for the cleanup tasks they perform across multiple instances of the objects they clean up."", 'commenter': 'ctubbsii'}, {'comment': ""Won't they still need a Cleaner in the case where the client is not closed?"", 'commenter': 'dlmarion'}, {'comment': ""I thought the client context already has a cleaner that ensures it is closed, because of the legacy Instance/Connector stuff that may not clean up the client context.\r\n\r\nHowever, even if it doesn't, Cleaner's (replacing finalizers) are a hack to close things during garbage collection, and are primarily useful for objects that don't explicitly get closed. For objects whose lifecycle is user-facing, like our Closeable clients, we don't need to specify a Cleaner... we (or the user) can just close them. Java helps us with this by giving us warnings about resource leaks and potential resource leaks for unclosed Closeable objects."", 'commenter': 'ctubbsii'}, {'comment': 'I see the something in SingletonReservation that closes AccumuloClients if that is what you are referencing. I will remove the Cleaners from ClientContext.', 'commenter': 'dlmarion'}, {'comment': 'It is. Thanks.', 'commenter': 'ctubbsii'}]"
2554,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -729,14 +787,18 @@ public AuthenticationToken token() {
 
   @Override
   public void close() {
-    closed = true;
+    closed.compareAndSet(false, true);
     if (thriftTransportPool != null) {
       thriftTransportPool.shutdown();
     }
     if (tableZooHelper != null) {
       tableZooHelper.close();
     }
     singletonReservation.close();
+    this.scannerReadaheadPool.shutdownNow(); // abort all tasks, client is shutting down
+    this.scannerPoolCleaner.clean();
+    this.cleanupThreadPool.shutdown(); // wait for shutdown tasks to execute
+    this.cleaner.clean();","[{'comment': 'I think these cleaners can go away, based on my previous comments.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 180654d', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -737,6 +783,8 @@ public void close() {
       tableZooHelper.close();
     }
     singletonReservation.close();
+    this.scannerReadaheadPool.shutdownNow(); // abort all tasks, client is shutting down
+    this.cleanupThreadPool.shutdown(); // wait for shutdown tasks to execute","[{'comment': ""These should be ordered before the `singletonReservation.close()` call above them. Also, I noticed the other thread pools held by this class are lazily instantiated, so could be null. Should these be lazily instantiated as well? It would save some resources when using a client for things that doesn't require them."", 'commenter': 'ctubbsii'}, {'comment': 'I can move the `close()` calls up. Regarding lazy instantiation, what other thread pools were you referencing? The class javadoc says `Any state in this object should be available at the time of its construction.`, does that on mean configuration?', 'commenter': 'dlmarion'}]"
2554,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -164,13 +175,48 @@ public ClientContext(SingletonReservation reservation, ClientInfo info,
     this.singletonReservation = Objects.requireNonNull(reservation);
     this.tableops = new TableOperationsImpl(this);
     this.namespaceops = new NamespaceOperationsImpl(this, tableops);
+    String uehClassName = serverConf.get(ClientProperty.UNCAUGHT_EXCEPTION_HANDLER.getKey());","[{'comment': ""The way this is being read from the config is a bit weird. It's reading a ClientProperty from an AccumuloConfiguration, which means that the client property would have had to exist in the `accumulo.properties` file, but not documented as a server-side property. The property is explicitly named `client.uncaught.exception.handler`, but the way it is loaded, it will set it for the server-side as well. I think this would be cleaner if it were renamed to `instance.uncaught.exception.handler` and put in `Property.java` with other server properties. You could even use ClassLoaderUtil directly, or via ConfigurationTypeHelper to load the class from the configuration.\r\n\r\nIf you wanted a separate client-side property name, you could do that (see how ClientConfConverter translates ZooKeeper host names, for an example), but I don't think that'd be necessary."", 'commenter': 'ctubbsii'}]"
2557,pom.xml,"@@ -1684,7 +1684,6 @@
                   -Xep:CheckReturnValue:OFF \
                   -Xep:MustBeClosedChecker:OFF \
                   -Xep:ReturnValueIgnored:OFF \
-                  -Xep:FutureReturnValueIgnored:ERROR \","[{'comment': ""Except that it's default severity is [WARNING](https://errorprone.info/bugpattern/FutureReturnValueIgnored) and all warnings are disabled in the [configuration](https://github.com/apache/accumulo/blob/main/pom.xml#L1682)"", 'commenter': 'dlmarion'}, {'comment': ""@dlmarion, sorry, my mistake. I thought it was under the Error pattern list vs the Warning pattern. I'll close this as not implemented. In a future PR related to some other EP related changes, I may consider moving the line beneath the 'warning' comment so keep separate Error patterns from Warning patterns. Would you have any objections to that re-arrangement?"", 'commenter': 'jmark99'}, {'comment': 'no, I have no issue with that. I would like to keep this at ERROR though, unless we remove the disable all warnings parameter, in case the issue is introduced in new code.', 'commenter': 'dlmarion'}, {'comment': 'No problem! Will definitely keep it as an error.', 'commenter': 'jmark99'}]"
2562,shell/src/main/java/org/apache/accumulo/shell/commands/FateCommand.java,"@@ -108,9 +109,8 @@ public JsonElement serialize(byte[] link, Type type, JsonSerializationContext co
     }
   }
 
-  private Option secretOption;
-  private Option statusOption;
-  private Option disablePaginationOpt;
+  private Option cancel, delete, dump, fail, list, print, secretOption, statusOption,
+      disablePaginationOpt;","[{'comment': 'nit. This makes diffs much easier to understand when code changes. I would strongly prefer one line per variable.', 'commenter': 'ctubbsii'}, {'comment': ""I think GH does a pretty good job of showing you the changes in one line, but I'll change it for you. I wonder if this is something we can enforce at build time."", 'commenter': 'dlmarion'}, {'comment': 'Addressed in c27679a', 'commenter': 'dlmarion'}, {'comment': ""I would love to enforce it with our formatter style, but I don't think it's possible."", 'commenter': 'ctubbsii'}, {'comment': ""For what it's worth, this is the recommendation from the [Code Conventions For the Java Programming Language](https://www.oracle.com/java/technologies/javase/codeconventions-declarations.html#2992)"", 'commenter': 'ctubbsii'}]"
2562,shell/src/main/java/org/apache/accumulo/shell/commands/FateCommand.java,"@@ -243,7 +248,7 @@ private boolean cancelSubmittedTxs(final Shell shellState, String[] args)
       throws AccumuloException, AccumuloSecurityException {
     ClientContext context = shellState.getContext();
     for (int i = 1; i < args.length; i++) {
-      Long txid = Long.parseLong(args[i]);
+      Long txid = Long.parseLong(args[i], 16);","[{'comment': 'Not sure why this is being auto-boxed here. `ManagerClient.cancelFateOperation(context, txid)` should also take a `long` not a `Long`, because by the time it gets to `FateService.Client.cancelFateOperation(...)` it is a `long`, not a `Long`. Using `Long` just unnecessarily auto-boxes and auto-unboxes.\r\n\r\n```suggestion\r\n      long txid = Long.parseLong(args[i], 16);\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""Also, was this a bug before that the radix wasn't 16?"", 'commenter': 'ctubbsii'}, {'comment': ""> Also, was this a bug before that the radix wasn't 16?\r\n\r\nYes, Mike found it in testing."", 'commenter': 'dlmarion'}, {'comment': 'Addressed in c27679a', 'commenter': 'dlmarion'}]"
2569,core/src/main/java/org/apache/accumulo/core/metrics/MetricsProducer.java,"@@ -617,6 +617,13 @@
   String METRICS_UPDATE_WALOG_WRITE = METRICS_UPDATE_PREFIX + ""walog.write"";
   String METRICS_UPDATE_MUTATION_ARRAY_SIZE = METRICS_UPDATE_PREFIX + ""mutation.arrays.size"";
 
+  String METRICS_PROPSTORE_PREFIX = ""accumulo.prop.store."";","[{'comment': 'Please update the class javadoc with these new properties', 'commenter': 'dlmarion'}]"
2569,core/src/main/java/org/apache/accumulo/core/conf/AccumuloConfiguration.java,"@@ -160,15 +160,17 @@ public long getUpdateCount() {
 
     PrefixProps prefixProps = cachedPrefixProps.get(property);
 
-    if (prefixProps == null || prefixProps.updateCount != getUpdateCount()) {
+    long currentCount = getUpdateCount();
+
+    if (prefixProps == null || prefixProps.updateCount != currentCount) {
       prefixCacheUpdateLock.lock();
       try {
         // Very important that update count is read before getting properties. Also only read it
         // once.
-        long updateCount = getUpdateCount();
+        long startCount = getUpdateCount();","[{'comment': 'I\'m not sure this needed a name change. It\'s more to review, at a minimum. It might be nice to go back through and try to minimize the changes that aren\'t ""germane"" to this PR, like change in whitespace or variable renames.', 'commenter': 'ctubbsii'}]"
2569,core/src/main/java/org/apache/accumulo/core/conf/AccumuloConfiguration.java,"@@ -183,7 +185,7 @@ public long getUpdateCount() {
           localPrefixes.putAll(cachedPrefixProps);
 
           // put the updates
-          prefixProps = new PrefixProps(propMap, updateCount);
+          prefixProps = new PrefixProps(propMap, getUpdateCount());","[{'comment': 'What was the rationale for calling this method a second time, instead of using the previously retrieved updateCount saved in the variable? What is the impact if the value retrieved here is different from the previously retrieved updateCount?', 'commenter': 'ctubbsii'}]"
2569,core/src/main/java/org/apache/accumulo/core/conf/ConfigurationTypeHelper.java,"@@ -181,7 +181,7 @@ public static double getFraction(String str) {
     try {
       instance = getClassInstance(context, clazzName, base);
     } catch (RuntimeException | IOException | ReflectiveOperationException e) {
-      log.warn(""Failed to load class {}"", clazzName, e);
+      log.warn(""Failed to load context: {} class: {}"", context, clazzName, e);","[{'comment': 'This log message reads like it failed to load the context rather than the class.\r\n```suggestion\r\n      log.warn(""Failed to load class {} in classloader context {}"", clazzName, context, e);\r\n```', 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/ServerContext.java,"@@ -107,6 +111,9 @@ private ServerContext(ServerInfo info) {
     this.info = info;
     zooReaderWriter = new ZooReaderWriter(info.getSiteConfiguration());
     serverDirs = info.getServerDirs();
+
+    propStore = new ZooPropStore.Builder(info.getInstanceID(), zooReaderWriter,
+        zooReaderWriter.getSessionTimeout()).build();","[{'comment': ""I'm not sure a builder is needed at all. Could just do:\r\n```suggestion\r\n    propStore = new ZooPropStore(this);\r\n```\r\n"", 'commenter': 'ctubbsii'}]"
2569,core/src/main/java/org/apache/accumulo/core/conf/AccumuloConfiguration.java,"@@ -597,4 +599,13 @@ public T derive() {
    * this configuration.
    */
   public void invalidateCache() {}
+
+  /**
+   * get a parent configuration or null if it does not exist.
+   *
+   * @since 2.1.0
+   */
+  public AccumuloConfiguration getParent() {
+    return null;","[{'comment': 'Should this return something?', 'commenter': 'milleruntime'}, {'comment': 'I think the idea is that subclasses override this if they have a parent.', 'commenter': 'ctubbsii'}, {'comment': 'Sub-classes that extend AccumuloConfiguration that have a hierarchy,  override this method to return a parent - for  ones that are stand alone (like DefaultConfiguration and ConfigurationCopy) then there is no hierarchy and null is appropriate.  The alternative would be to declare the method abstract and force each sub-class to provide an implementation. So the null is appropriate, it just depends on where the responsibility for returning null is implemented. Here, or the sub-classes.', 'commenter': 'EdColeman'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/ServerContext.java,"@@ -158,10 +165,8 @@ public synchronized ServerConfigurationFactory getServerConfFactory() {
   @Override
   public AccumuloConfiguration getConfiguration() {
     if (systemConfig == null) {
-      // system configuration uses its own instance of ZooCache
-      // this could be useful to keep its update counter independent
-      ZooCache propCache = new ZooCache(getZooReader(), null);
-      systemConfig = new ZooConfiguration(this, propCache, getSiteConfiguration());
+      systemConfig = new SystemConfiguration(log, this, PropCacheId.forSystem(getInstanceID()),","[{'comment': ""I think you don't need to pass in the SystemContext logger... it can use its own logger in SystemConfiguration."", 'commenter': 'ctubbsii'}]"
2569,test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java,"@@ -572,7 +572,7 @@ public void addauths() throws Exception {
         sleepUninterruptibly(500, TimeUnit.MILLISECONDS);
       }
     }
-    assertTrue(""Could not successfully see updated authoriations"", passed);
+    assertTrue(""Could not successfully see updated authorizations"", passed);","[{'comment': 'The changes in this class appear unrelated to this PR. If so, please submit them in a separate PR, so we can merge them quickly, without slowing down this one. Same for anything else minor in this that is unrelated.', 'commenter': 'ctubbsii'}]"
2569,test/src/main/java/org/apache/accumulo/test/functional/WatchTheWatchCountIT.java,"@@ -60,8 +60,8 @@ public void test() throws Exception {
       }
       c.tableOperations().list();
       String zooKeepers = ClientProperty.INSTANCE_ZOOKEEPERS.getValue(props);
-      final long MIN = 475L;
-      final long MAX = 900L;
+      final long MIN = 150L; // 475L;
+      final long MAX = 250L; // 900L;","[{'comment': ""The old values don't need to be commented out. A better comment might explain where the range of values comes from (like *X number of tables' properties times Y threads plus Z non-property watched fields* or whatever).\r\n```suggestion\r\n      final long MIN = 150L;\r\n      final long MAX = 250L;\r\n```"", 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/util/TablePropUtil.java,"@@ -18,48 +18,40 @@
  */
 package org.apache.accumulo.server.util;
 
-import static java.nio.charset.StandardCharsets.UTF_8;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 
-import org.apache.accumulo.core.Constants;
 import org.apache.accumulo.core.conf.Property;
 import org.apache.accumulo.core.data.TableId;
-import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
-import org.apache.accumulo.fate.zookeeper.ZooUtil.NodeExistsPolicy;
-import org.apache.accumulo.fate.zookeeper.ZooUtil.NodeMissingPolicy;
 import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.PropCacheId;
 import org.apache.zookeeper.KeeperException;
 
 public class TablePropUtil {
 
-  public static boolean setTableProperty(ServerContext context, TableId tableId, String property,
-      String value) throws KeeperException, InterruptedException {
-    return setTableProperty(context.getZooReaderWriter(), context.getZooKeeperRoot(), tableId,
-        property, value);
-  }
-
-  public static boolean setTableProperty(ZooReaderWriter zoo, String zkRoot, TableId tableId,
-      String property, String value) throws KeeperException, InterruptedException {
-    if (!Property.isTablePropertyValid(property, value))
-      return false;
-
-    // create the zk node for per-table properties for this table if it doesn't already exist
-    String zkTablePath = getTablePath(zkRoot, tableId);
-    zoo.putPersistentData(zkTablePath, new byte[0], NodeExistsPolicy.SKIP);
-
-    // create the zk node for this property and set it's data to the specified value
-    String zPath = zkTablePath + ""/"" + property;
-    zoo.putPersistentData(zPath, value.getBytes(UTF_8), NodeExistsPolicy.OVERWRITE);
+  public static boolean setTableProperties(ServerContext context, TableId tableId,
+      final Map<String,String> props) throws KeeperException, InterruptedException {
+    Map<String,String> tempProps = new HashMap<>(props);
+    tempProps.entrySet().removeIf(e -> !Property.isTablePropertyValid(e.getKey(), e.getValue()));
 
+    context.getPropStore().putAll(PropCacheId.forTable(context, tableId), props);
     return true;","[{'comment': 'make this `void` ?', 'commenter': 'dlmarion'}, {'comment': 'This used to return `false` if not a valid table property. `NamespacePropUtil.setNamespaceProperty` returns false. SystemPropUtil.setSystemProperty throws IllegalArgumentException. Suggest making these act the same. It may make sense to have a common interface to ensure that the behavior is the same. Something like:\r\n\r\n```\r\npublic interface PropUtil {\r\n\r\n  void setProperty()\r\n  \r\n  void removeProperty()\r\n\r\n}\r\n```', 'commenter': 'dlmarion'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/ServerContext.java,"@@ -107,6 +111,9 @@ private ServerContext(ServerInfo info) {
     this.info = info;
     zooReaderWriter = new ZooReaderWriter(info.getSiteConfiguration());
     serverDirs = info.getServerDirs();
+
+    propStore = new ZooPropStore.Builder(info.getInstanceID(), zooReaderWriter,","[{'comment': 'Why not call `ZooPropStore.initialize()` ?', 'commenter': 'dlmarion'}, {'comment': ""> Why not call `ZooPropStore.initialize()` ?\r\n\r\nI think initialize implies that it's creating the entry in ZK."", 'commenter': 'ctubbsii'}]"
2569,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java,"@@ -253,6 +253,8 @@ public TabletServerResourceManager(ServerContext context) {
     this.context = context;
     final AccumuloConfiguration acuConf = context.getConfiguration();
 
+    log.info(""Using configuration: {}"", acuConf);
+","[{'comment': 'We already log the configuration on startup for all the servers. So, this is a bit redundant and should be removed before merging.\r\n```suggestion\r\n\r\n```', 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/NamespaceConfiguration.java,"@@ -121,32 +74,23 @@ public void getProperties(Map<String,String> props, Predicate<String> filter) {
     if (getNamespaceId().equals(Namespace.ACCUMULO.id()))
       parentFilter = key -> isIteratorOrConstraint(key) ? false : filter.test(key);
 
-    getPropCacheAccessor().getProperties(props, getPath(), filter, parent, parentFilter);
+    getParent().getProperties(props, parentFilter != null ? parentFilter : filter);
+
+    Map<String,String> theseProps = getSnapshot();
+    for (Map.Entry<String,String> p : theseProps.entrySet()) {
+      if (filter.test(p.getKey()) && p.getValue() != null) {
+        props.put(p.getKey(), p.getValue());
+      }
+    }","[{'comment': '```suggestion\r\n    getSnapshot().entrySet().filter(e -> filter.test(e.getKey()) && e.getValue() != null)\r\n        .forEach(e -> props.put(e.getKey(), e.getValue()));\r\n```', 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/RuntimeFixedProperties.java,"@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.checkerframework.checker.nullness.qual.Nullable;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Utility class to a manage a fixed set of defined properties (designated in Properties as fixed).
+ * Certain properties are stored for persistence across restarts, they are read during start-up and
+ * remain unchanged for the life of the instance. Any updates to the properties will only be
+ * reflected with a restart.
+ * <p>
+ * Note that there are no guarantees that all services will always have the same values. If a fixed
+ * property value is changed and if all services are not restarted, they would be operating with
+ * different values.
+ */
+public class RuntimeFixedProperties {","[{'comment': 'Not sure if this class needs to be part of this PR, or if it could be its own change first. It seems like it could be evaluated on its own.', 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java,"@@ -29,15 +29,19 @@
 import org.apache.accumulo.core.data.InstanceId;
 import org.apache.accumulo.core.data.NamespaceId;
 import org.apache.accumulo.core.data.TableId;
-import org.apache.accumulo.fate.zookeeper.ZooCache;
-import org.apache.accumulo.fate.zookeeper.ZooCacheFactory;
 import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * A factor for configurations used by a server process. Instance of this class are thread-safe.
  */
 public class ServerConfigurationFactory extends ServerConfiguration {
 
+  // TODO - would it add clarity if log passed in by caller?
+  private static final Logger log = LoggerFactory.getLogger(ServerConfigurationFactory.class);
+","[{'comment': ""I don't think this class needs its own logger. It's just a trivial wrapper that we can't get rid of because it leaks into old balancer APIs. ServerConfiguration should have its own logger."", 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/ZooBasedConfiguration.java,"@@ -0,0 +1,284 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf;
+
+import static java.util.Objects.requireNonNull;
+
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.function.Predicate;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.apache.accumulo.server.conf.store.PropChangeListener;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.PropStoreException;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.checkerframework.checker.nullness.qual.Nullable;
+import org.slf4j.Logger;
+
+/**
+ * Instances maintain a local cache of the AccumuloConfiguration hierarchy that will be consistent
+ * with stored properties.
+ * <p>
+ * When calling getProperties - the local copy will be updated if ZooKeeper changes have been
+ * received.
+ * <p>
+ * The getUpdateCount() provides an optimization for clients - the count can be used to detect
+ * changes without reading the properties. When the update count changes, the next getProperties
+ * call will update the local copy and the change count.
+ */
+public class ZooBasedConfiguration extends AccumuloConfiguration implements PropChangeListener {
+
+  protected final Logger log;
+  private final AccumuloConfiguration parent;
+  private final PropCacheId propCacheId;
+  private final PropStore propStore;
+
+  private final AtomicReference<PropSnapshot> snapshotRef = new AtomicReference<>(null);
+
+  public ZooBasedConfiguration(Logger log, ServerContext context, PropCacheId propCacheId,
+      AccumuloConfiguration parent) {
+    this.log = requireNonNull(log, ""a Logger must be supplied"");
+    requireNonNull(context, ""the context cannot be null"");
+    this.propCacheId = requireNonNull(propCacheId, ""a PropCacheId must be supplied"");
+    this.parent = requireNonNull(parent, ""An AccumuloConfiguration parent must be supplied"");
+
+    this.propStore =
+        requireNonNull(context.getPropStore(), ""The PropStore must be supplied and exist"");
+
+    propStore.registerAsListener(propCacheId, this);
+
+    snapshotRef.set(updateSnapshot());
+
+  }
+
+  public long getDataVersion() {
+    var snapshot = snapshotRef.get();
+    if (snapshot == null) {
+      return updateSnapshot().getDataVersion();
+    }
+    return snapshot.getDataVersion();
+  }
+
+  /**
+   * The update count is the sum of the change count of this configuration and the change counts of
+   * the parents. The count is used to detect if any changes occurred in the configuration hierarchy
+   * and if the configuration needs to be recalculated to maintain consistency with values in the
+   * backend store.
+   * <p>
+   * The count is required to be an increasing value.
+   */
+  @Override
+  public long getUpdateCount() {
+    long count = 0;
+    long dataVersion = 0;
+    for (AccumuloConfiguration p = this; p != null; p = p.getParent()) {
+      if (p instanceof ZooBasedConfiguration) {
+        dataVersion = ((ZooBasedConfiguration) p).getDataVersion();
+      } else {
+        dataVersion = p.getUpdateCount();
+      }
+      count += dataVersion;
+    }
+
+    log.trace(""update count result for: {} - data version: {} update: {}"", propCacheId, dataVersion,
+        count);
+    return count;
+  }
+
+  @Override
+  public AccumuloConfiguration getParent() {
+    return parent;
+  }
+
+  public PropCacheId getCacheId() {
+    return propCacheId;
+  }
+
+  @Override
+  public @Nullable String get(final Property property) {
+    Map<String,String> props = getSnapshot();
+    String value = props.get(property.getKey());
+    if (value != null) {
+      return value;
+    }
+    AccumuloConfiguration parent = getParent();
+    if (parent != null) {
+      return parent.get(property);
+    }
+    return null;
+  }
+
+  @Override
+  public void getProperties(final Map<String,String> props, final Predicate<String> filter) {
+
+    parent.getProperties(props, filter);
+
+    Map<String,String> theseProps = getSnapshot();
+
+    log.trace(""getProperties() for: {} filter: {}, have: {}, passed: {}"", getCacheId(), filter,
+        theseProps, props);
+
+    for (Map.Entry<String,String> p : theseProps.entrySet()) {
+      if (filter.test(p.getKey()) && p.getValue() != null) {
+        log.trace(""passed filter - add to map: {} = {}"", p.getKey(), p.getValue());
+        props.put(p.getKey(), p.getValue());
+      }
+    }
+  }
+
+  @Override
+  public boolean isPropertySet(final Property property) {
+
+    Map<String,String> theseProps = getSnapshot();
+
+    if (theseProps.get(property.getKey()) != null) {
+      return true;
+    }
+
+    return getParent().isPropertySet(property);
+
+  }
+
+  public Map<String,String> getSnapshot() {
+    if (snapshotRef.get() == null) {
+      return updateSnapshot().getProps();
+    }
+    return snapshotRef.get().getProps();
+  }","[{'comment': ""There are two calls to snapshotRef. The second could be null after you've checked the first isn't.\r\n\r\n```suggestion\r\n    var snap = snapshotRef.get();\r\n    if (snap == null) {\r\n      return updateSnapshot().getProps();\r\n    }\r\n    return snap.getProps();\r\n  }\r\n```"", 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/ZooBasedConfiguration.java,"@@ -0,0 +1,284 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf;
+
+import static java.util.Objects.requireNonNull;
+
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.function.Predicate;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.apache.accumulo.server.conf.store.PropChangeListener;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.PropStoreException;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.checkerframework.checker.nullness.qual.Nullable;
+import org.slf4j.Logger;
+
+/**
+ * Instances maintain a local cache of the AccumuloConfiguration hierarchy that will be consistent
+ * with stored properties.
+ * <p>
+ * When calling getProperties - the local copy will be updated if ZooKeeper changes have been
+ * received.
+ * <p>
+ * The getUpdateCount() provides an optimization for clients - the count can be used to detect
+ * changes without reading the properties. When the update count changes, the next getProperties
+ * call will update the local copy and the change count.
+ */
+public class ZooBasedConfiguration extends AccumuloConfiguration implements PropChangeListener {
+
+  protected final Logger log;
+  private final AccumuloConfiguration parent;
+  private final PropCacheId propCacheId;
+  private final PropStore propStore;
+
+  private final AtomicReference<PropSnapshot> snapshotRef = new AtomicReference<>(null);
+
+  public ZooBasedConfiguration(Logger log, ServerContext context, PropCacheId propCacheId,
+      AccumuloConfiguration parent) {
+    this.log = requireNonNull(log, ""a Logger must be supplied"");
+    requireNonNull(context, ""the context cannot be null"");
+    this.propCacheId = requireNonNull(propCacheId, ""a PropCacheId must be supplied"");
+    this.parent = requireNonNull(parent, ""An AccumuloConfiguration parent must be supplied"");
+
+    this.propStore =
+        requireNonNull(context.getPropStore(), ""The PropStore must be supplied and exist"");
+
+    propStore.registerAsListener(propCacheId, this);
+
+    snapshotRef.set(updateSnapshot());
+
+  }
+
+  public long getDataVersion() {
+    var snapshot = snapshotRef.get();
+    if (snapshot == null) {
+      return updateSnapshot().getDataVersion();
+    }
+    return snapshot.getDataVersion();
+  }
+
+  /**
+   * The update count is the sum of the change count of this configuration and the change counts of
+   * the parents. The count is used to detect if any changes occurred in the configuration hierarchy
+   * and if the configuration needs to be recalculated to maintain consistency with values in the
+   * backend store.
+   * <p>
+   * The count is required to be an increasing value.
+   */
+  @Override
+  public long getUpdateCount() {
+    long count = 0;
+    long dataVersion = 0;
+    for (AccumuloConfiguration p = this; p != null; p = p.getParent()) {
+      if (p instanceof ZooBasedConfiguration) {
+        dataVersion = ((ZooBasedConfiguration) p).getDataVersion();
+      } else {
+        dataVersion = p.getUpdateCount();
+      }
+      count += dataVersion;
+    }
+
+    log.trace(""update count result for: {} - data version: {} update: {}"", propCacheId, dataVersion,
+        count);
+    return count;
+  }
+
+  @Override
+  public AccumuloConfiguration getParent() {
+    return parent;
+  }
+
+  public PropCacheId getCacheId() {
+    return propCacheId;
+  }
+
+  @Override
+  public @Nullable String get(final Property property) {
+    Map<String,String> props = getSnapshot();
+    String value = props.get(property.getKey());
+    if (value != null) {
+      return value;
+    }
+    AccumuloConfiguration parent = getParent();
+    if (parent != null) {
+      return parent.get(property);
+    }
+    return null;
+  }
+
+  @Override
+  public void getProperties(final Map<String,String> props, final Predicate<String> filter) {
+
+    parent.getProperties(props, filter);
+
+    Map<String,String> theseProps = getSnapshot();
+
+    log.trace(""getProperties() for: {} filter: {}, have: {}, passed: {}"", getCacheId(), filter,
+        theseProps, props);
+
+    for (Map.Entry<String,String> p : theseProps.entrySet()) {
+      if (filter.test(p.getKey()) && p.getValue() != null) {
+        log.trace(""passed filter - add to map: {} = {}"", p.getKey(), p.getValue());
+        props.put(p.getKey(), p.getValue());
+      }
+    }
+  }
+
+  @Override
+  public boolean isPropertySet(final Property property) {
+
+    Map<String,String> theseProps = getSnapshot();
+
+    if (theseProps.get(property.getKey()) != null) {
+      return true;
+    }
+
+    return getParent().isPropertySet(property);
+
+  }
+
+  public Map<String,String> getSnapshot() {
+    if (snapshotRef.get() == null) {
+      return updateSnapshot().getProps();
+    }
+    return snapshotRef.get().getProps();
+  }
+
+  @Override
+  public void invalidateCache() {
+    snapshotRef.set(null);
+  }
+
+  private final Lock updateLock = new ReentrantLock();
+
+  private @NonNull PropSnapshot updateSnapshot() throws PropStoreException {
+
+    PropSnapshot localSnapshot = snapshotRef.get();
+
+    if (localSnapshot != null) {
+      // no changes return locally cached config
+      return localSnapshot;
+    }
+    updateLock.lock();
+    int retryCount = 5;
+    try {
+      localSnapshot = snapshotRef.get();
+      // check for update while waiting for lock.
+      if (localSnapshot != null) {
+        return localSnapshot;
+      }
+
+      PropSnapshot propsRead;
+
+      long startCount;
+      do {
+        startCount = propStore.getNodeVersion(propCacheId);
+        propsRead = doRead();
+        if (propsRead.getDataVersion() == startCount) {
+          snapshotRef.set(propsRead);
+          return snapshotRef.get();
+        }
+      } while (--retryCount > 0);
+
+      snapshotRef.set(null);
+    } finally {
+      updateLock.unlock();
+    }
+    throw new IllegalStateException(
+        ""Failed to read property updates for "" + propCacheId + "" after "" + retryCount + "" tries"");
+
+  }
+
+  private PropSnapshot doRead() throws PropStoreException {
+
+    var vProps = propStore.get(propCacheId);
+    log.trace(""doRead() - updateSnapshot() for {}, returned: {}"", propCacheId, vProps);
+    if (vProps == null) {
+      // TODO - this could return marker instead?
+      // return new PropSnapshot(INVALID_DATA, Map.of());
+      throw new IllegalStateException(""Properties for "" + propCacheId + "" do not exist"");
+    } else {
+      return new PropSnapshot(vProps.getDataVersion(), vProps.getProperties());
+    }
+  }
+
+  @Override
+  public void zkChangeEvent(PropCacheId watchedId) {
+    if (propCacheId.equals(watchedId)) {
+      log.debug(""Received zookeeper property change event for {} - current version: {}"",
+          propCacheId,
+          snapshotRef.get() != null ? snapshotRef.get().getDataVersion() : ""no data version set"");","[{'comment': 'multiple get calls to snapshotRef could result in null after null check.', 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/ZooBasedConfiguration.java,"@@ -0,0 +1,284 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf;
+
+import static java.util.Objects.requireNonNull;
+
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.function.Predicate;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.apache.accumulo.server.conf.store.PropChangeListener;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.PropStoreException;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.checkerframework.checker.nullness.qual.Nullable;
+import org.slf4j.Logger;
+
+/**
+ * Instances maintain a local cache of the AccumuloConfiguration hierarchy that will be consistent
+ * with stored properties.
+ * <p>
+ * When calling getProperties - the local copy will be updated if ZooKeeper changes have been
+ * received.
+ * <p>
+ * The getUpdateCount() provides an optimization for clients - the count can be used to detect
+ * changes without reading the properties. When the update count changes, the next getProperties
+ * call will update the local copy and the change count.
+ */
+public class ZooBasedConfiguration extends AccumuloConfiguration implements PropChangeListener {
+
+  protected final Logger log;
+  private final AccumuloConfiguration parent;
+  private final PropCacheId propCacheId;
+  private final PropStore propStore;
+
+  private final AtomicReference<PropSnapshot> snapshotRef = new AtomicReference<>(null);
+
+  public ZooBasedConfiguration(Logger log, ServerContext context, PropCacheId propCacheId,
+      AccumuloConfiguration parent) {
+    this.log = requireNonNull(log, ""a Logger must be supplied"");
+    requireNonNull(context, ""the context cannot be null"");
+    this.propCacheId = requireNonNull(propCacheId, ""a PropCacheId must be supplied"");
+    this.parent = requireNonNull(parent, ""An AccumuloConfiguration parent must be supplied"");
+
+    this.propStore =
+        requireNonNull(context.getPropStore(), ""The PropStore must be supplied and exist"");
+
+    propStore.registerAsListener(propCacheId, this);
+
+    snapshotRef.set(updateSnapshot());
+
+  }
+
+  public long getDataVersion() {
+    var snapshot = snapshotRef.get();
+    if (snapshot == null) {
+      return updateSnapshot().getDataVersion();
+    }
+    return snapshot.getDataVersion();
+  }
+
+  /**
+   * The update count is the sum of the change count of this configuration and the change counts of
+   * the parents. The count is used to detect if any changes occurred in the configuration hierarchy
+   * and if the configuration needs to be recalculated to maintain consistency with values in the
+   * backend store.
+   * <p>
+   * The count is required to be an increasing value.
+   */
+  @Override
+  public long getUpdateCount() {
+    long count = 0;
+    long dataVersion = 0;
+    for (AccumuloConfiguration p = this; p != null; p = p.getParent()) {
+      if (p instanceof ZooBasedConfiguration) {
+        dataVersion = ((ZooBasedConfiguration) p).getDataVersion();
+      } else {
+        dataVersion = p.getUpdateCount();
+      }
+      count += dataVersion;
+    }
+
+    log.trace(""update count result for: {} - data version: {} update: {}"", propCacheId, dataVersion,
+        count);
+    return count;
+  }
+
+  @Override
+  public AccumuloConfiguration getParent() {
+    return parent;
+  }
+
+  public PropCacheId getCacheId() {
+    return propCacheId;
+  }
+
+  @Override
+  public @Nullable String get(final Property property) {
+    Map<String,String> props = getSnapshot();
+    String value = props.get(property.getKey());
+    if (value != null) {
+      return value;
+    }
+    AccumuloConfiguration parent = getParent();
+    if (parent != null) {
+      return parent.get(property);
+    }
+    return null;
+  }
+
+  @Override
+  public void getProperties(final Map<String,String> props, final Predicate<String> filter) {
+
+    parent.getProperties(props, filter);
+
+    Map<String,String> theseProps = getSnapshot();
+
+    log.trace(""getProperties() for: {} filter: {}, have: {}, passed: {}"", getCacheId(), filter,
+        theseProps, props);
+
+    for (Map.Entry<String,String> p : theseProps.entrySet()) {
+      if (filter.test(p.getKey()) && p.getValue() != null) {
+        log.trace(""passed filter - add to map: {} = {}"", p.getKey(), p.getValue());
+        props.put(p.getKey(), p.getValue());
+      }
+    }
+  }
+
+  @Override
+  public boolean isPropertySet(final Property property) {
+
+    Map<String,String> theseProps = getSnapshot();
+
+    if (theseProps.get(property.getKey()) != null) {
+      return true;
+    }
+
+    return getParent().isPropertySet(property);
+
+  }
+
+  public Map<String,String> getSnapshot() {
+    if (snapshotRef.get() == null) {
+      return updateSnapshot().getProps();
+    }
+    return snapshotRef.get().getProps();
+  }
+
+  @Override
+  public void invalidateCache() {
+    snapshotRef.set(null);
+  }
+
+  private final Lock updateLock = new ReentrantLock();
+
+  private @NonNull PropSnapshot updateSnapshot() throws PropStoreException {
+
+    PropSnapshot localSnapshot = snapshotRef.get();
+
+    if (localSnapshot != null) {
+      // no changes return locally cached config
+      return localSnapshot;
+    }
+    updateLock.lock();
+    int retryCount = 5;
+    try {
+      localSnapshot = snapshotRef.get();
+      // check for update while waiting for lock.
+      if (localSnapshot != null) {
+        return localSnapshot;
+      }
+
+      PropSnapshot propsRead;
+
+      long startCount;
+      do {
+        startCount = propStore.getNodeVersion(propCacheId);
+        propsRead = doRead();
+        if (propsRead.getDataVersion() == startCount) {
+          snapshotRef.set(propsRead);
+          return snapshotRef.get();
+        }
+      } while (--retryCount > 0);
+
+      snapshotRef.set(null);
+    } finally {
+      updateLock.unlock();
+    }
+    throw new IllegalStateException(
+        ""Failed to read property updates for "" + propCacheId + "" after "" + retryCount + "" tries"");
+
+  }
+
+  private PropSnapshot doRead() throws PropStoreException {
+
+    var vProps = propStore.get(propCacheId);
+    log.trace(""doRead() - updateSnapshot() for {}, returned: {}"", propCacheId, vProps);
+    if (vProps == null) {
+      // TODO - this could return marker instead?
+      // return new PropSnapshot(INVALID_DATA, Map.of());
+      throw new IllegalStateException(""Properties for "" + propCacheId + "" do not exist"");
+    } else {
+      return new PropSnapshot(vProps.getDataVersion(), vProps.getProperties());
+    }
+  }
+
+  @Override
+  public void zkChangeEvent(PropCacheId watchedId) {
+    if (propCacheId.equals(watchedId)) {
+      log.debug(""Received zookeeper property change event for {} - current version: {}"",
+          propCacheId,
+          snapshotRef.get() != null ? snapshotRef.get().getDataVersion() : ""no data version set"");
+      // snapshotRef.set(new PropSnapshot(INVALID_DATA_VER, Map.of()));
+      snapshotRef.set(null);
+    }
+  }
+
+  @Override
+  public void cacheChangeEvent(PropCacheId watchedId) {
+    if (propCacheId.equals(watchedId)) {
+      log.debug(""Received cache property change event for {} - current version: {}"", propCacheId,
+          snapshotRef.get() != null ? snapshotRef.get().getDataVersion() : ""no data version set"");","[{'comment': 'again', 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/store/PropCacheId.java,"@@ -0,0 +1,278 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.store;
+
+import static org.apache.accumulo.core.Constants.ZCONFIG;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_CONF;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_CONF;
+
+import java.util.Comparator;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.StringJoiner;
+
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.checkerframework.checker.nullness.qual.NonNull;
+
+/**
+ * Provides a strongly-typed id for storing properties in ZooKeeper. The path in ZooKeeper is
+ * determined by the instance id and the type (system, namespace and table), with different root
+ * paths.
+ * <p>
+ * Provides utility methods from constructing different id based on type and methods to parse a
+ * ZooKeeper path and return a prop cache id.
+ */
+public class PropCacheId implements Comparable<PropCacheId> {","[{'comment': ""> Should / could this extend `AbstractId` ?\r\n\r\nThis is more of an abstract reference to a path in ZK rather than a typed-POJO wrapper around a canonical String like AbstractId is. It's used more like a cache key, than an identifier. Could be renamed to PropCacheKey."", 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/store/PropCacheId.java,"@@ -0,0 +1,278 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.store;
+
+import static org.apache.accumulo.core.Constants.ZCONFIG;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_CONF;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_CONF;
+
+import java.util.Comparator;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.StringJoiner;
+
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.checkerframework.checker.nullness.qual.NonNull;
+
+/**
+ * Provides a strongly-typed id for storing properties in ZooKeeper. The path in ZooKeeper is
+ * determined by the instance id and the type (system, namespace and table), with different root
+ * paths.
+ * <p>
+ * Provides utility methods from constructing different id based on type and methods to parse a
+ * ZooKeeper path and return a prop cache id.
+ */
+public class PropCacheId implements Comparable<PropCacheId> {
+
+  public static final String PROP_NODE_NAME = ""encoded_props"";
+
+  // indices for path.split();
+  public static final int TYPE_TOKEN_POSITION = 3;
+  public static final int IID_TOKEN_POSITION = 2;
+  public static final int ID_TOKEN_POSITION = 4;
+
+  // remove starting slash from constant.
+  public static final String TABLES_NODE_NAME = ZTABLES.substring(1);
+  public static final String NAMESPACE_NODE_NAME = ZNAMESPACES.substring(1);
+
+  private final String path;
+  private final IdType idType;
+  private final NamespaceId namespaceId;
+  private final TableId tableId;
+
+  private PropCacheId(final String path, final IdType idType, final NamespaceId namespaceId,
+      final TableId tableId) {
+    this.path = path;
+    this.idType = idType;
+    this.namespaceId = namespaceId;
+    this.tableId = tableId;
+  }
+
+  /**
+   * Instantiate a system prop cache id using the instance id from the context.
+   *
+   * @param context
+   *          the system context specifying the instance id
+   * @return a prop cache id for system properties,
+   */
+  public static PropCacheId forSystem(final ServerContext context) {
+    return forSystem(context.getInstanceID());
+  }
+
+  /**
+   * Instantiate a system prop cache id.
+   *
+   * @param instanceId
+   *          the instance id.
+   * @return a prop cache id for system properties,
+   */
+  public static PropCacheId forSystem(final InstanceId instanceId) {
+    return new PropCacheId(ZooUtil.getRoot(instanceId) + ZCONFIG + ""/"" + PROP_NODE_NAME,
+        IdType.SYSTEM, null, null);
+  }
+
+  /**
+   * Instantiate a namespace prop cache id using the instance id from the context.
+   *
+   * @param context
+   *          the system context specifying the instance id
+   * @param namespaceId
+   *          the namespace id
+   * @return a prop cache id a namespaces properties,
+   */
+  public static PropCacheId forNamespace(final ServerContext context,
+      final NamespaceId namespaceId) {
+    return forNamespace(context.getInstanceID(), namespaceId);
+  }
+
+  /**
+   * Instantiate a namespace prop cache id using the instance id from the context.
+   *
+   * @param instanceId
+   *          the instance id
+   * @param namespaceId
+   *          the namespace id
+   * @return a prop cache id a namespaces properties,
+   */
+  public static PropCacheId forNamespace(final InstanceId instanceId,
+      final NamespaceId namespaceId) {
+    return new PropCacheId(ZooUtil.getRoot(instanceId) + ZNAMESPACES + ""/"" + namespaceId.canonical()
+        + ZNAMESPACE_CONF + ""/"" + PROP_NODE_NAME, IdType.NAMESPACE, namespaceId, null);
+  }
+
+  /**
+   * Instantiate a namespace prop cache id using the instance id from the context.
+   *
+   * @param context
+   *          the system context specifying the instance id
+   * @param tableId
+   *          the table id
+   * @return a prop cache id a namespaces properties,
+   */
+  public static PropCacheId forTable(final ServerContext context, final TableId tableId) {
+    return forTable(context.getInstanceID(), tableId);
+  }
+
+  /**
+   * Instantiate a namespace prop cache id using the instance id from the context.
+   *
+   * @param instanceId
+   *          the instance id
+   * @param tableId
+   *          the table id
+   * @return a prop cache id a namespaces properties,
+   */
+  public static PropCacheId forTable(final InstanceId instanceId, final TableId tableId) {
+    return new PropCacheId(ZooUtil.getRoot(instanceId) + ZTABLES + ""/"" + tableId.canonical()
+        + ZTABLE_CONF + ""/"" + PROP_NODE_NAME, IdType.TABLE, null, tableId);
+  }
+
+  /**
+   * Determine the prop cache id from a ZooKeeper path
+   *
+   * @param path
+   *          the path
+   * @return the prop cache id
+   */
+  public static Optional<PropCacheId> fromPath(final String path) {
+    String[] tokens = path.split(""/"");
+
+    InstanceId instanceId = InstanceId.of(tokens[IID_TOKEN_POSITION]);
+
+    IdType type = extractType(tokens);
+
+    switch (type) {
+      case SYSTEM:
+        return Optional.of(PropCacheId.forSystem(instanceId));
+      case NAMESPACE:
+        return Optional
+            .of(PropCacheId.forNamespace(instanceId, NamespaceId.of(tokens[ID_TOKEN_POSITION])));
+      case TABLE:
+        return Optional.of(PropCacheId.forTable(instanceId, TableId.of(tokens[ID_TOKEN_POSITION])));
+      case UNKNOWN:
+      default:
+        return Optional.empty();
+    }
+  }
+
+  /**
+   * Determine if the IdType is system, namespace or table from a tokenized path. To be a valid id,
+   * the final token is PROP_NODE_NAME and then the type is defined if the path has table or
+   * namespace in the path, otherwise it is assumed to be system.
+   *
+   * @param tokens
+   *          a path split into String[] of tokens
+   * @return the id type.
+   */
+  public static IdType extractType(final String[] tokens) {
+    if (tokens.length == 0 || !tokens[tokens.length - 1].equals(PROP_NODE_NAME)) {
+      // without tokens or it does not end with PROP_NAME_NAME
+      return IdType.UNKNOWN;
+    }
+    if (tokens[TYPE_TOKEN_POSITION].equals(TABLES_NODE_NAME)) {
+      return IdType.TABLE;
+    }
+    if (tokens[TYPE_TOKEN_POSITION].equals(NAMESPACE_NODE_NAME)) {
+      return IdType.NAMESPACE;
+    }
+    return IdType.SYSTEM;
+  }
+
+  public String getPath() {
+    return path;
+  }
+
+  public IdType getIdType() {
+    return idType;
+  }
+
+  @Override
+  public int compareTo(@NonNull PropCacheId other) {
+    return Comparator.comparing(PropCacheId::getIdType).thenComparing(PropCacheId::getPath)
+        .compare(this, other);
+  }
+
+  // TODO - remove optional and return null.
+  /**
+   * If the prop cache is for a namespace, return the namespace id.
+   *
+   * @return the namespace id.
+   */
+  public Optional<NamespaceId> getNamespaceId() {
+    return Optional.ofNullable(namespaceId);
+  }
+
+  /**
+   * if the prop cache is for a table, return the table id.
+   *
+   * @return the table id.
+   */
+  public Optional<TableId> getTableId() {
+    return Optional.ofNullable(tableId);
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o)
+      return true;
+    if (o == null || getClass() != o.getClass())
+      return false;
+    PropCacheId that = (PropCacheId) o;
+    return path.equals(that.path);
+  }
+
+  @Override
+  public int hashCode() {
+    return Objects.hash(path);
+  }
+
+  @Override
+  public String toString() {
+    switch (idType) {
+      case SYSTEM:
+        return new StringJoiner("", "", PropCacheId.class.getSimpleName() + ""["", ""]"")
+            .add(""idType=System"").toString();
+      case NAMESPACE:
+        return new StringJoiner("", "", PropCacheId.class.getSimpleName() + ""["", ""]"")
+            .add(""idType=Namespace"").add(""namespaceId="" + namespaceId).toString();
+      case TABLE:
+        return new StringJoiner("", "", PropCacheId.class.getSimpleName() + ""["", ""]"")
+            .add(""idType=Table"").add(""tableId="" + tableId).toString();
+      default:
+        return new StringJoiner("", "", PropCacheId.class.getSimpleName() + ""["", ""]"")
+            .add(""idType="" + idType).add(""namespaceId="" + namespaceId).add(""tableId="" + tableId)
+            .add(""path='"" + path + ""'"").toString();
+    }
+
+  }
+
+  /**
+   * Define types of properties stored in zookeeper. Note: default properties are not in zookeeper
+   * but come from code.
+   */
+  public enum IdType {
+    UNKNOWN, SYSTEM, NAMESPACE, TABLE
+  }","[{'comment': ""I don't think we need these types. It's sufficient to have the entry point have the static entry points. The underlying key should just be the ZK path constructed using the InstanceId and the selected entry point method."", 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTask.java,"@@ -0,0 +1,92 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.store.impl;
+
+import java.util.Set;
+
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.apache.accumulo.server.conf.store.PropChangeListener;
+
+/**
+ * Provides a simple runnable base task for notifying listeners for PropStore event change
+ * notifications.
+ */
+public abstract class PropStoreEventTask implements Runnable {
+
+  private final PropCacheId propCacheId;
+  private final Set<PropChangeListener> listeners;
+
+  private PropStoreEventTask(final PropCacheId propCacheId,
+      final Set<PropChangeListener> listeners) {
+    this.propCacheId = propCacheId;
+    this.listeners = listeners;
+  }
+
+  public static class PropStoreZkChangeEventTask extends PropStoreEventTask {
+
+    PropStoreZkChangeEventTask(final PropCacheId propCacheId,
+        final Set<PropChangeListener> listeners) {
+      super(propCacheId, listeners);
+    }
+
+    @Override
+    public void run() {
+      super.listeners.forEach(listener -> listener.zkChangeEvent(super.propCacheId));
+    }
+  }
+
+  public static class PropStoreCacheChangeEventTask extends PropStoreEventTask {
+
+    PropStoreCacheChangeEventTask(final PropCacheId propCacheId,
+        final Set<PropChangeListener> listeners) {
+      super(propCacheId, listeners);
+    }
+
+    @Override
+    public void run() {
+      super.listeners.forEach(listener -> listener.cacheChangeEvent(super.propCacheId));
+    }
+  }
+
+  public static class PropStoreDeleteEventTask extends PropStoreEventTask {
+
+    PropStoreDeleteEventTask(final PropCacheId propCacheId,
+        final Set<PropChangeListener> listeners) {
+      super(propCacheId, listeners);
+    }
+
+    @Override
+    public void run() {
+      super.listeners.forEach(listener -> listener.deleteEvent(super.propCacheId));
+    }
+  }
+
+  public static class PropStoreConnectionEventTask extends PropStoreEventTask {
+
+    PropStoreConnectionEventTask(final PropCacheId propCacheId,
+        final Set<PropChangeListener> listeners) {
+      super(null, listeners);
+    }
+
+    @Override
+    public void run() {
+      super.listeners.forEach(listener -> listener.connectionEvent());","[{'comment': '```suggestion\r\n      super.listeners.forEach(PropChangeListener::connectionEvent);\r\n```', 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreWatcher.java,"@@ -0,0 +1,253 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.store.impl;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.util.threads.ThreadPools;
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.apache.accumulo.server.conf.store.PropChangeListener;
+import org.apache.zookeeper.WatchedEvent;
+import org.apache.zookeeper.Watcher;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * This class serves as a translator between ZooKeeper events and converts them to PropStore events.
+ * Using this as an intermediary, the external listeners do not need to set / manage external
+ * ZooKeeper watchers, they can register for PropStore events if they need to take active action on
+ * change detection.
+ * <p>
+ * Users of the PropStore.get() will get properties that match what is stored in ZooKeeper for each
+ * call and do not need to manage any caching. However, the ability to receive active notification
+ * without needed to register / manage ZooKeeper watchers external to the PropStore is provided in
+ * case other code is relying on active notifications.
+ * <p>
+ * The notification occurs on a separate thread from the ZooKeeper notification handling, but
+ * listeners should not perform lengthy operations on the notification thread so that other listener
+ * notifications are not delayed.
+ */
+public class PropStoreWatcher implements Watcher {
+
+  private static final Logger log = LoggerFactory.getLogger(PropStoreWatcher.class);
+
+  private final ExecutorService executorService =
+      ThreadPools.getServerThreadPools().createFixedThreadPool(1, ""zoo_change_update"", false);
+
+  private final ReentrantReadWriteLock listenerLock = new ReentrantReadWriteLock();
+  private final ReentrantReadWriteLock.ReadLock listenerReadLock = listenerLock.readLock();
+  private final ReentrantReadWriteLock.WriteLock listenerWriteLock = listenerLock.writeLock();
+
+  // access should be guarded by acquiring the listener read or write lock
+  private final Map<PropCacheId,Set<PropChangeListener>> listeners = new HashMap<>();
+
+  private final ReadyMonitor zkReadyMonitor;
+
+  public PropStoreWatcher(final ReadyMonitor zkReadyMonitor) {
+    this.zkReadyMonitor = zkReadyMonitor;
+  }
+
+  public void registerListener(final PropCacheId propCacheId, final PropChangeListener listener) {
+    listenerWriteLock.lock();
+    try {
+      Set<PropChangeListener> set = listeners.computeIfAbsent(propCacheId, s -> new HashSet<>());
+      set.add(listener);
+    } finally {
+      listenerWriteLock.unlock();
+    }
+  }
+
+  /**
+   * Process a ZooKeeper event. This method does not reset the watcher. Subscribers are notified of
+   * the change - if they call get to update and respond to the change the watcher will be (re)set
+   * then. This helps clean up watchers by not automatically re-adding the watcher on the event but
+   * only if being used.
+   *
+   * @param event
+   *          ZooKeeper event.
+   */
+  @SuppressWarnings(""FutureReturnValueIgnored"") // currently, tasks are fire and forget
+  @Override
+  public void process(final WatchedEvent event) {
+
+    String path;
+    switch (event.getType()) {
+      case NodeDataChanged:
+        path = event.getPath();
+        log.trace(""handle change event for path: {}"", path);
+        PropCacheId.fromPath(path).ifPresent(this::signalZkChangeEvent);
+        break;
+      case NodeDeleted:
+        path = event.getPath();
+        log.trace(""handle delete event for path: {}"", path);
+        PropCacheId.fromPath(path).ifPresent(cacheId -> {
+          // notify listeners
+          Set<PropChangeListener> snapshot = getListenerSnapshot(cacheId);
+          if (snapshot != null) {
+            executorService
+                .submit(new PropStoreEventTask.PropStoreDeleteEventTask(cacheId, snapshot));
+          }
+
+          listenerCleanup(cacheId);
+
+        });
+
+        break;
+      case None:
+        Event.KeeperState state = event.getState();
+        switch (state) {
+          // pause - could reconnect
+          case ConnectedReadOnly:
+          case Disconnected:
+            log.debug(""ZooKeeper disconnected event received"");
+            zkReadyMonitor.clearReady();
+            executorService.submit(new PropStoreEventTask.PropStoreConnectionEventTask(null,
+                getAllListenersSnapshot()));
+            break;
+
+          // okay
+          case SyncConnected:
+            log.debug(""ZooKeeper connected event received"");
+            zkReadyMonitor.setReady();
+            break;
+
+          // terminal - never coming back.
+          case Expired:
+          case Closed:
+            log.info(""ZooKeeper connection closed event received"");
+            zkReadyMonitor.clearReady();
+            zkReadyMonitor.setClosed(); // terminal condition
+            executorService.submit(new PropStoreEventTask.PropStoreConnectionEventTask(null,
+                getAllListenersSnapshot()));
+            break;
+
+          default:
+            log.trace(""ignoring zooKeeper state: {}"", state);
+        }
+        break;
+      default:
+        break;
+    }
+
+  }
+
+  /**
+   * Submit task to notify registered listeners that the propCacheId node received an event
+   * notification from ZooKeeper and should be updated. The process can be initiated either by a
+   * ZooKeeper notification or a change detected in the cache based on a ZooKeeper event.
+   *
+   * @param propCacheId
+   *          the cache id
+   */
+  @SuppressWarnings(""FutureReturnValueIgnored"") // currently, tasks are fire and forget","[{'comment': ""Suppressing non-standard warnings can generate new warnings in compilers/IDEs that don't recognize these non-standard ones (for good reason: unrecognized warnings submissions could be typos and not what the user intends). The ignoring of the return value should be solved in a different way, such as calling execute instead of submit."", 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreWatcher.java,"@@ -0,0 +1,253 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.store.impl;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.util.threads.ThreadPools;
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.apache.accumulo.server.conf.store.PropChangeListener;
+import org.apache.zookeeper.WatchedEvent;
+import org.apache.zookeeper.Watcher;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * This class serves as a translator between ZooKeeper events and converts them to PropStore events.
+ * Using this as an intermediary, the external listeners do not need to set / manage external
+ * ZooKeeper watchers, they can register for PropStore events if they need to take active action on
+ * change detection.
+ * <p>
+ * Users of the PropStore.get() will get properties that match what is stored in ZooKeeper for each
+ * call and do not need to manage any caching. However, the ability to receive active notification
+ * without needed to register / manage ZooKeeper watchers external to the PropStore is provided in
+ * case other code is relying on active notifications.
+ * <p>
+ * The notification occurs on a separate thread from the ZooKeeper notification handling, but
+ * listeners should not perform lengthy operations on the notification thread so that other listener
+ * notifications are not delayed.
+ */
+public class PropStoreWatcher implements Watcher {
+
+  private static final Logger log = LoggerFactory.getLogger(PropStoreWatcher.class);
+
+  private final ExecutorService executorService =
+      ThreadPools.getServerThreadPools().createFixedThreadPool(1, ""zoo_change_update"", false);
+
+  private final ReentrantReadWriteLock listenerLock = new ReentrantReadWriteLock();
+  private final ReentrantReadWriteLock.ReadLock listenerReadLock = listenerLock.readLock();
+  private final ReentrantReadWriteLock.WriteLock listenerWriteLock = listenerLock.writeLock();
+
+  // access should be guarded by acquiring the listener read or write lock
+  private final Map<PropCacheId,Set<PropChangeListener>> listeners = new HashMap<>();
+
+  private final ReadyMonitor zkReadyMonitor;
+
+  public PropStoreWatcher(final ReadyMonitor zkReadyMonitor) {
+    this.zkReadyMonitor = zkReadyMonitor;
+  }
+
+  public void registerListener(final PropCacheId propCacheId, final PropChangeListener listener) {
+    listenerWriteLock.lock();
+    try {
+      Set<PropChangeListener> set = listeners.computeIfAbsent(propCacheId, s -> new HashSet<>());
+      set.add(listener);
+    } finally {
+      listenerWriteLock.unlock();
+    }
+  }
+
+  /**
+   * Process a ZooKeeper event. This method does not reset the watcher. Subscribers are notified of
+   * the change - if they call get to update and respond to the change the watcher will be (re)set
+   * then. This helps clean up watchers by not automatically re-adding the watcher on the event but
+   * only if being used.
+   *
+   * @param event
+   *          ZooKeeper event.
+   */
+  @SuppressWarnings(""FutureReturnValueIgnored"") // currently, tasks are fire and forget
+  @Override
+  public void process(final WatchedEvent event) {
+
+    String path;
+    switch (event.getType()) {
+      case NodeDataChanged:
+        path = event.getPath();
+        log.trace(""handle change event for path: {}"", path);
+        PropCacheId.fromPath(path).ifPresent(this::signalZkChangeEvent);
+        break;
+      case NodeDeleted:
+        path = event.getPath();
+        log.trace(""handle delete event for path: {}"", path);
+        PropCacheId.fromPath(path).ifPresent(cacheId -> {
+          // notify listeners
+          Set<PropChangeListener> snapshot = getListenerSnapshot(cacheId);
+          if (snapshot != null) {
+            executorService
+                .submit(new PropStoreEventTask.PropStoreDeleteEventTask(cacheId, snapshot));
+          }
+
+          listenerCleanup(cacheId);
+
+        });
+
+        break;
+      case None:
+        Event.KeeperState state = event.getState();
+        switch (state) {
+          // pause - could reconnect
+          case ConnectedReadOnly:
+          case Disconnected:
+            log.debug(""ZooKeeper disconnected event received"");
+            zkReadyMonitor.clearReady();
+            executorService.submit(new PropStoreEventTask.PropStoreConnectionEventTask(null,
+                getAllListenersSnapshot()));
+            break;
+
+          // okay
+          case SyncConnected:
+            log.debug(""ZooKeeper connected event received"");
+            zkReadyMonitor.setReady();
+            break;
+
+          // terminal - never coming back.
+          case Expired:
+          case Closed:
+            log.info(""ZooKeeper connection closed event received"");
+            zkReadyMonitor.clearReady();
+            zkReadyMonitor.setClosed(); // terminal condition
+            executorService.submit(new PropStoreEventTask.PropStoreConnectionEventTask(null,
+                getAllListenersSnapshot()));
+            break;
+
+          default:
+            log.trace(""ignoring zooKeeper state: {}"", state);
+        }
+        break;
+      default:
+        break;
+    }
+
+  }
+
+  /**
+   * Submit task to notify registered listeners that the propCacheId node received an event
+   * notification from ZooKeeper and should be updated. The process can be initiated either by a
+   * ZooKeeper notification or a change detected in the cache based on a ZooKeeper event.
+   *
+   * @param propCacheId
+   *          the cache id
+   */
+  @SuppressWarnings(""FutureReturnValueIgnored"") // currently, tasks are fire and forget
+  public void signalZkChangeEvent(final PropCacheId propCacheId) {
+    log.trace(""signal ZooKeeper change event: {}"", propCacheId);
+    Set<PropChangeListener> snapshot = getListenerSnapshot(propCacheId);
+    log.trace(""Sending change event to: {}"", snapshot);
+    if (snapshot != null) {
+      executorService
+          .submit(new PropStoreEventTask.PropStoreZkChangeEventTask(propCacheId, snapshot));
+    }
+  }
+
+  /**
+   * Submit task to notify registered listeners that the propCacheId node change was detected should
+   * be updated.
+   *
+   * @param propCacheId
+   *          the cache id
+   */
+  @SuppressWarnings(""FutureReturnValueIgnored"") // currently, tasks are fire and forget
+  public void signalCacheChangeEvent(final PropCacheId propCacheId) {
+    log.trace(""cache change event: {}"", propCacheId);
+    Set<PropChangeListener> snapshot = getListenerSnapshot(propCacheId);
+    if (snapshot != null) {
+      executorService
+          .submit(new PropStoreEventTask.PropStoreCacheChangeEventTask(propCacheId, snapshot));
+    }
+  }
+
+  /**
+   * Clean-up the active listeners set when an entry is removed from the cache, remove it from the
+   * active listeners.
+   *
+   * @param propCacheId
+   *          the cache id
+   */
+  public void listenerCleanup(final PropCacheId propCacheId) {
+    listenerWriteLock.lock();
+    try {
+      listeners.remove(propCacheId);
+    } finally {
+      listenerWriteLock.unlock();
+    }
+  }
+
+  /**
+   * Get an immutable snapshot of the listeners for a prop cache id. The set is intended for
+   * notification of changes for a specific prop cache id.
+   *
+   * @param PropCacheId
+   *          the prop cache id
+   * @return an immutable copy of listeners.
+   */
+  private Set<PropChangeListener> getListenerSnapshot(final PropCacheId PropCacheId) {
+
+    Set<PropChangeListener> snapshot = null;
+    listenerReadLock.lock();
+    try {
+      Set<PropChangeListener> set = listeners.get(PropCacheId);
+      if (set != null) {
+        snapshot = Set.copyOf(set);
+      }
+
+    } finally {
+      listenerReadLock.unlock();
+    }
+    return snapshot;
+  }
+
+  /**
+   * Get an immutable snapshot of the all listeners registered for event. The set is intended for
+   * connection event notifications that are not specific to an individual prop cache id.
+   *
+   * @return an immutable copy of all registered listeners.
+   */
+  private Set<PropChangeListener> getAllListenersSnapshot() {
+
+    Set<PropChangeListener> snapshot;
+    listenerReadLock.lock();
+    try {
+
+      snapshot = listeners.keySet().stream().flatMap(key -> listeners.get(key).stream())
+          .collect(Collectors.toSet());
+
+    } finally {
+      listenerReadLock.unlock();
+    }
+    return Collections.unmodifiableSet(snapshot);","[{'comment': 'This would require static import on Collectors.collectingAndThen and Collectors.toSet, as I have it below, but this could be simplified:\r\n```suggestion\r\n    listenerReadLock.lock();\r\n    try {\r\n      return listeners.keySet().stream().flatMap(key -> listeners.get(key).stream())\r\n          .collect(collectingAndThen(toSet(), Collections::unmodifiableSet));\r\n    } finally {\r\n      listenerReadLock.unlock();\r\n    }\r\n```\r\n', 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/util/ConfigConverter.java,"@@ -0,0 +1,241 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.StringJoiner;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.conf.DeprecatedPropertyUtil;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+// TODO - this is in progress and should not be merged without changes.
+// TODO - needs upgrade integration and testing.
+/**
+ * Convert pre-2.1 system, namespace and table properties to PropEncoded format.
+ *
+ * <pre>
+ * Source ZooKeeper paths:
+ *   srcSysPath - system config source = /accumulo/[iid-id]/config;
+ *   srcNsBasePath - namespace config source /accumulo/[iid]/namespaces;
+ *   srcTableBasePath - table config source /accumulo/[iid]/tables;
+ * </pre>
+ */
+public class ConfigConverter {
+
+  private static final Logger log = LoggerFactory.getLogger(ConfigConverter.class);
+
+  private final ZooReaderWriter zrw;
+  private final InstanceId instanceId;
+
+  private final PropStore propStore;
+
+  private final String zkBasePath; // base path for accumulo instance - /accumulo/[iid]
+
+  private final Set<String> legacyPaths = new HashSet<>();
+
+  public ConfigConverter(final ServerContext context) {
+
+    instanceId = context.getInstanceID();
+    zrw = context.getZooReaderWriter();
+    propStore = context.getPropStore();
+
+    zkBasePath = ZooUtil.getRoot(instanceId);
+  }
+
+  public synchronized static void convert(final ServerContext context,
+      final boolean deleteWhenComplete) {
+    ConfigConverter converter = new ConfigConverter(context);
+    converter.convertSys();
+    converter.convertNamespace();
+    converter.convertTables();
+
+    if (deleteWhenComplete) {
+      converter.removeLegacyPaths();
+    }
+  }
+
+  @Override
+  public String toString() {
+    return new StringJoiner("", "", ConfigConverter.class.getSimpleName() + ""["", ""]"")
+        .add(""converted="" + legacyPaths).toString();
+  }
+
+  public void convertSys() {
+    var sysId = PropCacheId.forSystem(instanceId);
+    var zkPathSysConfig = zkBasePath + Constants.ZCONFIG;
+
+    Map<String,String> props = readLegacyProps(zkPathSysConfig);
+
+    Map<String,String> renamedProps = new HashMap<>();
+    props.forEach((original, value) -> {
+      var finalName = DeprecatedPropertyUtil.getReplacementName(original,
+          (log, replacement) -> log
+              .info(""Automatically renaming deprecated property '{}' with its replacement '{}'""
+                  + "" in ZooKeeper configuration upgrade."", original, replacement));
+      renamedProps.put(finalName, value);
+    });
+
+    log.info(""system props: {} -> {}"", props, renamedProps);
+
+    writeConverted(sysId, renamedProps, zkPathSysConfig);
+
+    // delete - the confirmation and then delete done in two steps so that the removal is atomic.
+    // If the props were deleted as confirmed
+  }
+
+  public void convertNamespace() {
+    var zkPathNamespaceBase = zkBasePath + Constants.ZNAMESPACES;
+    try {
+      List<String> namespaces = zrw.getChildren(zkPathNamespaceBase);
+      for (String namespace : namespaces) {
+        String zkPropBasePath = zkPathNamespaceBase + ""/"" + namespace + Constants.ZNAMESPACE_CONF;
+        log.info(""NS:{} base path: {}"", namespace, zkPropBasePath);
+        Map<String,String> props = readLegacyProps(zkPropBasePath);
+        log.info(""Namespace props: {} - {}"", namespace, props);
+        writeConverted(PropCacheId.forNamespace(instanceId, NamespaceId.of(namespace)), props,
+            zkPropBasePath);
+      }
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Failed to convert namespace from ZooKeeper for path: "" + zkPathNamespaceBase, ex);
+    } catch (InterruptedException ex) {
+      throw new IllegalStateException(
+          ""Interrupted reading namespaces from ZooKeeper for path: "" + zkPathNamespaceBase, ex);
+    }
+  }
+
+  public void convertTables() {
+    var zkPathTableBase = zkBasePath + Constants.ZTABLES;
+    try {
+      List<String> tables = zrw.getChildren(zkPathTableBase);
+      for (String table : tables) {
+        String zkPropBasePath = zkPathTableBase + ""/"" + table + Constants.ZTABLE_CONF;
+        log.info(""table:{} base path: {}"", table, zkPropBasePath);
+        Map<String,String> props = readLegacyProps(zkPropBasePath);
+        log.info(""table props: {} - {}"", table, props);
+        writeConverted(PropCacheId.forTable(instanceId, TableId.of(table)), props, zkPropBasePath);
+      }
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Failed to convert tables from ZooKeeper for path: "" + zkPathTableBase, ex);
+    } catch (InterruptedException ex) {
+      throw new IllegalStateException(
+          ""Interrupted reading namespaces from ZooKeeper for path: "" + zkPathTableBase, ex);
+    }
+  }
+
+  private void removeLegacyPaths() {
+    for (String path : legacyPaths) {
+      log.debug(""delete ZooKeeper path: {}"", path);
+      try {
+        zrw.delete(path);
+      } catch (KeeperException ex) {
+        log.warn(
+            ""Failed to delete path on property conversion "" + path + "", reason"" + ex.getMessage());
+      } catch (InterruptedException ex) {
+        Thread.currentThread().interrupt();
+        throw new IllegalStateException(ex);
+      }
+    }
+  }
+
+  private Map<String,String> readLegacyProps(final String path) {
+    requireNonNull(path, ""A ZooKeeper path for configuration properties must be supplied"");
+    Map<String,String> props = new HashMap<>();
+    try {
+      List<String> children = zrw.getChildren(path);
+      log.info(""Looking in: {}, found: {}"", path, children);
+      for (String child : children) {
+        if (Property.isValidPropertyKey(child)) {
+          byte[] bytes = zrw.getData(path + ""/"" + child);
+          props.put(child, new String(bytes, UTF_8));
+          legacyPaths.add(path + ""/"" + child);
+        } else {
+          log.info(""Skipping invalid property: {} in {}"", child, path);
+        }
+      }
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""Failed to get children from ZooKeeper for path: "" + path,
+          ex);
+    } catch (InterruptedException ex) {
+      throw new IllegalStateException(
+          ""Interrupted reading children from ZooKeeper for path: "" + path, ex);
+    }
+    return props;
+  }
+
+  private void writeConverted(final PropCacheId propCacheId, final Map<String,String> props,","[{'comment': ""I don't think this converter util needs to use PropCacheId. I think it would be simpler if it just used a path directly. A lot of this code might be redundant."", 'commenter': 'ctubbsii'}]"
2569,server/base/src/main/java/org/apache/accumulo/server/conf/util/ConfigPropertyPrinter.java,"@@ -0,0 +1,199 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import java.io.BufferedWriter;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintStream;
+import java.io.PrintWriter;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.cli.ServerUtilOpts;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.PropCacheId;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+// TODO - this is in progress and should not be merged without changes.
+// TODO - implement json output (or remove option)
+@AutoService(KeywordExecutable.class)
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_OUT"",
+    justification = ""app is run in same security context as user providing the filename"")
+public class ConfigPropertyPrinter implements KeywordExecutable {
+
+  private static final Logger log = LoggerFactory.getLogger(ConfigPropertyPrinter.class);
+
+  public ConfigPropertyPrinter() {}
+
+  public static void main(String[] args) throws Exception {
+    new ConfigPropertyPrinter().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""config-property-print"";","[{'comment': 'Maybe `print-config`?', 'commenter': 'ctubbsii'}]"
2569,server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImport.java,"@@ -183,8 +183,15 @@ private void checkForMerge(final long tid, final Manager manager) throws Excepti
     VolumeManager fs = manager.getVolumeManager();
     final Path bulkDir = new Path(bulkInfo.sourceDir);
 
-    int maxTablets = Integer.parseInt(manager.getContext().getTableConfiguration(bulkInfo.tableId)
-        .get(Property.TABLE_BULK_MAX_TABLETS));
+    String value = manager.getContext().getTableConfiguration(bulkInfo.tableId)
+        .get(Property.TABLE_BULK_MAX_TABLETS);
+    if (value == null) {
+      value = Property.TABLE_BULK_MAX_TABLETS.getDefaultValue();
+      log.info(""Property not found "" + Property.TABLE_BULK_MAX_TABLETS + "" using default: "" + value
+          + "" for tableId: "" + bulkInfo.tableId + "" using default: "" + value);
+    }
+
+    int maxTablets = Integer.parseInt(value);","[{'comment': 'This is a change in behavior that causes it to use a different value than the user intended to specify when there is a typo, instead of erroring out. It also does so with merely an info message, and not even a warning about the error. This seems completely unrelated to this PR and could be evaluated on its own in a different PR.', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MetadataUpdateCount.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.tserver.tablet;
+
+import java.util.Objects;
+
+public class MetadataUpdateCount {
+  private final long startedCount;
+  private final long finishedCount;
+
+  MetadataUpdateCount(long startedCount, long finishedCount) {
+    this.startedCount = startedCount;
+    this.finishedCount = finishedCount;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o)
+      return true;
+    if (o == null || getClass() != o.getClass())
+      return false;","[{'comment': '```suggestion\r\n    if (!(o instanceof MetadataUpdateCount))\r\n      return false;\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'That code was generated by intellij.  I think the original code will reject subclasses where your suggestion will let subclasses pass through.  For this case I like rejecting subclasses as I would like the equals method to be very strict. ', 'commenter': 'keith-turner'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -69,12 +70,12 @@
   // ensure we only have one reader/writer of our bulk file notes at at time
   private final Object bulkFileImportLock = new Object();
 
-  // This must be incremented whenever datafileSizes is mutated
-  private long updateCount;
+  // These must be incremented before and after datafileSizes and metadata table updates
+  private AtomicLong metadataUpdatesStarted = new AtomicLong();
+  private AtomicLong metadataUpdatesCompleted = new AtomicLong();","[{'comment': '```suggestion\r\n  private final AtomicLong metadataUpdatesStarted = new AtomicLong();\r\n  private final AtomicLong metadataUpdatesCompleted = new AtomicLong();\r\n```', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1419,7 +1419,11 @@ private void closeConsistencyCheck() {
         }
       });
 
-      compareToDataInMemory(tabletMeta);
+      if (!tabletMeta.getFilesMap().equals(getDatafileManager().getDatafileSizes())) {
+        String msg = ""Data files in "" + extent + "" differ from in-memory data ""
+            + tabletMeta.getFilesMap() + "" "" + getDatafileManager().getDatafileSizes();
+        log.error(msg);
+      }","[{'comment': 'Could these maps change between the check and the message construction?\r\n\r\n```suggestion\r\n      var filesMap = tabletMeta.getFilesMap();\r\n      var datafileSizes = getDatafileManager().getDatafileSizes();\r\n      if (!filesMap.equals(datafileSizes)) {\r\n        String msg = ""Data files in "" + extent + "" differ from in-memory data "" + filesMap + "" "" + datafileSizes;\r\n        log.error(msg);\r\n      }\r\n```\r\n\r\nIf the objects themselves are mutable, then it could be even more of a problem and we might need to do a protective copy to do the comparison.', 'commenter': 'ctubbsii'}, {'comment': '> Could these maps change between the check and the message construction?\r\n\r\nNo, neither one of them should change.\r\n\r\nWhat `tabletMeta.getFilesMap()` returns comes originally from Ample when it read from the metadata table.  If this were changing it would be a really weird bug that would have a much wider impact.  What `getDatafileManager().getDatafileSizes()` returns is a copy of its files made using the same lock this method is holding.  So it should not change given the current code, but it would probably be cleaner to only get it once. ', 'commenter': 'keith-turner'}, {'comment': 'I made an update re this in 14cf0d2c80c5ba55d54d25b1b2fa221a6f13dee8.  In that update I also adjusted the log message.', 'commenter': 'keith-turner'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1435,22 +1439,28 @@ private void closeConsistencyCheck() {
     }
   }
 
-  private void compareToDataInMemory(TabletMetadata tabletMetadata) {
-    if (!tabletMetadata.getFilesMap().equals(getDatafileManager().getDatafileSizes())) {
-      String msg = ""Data files in "" + extent + "" differ from in-memory data ""
-          + tabletMetadata.getFilesMap() + "" "" + getDatafileManager().getDatafileSizes();
-      log.error(msg);
+  public synchronized void compareTabletInfo(MetadataUpdateCount updateCounter,
+      TabletMetadata tabletMetadata) {
+    if (isClosed() || isClosing()) {
+      return;
     }
-  }
 
-  public synchronized void compareTabletInfo(Long updateCounter, TabletMetadata tabletMetadata) {
-    if (isClosed() || isClosing()) {
+    if (updateCounter.overlapsUpdate() || !updateCounter.equals(this.getUpdateCount())) {
+      // for these cases do not even bother checking if the files are the same
       return;
     }","[{'comment': "":+1: for the first case, where we're checking while an update is occurring. But, I'm not sure I understand the second case. What situation does that skip?"", 'commenter': 'ctubbsii'}, {'comment': 'In the second case there is evidence that a change has occurred, so its an optimization to avoid comparing the maps.', 'commenter': 'keith-turner'}, {'comment': ""> In the second case there is evidence that a change has occurred, so its an optimization to avoid comparing the maps.\r\n\r\nIsn't this evidence that either:\r\n1. a change has occurred, or\r\n2. a possible inconsistency has been detected?\r\n"", 'commenter': 'ctubbsii'}, {'comment': 'If a change has occurred then we can not reliably check for inconsistencies, so need to rely on the fact that it will eventually check again later.', 'commenter': 'keith-turner'}, {'comment': 'The big picture is that we are periodically reading a tablets metadata from the metadata table and comparing that to what the tablet has in memory.  We only want to do this comparison when there were no changes to the tablet while were in the process of reading the metadata table.   So when we know for sure a change has occurred to the tablet while we were reading from the metadata table there is no need to compare what we got from the metadata table to the tablets current files.', 'commenter': 'keith-turner'}, {'comment': 'I misunderstood the object returned from `getUpdateCount()` as still being a primitive, and not the new MetadataUpdateCount type you created. It makes sense now. Thanks.', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -500,8 +519,8 @@ public int getNumFiles() {
     return datafileSizes.size();
   }
 
-  public long getUpdateCount() {
-    return updateCount;
+  public MetadataUpdateCount getUpdateCount() {
+    return new MetadataUpdateCount(metadataUpdatesStarted.get(), metadataUpdatesCompleted.get());","[{'comment': ""I have a slight concern about this code.  It may matter that the start count is read before the completed count, not sure still thinking out it.  Thinking of replacing the two separate atomic longs with a single atomic reference to a counter object with two longs.  Then I don't have to worry about this issue."", 'commenter': 'keith-turner'}, {'comment': 'changed this in fd31a15', 'commenter': 'keith-turner'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -246,22 +247,27 @@ else if (!bulkDir.equals(tpath.getTabletDir()))
       }
     }
 
-    synchronized (tablet) {
-      for (Entry<StoredTabletFile,DataFileValue> tpath : newFiles.entrySet()) {
-        if (datafileSizes.containsKey(tpath.getKey())) {
-          log.error(""Adding file that is already in set {}"", tpath.getKey());
+    metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementStart());","[{'comment': '```suggestion\r\n    metadataUpdateCount.updateAndGet(MetadataUpdateCount::incrementStart);\r\n```', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -246,22 +247,27 @@ else if (!bulkDir.equals(tpath.getTabletDir()))
       }
     }
 
-    synchronized (tablet) {
-      for (Entry<StoredTabletFile,DataFileValue> tpath : newFiles.entrySet()) {
-        if (datafileSizes.containsKey(tpath.getKey())) {
-          log.error(""Adding file that is already in set {}"", tpath.getKey());
+    metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementStart());
+    // do not place any code here between above stmt and try{}finally
+    try {
+      synchronized (tablet) {
+        for (Entry<StoredTabletFile,DataFileValue> tpath : newFiles.entrySet()) {
+          if (datafileSizes.containsKey(tpath.getKey())) {
+            log.error(""Adding file that is already in set {}"", tpath.getKey());
+          }
+          datafileSizes.put(tpath.getKey(), tpath.getValue());
         }
-        datafileSizes.put(tpath.getKey(), tpath.getValue());
-      }
-      updateCount++;
 
-      tablet.getTabletResources().importedMapFiles();
+        tablet.getTabletResources().importedMapFiles();
 
-      tablet.computeNumEntries();
-    }
+        tablet.computeNumEntries();
+      }
 
-    for (Entry<StoredTabletFile,DataFileValue> entry : newFiles.entrySet()) {
-      TabletLogger.bulkImported(tablet.getExtent(), entry.getKey());
+      for (Entry<StoredTabletFile,DataFileValue> entry : newFiles.entrySet()) {
+        TabletLogger.bulkImported(tablet.getExtent(), entry.getKey());
+      }
+    } finally {
+      metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementFinish());","[{'comment': '```suggestion\r\n      metadataUpdateCount.updateAndGet(MetadataUpdateCount::incrementFinish);\r\n```', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -357,35 +363,41 @@ else if (!bulkDir.equals(tpath.getTabletDir()))
       tablet.finishClearingUnusedLogs();
     }
 
-    do {
-      try {
-        // the purpose of making this update use the new commit session, instead of the old one
-        // passed in, is because the new one will reference the logs used by current memory...
+    metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementStart());","[{'comment': '```suggestion\r\n    metadataUpdateCount.updateAndGet(MetadataUpdateCount::incrementStart);\r\n```', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -357,35 +363,41 @@ else if (!bulkDir.equals(tpath.getTabletDir()))
       tablet.finishClearingUnusedLogs();
     }
 
-    do {
-      try {
-        // the purpose of making this update use the new commit session, instead of the old one
-        // passed in, is because the new one will reference the logs used by current memory...
+    metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementStart());
+    // do not place any code here between above stmt and try{}finally
+    try {
 
-        tablet.getTabletServer().minorCompactionFinished(
-            tablet.getTabletMemory().getCommitSession(), commitSession.getWALogSeq() + 2);
-        break;
-      } catch (IOException e) {
-        log.error(""Failed to write to write-ahead log "" + e.getMessage() + "" will retry"", e);
-        sleepUninterruptibly(1, TimeUnit.SECONDS);
-      }
-    } while (true);
+      do {
+        try {
+          // the purpose of making this update use the new commit session, instead of the old one
+          // passed in, is because the new one will reference the logs used by current memory...
 
-    synchronized (tablet) {
-      t1 = System.currentTimeMillis();
+          tablet.getTabletServer().minorCompactionFinished(
+              tablet.getTabletMemory().getCommitSession(), commitSession.getWALogSeq() + 2);
+          break;
+        } catch (IOException e) {
+          log.error(""Failed to write to write-ahead log "" + e.getMessage() + "" will retry"", e);
+          sleepUninterruptibly(1, TimeUnit.SECONDS);
+        }
+      } while (true);
+
+      synchronized (tablet) {
+        t1 = System.currentTimeMillis();
 
-      if (dfv.getNumEntries() > 0 && newFile.isPresent()) {
-        StoredTabletFile newFileStored = newFile.get();
-        if (datafileSizes.containsKey(newFileStored)) {
-          log.error(""Adding file that is already in set {}"", newFileStored);
+        if (dfv.getNumEntries() > 0 && newFile.isPresent()) {
+          StoredTabletFile newFileStored = newFile.get();
+          if (datafileSizes.containsKey(newFileStored)) {
+            log.error(""Adding file that is already in set {}"", newFileStored);
+          }
+          datafileSizes.put(newFileStored, dfv);
         }
-        datafileSizes.put(newFileStored, dfv);
-        updateCount++;
-      }
 
-      tablet.flushComplete(flushId);
+        tablet.flushComplete(flushId);
 
-      t2 = System.currentTimeMillis();
+        t2 = System.currentTimeMillis();
+      }
+    } finally {
+      metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementFinish());","[{'comment': '```suggestion\r\n      metadataUpdateCount.updateAndGet(MetadataUpdateCount::incrementFinish);\r\n```', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -431,49 +443,56 @@ StoredTabletFile bringMajorCompactionOnline(Set<StoredTabletFile> oldDatafiles,
 
     Long compactionIdToWrite = null;
 
-    synchronized (tablet) {
-      t1 = System.currentTimeMillis();
-
-      Preconditions.checkState(datafileSizes.keySet().containsAll(oldDatafiles),
-          ""Compacted files %s are not a subset of tablet files %s"", oldDatafiles,
-          datafileSizes.keySet());
-      if (dfv.getNumEntries() > 0) {
-        Preconditions.checkState(!datafileSizes.containsKey(newFile),
-            ""New compaction file %s already exist in tablet files %s"", newFile,
+    metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementStart());","[{'comment': '```suggestion\r\n    metadataUpdateCount.updateAndGet(MetadataUpdateCount::incrementStart);\r\n```', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -431,49 +443,56 @@ StoredTabletFile bringMajorCompactionOnline(Set<StoredTabletFile> oldDatafiles,
 
     Long compactionIdToWrite = null;
 
-    synchronized (tablet) {
-      t1 = System.currentTimeMillis();
-
-      Preconditions.checkState(datafileSizes.keySet().containsAll(oldDatafiles),
-          ""Compacted files %s are not a subset of tablet files %s"", oldDatafiles,
-          datafileSizes.keySet());
-      if (dfv.getNumEntries() > 0) {
-        Preconditions.checkState(!datafileSizes.containsKey(newFile),
-            ""New compaction file %s already exist in tablet files %s"", newFile,
+    metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementStart());
+    // do not place any code here between above stmt and try{}finally
+    try {
+
+      synchronized (tablet) {
+        t1 = System.currentTimeMillis();
+
+        Preconditions.checkState(datafileSizes.keySet().containsAll(oldDatafiles),
+            ""Compacted files %s are not a subset of tablet files %s"", oldDatafiles,
             datafileSizes.keySet());
-      }
+        if (dfv.getNumEntries() > 0) {
+          Preconditions.checkState(!datafileSizes.containsKey(newFile),
+              ""New compaction file %s already exist in tablet files %s"", newFile,
+              datafileSizes.keySet());
+        }
 
-      tablet.incrementDataSourceDeletions();
+        tablet.incrementDataSourceDeletions();
 
-      datafileSizes.keySet().removeAll(oldDatafiles);
+        datafileSizes.keySet().removeAll(oldDatafiles);
 
-      if (dfv.getNumEntries() > 0) {
-        datafileSizes.put(newFile, dfv);
-        // could be used by a follow on compaction in a multipass compaction
-      }
-      updateCount++;
+        if (dfv.getNumEntries() > 0) {
+          datafileSizes.put(newFile, dfv);
+          // could be used by a follow on compaction in a multipass compaction
+        }
 
-      tablet.computeNumEntries();
+        tablet.computeNumEntries();
 
-      lastLocation = tablet.resetLastLocation();
+        lastLocation = tablet.resetLastLocation();
 
-      if (compactionId != null && Collections.disjoint(selectedFiles, datafileSizes.keySet())) {
-        compactionIdToWrite = compactionId;
+        if (compactionId != null && Collections.disjoint(selectedFiles, datafileSizes.keySet())) {
+          compactionIdToWrite = compactionId;
+        }
+
+        t2 = System.currentTimeMillis();
       }
 
-      t2 = System.currentTimeMillis();
-    }
+      // known consistency issue between minor and major compactions - see ACCUMULO-18
+      Set<StoredTabletFile> filesInUseByScans = waitForScansToFinish(oldDatafiles);
+      if (!filesInUseByScans.isEmpty())
+        log.debug(""Adding scan refs to metadata {} {}"", extent, filesInUseByScans);
+      ManagerMetadataUtil.replaceDatafiles(tablet.getContext(), extent, oldDatafiles,
+          filesInUseByScans, newFile, compactionIdToWrite, dfv,
+          tablet.getTabletServer().getClientAddressString(), lastLocation,
+          tablet.getTabletServer().getLock(), ecid);
+      tablet.setLastCompactionID(compactionIdToWrite);
+      removeFilesAfterScan(filesInUseByScans);
 
-    // known consistency issue between minor and major compactions - see ACCUMULO-18
-    Set<StoredTabletFile> filesInUseByScans = waitForScansToFinish(oldDatafiles);
-    if (!filesInUseByScans.isEmpty())
-      log.debug(""Adding scan refs to metadata {} {}"", extent, filesInUseByScans);
-    ManagerMetadataUtil.replaceDatafiles(tablet.getContext(), extent, oldDatafiles,
-        filesInUseByScans, newFile, compactionIdToWrite, dfv,
-        tablet.getTabletServer().getClientAddressString(), lastLocation,
-        tablet.getTabletServer().getLock(), ecid);
-    tablet.setLastCompactionID(compactionIdToWrite);
-    removeFilesAfterScan(filesInUseByScans);
+    } finally {
+      metadataUpdateCount.updateAndGet(oldCount -> oldCount.incrementFinish());","[{'comment': '```suggestion\r\n      metadataUpdateCount.updateAndGet(MetadataUpdateCount::incrementFinish);\r\n```', 'commenter': 'ctubbsii'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1435,23 +1439,34 @@ private void closeConsistencyCheck() {
     }
   }
 
-  private void compareToDataInMemory(TabletMetadata tabletMetadata) {
-    if (!tabletMetadata.getFilesMap().equals(getDatafileManager().getDatafileSizes())) {
-      String msg = ""Data files in "" + extent + "" differ from in-memory data ""
-          + tabletMetadata.getFilesMap() + "" "" + getDatafileManager().getDatafileSizes();
-      log.error(msg);
-    }
-  }
-
-  public synchronized void compareTabletInfo(Long updateCounter, TabletMetadata tabletMetadata) {
+  public synchronized void compareTabletInfo(MetadataUpdateCount updateCounter,
+      TabletMetadata tabletMetadata) {","[{'comment': 'Does it matter which thread (the main thread vs during a close) is doing the check? This was one thing I was wondering while doing testing. If we add a boolean here of who is calling the check, we could bail if it is not the correct thread.', 'commenter': 'milleruntime'}, {'comment': 'It should not matter which thread is doing the check. The main thing that matters is the order in which the two parameters that are passed to the method are acquired.  The updateCounter must be acquired before tabletMetadata by the code calling this method (and they must go with the same tablet).', 'commenter': 'keith-turner'}, {'comment': 'Maybe we could add a key extent to the MetadataUpdateCount to allow more validations (make sure counts are for expected extent) in the method.', 'commenter': 'keith-turner'}, {'comment': 'I added some extra validations in e2580d9. ', 'commenter': 'keith-turner'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MetadataUpdateCount.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.tserver.tablet;
+
+import java.util.Objects;
+
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+
+public class MetadataUpdateCount {","[{'comment': 'Can you give a brief description of the purpose of this class? Could just copy other comments you made.', 'commenter': 'milleruntime'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/DatafileManager.java,"@@ -69,13 +70,16 @@
   // ensure we only have one reader/writer of our bulk file notes at at time
   private final Object bulkFileImportLock = new Object();
 
-  // This must be incremented whenever datafileSizes is mutated
-  private long updateCount;
+  // This must be incremented before and after datafileSizes and metadata table updates. These
+  // counts allow detection of overlapping operation w/o placing a lock around metadata table","[{'comment': 'I _think_ I know what you are trying to say with ""overlapping operation"" but maybe you could expand on it with an additional sentence? Or maybe reword ""overlapping operation"" to something else?', 'commenter': 'milleruntime'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1435,23 +1439,56 @@ private void closeConsistencyCheck() {
     }
   }
 
-  private void compareToDataInMemory(TabletMetadata tabletMetadata) {
-    if (!tabletMetadata.getFilesMap().equals(getDatafileManager().getDatafileSizes())) {
-      String msg = ""Data files in "" + extent + "" differ from in-memory data ""
-          + tabletMetadata.getFilesMap() + "" "" + getDatafileManager().getDatafileSizes();
-      log.error(msg);
-    }
-  }
+  /**
+   * Checks that tablet metadata from the metadata table matches what this tablet has in memory. The
+   * caller of this method must acquire the updateCounter parameter before acquiring the
+   * tabletMetadata.
+   *
+   * @param updateCounter
+   *          used to check for conucurrent updates in which case this check is a no-op. See
+   *          {@link #getUpdateCount()}
+   * @param tabletMetadata
+   *          the metadata for this tablet that was acquired from the metadata table.
+   */
+  public synchronized void compareTabletInfo(MetadataUpdateCount updateCounter,
+      TabletMetadata tabletMetadata) {
+
+    // verify the given counter is for this tablet, if this check fail it indicates a bug in the
+    // calling code
+    Preconditions.checkArgument(updateCounter.getExtent().equals(getExtent()),
+        ""Counter had unexpected extent %s != %s"", updateCounter.getExtent(), getExtent());
+
+    // verify the given tablet metadata is for this tablet, if this check fail it indicates a bug in
+    // the calling code
+    Preconditions.checkArgument(tabletMetadata.getExtent().equals(getExtent()),
+        ""Tablet metadata had unexpected extent %s != %s"", tabletMetadata.getExtent(), getExtent());
 
-  public synchronized void compareTabletInfo(Long updateCounter, TabletMetadata tabletMetadata) {
     if (isClosed() || isClosing()) {
+      log.trace(""AMCC Tablet {} was closed, so skipping check"", tabletMetadata.getExtent());
       return;
     }
-    // if the counter didn't change, compare metadata to what is in memory
-    if (updateCounter == this.getUpdateCount()) {
-      this.compareToDataInMemory(tabletMetadata);
+
+    var dataFileSizes = getDatafileManager().getDatafileSizes();
+
+    if (!tabletMetadata.getFilesMap().equals(dataFileSizes)) {
+      // The counters are modified outside of locks before and after tablet metadata operations and
+      // data file updates so its very important to aquire the 2nd counts after doing the equality","[{'comment': ""```suggestion\r\n      // data file updates so, it's very important to acquire the 2nd counts after doing the equality\r\n```"", 'commenter': 'milleruntime'}]"
2574,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1435,23 +1439,56 @@ private void closeConsistencyCheck() {
     }
   }
 
-  private void compareToDataInMemory(TabletMetadata tabletMetadata) {
-    if (!tabletMetadata.getFilesMap().equals(getDatafileManager().getDatafileSizes())) {
-      String msg = ""Data files in "" + extent + "" differ from in-memory data ""
-          + tabletMetadata.getFilesMap() + "" "" + getDatafileManager().getDatafileSizes();
-      log.error(msg);
-    }
-  }
+  /**
+   * Checks that tablet metadata from the metadata table matches what this tablet has in memory. The
+   * caller of this method must acquire the updateCounter parameter before acquiring the
+   * tabletMetadata.
+   *
+   * @param updateCounter
+   *          used to check for conucurrent updates in which case this check is a no-op. See
+   *          {@link #getUpdateCount()}
+   * @param tabletMetadata
+   *          the metadata for this tablet that was acquired from the metadata table.
+   */
+  public synchronized void compareTabletInfo(MetadataUpdateCount updateCounter,
+      TabletMetadata tabletMetadata) {
+
+    // verify the given counter is for this tablet, if this check fail it indicates a bug in the
+    // calling code
+    Preconditions.checkArgument(updateCounter.getExtent().equals(getExtent()),
+        ""Counter had unexpected extent %s != %s"", updateCounter.getExtent(), getExtent());
+
+    // verify the given tablet metadata is for this tablet, if this check fail it indicates a bug in
+    // the calling code
+    Preconditions.checkArgument(tabletMetadata.getExtent().equals(getExtent()),
+        ""Tablet metadata had unexpected extent %s != %s"", tabletMetadata.getExtent(), getExtent());
 
-  public synchronized void compareTabletInfo(Long updateCounter, TabletMetadata tabletMetadata) {
     if (isClosed() || isClosing()) {
+      log.trace(""AMCC Tablet {} was closed, so skipping check"", tabletMetadata.getExtent());
       return;
     }
-    // if the counter didn't change, compare metadata to what is in memory
-    if (updateCounter == this.getUpdateCount()) {
-      this.compareToDataInMemory(tabletMetadata);
+
+    var dataFileSizes = getDatafileManager().getDatafileSizes();
+
+    if (!tabletMetadata.getFilesMap().equals(dataFileSizes)) {
+      // The counters are modified outside of locks before and after tablet metadata operations and
+      // data file updates so its very important to aquire the 2nd counts after doing the equality
+      // check above. If the counts are the same (as the ones acquired before reading metadata
+      // table) after the equality check above then we know the tablet did not do any updates while
+      // we were reading metadata and then comparing.
+      var latestCount = this.getUpdateCount();
+      if (updateCounter.overlapsUpdate() || !updateCounter.equals(latestCount)) {
+        log.trace(
+            ""AMCC Tablet {} may have been updating its metadata while it was being read for check, so skipping check {} {}"",
+            tabletMetadata.getExtent(), updateCounter, latestCount);
+      } else {
+        log.error(""AMCC Data files in {} differ from in-memory data {} {} {} {}"", extent,
+            tabletMetadata.getFilesMap(), dataFileSizes, updateCounter, latestCount);","[{'comment': '```suggestion\r\n        log.error(\r\n            ""AMCC Data files in {} differ from in-memory data {}. dataFileSizes: {} updateCounter: {} latestCount: {}"",\r\n            extent, tabletMetadata.getFilesMap(), dataFileSizes, updateCounter, latestCount);\r\n```', 'commenter': 'milleruntime'}, {'comment': 'I am not sure what ""AMCC"" stands for but I guess it\'s OK to add uniqueness to the log statement. I just don\'t want to confuse users.', 'commenter': 'milleruntime'}, {'comment': 'AMCC was for Accumulo Metadata Consistency Check.  I can add a comment for that.  Do you think it should be removed?  I added because I like to be able to easily grep for all related messages.', 'commenter': 'keith-turner'}]"
2580,hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/its/mapreduce/AccumuloFileOutputFormatIT.java,"@@ -72,8 +73,8 @@
           .addOption(""modulus"", ""3"");
 
   @Override
-  protected int defaultTimeoutSeconds() {
-    return 4 * 60;
+  protected Duration defaultTimeoutDuration() {","[{'comment': ""If the method is going to be renamed anyway, `defaultTimeout` is sufficient. The type doesn't need to be included. It was only ever included before because we needed to know that it was seconds, and not, for example, millis."", 'commenter': 'ctubbsii'}, {'comment': 'Good idea. Addressed in [546a85d](https://github.com/apache/accumulo/pull/2580/commits/546a85dd508cfb0f2508d8372ed10df843bd6e52)', 'commenter': 'DomGarguilo'}]"
2580,test/src/main/java/org/apache/accumulo/harness/AccumuloITBase.java,"@@ -111,19 +111,18 @@ public static File createTestDir(String name) {
     }
 
     // if either value is zero, apply a very long timeout (effectively no timeout)
-    int totalTimeoutSeconds = timeoutFactor * defaultTimeoutSeconds();
-    if (totalTimeoutSeconds == 0) {
+    if (timeoutFactor == 0 || defaultTimeoutDuration().isZero()) {","[{'comment': ""If the the `defaultTimeoutDuration()` is 0, it should really be an error. We don't want infinite timeouts by default, only if the user explicitly set it with `-Dtimeout.factor=0`."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in [c03f65e](https://github.com/apache/accumulo/pull/2580/commits/c03f65e720d08abd1341872295d7faaa241d3b93)', 'commenter': 'DomGarguilo'}, {'comment': ""> We don't want infinite timeouts by default, only if the user explicitly set it with `-Dtimeout.factor=0`.\r\n\r\nI don't think the timeout will be infinite by default. I think the only cases where the infinite timeout would be applied (before [c03f65e](https://github.com/apache/accumulo/pull/2580/commits/c03f65e720d08abd1341872295d7faaa241d3b93)) would be if the user explicitly set it with `-Dtimeout.factor=0`, or if `defaultTimeout()` was explicitly overridden to return a duration of zero."", 'commenter': 'DomGarguilo'}, {'comment': ""> or if `defaultTimeout()` was explicitly overridden to return a duration of zero.\r\n\r\nRight. That's the case I'm talking about. This shouldn't happen. This should be an error. The only time it should be infinite is if the user says so explicitly with `-Dtimeout.factor=0`. None of our tests should ever have that as a their overridden default."", 'commenter': 'ctubbsii'}, {'comment': ""Your assertion check will cause the case I'm concerned about to fail, so this looks good to me now."", 'commenter': 'ctubbsii'}]"
2584,iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/junit5/BaseJUnit5IteratorTest.java,"@@ -0,0 +1,110 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.iteratortest.junit5;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertNotNull;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.iteratortest.IteratorTestInput;
+import org.apache.accumulo.iteratortest.IteratorTestOutput;
+import org.apache.accumulo.iteratortest.IteratorTestReport;
+import org.apache.accumulo.iteratortest.IteratorTestRunner;
+import org.apache.accumulo.iteratortest.testcases.IteratorTestCase;
+import org.junit.jupiter.api.TestInstance;
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.Arguments;
+import org.junit.jupiter.params.provider.MethodSource;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * A base JUnit5 test class for users to leverage with the JUnit ParameterizedTest.
+ * <p>
+ * Users should extend this class and override the parameters() method to provide a list of
+ * Arguments, each of which serve as the parameters for an iterator test case.
+ *
+ * <pre>
+ * public static List&lt;Arguments&gt; parameters() {
+ *   IteratorTestInput input = createIteratorInput();
+ *   IteratorTestOutput expectedOutput = createIteratorOutput();
+ *   List&lt;IteratorTestCase&gt; testCases = createTestCases();
+ *   return BaseJUnit5IteratorTest.createParameters(input, expectedOutput, testCases);
+ * }
+ * </pre>
+ */
+@TestInstance(TestInstance.Lifecycle.PER_CLASS)
+public abstract class BaseJUnit5IteratorTest {","[{'comment': 'This isn\'t strictly ""public API"", according to our API definition, but it was intended to be something that users can test their iterators with. This change should be mentioned in the release notes, since the parameter types have changed in the abstract method going from JUnit4 to JUnit5.\r\n\r\nIdeally, we\'d try to make our test framework agnostic to the underlying JUnit version.', 'commenter': 'ctubbsii'}, {'comment': '> This change should be mentioned in the release notes, since the parameter types have changed in the abstract method going from JUnit4 to JUnit5.\r\n\r\nIt is possible to keep the parameter types the same. I changed it to `List<Arguments>` because that seems more intuitive to me. If it seems better to leave it as `Object[][]` I can go ahead and revert it back to that.', 'commenter': 'DomGarguilo'}, {'comment': ""I agree the List version is better... I don't think we want to keep the 2D array. I'm pretty sure it was only that way because it had to be. I think it should be fine to document the change in parameter types in the release notes. The file is going to be renamed either way, and should be documented.\r\n\r\nI'm just thinking that maybe we can have our own types so it's not exposing the JUnit5 type, `Arguments`, but a type of our own that is analogous and we can easily translate into the JUnit5 type (or some other type if we move off of JUnit5 at some future point). The `Object[][]` type, while terrible, did have the advantage of not being strictly tied to JUnit4 or anything else... but perhaps we can do better?"", 'commenter': 'ctubbsii'}, {'comment': 'I tried creating an object, `IteratorTestParameters`, that wraps the input, output and test case for an individual test. This allows us to pass a list of `IteratorTestParameters` to the parameterized test rather than a list of `Arguments`(eliminating the reliance on JUnit specific types). Does this seem like it could be a good approach?', 'commenter': 'DomGarguilo'}, {'comment': 'I implemented the above idea in [590bd7c](https://github.com/apache/accumulo/pull/2584/commits/590bd7c49aebcf051f61d7006650d15368b547c4)', 'commenter': 'DomGarguilo'}]"
2584,test/src/test/java/org/apache/accumulo/test/iterator/SummingCombinerTest.java,"@@ -38,21 +38,21 @@
 import org.apache.accumulo.iteratortest.IteratorTestInput;
 import org.apache.accumulo.iteratortest.IteratorTestOutput;
 import org.apache.accumulo.iteratortest.environments.SimpleIteratorEnvironment;
-import org.apache.accumulo.iteratortest.junit4.BaseJUnit4IteratorTest;
+import org.apache.accumulo.iteratortest.junit5.BaseJUnit5IteratorTest;
 import org.apache.accumulo.iteratortest.testcases.IteratorTestCase;
-import org.junit.runners.Parameterized.Parameters;
+import org.junit.jupiter.params.provider.Arguments;
 
 /**
  * Iterator test harness tests for SummingCombiner
  */
-public class SummingCombinerTest extends BaseJUnit4IteratorTest {
+public class SummingCombinerTest extends BaseJUnit5IteratorTest {
 
-  @Parameters
-  public static Object[][] parameters() {
+  @Override
+  protected List<Arguments> parameters() {
     IteratorTestInput input = getIteratorInput();
     IteratorTestOutput output = getIteratorOutput();
     List<IteratorTestCase> tests = IteratorTestCaseFinder.findAllTestCases();
-    return BaseJUnit4IteratorTest.createParameters(input, output, tests);
+    return BaseJUnit5IteratorTest.createParameters(input, output, tests);
   }","[{'comment': 'Every test seems to do the same thing here. Maybe this can be cleaned up?', 'commenter': 'ctubbsii'}, {'comment': 'There are a few outliers, but most of them are the exact same. Did you have anything specific in mind for how this could be cleaned up? \r\n\r\nOne thing I can think of is remove the `parameters()` method and instead have the extending classes override `getIteratorInput()` `getIteratorOutput()` and maybe `getTests()` moving this duplicate logic into `BaseJUnit5IteratorTest`.', 'commenter': 'DomGarguilo'}, {'comment': 'Not sure if its much cleaner but I implemented the idea above in [0bb2170](https://github.com/apache/accumulo/pull/2584/commits/0bb2170fbe3d71ba1227136d4689ec0151b61ac2)', 'commenter': 'DomGarguilo'}]"
2584,test/src/test/java/org/apache/accumulo/test/iterator/WholeRowIteratorTest.java,"@@ -32,27 +32,33 @@
 import org.apache.accumulo.iteratortest.IteratorTestCaseFinder;
 import org.apache.accumulo.iteratortest.IteratorTestInput;
 import org.apache.accumulo.iteratortest.IteratorTestOutput;
-import org.apache.accumulo.iteratortest.junit4.BaseJUnit4IteratorTest;
+import org.apache.accumulo.iteratortest.junit5.BaseJUnit5IteratorTest;
 import org.apache.accumulo.iteratortest.testcases.IteratorTestCase;
 import org.apache.hadoop.io.Text;
-import org.junit.runners.Parameterized.Parameters;
 
 /**
  * Framework tests for {@link WholeRowIterator}.
  */
-public class WholeRowIteratorTest extends BaseJUnit4IteratorTest {
-
-  @Parameters
-  public static Object[][] parameters() {
-    IteratorTestInput input = getIteratorInput();
-    IteratorTestOutput output = getIteratorOutput();
-    List<IteratorTestCase> tests = IteratorTestCaseFinder.findAllTestCases();
-    return BaseJUnit4IteratorTest.createParameters(input, output, tests);
-  }","[{'comment': ""Okay, so here's the main problem with this PR: the previous code expected input of the form of a 2D array that looked like:\r\n\r\n```\r\ninput1    output1    testcase1\r\ninput2    output2    testcase2\r\ninput3    output3    testcase3\r\n```\r\n\r\nIt provided a convenience method to locate test cases, and automatically create the 2D array with the same input and output parameters for every test case:\r\n\r\n```\r\ninput1    output1    testcase1\r\ninput1    output1    testcase2\r\ninput1    output1    testcase3\r\n```\r\n\r\nPerhaps our tests only ever used a single input and a single output, and ran every test case with those... so it's not obvious that this PR breaks the use case of having different inputs and outputs for different test cases.\r\n\r\nThe new code in this PR changes things, so that there's a single `getInteratorInput()` method and a single `getIteratorOutput()` method. However, that's too constraining, and eliminates the possibility of having different inputs for different test cases.\r\n\r\nWith these changes, it's still possible to run different inputs and outputs, but it would have to exist in an entirely separate test class (or separate inner classes, each extending the base test class). But, that shouldn't be necessary."", 'commenter': 'ctubbsii'}]"
2593,server/manager/src/main/java/org/apache/accumulo/manager/ManagerTime.java,"@@ -64,6 +64,8 @@ public ManagerTime(Manager manager, AccumuloConfiguration conf) throws IOExcepti
       throw new IOException(""Error updating manager time"", ex);
     }
 
+    // Previous versions of this class did not use a shared thread pool, it created its own
+    // Timer instance.","[{'comment': ""I'm not sure what purpose this comment serves. It's documenting a previous version of the code. It doesn't apply to the code here. If it is needed and kept, it should be reworded to describe the current code and the reason for the current implementation, not reference a previous version that no longer applies."", 'commenter': 'ctubbsii'}, {'comment': 'Comment removed in e71efba', 'commenter': 'dlmarion'}]"
2603,assemble/bin/accumulo-cluster,"@@ -129,21 +147,9 @@ function control_service() {
     [[ ""$service"" == ""compactor"" ]] && ACCUMULO_SERVICE_INSTANCE=""${inst_id}_${5}""
 
     if [[ $host == localhost || $host == ""$(hostname -s)"" || $host == ""$(hostname -f)"" || $host == $(get_ip) ]] ; then
-      #
-      # The server processes take arguments (e.g. -a, -p, -o, -q [in the case of the Compactor]).
-      #
-      if [[ $# -gt 3 ]]; then
-        ACCUMULO_SERVICE_INSTANCE=""${ACCUMULO_SERVICE_INSTANCE}"" ""${bin}/accumulo-service"" ""$service"" ""$control_cmd"" ""${@:4}""
-      else
-        ACCUMULO_SERVICE_INSTANCE=""${ACCUMULO_SERVICE_INSTANCE}"" ""${bin}/accumulo-service"" ""$service"" ""$control_cmd""
-      fi
+      ACCUMULO_SERVICE_INSTANCE=""${ACCUMULO_SERVICE_INSTANCE}"" ""${bin}/accumulo-service"" ""$service"" ""$control_cmd"" ""$EXTRA_ARGS""","[{'comment': 'This won\'t work as intended. `EXTRA_ARGS` is scalar here, so the quoting will prevent multiple extra args from being recognized as separate arguments. It previously would have worked, because `@` is an array, so quoting that is equivalent to it being replaced by each of its elements being individually quoted. This is a bash quirk regarding arrays.\r\n\r\nIf you want this to work the way it did before, you have to make `EXTRA_ARGS` an array, and quote it like `""${EXTRA_ARGS[@]}""`. But, it is used as a scalar in other places, so you\'d need to change those to be `""${EXTRA_ARGS[*]}""`. I think keeping it simple and avoiding the array syntax was a goal, and why it was written the way it was before.', 'commenter': 'ctubbsii'}]"
2603,assemble/bin/accumulo-cluster,"@@ -119,6 +119,24 @@ function control_service() {
   host=""$2""
   service=""$3""
 
+  #
+  # The server processes take arguments (e.g. -a, -p, -o, -q [in the case of the Compactor]).
+  #
+  if [[ $# -gt 3 ]]; then
+    EXTRA_ARGS=""${@:4}""
+  else
+    EXTRA_ARGS=""""
+  fi
+
+  #
+  # If the host is an IP address, then pass the IP address in the -a argument to the server. If
+  # this is not done, then the default IP address if 0.0.0.0 is assigned and the host name is used
+  # in the ZooKeeper address (which may not be resolvable in some systems)
+  #
+  if [[ $host =~ ^[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
+    EXTRA_ARGS=""-a $host $EXTRA_ARGS""","[{'comment': 'Instead of reusing the `EXTRA_ARGS` mechanism, it would probably be better to just add a new variable for this. At the very least, it would make this diff more intuitive and easier to review.', 'commenter': 'ctubbsii'}]"
2603,assemble/bin/accumulo-cluster,"@@ -119,6 +119,24 @@ function control_service() {
   host=""$2""
   service=""$3""
 
+  #
+  # The server processes take arguments (e.g. -a, -p, -o, -q [in the case of the Compactor]).
+  #
+  if [[ $# -gt 3 ]]; then
+    EXTRA_ARGS=""${@:4}""
+  else
+    EXTRA_ARGS=""""
+  fi
+
+  #
+  # If the host is an IP address, then pass the IP address in the -a argument to the server. If
+  # this is not done, then the default IP address if 0.0.0.0 is assigned and the host name is used","[{'comment': 'This is a strange condition on which to decide whether to pass the `-a` or not. `-a` should work regardless of whether it is a hostname or an IP address. This is a longstanding issue with the way we\'ve done our scripts. `-a` is overloaded to mean both ""bind address"" and ""service address to advertise"". The check whether it\'s an IP address seems to imply that `-a` is only used as a bind address. However, a hostname can also refer to a specific NIC, and thus be used for determining which NIC to bind to, and a user may need to specify a hostname to use as the advertisement address, because the IP address it binds to needs to be a local address.\r\n\r\nLong-term, we really need to have separate configs for these, and they don\'t need to be command-line options only, but can actually be configuration properties set in the `accumulo.properties` file.\r\n\r\nShort-term, maybe this should always pass the `-a` option, regardless of whether this is an IP address or a hostname?', 'commenter': 'ctubbsii'}, {'comment': ""> maybe this should always pass the -a option\r\n\r\nI'm good with this"", 'commenter': 'dlmarion'}]"
2608,start/src/test/java/org/apache/accumulo/start/classloader/vfs/AccumuloVFSClassLoaderTest.java,"@@ -19,21 +19,22 @@
 package org.apache.accumulo.start.classloader.vfs;
 
 import static java.nio.charset.StandardCharsets.UTF_8;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
 import java.io.File;
 import java.io.FileWriter;
 import java.net.URLClassLoader;
+import java.util.Objects;
 
 import org.apache.accumulo.start.classloader.AccumuloClassLoader;
 import org.apache.commons.io.FileUtils;
 import org.apache.commons.vfs2.FileSystemManager;
 import org.apache.commons.vfs2.impl.VFSClassLoader;
-import org.junit.Rule;
+// import org.junit.jupiter.api.Test;
 import org.junit.Test;
-import org.junit.rules.TemporaryFolder;
+import org.junit.jupiter.api.io.TempDir;","[{'comment': ""This test can't use Jupiter APIs, because it uses PowerMock. It needs to stick to the vintage APIs, which we might need to keep in the pom.xml alongside the jupiter API."", 'commenter': 'ctubbsii'}]"
2608,start/src/test/java/org/apache/accumulo/start/classloader/vfs/ContextManagerTest.java,"@@ -61,16 +59,15 @@ static FileSystemManager getVFS() {
     }
   }
 
-  @Before
+  @BeforeEach
   public void setup() throws Exception {
-
     vfs = getVFS();
-    folder1 = tempFolder.newFolder();
-    folder2 = tempFolder.newFolder();
+    folder1 = new File(tempFolder, ""folder1"");
+    folder2 = new File(tempFolder, ""folder2"");","[{'comment': 'It looks like the tests pass fine like this but it might be a good idea to create a unique sub-directory within the `private static File tempFolder` for each test like you did in AccumuloReloadingVFSClassLoaderTest with `folder1` and `tmpDir` and then create these folders within that sub-directory.', 'commenter': 'DomGarguilo'}, {'comment': ""The current code used these folder names. It's probably best to just leave them as-is, since everything still works as expected, and such a change would be unrelated to the actual conversion to the jupiter engine."", 'commenter': 'ctubbsii'}, {'comment': 'The current way that these folders are created is different from before these changes were made. Before, a Junit4 TemporaryFolder was created for each test method and folder1 and folder2 were created inside that per-test-method directory giving them a unique path. Now, with the static TempDir, a single folder is created for all test methods and each test method will create these folders with the same path which may cause problems in certain situations. I guess its fine for now since these tests pass. I was just thinking that it could potentially cause issues in the future if more test methods are added that use these folders. ', 'commenter': 'DomGarguilo'}]"
2612,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -1014,11 +1014,14 @@ public ZooReader getZooReader() {
     return zooReader;
   }
 
+  protected long getTransportPoolMaxAgeMillis() {
+    return 3000; // 3 seconds for clients
+  }","[{'comment': 'I think this is great. In fact, if we can make it configurable via a client property (ClientProperty.GENERAL_RPC_TIMEOUT ?), then we can likely close [ACCUMULO-2069](https://issues.apache.org/jira/browse/ACCUMULO-2069). I remember when I submitted that issue that I did some testing and the overhead of closing and recreating connections was significant.', 'commenter': 'dlmarion'}, {'comment': ""I can definitely change this so it uses the normal `Property.GENERAL_RPC_TIMEOUT`. I didn't do it before because we have a default value of 2 minutes for our general RPC timeouts, and that seemed to be very far from the 3 second hard-coded value previously used. So, I went with a change that left the current value in place.\r\n\r\nHowever, I think it's probably okay to increase it to 2 minutes by default, after reading the comment on [ACCUMULO-2069 (comment)](https://issues.apache.org/jira/browse/ACCUMULO-2069?focusedCommentId=13861091&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13861091). I think the value of 3 seconds was probably used so clients were more responsive in the absence of a close method. However, that is no longer necessary, since there is a proper close method on clients now, and all the transports get closed when the client is closed. So, it's okay that they are left around longer."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 410bd68ce668287f4a5f18d313318bdf899e5927', 'commenter': 'ctubbsii'}]"
2622,core/src/main/java/org/apache/accumulo/core/clientImpl/ServerClient.java,"@@ -88,6 +88,9 @@ public static <T> T executeRaw(ClientContext context,
   public static <CT extends TServiceClient,RT> RT executeRaw(ClientContext context,
       TServiceClientFactory<CT> factory, ClientExecReturn<RT,CT> exec) throws Exception {
     while (true) {
+      if (Thread.currentThread().isInterrupted()) {
+        throw new AccumuloException(""Thread interrupted"");","[{'comment': 'You could improve the `AccumuloException` by including more information and moving it down into the try/catch. You could include the `server` or there is a bunch of stuff in `ClientContext`. ', 'commenter': 'milleruntime'}]"
2622,core/src/main/java/org/apache/accumulo/core/clientImpl/ServerClient.java,"@@ -88,6 +88,9 @@ public static <T> T executeRaw(ClientContext context,
   public static <CT extends TServiceClient,RT> RT executeRaw(ClientContext context,
       TServiceClientFactory<CT> factory, ClientExecReturn<RT,CT> exec) throws Exception {
     while (true) {
+      if (Thread.currentThread().isInterrupted()) {","[{'comment': ""I made a comment with an alternate solution, but I think you want to call `Thread.interrupted` here as the Threads' interrupted state will be reset (to false) if it's true. https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Thread.html#interrupted()"", 'commenter': 'dlmarion'}]"
2622,core/src/main/java/org/apache/accumulo/core/rpc/TTimeoutTransport.java,"@@ -18,14 +18,11 @@
  */
 package org.apache.accumulo.core.rpc;
 
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
+import java.io.*;","[{'comment': 'Our style does not permit use of wildcard imports.\r\nPlease use individual imports.', 'commenter': 'ctubbsii'}, {'comment': 'Ah, my editor must have changed this. Will fix', 'commenter': 'nikita-sirohi'}]"
2622,core/src/main/java/org/apache/accumulo/core/rpc/TTimeoutTransport.java,"@@ -87,6 +84,12 @@ TTransport createInternal(SocketAddress addr, long timeoutMillis) throws TTransp
       socket = openSocket(addr, (int) timeoutMillis);
     } catch (IOException e) {
       // openSocket handles closing the Socket on error
+      if (e instanceof AsynchronousCloseException) {
+        if (Thread.currentThread().isInterrupted()) {
+          Thread.currentThread().interrupt();
+          throw new UncheckedIOException(e);
+        }
+      }","[{'comment': ""I think this strategy is okay, but I don't think we actually want to do this for other AsynchronousCloseExceptions. I'm not sure what exactly can cause those, but I imagine if those happen, they would happen because of very special edge cases we'd probably want to explicitly handle.\r\n\r\nFor this, I think it's sufficient to just check ClosedByInterruptException. The above code sets the interrupted state flag before it throws UncheckedIOException. I don't think that's necessary, since the flag will have already been set. Even if it happens that it had been cleared, I don't think it matters, because the intent is to let the UncheckedIOException propagate all the way up and terminate the thread. It doesn't really matter if the flag is true or false when the thread terminates this way.\r\n\r\nSo, more succinctly, I think these blocks can be written as:\r\n\r\n```suggestion\r\n      if (e instanceof ClosedByInterruptException) {\r\n        throw new UncheckedIOException(e);\r\n      }\r\n```"", 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii  and I talked about this issue after I put up my most recent comment. Apologies for not updating the PR. He suggested a minor tweak to make the fix more targeted to the original issue.\r\n\r\n```suggestion\r\n      if (e instanceof ClosedByInterruptException) {\r\n        Thread.currentThread().interrupt();\r\n        throw new UncheckedIOException(e);\r\n      }\r\n```', 'commenter': 'dlmarion'}, {'comment': 'Ok, makes sense. Will update', 'commenter': 'nikita-sirohi'}]"
2622,server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java,"@@ -527,6 +529,12 @@ public static ServerAddress createSaslThreadPoolServer(HostAndPort address, TPro
       serverUser = UserGroupInformation.getLoginUser();
     } catch (IOException e) {
       transport.close();","[{'comment': 'Can this close trigger its own IOException?', 'commenter': 'ctubbsii'}, {'comment': ""It's enclosed in a try, and said exception would just be logged."", 'commenter': 'nikita-sirohi'}]"
2622,core/src/main/java/org/apache/accumulo/core/rpc/TTimeoutTransport.java,"@@ -100,6 +103,12 @@ TTransport createInternal(SocketAddress addr, long timeoutMillis) throws TTransp
       return new TIOStreamTransport(input, output);
     } catch (IOException e) {
       closeSocket(socket, e);","[{'comment': 'Can this closeSocket trigger its own IOException?', 'commenter': 'ctubbsii'}, {'comment': 'Yes, but it\'ll then be caught and logged, not re thrown. Like so:\r\n\r\n```java\r\n  private void closeSocket(Socket socket, Exception e) {\r\n    try {\r\n      if (socket != null)\r\n        socket.close();\r\n    } catch (IOException ioe) {\r\n      log.error(""Failed to close socket after unsuccessful I/O stream setup"", e);\r\n    }\r\n  }\r\n```', 'commenter': 'nikita-sirohi'}, {'comment': ""Uh... that snippet does not look nice... not sure how to format that nicely: but it's in TTimeoutTransport.java"", 'commenter': 'nikita-sirohi'}, {'comment': '@nikita-sirohi I formatted your code snippet. See https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks#syntax-highlighting', 'commenter': 'ctubbsii'}]"
2646,test/src/main/java/org/apache/accumulo/test/functional/RowDeleteIT.java,"@@ -105,8 +106,7 @@ public void run() throws Exception {
       }
 
       try (Scanner scanner = c.createScanner(tableName, Authorizations.EMPTY)) {
-        count = Iterators.size(scanner.iterator());
-        assertEquals(0, count, ""count == "" + count);
+        assertTrue(scanner.stream().findAny().isEmpty());
         bw.close();","[{'comment': ""Your change looks fine, but it looks like there's something weird with the existing `bw.close()`. It won't be closed if the assertion fails. The BatchWriter should be in a try-with-resources block of its own, wrapping this scanner block and other stuff above here. That way, it's always closed."", 'commenter': 'ctubbsii'}]"
2646,test/src/main/java/org/apache/accumulo/test/mapred/AccumuloOutputFormatIT.java,"@@ -233,11 +232,9 @@ public void testMR() throws Exception {
       assertNull(e1);
 
       try (Scanner scanner = c.createScanner(table2, new Authorizations())) {
-        Iterator<Entry<Key,Value>> iter = scanner.iterator();
-        assertTrue(iter.hasNext());
-        Entry<Key,Value> entry = iter.next();
-        assertEquals(Integer.parseInt(new String(entry.getValue().get())), 100);
-        assertFalse(iter.hasNext());
+        int actual = scanner.stream().map(Entry::getValue).map(Value::get).map(String::new)
+            .map(Integer::parseInt).collect(onlyElement());
+        assertEquals(actual, 100);","[{'comment': 'Expected should come first for the failure messages to be correct (saw this in several places throughout this PR).\r\n\r\n```suggestion\r\n        assertEquals(100, actual);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Good catch, all should be addressed in cf5fb07', 'commenter': 'DomGarguilo'}]"
2646,test/src/main/java/org/apache/accumulo/test/mapreduce/AccumuloOutputFormatIT.java,"@@ -164,11 +162,9 @@ public void testMR() throws Exception {
       assertNull(e1);
 
       try (Scanner scanner = c.createScanner(table2, new Authorizations())) {
-        Iterator<Entry<Key,Value>> iter = scanner.iterator();
-        assertTrue(iter.hasNext());
-        Entry<Key,Value> entry = iter.next();
-        assertEquals(Integer.parseInt(new String(entry.getValue().get())), 100);
-        assertFalse(iter.hasNext());
+        int actual = scanner.stream().map(Entry::getValue).map(Value::get).map(String::new)
+            .map(Integer::parseInt).collect(onlyElement());","[{'comment': ""It's slightly shorter if you just call parseInt on the only element (some suggestion applies elsewhere in this PR):\r\n\r\n```suggestion\r\n        int actual = Integer.parseInt(scanner.stream().map(Entry::getValue).map(Value::get).map(String::new).collect(onlyElement()));\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Personally, it seems to be a bit more intuitive or readable, when using multiple `.map()` operations, to just keep with that pattern and do the `parseInt` within a `.map()` as well, even if it is slightly longer.', 'commenter': 'DomGarguilo'}, {'comment': ""This reminds me of my comment in the description of this PR:\r\n> I also wanted to see if anyone had an opinion on which of the following equivalent options is better:\r\n> \r\n>     1. `scanner.stream().map(Entry::getValue).map(Value::get).map(String::new).map(Integer::parseInt).collect(onlyElement());`\r\n> \r\n>     2. `scanner.stream().map(entry -> Integer.parseInt(new String(entry.getValue().get()))).collect(onlyElement());`\r\n\r\nI'm starting to think I prefer option 1 over option 2. It seems easier to read and keep track of what is happening especially with all the nested parentheses in option 2."", 'commenter': 'DomGarguilo'}, {'comment': ""I've used both. I prefer option 1, if all things are equal, but use option 2 on a case-by-case basis if it's shorter, more clear, or have reason to think it might perform better."", 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/SecurityOperationsImpl.java,"@@ -227,8 +260,14 @@ public void grantTablePermission(final String principal, final String table,
     checkArgument(permission != null, ""permission is null"");
 
     try {
-      executeVoid(client -> client.grantTablePermission(TraceUtil.traceInfo(), context.rpcCreds(),
-          principal, table, permission.getId()));
+      exec(new ThriftClientTypes.ThriftClientType.Exec<Void,ClientService.Client>() {
+        @Override
+        public Void execute(Client client) throws Exception {
+          client.grantTablePermission(TraceUtil.traceInfo(), context.rpcCreds(), principal, table,
+              permission.getId());
+          return null;
+        }
+      });","[{'comment': ""Do these all have to be anonymous inner classes for some reason? Why don't lambdas work here anymore?"", 'commenter': 'ctubbsii'}, {'comment': 'This has been resolved with the latest changes.', 'commenter': 'dlmarion'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/ConnectorImpl.java,"@@ -66,10 +67,11 @@ public ConnectorImpl(ClientContext context) throws AccumuloSecurityException, Ac
     // server jar
     final String tokenClassName = context.getCredentials().getToken().getClass().getName();
     if (!SYSTEM_TOKEN_NAME.equals(tokenClassName)) {
-      ServerClient.executeVoid(context, iface -> {
-        if (!iface.authenticate(TraceUtil.traceInfo(), context.rpcCreds()))
+      ThriftClientTypes.CLIENT.executeOnTServer(context, client -> {","[{'comment': 'Is it too much bloat to add another method for void operations?', 'commenter': 'ctubbsii'}, {'comment': 'executeVoid method added in the latest changes.', 'commenter': 'dlmarion'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java,"@@ -120,15 +124,19 @@ private void checkLocalityGroups(String propChanged)
   @Override
   public Map<String,String> getSystemConfiguration()
       throws AccumuloException, AccumuloSecurityException {
-    return ServerClient.execute(context, client -> client.getConfiguration(TraceUtil.traceInfo(),
-        context.rpcCreds(), ConfigurationType.CURRENT));
+    return ThriftClientTypes.CLIENT.executeOnTServer(context, client -> {
+      return client.getConfiguration(TraceUtil.traceInfo(), context.rpcCreds(),
+          ConfigurationType.CURRENT);
+    });","[{'comment': 'Some of these added the extra curly braces, which requires use of the explicit `return` keyword and trailing `;` -- is that done for formatting? Or can we go with the simpler lambda syntax with the implicit `return` and no `;` ?', 'commenter': 'ctubbsii'}, {'comment': 'It appears that the explicit `return`, `{}`, trailing `;` can be removed when it can determine the return type. For example, the method that you reference above becomes: \r\n```\r\n  @Override\r\n  public Map<String,String> getSystemConfiguration()\r\n      throws AccumuloException, AccumuloSecurityException {\r\n    return ThriftClientTypes.CLIENT.executeOnTServer(context, client ->\r\n      client.getConfiguration(TraceUtil.traceInfo(), context.rpcCreds(),\r\n          ConfigurationType.CURRENT));\r\n  }\r\n```\r\n\r\nI didn\'t realize that there was an implicit return. I\'ll see what I can do to consolidate this even further.\r\n\r\nHowever, there is another method in this class where removing these things does not work as it cannot determine the type for `R` in `ThriftClientType.executeAdminOnManager` :\r\n```\r\n  @Override\r\n  public void removeProperty(final String property)\r\n      throws AccumuloException, AccumuloSecurityException {\r\n    checkArgument(property != null, ""property is null"");\r\n    DeprecatedPropertyUtil.getReplacementName(property, (log, replacement) -> {\r\n      // force a warning on the client side, but send the name the user used to the server-side\r\n      // to trigger a warning in the server logs, and to handle it there\r\n      log.warn(""{} was deprecated and will be removed in a future release; assuming user meant""\r\n          + "" its replacement {} and will remove that instead"", property, replacement);\r\n    });\r\n    ThriftClientTypes.MANAGER.executeAdminOnManager(context, client -> {\r\n      client.removeSystemProperty(TraceUtil.traceInfo(), context.rpcCreds(), property);\r\n      return null;\r\n    });\r\n    checkLocalityGroups(property);\r\n  }\r\n```', 'commenter': 'dlmarion'}, {'comment': 'This has been resolved with the latest changes.', 'commenter': 'dlmarion'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsImpl.java,"@@ -180,9 +180,17 @@ public void setProperty(final String namespace, final String property, final Str
     checkArgument(property != null, ""property is null"");
     checkArgument(value != null, ""value is null"");
 
-    ManagerClient.executeNamespace(context,
-        client -> client.setNamespaceProperty(TraceUtil.traceInfo(), context.rpcCreds(), namespace,
-            property, value));
+    try {
+      ThriftClientTypes.MANAGER.executeOnManager(context, client -> {
+        client.setNamespaceProperty(TraceUtil.traceInfo(), context.rpcCreds(), namespace, property,
+            value);
+        return null;
+      });
+    } catch (TableNotFoundException e) {
+      if (e.getCause() instanceof NamespaceNotFoundException)
+        throw (NamespaceNotFoundException) e.getCause();
+    }","[{'comment': ""I don't think it's possible for these namespace operations to throw TableNotFoundException. Are these getting caught here because TNFE is declared as thrown in the `executeOnManager` method?"", 'commenter': 'ctubbsii'}, {'comment': 'Yes, because I reduced the number of methods and just handled the exceptional case here. The old code in `ManagerClient.executeNamespace` caught the TNFE and performed this logic.', 'commenter': 'dlmarion'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsImpl.java,"@@ -180,9 +180,17 @@ public void setProperty(final String namespace, final String property, final Str
     checkArgument(property != null, ""property is null"");
     checkArgument(value != null, ""value is null"");
 
-    ManagerClient.executeNamespace(context,
-        client -> client.setNamespaceProperty(TraceUtil.traceInfo(), context.rpcCreds(), namespace,
-            property, value));
+    try {
+      ThriftClientTypes.MANAGER.executeOnManager(context, client -> {","[{'comment': ""It seems weird that we need `executeOnManager` when calling on `MANAGER`, but I haven't gotten very far in the review yet, so maybe this will be obvious later."", 'commenter': 'ctubbsii'}, {'comment': 'The methods on ThriftClientType have been renamed in the latest set of changes.', 'commenter': 'dlmarion'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsImpl.java,"@@ -203,19 +218,23 @@ public Map<String,String> getConfiguration(final String namespace)
     EXISTING_NAMESPACE_NAME.validate(namespace);
 
     try {
-      return ServerClient.executeRaw(context, client -> client
-          .getNamespaceConfiguration(TraceUtil.traceInfo(), context.rpcCreds(), namespace));
-    } catch (ThriftTableOperationException e) {
-      switch (e.getType()) {
-        case NAMESPACE_NOTFOUND:
-          throw new NamespaceNotFoundException(e);
-        case OTHER:
-        default:
-          throw new AccumuloException(e.description, e);
-      }
+      return ThriftClientTypes.CLIENT.executeOnTServer(context, client -> {
+        return client.getNamespaceConfiguration(TraceUtil.traceInfo(), context.rpcCreds(),
+            namespace);
+      });
     } catch (AccumuloException e) {
+      Throwable t = e.getCause();
+      if (t instanceof ThriftTableOperationException) {
+        ThriftTableOperationException ttoe = (ThriftTableOperationException) t;
+        switch (ttoe.getType()) {
+          case NAMESPACE_NOTFOUND:
+            throw new NamespaceNotFoundException(ttoe);
+          default:
+            throw e;
+        }","[{'comment': ""I know you copied this code and didn't write it new, but it's probably overkill (in the sense that it hurts readability) to have a switch statement for a single if/else condition.\r\n```suggestion\r\n        if (ttoe.getType() == TableOperationExceptionType.NAMESPACE_NOTFOUND)\r\n            throw new NamespaceNotFoundException(ttoe);\r\n        throw e;\r\n```\r\n\r\nThis logic is also done in a few places. It might be worth putting it in a single place for reuse."", 'commenter': 'ctubbsii'}, {'comment': 'Suggestion implemented.', 'commenter': 'dlmarion'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsImpl.java,"@@ -203,19 +218,23 @@ public Map<String,String> getConfiguration(final String namespace)
     EXISTING_NAMESPACE_NAME.validate(namespace);
 
     try {
-      return ServerClient.executeRaw(context, client -> client
-          .getNamespaceConfiguration(TraceUtil.traceInfo(), context.rpcCreds(), namespace));
-    } catch (ThriftTableOperationException e) {
-      switch (e.getType()) {
-        case NAMESPACE_NOTFOUND:
-          throw new NamespaceNotFoundException(e);
-        case OTHER:
-        default:
-          throw new AccumuloException(e.description, e);
-      }
+      return ThriftClientTypes.CLIENT.executeOnTServer(context, client -> {
+        return client.getNamespaceConfiguration(TraceUtil.traceInfo(), context.rpcCreds(),
+            namespace);
+      });
     } catch (AccumuloException e) {
+      Throwable t = e.getCause();
+      if (t instanceof ThriftTableOperationException) {
+        ThriftTableOperationException ttoe = (ThriftTableOperationException) t;
+        switch (ttoe.getType()) {
+          case NAMESPACE_NOTFOUND:
+            throw new NamespaceNotFoundException(ttoe);
+          default:
+            throw e;
+        }
+      }
       throw e;
-    } catch (Exception e) {
+    } catch (AccumuloSecurityException e) {","[{'comment': 'Is this a change in behavior that matters? Did we also want to catch RTEs here?', 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/SecurityOperationsImpl.java,"@@ -51,45 +53,25 @@ public class SecurityOperationsImpl implements SecurityOperations {
 
   private final ClientContext context;
 
-  private void executeVoid(ClientExec<ClientService.Client> exec)
+  private <R> R exec(Exec<R,ClientService.Client> exec)","[{'comment': ""The `<R>` parameter isn't obvious here. It could be renamed or a small comment added to explain."", 'commenter': 'ctubbsii'}, {'comment': 'javadoc added.', 'commenter': 'dlmarion'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/SecurityOperationsImpl.java,"@@ -51,45 +53,25 @@ public class SecurityOperationsImpl implements SecurityOperations {
 
   private final ClientContext context;
 
-  private void executeVoid(ClientExec<ClientService.Client> exec)
+  private <R> R exec(Exec<R,ClientService.Client> exec)
       throws AccumuloException, AccumuloSecurityException {
     try {
-      ServerClient.executeRawVoid(context, exec);
-    } catch (ThriftTableOperationException ttoe) {
-      // recast missing table
-      if (ttoe.getType() == TableOperationExceptionType.NOTFOUND)
-        throw new AccumuloSecurityException(null, SecurityErrorCode.TABLE_DOESNT_EXIST);
-      else if (ttoe.getType() == TableOperationExceptionType.NAMESPACE_NOTFOUND)
-        throw new AccumuloSecurityException(null, SecurityErrorCode.NAMESPACE_DOESNT_EXIST);
-      else
-        throw new AccumuloException(ttoe);
-    } catch (ThriftSecurityException e) {
-      throw new AccumuloSecurityException(e.user, e.code, e);
-    } catch (AccumuloException e) {
-      throw e;
-    } catch (Exception e) {
-      throw new AccumuloException(e);
-    }
-  }
-
-  private <T> T execute(ClientExecReturn<T,ClientService.Client> exec)
-      throws AccumuloException, AccumuloSecurityException {
-    try {
-      return ServerClient.executeRaw(context, exec);
-    } catch (ThriftTableOperationException ttoe) {
-      // recast missing table
-      if (ttoe.getType() == TableOperationExceptionType.NOTFOUND)
-        throw new AccumuloSecurityException(null, SecurityErrorCode.TABLE_DOESNT_EXIST);
-      else if (ttoe.getType() == TableOperationExceptionType.NAMESPACE_NOTFOUND)
-        throw new AccumuloSecurityException(null, SecurityErrorCode.NAMESPACE_DOESNT_EXIST);
-      else
-        throw new AccumuloException(ttoe);
-    } catch (ThriftSecurityException e) {
-      throw new AccumuloSecurityException(e.user, e.code, e);
+      return ThriftClientTypes.CLIENT.executeOnTServer(context, client -> {
+        return exec.execute(client);
+      });","[{'comment': '```suggestion\r\n      return ThriftClientTypes.CLIENT.executeOnTServer(context, client -> exec.execute(client));\r\n```', 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/SecurityOperationsImpl.java,"@@ -51,45 +53,25 @@ public class SecurityOperationsImpl implements SecurityOperations {
 
   private final ClientContext context;
 
-  private void executeVoid(ClientExec<ClientService.Client> exec)
+  private <R> R exec(Exec<R,ClientService.Client> exec)
       throws AccumuloException, AccumuloSecurityException {
     try {
-      ServerClient.executeRawVoid(context, exec);
-    } catch (ThriftTableOperationException ttoe) {
-      // recast missing table
-      if (ttoe.getType() == TableOperationExceptionType.NOTFOUND)
-        throw new AccumuloSecurityException(null, SecurityErrorCode.TABLE_DOESNT_EXIST);
-      else if (ttoe.getType() == TableOperationExceptionType.NAMESPACE_NOTFOUND)
-        throw new AccumuloSecurityException(null, SecurityErrorCode.NAMESPACE_DOESNT_EXIST);
-      else
-        throw new AccumuloException(ttoe);
-    } catch (ThriftSecurityException e) {
-      throw new AccumuloSecurityException(e.user, e.code, e);
-    } catch (AccumuloException e) {
-      throw e;
-    } catch (Exception e) {
-      throw new AccumuloException(e);
-    }
-  }
-
-  private <T> T execute(ClientExecReturn<T,ClientService.Client> exec)
-      throws AccumuloException, AccumuloSecurityException {
-    try {
-      return ServerClient.executeRaw(context, exec);
-    } catch (ThriftTableOperationException ttoe) {
-      // recast missing table
-      if (ttoe.getType() == TableOperationExceptionType.NOTFOUND)
-        throw new AccumuloSecurityException(null, SecurityErrorCode.TABLE_DOESNT_EXIST);
-      else if (ttoe.getType() == TableOperationExceptionType.NAMESPACE_NOTFOUND)
-        throw new AccumuloSecurityException(null, SecurityErrorCode.NAMESPACE_DOESNT_EXIST);
-      else
-        throw new AccumuloException(ttoe);
-    } catch (ThriftSecurityException e) {
-      throw new AccumuloSecurityException(e.user, e.code, e);
+      return ThriftClientTypes.CLIENT.executeOnTServer(context, client -> {
+        return exec.execute(client);
+      });
     } catch (AccumuloException e) {
+      Throwable t = e.getCause();
+      if (t instanceof ThriftTableOperationException) {
+        ThriftTableOperationException ttoe = (ThriftTableOperationException) t;
+        // recast missing table
+        if (ttoe.getType() == TableOperationExceptionType.NOTFOUND)
+          throw new AccumuloSecurityException(null, SecurityErrorCode.TABLE_DOESNT_EXIST);
+        else if (ttoe.getType() == TableOperationExceptionType.NAMESPACE_NOTFOUND)
+          throw new AccumuloSecurityException(null, SecurityErrorCode.NAMESPACE_DOESNT_EXIST);
+        else
+          throw e;
+      }","[{'comment': ""It's really hard to tell whether the exception handling behavior is different after this PR from before this PR. How much validation have you done to check that there's no change in behavior before and after these changes?"", 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -253,11 +253,11 @@ public void create(String tableName, NewTableConfiguration ntc)
     }
   }
 
-  private long beginFateOperation() throws ThriftSecurityException, TException {
+  private long beginFateOperation() throws ThriftSecurityException, TException, AccumuloException {","[{'comment': ""Based on the fact that this method previously returned only Thrift-specific exceptions, it seems clear that the intent was to handle thrift-related failures in the caller, and to convert exceptions to client-facing ones in the caller. With this now throwing AccumuloException in addition to Thrift-specific exceptions, it's a bit confusing. I would think that all the RPC-related exceptions would have been handled or none would have been, at any given point in the utility code. Throwing both implies they may have been partially handled, and that they need to be handled in multiple places (inside here, and again by the caller)."", 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/rpc/ThriftClientTypes.java,"@@ -18,26 +18,68 @@
  */
 package org.apache.accumulo.core.rpc;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+import static java.util.concurrent.TimeUnit.MILLISECONDS;
+import static org.apache.accumulo.fate.util.UtilWaitThread.sleepUninterruptibly;
+
+import java.net.UnknownHostException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.AccumuloSecurityException;
+import org.apache.accumulo.core.client.NamespaceNotFoundException;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.clientImpl.AccumuloServerException;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.clientImpl.ThriftTransportKey;
 import org.apache.accumulo.core.clientImpl.thrift.ClientService;
+import org.apache.accumulo.core.clientImpl.thrift.ThriftNotActiveServiceException;
+import org.apache.accumulo.core.clientImpl.thrift.ThriftSecurityException;
+import org.apache.accumulo.core.clientImpl.thrift.ThriftTableOperationException;
 import org.apache.accumulo.core.compaction.thrift.CompactionCoordinatorService;
 import org.apache.accumulo.core.compaction.thrift.CompactorService;
 import org.apache.accumulo.core.gc.thrift.GCMonitorService;
 import org.apache.accumulo.core.manager.thrift.FateService;
 import org.apache.accumulo.core.manager.thrift.ManagerClientService;
+import org.apache.accumulo.core.manager.thrift.ManagerClientService.Client;
 import org.apache.accumulo.core.replication.thrift.ReplicationCoordinator;
 import org.apache.accumulo.core.replication.thrift.ReplicationServicer;
 import org.apache.accumulo.core.tabletserver.thrift.TabletClientService;
 import org.apache.accumulo.core.tabletserver.thrift.TabletScanClientService;
+import org.apache.accumulo.core.util.HostAndPort;
+import org.apache.accumulo.core.util.Pair;
+import org.apache.accumulo.core.util.ServerServices;
+import org.apache.accumulo.core.util.ServerServices.Service;
+import org.apache.accumulo.fate.zookeeper.ServiceLock;
+import org.apache.accumulo.fate.zookeeper.ZooCache;
+import org.apache.accumulo.fate.zookeeper.ZooReader;
+import org.apache.thrift.TApplicationException;
 import org.apache.thrift.TServiceClient;
 import org.apache.thrift.TServiceClientFactory;
 import org.apache.thrift.protocol.TMultiplexedProtocol;
 import org.apache.thrift.protocol.TProtocol;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class ThriftClientTypes {
 
+  private static final Logger LOG = LoggerFactory.getLogger(ThriftClientTypes.class);
+
   public static class ThriftClientType<C extends TServiceClient,
       F extends TServiceClientFactory<C>> {
 
+    @FunctionalInterface
+    public interface Exec<R,C> {
+      R execute(C client) throws Exception;
+    }
+","[{'comment': ""This is the first time I've seen somebody explicitly use `@FunctionalInterface`. I don't think it's necessary, as interfaces that are effectively functional are just as good. It's not a problem, though.\r\n\r\nIn any case, this should probably have a small javadoc or comment explaining what the generic parameters are for... or at least better named generic parameters (they don't need to be a single letter)."", 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/rpc/ThriftClientTypes.java,"@@ -60,10 +102,153 @@ public C getClient(TProtocol prot) {
       return clientFactory.getClient(new TMultiplexedProtocol(prot, getServiceName()));
     }
 
+    public Pair<String,C> getTabletServerConnection(ClientContext context,
+        boolean preferCachedConnections) throws TTransportException {
+      throw new UnsupportedOperationException(""This method has not been implemented"");
+    }
+
+    public <R> R executeOnTServer(ClientContext context, Exec<R,C> exec)
+        throws AccumuloException, AccumuloSecurityException {
+      while (true) {
+        String server = null;
+        C client = null;
+        try {
+          Pair<String,C> pair = getTabletServerConnection(context, true);
+          server = pair.getFirst();
+          client = pair.getSecond();
+          return exec.execute(client);
+        } catch (ThriftSecurityException e) {
+          throw new AccumuloSecurityException(e.user, e.code, e);
+        } catch (TApplicationException tae) {
+          throw new AccumuloServerException(server, tae);
+        } catch (TTransportException tte) {
+          LOG.debug(""ClientService request failed "" + server + "", retrying ... "", tte);
+          sleepUninterruptibly(100, MILLISECONDS);
+        } catch (Exception e) {","[{'comment': ""Can we make this catch-all more explicit?\r\nThere can't be very many other checked exceptions we're expecting, so this could just be:\r\n\r\n```suggestion\r\n        } catch (TException | RuntimeException e) {\r\n```"", 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/rpc/ThriftClientTypes.java,"@@ -60,10 +102,153 @@ public C getClient(TProtocol prot) {
       return clientFactory.getClient(new TMultiplexedProtocol(prot, getServiceName()));
     }
 
+    public Pair<String,C> getTabletServerConnection(ClientContext context,
+        boolean preferCachedConnections) throws TTransportException {
+      throw new UnsupportedOperationException(""This method has not been implemented"");
+    }
+
+    public <R> R executeOnTServer(ClientContext context, Exec<R,C> exec)
+        throws AccumuloException, AccumuloSecurityException {
+      while (true) {
+        String server = null;
+        C client = null;
+        try {
+          Pair<String,C> pair = getTabletServerConnection(context, true);
+          server = pair.getFirst();
+          client = pair.getSecond();
+          return exec.execute(client);
+        } catch (ThriftSecurityException e) {
+          throw new AccumuloSecurityException(e.user, e.code, e);
+        } catch (TApplicationException tae) {
+          throw new AccumuloServerException(server, tae);
+        } catch (TTransportException tte) {
+          LOG.debug(""ClientService request failed "" + server + "", retrying ... "", tte);
+          sleepUninterruptibly(100, MILLISECONDS);
+        } catch (Exception e) {
+          throw new AccumuloException(e);
+        } finally {
+          if (client != null) {
+            ThriftUtil.close(client, context);
+          }
+        }
+      }
+    }
+
+    public C getManagerConnection(ClientContext context) {
+      throw new UnsupportedOperationException(""This method has not been implemented"");
+    }
+
+    public C getManagerConnectionWithRetry(ClientContext context) throws AccumuloException {
+      while (true) {
+
+        C result = getManagerConnection(context);
+        if (result != null)
+          return result;
+        sleepUninterruptibly(250, MILLISECONDS);
+      }
+    }
+
+    public <R> R executeAdminOnManager(ClientContext context, Exec<R,C> exec)
+        throws AccumuloException, AccumuloSecurityException {
+      try {
+        return executeOnManager(context, exec);
+      } catch (TableNotFoundException e) {
+        throw new AssertionError(e);
+      }
+    }
+
+    public <R> R executeOnManager(ClientContext context, Exec<R,C> exec)
+        throws AccumuloException, AccumuloSecurityException, TableNotFoundException {
+      C client = null;
+      while (true) {
+        try {
+          client = getManagerConnectionWithRetry(context);
+          return exec.execute(client);
+        } catch (TTransportException tte) {
+          LOG.debug(""ManagerClient request failed, retrying ... "", tte);
+          sleepUninterruptibly(100, MILLISECONDS);
+        } catch (ThriftSecurityException e) {
+          throw new AccumuloSecurityException(e.user, e.code, e);
+        } catch (AccumuloException e) {
+          throw e;
+        } catch (ThriftTableOperationException e) {
+          switch (e.getType()) {
+            case NAMESPACE_NOTFOUND:
+              throw new TableNotFoundException(e.getTableName(), new NamespaceNotFoundException(e));
+            case NOTFOUND:
+              throw new TableNotFoundException(e);
+            default:
+              throw new AccumuloException(e);
+          }
+        } catch (ThriftNotActiveServiceException e) {
+          // Let it loop, fetching a new location
+          LOG.debug(""Contacted a Manager which is no longer active, retrying"");
+          sleepUninterruptibly(100, MILLISECONDS);
+        } catch (Exception e) {
+          throw new AccumuloException(e);
+        } finally {
+          if (client != null)
+            ThriftUtil.close(client, context);
+        }
+      }
+
+    }
+
   }
 
   public static final ThriftClientType<ClientService.Client,ClientService.Client.Factory> CLIENT =
-      new ThriftClientType<>(""ClientService"", new ClientService.Client.Factory());
+      new ThriftClientType<>(""ClientService"", new ClientService.Client.Factory()) {
+        volatile boolean warnedAboutTServersBeingDown = false;
+
+        @Override
+        public Pair<String,org.apache.accumulo.core.clientImpl.thrift.ClientService.Client>","[{'comment': 'Do we need fully-qualified class names here? Is there a name conflict or a deprecation warning that requires us to avoid the import?', 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/rpc/ThriftClientTypes.java,"@@ -74,26 +259,217 @@ public C getClient(TProtocol prot) {
           ""CompactionCoordinatorService"", new CompactionCoordinatorService.Client.Factory());
 
   public static final ThriftClientType<FateService.Client,FateService.Client.Factory> FATE =
-      new ThriftClientType<>(""FateService"", new FateService.Client.Factory());
+      new ThriftClientType<>(""FateService"", new FateService.Client.Factory()) {
+
+        @Override
+        public org.apache.accumulo.core.manager.thrift.FateService.Client
+            getManagerConnection(ClientContext context) {
+          checkArgument(context != null, ""context is null"");
+
+          List<String> locations = context.getManagerLocations();
+
+          if (locations.isEmpty()) {
+            LOG.debug(""No managers..."");
+            return null;
+          }
+
+          HostAndPort manager = HostAndPort.fromString(locations.get(0));
+          if (manager.getPort() == 0)
+            return null;
+
+          try {
+            // Manager requests can take a long time: don't ever time out
+            return ThriftUtil.getClientNoTimeout(ThriftClientTypes.FATE, manager, context);
+          } catch (TTransportException tte) {
+            Throwable cause = tte.getCause();
+            if (cause != null && cause instanceof UnknownHostException) {
+              // do not expect to recover from this
+              throw new RuntimeException(tte);
+            }
+            LOG.debug(""Failed to connect to manager="" + manager + "", will retry... "", tte);
+            return null;
+          }
+        }
+      };
 
   public static final ThriftClientType<GCMonitorService.Client,GCMonitorService.Client.Factory> GC =
       new ThriftClientType<>(""GCMonitorService"", new GCMonitorService.Client.Factory());
 
   public static final ThriftClientType<ManagerClientService.Client,
-      ManagerClientService.Client.Factory> MANAGER =
-          new ThriftClientType<>(""ManagerClientService"", new ManagerClientService.Client.Factory());
+      ManagerClientService.Client.Factory> MANAGER = new ThriftClientType<>(""ManagerClientService"",
+          new ManagerClientService.Client.Factory()) {
+
+        @Override
+        public Client getManagerConnection(ClientContext context) {
+          checkArgument(context != null, ""context is null"");
+
+          List<String> locations = context.getManagerLocations();
+
+          if (locations.isEmpty()) {
+            LOG.debug(""No managers..."");
+            return null;
+          }
+
+          HostAndPort manager = HostAndPort.fromString(locations.get(0));
+          if (manager.getPort() == 0)
+            return null;
+
+          try {
+            // Manager requests can take a long time: don't ever time out
+            return ThriftUtil.getClientNoTimeout(ThriftClientTypes.MANAGER, manager, context);
+          } catch (TTransportException tte) {
+            Throwable cause = tte.getCause();
+            if (cause != null && cause instanceof UnknownHostException) {
+              // do not expect to recover from this
+              throw new RuntimeException(tte);
+            }
+            LOG.debug(""Failed to connect to manager="" + manager + "", will retry... "", tte);
+            return null;
+          }
+
+        }
+
+      };
 
   public static final ThriftClientType<ReplicationCoordinator.Client,
       ReplicationCoordinator.Client.Factory> REPLICATION_COORDINATOR = new ThriftClientType<>(","[{'comment': ""Instead of having all these as anonymous inner classes inline'd into this one class, can we make these subclasses each in their own file, and just have static final fields here that reference an instance of them?"", 'commenter': 'ctubbsii'}]"
2647,core/src/main/java/org/apache/accumulo/core/rpc/ThriftUtil.java,"@@ -148,6 +148,15 @@ public static <T extends TServiceClient> T getClient(ThriftClientType<T,?> type,
     return createClient(type, transport);
   }
 
+  public static void close(TServiceClient client, ClientContext context) {
+    if (client != null && client.getInputProtocol() != null
+        && client.getInputProtocol().getTransport() != null) {
+      context.getTransportPool().returnTransport(client.getInputProtocol().getTransport());
+    } else {
+      log.debug(""Attempt to close null connection to a server"", new Exception());","[{'comment': 'Should the log message indicate which of the 3 conditions were null? The transport, the protocol, or the client? More information here might be helpful if this starts showing up in the debug logs.', 'commenter': 'ctubbsii'}]"
2647,server/tserver/src/main/java/org/apache/accumulo/tserver/replication/ClientExecReturn.java,"@@ -16,8 +16,9 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.accumulo.core.clientImpl;
+package org.apache.accumulo.tserver.replication;
 
+@Deprecated
 public interface ClientExecReturn<T,C> {
   T execute(C client) throws Exception;","[{'comment': ""Isn't this the same as your new interface? Can't this just be replaced with that one and this one be deleted?"", 'commenter': 'ctubbsii'}]"
2647,shell/pom.xml,"@@ -79,6 +79,10 @@
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-client-api</artifactId>
     </dependency>
+    <dependency>
+      <groupId>org.apache.thrift</groupId>
+      <artifactId>libthrift</artifactId>
+    </dependency>","[{'comment': ""The shell really should not have a direct dependency on libthrift at all. Ideally, it should only use our public API directly. We've been trying to move in that direction. So, whatever caused thrift types to leak in here, should probably be removed."", 'commenter': 'ctubbsii'}]"
2647,shell/src/main/java/org/apache/accumulo/shell/commands/FateCommand.java,"@@ -287,6 +296,34 @@ protected boolean cancelSubmittedTxs(final Shell shellState, String[] args)
     return true;
   }
 
+  private static boolean cancelFateOperation(ClientContext context, long txid,
+      final Shell shellState) throws AccumuloException, AccumuloSecurityException {
+    while (true) {
+      FateService.Client client = null;
+      try {
+        client = ThriftClientTypes.FATE.getManagerConnectionWithRetry(context);
+        return client.cancelFateOperation(TraceUtil.traceInfo(), context.rpcCreds(), txid);
+      } catch (TTransportException tte) {
+        shellState.getWriter()
+            .println(""ManagerClient request failed, retrying. Cause: "" + tte.getMessage());
+        sleepUninterruptibly(100, TimeUnit.MILLISECONDS);","[{'comment': ""I think we need to abstract things a bit further, if we're leaking these Thrift types into the shell. That implies we're not doing a great job of limiting ourselves to public API types.\r\n\r\nI actually don't think we need to do as much work as we're doing here to retry any of this or to convert all the exceptions here. The shell is for humans... we can just dump any stack trace and the user can press the up arrow and try again if they need to."", 'commenter': 'ctubbsii'}]"
2647,shell/src/main/java/org/apache/accumulo/shell/commands/ListBulkCommand.java,"@@ -52,22 +47,10 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
 
     List<String> tservers;
 
-    ManagerMonitorInfo stats;
-    ManagerClientService.Iface client = null;
     ClientContext context = shellState.getContext();
-    while (true) {
-      try {
-        client = ManagerClient.getConnectionWithRetry(context);
-        stats = client.getManagerStats(TraceUtil.traceInfo(), context.rpcCreds());
-        break;
-      } catch (ThriftNotActiveServiceException e) {
-        // Let it loop, fetching a new location
-        sleepUninterruptibly(100, TimeUnit.MILLISECONDS);
-      } finally {
-        if (client != null)
-          ManagerClient.close(client, context);
-      }
-    }
+    ManagerMonitorInfo stats = ThriftClientTypes.MANAGER.executeAdminOnManager(context, client -> {
+      return client.getManagerStats(TraceUtil.traceInfo(), context.rpcCreds());
+    });","[{'comment': 'Not sure, but this might format better:\r\n```suggestion\r\n    ManagerMonitorInfo stats = ThriftClientTypes.MANAGER.executeAdminOnManager(context,\r\n    client -> client.getManagerStats(TraceUtil.traceInfo(), context.rpcCreds()));\r\n```', 'commenter': 'ctubbsii'}]"
2647,test/src/main/java/org/apache/accumulo/test/functional/ManagerApiIT.java,"@@ -60,12 +60,14 @@ public class ManagerApiIT extends SharedMiniClusterBase {
   private static Credentials rootUser;
   private static Credentials regularUser;
   private static Credentials privilegedUser;
+  private static InstanceId instanceId;
 
   @Override
   protected Duration defaultTimeout() {
     return Duration.ofMinutes(1);
   }
 
+  @SuppressWarnings(""deprecation"")","[{'comment': 'Instead of suppressing the entire method, can this be narrowed down to the specific deprecated item?', 'commenter': 'ctubbsii'}]"
2647,test/src/main/java/org/apache/accumulo/test/replication/GarbageCollectorCommunicatesWithTServersIT.java,"@@ -384,8 +383,9 @@ public void testUnreferencedWalInTserverIsClosed() throws Exception {
 
     // Get the tservers which the manager deems as active
     final ClientContext context = (ClientContext) client;
-    List<String> tservers = ManagerClient.execute(context,
-        cli -> cli.getActiveTservers(TraceUtil.traceInfo(), context.rpcCreds()));
+    List<String> tservers = ThriftClientTypes.MANAGER.executeAdminOnManager(context, (mgr) -> {
+      return mgr.getActiveTservers(TraceUtil.traceInfo(), context.rpcCreds());
+    });","[{'comment': '```suggestion\r\n    List<String> tservers = ThriftClientTypes.MANAGER.executeAdminOnManager(context\r\n        mgr -> mgr.getActiveTservers(TraceUtil.traceInfo(), context.rpcCreds()));\r\n```', 'commenter': 'ctubbsii'}]"
2648,shell/src/main/java/org/apache/accumulo/shell/ShellOptionsJC.java,"@@ -236,4 +239,14 @@ public Properties getClientProperties() {
     return props;
   }
 
+  static class PositiveInteger implements IParameterValidator {
+    public void validate(String name, String value) throws ParameterException {","[{'comment': '```suggestion\r\n    @Override\r\n    public void validate(String name, String value) throws ParameterException {\r\n```', 'commenter': 'ctubbsii'}]"
2648,shell/src/main/java/org/apache/accumulo/shell/ShellOptionsJC.java,"@@ -236,4 +239,14 @@ public Properties getClientProperties() {
     return props;
   }
 
+  static class PositiveInteger implements IParameterValidator {
+    public void validate(String name, String value) throws ParameterException {
+      int n = Integer.parseInt(value);
+      if (n < 0) {
+        throw new ParameterException(
+            ""Parameter "" + name + "" should be positive (found "" + value + "")"");
+      }","[{'comment': '```suggestion\r\n      int n = -1;\r\n      try {\r\n        n = Integer.parseInt(value);\r\n      } catch (NumberFormatException e) {\r\n        // ignore, will be handled below\r\n      }\r\n      if (n < 0) {\r\n        throw new ParameterException(\r\n            ""Parameter "" + name + "" should be a positive integer (was "" + value + "")"");\r\n      }\r\n```', 'commenter': 'ctubbsii'}]"
2648,test/src/main/java/org/apache/accumulo/test/ConditionalWriterIT.java,"@@ -883,15 +884,19 @@ public void testBigBatch() throws Exception {
 
         int count = 0;
 
-        // TODO check got each row back
+        Set<String> rowsReceived = new HashSet<>();
         while (results.hasNext()) {
           Result result = results.next();
+          rowsReceived.add(new String(result.getMutation().getRow()));","[{'comment': 'Should explicitly use `UTF_8` when constructing a String from byte array, otherwise, this could fail on some platforms where `UTF_8` is not the default.', 'commenter': 'ctubbsii'}]"
2648,test/src/main/java/org/apache/accumulo/test/ConditionalWriterIT.java,"@@ -883,15 +884,19 @@ public void testBigBatch() throws Exception {
 
         int count = 0;
 
-        // TODO check got each row back
+        Set<String> rowsReceived = new HashSet<>();
         while (results.hasNext()) {
           Result result = results.next();
+          rowsReceived.add(new String(result.getMutation().getRow()));
           assertEquals(Status.ACCEPTED, result.getStatus());
           count++;
         }
 
         assertEquals(num, count, ""Did not receive the expected number of results"");
 
+        Set<String> rowsExpected = rows.stream().map(String::new).collect(Collectors.toSet());","[{'comment': 'Should explicitly use `UTF_8` when constructing new strings here.', 'commenter': 'ctubbsii'}]"
2648,test/src/main/java/org/apache/accumulo/test/ExistingMacIT.java,"@@ -88,20 +88,21 @@ private void createEmptyConfig(File confFile) throws IOException {
   @Test
   public void testExistingInstance() throws Exception {
 
+    final String rootUser = ""root"";
     AccumuloClient client =
-        getCluster().createAccumuloClient(""root"", new PasswordToken(ROOT_PASSWORD));
+        getCluster().createAccumuloClient(rootUser, new PasswordToken(ROOT_PASSWORD));
 
-    client.tableOperations().create(""table1"");
+    final String table1 = ""table1"";","[{'comment': 'This table name should be unique for this test method.', 'commenter': 'ctubbsii'}]"
2651,server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java,"@@ -79,7 +79,7 @@ public static void main(String[] args) throws Exception {
       argsList.add(""--new"");
       argsList.addAll(Arrays.asList(args));
 
-      opts.parseArgs(ChangeSecret.class.getName(), args);
+      opts.parseArgs(ChangeSecret.class.getName(), argsList.toArray(new String[0]));","[{'comment': 'All of this seems quite dubious to me. This is a very convoluted way to get JCommander to do the prompting for us. Instead of doing that, we can just prompt directly for the passwords:\r\n\r\n```suggestion\r\n      opts.parseArgs(ChangeSecret.class.getName(), args);\r\n      String oldPass = String.valueOf(System.console().readPassword(""Old password: ""));\r\n      String newPass = String.valueOf(System.console().readPassword(""New password: ""));\r\n```\r\n\r\nCan then just completely remove the embedded `Opts` class, and use `ServerUtilOpts` directly. This could use some tweaks, though... like using `!StringUtils.isBlank(oldPass)` instead of checking if `opts.oldPass != null` below.', 'commenter': 'ctubbsii'}, {'comment': 'That is fine. I was just attempting to keep it in the form that it was originally written in case there was a particular reason for that format.', 'commenter': 'jmark99'}, {'comment': ""Yeah, your code would have fixed the bug. But, I think it's probably better to fix the problem that caused the bug: the unnecessary complexity by trying to use JCommander to do the prompting."", 'commenter': 'ctubbsii'}, {'comment': 'If you want to merge your fixes as-is, I can do a follow-up to implement my suggestion.', 'commenter': 'ctubbsii'}, {'comment': 'Committed the suggestion before seeing your last comment. Let me get it  in a working state and then I can commit and you can add additional modifications.', 'commenter': 'jmark99'}]"
2651,server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java,"@@ -45,51 +45,37 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.zookeeper.ZooDefs.Ids;
+import org.apache.zookeeper.common.StringUtils;
 import org.apache.zookeeper.data.ACL;
 import org.apache.zookeeper.data.Stat;
 
-import com.beust.jcommander.Parameter;
-
 import io.opentelemetry.api.trace.Span;
 import io.opentelemetry.context.Scope;
 
 public class ChangeSecret {
 
-  static class Opts extends ServerUtilOpts {
-    @Parameter(names = ""--old"", description = ""old zookeeper password"", password = true,
-        hidden = true)
-    String oldPass;
-    @Parameter(names = ""--new"", description = ""new zookeeper password"", password = true,
-        hidden = true)
-    String newPass;
-  }
-
   public static void main(String[] args) throws Exception {
     var siteConfig = SiteConfiguration.auto();
     var hadoopConf = new Configuration();
 
-    Opts opts = new Opts();
+    ServerUtilOpts opts = new ServerUtilOpts();
     ServerContext context = opts.getServerContext();
     try (var fs = context.getVolumeManager()) {
       ServerDirs serverDirs = new ServerDirs(siteConfig, hadoopConf);
       verifyHdfsWritePermission(serverDirs, fs);
 
-      List<String> argsList = new ArrayList<>(args.length + 2);
-      argsList.add(""--old"");
-      argsList.add(""--new"");
-      argsList.addAll(Arrays.asList(args));
-
-      opts.parseArgs(ChangeSecret.class.getName(), args);
+      String oldPass = String.valueOf(System.console().readPassword(""Old password: ""));
+      String newPass = String.valueOf(System.console().readPassword(""New password: ""));","[{'comment': 'I just realized it might make sense if we make it prompts say ""secret"" instead of ""password"".\r\n\r\n```suggestion\r\n      String oldPass = String.valueOf(System.console().readPassword(""Old secret: ""));\r\n      String newPass = String.valueOf(System.console().readPassword(""New secret: ""));\r\n```', 'commenter': 'ctubbsii'}]"
2651,server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java,"@@ -45,51 +45,37 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.zookeeper.ZooDefs.Ids;
+import org.apache.zookeeper.common.StringUtils;","[{'comment': 'This should use the Apache Commons StringUtils, not the one from ZooKeeper.', 'commenter': 'ctubbsii'}, {'comment': 'Yep! Intellij added the zk one and I missed it.', 'commenter': 'jmark99'}]"
2661,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -182,6 +169,7 @@ private enum CloseState {
 
   private boolean updatingFlushID = false;
 
+  // TODO volatile ?","[{'comment': 'Please resolve this before merging.', 'commenter': 'ctubbsii'}, {'comment': 'resolved in 58b0e73', 'commenter': 'dlmarion'}]"
2661,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/TabletBase.java,"@@ -0,0 +1,463 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.tserver.tablet;
+
+import static java.util.Objects.requireNonNull;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.ByteSequence;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
+import org.apache.accumulo.core.iterators.YieldCallback;
+import org.apache.accumulo.core.iteratorsImpl.system.IterationInterruptedException;
+import org.apache.accumulo.core.iteratorsImpl.system.SourceSwitchingIterator;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.TabletFile;
+import org.apache.accumulo.core.metadata.schema.DataFileValue;
+import org.apache.accumulo.core.sample.impl.SamplerConfigurationImpl;
+import org.apache.accumulo.core.security.ColumnVisibility;
+import org.apache.accumulo.core.util.LocalityGroupUtil;
+import org.apache.accumulo.core.util.Pair;
+import org.apache.accumulo.core.util.ShutdownUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.TableConfiguration;
+import org.apache.accumulo.server.fs.TooManyFilesException;
+import org.apache.accumulo.tserver.InMemoryMap;
+import org.apache.accumulo.tserver.TabletServerResourceManager;
+import org.apache.accumulo.tserver.metrics.TabletServerScanMetrics;
+import org.apache.accumulo.tserver.scan.ScanParameters;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * This class exists to share code for scanning a tablet between TabletHostingServer implementations
+ */
+public abstract class TabletBase {
+
+  private static final Logger log = LoggerFactory.getLogger(TabletBase.class);
+
+  private static final byte[] EMPTY_BYTES = new byte[0];
+
+  protected final KeyExtent extent;
+  protected final ServerContext context;
+
+  // TODO these are written in synch block but read w/o sync so maybe should be volatile","[{'comment': ' Please address all TODOs before merging.', 'commenter': 'ctubbsii'}, {'comment': 'resolved in 58b0e73', 'commenter': 'dlmarion'}]"
2661,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/TabletBase.java,"@@ -0,0 +1,463 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.tserver.tablet;
+
+import static java.util.Objects.requireNonNull;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.ByteSequence;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
+import org.apache.accumulo.core.iterators.YieldCallback;
+import org.apache.accumulo.core.iteratorsImpl.system.IterationInterruptedException;
+import org.apache.accumulo.core.iteratorsImpl.system.SourceSwitchingIterator;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.TabletFile;
+import org.apache.accumulo.core.metadata.schema.DataFileValue;
+import org.apache.accumulo.core.sample.impl.SamplerConfigurationImpl;
+import org.apache.accumulo.core.security.ColumnVisibility;
+import org.apache.accumulo.core.util.LocalityGroupUtil;
+import org.apache.accumulo.core.util.Pair;
+import org.apache.accumulo.core.util.ShutdownUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.TableConfiguration;
+import org.apache.accumulo.server.fs.TooManyFilesException;
+import org.apache.accumulo.tserver.InMemoryMap;
+import org.apache.accumulo.tserver.TabletServerResourceManager;
+import org.apache.accumulo.tserver.metrics.TabletServerScanMetrics;
+import org.apache.accumulo.tserver.scan.ScanParameters;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * This class exists to share code for scanning a tablet between TabletHostingServer implementations
+ */
+public abstract class TabletBase {","[{'comment': 'The body of this base class is just stuff moved from Tablet, right? Is there anything else in here that is new that we should pay particular attention to?', 'commenter': 'ctubbsii'}, {'comment': 'I believe that is the case. That should be the case. @keith-turner wrote the original change against the Scan Server feature branch. I pulled out what I could as a separate PR to get the smaller changeset upstream of our feature. Like you said above, smaller incremental changes that can be reviewed easier.', 'commenter': 'dlmarion'}, {'comment': ""This change is to isolate the read the read code from the tablet and move it up a level.  In Dave's scan server branch we have a SnapshotTablet that extends TabletBase and offers a readonly view of a snapshot of a tablet.  "", 'commenter': 'keith-turner'}]"
2661,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletHostingServer.java,"@@ -0,0 +1,57 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.tserver;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.fate.zookeeper.ServiceLock;
+import org.apache.accumulo.fate.zookeeper.ZooCache;
+import org.apache.accumulo.server.GarbageCollectionLogger;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.TableConfiguration;
+import org.apache.accumulo.tserver.metrics.TabletServerScanMetrics;
+import org.apache.accumulo.tserver.session.Session;
+import org.apache.accumulo.tserver.session.SessionManager;
+import org.apache.accumulo.tserver.tablet.Tablet;
+
+public interface TabletHostingServer {","[{'comment': 'In the branch we had the following comment.  \r\n\r\n```java\r\n/**\r\n * This interface exist to support passing a {@link TabletServer} or {@link ScanServer} to a method\r\n * that can take either.\r\n */\r\n```\r\n\r\nit does not make sense here, but we need to remember to add it back.', 'commenter': 'keith-turner'}, {'comment': 'Can you give a description of this class? I like to put javadoc on interfaces.', 'commenter': 'milleruntime'}, {'comment': 'Javadoc for this was added in #2665 ', 'commenter': 'dlmarion'}]"
2666,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/replication/ReplicationResource.java,"@@ -149,6 +149,12 @@ public List<ReplicationInformation> getReplicationInformation()
       });
     }
 
+    //Ensure replication table is online before attempting to create BatchScanner
+    if (!ReplicationTable.isOnline(client)) {
+        log.error(""Replication table is offline"");","[{'comment': 'I suspect the formatter will add a space when it builds. I enabled a build on this PR, so we\'ll find out.\r\n\r\nThe message can also be a debug instead of an error, since this is probably a normal situation (it\'s not a problem if the user clicks the page).\r\n\r\n```suggestion\r\n    // Ensure replication table is online before attempting to create BatchScanner\r\n    if (!ReplicationTable.isOnline(client)) {\r\n        log.debug(""Replication page requested, but replication table is offline"");\r\n```', 'commenter': 'ctubbsii'}, {'comment': '```suggestion\r\n    //Ensure replication table is online before attempting to create BatchScanner\r\n    if (!ReplicationTable.isOnline(client)) {\r\n        log.error(""Replication table is offline"");\r\n```', 'commenter': 'FerFlovi'}, {'comment': '@ctubbsii In the Monitor, the Replication page already indicates ""Replication table is offline"". Is this sufficient?', 'commenter': 'tchaie'}, {'comment': ""If that's already the case, then nothing needs to be done in that regard."", 'commenter': 'ctubbsii'}, {'comment': 'Maybe better as a separate issue, but is that how the message reads on the monitor? It may be more informative if the message indicated something along the lines of:\r\n \r\n```Replication is disabled - the replication table is offline```\r\n\r\nMy thinking is that not using replication is more common. And having the replication off-line is the expected (desired) state.  Clarifying that may help if someone somehow interrupted off-line as bad.  Only people using replication should care.\r\n', 'commenter': 'EdColeman'}]"
2672,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -127,9 +127,13 @@ private SortedMap<String,String> makeRelative(Collection<String> candidates) {
     return ret;
   }
 
-  private void confirmDeletes(GarbageCollectionEnvironment gce,
+  /**
+   * Return the number of BLIP flags seen.
+   */
+  private long confirmDeletes(GarbageCollectionEnvironment gce,","[{'comment': 'I feel like this method should be renamed to `removeBlipCandidates`  or something a little more descriptive.', 'commenter': 'dlmarion'}, {'comment': 'It actually does a bunch of other stuff. I will try and split it up into smaller methods.', 'commenter': 'milleruntime'}]"
2672,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -206,6 +212,7 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
 
     confirmDeletesFromReplication(gce.getReplicationNeededIterator(),
         candidateMap.entrySet().iterator());
+    return blipCount;
   }
 
   protected void confirmDeletesFromReplication(","[{'comment': 'This could be marked as deprecated so we know to remove it later.', 'commenter': 'dlmarion'}, {'comment': ""A grep for 'replication' is just as good as a grep for 'deprecated', and avoiding the deprecated annotation means less warnings suppression annotations. I could go either way."", 'commenter': 'ctubbsii'}]"
2672,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -269,17 +276,19 @@ private void cleanUpDeletedTableDirs(GarbageCollectionEnvironment gce,
     }
   }
 
-  private void confirmDeletesTrace(GarbageCollectionEnvironment gce,
+  private long confirmDeletesTrace(GarbageCollectionEnvironment gce,","[{'comment': ""I'm not sure we need a separate method to enable tracing. I think we should add the tracing to the `confirmDeletes` method and remove this."", 'commenter': 'dlmarion'}, {'comment': 'That is a good idea. I will give that a try.', 'commenter': 'milleruntime'}]"
2672,server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectionTest.java,"@@ -622,15 +622,16 @@ public void test() throws Exception {
     gce.candidates.clear();
     gce.candidates.add(""/9/default_tablet"");
     gce.candidates.add(""/9/default_tablet/someFile"");
-    gca.collect(gce);
+    long blipCount = gca.collect(gce);
     assertRemoved(gce);
+    assertEquals(0, blipCount);
 
     gce = new TestGCE();
     gce.blips.add(""/1636/b-0001"");
     gce.candidates.add(""/1636/b-0001/I0000"");
-    gca.collect(gce);
+    blipCount = gca.collect(gce);
     assertRemoved(gce);
-
+    assertEquals(1, blipCount);","[{'comment': 'One more test would be nice, to cover cases 0,1,>1', 'commenter': 'keith-turner'}, {'comment': 'Yeah I can add another test.', 'commenter': 'milleruntime'}]"
2672,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -200,17 +166,61 @@ private void confirmDeletes(GarbageCollectionEnvironment gce,
         String dir = reference.substring(0, reference.lastIndexOf('/'));
         if (candidateMap.remove(dir) != null)
           log.debug(""Candidate was still in use: {}"", reference);
+      }
+    }
+  }
+
+  private long removeBlipCandidates(GarbageCollectionEnvironment gce,
+      SortedMap<String,String> candidateMap) throws TableNotFoundException {
+    boolean checkForBulkProcessingFiles = false;
+    long blipCount = 0;
+    Iterator<String> relativePaths = candidateMap.keySet().iterator();
+
+    while (!checkForBulkProcessingFiles && relativePaths.hasNext())
+      checkForBulkProcessingFiles |=
+          relativePaths.next().toLowerCase(Locale.ENGLISH).contains(Constants.BULK_PREFIX);","[{'comment': 'Could possibly shorten this with stream anyMatch()', 'commenter': 'keith-turner'}, {'comment': 'Yeah, that cleans it up nicely.', 'commenter': 'milleruntime'}]"
2672,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -312,25 +327,28 @@ public void collect(GarbageCollectionEnvironment gce) throws TableNotFoundExcept
       } finally {
         candidatesSpan.end();
       }
-      deleteBatch(gce, batchOfCandidates);
+      totalBlips += deleteBatch(gce, batchOfCandidates);","[{'comment': 'This may count the same blip multiple times.  This loop can take a subset of the candidates and then loop over all of the blips.  So if there are 5 subsets of candidates and 100 total blips in the metadata table then it may return 500 instead of 100.  Could just take the last count.\r\n\r\n```suggestion\r\n      totalBlips = deleteBatch(gce, batchOfCandidates);\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Ah Ok. I will do a follow up fix.', 'commenter': 'milleruntime'}]"
2679,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/functions.js,"@@ -426,25 +473,34 @@ function getTraceOfType(type, minutes) {
  */
 function getTraceShow(id) {
   var call = '/rest/trace/show/' + id;
+  console.info(""Call to "" + call);
   return $.getJSON(call, function(data) {
-    sessionStorage.traceShow = JSON.stringify(data);
+    var jsonDataStr = JSON.stringify(data);
+    sessionStorage.traceShow = jsonDataStr;
+    console.info(""REST call to "" + call + "" stored in sessionStorage.traceShow = "" + jsonDataStr);
   });
 }
 
 /**
  * REST GET call for the logs, stores it on a sessionStorage variable
  */
 function getLogs() {
-  return $.getJSON('/rest/logs', function(data) {
-    sessionStorage.logs = JSON.stringify(data);
+  var call = '/rest/logs';
+  console.info(""Call to "" + call);
+  return $.getJSON(call, function(data) {
+    var jsonDataStr = JSON.stringify(data);
+    sessionStorage.logs = jsonDataStr;
+    console.info(""REST call to "" + call + "" stored in sessionStorage.logs = "" + jsonDataStr);
   });","[{'comment': 'A lot of these do the same thing. You can probably simplify these by placing them in a new function that takes a parameter for the session storage variable. In this case, it would be ""logs"". I found https://stackoverflow.com/a/2132814/196405 , which suggests sessionStorage.logs is equivalent to: sessionStorage[""logs""], so you could pass ""logs"" as a parameter.\r\n\r\nAlso, I\'m thinking these console messages should probably use `console.debug`, at least for the second one.\r\nThe ""Call to "" one could probably stay console.info. It might also be better worded as: ""Retrieving "" or ""Loading data from "" instead of ""Call to "".', 'commenter': 'ctubbsii'}, {'comment': 'There are also setters and getters for the session storage variables that might make it easier when refactoring shared code into a function: https://www.w3schools.com/jsref/prop_win_sessionstorage.asp', 'commenter': 'DomGarguilo'}]"
2679,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/functions.js,"@@ -256,24 +256,50 @@ function createTableCell(index, sortValue, showValue) {
       '"">' + showValue + '</td>';
 }
 
+/**
+ * Builds console logging message for REST GET calls
+ * @param {string} REST url called
+ * @param {object} data returned from REST call
+ * @param {string} session storage/global variable to hold REST data
+ */
+ function createRestGetLog(call, data, sessionDataVar){
+    var jsonDataStr = JSON.stringify(data);
+
+    //Handle data to be stored in global variable instead of session storage
+    if (sessionDataVar===""NAMESPACES"") {
+      NAMESPACES = jsonDataStr;
+      console.debug(""REST GET call to "" + call +
+       "" stored in "" + sessionDataVar + "" = "" + NAMESPACES);
+    }
+    else {
+      sessionStorage[sessionDataVar] = jsonDataStr;
+      console.debug(""REST GET request to "" + call +
+        "" stored in sessionStorage."" + sessionDataVar + "" = "" + sessionStorage[sessionDataVar]);
+    }
+ }
+
 ///// REST Calls /////////////
 
 /**
  * REST GET call for the manager information,
  * stores it on a sessionStorage variable
  */
 function getManager() {
-  return $.getJSON('/rest/manager', function(data) {
-    sessionStorage.manager = JSON.stringify(data);
+  var call = '/rest/manager';
+  console.info(""Retrieving "" + call);
+  return $.getJSON(call, function(data) {
+    createRestGetLog(call, data, 'manager');
   });
 }","[{'comment': ""Since the same pattern is used repeatedly, this code could be simplified even further, by replacing it with a new function, something like `getJSONforTable('/rest/manager', 'manager');`"", 'commenter': 'ctubbsii'}]"
2679,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/functions.js,"@@ -507,20 +563,25 @@ function getProblemDetails() {
  * stores it on a sessionStorage variable
  */
 function getReplication() {
-  return $.getJSON('/rest/replication', function(data) {
-    sessionStorage.replication = JSON.stringify(data);
+  var call = '/rest/replication';
+  console.info(""Retrieving "" + call);
+  return $.getJSON(call, function(data) {
+    createRestGetLog(call, data, 'replication');
   });
 }
 
 //// Overview Plots Rest Calls
+//// Note: console.debug messages may not show by default in some browsers","[{'comment': ""I think the logging level concept is pretty standard, so you probably don't need this comment here.\r\n\r\n```suggestion\r\n```\r\n\r\nOr, you could clarify that the choice of using debug was intentional:\r\n \r\n```suggestion\r\n//// Note: console.debug avoids spamming the console by default in browsers that support it\r\n```"", 'commenter': 'ctubbsii'}]"
2679,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/functions.js,"@@ -299,19 +320,15 @@ function getNamespaceTables(namespaces) {
     namespaceList = namespaces.toString();
 
     var call = '/rest/tables/namespaces/' + namespaceList;
-    return $.getJSON(call, function(data) {
-      sessionStorage.tables = JSON.stringify(data);
-    });
+    return getJSONForTable(call, 'tables');","[{'comment': ""It probably doesn't really matter, but some of these that create the variable, and only use it once inside the function call, could just inline the variable to turn these into one-liners:\r\n\r\n```javascript\r\n    return getJSONForTable('/rest/tables/namespaces/' + namespaceList, 'tables');\r\n```\r\n\r\nThere are several such occurrences in this PR that could be changed like this."", 'commenter': 'ctubbsii'}]"
2679,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/functions.js,"@@ -460,6 +456,7 @@ function clearTableProblems(tableID) {
   call = sanitize(call);
   // make the rest call, passing success function callback
   $.post(call, function () {
+    console.info(""REST POST call to "" + call);
     refreshProblems();
   });","[{'comment': 'I think this pattern shows up a couple of times and could be put into it\'s own function (the following is only pseudo-code, so I don\'t expect it to work as-is):\r\n\r\n```javascript\r\nfunction doLoggedPostCall(endpoint, callback) {\r\n  console.info(""POST call to "" + endpoint);\r\n  $.post(endpoint, function() {\r\n    console.debug(""POST call to "" + endpoint + "" : success""); // not sure the appropriate message here\r\n    if (callback != null) {\r\n      // execute callback; for example, refreshProblems() in this case... could be something else in other cases.\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThen, This becomes:\r\n```javascript\r\n  doLoggedPostCall(call, refreshProblems);\r\n```', 'commenter': 'ctubbsii'}]"
2691,server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImpl.java,"@@ -130,13 +135,18 @@ public Builder(final ZooPropLoader zooPropLoader, final PropStoreMetrics metrics
     }
 
     public PropCacheCaffeineImpl build() {
-      return new PropCacheCaffeineImpl(zooPropLoader, metrics, ticker);
+      return new PropCacheCaffeineImpl(zooPropLoader, metrics, ticker, runTasksInline);
     }
 
     public Builder withTicker(final Ticker ticker) {
       this.ticker = ticker;
       return this;
     }
+
+    public Builder forTests() {","[{'comment': 'You could pass in the ticker here and remove the withTicker() method - the ticker is only used for testing and if the in-lineing is also only for test, the functionallity could just be located in one method.  forTesting() seems to express that better than withTicker(), but either way.', 'commenter': 'EdColeman'}, {'comment': 'I thought about this, but I noticed that `withTicker` is used outside of the tests, in the `ZooPropStore` constructor. ', 'commenter': 'dlmarion'}, {'comment': 'Looking now, that constructor is only called from a test. I can look at adding this change. Are there other tests that can use `forTests()` and remove `sleep` calls?', 'commenter': 'dlmarion'}, {'comment': 'The constructor in ZooPropStore looks like it could call forTests(ticker) - or an overloaded constructor added with the ticker created.', 'commenter': 'EdColeman'}, {'comment': 'Maybe? - Some of them are used to force the current, test thread to yield to help ZooKeeper notifications having an opportunity to propagate to set-up the follow-on test conditions. ', 'commenter': 'EdColeman'}]"
2691,server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoaderTest.java,"@@ -548,14 +543,13 @@ public void refreshSameVersionTest() throws Exception {
 
     replay(context, zrw, propStoreWatcher, cacheMetrics, mockProps);
 
-    PropCacheCaffeineImpl cache = new PropCacheCaffeineImpl.Builder(loader, cacheMetrics)
-        .withTicker(ticker).forTests().build();
+    PropCacheCaffeineImpl cache =
+        new PropCacheCaffeineImpl.Builder(loader, cacheMetrics).forTests(ticker).build();
 
     // prime cache
     cache.get(propCacheKey);
 
     ticker.advance(30, TimeUnit.MINUTES);
-    cache.cleanUp();
 ","[{'comment': 'Why is this being removed?  The test is going to rely on the cache cleaner thread ran,', 'commenter': 'EdColeman'}, {'comment': 'Because PropCacheCaffeineImpl.Builder is being created with `forTests` which performs all maintenance tasks in the current test Thread. In testing my earlier changes calling `cleanUp` may have forced the background tasks to kick off, but they were still being done asynchronously.', 'commenter': 'dlmarion'}]"
2691,test/src/main/java/org/apache/accumulo/test/conf/store/ZooBasedConfigIT.java,"@@ -371,24 +360,4 @@ public String toString() {
     }
   }
 
-  private static class TestTicker implements Ticker {","[{'comment': ""I don't think this should be removed - a fake timer is necessary to exercise the functionally in the test."", 'commenter': 'EdColeman'}, {'comment': ""I didn't see where the fake timer was actually being used *and* the test passes without it. I'm good with adding it back, but can you explain how/where it's being used? The only place where the fake timer is being used is [here](https://github.com/apache/accumulo/blob/main/test/src/main/java/org/apache/accumulo/test/conf/store/ZooBasedConfigIT.java#L282) and its not clear to me how that works with the test."", 'commenter': 'dlmarion'}, {'comment': 'Can you file leave this alone until I have a change to look into it deeper?', 'commenter': 'EdColeman'}, {'comment': 'I would at least need to remove the call to cleanup above for it to compile.', 'commenter': 'dlmarion'}]"
2695,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -1581,76 +1578,35 @@ public static boolean isSensitive(String key) {
     Property prop = propertiesByKey.get(key);
     if (prop != null) {
       return prop.isSensitive();
-    } else {
-      for (String prefix : validPrefixes) {
-        if (key.startsWith(prefix)) {
-          if (propertiesByKey.get(prefix).isSensitive()) {
-            return true;
-          }
-        }
-      }
     }
-    return false;
+    return validPrefixes.stream().filter(key::startsWith).map(propertiesByKey::get)
+        .anyMatch(Property::isSensitive);
   }
 
   private <T extends Annotation> boolean hasAnnotation(Class<T> annotationType) {
-    Logger log = LoggerFactory.getLogger(getClass());
-    try {
-      for (Annotation a : getClass().getField(name()).getAnnotations()) {
-        if (annotationType.isInstance(a)) {
-          return true;
-        }
-      }
-    } catch (SecurityException | NoSuchFieldException e) {
-      log.error(""{}"", e.getMessage(), e);
-    }
-    return false;
+    return getAnnotation(annotationType) != null;
   }
 
   private <T extends Annotation> T getAnnotation(Class<T> annotationType) {
-    Logger log = LoggerFactory.getLogger(getClass());
     try {
-      for (Annotation a : getClass().getField(name()).getAnnotations()) {
-        if (annotationType.isInstance(a)) {
-          @SuppressWarnings(""unchecked"")
-          T uncheckedA = (T) a;
-          return uncheckedA;
-        }
-      }
+      return getClass().getField(name()).getAnnotation(annotationType);
     } catch (SecurityException | NoSuchFieldException e) {
-      log.error(""{}"", e.getMessage(), e);
+      LoggerFactory.getLogger(getClass()).error(""{}"", e.getMessage(), e);","[{'comment': 'We should create a static final Logger in the class.', 'commenter': 'dlmarion'}, {'comment': ""Maybe. I assumed that this was done this way previously because this is an enum, not a regular class. There may have been a bootstrapping issue with classloading because of the way enums are handled. I doubt this is a problem, but I did notice that one benefit here is that if there's no error (and there never should be for these), then there's no reason to bootstrap logging. This results in fewer spam messages about a logger sink not being configured in my IDE and in some tests.\r\n\r\nSince I didn't actually change this here... and merely inlined the variable, I'm inclined to leave it this way."", 'commenter': 'ctubbsii'}]"
2695,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -1828,24 +1785,24 @@ public static <T> T createInstanceFromPropertyName(AccumuloConfiguration conf, P
     // Precomputing information here avoids :
     // * Computing it each time a method is called
     // * Using synch to compute the first time a method is called
-    for (Property p : Property.values()) {
-      propertiesByKey.put(p.getKey(), p);
-      if (p.getType().equals(PropertyType.PREFIX)) {
-        validPrefixes.add(p.getKey());
-      } else {
-        validProperties.add(p.getKey());
-      }
-      // exclude prefix types (prevents setting a prefix type like table.custom or
-      // table.constraint, directly, since they aren't valid properties on their own)
-      if (!p.getType().equals(PropertyType.PREFIX)
-          && p.getKey().startsWith(Property.TABLE_PREFIX.getKey())) {
-        validTableProperties.add(p.getKey());
-      }
-    }
+    Predicate<Property> isPrefix = p -> p.getType() == PropertyType.PREFIX;
+    Arrays.stream(Property.values())
+        // record all properties by key
+        .peek(p -> propertiesByKey.put(p.getKey(), p))
+        // save all the prefix properties
+        .peek(p -> {
+          if (isPrefix.test(p))
+            validPrefixes.add(p.getKey());
+        })
+        // only use the keys for the non-prefix properties from here on
+        .filter(isPrefix.negate()).map(Property::getKey)
+        // everything left is a valid property
+        .peek(validProperties::add)
+        // but some are also valid table properties
+        .filter(k -> k.startsWith(Property.TABLE_PREFIX.getKey()))
+        .forEach(validTableProperties::add);","[{'comment': ""I don't understand the impetus to change everything to use streams. When looking for information about the performance of for-loops vs streams it seems the consensus is that for-loops are faster unless you have a small set of data to iterate over or you are using parallel streams."", 'commenter': 'dlmarion'}, {'comment': 'Yeah, I debated with myself on this one. But I preferred the stream because I was able to structure it in this nice linear workflow, with comments, without the need to add some conditional checks to filter out table property prefixes.', 'commenter': 'ctubbsii'}]"
2701,core/src/main/java/org/apache/accumulo/core/file/rfile/CreateEmpty.java,"@@ -83,7 +84,7 @@ public static void main(String[] args) throws Exception {
 
   @Override
   public String keyword() {
-    return ""create-empty"";
+    return EXE_NAME;","[{'comment': ""I'm not sure I see the benefit of this... it seems to just relocate the constant from an inline literal to a new line that's really only ever needed once (see my subsequent comment about why it's not needed for the test)."", 'commenter': 'ctubbsii'}]"
2701,test/src/main/java/org/apache/accumulo/test/start/KeywordStartIT.java,"@@ -109,34 +109,35 @@ public void testCheckDuplicates() {
   public void testExpectedClasses() {
     assumeTrue(new File(System.getProperty(""user.dir"") + ""/src"").exists());
     TreeMap<String,Class<? extends KeywordExecutable>> expectSet = new TreeMap<>();
-    expectSet.put(""admin"", Admin.class);
-    expectSet.put(""check-compaction-config"", CheckCompactionConfig.class);
-    expectSet.put(""check-server-config"", CheckServerConfig.class);
-    expectSet.put(""compaction-coordinator"", CoordinatorExecutable.class);
-    expectSet.put(""compactor"", CompactorExecutable.class);
-    expectSet.put(""config-upgrade"", ConfigPropertyUpgrader.class);
-    expectSet.put(""convert-config"", ConvertConfig.class);
-    expectSet.put(""create-token"", CreateToken.class);
-    expectSet.put(""ec-admin"", ECAdmin.class);
-    expectSet.put(""gc"", GCExecutable.class);
-    expectSet.put(""generate-splits"", GenerateSplits.class);
-    expectSet.put(""help"", Help.class);
-    expectSet.put(""info"", Info.class);
-    expectSet.put(""init"", Initialize.class);
-    expectSet.put(""login-info"", LoginProperties.class);
-    expectSet.put(""manager"", ManagerExecutable.class);
-    expectSet.put(""master"", org.apache.accumulo.manager.MasterExecutable.class);
-    expectSet.put(""minicluster"", MiniClusterExecutable.class);
-    expectSet.put(""monitor"", MonitorExecutable.class);
-    expectSet.put(""rfile-info"", PrintInfo.class);
-    expectSet.put(""wal-info"", LogReader.class);
-    expectSet.put(""shell"", Shell.class);
-    expectSet.put(""tserver"", TServerExecutable.class);
-    expectSet.put(""version"", Version.class);
-    expectSet.put(""zookeeper"", ZooKeeperMain.class);
-    expectSet.put(""create-empty"", CreateEmpty.class);
-    expectSet.put(""split-large"", SplitLarge.class);
-    expectSet.put(""zoo-zap"", ZooZap.class);
+    expectSet.put(Admin.EXE_NAME, Admin.class);
+    expectSet.put(CheckCompactionConfig.EXE_NAME, CheckCompactionConfig.class);","[{'comment': ""The whole point of the checks here is to ensure that the string isn't changed. By doing this level of indirection, if the value of the constant `EXE_NAME` is changed, then this test will pass, even if it shouldn't have been changed. This test no longer guards against such changes.\r\nThis is now effectively just checking that `SomeClass.EXE_NAME == SomeClass.keyword()` for every `SomeClass`. But, that's basically tautological the way all the classes have changed."", 'commenter': 'ctubbsii'}, {'comment': ""I'll close this and submit a PR that updates the test documentation to include this. I was unclear on the reason for the test being constructed as it is. When adding / changing a command it is not obvious that the KeywordIT test must also be changed - well, at least until if fails, which is its purpose, but frustrating from a development perspective."", 'commenter': 'EdColeman'}]"
2707,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -114,16 +120,27 @@ public static byte[] createPass(byte[] password) throws AccumuloException {
     return cryptHash.getBytes(UTF_8);
   }
 
+  private static final Map<List<Byte>,String> CHECKED_CRYPT_PASSWORDS =","[{'comment': ""Hadoop's Text or Accumulo's ByteSequence would probably be more efficient for the key than List<Byte>.\r\n\r\n```suggestion\r\n  private static final Map<ByteSequence,String> CHECKED_CRYPT_PASSWORDS =\r\n```\r\n\r\nAlso thinking a guava cache w/ extremely short TTL (like 1 min) would be better than fixed size lrumap.  An extremely short duration should still amortize the cost nicely.\r\n"", 'commenter': 'keith-turner'}, {'comment': ""I'll wait to hear back from @ctubbsii before I update this. The short TTL approach may be better in the case of a password change."", 'commenter': 'dlmarion'}, {'comment': ""> Hadoop's Text or Accumulo's ByteSequence would probably be more efficient for the key than List.\r\n\r\nWhy do you say that? LRUMap is based on a hash map, and the list returned by `Bytes.asList()` implements `hashCode()`."", 'commenter': 'dlmarion'}, {'comment': '> Why do you say that?\r\n\r\nAn array of Byte is an array of object pointers to Byte objects, so unless the JVM does something to optimize it then its much less efficient than an array of byte primitives w/o any pointers.   Text and ByteSequence just wrap byte[].', 'commenter': 'keith-turner'}, {'comment': 'Maybe the Bytes.asList() function makes everything efficient under the covers.  I did not really look at that.  It would depend on its hashcode and equals functions.  If those are just using byte[], then that would address my concerns.', 'commenter': 'keith-turner'}]"
2707,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -114,16 +120,27 @@ public static byte[] createPass(byte[] password) throws AccumuloException {
     return cryptHash.getBytes(UTF_8);
   }
 
+  private static final Map<List<Byte>,String> CHECKED_CRYPT_PASSWORDS =
+      Collections.synchronizedMap(new LRUMap<>(16));
+
   public static boolean checkCryptPass(byte[] password, byte[] zkData) {
+    List<Byte> key = Bytes.asList(password);
     String zkDataString = new String(zkData, UTF_8);
+    if (CHECKED_CRYPT_PASSWORDS.getOrDefault(key, """").equals(zkDataString)) {
+      return true;
+    }
     String cryptHash;
     try {
       cryptHash = Crypt.crypt(password, zkDataString);","[{'comment': 'Feels more safe to me if we explicitly cache the input and output of the cryptHash function. So something like the followin psuedocode (not sure of the best way to write it w/o looking at cache APIs)\r\n\r\n```\r\n   var key = password  + zkData\r\n   cryptHash = getFromCache(key)\r\n   if(cryptHash == null) {\r\n       cryptHash = Crypt.crypt(password, zkDataString);\r\n       putInCache(key, cryptHash);\r\n   }\r\n   \r\n   boolean matches = MessageDigest.isEqual(zkData, cryptHash.getBytes(UTF_8));\r\n```\r\n', 'commenter': 'keith-turner'}, {'comment': 'I was thinking that we would not want to store the output of any crypt function. Instead, I was just capturing the fact that two pieces of information had returned a true result in the recent past, and using it to short circuit the method. Is there any benefit to recalculating whether the two hashes are equal?', 'commenter': 'dlmarion'}, {'comment': 'Was just thinking very literally and trying to avoid shortcuts since its security related.  The function takes two inputs and gives a single output,  so was thinking of caching that functions input and output exactly rather than caching a partial was more secure.  Caching only a single input parameter (the password) felt kind dicey.  Like what if two account have the same password with different salts, then what will happen with the cache?', 'commenter': 'keith-turner'}, {'comment': ""> Ah, I see. I didn't think about the password reuse case. I'll adjust."", 'commenter': 'dlmarion'}]"
2707,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -114,16 +119,38 @@ public static byte[] createPass(byte[] password) throws AccumuloException {
     return cryptHash.getBytes(UTF_8);
   }
 
+  private static final Cache<String,String> CRYPT_PASSWORD_CACHE = Caffeine.newBuilder()
+      .scheduler(Scheduler.systemScheduler()).expireAfterWrite(3, TimeUnit.SECONDS).build();
+
   public static boolean checkCryptPass(byte[] password, byte[] zkData) {
     String zkDataString = new String(zkData, UTF_8);
-    String cryptHash;
-    try {
-      cryptHash = Crypt.crypt(password, zkDataString);
-    } catch (IllegalArgumentException e) {
-      log.error(""Unrecognized hash format"", e);
-      return false;
+    String key = new String(password, UTF_8) + zkDataString;
+    String cryptHash = CRYPT_PASSWORD_CACHE.getIfPresent(key);
+    boolean matches = false;
+    if (cryptHash != null) {
+      matches = MessageDigest.isEqual(zkData, cryptHash.getBytes(UTF_8));
+      // If matches then zkData has not changed from when it was put into the cache
+      if (matches) {
+        return true;
+      } else {
+        // remove the non-matching entry from the cache
+        CRYPT_PASSWORD_CACHE.invalidate(key);
+      }
+    }
+    // Either !matches or was not cached
+    if (!matches) {
+      try {
+        cryptHash = Crypt.crypt(password, zkDataString);
+      } catch (IllegalArgumentException e) {
+        log.error(""Unrecognized hash format"", e);
+        return false;
+      }
+    }
+    matches = MessageDigest.isEqual(zkData, cryptHash.getBytes(UTF_8));
+    if (matches) {
+      CRYPT_PASSWORD_CACHE.put(key, cryptHash);","[{'comment': 'Looking at the `if (!matches) {` case it seems like if execution got to that point in the code that it would always be true.  If that is the case, then could remove `if (!matches) {`.\r\n\r\n```suggestion\r\n    if (cryptHash != null) {\r\n      // If matches then zkData has not changed from when it was put into the cache\r\n      if ( MessageDigest.isEqual(zkData, cryptHash.getBytes(UTF_8))) {\r\n        return true;\r\n      } else {\r\n        // remove the non-matching entry from the cache\r\n        CRYPT_PASSWORD_CACHE.invalidate(key);\r\n      }\r\n    }\r\n      try {\r\n        cryptHash = Crypt.crypt(password, zkDataString);\r\n      } catch (IllegalArgumentException e) {\r\n        log.error(""Unrecognized hash format"", e);\r\n        return false;\r\n      }\r\n    boolean matches = MessageDigest.isEqual(zkData, cryptHash.getBytes(UTF_8));\r\n    if (matches) {\r\n      CRYPT_PASSWORD_CACHE.put(key, cryptHash);\r\n```', 'commenter': 'keith-turner'}, {'comment': ""I think that's true. I'll apply your changes manually, it doesn't look like it will pass the formatter check\r\n"", 'commenter': 'dlmarion'}]"
2707,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -114,16 +119,34 @@ public static byte[] createPass(byte[] password) throws AccumuloException {
     return cryptHash.getBytes(UTF_8);
   }
 
+  private static final Cache<String,String> CRYPT_PASSWORD_CACHE = Caffeine.newBuilder()
+      .scheduler(Scheduler.systemScheduler()).expireAfterWrite(3, TimeUnit.SECONDS).build();","[{'comment': ""Wouldn't we want expireAfterAccess rather than write? If it's still frequently being accessed, it'd be good to keep it around, regardless of the last time it was written to the cache.\r\n\r\nIt also might be good to put a cap on the size of this cache.\r\n\r\nFor this to be of benefit, we'd have to be creating a lot of connections pretty quickly. I'm not sure how realistic that would be. Does 3 seconds cover a realistic scenario? Should it be 10? More? Maybe a size-limited LRU cache would be better?\r\n"", 'commenter': 'ctubbsii'}, {'comment': ""> For this to be of benefit, we'd have to be creating a lot of connections pretty quickly. I'm not sure how realistic that would be.\r\n\r\nI think that is what is described in #2700 as part of a test. I could see real work implications, typically on application startup where you have a thundering herd type situation.\r\n\r\n> Wouldn't we want expireAfterAccess rather than write? If it's still frequently being accessed, it'd be good to keep it around, regardless of the last time it was written to the cache.\r\n\r\nThis was based on an earlier comment about have really short times in the cache. I think at this point it could be longer and based on access rather than write.\r\n\r\n> I'm not sure this is worth it. The underlying issue seems highly dependent on specific JVM versions, and specific JCE providers and their native instruction hardware optimizations.\r\n\r\nI would agree, but I would also say that we are doing work that we might not have to do.\r\n\r\n"", 'commenter': 'dlmarion'}, {'comment': 'In 67e6332 I changed the expiration criteria from afterWrite to afterAccess, put a max size on the cache to 64, and changed the expiration time from 3s to 1m based on a comment earlier by @keith-turner \r\n', 'commenter': 'dlmarion'}, {'comment': ""> > For this to be of benefit, we'd have to be creating a lot of connections pretty quickly. I'm not sure how realistic that would be.\r\n> \r\n> I think that is what is described in #2700 as part of a test. I could see real work implications, typically on application startup where you have a thundering herd type situation.\r\n\r\nI suppose. I'm just not sure how well the observations of lock contention or brief periods of high CPU usage translate to end user experience, or overall user application performance. This isn't much complexity, but in general, I'm resistant to specialized code in our application that tries to work around performance limitations in user hardware/external libraries, just because of separation of responsibilities. I'm also resistant to workarounds that cater to specific deployment use cases when better deployment decisions (like choosing a better JCE library or deploying using hardware with crypto extensions) could also address the problem. This hits on both those points, and it's not clear it's even noticeable to users. But, the workaround is simple enough, so maybe it's okay."", 'commenter': 'ctubbsii'}, {'comment': ""@keith-turner thinks he has a solution for the other locking issues he saw in #2700 in the ScanServer. I'll suggest to him that he test with his solution first to see if this is still an issue, then test with this to see if it solves it. I'll wait to merge this until we hear back."", 'commenter': 'dlmarion'}]"
2707,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -114,16 +119,34 @@ public static byte[] createPass(byte[] password) throws AccumuloException {
     return cryptHash.getBytes(UTF_8);
   }
 
+  private static final Cache<String,String> CRYPT_PASSWORD_CACHE = Caffeine.newBuilder()
+      .scheduler(Scheduler.systemScheduler()).expireAfterWrite(3, TimeUnit.SECONDS).build();
+
   public static boolean checkCryptPass(byte[] password, byte[] zkData) {
-    String zkDataString = new String(zkData, UTF_8);
-    String cryptHash;
+    final String zkDataString = new String(zkData, UTF_8);
+    final String key = new String(password, UTF_8) + zkDataString;","[{'comment': ""Is there a risk of a contrived password manipulating this cache key? If so, can it be avoided by swapping the structured zkDataString and the password? What happens if the password byte array can't be encoded as UTF-8? Would it be better to use a key based on a byte buffer or Text object, or a fast hex-encoding of the byte array?"", 'commenter': 'ctubbsii'}, {'comment': 'I changed the key type from String to ByteBuffer in 67e6332', 'commenter': 'dlmarion'}]"
2707,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKSecurityTool.java,"@@ -114,16 +120,36 @@ public static byte[] createPass(byte[] password) throws AccumuloException {
     return cryptHash.getBytes(UTF_8);
   }
 
+  private static final Cache<ByteBuffer,String> CRYPT_PASSWORD_CACHE =
+      Caffeine.newBuilder().scheduler(Scheduler.systemScheduler())
+          .expireAfterAccess(Duration.ofMinutes(1)).initialCapacity(4).maximumSize(64).build();
+
   public static boolean checkCryptPass(byte[] password, byte[] zkData) {","[{'comment': ""A brief comment stating the overall intent would be nice. The logic makes sense now, but I realized that in future, somebody could wonder why we're bothering to do this.\r\n\r\n```suggestion\r\n\r\n  // This uses a cache to avoid repeated expensive calls to Crypt.crypt for recent inputs\r\n  public static boolean checkCryptPass(byte[] password, byte[] zkData) {\r\n```"", 'commenter': 'ctubbsii'}]"
2707,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -157,6 +157,13 @@ public enum Property {
       ""The permission handler class that accumulo will use to determine if a ""
           + ""user has privilege to perform an action"",
       ""1.5.0""),
+  INSTANCE_SECURITY_ZK_AUTH_CACHE_ENABLED(""instance.security.authenticator.zk.cache.enabled"",
+      ""true"", PropertyType.BOOLEAN,
+      ""Enables the temporary caching of successfully authenticated""
+          + "" user passwords in org.apache.accumulo.server.security.handler.ZKAuthenticator to""
+          + "" mitigate the performance penalties of having to compute the password hash""
+          + "" on every API call"",
+      ""2.1.0""),","[{'comment': ""Since this is an internal optimization, I don't want to bloat users with extra configuration. If in future, we evaluate this again, and it's no longer needed due to improvements in the JDK or in commons-codec, then this property will become OBE, and we'll have churn removing it.\r\n\r\nAlso, it only applies to ZKAuthenticator, which is itself configurable. If they don't want this, they can just replace ZKAuthenticator with a different custom authenticator. So, there's already a control knob for this that is available to users. This option is effectively redundant, and it's confusing if they've used a custom authenticator already.\r\n"", 'commenter': 'ctubbsii'}, {'comment': 'I reverted the commit that added this property', 'commenter': 'dlmarion'}]"
2709,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooCacheFactory.java,"@@ -18,18 +18,17 @@
  */
 package org.apache.accumulo.fate.zookeeper;
 
-import java.util.HashMap;
-import java.util.Map;
-
 import org.apache.accumulo.core.singletons.SingletonManager;
 import org.apache.accumulo.core.singletons.SingletonService;
 
+import com.github.benmanes.caffeine.cache.Cache;
+import com.github.benmanes.caffeine.cache.Caffeine;
+
 /**
  * A factory for {@link ZooCache} instances.
  */
 public class ZooCacheFactory {
-  // TODO: make this better - LRU, soft references, ...
-  private static Map<String,ZooCache> instances = new HashMap<>();
+  private static Cache<String,ZooCache> instances = Caffeine.newBuilder().maximumSize(5).build();","[{'comment': 'Why a max size of 5?  What determines that number?', 'commenter': 'EdColeman'}, {'comment': ""For Caffeine, specifying a size turns the Cache into an LRU Cache (see https://github.com/ben-manes/caffeine/wiki/Eviction#size-based). As for the number 5, it was arbitrary. I don't know how many ZK's a ClientContext actually communicates with, it's probably 1. But a Cache of 1 didn't make sense. What's your thought?"", 'commenter': 'dlmarion'}, {'comment': ""I don't know - I think the use-case for multiple instances in a VM is to support testing. Normally that number should be 1.  Does a caffeine cache pre-allocate entries up to max size on construction? If not, then a larger number could be appropriate?"", 'commenter': 'EdColeman'}, {'comment': 'It does not (`initialCapacity(c)` would do that).', 'commenter': 'ben-manes'}]"
2709,core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooCacheFactory.java,"@@ -119,7 +118,8 @@ public ZooCache getNewZooCache(String zooKeepers, int sessionTimeout) {
    */
   void reset() {
     synchronized (instances) {
-      instances.clear();
+      instances.invalidateAll();
+      instances.cleanUp();","[{'comment': 'fyi, `cleanUp` is implicit if clearing the cache so that call is not needed.', 'commenter': 'ben-manes'}]"
2712,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -244,9 +244,12 @@ public enum Property {
   GENERAL_OPENTELEMETRY_ENABLED(""general.opentelemetry.enabled"", ""false"", PropertyType.BOOLEAN,
       ""Enables tracing functionality using OpenTelemetry (assuming OpenTelemetry is configured)."",
       ""2.1.0""),
+  @Deprecated","[{'comment': 'This should also have a `@ReplacedBy` annotation', 'commenter': 'dlmarion'}, {'comment': 'Fixed in commit b086f0234deac1b629713aa488a5fad2944de665', 'commenter': 'kitswas'}]"
2712,core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java,"@@ -515,7 +519,7 @@ public Future<?> submit(Runnable task) {
   public ScheduledThreadPoolExecutor
       createGeneralScheduledExecutorService(AccumuloConfiguration conf) {
     return (ScheduledThreadPoolExecutor) createExecutorService(conf,
-        Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE, true);
+        Property.GENERAL_THREADPOOL_SIZE, true);","[{'comment': ""I'm not sure this is sufficient. The old property is deprecated, not removed. This means that users may have the old property, and not the new one, in their configuration. I think you need to check for both and maybe use the one that has a larger value."", 'commenter': 'dlmarion'}, {'comment': 'May want to look at what other properties are doing. I thought we had a replacement mechanism that other code had used.', 'commenter': 'ctubbsii'}, {'comment': ""> I'm not sure this is sufficient. The old property is deprecated, not removed. This means that users may have the old property, and not the new one, in their configuration. I think you need to check for both and maybe use the one that has a larger value.\r\n\r\nSince `createGeneralScheduledExecutorService()` creates a new service, I felt it should use the new property `GENERAL_THREADPOOL_SIZE` to do so.\r\n\r\nYou suggestion, to use the one that has a larger value, is valid.\r\n\r\nAlternatively, for backward compatibility, we can have an overloaded version of the function that uses the new property and leave the old function as it is.\r\n"", 'commenter': 'kitswas'}, {'comment': '> May want to look at what other properties are doing. I thought we had a replacement mechanism that other code had used.\r\n\r\n```\r\nserverConfig.resolve(Property.MANAGER_RENAME_THREADS, Property.MANAGER_BULK_RENAME_THREADS)\r\n```\r\nThe property `MANAGER_BULK_RENAME_THREADS` seems to be undergoing deprecation in similar versions.\r\n', 'commenter': 'kitswas'}, {'comment': '@ctubbsii \r\nhttps://github.com/apache/accumulo/pull/2712#discussion_r874805835\r\nShould I use this?', 'commenter': 'kitswas'}, {'comment': 'That looks like what I was thinking.', 'commenter': 'ctubbsii'}]"
2712,core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java,"@@ -512,10 +516,16 @@ public Future<?> submit(Runnable task) {
    * If you need the server-side shared ScheduledThreadPoolExecutor, then use
    * ServerContext.getScheduledExecutor()
    */
+  @SuppressWarnings(""deprecation"")","[{'comment': 'You can narrow this warnings suppression to a variable rather than the whole method by doing something like:\r\n```java\r\nProperty prop = conf.resolve(Property.GENERAL_THREADPOOL_SIZE, Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE);\r\n```\r\nand then put the warnings suppression only on that variable, then use that variable in the next call to createExecutorService.', 'commenter': 'ctubbsii'}, {'comment': 'I will do this when Exception Handling is no longer required. \r\nOtherwise, it only serves to clutter the code.\r\n', 'commenter': 'kitswas'}, {'comment': 'I did it in 58e8ec5b960dd2f6a4c20b4b0c2212ee67b0e5e6', 'commenter': 'ctubbsii'}]"
2716,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -51,64 +51,91 @@ public class GarbageCollectionAlgorithm {
 
   private static final Logger log = LoggerFactory.getLogger(GarbageCollectionAlgorithm.class);
 
+  /**
+   * This method takes a file or directory path and returns a relative path in 1 of 2 forms:
+   *
+   * <pre>
+   *      1- For files: table-id/tablet-directory/filename.rf
+   *      2- For directories: table-id/tablet-directory
+   * </pre>
+   *
+   * For example, for full file path like hdfs://foo:6000/accumulo/tables/4/t0/F000.rf it will
+   * return 4/t0/F000.rf. For a directory that already is relative, like 4/t0, it will just return
+   * the original path. This method will also remove prefixed relative path.
+   */
   private String makeRelative(String path, int expectedLen) {
     String relPath = path;
 
+    // remove prefixed old relative path
     if (relPath.startsWith(""../""))
       relPath = relPath.substring(3);
 
+    // remove trailing slash
     while (relPath.endsWith(""/""))
       relPath = relPath.substring(0, relPath.length() - 1);
 
+    // remove beginning slash
     while (relPath.startsWith(""/""))
       relPath = relPath.substring(1);
 
-    String[] tokens = relPath.split(""/"");
-
-    // handle paths like a//b///c
-    boolean containsEmpty = false;
-    for (String token : tokens) {
-      if (token.equals("""")) {
-        containsEmpty = true;
-        break;
-      }
-    }
-
-    if (containsEmpty) {
-      ArrayList<String> tmp = new ArrayList<>();
-      for (String token : tokens) {
-        if (!token.equals("""")) {
-          tmp.add(token);
-        }
-      }
-
-      tokens = tmp.toArray(new String[tmp.size()]);
-    }
+    String[] tokens = removeEmptyTokensFromPath(relPath);
 
     if (tokens.length > 3 && path.contains("":"")) {
+      // full file path like hdfs://foo:6000/accumulo/tables/4/t0/F000.rf
       if (tokens[tokens.length - 4].equals(Constants.TABLE_DIR)
           && (expectedLen == 0 || expectedLen == 3)) {
+        // return the last 3 tokens after tables, like 4/t0/F000.rf
         relPath = tokens[tokens.length - 3] + ""/"" + tokens[tokens.length - 2] + ""/""
             + tokens[tokens.length - 1];
       } else if (tokens[tokens.length - 3].equals(Constants.TABLE_DIR)
           && (expectedLen == 0 || expectedLen == 2)) {
+        // return the last 2 tokens after tables, like 4/t0
         relPath = tokens[tokens.length - 2] + ""/"" + tokens[tokens.length - 1];
       } else {
-        throw new IllegalArgumentException(path);
+        throw new IllegalArgumentException(""Failed to make path relative. Bad reference: "" + path);
       }
     } else if (tokens.length == 3 && (expectedLen == 0 || expectedLen == 3)
         && !path.contains("":"")) {
+      // we already have a relative path so return it, like 4/t0/F000.rf
       relPath = tokens[0] + ""/"" + tokens[1] + ""/"" + tokens[2];
     } else if (tokens.length == 2 && (expectedLen == 0 || expectedLen == 2)
         && !path.contains("":"")) {
+      // return the last 2 tokens of the relative path, like 4/t0
       relPath = tokens[0] + ""/"" + tokens[1];
     } else {
-      throw new IllegalArgumentException(path);
+      throw new IllegalArgumentException(""Failed to make path relative. Bad reference: "" + path);
     }
 
     return relPath;
   }
 
+  /**
+   * Handle paths like a//b///c by dropping the empty tokens.
+   */
+  private String[] removeEmptyTokensFromPath(String relPath) {
+    String[] tokens = relPath.split(""/"");
+
+    boolean containsEmpty = false;
+    for (String token : tokens) {
+      if (token.equals("""")) {
+        containsEmpty = true;
+        break;
+      }
+    }
+
+    if (containsEmpty) {","[{'comment': 'This can be shortened quite a bit:\r\n```suggestion\r\n    if (Arrays.stream(tokens).anyMatch(""""::equals)) {\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Nice!', 'commenter': 'milleruntime'}, {'comment': 'I dropped this method in favor of your other suggestion.', 'commenter': 'milleruntime'}]"
2716,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -51,64 +51,91 @@ public class GarbageCollectionAlgorithm {
 
   private static final Logger log = LoggerFactory.getLogger(GarbageCollectionAlgorithm.class);
 
+  /**
+   * This method takes a file or directory path and returns a relative path in 1 of 2 forms:
+   *
+   * <pre>
+   *      1- For files: table-id/tablet-directory/filename.rf
+   *      2- For directories: table-id/tablet-directory
+   * </pre>
+   *
+   * For example, for full file path like hdfs://foo:6000/accumulo/tables/4/t0/F000.rf it will
+   * return 4/t0/F000.rf. For a directory that already is relative, like 4/t0, it will just return
+   * the original path. This method will also remove prefixed relative path.
+   */
   private String makeRelative(String path, int expectedLen) {
     String relPath = path;
 
+    // remove prefixed old relative path
     if (relPath.startsWith(""../""))
       relPath = relPath.substring(3);
 
+    // remove trailing slash
     while (relPath.endsWith(""/""))
       relPath = relPath.substring(0, relPath.length() - 1);
 
+    // remove beginning slash
     while (relPath.startsWith(""/""))
       relPath = relPath.substring(1);
 
-    String[] tokens = relPath.split(""/"");
-
-    // handle paths like a//b///c
-    boolean containsEmpty = false;
-    for (String token : tokens) {
-      if (token.equals("""")) {
-        containsEmpty = true;
-        break;
-      }
-    }
-
-    if (containsEmpty) {
-      ArrayList<String> tmp = new ArrayList<>();
-      for (String token : tokens) {
-        if (!token.equals("""")) {
-          tmp.add(token);
-        }
-      }
-
-      tokens = tmp.toArray(new String[tmp.size()]);
-    }
+    String[] tokens = removeEmptyTokensFromPath(relPath);","[{'comment': 'On second thought, this entire method can go away in favor of a relatively simple pipeline that strips out the empty components directly into an array result:\r\n```suggestion\r\n    String[] tokens = stream(relPath.split(""/"")).filter(not(""""::equals)).toArray(String[]::new);\r\n```\r\n\r\n(`stream()` is a statically imported `Arrays.stream()` and `not` is `Predicate.not`; static import is optional, but it becomes an easy to read one-liner if you do)', 'commenter': 'ctubbsii'}, {'comment': 'Cool, I will take a look, thanks.', 'commenter': 'milleruntime'}]"
2724,core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java,"@@ -281,7 +281,7 @@ static class BloomFilterLoader {
 
     /**
      * Prevent potential CRLF injection into logs from read in user data See","[{'comment': 'Super small thing. I saw in all of the other changes you made you added a period  between data and See if it did not have it already. Just for consistency sake I figured I would point it out.\r\n\r\n```suggestion\r\n     * Prevent potential CRLF injection into logs from read in user data. See\r\n```', 'commenter': 'foster33'}, {'comment': 'Thanks!', 'commenter': 'milleruntime'}]"
2724,core/src/main/java/org/apache/accumulo/core/client/Accumulo.java,"@@ -70,8 +70,9 @@ private Accumulo() {}
    * </code>
    * </pre>
    *
-   * For a list of all client properties see the documentation on the Accumulo website:
-   * https://accumulo.apache.org/docs/2.x/configuration/client-properties
+   * For a list of all client properties see the documentation on the","[{'comment': '```suggestion\r\n   * For a list of all client properties, see the documentation on the\r\n```', 'commenter': 'ctubbsii'}]"
2724,core/src/main/java/org/apache/accumulo/core/client/Accumulo.java,"@@ -90,8 +91,9 @@ public static AccumuloClient.PropertyOptions<AccumuloClient> newClient() {
    * </code>
    * </pre>
    *
-   * For a list of all client properties see the documentation on the Accumulo website:
-   * https://accumulo.apache.org/docs/2.x/configuration/client-properties
+   * For a list of all client properties see the documentation on the","[{'comment': '```suggestion\r\n   * For a list of all client properties, see the documentation on the\r\n```', 'commenter': 'ctubbsii'}]"
2724,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -1620,8 +1620,8 @@ public void importTable(String tableName, Set<String> importDirs)
   }
 
   /**
-   * Prevent potential CRLF injection into logs from read in user data See
-   * https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS
+   * Prevent potential CRLF injection into logs from read in user data. See
+   * <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug description</a>","[{'comment': ""This also works, and doesn't require coming up with a good text description for the link:\r\n\r\n```suggestion\r\n   * Prevent potential CRLF injection into logs from read in user data.\r\n   *\r\n   * @see https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS\r\n```"", 'commenter': 'ctubbsii'}, {'comment': ""It didn't like that...\r\n<pre>\r\n[WARNING] src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java:[1625] (javadoc) NonEmptyAtclauseDescription: Javadoc comment at column 16 has parse error. Details: mismatched input ':' expecting <EOF> while parsing JAVADOC\r\n</pre>"", 'commenter': 'milleruntime'}, {'comment': 'Weird, then at least could include the word ""the"" for readability.\r\n\r\n```suggestion\r\n   * Prevent potential CRLF injection into logs from read in user data. See\r\n   * the <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug description</a>\r\n```', 'commenter': 'ctubbsii'}]"
2724,server/manager/src/main/java/org/apache/accumulo/manager/tableOps/tableImport/ImportTable.java,"@@ -53,8 +53,9 @@
 
 /**
  * Serialization updated for supporting multiple volumes in import table from 1L to 2L See:
- * https://github.com/apache/accumulo/issues/1053 https://github.com/apache/accumulo/pull/1060
- * https://github.com/apache/accumulo/issues/1576
+ * <a href=""https://github.com/apache/accumulo/issues/1053"">1053</a>
+ * <a href=""https://github.com/apache/accumulo/pull/1060"">1060</a>
+ * <a href=""https://github.com/apache/accumulo/issues/1576"">1576</a>","[{'comment': ""None of these are needed. We don't need to embed references to historical changes to the code in the code itself. The git history does that already. This would get out of hand if we did this more often. These references can just be removed."", 'commenter': 'ctubbsii'}, {'comment': 'I think I may have done this. I was trying to give context to the Version change since keeping track of serialized versions can be a pain. Maybe would be better to link with AccumloDataVersion or something else.', 'commenter': 'milleruntime'}, {'comment': '```suggestion\r\n```', 'commenter': 'ctubbsii'}]"
2724,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -1620,8 +1620,9 @@ public void importTable(String tableName, Set<String> importDirs)
   }
 
   /**
-   * Prevent potential CRLF injection into logs from read in user data See
-   * https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS
+   * Prevent potential CRLF injection into logs from read in user data.
+   *
+   * @see <a href=https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS/>","[{'comment': '```suggestion\r\n   * Prevent potential CRLF injection into logs from read in user data. See\r\n   * <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug description</a>\r\n```', 'commenter': 'DomGarguilo'}]"
2724,core/src/main/java/org/apache/accumulo/core/iterators/user/IntersectingIterator.java,"@@ -59,7 +59,7 @@
  * {@link #init(SortedKeyValueIterator, Map, IteratorEnvironment)} method.
  *
  * An example of using the IntersectingIterator is available at
- * https://github.com/apache/accumulo-examples/blob/main/docs/shard.md
+ * <a href=""https://github.com/apache/accumulo-examples/blob/main/docs/shard.md"">examples</a>","[{'comment': '```suggestion\r\n * <a href=""https://github.com/apache/accumulo-examples/blob/main/docs/shard.md"">the examples repo</a>\r\n```', 'commenter': 'ctubbsii'}]"
2724,minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java,"@@ -122,8 +122,8 @@ public Entry<Integer,String> execWithStdout(Class<?> clz, String[] args) throws
   }
 
   /**
-   * Prevent potential CRLF injection into logs from read in user data See
-   * https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS
+   * Prevent potential CRLF injection into logs from read in user data. See
+   * <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug report</a>","[{'comment': '```suggestion\r\n   * the <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug description</a>\r\n```', 'commenter': 'ctubbsii'}]"
2724,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/problems/ProblemsResource.java,"@@ -174,7 +174,7 @@ public void clearDetailsProblems(
 
   /**
    * Prevent potential CRLF injection into logs from read in user data. See
-   * https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS
+   * <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug report</a>","[{'comment': '```suggestion\r\n   * the <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug description</a>\r\n```', 'commenter': 'ctubbsii'}]"
2724,shell/src/main/java/org/apache/accumulo/shell/Shell.java,"@@ -673,8 +673,8 @@ public String getDefaultPrompt() {
   }
 
   /**
-   * Prevent potential CRLF injection into logs from read in user data See
-   * https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS
+   * Prevent potential CRLF injection into logs from read in user data. See
+   * <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug report</a>","[{'comment': '```suggestion\r\n   * the <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug description</a>\r\n```', 'commenter': 'ctubbsii'}]"
2724,core/src/main/java/org/apache/accumulo/core/iterators/user/IntersectingIterator.java,"@@ -59,7 +59,8 @@
  * {@link #init(SortedKeyValueIterator, Map, IteratorEnvironment)} method.
  *
  * An example of using the IntersectingIterator is available at","[{'comment': '```suggestion\r\n * An example of using the IntersectingIterator is available in\r\n```', 'commenter': 'ctubbsii'}]"
2724,core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java,"@@ -280,8 +280,8 @@ static class BloomFilterLoader {
     }
 
     /**
-     * Prevent potential CRLF injection into logs from read in user data See
-     * https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS
+     * Prevent potential CRLF injection into logs from read in user data. See
+     * <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug description</a>","[{'comment': '```suggestion\r\n     * the <a href=""https://find-sec-bugs.github.io/bugs.htm#CRLF_INJECTION_LOGS"">bug description</a>\r\n```', 'commenter': 'ctubbsii'}]"
2724,server/manager/src/main/java/org/apache/accumulo/manager/tableOps/tableImport/ImportTable.java,"@@ -53,8 +53,9 @@
 
 /**
  * Serialization updated for supporting multiple volumes in import table from 1L to 2L See:","[{'comment': '```suggestion\r\n * Serialization updated for supporting multiple volumes in import table from 1L to 2L.\r\n```', 'commenter': 'ctubbsii'}]"
2751,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java,"@@ -0,0 +1,540 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZINSTANCES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_NAME;
+import static org.apache.accumulo.core.Constants.ZROOT;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAME;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAMESPACE;
+
+import java.io.BufferedWriter;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.TreeSet;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.clientImpl.Namespace;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReader;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+@AutoService(KeywordExecutable.class)
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_OUT"",
+    justification = ""app is run in same security context as user providing the filename"")
+public class ZooInfoViewer implements KeywordExecutable {
+  public static final DateTimeFormatter tsFormat =
+      DateTimeFormatter.ISO_OFFSET_DATE_TIME.withZone(ZoneId.from(ZoneOffset.UTC));
+  private static final Logger log = LoggerFactory.getLogger(ZooInfoViewer.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  private static final String INDENT = ""  "";
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooInfoViewer() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooInfoViewer().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-info-viewer"";
+  }
+
+  @Override
+  public String description() {
+    return ""view Accumulo instance and property information stored in ZooKeeper"";
+  }
+
+  @Override
+  public void execute(String[] args) {
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), args);
+
+    log.info(""print ids map: {}"", opts.printIdMap);
+    log.info(""print properties: {}"", opts.printProps);
+    log.info(""print instances: {}"", opts.printInstanceIds);
+
+    ZooReader zooReader = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zooReader, opts);
+    try {
+      generateReport(iid, opts, zooReader);
+    } catch (FileNotFoundException ex) {
+      throw new IllegalStateException(""Failed to generate ZooKeeper info report"", ex);","[{'comment': ""UncheckedIOException would be more appropriate here. However, there's no point in catching it at all. You can just throw it. The KeywordExecutable interface has a `throws Exception` in the interface API. All this does is add extra `caused by` clauses, making it harder for the user to parse the stack trace.\r\n"", 'commenter': 'ctubbsii'}]"
2751,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java,"@@ -0,0 +1,540 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZINSTANCES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_NAME;
+import static org.apache.accumulo.core.Constants.ZROOT;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAME;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAMESPACE;
+
+import java.io.BufferedWriter;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.TreeSet;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.clientImpl.Namespace;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReader;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+@AutoService(KeywordExecutable.class)
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_OUT"",
+    justification = ""app is run in same security context as user providing the filename"")
+public class ZooInfoViewer implements KeywordExecutable {
+  public static final DateTimeFormatter tsFormat =","[{'comment': 'does this constant need to be public?', 'commenter': 'ctubbsii'}]"
2751,server/base/src/test/java/org/apache/accumulo/server/conf/util/ZooInfoViewerTest.java,"@@ -0,0 +1,437 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZINSTANCES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_NAME;
+import static org.apache.accumulo.core.Constants.ZROOT;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAME;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAMESPACE;
+import static org.easymock.EasyMock.anyObject;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.eq;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.BufferedReader;
+import java.io.FileReader;
+import java.time.Instant;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.UUID;
+
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReader;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.conf.codec.VersionedPropCodec;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.zookeeper.Watcher;
+import org.apache.zookeeper.data.Stat;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+public class ZooInfoViewerTest {
+
+  private final Logger log = LoggerFactory.getLogger(ZooInfoViewerTest.class);
+
+  private final VersionedPropCodec propCodec = VersionedPropCodec.getDefault();
+
+  @Test
+  public void simpleOutput() {
+    // StringWriter writer = new StringWriter();
+  }
+
+  @Test
+  public void optionsAllDefault() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    assertTrue(opts.printAllProps());
+    assertTrue(opts.printSysProps());
+    assertTrue(opts.printNamespaceProps());
+    assertTrue(opts.printTableProps());
+  }
+
+  @Test
+  public void onlySys() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""--system""});
+
+    assertFalse(opts.printAllProps());
+    assertTrue(opts.printSysProps());
+    assertFalse(opts.printNamespaceProps());
+    assertFalse(opts.printTableProps());
+  }
+
+  @Test
+  public void onlyNamespaces() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""-ns"", ""ns1"", ""ns2""});
+
+    assertFalse(opts.printAllProps());
+    assertFalse(opts.printSysProps());
+    assertTrue(opts.printNamespaceProps());
+    assertEquals(2, opts.getNamespaces().size());
+    assertFalse(opts.printTableProps());
+    assertEquals(0, opts.getTables().size());
+  }
+
+  @Test
+  public void allLongOpts() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(),
+        new String[] {""--system"", ""--namespaces"", ""ns1"", ""ns2"", ""--tables"", ""tb1"", ""tbl2""});
+
+    log.debug(""namespaces: {}"", opts.getNamespaces());
+    log.debug(""tables: {}"", opts.getTables());
+
+    assertFalse(opts.printAllProps());
+    assertTrue(opts.printSysProps());
+    assertTrue(opts.printNamespaceProps());
+    assertTrue(opts.printTableProps());
+    assertEquals(2, opts.getNamespaces().size());
+    assertEquals(2, opts.getTables().size());
+  }
+
+  @Test
+  public void allOpts() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""-t"", ""tb1"", ""tbl2""});
+
+    assertFalse(opts.printAllProps());
+    assertFalse(opts.printSysProps());
+    assertFalse(opts.printNamespaceProps());
+    assertEquals(0, opts.getNamespaces().size());
+    assertTrue(opts.printTableProps());
+    assertEquals(2, opts.getTables().size());
+  }
+
+  @Test
+  public void fetchInstancesFromZk() throws Exception {
+
+    String instAName = ""INST_A"";
+    InstanceId instA = InstanceId.of(UUID.randomUUID());
+    String instBName = ""INST_B"";
+    InstanceId instB = InstanceId.of(UUID.randomUUID());
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    String namePath = ZROOT + ZINSTANCES;
+    expect(zooReader.getChildren(eq(namePath))).andReturn(List.of(instAName, instBName)).once();
+    expect(zooReader.getData(eq(namePath + ""/"" + instAName)))
+        .andReturn(instA.canonical().getBytes(UTF_8)).once();
+    expect(zooReader.getData(eq(namePath + ""/"" + instBName)))
+        .andReturn(instB.canonical().getBytes(UTF_8)).once();
+    replay(zooReader);
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    Map<String,InstanceId> instanceMap = viewer.readInstancesFromZk(zooReader);
+
+    log.trace(""id map returned: {}"", instanceMap);
+    assertEquals(instA, instanceMap.get(instAName));
+    assertEquals(instB, instanceMap.get(instBName));
+    verify(zooReader);
+  }
+
+  /**
+   * Expect that instance id passed is returned, instance name and zooReader are ignored.
+   */
+  @Test
+  public void instanceIdOption() throws Exception {
+
+    String instAName = ""INST_A"";
+    InstanceId instA = InstanceId.of(UUID.randomUUID());
+    String instBName = ""INST_B"";
+    InstanceId instB = InstanceId.of(UUID.randomUUID());
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    String namePath = ZROOT + ZINSTANCES;
+    expect(zooReader.getChildren(eq(namePath))).andReturn(List.of(instAName, instBName)).once();
+    expect(zooReader.getData(eq(namePath + ""/"" + instAName)))
+        .andReturn(instA.canonical().getBytes(UTF_8)).once();
+    expect(zooReader.getData(eq(namePath + ""/"" + instBName)))
+        .andReturn(instB.canonical().getBytes(UTF_8)).once();
+    replay(zooReader);
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""--instanceName"", instBName});
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    InstanceId found = viewer.getInstanceId(zooReader, opts);
+
+    assertEquals(instB, found);
+
+    verify(zooReader);
+  }
+
+  /**
+   *
+   */
+  @Test
+  public void instanceNameTest() {
+    String uuid = UUID.randomUUID().toString();
+    ZooReader zooReader = createMock(ZooReader.class);
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(),
+        new String[] {""--instanceId"", uuid, ""--instanceName"", ""foo""});
+    replay(zooReader);
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    InstanceId found = viewer.getInstanceId(zooReader, opts);
+
+    assertEquals(InstanceId.of(uuid), found);
+
+    verify(zooReader);
+  }
+
+  @Test
+  public void instanceIdOutputTest() throws Exception {
+    String uuid = UUID.randomUUID().toString();
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    var instanceName = ""test"";
+    expect(zooReader.getChildren(eq(ZROOT + ZINSTANCES))).andReturn(List.of(instanceName)).once();
+    expect(zooReader.getData(eq(ZROOT + ZINSTANCES + ""/"" + instanceName)))
+        .andReturn(uuid.getBytes(UTF_8)).once();
+    replay(zooReader);
+
+    String testFileName = ""./target/zoo-info-viewer-"" + System.currentTimeMillis() + "".txt"";
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(),
+        new String[] {""--instanceId"", uuid, ""--print-instances"", ""--outfile"", testFileName});
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    // InstanceId found = viewer.getInstanceId(zooReader, opts);
+    viewer.generateReport(InstanceId.of(uuid), opts, zooReader);
+    // assertEquals(InstanceId.of(uuid), found);
+
+    verify(zooReader);
+
+    String line;
+    try (BufferedReader in = new BufferedReader(new FileReader(testFileName, UTF_8))) {
+      boolean found = false;
+      while ((line = in.readLine()) != null) {
+        if (line.contains(""="")) {
+          String trimmed = line.trim();
+          found = trimmed.startsWith(instanceName) && trimmed.endsWith(uuid);
+          break;
+        }
+      }
+      assertTrue(found, ""expected instance name, instance id not found"");
+
+    }
+  }
+
+  @Test
+  public void instanceNameOutputTest() throws Exception {
+    String uuid = UUID.randomUUID().toString();
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    var instanceName = ""test"";
+    expect(zooReader.getChildren(eq(ZROOT + ZINSTANCES))).andReturn(List.of(instanceName)).once();
+    expect(zooReader.getData(eq(ZROOT + ZINSTANCES + ""/"" + instanceName)))
+        .andReturn(uuid.getBytes(UTF_8)).once();
+    replay(zooReader);
+
+    String testFileName = ""./target/zoo-info-viewer-"" + System.currentTimeMillis() + "".txt"";
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""--instanceName"", instanceName,
+        ""--print-instances"", ""--outfile"", testFileName});
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    // InstanceId found = viewer.getInstanceId(zooReader, opts);
+    viewer.generateReport(InstanceId.of(uuid), opts, zooReader);
+    // assertEquals(InstanceId.of(uuid), found);
+
+    verify(zooReader);
+
+    String line;
+    try (BufferedReader in = new BufferedReader(new FileReader(testFileName, UTF_8))) {
+      boolean found = false;
+      while ((line = in.readLine()) != null) {
+        if (line.contains(""="")) {
+          String trimmed = line.trim();
+          found = trimmed.startsWith(instanceName) && trimmed.endsWith(uuid);
+          break;
+        }
+      }
+      assertTrue(found, ""expected instance name, instance id not found"");
+    }
+  }
+
+  @SuppressFBWarnings(value = ""CRLF_INJECTION_LOGS"",
+      justification = ""test output of generated output"")
+  @Test
+  public void propTest() throws Exception {
+    String uuid = UUID.randomUUID().toString();
+    InstanceId iid = InstanceId.of(uuid);
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    var instanceName = ""test"";
+    expect(zooReader.getChildren(eq(ZROOT + ZINSTANCES))).andReturn(List.of(instanceName))
+        .anyTimes();
+    expect(zooReader.getData(eq(ZROOT + ZINSTANCES + ""/"" + instanceName)))
+        .andReturn(uuid.getBytes(UTF_8)).anyTimes();
+
+    var sysPropBytes = propCodec
+        .toBytes(new VersionedProperties(123, Instant.now(), Map.of(""s1"", ""sv1"", ""s2"", ""sv2"")));
+    expect(zooReader.getData(eq(SystemPropKey.of(iid).getNodePath()), anyObject(Watcher.class),
+        anyObject(Stat.class))).andReturn(sysPropBytes).anyTimes();
+
+    var nsBasePath = ZooUtil.getRoot(iid) + ZNAMESPACES;
+    expect(zooReader.getChildren(nsBasePath)).andReturn(List.of(""a"")).anyTimes();
+    expect(zooReader.getData(eq(nsBasePath + ""/a"" + ZNAMESPACE_NAME)))
+        .andReturn(""a_name"".getBytes(UTF_8)).anyTimes();
+    var nsPropBytes =
+        propCodec.toBytes(new VersionedProperties(123, Instant.now(), Map.of(""n1"", ""nv1"")));
+    NamespaceId nsId = NamespaceId.of(""a"");
+    expect(zooReader.getData(eq(NamespacePropKey.of(iid, nsId).getNodePath()),
+        anyObject(Watcher.class), anyObject(Stat.class))).andReturn(nsPropBytes).anyTimes();
+
+    var tBasePath = ZooUtil.getRoot(iid) + ZTABLES;
+    expect(zooReader.getChildren(tBasePath)).andReturn(List.of(""t"")).anyTimes();
+    expect(zooReader.getData(eq(tBasePath + ""/t"" + ZTABLE_NAME)))
+        .andReturn(""t_table"".getBytes(UTF_8)).anyTimes();
+    var tPropBytes =
+        propCodec.toBytes(new VersionedProperties(123, Instant.now(), Map.of(""t1"", ""tv1"")));
+    TableId tid = TableId.of(""t"");
+    expect(zooReader.getData(eq(TablePropKey.of(iid, tid).getNodePath()), anyObject(Watcher.class),
+        anyObject(Stat.class))).andReturn(tPropBytes).anyTimes();
+    expect(zooReader.getData(tBasePath + ""/t"" + ZTABLE_NAMESPACE))
+        .andReturn(""+default"".getBytes(UTF_8)).anyTimes();
+
+    replay(zooReader);
+
+    NamespacePropKey nsKey = NamespacePropKey.of(iid, nsId);
+    log.trace(""namespace base path: {}"", nsKey.getBasePath());
+
+    String testFileName = ""./target/zoo-info-viewer-"" + System.currentTimeMillis() + "".txt"";
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(),
+        new String[] {""--instanceId"", uuid, ""--print-props"", ""--outfile"", testFileName});
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    // InstanceId found = viewer.getInstanceId(zooReader, opts);
+    viewer.generateReport(InstanceId.of(uuid), opts, zooReader);
+    // assertEquals(InstanceId.of(uuid), found);
+
+    verify(zooReader);
+
+    String line;
+    try (BufferedReader in = new BufferedReader(new FileReader(testFileName, UTF_8))) {
+      Map<String,String> props = new HashMap<>();
+      while ((line = in.readLine()) != null) {
+        if (line.contains(""="")) {
+          log.trace(""matched line: {}"", line);
+          String trimmed = line.trim();
+          String[] kv = trimmed.split(""="");
+          props.put(kv[0], kv[1]);
+        }
+      }
+      assertEquals(4, props.size());
+      assertEquals(""sv1"", props.get(""s1""));
+      assertEquals(""sv2"", props.get(""s2""));
+      assertEquals(""nv1"", props.get(""n1""));
+      assertEquals(""tv1"", props.get(""t1""));","[{'comment': 'You could probably just do `assertEquals(Map.of(...), props)`', 'commenter': 'ctubbsii'}]"
2751,server/base/src/test/java/org/apache/accumulo/server/conf/util/ZooInfoViewerTest.java,"@@ -0,0 +1,437 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZINSTANCES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_NAME;
+import static org.apache.accumulo.core.Constants.ZROOT;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAME;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAMESPACE;
+import static org.easymock.EasyMock.anyObject;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.eq;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.BufferedReader;
+import java.io.FileReader;
+import java.time.Instant;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.UUID;
+
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReader;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.conf.codec.VersionedPropCodec;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.zookeeper.Watcher;
+import org.apache.zookeeper.data.Stat;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+public class ZooInfoViewerTest {
+
+  private final Logger log = LoggerFactory.getLogger(ZooInfoViewerTest.class);
+
+  private final VersionedPropCodec propCodec = VersionedPropCodec.getDefault();
+
+  @Test
+  public void simpleOutput() {
+    // StringWriter writer = new StringWriter();
+  }
+
+  @Test
+  public void optionsAllDefault() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    assertTrue(opts.printAllProps());
+    assertTrue(opts.printSysProps());
+    assertTrue(opts.printNamespaceProps());
+    assertTrue(opts.printTableProps());
+  }
+
+  @Test
+  public void onlySys() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""--system""});
+
+    assertFalse(opts.printAllProps());
+    assertTrue(opts.printSysProps());
+    assertFalse(opts.printNamespaceProps());
+    assertFalse(opts.printTableProps());
+  }
+
+  @Test
+  public void onlyNamespaces() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""-ns"", ""ns1"", ""ns2""});
+
+    assertFalse(opts.printAllProps());
+    assertFalse(opts.printSysProps());
+    assertTrue(opts.printNamespaceProps());
+    assertEquals(2, opts.getNamespaces().size());
+    assertFalse(opts.printTableProps());
+    assertEquals(0, opts.getTables().size());
+  }
+
+  @Test
+  public void allLongOpts() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(),
+        new String[] {""--system"", ""--namespaces"", ""ns1"", ""ns2"", ""--tables"", ""tb1"", ""tbl2""});
+
+    log.debug(""namespaces: {}"", opts.getNamespaces());
+    log.debug(""tables: {}"", opts.getTables());
+
+    assertFalse(opts.printAllProps());
+    assertTrue(opts.printSysProps());
+    assertTrue(opts.printNamespaceProps());
+    assertTrue(opts.printTableProps());
+    assertEquals(2, opts.getNamespaces().size());
+    assertEquals(2, opts.getTables().size());
+  }
+
+  @Test
+  public void allOpts() {
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""-t"", ""tb1"", ""tbl2""});
+
+    assertFalse(opts.printAllProps());
+    assertFalse(opts.printSysProps());
+    assertFalse(opts.printNamespaceProps());
+    assertEquals(0, opts.getNamespaces().size());
+    assertTrue(opts.printTableProps());
+    assertEquals(2, opts.getTables().size());
+  }
+
+  @Test
+  public void fetchInstancesFromZk() throws Exception {
+
+    String instAName = ""INST_A"";
+    InstanceId instA = InstanceId.of(UUID.randomUUID());
+    String instBName = ""INST_B"";
+    InstanceId instB = InstanceId.of(UUID.randomUUID());
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    String namePath = ZROOT + ZINSTANCES;
+    expect(zooReader.getChildren(eq(namePath))).andReturn(List.of(instAName, instBName)).once();
+    expect(zooReader.getData(eq(namePath + ""/"" + instAName)))
+        .andReturn(instA.canonical().getBytes(UTF_8)).once();
+    expect(zooReader.getData(eq(namePath + ""/"" + instBName)))
+        .andReturn(instB.canonical().getBytes(UTF_8)).once();
+    replay(zooReader);
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    Map<String,InstanceId> instanceMap = viewer.readInstancesFromZk(zooReader);
+
+    log.trace(""id map returned: {}"", instanceMap);
+    assertEquals(instA, instanceMap.get(instAName));
+    assertEquals(instB, instanceMap.get(instBName));
+    verify(zooReader);
+  }
+
+  /**
+   * Expect that instance id passed is returned, instance name and zooReader are ignored.
+   */
+  @Test
+  public void instanceIdOption() throws Exception {
+
+    String instAName = ""INST_A"";
+    InstanceId instA = InstanceId.of(UUID.randomUUID());
+    String instBName = ""INST_B"";
+    InstanceId instB = InstanceId.of(UUID.randomUUID());
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    String namePath = ZROOT + ZINSTANCES;
+    expect(zooReader.getChildren(eq(namePath))).andReturn(List.of(instAName, instBName)).once();
+    expect(zooReader.getData(eq(namePath + ""/"" + instAName)))
+        .andReturn(instA.canonical().getBytes(UTF_8)).once();
+    expect(zooReader.getData(eq(namePath + ""/"" + instBName)))
+        .andReturn(instB.canonical().getBytes(UTF_8)).once();
+    replay(zooReader);
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""--instanceName"", instBName});
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    InstanceId found = viewer.getInstanceId(zooReader, opts);
+
+    assertEquals(instB, found);
+
+    verify(zooReader);
+  }
+
+  /**
+   *
+   */
+  @Test
+  public void instanceNameTest() {
+    String uuid = UUID.randomUUID().toString();
+    ZooReader zooReader = createMock(ZooReader.class);
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(),
+        new String[] {""--instanceId"", uuid, ""--instanceName"", ""foo""});
+    replay(zooReader);
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    InstanceId found = viewer.getInstanceId(zooReader, opts);
+
+    assertEquals(InstanceId.of(uuid), found);
+
+    verify(zooReader);
+  }
+
+  @Test
+  public void instanceIdOutputTest() throws Exception {
+    String uuid = UUID.randomUUID().toString();
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    var instanceName = ""test"";
+    expect(zooReader.getChildren(eq(ZROOT + ZINSTANCES))).andReturn(List.of(instanceName)).once();
+    expect(zooReader.getData(eq(ZROOT + ZINSTANCES + ""/"" + instanceName)))
+        .andReturn(uuid.getBytes(UTF_8)).once();
+    replay(zooReader);
+
+    String testFileName = ""./target/zoo-info-viewer-"" + System.currentTimeMillis() + "".txt"";
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(),
+        new String[] {""--instanceId"", uuid, ""--print-instances"", ""--outfile"", testFileName});
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    // InstanceId found = viewer.getInstanceId(zooReader, opts);
+    viewer.generateReport(InstanceId.of(uuid), opts, zooReader);
+    // assertEquals(InstanceId.of(uuid), found);
+
+    verify(zooReader);
+
+    String line;
+    try (BufferedReader in = new BufferedReader(new FileReader(testFileName, UTF_8))) {
+      boolean found = false;
+      while ((line = in.readLine()) != null) {
+        if (line.contains(""="")) {
+          String trimmed = line.trim();
+          found = trimmed.startsWith(instanceName) && trimmed.endsWith(uuid);
+          break;
+        }
+      }
+      assertTrue(found, ""expected instance name, instance id not found"");
+
+    }
+  }
+
+  @Test
+  public void instanceNameOutputTest() throws Exception {
+    String uuid = UUID.randomUUID().toString();
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    var instanceName = ""test"";
+    expect(zooReader.getChildren(eq(ZROOT + ZINSTANCES))).andReturn(List.of(instanceName)).once();
+    expect(zooReader.getData(eq(ZROOT + ZINSTANCES + ""/"" + instanceName)))
+        .andReturn(uuid.getBytes(UTF_8)).once();
+    replay(zooReader);
+
+    String testFileName = ""./target/zoo-info-viewer-"" + System.currentTimeMillis() + "".txt"";
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), new String[] {""--instanceName"", instanceName,
+        ""--print-instances"", ""--outfile"", testFileName});
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    // InstanceId found = viewer.getInstanceId(zooReader, opts);
+    viewer.generateReport(InstanceId.of(uuid), opts, zooReader);
+    // assertEquals(InstanceId.of(uuid), found);
+
+    verify(zooReader);
+
+    String line;
+    try (BufferedReader in = new BufferedReader(new FileReader(testFileName, UTF_8))) {
+      boolean found = false;
+      while ((line = in.readLine()) != null) {
+        if (line.contains(""="")) {
+          String trimmed = line.trim();
+          found = trimmed.startsWith(instanceName) && trimmed.endsWith(uuid);
+          break;
+        }
+      }
+      assertTrue(found, ""expected instance name, instance id not found"");
+    }
+  }
+
+  @SuppressFBWarnings(value = ""CRLF_INJECTION_LOGS"",
+      justification = ""test output of generated output"")
+  @Test
+  public void propTest() throws Exception {
+    String uuid = UUID.randomUUID().toString();
+    InstanceId iid = InstanceId.of(uuid);
+
+    ZooReader zooReader = createMock(ZooReader.class);
+    var instanceName = ""test"";
+    expect(zooReader.getChildren(eq(ZROOT + ZINSTANCES))).andReturn(List.of(instanceName))
+        .anyTimes();
+    expect(zooReader.getData(eq(ZROOT + ZINSTANCES + ""/"" + instanceName)))
+        .andReturn(uuid.getBytes(UTF_8)).anyTimes();
+
+    var sysPropBytes = propCodec
+        .toBytes(new VersionedProperties(123, Instant.now(), Map.of(""s1"", ""sv1"", ""s2"", ""sv2"")));
+    expect(zooReader.getData(eq(SystemPropKey.of(iid).getNodePath()), anyObject(Watcher.class),
+        anyObject(Stat.class))).andReturn(sysPropBytes).anyTimes();
+
+    var nsBasePath = ZooUtil.getRoot(iid) + ZNAMESPACES;
+    expect(zooReader.getChildren(nsBasePath)).andReturn(List.of(""a"")).anyTimes();
+    expect(zooReader.getData(eq(nsBasePath + ""/a"" + ZNAMESPACE_NAME)))
+        .andReturn(""a_name"".getBytes(UTF_8)).anyTimes();
+    var nsPropBytes =
+        propCodec.toBytes(new VersionedProperties(123, Instant.now(), Map.of(""n1"", ""nv1"")));
+    NamespaceId nsId = NamespaceId.of(""a"");
+    expect(zooReader.getData(eq(NamespacePropKey.of(iid, nsId).getNodePath()),
+        anyObject(Watcher.class), anyObject(Stat.class))).andReturn(nsPropBytes).anyTimes();
+
+    var tBasePath = ZooUtil.getRoot(iid) + ZTABLES;
+    expect(zooReader.getChildren(tBasePath)).andReturn(List.of(""t"")).anyTimes();
+    expect(zooReader.getData(eq(tBasePath + ""/t"" + ZTABLE_NAME)))
+        .andReturn(""t_table"".getBytes(UTF_8)).anyTimes();
+    var tPropBytes =
+        propCodec.toBytes(new VersionedProperties(123, Instant.now(), Map.of(""t1"", ""tv1"")));
+    TableId tid = TableId.of(""t"");
+    expect(zooReader.getData(eq(TablePropKey.of(iid, tid).getNodePath()), anyObject(Watcher.class),
+        anyObject(Stat.class))).andReturn(tPropBytes).anyTimes();
+    expect(zooReader.getData(tBasePath + ""/t"" + ZTABLE_NAMESPACE))
+        .andReturn(""+default"".getBytes(UTF_8)).anyTimes();
+
+    replay(zooReader);
+
+    NamespacePropKey nsKey = NamespacePropKey.of(iid, nsId);
+    log.trace(""namespace base path: {}"", nsKey.getBasePath());
+
+    String testFileName = ""./target/zoo-info-viewer-"" + System.currentTimeMillis() + "".txt"";
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(),
+        new String[] {""--instanceId"", uuid, ""--print-props"", ""--outfile"", testFileName});
+
+    ZooInfoViewer viewer = new ZooInfoViewer();
+    // InstanceId found = viewer.getInstanceId(zooReader, opts);
+    viewer.generateReport(InstanceId.of(uuid), opts, zooReader);
+    // assertEquals(InstanceId.of(uuid), found);
+
+    verify(zooReader);
+
+    String line;
+    try (BufferedReader in = new BufferedReader(new FileReader(testFileName, UTF_8))) {","[{'comment': 'BufferedReader to read input like this is very old school style. You could use java.util.Scanner, or since the file is very small, just read all the lines with `Files.readAllLines(path)` and loop over that result.', 'commenter': 'ctubbsii'}, {'comment': 'Used scanner', 'commenter': 'EdColeman'}]"
2751,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java,"@@ -0,0 +1,540 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZINSTANCES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_NAME;
+import static org.apache.accumulo.core.Constants.ZROOT;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAME;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAMESPACE;
+
+import java.io.BufferedWriter;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.TreeSet;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.clientImpl.Namespace;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReader;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+@AutoService(KeywordExecutable.class)
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_OUT"",
+    justification = ""app is run in same security context as user providing the filename"")
+public class ZooInfoViewer implements KeywordExecutable {
+  public static final DateTimeFormatter tsFormat =
+      DateTimeFormatter.ISO_OFFSET_DATE_TIME.withZone(ZoneId.from(ZoneOffset.UTC));
+  private static final Logger log = LoggerFactory.getLogger(ZooInfoViewer.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  private static final String INDENT = ""  "";
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooInfoViewer() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooInfoViewer().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-info-viewer"";
+  }
+
+  @Override
+  public String description() {
+    return ""view Accumulo instance and property information stored in ZooKeeper"";
+  }
+
+  @Override
+  public void execute(String[] args) {
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), args);
+
+    log.info(""print ids map: {}"", opts.printIdMap);
+    log.info(""print properties: {}"", opts.printProps);
+    log.info(""print instances: {}"", opts.printInstanceIds);
+
+    ZooReader zooReader = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zooReader, opts);
+    try {
+      generateReport(iid, opts, zooReader);
+    } catch (FileNotFoundException ex) {
+      throw new IllegalStateException(""Failed to generate ZooKeeper info report"", ex);
+    }
+  }
+
+  void generateReport(final InstanceId iid, final ZooInfoViewer.Opts opts,
+      final ZooReader zooReader) throws FileNotFoundException {
+
+    OutputStream outStream;
+
+    String outfile = opts.getOutfile();
+    if (outfile == null || outfile.isEmpty()) {
+      log.trace(""No output file, using stdout."");
+      outStream = System.out;
+    } else {
+      outStream = new FileOutputStream(outfile);
+    }
+
+    try (PrintWriter writer =
+        new PrintWriter(new BufferedWriter(new OutputStreamWriter(outStream, UTF_8)))) {
+
+      writer.println(""-----------------------------------------------"");
+      writer.println(""Report Time: "" + tsFormat.format(Instant.now()));
+      writer.println(""-----------------------------------------------"");
+      if (opts.printInstanceIds) {
+        Map<String,InstanceId> instanceMap = readInstancesFromZk(zooReader);
+        printInstanceIds(instanceMap, writer);
+      }
+
+      if (opts.printIdMap) {
+        printIdMapping(iid, zooReader, writer);
+      }
+
+      if (opts.printProps) {
+        printProps(iid, zooReader, opts, writer);
+      }
+
+      writer.println(""-----------------------------------------------"");
+    }
+  }
+
+  /**
+   * Get the instanceID from the command line options, or from value stored in HDFS. The search
+   * order is:
+   * <ol>
+   * <li>command line: --instanceId option</li>
+   * <li>command line: --instanceName option</li>
+   * <li>HDFS</li>
+   * </ol>
+   *
+   * @param zooReader
+   *          a ZooReader
+   * @param opts
+   *          the parsed command line options.
+   * @return an instance id
+   */
+  InstanceId getInstanceId(final ZooReader zooReader, final ZooInfoViewer.Opts opts) {
+
+    if (!opts.instanceId.isEmpty()) {
+      return InstanceId.of(opts.instanceId);
+    }
+    if (!opts.instanceName.isEmpty()) {
+      Map<String,InstanceId> instanceNameToIdMap = readInstancesFromZk(zooReader);
+      String instanceName = opts.instanceName;
+      for (Map.Entry<String,InstanceId> e : instanceNameToIdMap.entrySet()) {
+        if (e.getKey().equals(instanceName)) {
+          return e.getValue();
+        }
+      }
+      throw new IllegalArgumentException(
+          ""Specified instance name '"" + instanceName + ""' not found in ZooKeeper"");
+    }
+
+    try (ServerContext context = new ServerContext(SiteConfiguration.auto())) {
+      return context.getInstanceID();
+    } catch (Exception ex) {
+      throw new IllegalArgumentException(
+          ""Failed to read instance id from HDFS. Instances can be specified on the command line"",
+          ex);
+    }
+  }
+
+  Map<NamespaceId,String> getNamespaceIdToNameMap(InstanceId iid, final ZooReader zooReader) {
+    SortedMap<NamespaceId,String> namespaceToName = new TreeMap<>();
+    String zooNsRoot = ZooUtil.getRoot(iid) + ZNAMESPACES;
+    try {
+      List<String> nsids = zooReader.getChildren(zooNsRoot);
+      for (String id : nsids) {
+        String path = zooNsRoot + ""/"" + id + ZNAMESPACE_NAME;
+        String name = new String(zooReader.getData(path), UTF_8);
+        namespaceToName.put(NamespaceId.of(id), name);
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading namespace ids from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""Failed to read namespace ids from ZooKeeper"", ex);
+    }
+    return namespaceToName;
+  }
+
+  private void printProps(final InstanceId iid, final ZooReader zooReader, final Opts opts,
+      final PrintWriter writer) {
+
+    if (opts.printAllProps()) {
+      log.info(""all: {}"", opts.printAllProps());
+    } else {
+      log.info(""Filters:"");
+      log.info(""system: {}"", opts.printSysProps());
+      log.info(""namespaces: {} {}"", opts.printNamespaceProps(),
+          opts.getNamespaces().size() > 0 ? opts.getNamespaces() : """");
+      log.info(""tables: {} {}"", opts.printTableProps(),
+          opts.getTables().size() > 0 ? opts.getTables() : """");
+    }
+
+    writer.printf(""ZooKeeper properties for instance ID: %s\n\n"", iid.canonical());
+    if (opts.printSysProps()) {
+      printSortedProps(writer, Map.of(""System"", fetchSystemProp(iid, zooReader)));
+    }
+
+    if (opts.printNamespaceProps()) {
+      Map<NamespaceId,String> id2NamespaceMap = getNamespaceIdToNameMap(iid, zooReader);
+
+      Map<String,VersionedProperties> nsProps =
+          fetchNamespaceProps(iid, zooReader, id2NamespaceMap, opts.getNamespaces());
+
+      writer.println(""Namespace: "");
+      printSortedProps(writer, nsProps);
+      writer.flush();
+    }
+
+    if (opts.printTableProps()) {
+      Map<String,VersionedProperties> tProps = fetchTableProps(iid, opts.getTables(), zooReader);
+      writer.println(""Tables: "");
+      printSortedProps(writer, tProps);
+    }
+    writer.println();
+  }
+
+  private void printIdMapping(InstanceId iid, ZooReader zooReader, PrintWriter writer) {
+    // namespaces
+    Map<NamespaceId,String> id2NamespaceMap = getNamespaceIdToNameMap(iid, zooReader);
+    writer.println(""ID Mapping (id => name) for instance: "" + iid);
+    writer.println(""Namespace ids:"");
+    for (Map.Entry<NamespaceId,String> e : id2NamespaceMap.entrySet()) {
+      String v = e.getValue().isEmpty() ? ""\""\"""" : e.getValue();
+      writer.printf(""%s%-9s => %24s\n"", INDENT, e.getKey(), v);
+    }
+    writer.println();
+    // tables
+    Map<TableId,String> id2TableMap = getTableIdToName(iid, id2NamespaceMap, zooReader);
+    writer.println(""Table ids:"");
+    for (Map.Entry<TableId,String> e : id2TableMap.entrySet()) {
+      writer.printf(""%s%-9s => %24s\n"", INDENT, e.getKey(), e.getValue());
+    }
+    writer.println();
+  }
+
+  /**
+   * Read the instance names and instance ids from ZooKeeper. The storage structure in ZooKeeper is:
+   *
+   * <pre>
+   *   /accumulo/instances/instance_name  - with the instance id stored as data.
+   * </pre>
+   *
+   * @return a map of (instance name, instance id) entries
+   */
+  Map<String,InstanceId> readInstancesFromZk(final ZooReader zooReader) {
+    String instanceRoot = ZROOT + ZINSTANCES;
+    Map<String,InstanceId> idMap = new TreeMap<>();
+    try {
+      List<String> names = zooReader.getChildren(instanceRoot);
+      names.forEach(name -> {
+        InstanceId iid = getInstanceIdForName(zooReader, name);
+        idMap.put(name, iid);
+      });
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading instance name info from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""Failed to read instance name info from ZooKeeper"", ex);
+    }
+    return idMap;
+  }
+
+  private InstanceId getInstanceIdForName(ZooReader zooReader, String name) {
+    String instanceRoot = ZROOT + ZINSTANCES;
+    String path = """";
+    try {
+      path = instanceRoot + ""/"" + name;
+      byte[] uuid = zooReader.getData(path);
+      return InstanceId.of(UUID.fromString(new String(uuid, UTF_8)));
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading instance id from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      log.warn(""Failed to read instance id for "" + path);
+      return null;
+    }
+  }
+
+  private void printInstanceIds(final Map<String,InstanceId> instanceIdMap, PrintWriter writer) {
+    writer.println(""Instances (Instance Name, Instance ID)"");
+    instanceIdMap.forEach((name, iid) -> writer.println(name + ""="" + iid));
+    writer.println();
+  }
+
+  private Map<String,VersionedProperties> fetchNamespaceProps(InstanceId iid, ZooReader zooReader,
+      Map<NamespaceId,String> id2NamespaceMap, List<String> namespaces) {
+
+    Set<String> cmdOptNamespaces = new TreeSet<>(namespaces);
+
+    Map<NamespaceId,String> filteredIds;
+    if (cmdOptNamespaces.isEmpty()) {
+      filteredIds = id2NamespaceMap;
+    } else {
+      filteredIds =
+          id2NamespaceMap.entrySet().stream().filter(e -> cmdOptNamespaces.contains(e.getValue()))
+              .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+    }
+    log.trace(""ns filter: {}"", filteredIds);
+    Map<String,VersionedProperties> results = new TreeMap<>();
+
+    filteredIds.forEach((nid, name) -> {
+      try {
+        var key = NamespacePropKey.of(iid, nid);
+        log.trace(""fetch props from path: {}"", key.getNodePath());
+        var props = ZooPropStore.readFromZk(key, nullWatcher, zooReader);
+        results.put(name, props);
+      } catch (InterruptedException ex) {
+        Thread.currentThread().interrupt();
+        throw new IllegalStateException(""Interrupted reading table properties from ZooKeeper"", ex);
+      } catch (IOException | KeeperException ex) {
+        throw new IllegalStateException(""Failed to read table properties from ZooKeeper"", ex);
+      }
+    });
+
+    return results;
+  }
+
+  private Map<String,VersionedProperties> fetchTableProps(final InstanceId iid,
+      final List<String> tables, final ZooReader zooReader) {
+
+    Set<String> cmdOptTables = new TreeSet<>(tables);
+
+    Map<NamespaceId,String> id2NamespaceMap = getNamespaceIdToNameMap(iid, zooReader);
+    Map<TableId,String> allIds = getTableIdToName(iid, id2NamespaceMap, zooReader);
+
+    Map<TableId,String> filteredIds;
+    if (cmdOptTables.isEmpty()) {
+      filteredIds = allIds;
+    } else {
+      filteredIds = allIds.entrySet().stream().filter(e -> cmdOptTables.contains(e.getValue()))
+          .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+    }
+
+    log.trace(""Looking for: {}"", filteredIds);
+
+    Map<String,VersionedProperties> results = new TreeMap<>();
+
+    filteredIds.forEach((tid, name) -> {
+      try {
+        var key = TablePropKey.of(iid, tid);
+        log.trace(""fetch props from path: {}"", key.getNodePath());
+        var props = ZooPropStore.readFromZk(key, nullWatcher, zooReader);
+        results.put(name, props);
+      } catch (InterruptedException ex) {
+        Thread.currentThread().interrupt();
+        throw new IllegalStateException(""Interrupted reading table properties from ZooKeeper"", ex);
+      } catch (IOException | KeeperException ex) {
+        throw new IllegalStateException(""Failed to read table properties from ZooKeeper"", ex);
+      }
+    });
+
+    return results;
+  }
+
+  private Map<TableId,String> getTableIdToName(InstanceId iid,
+      Map<NamespaceId,String> id2NamespaceMap, ZooReader zooReader) {
+    SortedMap<TableId,String> idToName = new TreeMap<>();
+
+    String zooTables = ZooUtil.getRoot(iid) + ZTABLES;
+    try {
+      List<String> tids = zooReader.getChildren(zooTables);
+      for (String t : tids) {
+        String path = zooTables + ""/"" + t;
+        String tname = new String(zooReader.getData(path + ZTABLE_NAME), UTF_8);
+        NamespaceId tNsId =
+            NamespaceId.of(new String(zooReader.getData(path + ZTABLE_NAMESPACE), UTF_8));
+        if (tNsId.equals(Namespace.DEFAULT.id())) {
+          idToName.put(TableId.of(t), tname);
+        } else {
+          idToName.put(TableId.of(t), id2NamespaceMap.get(tNsId) + ""."" + tname);
+        }
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading table ids from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""Failed reading table id info from ZooKeeper"");
+    }
+    return idToName;
+  }
+
+  private void printSortedProps(final PrintWriter writer,
+      final Map<String,VersionedProperties> props) {
+    log.trace(""Printing: {}"", props);
+    props.forEach((n, p) -> {
+      writer.printf(""Name: %s, Data Version:%s, Data Timestamp: %s:\n"", n, p.getDataVersion(),
+          tsFormat.format(p.getTimestamp()));
+      Map<String,String> pMap = p.asMap();
+      if (pMap.isEmpty()) {
+        writer.println(""-- none --"");
+      } else {
+        TreeMap<String,String> sorted = new TreeMap<>(pMap);
+        sorted.forEach((name, value) -> writer.printf(""%s%s=%s\n"", INDENT, name, value));
+      }
+      writer.println();
+    });
+  }
+
+  private VersionedProperties fetchSystemProp(final InstanceId iid, final ZooReader zooReader) {
+
+    try {
+      SystemPropKey propKey = SystemPropKey.of(iid);
+      return ZooPropStore.readFromZk(propKey, nullWatcher, zooReader);
+    } catch (IOException | KeeperException | InterruptedException ex) {
+      throw new IllegalStateException(""Failed to read system properties from ZooKeeper"", ex);
+    }
+  }
+
+  static class Opts extends ConfigOpts {
+    @Parameter(names = {""--outfile""},
+        description = ""Write the output to a file, if the file exists will not be overwritten."")
+    public String outfile = """";
+
+    @Parameter(names = {""--print-id-map""},
+        description = ""print the namespace and table id, name mappings stored in ZooKeeper"")
+    public boolean printIdMap = false;
+
+    @Parameter(names = {""--print-props""},
+        description = ""print the property values stored in ZooKeeper, can be filtered with --namespaces and --tables options"")","[{'comment': 'Should mention `--system` is also a valid flag to combine with this', 'commenter': 'ctubbsii'}]"
2751,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java,"@@ -0,0 +1,540 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZINSTANCES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_NAME;
+import static org.apache.accumulo.core.Constants.ZROOT;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAME;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAMESPACE;
+
+import java.io.BufferedWriter;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.TreeSet;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.clientImpl.Namespace;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReader;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+@AutoService(KeywordExecutable.class)
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_OUT"",
+    justification = ""app is run in same security context as user providing the filename"")
+public class ZooInfoViewer implements KeywordExecutable {
+  public static final DateTimeFormatter tsFormat =
+      DateTimeFormatter.ISO_OFFSET_DATE_TIME.withZone(ZoneId.from(ZoneOffset.UTC));
+  private static final Logger log = LoggerFactory.getLogger(ZooInfoViewer.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  private static final String INDENT = ""  "";
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooInfoViewer() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooInfoViewer().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-info-viewer"";
+  }
+
+  @Override
+  public String description() {
+    return ""view Accumulo instance and property information stored in ZooKeeper"";
+  }
+
+  @Override
+  public void execute(String[] args) {
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), args);
+
+    log.info(""print ids map: {}"", opts.printIdMap);
+    log.info(""print properties: {}"", opts.printProps);
+    log.info(""print instances: {}"", opts.printInstanceIds);
+
+    ZooReader zooReader = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zooReader, opts);
+    try {
+      generateReport(iid, opts, zooReader);
+    } catch (FileNotFoundException ex) {
+      throw new IllegalStateException(""Failed to generate ZooKeeper info report"", ex);
+    }
+  }
+
+  void generateReport(final InstanceId iid, final ZooInfoViewer.Opts opts,
+      final ZooReader zooReader) throws FileNotFoundException {
+
+    OutputStream outStream;
+
+    String outfile = opts.getOutfile();
+    if (outfile == null || outfile.isEmpty()) {
+      log.trace(""No output file, using stdout."");
+      outStream = System.out;
+    } else {
+      outStream = new FileOutputStream(outfile);
+    }
+
+    try (PrintWriter writer =
+        new PrintWriter(new BufferedWriter(new OutputStreamWriter(outStream, UTF_8)))) {
+
+      writer.println(""-----------------------------------------------"");
+      writer.println(""Report Time: "" + tsFormat.format(Instant.now()));
+      writer.println(""-----------------------------------------------"");
+      if (opts.printInstanceIds) {
+        Map<String,InstanceId> instanceMap = readInstancesFromZk(zooReader);
+        printInstanceIds(instanceMap, writer);
+      }
+
+      if (opts.printIdMap) {
+        printIdMapping(iid, zooReader, writer);
+      }
+
+      if (opts.printProps) {
+        printProps(iid, zooReader, opts, writer);
+      }
+
+      writer.println(""-----------------------------------------------"");
+    }
+  }
+
+  /**
+   * Get the instanceID from the command line options, or from value stored in HDFS. The search
+   * order is:
+   * <ol>
+   * <li>command line: --instanceId option</li>
+   * <li>command line: --instanceName option</li>
+   * <li>HDFS</li>
+   * </ol>
+   *
+   * @param zooReader
+   *          a ZooReader
+   * @param opts
+   *          the parsed command line options.
+   * @return an instance id
+   */
+  InstanceId getInstanceId(final ZooReader zooReader, final ZooInfoViewer.Opts opts) {
+
+    if (!opts.instanceId.isEmpty()) {
+      return InstanceId.of(opts.instanceId);
+    }
+    if (!opts.instanceName.isEmpty()) {
+      Map<String,InstanceId> instanceNameToIdMap = readInstancesFromZk(zooReader);
+      String instanceName = opts.instanceName;
+      for (Map.Entry<String,InstanceId> e : instanceNameToIdMap.entrySet()) {
+        if (e.getKey().equals(instanceName)) {
+          return e.getValue();
+        }
+      }
+      throw new IllegalArgumentException(
+          ""Specified instance name '"" + instanceName + ""' not found in ZooKeeper"");
+    }
+
+    try (ServerContext context = new ServerContext(SiteConfiguration.auto())) {
+      return context.getInstanceID();
+    } catch (Exception ex) {
+      throw new IllegalArgumentException(
+          ""Failed to read instance id from HDFS. Instances can be specified on the command line"",
+          ex);
+    }
+  }
+
+  Map<NamespaceId,String> getNamespaceIdToNameMap(InstanceId iid, final ZooReader zooReader) {
+    SortedMap<NamespaceId,String> namespaceToName = new TreeMap<>();
+    String zooNsRoot = ZooUtil.getRoot(iid) + ZNAMESPACES;
+    try {
+      List<String> nsids = zooReader.getChildren(zooNsRoot);
+      for (String id : nsids) {
+        String path = zooNsRoot + ""/"" + id + ZNAMESPACE_NAME;
+        String name = new String(zooReader.getData(path), UTF_8);
+        namespaceToName.put(NamespaceId.of(id), name);
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading namespace ids from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""Failed to read namespace ids from ZooKeeper"", ex);
+    }
+    return namespaceToName;
+  }
+
+  private void printProps(final InstanceId iid, final ZooReader zooReader, final Opts opts,
+      final PrintWriter writer) {
+
+    if (opts.printAllProps()) {
+      log.info(""all: {}"", opts.printAllProps());
+    } else {
+      log.info(""Filters:"");
+      log.info(""system: {}"", opts.printSysProps());
+      log.info(""namespaces: {} {}"", opts.printNamespaceProps(),
+          opts.getNamespaces().size() > 0 ? opts.getNamespaces() : """");
+      log.info(""tables: {} {}"", opts.printTableProps(),
+          opts.getTables().size() > 0 ? opts.getTables() : """");
+    }
+
+    writer.printf(""ZooKeeper properties for instance ID: %s\n\n"", iid.canonical());
+    if (opts.printSysProps()) {
+      printSortedProps(writer, Map.of(""System"", fetchSystemProp(iid, zooReader)));
+    }
+
+    if (opts.printNamespaceProps()) {
+      Map<NamespaceId,String> id2NamespaceMap = getNamespaceIdToNameMap(iid, zooReader);
+
+      Map<String,VersionedProperties> nsProps =
+          fetchNamespaceProps(iid, zooReader, id2NamespaceMap, opts.getNamespaces());
+
+      writer.println(""Namespace: "");
+      printSortedProps(writer, nsProps);
+      writer.flush();
+    }
+
+    if (opts.printTableProps()) {
+      Map<String,VersionedProperties> tProps = fetchTableProps(iid, opts.getTables(), zooReader);
+      writer.println(""Tables: "");
+      printSortedProps(writer, tProps);
+    }
+    writer.println();
+  }
+
+  private void printIdMapping(InstanceId iid, ZooReader zooReader, PrintWriter writer) {
+    // namespaces
+    Map<NamespaceId,String> id2NamespaceMap = getNamespaceIdToNameMap(iid, zooReader);
+    writer.println(""ID Mapping (id => name) for instance: "" + iid);
+    writer.println(""Namespace ids:"");
+    for (Map.Entry<NamespaceId,String> e : id2NamespaceMap.entrySet()) {
+      String v = e.getValue().isEmpty() ? ""\""\"""" : e.getValue();
+      writer.printf(""%s%-9s => %24s\n"", INDENT, e.getKey(), v);
+    }
+    writer.println();
+    // tables
+    Map<TableId,String> id2TableMap = getTableIdToName(iid, id2NamespaceMap, zooReader);
+    writer.println(""Table ids:"");
+    for (Map.Entry<TableId,String> e : id2TableMap.entrySet()) {
+      writer.printf(""%s%-9s => %24s\n"", INDENT, e.getKey(), e.getValue());
+    }
+    writer.println();
+  }
+
+  /**
+   * Read the instance names and instance ids from ZooKeeper. The storage structure in ZooKeeper is:
+   *
+   * <pre>
+   *   /accumulo/instances/instance_name  - with the instance id stored as data.
+   * </pre>
+   *
+   * @return a map of (instance name, instance id) entries
+   */
+  Map<String,InstanceId> readInstancesFromZk(final ZooReader zooReader) {
+    String instanceRoot = ZROOT + ZINSTANCES;
+    Map<String,InstanceId> idMap = new TreeMap<>();
+    try {
+      List<String> names = zooReader.getChildren(instanceRoot);
+      names.forEach(name -> {
+        InstanceId iid = getInstanceIdForName(zooReader, name);
+        idMap.put(name, iid);
+      });
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading instance name info from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""Failed to read instance name info from ZooKeeper"", ex);
+    }
+    return idMap;
+  }
+
+  private InstanceId getInstanceIdForName(ZooReader zooReader, String name) {
+    String instanceRoot = ZROOT + ZINSTANCES;
+    String path = """";
+    try {
+      path = instanceRoot + ""/"" + name;
+      byte[] uuid = zooReader.getData(path);
+      return InstanceId.of(UUID.fromString(new String(uuid, UTF_8)));
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading instance id from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      log.warn(""Failed to read instance id for "" + path);
+      return null;
+    }
+  }
+
+  private void printInstanceIds(final Map<String,InstanceId> instanceIdMap, PrintWriter writer) {
+    writer.println(""Instances (Instance Name, Instance ID)"");
+    instanceIdMap.forEach((name, iid) -> writer.println(name + ""="" + iid));
+    writer.println();
+  }
+
+  private Map<String,VersionedProperties> fetchNamespaceProps(InstanceId iid, ZooReader zooReader,
+      Map<NamespaceId,String> id2NamespaceMap, List<String> namespaces) {
+
+    Set<String> cmdOptNamespaces = new TreeSet<>(namespaces);
+
+    Map<NamespaceId,String> filteredIds;
+    if (cmdOptNamespaces.isEmpty()) {
+      filteredIds = id2NamespaceMap;
+    } else {
+      filteredIds =
+          id2NamespaceMap.entrySet().stream().filter(e -> cmdOptNamespaces.contains(e.getValue()))
+              .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+    }
+    log.trace(""ns filter: {}"", filteredIds);
+    Map<String,VersionedProperties> results = new TreeMap<>();
+
+    filteredIds.forEach((nid, name) -> {
+      try {
+        var key = NamespacePropKey.of(iid, nid);
+        log.trace(""fetch props from path: {}"", key.getNodePath());
+        var props = ZooPropStore.readFromZk(key, nullWatcher, zooReader);
+        results.put(name, props);
+      } catch (InterruptedException ex) {
+        Thread.currentThread().interrupt();
+        throw new IllegalStateException(""Interrupted reading table properties from ZooKeeper"", ex);
+      } catch (IOException | KeeperException ex) {
+        throw new IllegalStateException(""Failed to read table properties from ZooKeeper"", ex);
+      }
+    });
+
+    return results;
+  }
+
+  private Map<String,VersionedProperties> fetchTableProps(final InstanceId iid,
+      final List<String> tables, final ZooReader zooReader) {
+
+    Set<String> cmdOptTables = new TreeSet<>(tables);
+
+    Map<NamespaceId,String> id2NamespaceMap = getNamespaceIdToNameMap(iid, zooReader);
+    Map<TableId,String> allIds = getTableIdToName(iid, id2NamespaceMap, zooReader);
+
+    Map<TableId,String> filteredIds;
+    if (cmdOptTables.isEmpty()) {
+      filteredIds = allIds;
+    } else {
+      filteredIds = allIds.entrySet().stream().filter(e -> cmdOptTables.contains(e.getValue()))
+          .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+    }
+
+    log.trace(""Looking for: {}"", filteredIds);
+
+    Map<String,VersionedProperties> results = new TreeMap<>();
+
+    filteredIds.forEach((tid, name) -> {
+      try {
+        var key = TablePropKey.of(iid, tid);
+        log.trace(""fetch props from path: {}"", key.getNodePath());
+        var props = ZooPropStore.readFromZk(key, nullWatcher, zooReader);
+        results.put(name, props);
+      } catch (InterruptedException ex) {
+        Thread.currentThread().interrupt();
+        throw new IllegalStateException(""Interrupted reading table properties from ZooKeeper"", ex);
+      } catch (IOException | KeeperException ex) {
+        throw new IllegalStateException(""Failed to read table properties from ZooKeeper"", ex);
+      }
+    });
+
+    return results;
+  }
+
+  private Map<TableId,String> getTableIdToName(InstanceId iid,
+      Map<NamespaceId,String> id2NamespaceMap, ZooReader zooReader) {
+    SortedMap<TableId,String> idToName = new TreeMap<>();
+
+    String zooTables = ZooUtil.getRoot(iid) + ZTABLES;
+    try {
+      List<String> tids = zooReader.getChildren(zooTables);
+      for (String t : tids) {
+        String path = zooTables + ""/"" + t;
+        String tname = new String(zooReader.getData(path + ZTABLE_NAME), UTF_8);
+        NamespaceId tNsId =
+            NamespaceId.of(new String(zooReader.getData(path + ZTABLE_NAMESPACE), UTF_8));
+        if (tNsId.equals(Namespace.DEFAULT.id())) {
+          idToName.put(TableId.of(t), tname);
+        } else {
+          idToName.put(TableId.of(t), id2NamespaceMap.get(tNsId) + ""."" + tname);
+        }
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading table ids from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""Failed reading table id info from ZooKeeper"");
+    }
+    return idToName;
+  }
+
+  private void printSortedProps(final PrintWriter writer,
+      final Map<String,VersionedProperties> props) {
+    log.trace(""Printing: {}"", props);
+    props.forEach((n, p) -> {
+      writer.printf(""Name: %s, Data Version:%s, Data Timestamp: %s:\n"", n, p.getDataVersion(),
+          tsFormat.format(p.getTimestamp()));
+      Map<String,String> pMap = p.asMap();
+      if (pMap.isEmpty()) {
+        writer.println(""-- none --"");
+      } else {
+        TreeMap<String,String> sorted = new TreeMap<>(pMap);
+        sorted.forEach((name, value) -> writer.printf(""%s%s=%s\n"", INDENT, name, value));
+      }
+      writer.println();
+    });
+  }
+
+  private VersionedProperties fetchSystemProp(final InstanceId iid, final ZooReader zooReader) {
+
+    try {
+      SystemPropKey propKey = SystemPropKey.of(iid);
+      return ZooPropStore.readFromZk(propKey, nullWatcher, zooReader);
+    } catch (IOException | KeeperException | InterruptedException ex) {
+      throw new IllegalStateException(""Failed to read system properties from ZooKeeper"", ex);","[{'comment': ""This is a CLI utility, it's better to just throw it so it reaches through main."", 'commenter': 'ctubbsii'}]"
2751,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java,"@@ -0,0 +1,540 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZINSTANCES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZNAMESPACE_NAME;
+import static org.apache.accumulo.core.Constants.ZROOT;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAME;
+import static org.apache.accumulo.core.Constants.ZTABLE_NAMESPACE;
+
+import java.io.BufferedWriter;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.TreeSet;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.clientImpl.Namespace;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.fate.zookeeper.ZooReader;
+import org.apache.accumulo.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+@AutoService(KeywordExecutable.class)
+@SuppressFBWarnings(value = ""PATH_TRAVERSAL_OUT"",
+    justification = ""app is run in same security context as user providing the filename"")
+public class ZooInfoViewer implements KeywordExecutable {
+  public static final DateTimeFormatter tsFormat =
+      DateTimeFormatter.ISO_OFFSET_DATE_TIME.withZone(ZoneId.from(ZoneOffset.UTC));
+  private static final Logger log = LoggerFactory.getLogger(ZooInfoViewer.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  private static final String INDENT = ""  "";
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooInfoViewer() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooInfoViewer().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-info-viewer"";
+  }
+
+  @Override
+  public String description() {
+    return ""view Accumulo instance and property information stored in ZooKeeper"";
+  }
+
+  @Override
+  public void execute(String[] args) {
+
+    ZooInfoViewer.Opts opts = new ZooInfoViewer.Opts();
+    opts.parseArgs(ZooInfoViewer.class.getName(), args);
+
+    log.info(""print ids map: {}"", opts.printIdMap);
+    log.info(""print properties: {}"", opts.printProps);
+    log.info(""print instances: {}"", opts.printInstanceIds);
+
+    ZooReader zooReader = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zooReader, opts);
+    try {
+      generateReport(iid, opts, zooReader);
+    } catch (FileNotFoundException ex) {
+      throw new IllegalStateException(""Failed to generate ZooKeeper info report"", ex);
+    }
+  }
+
+  void generateReport(final InstanceId iid, final ZooInfoViewer.Opts opts,
+      final ZooReader zooReader) throws FileNotFoundException {
+
+    OutputStream outStream;
+
+    String outfile = opts.getOutfile();
+    if (outfile == null || outfile.isEmpty()) {
+      log.trace(""No output file, using stdout."");
+      outStream = System.out;
+    } else {
+      outStream = new FileOutputStream(outfile);
+    }
+
+    try (PrintWriter writer =
+        new PrintWriter(new BufferedWriter(new OutputStreamWriter(outStream, UTF_8)))) {
+
+      writer.println(""-----------------------------------------------"");
+      writer.println(""Report Time: "" + tsFormat.format(Instant.now()));
+      writer.println(""-----------------------------------------------"");
+      if (opts.printInstanceIds) {
+        Map<String,InstanceId> instanceMap = readInstancesFromZk(zooReader);
+        printInstanceIds(instanceMap, writer);
+      }
+
+      if (opts.printIdMap) {
+        printIdMapping(iid, zooReader, writer);
+      }
+
+      if (opts.printProps) {
+        printProps(iid, zooReader, opts, writer);
+      }
+
+      writer.println(""-----------------------------------------------"");
+    }
+  }
+
+  /**
+   * Get the instanceID from the command line options, or from value stored in HDFS. The search
+   * order is:
+   * <ol>
+   * <li>command line: --instanceId option</li>
+   * <li>command line: --instanceName option</li>
+   * <li>HDFS</li>
+   * </ol>
+   *
+   * @param zooReader
+   *          a ZooReader
+   * @param opts
+   *          the parsed command line options.
+   * @return an instance id
+   */
+  InstanceId getInstanceId(final ZooReader zooReader, final ZooInfoViewer.Opts opts) {
+
+    if (!opts.instanceId.isEmpty()) {
+      return InstanceId.of(opts.instanceId);
+    }
+    if (!opts.instanceName.isEmpty()) {
+      Map<String,InstanceId> instanceNameToIdMap = readInstancesFromZk(zooReader);
+      String instanceName = opts.instanceName;
+      for (Map.Entry<String,InstanceId> e : instanceNameToIdMap.entrySet()) {
+        if (e.getKey().equals(instanceName)) {
+          return e.getValue();
+        }
+      }
+      throw new IllegalArgumentException(
+          ""Specified instance name '"" + instanceName + ""' not found in ZooKeeper"");
+    }
+
+    try (ServerContext context = new ServerContext(SiteConfiguration.auto())) {
+      return context.getInstanceID();
+    } catch (Exception ex) {
+      throw new IllegalArgumentException(
+          ""Failed to read instance id from HDFS. Instances can be specified on the command line"",
+          ex);
+    }
+  }
+
+  Map<NamespaceId,String> getNamespaceIdToNameMap(InstanceId iid, final ZooReader zooReader) {
+    SortedMap<NamespaceId,String> namespaceToName = new TreeMap<>();
+    String zooNsRoot = ZooUtil.getRoot(iid) + ZNAMESPACES;
+    try {
+      List<String> nsids = zooReader.getChildren(zooNsRoot);
+      for (String id : nsids) {
+        String path = zooNsRoot + ""/"" + id + ZNAMESPACE_NAME;
+        String name = new String(zooReader.getData(path), UTF_8);
+        namespaceToName.put(NamespaceId.of(id), name);
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading namespace ids from ZooKeeper"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""Failed to read namespace ids from ZooKeeper"", ex);
+    }
+    return namespaceToName;
+  }
+
+  private void printProps(final InstanceId iid, final ZooReader zooReader, final Opts opts,
+      final PrintWriter writer) {
+
+    if (opts.printAllProps()) {
+      log.info(""all: {}"", opts.printAllProps());
+    } else {
+      log.info(""Filters:"");
+      log.info(""system: {}"", opts.printSysProps());
+      log.info(""namespaces: {} {}"", opts.printNamespaceProps(),
+          opts.getNamespaces().size() > 0 ? opts.getNamespaces() : """");
+      log.info(""tables: {} {}"", opts.printTableProps(),
+          opts.getTables().size() > 0 ? opts.getTables() : """");","[{'comment': ""The default log configuration will print these to the console. These log messages will be interspersed with the non-log STDOUT. By default, though, they will go to STDERR. That may be what you want (so you can pipe them to another command without the extra info)... but if you intend them to always be included in the output, they should use the same console writer as the printing of the values.\r\n\r\nThe other thing here is that you have multiple log statements in a row. This seems a bit unnecessary... like you're using logging for a non-logging purpose."", 'commenter': 'ctubbsii'}, {'comment': ""The multiple lines are to improve the readability - it is much easier to see the files options by type than scanning a long line. If feel that it also makes it clearer what the following output may or may not contain because of user options.\r\n\r\nThe use of the logger also clearly separates the text output from the command and will not appear in an output file (if specified) this makes follow-on processing easier if that is the user's end goal.\r\n\r\nAt one point, I did have one log statement print multiple lines, but that seemed odd in the output."", 'commenter': 'EdColeman'}, {'comment': ""I think it might be better to try to make the log messages more succinct, rather than try to format their output inline with the other output. I can try to think on it and come up with a proposed change.\r\n\r\nAlso, I was thinking about suggesting a more computer-friendly output format for follow-on processing. Similar to GPG's `--with-colons`, or like the `--short` option for `git status`.\r\n\r\nHaving more options, like only printing the specific configuration being asked, could also help with the output... less information is needed to organize the output if you are only getting one table or namespace or system scope at a time."", 'commenter': 'ctubbsii'}, {'comment': ""One thing to consider is that this is geared towards emergency use and while it is really helpful to have the information, its usage is going to be situational and highly dependent on what needs to be done.  For a while I had a json output option (for properties) but it seemed to add little utility.  With the current output it is structured enough that grep / awk can generate shell command(s) without much issue.  The most common cases that I thought of:\r\n\r\n - Dump a table's property to setup a mirror system\r\n - Dump the id mapping to recover from issues where a bulk re-import is the only option.\r\n\r\nThe sticking point for me was that sometimes knowing the id is really important - but ids are specific to an instance and have no utility across instances.  \r\n\r\nI'm not opposed to other options - just relaying things that I had considered in case that shapes any suggestions."", 'commenter': 'EdColeman'}, {'comment': ""```\r\nHaving more options, like only printing the specific configuration being asked, could also help with the output... less information is needed to organize the output if you are only getting one table or namespace or system scope at a time.\r\n```\r\n\r\nIf you specify a filter - you only receive output for those items.  For example\r\n```\r\naccumulo zoo-info-viewer --instanceName uno --print-props -t tbl1\r\n```\r\n# Sample output (single table)\r\n<details>\r\n  <summary>Click to expand!</summary>\r\n  \r\n  ```\r\n-----------------------------------------------\r\nReport Time: 2022-06-03T10:42:05.718655Z\r\n-----------------------------------------------\r\nZooKeeper properties for instance ID: 33ac4ded-7ead-436b-983e-401e50464190\r\n\r\nTables: \r\nName: tbl1, Data Version:1, Data Timestamp: 2022-06-03T10:37:29.810437Z:\r\n  table.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstraint\r\n  table.iterator.majc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator\r\n  table.iterator.majc.vers.opt.maxVersions=1\r\n  table.iterator.minc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator\r\n  table.iterator.minc.vers.opt.maxVersions=1\r\n  table.iterator.scan.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator\r\n  table.iterator.scan.vers.opt.maxVersions=1\r\n\r\n\r\n-----------------------------------------------\r\n  ```\r\n</details>\r\n\r\nThe choice of providing name=value form was deliberate.  1) It allows an easy filter and also matches the syntax for the shell `config` command.  So, piping the command output to something like ` | grep '=' | tr '\\n''  ` is just about the format needed to make a shell config command.\r\n"", 'commenter': 'EdColeman'}, {'comment': 'I will defer this for a follow on if someone else thinks it needs to change.', 'commenter': 'EdColeman'}]"
2767,core/src/main/java/org/apache/accumulo/core/gc/Reference.java,"@@ -19,50 +19,27 @@
 package org.apache.accumulo.core.gc;
 
 import org.apache.accumulo.core.data.TableId;
-import org.apache.accumulo.core.metadata.schema.MetadataSchema;
 
 /**
- * A GC reference to a tablet file or directory.
+ * A GC reference used for collecting files and directories into a single stream.
  */
-public class Reference implements Comparable<Reference> {
-  // parts of an absolute URI, like ""hdfs://1.2.3.4/accumulo/tables/2a/t-0003""
-  public final TableId tableId; // 2a
+public interface Reference {
+  /**
+   * Only return true if the reference is a directory.
+   */
+  boolean isDirectory();
 
-  // the exact string that is stored in the metadata
-  public final String metadataEntry;
+  /**
+   * Get the {@link TableId} of the reference.
+   */
+  TableId getTableId();
 
-  public Reference(TableId tableId, String metadataEntry) {
-    MetadataSchema.TabletsSection.ServerColumnFamily.validateDirCol(tableId.canonical());
-    this.tableId = tableId;
-    this.metadataEntry = metadataEntry;
-  }
-
-  @Override
-  public int compareTo(Reference that) {
-    if (equals(that)) {
-      return 0;
-    } else {
-      return this.metadataEntry.compareTo(that.metadataEntry);
-    }
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj)
-      return true;
-    if (obj == null)
-      return false;
-    if (getClass() != obj.getClass())
-      return false;
-    Reference other = (Reference) obj;
-    if (metadataEntry == null) {
-      return other.metadataEntry == null;
-    } else
-      return metadataEntry.equals(other.metadataEntry);
-  }
-
-  @Override
-  public int hashCode() {
-    return this.metadataEntry.hashCode();
-  }
+  /**
+   * Get the exact string stored in the metadata table for this file or directory. A file will be
+   * read from the Tablet ""file"" column family:","[{'comment': ""I'm not sure that this is true. Certainly file references will be created using the `file` colf int he metadata table, but also the `scan` entries. The ScanServer feature adds a new section of references to the metadata table for scan server related file references."", 'commenter': 'dlmarion'}, {'comment': 'Which part?', 'commenter': 'milleruntime'}, {'comment': 'Are you saying they could also include `scan` entries? ', 'commenter': 'milleruntime'}, {'comment': 'I guess my point here is that the value in the Reference will be the value from the `file` colf, but that value may not necessarily come from the `file` colf at runtime. See the [GCRun](https://github.com/apache/accumulo/pull/2665/files#diff-f606ce0c5767f33efef8cabb65411beaafb9c6cb556c1f990ebfd3514b6a7cc8) changes in the ScanServer branch.', 'commenter': 'dlmarion'}, {'comment': 'In this branch, it is coming from 2 sources, scans and file column families. \r\n<pre>\r\n// combine all the entries read from file and scan columns in the metadata table\r\nvar fileStream = Stream.concat(tm.getFiles().stream(), tm.getScans().stream());\r\n</pre>', 'commenter': 'milleruntime'}]"
2767,core/src/main/java/org/apache/accumulo/core/gc/ReferenceDirectory.java,"@@ -21,13 +21,22 @@
 import org.apache.accumulo.core.data.TableId;
 
 /**
- * Part of the Tablet File path that is definitely a directory.
+ * A GC reference to a Tablet directory, like t-0003.
  */
-public class ReferenceDirectory extends Reference {
-  public final String tabletDir; // t-0003
+public class ReferenceDirectory extends ReferenceFile {
+  private final String tabletDir; // t-0003
 
   public ReferenceDirectory(TableId tableId, String dirName) {
     super(tableId, dirName);","[{'comment': 'In this case, the `metadataEntry` is only the dirname, like `t-0003` ? Or, is the example wrong?', 'commenter': 'dlmarion'}, {'comment': 'The metadataEntry will look like: `1;2cccccccccccccd1 srv:dir []   t-000003z`. So technically, the dirName is only part of the metadaEntry.\r\n', 'commenter': 'milleruntime'}, {'comment': 'Ok, thanks for the clarification.', 'commenter': 'dlmarion'}, {'comment': 'Would some validation make sense for this new class?  Something like the following as an example, but not sure its correct.\r\n\r\n```suggestion\r\n  private static validateDirName(String dirName) {\r\n     checkArgument(dirName.matches(""[0-9A-Za-z]+"")));\r\n  }\r\n \r\n  public ReferenceDirectory(TableId tableId, String dirName) {\r\n    super(tableId, validateDirName(dirName));\r\n```', 'commenter': 'keith-turner'}, {'comment': 'We do have a method to check the dirName, but I think it could be improved:\r\nhttps://github.com/apache/accumulo/blob/cf57e343de77f416de10c22e0ed05ef37f433b36/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java#L199', 'commenter': 'milleruntime'}, {'comment': 'I think this needs an underscore and ""-"". We have the value ""default_tablet"" and tablet directories have a dash like:\r\n<pre>\r\n2;9 srv:dir []\tt-000009x\r\n2< srv:dir []\tdefault_tablet\r\n</pre>', 'commenter': 'milleruntime'}]"
2767,server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java,"@@ -139,21 +140,29 @@ public Stream<String> getBlipPaths() throws TableNotFoundException {
   public Stream<Reference> getReferences() {
     Stream<TabletMetadata> tabletStream;
 
+    // create a stream of metadata entries read from file, scan and tablet dir columns
     if (level == Ample.DataLevel.ROOT) {
       tabletStream = Stream.of(context.getAmple().readTablet(RootTable.EXTENT, DIR, FILES, SCANS));
     } else {
-      tabletStream = TabletsMetadata.builder(context).scanTable(level.metaTable())
-          .checkConsistency().fetch(DIR, FILES, SCANS).build().stream();
+      try (var tabletsMetadata = TabletsMetadata.builder(context).scanTable(level.metaTable())
+          .checkConsistency().fetch(DIR, FILES, SCANS).build()) {
+        tabletStream = tabletsMetadata.stream();
+      }
     }
 
+    // there is a lot going on in this ""one line"" so see below for more info
     return tabletStream.flatMap(tm -> {
-      Stream<Reference> refs = Stream.concat(tm.getFiles().stream(), tm.getScans().stream())
-          .map(f -> new Reference(tm.getTableId(), f.getMetaUpdateDelete()));
+      // combine all the entries read from file and scan columns in the metadata table
+      var fileStream = Stream.concat(tm.getFiles().stream(), tm.getScans().stream());
+      // map the files to Reference objects
+      var stream = fileStream.map(f -> new ReferenceFile(tm.getTableId(), f.getMetaUpdateDelete()));
+      // if dirName is populated then we have a tablet directory aka srv:dir
       if (tm.getDirName() != null) {
-        refs = Stream.concat(refs,
-            Stream.of(new ReferenceDirectory(tm.getTableId(), tm.getDirName())));
+        // add the tablet directory to the stream
+        var tabletDir = new ReferenceDirectory(tm.getTableId(), tm.getDirName());","[{'comment': 'Is `tm.getDirName()` a fully qualified path, or just the name of the directory?', 'commenter': 'dlmarion'}, {'comment': 'See my other comment but usually the `dirName` will be just the name of the directory.', 'commenter': 'milleruntime'}]"
2767,server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java,"@@ -170,6 +172,9 @@ private void removeCandidatesInUse(GarbageCollectionEnvironment gce,
           log.debug(""Candidate was still in use: {}"", relativePath);
       }
     }
+
+    // close underlying scanner
+    refStream.close();","[{'comment': 'Does this actually close the underlying scanner? Or just the stream?', 'commenter': 'ctubbsii'}, {'comment': 'That is a good question. I was thinking it closes both but maybe I can write a test to see. The scanner was being closed by the `try-with-resources` on the `TabletsMetadata` but that class is the one that is `AutoCloseable`. If we want to be able to close the underlying scanner (which I think would be good) then we probably have to refactor the GC code.', 'commenter': 'milleruntime'}, {'comment': 'It was just closing the stream, essentially a no-op close so i dropped it in https://github.com/apache/accumulo/pull/2767/commits/b9f34e0957513b4e7e47e1035af0ecfc0d4ecd2c', 'commenter': 'milleruntime'}]"
2767,server/base/src/main/java/org/apache/accumulo/server/gc/GcVolumeUtil.java,"@@ -24,20 +24,20 @@
 
 import org.apache.accumulo.core.Constants;
 import org.apache.accumulo.core.data.TableId;
-import org.apache.accumulo.core.gc.Reference;
 import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily;
 import org.apache.accumulo.server.fs.VolumeManager;
 import org.apache.hadoop.fs.Path;
 
 public class GcVolumeUtil {
   // AGCAV : Accumulo Garbage Collector All Volumes
-  private static final String ALL_VOLUMES_PREFIX = ""agcav:/"";
+  static final String ALL_VOLUMES_PREFIX = ""agcav:/"";
 
-  public static Reference getDeleteTabletOnAllVolumesUri(TableId tableId, String dirName) {
+  public static AllVolumesDirectory getDeleteTabletOnAllVolumesUri(TableId tableId,","[{'comment': ""Would it make sense to move this method to the new AllVolumesDirectory class?  I am not sure if that makes sense because I don't know enough about the surrounding context."", 'commenter': 'keith-turner'}, {'comment': 'Yeah. I moved it in https://github.com/apache/accumulo/pull/2767/commits/4818a62962c1186d313072eeb14aa0345f10f50b', 'commenter': 'milleruntime'}]"
2767,core/src/main/java/org/apache/accumulo/core/gc/ReferenceFile.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.gc;
+
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+
+/**
+ * A GC reference used for streaming and delete markers. This type is a file. Subclass is a
+ * directory.
+ */
+public class ReferenceFile implements Reference, Comparable<ReferenceFile> {
+  // parts of an absolute URI, like ""hdfs://1.2.3.4/accumulo/tables/2a/t-0003""
+  public final TableId tableId; // 2a
+
+  // the exact string that is stored in the metadata
+  public final String metadataEntry;
+
+  public ReferenceFile(TableId tableId, String metadataEntry) {
+    MetadataSchema.TabletsSection.ServerColumnFamily.validateDirCol(tableId.canonical());
+    this.tableId = tableId;
+    this.metadataEntry = metadataEntry;
+  }
+
+  @Override
+  public boolean isDirectory() {
+    return false;
+  }
+
+  @Override
+  public TableId getTableId() {
+    return tableId;
+  }
+
+  @Override
+  public String getMetadataEntry() {
+    return metadataEntry;
+  }
+
+  @Override
+  public int compareTo(ReferenceFile that) {
+    if (equals(that)) {
+      return 0;
+    } else {
+      return this.metadataEntry.compareTo(that.metadataEntry);","[{'comment': 'Is this sort order important for the correctness of GC Algorithm?', 'commenter': 'keith-turner'}, {'comment': 'Yes but the main spot where the collection is created is in the deleteTablets() method in `TabletGroupWatcher`:\r\nhttps://github.com/apache/accumulo/blob/cf57e343de77f416de10c22e0ed05ef37f433b36/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java#L639\r\n\r\nFrom there the GC method `putGcFileAndDirCandidates()` gets called.', 'commenter': 'milleruntime'}]"
2767,server/base/src/main/java/org/apache/accumulo/server/gc/AllVolumesDirectory.java,"@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.gc;
+
+import static org.apache.accumulo.server.gc.GcVolumeUtil.ALL_VOLUMES_PREFIX;
+
+import java.util.Objects;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.gc.ReferenceDirectory;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+import org.apache.hadoop.fs.Path;
+
+/**
+ * A specially encoded GC Reference to a directory with the {@link GcVolumeUtil#ALL_VOLUMES_PREFIX}
+ */
+public class AllVolumesDirectory extends ReferenceDirectory {","[{'comment': 'I am trying to figure out why this extends reference.  Does not seem it is used as a reference in the GC algorithm.  Seems that `GCRun.getReferences()` will never return this type.  Conceptually I think the ALL_VOLUMES_PREFIX type is a special type of GC candidate, but not a reference.  I am still poking around in the code trying to understand the larger context, will circle back to it in the morning.', 'commenter': 'keith-turner'}, {'comment': '> Seems that GCRun.getReferences() will never return this type.\r\n\r\nThat is correct. I had created ReferenceFile to represent Files and ReferenceDirectory to represent Directories for the GC Algorithm getReferences() method. Then I wanted a type for the new special ALL_VOLUMES_PREFIX Type and figured since it is a directory to make it extend ReferenceDirectory. The TabletGroupWatcher uses the ReferenceFile type to gather up the deletes during a merge here: https://github.com/milleruntime/accumulo/blob/4818a62962c1186d313072eeb14aa0345f10f50b/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java#L639\r\n\r\nIt might make more sense to make AllVolumesDirectory extend ReferenceFile, the same way ReferenceDirectory does.', 'commenter': 'milleruntime'}, {'comment': 'Check out [df7f4f9](https://github.com/apache/accumulo/pull/2767/commits/df7f4f977f400c8eb5717d14bfb69587545d4008). I was able to make metadataEntry final by having AllVolumesDirectory extend ReferenceFile. ', 'commenter': 'milleruntime'}, {'comment': ""The following comment is just for discussion, I don't think any action needs to be taken in this PR based on it.  The GC deals with two inputs conceptually candidates and references.  Candidates are files that could be possibly be deleted if they are not defeated by a reference.  Thinking about this I was wondering if it would make sense to have a GcCandidate type in addition to the Reference type.  If so, then the AllVolumesDirectory would be a GcCandidate.  However I am not sure if this makes sense w/o further looking and it would not be something for this PR.  Wondering if you even think this makes sense.\r\n\r\nFor this PR we may want to document that the reference type is also used for GC candidates."", 'commenter': 'keith-turner'}, {'comment': '> Thinking about this I was wondering if it would make sense to have a GcCandidate type in addition to the Reference type.\r\n\r\nFor sure. I had some ideas about a strong type for candidates and a backing Map class to replace the Map we pass around. But I was trying to keep changes to  to a minimum to be less disruptive so close to a release candidate.\r\n\r\n> For this PR we may want to document that the reference type is also used for GC candidates.\r\n\r\nI can add some comments.', 'commenter': 'milleruntime'}]"
2767,server/base/src/main/java/org/apache/accumulo/server/gc/AllVolumesDirectory.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.gc;
+
+import static org.apache.accumulo.server.gc.GcVolumeUtil.ALL_VOLUMES_PREFIX;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.gc.ReferenceFile;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+import org.apache.hadoop.fs.Path;
+
+/**
+ * A specially encoded GC Reference to a directory with the {@link GcVolumeUtil#ALL_VOLUMES_PREFIX}
+ */
+public class AllVolumesDirectory extends ReferenceFile {","[{'comment': 'Does this need to override `isDirectory()` and return `true` ?', 'commenter': 'dlmarion'}, {'comment': 'Fixed in 2af897e39fe077d37e31e44ba8a82a3fa9969376', 'commenter': 'milleruntime'}]"
2769,server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java,"@@ -48,11 +71,32 @@ public class ServerConfigurationFactory extends ServerConfiguration {
   private final SiteConfiguration siteConfig;
   private final Supplier<SystemConfiguration> systemConfig;
 
+  private final ScheduledFuture<?> refreshTaskFuture;
+
+  private final DeleteWatcher deleteWatcher =
+      new DeleteWatcher(tableConfigs, namespaceConfigs, tableParentConfigs);","[{'comment': ""Could simplify this by making the class non-static, then it could refer to it's parent class' fields, rather than passing references to some of its fields like this."", 'commenter': 'ctubbsii'}]"
2769,server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java,"@@ -48,11 +71,32 @@ public class ServerConfigurationFactory extends ServerConfiguration {
   private final SiteConfiguration siteConfig;
   private final Supplier<SystemConfiguration> systemConfig;
 
+  private final ScheduledFuture<?> refreshTaskFuture;
+
+  private final DeleteWatcher deleteWatcher =
+      new DeleteWatcher(tableConfigs, namespaceConfigs, tableParentConfigs);
+
   public ServerConfigurationFactory(ServerContext context, SiteConfiguration siteConfig) {
     this.context = context;
     this.siteConfig = siteConfig;
     systemConfig = Suppliers.memoize(
         () -> new SystemConfiguration(context, SystemPropKey.of(context), getSiteConfiguration()));
+
+    if (context.threadPools() != null) {
+      // simplify testing - only create refresh thread when operating in a context with thread pool","[{'comment': ""I'm not sure I understand what this simplifies. This is always non-null. The only difference is which uncaught exception handler it uses."", 'commenter': 'ctubbsii'}, {'comment': 'When mocking it is null - otherwise the pool needs to added to the mock and it seemed to cause a circular dependency in the mock.   ', 'commenter': 'EdColeman'}, {'comment': 'Okay. I hope we can find another way to address this. Writing the code to specifically work around issues for a particular mocking framework seems like a less than ideal scenario.', 'commenter': 'ctubbsii'}]"
2769,server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java,"@@ -48,11 +71,32 @@ public class ServerConfigurationFactory extends ServerConfiguration {
   private final SiteConfiguration siteConfig;
   private final Supplier<SystemConfiguration> systemConfig;
 
+  private final ScheduledFuture<?> refreshTaskFuture;
+
+  private final DeleteWatcher deleteWatcher =
+      new DeleteWatcher(tableConfigs, namespaceConfigs, tableParentConfigs);
+
   public ServerConfigurationFactory(ServerContext context, SiteConfiguration siteConfig) {
     this.context = context;
     this.siteConfig = siteConfig;
     systemConfig = Suppliers.memoize(
         () -> new SystemConfiguration(context, SystemPropKey.of(context), getSiteConfiguration()));
+
+    if (context.threadPools() != null) {
+      // simplify testing - only create refresh thread when operating in a context with thread pool
+      // initialized
+      ScheduledThreadPoolExecutor threadPool = context.threadPools()
+          .createScheduledExecutorService(1, this.getClass().getSimpleName(), false);
+      Runnable refreshTask = this::verifySnapshotVersions;","[{'comment': ""I'm not sure I understand the point of this scheduled task being done inside the ServerConfigurationFactory. This class is intended to be very lightweight... just keeping simple references to configurations. It shouldn't be responsible for the snapshots contained inside those configurations. That seems like a job for the ZooBasedConfiguration class."", 'commenter': 'ctubbsii'}]"
2769,server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java,"@@ -99,10 +144,139 @@ public NamespaceConfiguration getNamespaceConfigurationForTable(TableId tableId)
   @Override
   public NamespaceConfiguration getNamespaceConfiguration(NamespaceId namespaceId) {
     return namespaceConfigs.computeIfAbsent(namespaceId, key -> {
+      context.getPropStore().registerAsListener(NamespacePropKey.of(context, namespaceId),
+          deleteWatcher);
       var conf = new NamespaceConfiguration(context, namespaceId, getSystemConfiguration());
       ConfigCheckUtil.validate(conf);
       return conf;
     });
   }
 
+  /**
+   * Check that the stored version in ZooKeeper matches the version held in the local snapshot. When
+   * a mismatch is detected, a change event is sent to the prop store which will cause a re-load. If
+   * the Zookeeper node has been deleted, the local cache entries are removed.
+   * <p>
+   * This method is designed to be called as a scheduled task, so it does not propagate exceptions
+   * so the scheduled tasks will continue to run.
+   */
+  private void verifySnapshotVersions() {
+
+    int nsRefreshCount = 0;
+    int tblRefreshCount = 0;
+
+    long refreshStart = System.nanoTime();
+
+    ZooReader zooReader = context.getZooReader();
+
+    try {
+      refresh(systemConfig.get(), zooReader);
+    } catch (Throwable t) {
+      log.debug(""Exception occurred during system config refresh"", t.getCause());
+    }
+
+    try {
+      for (Map.Entry<NamespaceId,NamespaceConfiguration> entry : namespaceConfigs.entrySet()) {
+        if (!refresh(entry.getValue(), zooReader)) {
+          namespaceConfigs.remove(entry.getKey());
+        }
+        nsRefreshCount++;
+      }
+    } catch (Throwable t) {
+      log.debug(
+          ""Exception occurred during namespace refresh - cycle may not have completed on this pass"",
+          t.getCause());
+    }
+    try {
+      for (Map.Entry<TableId,TableConfiguration> entry : tableConfigs.entrySet()) {
+        if (!refresh(entry.getValue(), zooReader)) {
+          tableConfigs.remove(entry.getKey());
+          tableParentConfigs.remove(entry.getKey());
+        }
+        tblRefreshCount++;
+      }
+    } catch (Throwable t) {
+      log.debug(
+          ""Exception occurred during table refresh - cycle may not have completed on this pass"",
+          t.getCause());
+    }
+
+    log.debug(
+        ""configuration snapshot refresh completed. Total runtime {} ms for local namespaces: {}, tables: {}"",
+        MILLISECONDS.convert(System.nanoTime() - refreshStart, NANOSECONDS), nsRefreshCount,
+        tblRefreshCount);
+  }
+
+  private boolean refresh(ZooBasedConfiguration config, ZooReader zooReader) {
+    final PropStoreKey<?> key = config.getPropStoreKey();
+    try {
+      Stat stat = zooReader.getStatus(key.getPath());
+      log.trace(""configuration snapshot refresh: stat returned: {} for {}"", stat, key);
+      if (stat == null) {
+        return false;
+      }
+      if (config.getDataVersion() != stat.getVersion()) {
+        log.debug(
+            ""configuration snapshot refresh - difference found. forcing configuration update for {}}"",
+            key);
+        config.zkChangeEvent(key);
+      }
+      // add small jitter between calls.
+      int randDelay = ThreadLocalRandom.current().nextInt(0, 23);
+      Thread.sleep(randDelay);
+    } catch (KeeperException.NoNodeException ex) {
+      config.zkChangeEvent(key);
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""Interrupted reading from ZooKeeper during snapshot refresh."",
+          ex);
+    } catch (KeeperException | IllegalStateException ex) {
+      log.debug(""configuration snapshot refresh: Exception during refresh - key"" + key, ex);
+    }
+    return true;
+  }
+
+  private static class DeleteWatcher implements PropChangeListener {
+
+    final Map<TableId,TableConfiguration> tableConfigs;
+    final Map<NamespaceId,NamespaceConfiguration> namespaceConfigs;
+    final Map<TableId,NamespaceConfiguration> tableParentConfigs;
+
+    DeleteWatcher(final Map<TableId,TableConfiguration> tableConfigs,
+        final Map<NamespaceId,NamespaceConfiguration> namespaceConfigs,
+        final Map<TableId,NamespaceConfiguration> tableParentConfigs) {
+      this.tableConfigs = tableConfigs;
+      this.namespaceConfigs = namespaceConfigs;
+      this.tableParentConfigs = tableParentConfigs;
+    }
+
+    @Override
+    public void zkChangeEvent(PropStoreKey<?> propStoreKey) {
+      // no-op. changes handled by prop store impl
+    }
+
+    @Override
+    public void cacheChangeEvent(PropStoreKey<?> propStoreKey) {
+      // no-op. changes handled by prop store impl
+    }
+
+    @Override
+    public void deleteEvent(PropStoreKey<?> propStoreKey) {
+      if (propStoreKey instanceof NamespacePropKey) {
+        log.trace(""configuration snapshot refresh: Handle namespace delete for {}"", propStoreKey);
+        namespaceConfigs.remove(((NamespacePropKey) propStoreKey).getId());
+        return;
+      }
+      if (propStoreKey instanceof TablePropKey) {
+        log.trace(""configuration snapshot refresh: Handle table delete for {}"", propStoreKey);
+        tableConfigs.remove(((TablePropKey) propStoreKey).getId());
+        tableConfigs.remove(((TablePropKey) propStoreKey).getId());","[{'comment': 'This is removed twice. I think the intent was:\r\n\r\n```suggestion\r\n        tableConfigs.remove(((TablePropKey) propStoreKey).getId());\r\n        tableParentConfigs.remove(((TablePropKey) propStoreKey).getId());\r\n```', 'commenter': 'ctubbsii'}]"
2790,server/tserver/src/main/java/org/apache/accumulo/tserver/scan/LookupTask.java,"@@ -87,23 +85,23 @@ public void run() {
       Key partNextKey = null;
       boolean partNextKeyInclusive = false;
 
-      Iterator<Entry<KeyExtent,List<Range>>> iter = session.queries.entrySet().iterator();
+      // Iterator<Entry<KeyExtent,List<Range>>> iter = session.queries.entrySet().iterator();
+      log.error(""number of extents: "" + session.queries.size() + "" -- "" + session.queries.keySet());","[{'comment': 'Is this an error? If so, could the message also include what the error / issue is?', 'commenter': 'EdColeman'}, {'comment': ""I left those lines in by accident & will remove them.  I'm thinking of adding session.queries to a LinkedList<Pair<KeyExtent, List<Range>>> so that we can add to the back and remove from the front instead of using session.queries.iterator().next() when we're not really iterating.\r\n\r\nThe fact that partScan/partNextKey/partNextKeyInclusive only report back one partial scan when there could be multiple partial scans still seems problematic, but that may be a topic for a subsequent issue,"", 'commenter': 'billoley'}, {'comment': 'I decided that adding Pair<KeyExtent, List<Range>> of an unfinished Range back to the front of the LinkedList ensures that it gets handled next.  That means that if the LookupTask ends due to time or result size then a record of the last partial scan will get returned.  If the unfinished ranges get fully scanned, then the KeyExtent will be recorded as a full scan and the record of a partial scan will be removed.  This seems to make the most sense. ', 'commenter': 'billoley'}]"
2790,server/tserver/src/main/java/org/apache/accumulo/tserver/scan/LookupTask.java,"@@ -87,23 +85,23 @@ public void run() {
       Key partNextKey = null;
       boolean partNextKeyInclusive = false;
 
-      Iterator<Entry<KeyExtent,List<Range>>> iter = session.queries.entrySet().iterator();
+      // Iterator<Entry<KeyExtent,List<Range>>> iter = session.queries.entrySet().iterator();
+      log.error(""number of extents: "" + session.queries.size() + "" -- "" + session.queries.keySet());
 
       // check the time so that the read ahead thread is not monopolized
-      while (iter.hasNext() && bytesAdded < maxResultsSize
+      while (!session.queries.isEmpty() && bytesAdded < maxResultsSize
           && (System.currentTimeMillis() - startTime) < maxScanTime) {
-        Entry<KeyExtent,List<Range>> entry = iter.next();
-
-        iter.remove();
+        KeyExtent extent = session.queries.keySet().iterator().next();","[{'comment': 'Question: Does this really walk the iterator - or is it always returning the first entry pointed at by the iterator?', 'commenter': 'EdColeman'}, {'comment': 'Was just trying to get the first element, not iterating.  In any case, I modified the code to use a LinkedList.', 'commenter': 'billoley'}]"
2790,server/tserver/src/main/java/org/apache/accumulo/tserver/scan/LookupTask.java,"@@ -124,20 +127,28 @@ public void run() {
           interruptFlag.set(false);
 
         } catch (IOException e) {
-          log.warn(""lookup failed for tablet "" + entry.getKey(), e);
+          log.warn(""lookup failed for tablet "" + extent, e);
           throw new RuntimeException(e);
         }
 
         bytesAdded += lookupResult.bytesAdded;
 
         if (lookupResult.unfinishedRanges.isEmpty()) {
-          fullScans.add(entry.getKey());
+          fullScans.add(extent);
+          // if this extent was previously saved, but now completed then reset these values
+          if (partScan != null && partScan.equals(extent)) {
+            partScan = null;
+            partNextKey = null;
+            partNextKeyInclusive = false;
+          }
         } else {
           if (lookupResult.closed) {
-            failures.put(entry.getKey(), lookupResult.unfinishedRanges);
+            failures.put(extent, lookupResult.unfinishedRanges);
           } else {
-            session.queries.put(entry.getKey(), lookupResult.unfinishedRanges);","[{'comment': 'How were we not always getting concurrent modification exceptions here?  Weird', 'commenter': 'ivakegg'}, {'comment': 'CME occurred the next time iter.next() was called, so the while condition would have to allow one more execution.  My theories are:  1) low number of yields 2) only one extent 3) yield happens on last extent 4) timeout in while coincides with yield', 'commenter': 'billoley'}]"
2790,server/tserver/src/main/java/org/apache/accumulo/tserver/scan/LookupTask.java,"@@ -79,6 +79,9 @@ public void run() {
       long maxScanTime = 4000;
 
       long startTime = System.currentTimeMillis();
+      LinkedList<Pair<KeyExtent,List<Range>>> queryQueue = new LinkedList<>();
+      session.queries.entrySet()
+          .forEach(e -> queryQueue.addLast(new Pair<>(e.getKey(), e.getValue())));","[{'comment': 'We need to make sure the session.queries is reset with what is left in the queryQueue after the loop.  Otherwise whenever the scan session is restarted (e.g. bytesAdded > maxResultsSize) then we need to restart where we left off.', 'commenter': 'ivakegg'}, {'comment': 'Committed a change to remove and add to session.queries whenever a change is made to queryQueue.  ', 'commenter': 'billoley'}]"
2790,server/tserver/src/main/java/org/apache/accumulo/tserver/scan/LookupTask.java,"@@ -114,8 +116,10 @@ public void run() {
           if (isCancelled())
             interruptFlag.set(true);
 
-          lookupResult = tablet.lookup(entry.getValue(), results, session.scanParams,
+          List<KVEntry> tabletResults = new ArrayList<>();","[{'comment': 'I wonder if a comment here on why we are doing it this way since the reason is non-obious.', 'commenter': 'ivakegg'}, {'comment': 'Done', 'commenter': 'billoley'}]"
2798,assemble/bin/accumulo-cluster,"@@ -429,6 +429,10 @@ function main() {
 
   case ""$1"" in
     create-config)
+      if [[ -f ""$conf""/cluster.yaml ]]; then
+         echo ""ERROR : $conf/cluster.yaml already exists, not overwriting""
+         exit 1
+      fi","[{'comment': '```suggestion\r\n      if [[ -f ""$conf""/cluster.yaml ]]; then\r\n        echo ""ERROR : $conf/cluster.yaml already exists, not overwriting""\r\n        exit 1\r\n      fi\r\n```', 'commenter': 'dlmarion'}, {'comment': 'Looks like there are 3 spaces vs 2.', 'commenter': 'dlmarion'}]"
2801,test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java,"@@ -2139,6 +2139,100 @@ public void testFateCommandWithSlowCompaction() throws Exception {
     if (orgProps != null) {
       System.setProperty(""accumulo.properties"", orgProps);
     }
+  }
+
+  @Test
+  public void testFateSummaryCommandWithSlowCompaction() throws Exception {
+    String namespace = ""ns1"";
+    final String table = namespace + ""."" + getUniqueNames(1)[0];
 
+    String orgProps = System.getProperty(""accumulo.properties"");
+
+    System.setProperty(""accumulo.properties"",
+        ""file://"" + getCluster().getConfig().getAccumuloPropsFile().getCanonicalPath());
+    // compact
+    ts.exec(""createnamespace "" + namespace);
+    ts.exec(""createtable "" + table);
+    ts.exec(""addsplits h m r w -t "" + table);
+    ts.exec(""offline -t "" + table);
+    ts.exec(""online h m r w -t "" + table);
+
+    // setup SlowIterator to sleep for 10 seconds
+    ts.exec(""config -t "" + table
+        + "" -s table.iterator.majc.slow=1,org.apache.accumulo.test.functional.SlowIterator"");
+    ts.exec(""config -t "" + table + "" -s table.iterator.majc.slow.opt.sleepTime=10000"");
+
+    // make two files
+    ts.exec(""insert a1 b c v_a1"");
+    ts.exec(""insert a2 b c v_a2"");
+    ts.exec(""flush -w"");
+    ts.exec(""insert x1 b c v_x1"");
+    ts.exec(""insert x2 b c v_x2"");
+    ts.exec(""flush -w"");
+
+    // no transactions running
+
+    String cmdOut =
+        ts.exec(""fate -summary -np json -t NEW IN_PROGRESS FAILED"", true, ""reportTime"", true);
+    // strip command included in shell output
+    String jsonOut = cmdOut.substring(cmdOut.indexOf(""{""));
+    SummaryReport report = SummaryReport.fromJson(jsonOut);
+
+    // validate blank report
+    assertNotNull(report);
+    assertNotEquals(0, report.getReportTime());
+    assertEquals(Set.of(""NEW"", ""IN_PROGRESS"", ""FAILED""), report.getStatusFilterNames());
+    assertEquals(Map.of(), report.getStatusCounts());
+    assertEquals(Map.of(), report.getStepCounts());
+    assertEquals(Map.of(), report.getCmdCounts());
+    assertEquals(Set.of(), report.getFateDetails());
+
+    ts.exec(""fate -summary -np"", true, ""Report Time:"", true);
+
+    // merge two files into one
+    ts.exec(""compact -t "" + table);
+    Thread.sleep(1_000);
+    // start 2nd transaction
+    ts.exec(""compact -t "" + table);
+    Thread.sleep(3_000);
+
+    // 2 compactions should be running so parse the output to get one of the transaction ids
+    log.debug(""Calling fate summary"");
+    ts.exec(""fate -summary -np"", true, ""Report Time:"", true);","[{'comment': 'this is called here and then immediately after. Is that needed?', 'commenter': 'dlmarion'}, {'comment': 'It is testing different option configurations, In this case no filters.  The first call \r\n```\r\nts.exec(""fate -summary -np"", true, ""Report Time:"", true);\r\n```\r\nbasically checks the output is created - the call succeeds and has a report time.  The second call asks for json output (and no filters) - with the json, it is easier to validate additional things, more that the command didn\'t error.', 'commenter': 'EdColeman'}, {'comment': 'I see, the calls are different.\r\n\r\n> The first call basically checks the output is created - the call succeeds and has a report time. \r\n\r\nThe output is not captured nor checked. You are relying on it throwing an exception to fail the test?', 'commenter': 'dlmarion'}, {'comment': 'The `ts.exec` call requires that the command 1) completes sucessfully, 2) contains the expected text (Report Time) - so it a basic test that validates that the call can be submitted and returns something.\r\n\r\nWith the json format, it was much easier to pull out expected vales rather than parsing the output.  ', 'commenter': 'EdColeman'}, {'comment': 'Looking further at what type of object `ts` is, this makes more sense.', 'commenter': 'dlmarion'}]"
2801,shell/src/main/java/org/apache/accumulo/shell/commands/summaryReport/SummaryReport.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.shell.commands.summaryReport;
+
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.time.temporal.ChronoUnit;
+import java.util.ArrayList;
+import java.util.EnumSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.accumulo.fate.AdminUtil;
+import org.apache.accumulo.fate.ReadOnlyTStore;
+
+import com.google.gson.Gson;
+import com.google.gson.GsonBuilder;
+
+public class SummaryReport {","[{'comment': ""I like the separate classes but it is confusing to have a `summaryReport` package when that is not the command. Maybe include them in the same package as FateCommand? Or Create a new package called `fate`?  If you don't want to move `FateCommand` you could just prefix the new classes with `FateCommand`. "", 'commenter': 'milleruntime'}, {'comment': ""I posted a change renaming the package to `fateCommand` and added `Fate`  so that its `FateSummaryReport` and `FateTxnDetails`\r\n\r\nI didn't move FateCommand itself - there seems to be some value having all of the commands at the same level.\r\n\r\n@milleruntime - Better?\r\n"", 'commenter': 'EdColeman'}]"
2811,core/src/main/java/org/apache/accumulo/core/iteratorsImpl/system/LocalityGroupIterator.java,"@@ -163,26 +164,14 @@ static final Collection<LocalityGroup> _seek(HeapIterator hiter, LocalityGroupCo
       Range range, Collection<ByteSequence> columnFamilies, boolean inclusive) throws IOException {
     hiter.clear();
 
-    Set<ByteSequence> cfSet;
-    if (columnFamilies.isEmpty()) {
-      cfSet = Collections.emptySet();
-    } else {
-      if (columnFamilies instanceof Set<?>) {
-        cfSet = (Set<ByteSequence>) columnFamilies;
-      } else {
-        cfSet = new HashSet<>();
-        cfSet.addAll(columnFamilies);
-      }
-    }
+    final Set<ByteSequence> cfSet = getCfSet(columnFamilies);
 
     // determine the set of groups to use
-    Collection<LocalityGroup> groups = Collections.emptyList();
+    final Collection<LocalityGroup> groups;
 
     // if no column families specified, then include all groups unless !inclusive
     if (cfSet.isEmpty()) {
-      if (!inclusive) {
-        groups = lgContext.groups;
-      }
+      groups = inclusive ? List.of() : lgContext.groups;
     } else {
       groups = new HashSet<>();","[{'comment': 'You might be able to create a method for the work done in this else. Something like getGroups().', 'commenter': 'milleruntime'}, {'comment': 'Or create a method for all the logic for ""determining the set of groups to use"".', 'commenter': 'milleruntime'}, {'comment': 'Extracted all of the logic into its own method in [4b51e91](https://github.com/apache/accumulo/pull/2811/commits/4b51e91e20510be1c5913e68e0d8fbba2be0d361)', 'commenter': 'DomGarguilo'}]"
2811,server/tserver/src/main/java/org/apache/accumulo/tserver/TabletClientHandler.java,"@@ -1202,33 +1202,25 @@ public void flush(TInfo tinfo, TCredentials credentials, String lock, String tab
       throw new RuntimeException(e);
     }
 
-    ArrayList<Tablet> tabletsToFlush = new ArrayList<>();
-
     KeyExtent ke = new KeyExtent(TableId.of(tableId), ByteBufferUtil.toText(endRow),
         ByteBufferUtil.toText(startRow));
 
-    for (Tablet tablet : server.getOnlineTablets().values()) {
-      if (ke.overlaps(tablet.getExtent())) {
-        tabletsToFlush.add(tablet);
-      }
-    }
-
-    Long flushID = null;
+    final Long[] flushID = {null};","[{'comment': 'I think this workaround to make the object final could give you a ConcurrentModificationException or some type of concurrency problems. They made it a compile time error for a reason, this answer here gives a good explanation https://stackoverflow.com/questions/34865383/variable-used-in-lambda-expression-should-be-final-or-effectively-final\r\n\r\nI am not sure how the equivalent logic could be accomplished with a stream. It may be better to use a traditional for loop.', 'commenter': 'milleruntime'}, {'comment': 'Good catch. Should be fixed by [09986b2](https://github.com/apache/accumulo/pull/2811/commits/09986b2f730c2e5cd0667de5bb01a0f04615955b).', 'commenter': 'DomGarguilo'}]"
2811,core/src/main/java/org/apache/accumulo/core/data/Key.java,"@@ -975,30 +981,26 @@ public void write(DataOutput out) throws IOException {
    * @return true if specified parts of keys match, false otherwise
    */
   public boolean equals(Key other, PartialKey part) {
-    switch (part) {
-      case ROW:
-        return isEqual(row, other.row);
-      case ROW_COLFAM:
-        return isEqual(row, other.row) && isEqual(colFamily, other.colFamily);
-      case ROW_COLFAM_COLQUAL:
-        return isEqual(row, other.row) && isEqual(colFamily, other.colFamily)
-            && isEqual(colQualifier, other.colQualifier);
-      case ROW_COLFAM_COLQUAL_COLVIS:
-        return isEqual(row, other.row) && isEqual(colFamily, other.colFamily)
-            && isEqual(colQualifier, other.colQualifier)
-            && isEqual(colVisibility, other.colVisibility);
-      case ROW_COLFAM_COLQUAL_COLVIS_TIME:
-        return isEqual(row, other.row) && isEqual(colFamily, other.colFamily)
-            && isEqual(colQualifier, other.colQualifier)
-            && isEqual(colVisibility, other.colVisibility) && timestamp == other.timestamp;
-      case ROW_COLFAM_COLQUAL_COLVIS_TIME_DEL:
-        return isEqual(row, other.row) && isEqual(colFamily, other.colFamily)
-            && isEqual(colQualifier, other.colQualifier)
-            && isEqual(colVisibility, other.colVisibility) && timestamp == other.timestamp
-            && deleted == other.deleted;
-      default:
-        throw new IllegalArgumentException(""Unrecognized partial key specification "" + part);
-    }
+    boolean result = isEqual(row, other.row);
+    if (part == ROW)
+      return result;
+    result &= isEqual(colFamily, other.colFamily);
+    if (part == ROW_COLFAM)
+      return result;
+    result &= isEqual(colQualifier, other.colQualifier);
+    if (part == ROW_COLFAM_COLQUAL)
+      return result;
+    result &= isEqual(colVisibility, other.colVisibility);
+    if (part == ROW_COLFAM_COLQUAL_COLVIS)
+      return result;
+    result &= (timestamp == other.timestamp);
+    if (part == ROW_COLFAM_COLQUAL_COLVIS_TIME)
+      return result;
+    result &= (deleted == other.deleted);
+    if (part == ROW_COLFAM_COLQUAL_COLVIS_TIME_DEL)","[{'comment': ""The way the code was written before it would short circuit.  Like in the old code if the rows were not equal it would not do any comparisons after that.  In the new code when the rows are not equal it still compares the rest of the fields. Seems like this could really make a difference for some data.  Not sure we should manually short circuit because I don't know how that would compare to what the compiler does."", 'commenter': 'keith-turner'}, {'comment': ""Good catch. I'll revert this to how it was."", 'commenter': 'DomGarguilo'}, {'comment': '@keith-turner this has been reverted in 5238f81', 'commenter': 'DomGarguilo'}]"
2811,core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java,"@@ -135,6 +118,17 @@ else if (cvCommonPrefixLen > 1)
       fieldsSame |= DELETED;
   }
 
+  private int getCommonPrefixLen(ByteSequence prevKeyScratch, ByteSequence keyScratch, byte rowSame,
+      byte commonPrefix) {
+    int commonPrefixLen = getCommonPrefix(prevKeyScratch, keyScratch);
+    if (commonPrefixLen == -1) {
+      fieldsSame |= rowSame;","[{'comment': 'Need some other variable name for `rowSame` (because it not always a row field thats being dealt with), tried using `fieldBit` instead.\r\n\r\n```suggestion\r\n  private int getCommonPrefixLen(ByteSequence prevKeyScratch, ByteSequence keyScratch, byte fieldBit,\r\n      byte commonPrefix) {\r\n    int commonPrefixLen = getCommonPrefix(prevKeyScratch, keyScratch);\r\n    if (commonPrefixLen == -1) {\r\n      fieldsSame |= fieldBit;\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Addressed in c86dac5', 'commenter': 'DomGarguilo'}]"
2811,core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java,"@@ -221,6 +188,17 @@ public void readFields(DataInput in) throws IOException {
     this.prevKey = this.key;
   }
 
+  private byte[] getData(DataInput in, byte same, byte commonPrefix, Supplier<ByteSequence> data)","[{'comment': 'I suggested using `fieldBit` in a previous comment instead of `rowSame`.  What ever you end using in the other method it would be good to make the variable name here `same` consistent with the name in the other method.  Could change `rowSame` to `same` in the other method to make it consistent with this method.\r\n\r\nBelow are suggested renamings for overall consistency.\r\n\r\n```suggestion\r\n  private byte[] getData(DataInput in, byte fieldBit, byte commonPrefix, Supplier<ByteSequence> fieldReader)\r\n```', 'commenter': 'keith-turner'}, {'comment': 'addressed in c86dac5', 'commenter': 'DomGarguilo'}]"
2811,core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java,"@@ -174,40 +168,13 @@ public void readFields(DataInput in) throws IOException {
       fieldsPrefixed = 0;
     }
 
-    byte[] row, cf, cq, cv;
-    long ts;
-
-    if ((fieldsSame & ROW_SAME) == ROW_SAME) {
-      row = prevKey.getRowData().toArray();
-    } else if ((fieldsPrefixed & ROW_COMMON_PREFIX) == ROW_COMMON_PREFIX) {
-      row = readPrefix(in, prevKey.getRowData());
-    } else {
-      row = read(in);
-    }
-
-    if ((fieldsSame & CF_SAME) == CF_SAME) {
-      cf = prevKey.getColumnFamilyData().toArray();
-    } else if ((fieldsPrefixed & CF_COMMON_PREFIX) == CF_COMMON_PREFIX) {
-      cf = readPrefix(in, prevKey.getColumnFamilyData());
-    } else {
-      cf = read(in);
-    }
-
-    if ((fieldsSame & CQ_SAME) == CQ_SAME) {
-      cq = prevKey.getColumnQualifierData().toArray();
-    } else if ((fieldsPrefixed & CQ_COMMON_PREFIX) == CQ_COMMON_PREFIX) {
-      cq = readPrefix(in, prevKey.getColumnQualifierData());
-    } else {
-      cq = read(in);
-    }
+    final byte[] row, cf, cq, cv;
+    final long ts;
 
-    if ((fieldsSame & CV_SAME) == CV_SAME) {
-      cv = prevKey.getColumnVisibilityData().toArray();
-    } else if ((fieldsPrefixed & CV_COMMON_PREFIX) == CV_COMMON_PREFIX) {
-      cv = readPrefix(in, prevKey.getColumnVisibilityData());
-    } else {
-      cv = read(in);
-    }
+    row = getData(in, ROW_SAME, ROW_COMMON_PREFIX, () -> prevKey.getRowData());","[{'comment': 'This is a really nice change.', 'commenter': 'keith-turner'}]"
2811,core/src/main/java/org/apache/accumulo/core/iteratorsImpl/system/LocalityGroupIterator.java,"@@ -207,35 +208,33 @@ static final Collection<LocalityGroup> _seek(HeapIterator hiter, LocalityGroupCo
        * other
        */
       if (!inclusive) {
-        for (Entry<ByteSequence,LocalityGroup> entry : lgContext.groupByCf.entrySet()) {
-          if (!cfSet.contains(entry.getKey())) {
-            groups.add(entry.getValue());
-          }
-        }
+        lgContext.groupByCf.entrySet().stream().filter(entry -> !cfSet.contains(entry.getKey()))
+            .map(Entry::getValue).forEach(groups::add);
       } else if (lgContext.groupByCf.size() <= cfSet.size()) {
-        for (Entry<ByteSequence,LocalityGroup> entry : lgContext.groupByCf.entrySet()) {
-          if (cfSet.contains(entry.getKey())) {
-            groups.add(entry.getValue());
-          }
-        }
+        lgContext.groupByCf.entrySet().stream().filter(entry -> cfSet.contains(entry.getKey()))
+            .map(Entry::getValue).forEach(groups::add);
       } else {
-        for (ByteSequence cf : cfSet) {
-          LocalityGroup group = lgContext.groupByCf.get(cf);
-          if (group != null) {
-            groups.add(group);
-          }
-        }
+        cfSet.stream().map(lgContext.groupByCf::get).filter(Objects::nonNull).forEach(groups::add);
       }
     }
 
-    for (LocalityGroup lgr : groups) {
-      lgr.getIterator().seek(range, EMPTY_CF_SET, false);
-      hiter.addSource(lgr.getIterator());
-    }
-
     return groups;
   }
 
+  private static Set<ByteSequence> getCfSet(Collection<ByteSequence> columnFamilies) {
+    final Set<ByteSequence> cfSet;
+    if (columnFamilies.isEmpty()) {
+      cfSet = Collections.emptySet();
+    } else {
+      if (columnFamilies instanceof Set<?>) {
+        cfSet = (Set<ByteSequence>) columnFamilies;
+      } else {
+        cfSet = new HashSet<>(columnFamilies);","[{'comment': 'Could do the following, might be better since its a fixed size set that is not later mutated.\r\n\r\n```suggestion\r\n        cfSet = Set.copyOf(columnFamilies);\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Addressed in c86dac5', 'commenter': 'DomGarguilo'}]"
2811,server/tserver/src/main/java/org/apache/accumulo/tserver/scan/LookupTask.java,"@@ -95,9 +95,14 @@ public void run() {
       // check the time so that the read ahead thread is not monopolized
       while (iter.hasNext() && bytesAdded < maxResultsSize
           && (System.currentTimeMillis() - startTime) < maxScanTime) {
-        Entry<KeyExtent,List<Range>> entry = iter.next();
-        KeyExtent extent = entry.getKey();
-        List<Range> ranges = entry.getValue();
+        final KeyExtent extent;
+        final List<Range> ranges;
+        {
+          final Entry<KeyExtent,List<Range>> entry = iter.next();
+          extent = entry.getKey();
+          ranges = entry.getValue();
+        }","[{'comment': 'Was this little code block created just to narrowly scope `entry`?', 'commenter': 'keith-turner'}, {'comment': '> Was this little code block created just to narrowly scope `entry`?\r\n\r\nYes. Maybe its not worth doing as it might cause confusion? What do you think?', 'commenter': 'DomGarguilo'}, {'comment': ""> Yes. Maybe its not worth doing as it might cause confusion? What do you think?\r\n\r\nWhen I first saw it I had no idea why it was there.  I looked at it for a bit and guessed why it might be there. I think its an interesting technique and it reminds me of how in golang a function can return zero or more things.  Where in java a functions can only return zero or one things.   I really like how golang works.   To bad we can't write the following in java\r\n\r\n```java\r\n(entry,ranges) = iter.next();\r\n```\r\n\r\nAnyway, I am not completely sure if it should stay or go, but I am leaning towards removing it.   One thing that helped me think through this is asking would this be a good practice throughout the code?   It seems like this technique could add a lot of noise that may make the code harder to read if used frequently, so that's a reason not to use it elsewhere and here.  The tight scoping could help avoid some bugs in larger methods, but maybe if we have a large method we just need to make it smaller like you have done for methods in this PR."", 'commenter': 'keith-turner'}, {'comment': 'Addressed in c86dac5', 'commenter': 'DomGarguilo'}]"
2811,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/CompactableImpl.java,"@@ -396,80 +396,89 @@ Set<StoredTabletFile> getCandidates(Set<StoredTabletFile> currFiles, CompactionK
           if (isCompactionStratConfigured)
             return Set.of();
 
-          switch (selectStatus) {
-            case NOT_ACTIVE:
-            case CANCELED: {
-              Set<StoredTabletFile> candidates = new HashSet<>(currFiles);
-              candidates.removeAll(allCompactingFiles);
-              return Collections.unmodifiableSet(candidates);
-            }
-            case NEW:
-            case SELECTING:
-              return Set.of();
-            case SELECTED: {
-              Set<StoredTabletFile> candidates = new HashSet<>(currFiles);
-              candidates.removeAll(allCompactingFiles);
-              if (getNanoTime() - selectedTimeNanos < selectedExpirationDuration.toNanos()) {
-                candidates.removeAll(selectedFiles);
-              }
-              return Collections.unmodifiableSet(candidates);
-            }
-            case RESERVED: {
-              Set<StoredTabletFile> candidates = new HashSet<>(currFiles);
-              candidates.removeAll(allCompactingFiles);
-              candidates.removeAll(selectedFiles);
-              return Collections.unmodifiableSet(candidates);
-            }
-            default:
-              throw new AssertionError();
-          }
+          return handleSystemCompaction(currFiles);
         }
         case SELECTOR:
           // intentional fall through
         case USER:
-          switch (selectStatus) {
-            case NOT_ACTIVE:
-            case NEW:
-            case SELECTING:
-            case CANCELED:
-              return Set.of();
-            case SELECTED:
-            case RESERVED: {
-              if (selectKind == kind) {
-                Set<StoredTabletFile> candidates = new HashSet<>(selectedFiles);
-                candidates.removeAll(allCompactingFiles);
-                candidates = Collections.unmodifiableSet(candidates);
-                Preconditions.checkState(currFiles.containsAll(candidates),
-                    ""selected files not in all files %s %s"", candidates, currFiles);
-                return candidates;
-              } else {
-                return Set.of();
-              }
-            }
-            default:
-              throw new AssertionError();
-          }
+          return handleUserSelectorCompaction(currFiles, kind);
         case CHOP: {
-          switch (chopStatus) {
-            case NOT_ACTIVE:
-            case SELECTING:
-            case MARKING:
-              return Set.of();
-            case SELECTED: {
-              if (selectStatus == FileSelectionStatus.NEW
-                  || selectStatus == FileSelectionStatus.SELECTING)
-                return Set.of();
-
-              var filesToChop = getFilesToChop(currFiles);
-              filesToChop.removeAll(allCompactingFiles);
-              if (selectStatus == FileSelectionStatus.SELECTED
-                  || selectStatus == FileSelectionStatus.RESERVED)
-                filesToChop.removeAll(selectedFiles);
-              return Collections.unmodifiableSet(filesToChop);
-            }
-            default:
-              throw new AssertionError();
+          return handleChopCompaction(currFiles);
+        }
+        default:
+          throw new AssertionError();
+      }
+    }
+
+    private Set<StoredTabletFile> handleChopCompaction(Set<StoredTabletFile> currFiles) {
+      switch (chopStatus) {
+        case NOT_ACTIVE:
+        case SELECTING:
+        case MARKING:
+          return Set.of();
+        case SELECTED: {
+          if (selectStatus == FileSelectionStatus.NEW
+              || selectStatus == FileSelectionStatus.SELECTING)
+            return Set.of();
+
+          var filesToChop = getFilesToChop(currFiles);
+          filesToChop.removeAll(allCompactingFiles);
+          if (selectStatus == FileSelectionStatus.SELECTED
+              || selectStatus == FileSelectionStatus.RESERVED)
+            filesToChop.removeAll(selectedFiles);
+          return Collections.unmodifiableSet(filesToChop);
+        }
+        default:
+          throw new AssertionError();
+      }
+    }
+
+    private Set<StoredTabletFile> handleUserSelectorCompaction(Set<StoredTabletFile> currFiles,
+        CompactionKind kind) {
+      switch (selectStatus) {
+        case NOT_ACTIVE:
+        case NEW:
+        case SELECTING:
+        case CANCELED:
+          return Set.of();
+        case SELECTED:
+        case RESERVED: {
+          if (selectKind == kind) {
+            Set<StoredTabletFile> candidates = Sets.difference(selectedFiles, allCompactingFiles);
+            Preconditions.checkState(currFiles.containsAll(candidates),
+                ""selected files not in all files %s %s"", candidates, currFiles);
+            return Collections.unmodifiableSet(candidates);","[{'comment': 'This code is called in a sync block in which `selectedFiles` and `allCompactingFiles` will not be changing.  The functions `Sets.difference()` and `Collections.unmodifiableSet` may keep references to the passed in sets but will not copy.  So this function may return a set that internally references   `selectedFiles` and `allCompactingFiles` which may be changing after the method returns.  Its therefore important to copy as the original code did.\r\n\r\nThe following change will copy.  I have no idea if its less or more efficient than the original code, but I think `Sets.difference()` plus `Set.copyOf()` is more readable than the original code so I would go with it. \r\n\r\n```suggestion\r\n            //must create a copy because the sets passed to Sets.difference could change after this method returns\r\n            return Set.copyOf(candidates);\r\n```\r\n\r\nI looked around at the other changes and I think all of those copy, but it would be good if you could double check for any other cases like this in the changes in this file.', 'commenter': 'keith-turner'}, {'comment': 'Addressed in 7b5a7d9. I added `Set.copyOf()` to one other spot so take a look and see what you think.', 'commenter': 'DomGarguilo'}, {'comment': '> I added Set.copyOf() to one other spot so take a look and see what you think.\r\n\r\nyeah that needed the same change, I missed that.', 'commenter': 'keith-turner'}]"
2811,core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java,"@@ -174,40 +168,13 @@ public void readFields(DataInput in) throws IOException {
       fieldsPrefixed = 0;
     }
 
-    byte[] row, cf, cq, cv;
-    long ts;
-
-    if ((fieldsSame & ROW_SAME) == ROW_SAME) {
-      row = prevKey.getRowData().toArray();
-    } else if ((fieldsPrefixed & ROW_COMMON_PREFIX) == ROW_COMMON_PREFIX) {
-      row = readPrefix(in, prevKey.getRowData());
-    } else {
-      row = read(in);
-    }
+    final byte[] row, cf, cq, cv;
+    final long ts;
 
-    if ((fieldsSame & CF_SAME) == CF_SAME) {
-      cf = prevKey.getColumnFamilyData().toArray();
-    } else if ((fieldsPrefixed & CF_COMMON_PREFIX) == CF_COMMON_PREFIX) {
-      cf = readPrefix(in, prevKey.getColumnFamilyData());
-    } else {
-      cf = read(in);
-    }
-
-    if ((fieldsSame & CQ_SAME) == CQ_SAME) {
-      cq = prevKey.getColumnQualifierData().toArray();
-    } else if ((fieldsPrefixed & CQ_COMMON_PREFIX) == CQ_COMMON_PREFIX) {
-      cq = readPrefix(in, prevKey.getColumnQualifierData());
-    } else {
-      cq = read(in);
-    }
-
-    if ((fieldsSame & CV_SAME) == CV_SAME) {
-      cv = prevKey.getColumnVisibilityData().toArray();
-    } else if ((fieldsPrefixed & CV_COMMON_PREFIX) == CV_COMMON_PREFIX) {
-      cv = readPrefix(in, prevKey.getColumnVisibilityData());
-    } else {
-      cv = read(in);
-    }
+    row = getData(in, ROW_SAME, ROW_COMMON_PREFIX, () -> prevKey.getRowData());
+    cf = getData(in, CF_SAME, CF_COMMON_PREFIX, () -> prevKey.getColumnFamilyData());
+    cq = getData(in, CQ_SAME, CQ_COMMON_PREFIX, () -> prevKey.getColumnQualifierData());
+    cv = getData(in, CV_SAME, CV_COMMON_PREFIX, () -> prevKey.getColumnVisibilityData());","[{'comment': 'I love the use of lambdas, but I think these can just use the form like `prevKey::getColumnFamilyData`, right?', 'commenter': 'ctubbsii'}, {'comment': 'Yea looks like it. Nice catch. Included in 59e8357', 'commenter': 'DomGarguilo'}, {'comment': 'For some reason this is causing some tests to fail so I am going to revert the changes in 59e8357', 'commenter': 'DomGarguilo'}, {'comment': 'Reverted in [9a042ab](https://github.com/apache/accumulo/pull/2811/commits/9a042ab29a668c1e8a8d036ebf724f95ceedd8c6)', 'commenter': 'DomGarguilo'}, {'comment': 'Can you provide more details on the test failures this causes? It seems odd that it would cause test failures, since this should just be syntactic sugar and not a functional change.', 'commenter': 'ctubbsii'}, {'comment': ""Oh, nevermind. I think I see the test failures. It looks like you can't use `variable::method` on a null variable, but `() -> variable.method()` works fine, so long as it doesn't actually get executed. That's a bit weird, but I'm okay leaving this as-is if it works around a test failure."", 'commenter': 'ctubbsii'}, {'comment': 'The test errors can be seen [here](https://github.com/apache/accumulo/actions/runs/3152941923/jobs/5128865689#step:6:2393)\r\n\r\nhere are the failing tests:\r\n```\r\n[ERROR] Errors: \r\n[ERROR]   RFileClientTest.testCache:523->lambda$testCache$0:525 » NullPointer\r\n[ERROR]   BlockIndexTest.test1:91 » NullPointer\r\n[ERROR]   BlockIndexTest.testSame:163 » NullPointer\r\n[ERROR]   RFileTest.test11:1213 » NullPointer\r\n[ERROR]   RFileTest.test17:1409 » NullPointer\r\n[ERROR]   RFileTest.test18:1542->t18Verify:1494 » NullPointer\r\n[ERROR]   RFileTest.test3:472 » NullPointer\r\n[ERROR]   RFileTest.test8:758 » NullPointer\r\n[ERROR]   RFileTest.testEncRFile11:1842->test11:1213 » NullPointer\r\n[ERROR]   RFileTest.testEncRFile17:1876->test17:1409 » NullPointer\r\n[ERROR]   RFileTest.testEncRFile18:1882->test18:1542->t18Verify:1494 » NullPointer\r\n[ERROR]   RFileTest.testEncRFile3:1786->test3:472 » NullPointer\r\n[ERROR]   RFileTest.testEncRFile8:1821->test8:758 » NullPointer\r\n[ERROR]   RFileTest.testEncSample:2195->testSampleLG:2174->checkSample:2006 » NullPointer\r\n[ERROR]   RFileTest.testEncryptedRFiles:1898->test3:472 » NullPointer\r\n[ERROR]   RFileTest.testOldVersions:1684->runVersionTest:1727 » NullPointer\r\n[ERROR]   RFileTest.testOldVersionsWithCrypto:1693->runVersionTest:1727 » NullPointer\r\n[ERROR]   RFileTest.testReseekUnconsumed:1634 » NullPointer\r\n[ERROR]   RFileTest.testSampleLG:2174->checkSample:2006 » NullPointe\r\n```\r\nLocally i kept stepping back through commits to determine that commit was causing these failures. I also think its weird that this causes failures but it seems like it is.', 'commenter': 'DomGarguilo'}]"
2811,core/src/main/java/org/apache/accumulo/core/spi/balancer/SimpleLoadBalancer.java,"@@ -297,10 +275,7 @@ List<TabletMigration> move(ServerCounts tooMuch, ServerCounts tooLittle, int cou
        * is only one tabletserver that holds all of the tablets. Here we check to see if in fact
        * that is the case and if so set the value to 0.
        */
-      tooLittleCount = tooLittleMap.get(table);
-      if (tooLittleCount == null) {
-        tooLittleCount = 0;
-      }
+      Integer tooLittleCount = tooLittleMap.getOrDefault(table, 0);","[{'comment': ""I briefly looked, and didn't see an Integer.ZERO. Such a thing would be useful here, if it existed, so we don't have to keep boxing `0`. But, I'm sure this is fine. The JRE can optimize stuff like this if it wanted to."", 'commenter': 'ctubbsii'}]"
2817,server/base/src/main/java/org/apache/accumulo/server/util/Admin.java,"@@ -217,6 +226,9 @@ public void execute(final String[] args) {
     RandomizeVolumesCommand randomizeVolumesOpts = new RandomizeVolumesCommand();
     cl.addCommand(""randomizeVolumes"", randomizeVolumesOpts);
 
+    TabletServerLocksCommand tServerLocksOpts = new TabletServerLocksCommand();
+    cl.addCommand(""tServerLocks"", tServerLocksOpts);","[{'comment': 'Could call the command `locks`. That is a lot shorter and easier to type.', 'commenter': 'milleruntime'}]"
2817,server/base/src/main/java/org/apache/accumulo/server/util/Admin.java,"@@ -162,6 +162,15 @@ static class RandomizeVolumesCommand {
     String tableName = null;
   }
 
+  @Parameters(commandDescription = ""List or delete Tablet Server locks"")
+  static class TabletServerLocksCommand {
+    @Parameter(names = ""-list"", description = ""print the tablet server locks"")
+    boolean list = false;
+
+    @Parameter(names = ""-delete"", description = ""specific a tablet server lock to delete"")","[{'comment': '```suggestion\r\n    @Parameter(names = ""-delete"", description = ""specify a tablet server lock to delete"")\r\n```', 'commenter': 'milleruntime'}]"
2817,server/base/src/main/java/org/apache/accumulo/server/util/TabletServerLocks.java,"@@ -41,38 +41,50 @@ static class Opts extends Help {
     String delete = null;
   }
 
-  public static void main(String[] args) throws Exception {
-
-    try (var context = new ServerContext(SiteConfiguration.auto())) {
-      String tserverPath = context.getZooKeeperRoot() + Constants.ZTSERVERS;
-      Opts opts = new Opts();
-      opts.parseArgs(TabletServerLocks.class.getName(), args);
+  public static void tabletServerLocks(String commandName, final ServerContext context,
+      final String lock, final boolean list, final String delete) throws Exception {
 
-      ZooCache cache = context.getZooCache();
-      ZooReaderWriter zoo = context.getZooReaderWriter();
+    String tserverPath = context.getZooKeeperRoot() + Constants.ZTSERVERS;
 
-      if (opts.list) {
+    ZooCache cache = context.getZooCache();
+    ZooReaderWriter zoo = context.getZooReaderWriter();
 
-        List<String> tabletServers = zoo.getChildren(tserverPath);
+    if (list) {
+      List<String> tabletServers = zoo.getChildren(tserverPath);
 
-        for (String tabletServer : tabletServers) {
-          var zLockPath = ServiceLock.path(tserverPath + ""/"" + tabletServer);
-          byte[] lockData = ServiceLock.getLockData(cache, zLockPath, null);
-          String holder = null;
-          if (lockData != null) {
-            holder = new String(lockData, UTF_8);
-          }
-
-          System.out.printf(""%32s %16s%n"", tabletServer, holder);
+      for (String tabletServer : tabletServers) {
+        var zLockPath = ServiceLock.path(tserverPath + ""/"" + tabletServer);
+        byte[] lockData = ServiceLock.getLockData(cache, zLockPath, null);
+        String holder = null;
+        if (lockData != null) {
+          holder = new String(lockData, UTF_8);
         }
-      } else if (opts.delete != null) {
-        ServiceLock.deleteLock(zoo, ServiceLock.path(tserverPath + ""/"" + args[1]));
-      } else {
-        System.out.println(
-            ""Usage : "" + TabletServerLocks.class.getName() + "" -list|-delete <tserver lock>"");
+
+        System.out.printf(""%32s %16s%n"", tabletServer, holder);
       }
+    } else if (delete != null) {
+      if (lock == null) {
+        printUsage(commandName);
+      }
+      ServiceLock.deleteLock(zoo, ServiceLock.path(tserverPath + ""/"" + lock));
+    } else {
+      printUsage(commandName);
+    }
+  }
+
+  public static void main(String[] args) throws Exception {
 
+    try (var context = new ServerContext(SiteConfiguration.auto())) {","[{'comment': 'Should this command also aim to refactor the use of SiteConfiguration like in issue #2155?', 'commenter': 'Manno15'}, {'comment': 'Yeah that is a good point. This one may need more refactoring to allow for that though. I think the main method should probably be dropped too if its going to be run through the Admin command. Then there should be a way to get the `ServerContext` from Admin.', 'commenter': 'milleruntime'}, {'comment': ""See Christopher's [comment](https://github.com/apache/accumulo/pull/2817#issuecomment-1208327394) about the difference between running in the Shell vs Admin."", 'commenter': 'milleruntime'}]"
2824,server/base/src/main/java/org/apache/accumulo/server/GarbageCollectionLogger.java,"@@ -35,7 +36,7 @@ public class GarbageCollectionLogger {
   private final HashMap<String,Long> prevGcTime = new HashMap<>();
   private long lastMemorySize = 0;
   private long gcTimeIncreasedCount = 0;
-  private static long lastMemoryCheckTime = 0;
+  private static AtomicLong lastMemoryCheckTime = new AtomicLong(0);","[{'comment': 'This should be final.\r\n\r\n```suggestion\r\n  private static final AtomicLong lastMemoryCheckTime = new AtomicLong(0);\r\n```', 'commenter': 'ctubbsii'}]"
2824,pom.xml,"@@ -720,7 +720,7 @@
         <plugin>
           <groupId>com.github.spotbugs</groupId>
           <artifactId>spotbugs-maven-plugin</artifactId>
-          <version>4.5.3.0</version>
+          <version>4.7.1.0</version>","[{'comment': ""I had been working on getting us to 4.7.0.0. It generated a bunch of new warnings. If this passes, I'm wondering if they rolled some of those changes back a bit for 4.7.1.0, due to them being too aggressive?"", 'commenter': 'ctubbsii'}]"
2847,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/tables.ftl,"@@ -87,9 +87,14 @@
           tableList.ajax.reload(null, false ); // user paging is not reset on reload
         }
       </script>
+      <div class=""row"">
+         <div class=""col-xs-12"">
+            <h3>Table Overview</h3>
+         </div>
+      </div>
       <div>
-        <table id=""tableList"" class=""table table-bordered table-striped table-condensed"">
-          <caption><span class=""table-caption"">${tablesTitle}</span><br />","[{'comment': 'Why were all of the field substitutions (`${}`) replaced with hardcoded values?', 'commenter': 'DomGarguilo'}, {'comment': ""I put captions on each table and sometimes the 'title' of the page (that was at the top of the page) worked as well for a table caption so I added a different, hardcoded value for the page's overall title. I put the field substitution values back for the pages' titles and just changed the individual tables' captions."", 'commenter': 'AlbertWhitlock'}]"
2847,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/tservers.ftl,"@@ -18,39 +18,15 @@
     under the License.
 
 -->
-      <div class=""row"">
+    <div class=""row"">
         <div class=""col-xs-12"">
-          <h3>${title}</h3>
+            <h3>Tablet Server Overview</h3>
         </div>
-      </div>
+    </div>","[{'comment': 'looks like this div is out of line with the others. also the `<h3>` has one or two extra spaces added.', 'commenter': 'DomGarguilo'}]"
2847,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/scans.ftl,"@@ -19,13 +19,14 @@
 
 -->
       <div class=""row"">
-        <div class=""col-xs-12"">
-          <h3>${title}</h3>
-        </div>
+         <div class=""col-xs-12"">
+            <h3>Active Scans Overview</h3>
+         </div>","[{'comment': 'Looks like some extra spaces got added on these lines', 'commenter': 'DomGarguilo'}]"
2847,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/navbar.ftl,"@@ -18,73 +18,71 @@
     under the License.
 
 -->
-    <div id=""navbar"" class=""navbar navbar-inverse navbar-fixed-top"">
-      <div class=""container-fluid"">
-        <!-- toggle -->
-        <div class=""navbar-header"">
-          <button type=""button"" class=""navbar-toggle collapsed"" data-toggle=""collapse"" data-target=""#nav-items"" aria-expanded=""false"">
-            <span class=""sr-only"">Toggle navigation</span>
-            <span class=""icon-bar""></span>
-            <span class=""icon-bar""></span>
-            <span class=""icon-bar""></span>
-          </button>
-          <a class=""navbar-brand"" id=""headertitle"" href=""/"">
-            <img id=""accumulo-avatar"" alt=""accumulo"" class=""navbar-left pull-left"" src=""/resources/images/accumulo-avatar.png"" />
-            ${instance_name}
-          </a>
-        </div>
-        <!-- Nav links -->
-        <div class=""collapse navbar-collapse"" id=""nav-items"">
-          <ul class=""nav navbar-nav navbar-right"">
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false""><span id=""statusNotification"" class=""icon-dot normal""></span>&nbsp;Servers&nbsp;<span class=""caret""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a href=""/manager""><span id=""managerStatusNotification"" class=""icon-dot normal""></span>&nbsp;Manager&nbsp;Server&nbsp;</a></li>
-                <li><a href=""/tservers""><span id=""serverStatusNotification"" class=""icon-dot normal""></span>&nbsp;Tablet&nbsp;Servers&nbsp;</a></li>
-                <li><a href=""/gc""><span id=""gcStatusNotification"" class=""icon-dot normal""></span>&nbsp;Garbage&nbsp;collector&nbsp;</a></li>
-              </ul>
-            </li>
-            <li><a href=""/tables"">Tables</a></li>
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">
-                Activity <span class=""caret""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a href=""/compactions"">Active&nbsp;Compactions</a></li>
-                <li><a href=""/scans"">Active&nbsp;Scans</a></li>
-                <li><a href=""/bulkImports"">Bulk&nbsp;Imports</a></li>
-                <li><a href=""/ec"">External&nbsp;Compactions</a></li>
-                <li><a href=""/replication"">Replication</a></li>
-              </ul>
-            </li>
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">Debug&nbsp;<span id=""errorsNotification"" class=""badge""></span><span class=""caret""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a href=""/log"">Recent&nbsp;Logs&nbsp;<span id=""recentLogsNotifications"" class=""badge""></span></a></li>
-                <li><a href=""/problems"">Table&nbsp;Problems&nbsp;<span id=""tableProblemsNotifications"" class=""badge""></span></a></li>
-              </ul>
-            </li>
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">
-                REST <span class=""caret""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a href=""/rest/xml"">XML Summary</a></li>
-                <li><a href=""/rest/json"">JSON Summary</a></li>
-              </ul>
-            </li>
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">
-                <span class=""glyphicon glyphicon-option-vertical""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a class=""auto-refresh"" style=""cursor:pointer"">Auto-Refresh</a></li>
-                <li><a data-toggle=""modal"" href=""#aboutModal"">About</a></li>
-              </ul>
-            </li>
-          </ul>
-        </div>
-      </div>
+<nav class=""navbar navbar-expand-lg navbar-dark bg-dark"">","[{'comment': 'Not sure what happened, but it looks like the formatting on this whole file got messed up somehow- should probably be returned to proper formatting.', 'commenter': 'DomGarguilo'}]"
2847,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/navbar.ftl,"@@ -18,73 +18,71 @@
     under the License.
 
 -->
-    <div id=""navbar"" class=""navbar navbar-inverse navbar-fixed-top"">
-      <div class=""container-fluid"">
-        <!-- toggle -->
-        <div class=""navbar-header"">
-          <button type=""button"" class=""navbar-toggle collapsed"" data-toggle=""collapse"" data-target=""#nav-items"" aria-expanded=""false"">
-            <span class=""sr-only"">Toggle navigation</span>
-            <span class=""icon-bar""></span>
-            <span class=""icon-bar""></span>
-            <span class=""icon-bar""></span>
-          </button>
-          <a class=""navbar-brand"" id=""headertitle"" href=""/"">
-            <img id=""accumulo-avatar"" alt=""accumulo"" class=""navbar-left pull-left"" src=""/resources/images/accumulo-avatar.png"" />
-            ${instance_name}
-          </a>
-        </div>
-        <!-- Nav links -->
-        <div class=""collapse navbar-collapse"" id=""nav-items"">
-          <ul class=""nav navbar-nav navbar-right"">
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false""><span id=""statusNotification"" class=""icon-dot normal""></span>&nbsp;Servers&nbsp;<span class=""caret""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a href=""/manager""><span id=""managerStatusNotification"" class=""icon-dot normal""></span>&nbsp;Manager&nbsp;Server&nbsp;</a></li>
-                <li><a href=""/tservers""><span id=""serverStatusNotification"" class=""icon-dot normal""></span>&nbsp;Tablet&nbsp;Servers&nbsp;</a></li>
-                <li><a href=""/gc""><span id=""gcStatusNotification"" class=""icon-dot normal""></span>&nbsp;Garbage&nbsp;collector&nbsp;</a></li>
-              </ul>
-            </li>
-            <li><a href=""/tables"">Tables</a></li>
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">
-                Activity <span class=""caret""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a href=""/compactions"">Active&nbsp;Compactions</a></li>
-                <li><a href=""/scans"">Active&nbsp;Scans</a></li>
-                <li><a href=""/bulkImports"">Bulk&nbsp;Imports</a></li>
-                <li><a href=""/ec"">External&nbsp;Compactions</a></li>
-                <li><a href=""/replication"">Replication</a></li>
-              </ul>
-            </li>
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">Debug&nbsp;<span id=""errorsNotification"" class=""badge""></span><span class=""caret""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a href=""/log"">Recent&nbsp;Logs&nbsp;<span id=""recentLogsNotifications"" class=""badge""></span></a></li>
-                <li><a href=""/problems"">Table&nbsp;Problems&nbsp;<span id=""tableProblemsNotifications"" class=""badge""></span></a></li>
-              </ul>
-            </li>
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">
-                REST <span class=""caret""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a href=""/rest/xml"">XML Summary</a></li>
-                <li><a href=""/rest/json"">JSON Summary</a></li>
-              </ul>
-            </li>
-            <li class=""dropdown"">
-              <a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"" role=""button"" aria-haspopup=""true"" aria-expanded=""false"">
-                <span class=""glyphicon glyphicon-option-vertical""></span>
-              </a>
-              <ul class=""dropdown-menu"">
-                <li><a class=""auto-refresh"" style=""cursor:pointer"">Auto-Refresh</a></li>
-                <li><a data-toggle=""modal"" href=""#aboutModal"">About</a></li>
-              </ul>
-            </li>
-          </ul>
-        </div>
-      </div>
+<nav class=""navbar navbar-expand-lg navbar-dark bg-dark"">
+  <div class=""container-fluid"">
+    <a class=""navbar-brand"" id=""headertitle"" href=""/"">
+                <img id=""accumulo-avatar"" alt=""accumulo"" class=""navbar-left pull-left"" src=""/resources/images/accumulo-avatar.png"" />
+                uno","[{'comment': 'The instance name will not always be uno. Thats why there was the `${instance_name}` that gets replaces with the instance name dynamically', 'commenter': 'DomGarguilo'}, {'comment': 'There could be value in also trying running this in a small cluster (could use muchos) to check that it is not relying on uno / mini-cluster specific configuration / conventions that would not be valid in a cluster / production environment.   (Triggered by @DomGarguilo comment that the name might not always be uno)', 'commenter': 'EdColeman'}, {'comment': 'Changed it back to ${instance_name}', 'commenter': 'AlbertWhitlock'}]"
2858,core/src/main/java/org/apache/accumulo/core/metrics/MetricsUtil.java,"@@ -76,6 +77,8 @@ private static void initializeMetrics(boolean enabled, boolean jvmMetricsEnabled
 
       if (address != null) {
         tags.add(Tag.of(""Address"", address.toString()));
+      } else {
+        throw new NullArgumentException(); ","[{'comment': 'Why not IllegalArgumentException with a description - no need to add a new dependency for a standard exception.', 'commenter': 'EdColeman'}]"
2870,shell/src/main/java/org/apache/accumulo/shell/Shell.java,"@@ -1093,11 +1089,7 @@ public final void printRecords(Iterable<Entry<Key,Value>> scanner, FormatterConf
   }
 
   public static String repeat(String s, int c) {
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < c; i++) {
-      sb.append(s);
-    }
-    return sb.toString();
+    return String.valueOf(s).repeat(Math.max(0, c));","[{'comment': '```suggestion\r\n    return s.repeat(Math.max(0, c));\r\n```', 'commenter': 'dlmarion'}, {'comment': 'Fixed in 15e9cfc223', 'commenter': 'EdColeman'}]"
2870,shell/src/main/java/org/apache/accumulo/shell/Shell.java,"@@ -812,23 +808,23 @@ public void execCommand(String input, boolean ignoreAuthTimeout, boolean echoPro
   private ShellCompletor setupCompletion() {
     rootToken = new Token();
 
-    Set<String> tableNames = null;
+    Set<String> tableNames;
     try {
       tableNames = accumuloClient.tableOperations().list();
     } catch (Exception e) {
       log.debug(""Unable to obtain list of tables"", e);
       tableNames = Collections.emptySet();
     }
 
-    Set<String> userlist = null;
+    Set<String> userlist;","[{'comment': ""IIRC we had to initialize variables to null, and now we don't?"", 'commenter': 'dlmarion'}, {'comment': ""I believe the ide is now smart enough to see that the value is set on all paths before it is used, so the initial set is redundant.  The IDE flags it with \r\n```\r\nVariable 'userlist' initializer 'null' is redundant\r\n```\r\n\r\nOn the other hand, this will be flagged as an error - `variable n might not have been initialized`\r\n\r\n```\r\n   String n;\r\n    if(n == null){\r\n\r\n    }\r\n```"", 'commenter': 'EdColeman'}]"
2880,core/src/main/java/org/apache/accumulo/core/clientImpl/ScanAttemptsImpl.java,"@@ -111,16 +110,27 @@ public void report(ScanAttempt.Result result) {
   }
 
   Map<TabletId,Collection<ScanAttemptImpl>> snapshot() {","[{'comment': 'Suggest turning the comment you removed at line 114 into a method javadoc. Just so we now the top level goal of the method. This gives a little more context to the comments inside the method.', 'commenter': 'dlmarion'}, {'comment': 'Added in a7043ff', 'commenter': 'DomGarguilo'}]"
2881,server/base/src/main/java/org/apache/accumulo/server/util/Admin.java,"@@ -226,49 +226,52 @@ public void execute(final String[] args) {
     JCommander cl = new JCommander(opts);
     cl.setProgramName(""accumulo admin"");
 
-    CheckTabletsCommand checkTabletsCommand = new CheckTabletsCommand();
-    cl.addCommand(""checkTablets"", checkTabletsCommand);
-
     ChangeSecretCommand changeSecretCommand = new ChangeSecretCommand();
     cl.addCommand(""changeSecret"", changeSecretCommand);
 
+    CheckTabletsCommand checkTabletsCommand = new CheckTabletsCommand();
+    cl.addCommand(""checkTablets"", checkTabletsCommand);
+
     DeleteZooInstanceCommand deleteZooInstanceOpts = new DeleteZooInstanceCommand();
     cl.addCommand(""deleteZooInstance"", deleteZooInstanceOpts);
 
-    RestoreZooCommand restoreZooOpts = new RestoreZooCommand();
-    cl.addCommand(""restoreZoo"", restoreZooOpts);
+    DumpConfigCommand dumpConfigCommand = new DumpConfigCommand();
+    cl.addCommand(""dumpConfig"", dumpConfigCommand);
 
     ListInstancesCommand listIntancesOpts = new ListInstancesCommand();
     cl.addCommand(""listInstances"", listIntancesOpts);","[{'comment': '```suggestion\r\n    ListInstancesCommand listInstancesOpts = new ListInstancesCommand();\r\n    cl.addCommand(""listInstances"", listInstancesOpts);\r\n```\r\nI know you didn\'t make this change but thought it would be worth it to fix this typo while changes are being made to this file.', 'commenter': 'DomGarguilo'}, {'comment': '@DomGarguilo - good catch, typo fixed. Is this good to merge or do you want to wait for @milleruntime to take a look too?', 'commenter': 'cshannon'}]"
2895,core/src/main/java/org/apache/accumulo/core/spi/crypto/AESCryptoService.java,"@@ -516,48 +559,45 @@ public static Key generateKey(SecureRandom random, int size) {
     return new SecretKeySpec(bytes, ""AES"");
   }
 
-  @SuppressFBWarnings(value = ""CIPHER_INTEGRITY"",
-      justification = ""integrity not needed for key wrap"")
-  public static Key unwrapKey(byte[] fek, Key kek) {
-    Key result = null;
+  public static synchronized Key unwrapKey(byte[] fek, Key kek) {","[{'comment': ""I don't think this needs the `synchronized` modifier. This may have been included with an earlier change of mine and then not removed when I changed the code to use a ThreadLocal."", 'commenter': 'dlmarion'}, {'comment': 'Fixed in https://github.com/apache/accumulo/pull/2895/commits/5aba17923c94b184cfe6fb3b428339ad6ce6d59b', 'commenter': 'milleruntime'}]"
2895,core/src/main/java/org/apache/accumulo/core/spi/crypto/AESCryptoService.java,"@@ -516,48 +559,45 @@ public static Key generateKey(SecureRandom random, int size) {
     return new SecretKeySpec(bytes, ""AES"");
   }
 
-  @SuppressFBWarnings(value = ""CIPHER_INTEGRITY"",
-      justification = ""integrity not needed for key wrap"")
-  public static Key unwrapKey(byte[] fek, Key kek) {
-    Key result = null;
+  public static synchronized Key unwrapKey(byte[] fek, Key kek) {
     try {
-      Cipher c = Cipher.getInstance(KEY_WRAP_TRANSFORM);
+      final Cipher c = KEY_UNWRAP_CIPHER.get();
       c.init(Cipher.UNWRAP_MODE, kek);
-      result = c.unwrap(fek, ""AES"", Cipher.SECRET_KEY);
-    } catch (InvalidKeyException | NoSuchAlgorithmException | NoSuchPaddingException e) {
+      return c.unwrap(fek, ""AES"", Cipher.SECRET_KEY);
+    } catch (InvalidKeyException | NoSuchAlgorithmException e) {
       throw new CryptoException(""Unable to unwrap file encryption key"", e);
     }
-    return result;
   }
 
-  @SuppressFBWarnings(value = ""CIPHER_INTEGRITY"",
-      justification = ""integrity not needed for key wrap"")
-  public static byte[] wrapKey(Key fek, Key kek) {
-    byte[] result = null;
+  public static synchronized byte[] wrapKey(Key fek, Key kek) {","[{'comment': 'Same comment here.', 'commenter': 'dlmarion'}, {'comment': 'Fixed in https://github.com/apache/accumulo/pull/2895/commits/5aba17923c94b184cfe6fb3b428339ad6ce6d59b', 'commenter': 'milleruntime'}]"
2900,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -1071,6 +1071,24 @@ Map<String,Integer> listConstraints(String tableName)
   List<DiskUsage> getDiskUsage(Set<String> tables)
       throws AccumuloException, AccumuloSecurityException, TableNotFoundException;
 
+  /**
+   * Gets the number of bytes being used in the files for a set of tables. This operation will use
+   * the client to scan the metadata table to compute the size metrics for the tables. For the most
+   * accurate information a flush or compaction can be run first on the set of tables.","[{'comment': 'Should note that the sizes in the metadata table are estimates.', 'commenter': 'milleruntime'}, {'comment': ""And importantly, not all the files have these estimated file sizes. They aren't required."", 'commenter': 'ctubbsii'}]"
2900,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -1071,6 +1071,24 @@ Map<String,Integer> listConstraints(String tableName)
   List<DiskUsage> getDiskUsage(Set<String> tables)
       throws AccumuloException, AccumuloSecurityException, TableNotFoundException;
 
+  /**
+   * Gets the number of bytes being used in the files for a set of tables. This operation will use
+   * the client to scan the metadata table to compute the size metrics for the tables. For the most
+   * accurate information a flush or compaction can be run first on the set of tables.
+   *
+   * @param tables
+   *          set of tables to compute usage across
+   * @param computeShared
+   *          whether to compute size metrics across shared files
+   * @param auths
+   *          authorizations to scan the metadata table
+   * @return The disk usage result
+   *
+   * @since 2.1.0
+   */
+  TableDiskUsageResult getDiskUsageFromMetadata(Set<String> tables, boolean computeShared,
+      Authorizations auths) throws TableNotFoundException;","[{'comment': 'Another name could be `estimateDiskUsage()`.', 'commenter': 'milleruntime'}]"
2900,server/base/src/main/java/org/apache/accumulo/server/util/HdfsTableDiskUsage.java,"@@ -308,13 +184,12 @@ static class Opts extends ServerUtilOpts {
 
   public static void main(String[] args) throws Exception {
     Opts opts = new Opts();
-    opts.parseArgs(TableDiskUsage.class.getName(), args);
-    Span span = TraceUtil.startSpan(TableDiskUsage.class, ""main"");
+    opts.parseArgs(HdfsTableDiskUsage.class.getName(), args);
+    Span span = TraceUtil.startSpan(HdfsTableDiskUsage.class, ""main"");
     try (Scope scope = span.makeCurrent()) {
       try (AccumuloClient client = Accumulo.newClient().from(opts.getClientProps()).build()) {
         VolumeManager fs = opts.getServerContext().getVolumeManager();
-        org.apache.accumulo.server.util.TableDiskUsage.printDiskUsage(opts.tables, fs, client,
-            false);
+        HdfsTableDiskUsage.printDiskUsage(opts.tables, fs, client, false);","[{'comment': '```suggestion\r\n        HdfsTableDiskUsage hdfsTableDiskUsage = new HdfsTableDiskUsage();\r\n        hdfsTableDiskUsage.printDiskUsage(opts.tables, fs, client, false);\r\n```', 'commenter': 'milleruntime'}]"
2900,core/src/main/java/org/apache/accumulo/core/util/tables/MetadataTableDiskUsage.java,"@@ -0,0 +1,192 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util.tables;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.admin.TableDiskUsageResult;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.RootTable;
+import org.apache.accumulo.core.metadata.TabletFile;
+import org.apache.accumulo.core.metadata.schema.DataFileValue;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.hadoop.fs.Path;
+
+/**
+ * This utility class will scan the Accumulo Metadata table to compute the disk usage for a table or
+ * table(s) by using the size value stored in columns that contain the column family
+ * {@link MetadataSchema.TabletsSection.DataFileColumnFamily}.
+ *
+ * This class will also optionally track shared files to computed shared usage across all tables
+ * that are provided as part of the Set of tables when getting disk usage.
+ *
+ * Because the metadata table is used for computing usage and not the actual files in HDFS the
+ * results will be an estimate. Older entries may exist with no file metadata (resulting in size 0)
+ * and other actions in the cluster can impact the estimated size such as flushes, tablet splits,
+ * compactions, etc.
+ *
+ * For the most accurate information a compaction should first be run on the set of tables being
+ * computed.
+ */
+public class MetadataTableDiskUsage extends TableDiskUsage {
+
+  public MetadataTableDiskUsage(final Set<TableId> tableIds) {
+    // Add each tableID
+    Objects.requireNonNull(tableIds).forEach(tableId -> addTable(tableId));
+  }
+
+  /**
+   * Compute the estimated disk usage for the given set of tables by scanning the Metadata table for
+   * file sizes. Also will compute shared usage across tables.
+   *
+   * @param tableNames
+   *          set of tables to compute an estimated disk usage for
+   * @param auths
+   *          authorizations to scan the metadata table
+   * @return the computed estimated usage results
+   *
+   * @throws TableNotFoundException
+   *           if the table(s) do not exist
+   */
+  public static TableDiskUsageResult getDiskUsage(Set<String> tableNames, AccumuloClient client,","[{'comment': '```suggestion\r\n  public TableDiskUsageResult getDiskUsage(Set<String> tableNames, AccumuloClient client,\r\n```', 'commenter': 'milleruntime'}]"
2900,core/src/main/java/org/apache/accumulo/core/util/tables/MetadataTableDiskUsage.java,"@@ -0,0 +1,192 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util.tables;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.client.admin.TableDiskUsageResult;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.RootTable;
+import org.apache.accumulo.core.metadata.TabletFile;
+import org.apache.accumulo.core.metadata.schema.DataFileValue;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.hadoop.fs.Path;
+
+/**
+ * This utility class will scan the Accumulo Metadata table to compute the disk usage for a table or
+ * table(s) by using the size value stored in columns that contain the column family
+ * {@link MetadataSchema.TabletsSection.DataFileColumnFamily}.
+ *
+ * This class will also optionally track shared files to computed shared usage across all tables
+ * that are provided as part of the Set of tables when getting disk usage.
+ *
+ * Because the metadata table is used for computing usage and not the actual files in HDFS the
+ * results will be an estimate. Older entries may exist with no file metadata (resulting in size 0)
+ * and other actions in the cluster can impact the estimated size such as flushes, tablet splits,
+ * compactions, etc.
+ *
+ * For the most accurate information a compaction should first be run on the set of tables being
+ * computed.
+ */
+public class MetadataTableDiskUsage extends TableDiskUsage {
+
+  public MetadataTableDiskUsage(final Set<TableId> tableIds) {
+    // Add each tableID
+    Objects.requireNonNull(tableIds).forEach(tableId -> addTable(tableId));
+  }
+
+  /**
+   * Compute the estimated disk usage for the given set of tables by scanning the Metadata table for
+   * file sizes. Also will compute shared usage across tables.
+   *
+   * @param tableNames
+   *          set of tables to compute an estimated disk usage for
+   * @param auths
+   *          authorizations to scan the metadata table
+   * @return the computed estimated usage results
+   *
+   * @throws TableNotFoundException
+   *           if the table(s) do not exist
+   */
+  public static TableDiskUsageResult getDiskUsage(Set<String> tableNames, AccumuloClient client,
+      Authorizations auths) throws TableNotFoundException {
+    return getDiskUsage(tableNames, true, client, auths);
+  }
+
+  /**
+   * Compute the estimated disk usage for the given set of tables by scanning the Metadata table for
+   * file sizes. Optionally computes shared usage across tables.
+   *
+   * @param tableNames
+   *          set of tables to compute an estimated disk usage for
+   * @param computeShared
+   *          whether to compute size metrics across shared files
+   * @param auths
+   *          authorizations to scan the metadata table
+   * @return the computed estimated usage results
+   *
+   * @throws TableNotFoundException
+   *           if the table(s) do not exist
+   */
+  public static TableDiskUsageResult getDiskUsage(Set<String> tableNames, boolean computeShared,","[{'comment': '```suggestion\r\n  public TableDiskUsageResult getDiskUsage(Set<String> tableNames, boolean computeShared,\r\n```', 'commenter': 'milleruntime'}]"
2900,core/src/main/java/org/apache/accumulo/core/util/tables/TableDiskUsage.java,"@@ -0,0 +1,180 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util.tables;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.SortedSet;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.data.TableId;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public abstract class TableDiskUsage {
+
+  private static final Logger log = LoggerFactory.getLogger(TableDiskUsage.class);
+  private int nextInternalId = 0;
+  private Map<TableId,Integer> internalIds = new HashMap<>();
+  private Map<Integer,TableId> externalIds = new HashMap<>();
+  private Map<String,Integer[]> tableFiles = new HashMap<>();
+  private Map<String,Long> fileSizes = new HashMap<>();
+
+  protected void addTableIfAbsent(TableId tableId) {
+    if (!internalIds.containsKey(tableId)) {
+      addTable(tableId);
+    }
+  }
+
+  protected void addTable(TableId tableId) {
+    if (internalIds.containsKey(tableId)) {
+      throw new IllegalArgumentException(""Already added table "" + tableId);
+    }
+
+    // Keep an internal counter for each table added
+    int iid = nextInternalId++;
+
+    // Store the table id to the internal id
+    internalIds.put(tableId, iid);
+    // Store the internal id to the table id
+    externalIds.put(iid, tableId);
+  }
+
+  protected void linkFileAndTable(TableId tableId, String file) {
+    // get the internal id for this table
+    int internalId = internalIds.get(tableId);
+
+    // Initialize a bitset for tables (internal IDs) that reference this file
+    Integer[] tables = tableFiles.get(file);
+    if (tables == null) {
+      tables = new Integer[internalIds.size()];
+      for (int i = 0; i < tables.length; i++) {
+        tables[i] = 0;
+      }
+      tableFiles.put(file, tables);
+    }
+
+    // Update the bitset to track that this table has seen this file
+    tables[internalId] = 1;
+  }
+
+  protected void addFileSize(String file, long size) {
+    fileSizes.put(file, size);
+  }
+
+  protected Map<List<TableId>,Long> calculateSharedUsage() {","[{'comment': '```suggestion\r\n  private Map<List<TableId>,Long> calculateSharedUsage() {\r\n```', 'commenter': 'milleruntime'}]"
2900,core/src/main/java/org/apache/accumulo/core/util/tables/TableDiskUsage.java,"@@ -0,0 +1,180 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util.tables;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.SortedSet;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.data.TableId;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public abstract class TableDiskUsage {
+
+  private static final Logger log = LoggerFactory.getLogger(TableDiskUsage.class);
+  private int nextInternalId = 0;
+  private Map<TableId,Integer> internalIds = new HashMap<>();
+  private Map<Integer,TableId> externalIds = new HashMap<>();
+  private Map<String,Integer[]> tableFiles = new HashMap<>();
+  private Map<String,Long> fileSizes = new HashMap<>();
+
+  protected void addTableIfAbsent(TableId tableId) {
+    if (!internalIds.containsKey(tableId)) {
+      addTable(tableId);
+    }
+  }
+
+  protected void addTable(TableId tableId) {
+    if (internalIds.containsKey(tableId)) {
+      throw new IllegalArgumentException(""Already added table "" + tableId);
+    }
+
+    // Keep an internal counter for each table added
+    int iid = nextInternalId++;
+
+    // Store the table id to the internal id
+    internalIds.put(tableId, iid);
+    // Store the internal id to the table id
+    externalIds.put(iid, tableId);
+  }
+
+  protected void linkFileAndTable(TableId tableId, String file) {
+    // get the internal id for this table
+    int internalId = internalIds.get(tableId);
+
+    // Initialize a bitset for tables (internal IDs) that reference this file
+    Integer[] tables = tableFiles.get(file);
+    if (tables == null) {
+      tables = new Integer[internalIds.size()];
+      for (int i = 0; i < tables.length; i++) {
+        tables[i] = 0;
+      }
+      tableFiles.put(file, tables);
+    }
+
+    // Update the bitset to track that this table has seen this file
+    tables[internalId] = 1;
+  }
+
+  protected void addFileSize(String file, long size) {
+    fileSizes.put(file, size);
+  }
+
+  protected Map<List<TableId>,Long> calculateSharedUsage() {
+    // Bitset of tables that contain a file and total usage by all files that share that usage
+    Map<List<Integer>,Long> usage = new HashMap<>();
+
+    if (log.isTraceEnabled()) {
+      log.trace(""fileSizes {}"", fileSizes);
+    }
+    // For each file w/ referenced-table bitset
+    for (Entry<String,Integer[]> entry : tableFiles.entrySet()) {
+      if (log.isTraceEnabled()) {
+        log.trace(""file {} table bitset {}"", entry.getKey(), Arrays.toString(entry.getValue()));
+      }
+      List<Integer> key = Arrays.asList(entry.getValue());
+      Long size = fileSizes.get(entry.getKey());
+
+      Long tablesUsage = usage.getOrDefault(key, 0L);
+      tablesUsage += size;
+      usage.put(key, tablesUsage);
+    }
+
+    final Map<List<TableId>,Long> externalUsage = new HashMap<>();
+
+    for (Entry<List<Integer>,Long> entry : usage.entrySet()) {
+      List<TableId> externalKey = new ArrayList<>();
+      List<Integer> key = entry.getKey();
+      // table bitset
+      for (int i = 0; i < key.size(); i++)
+        if (key.get(i) != 0) {
+          // Convert by internal id to the table id
+          externalKey.add(externalIds.get(i));
+        }
+
+      // list of table ids and size of files shared across the tables
+      externalUsage.put(externalKey, entry.getValue());
+    }
+
+    // mapping of all enumerations of files being referenced by tables and total size of files who
+    // share the same reference
+    return externalUsage;
+  }
+
+  protected static SortedMap<SortedSet<String>,Long> buildSharedUsageMap(final TableDiskUsage tdu,","[{'comment': '```suggestion\r\n  protected SortedMap<SortedSet<String>,Long> buildSharedUsageMap(final TableDiskUsage tdu,\r\n```', 'commenter': 'milleruntime'}]"
2900,server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java,"@@ -439,10 +440,10 @@ public List<TDiskUsage> getDiskUsage(Set<String> tables, TCredentials credential
       }
 
       // use the same set of tableIds that were validated above to avoid race conditions
-      Map<TreeSet<String>,Long> diskUsage =
-          TableDiskUsage.getDiskUsage(tableIds, context.getVolumeManager(), context);
+      Map<SortedSet<String>,Long> diskUsage =
+          HdfsTableDiskUsage.getDiskUsage(tableIds, context.getVolumeManager(), context);","[{'comment': '```suggestion\r\n      HdfsTableDiskUsage hdfsTableDiskUsage = new HdfsTableDiskUsage();\r\n      Map<SortedSet<String>,Long> diskUsage =\r\n          hdfsTableDiskUsage.getDiskUsage(tableIds, context.getVolumeManager(), context);\r\n```', 'commenter': 'milleruntime'}]"
2900,core/src/main/java/org/apache/accumulo/core/client/admin/DiskUsage.java,"@@ -21,10 +21,13 @@
 import java.util.Objects;
 import java.util.SortedSet;
 
+/**
+ * This class is used to track the shared disk usage between multiple tables.
+ */
 public class DiskUsage {
 
-  protected final SortedSet<String> tables;
-  protected Long usage;
+  private final SortedSet<String> tables;
+  private final Long usage;","[{'comment': ""I'm not sure why we use the boxed version of Long here instead of the primitive. That could be fixed throughout this class. Not sure if we depend on null references anywhere... but if we do, they could be replaced with OptionalLong instead. This comment is not necessarily related to this PR's changes. Just an observation."", 'commenter': 'ctubbsii'}, {'comment': ""Good point, I'll take a peak and see if I can swap it out and if null is needed."", 'commenter': 'cshannon'}]"
2900,core/src/main/java/org/apache/accumulo/core/client/admin/TableDiskUsageResult.java,"@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.client.admin;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.SortedSet;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.data.TableId;
+
+import com.google.common.collect.ImmutableMap;
+
+/**
+ * Class for returning the disk usage results computed by
+ * {@link org.apache.accumulo.core.util.tables.MetadataTableDiskUsage}
+ */
+public class TableDiskUsageResult {
+
+  // Track the total disk usage across all files for the Table
+  private final Map<TableId,AtomicLong> tableUsages;
+
+  // Track the total disk usage across a volume for each Table
+  private final Map<TableId,Map<String,AtomicLong>> volumeUsages;
+
+  // This tracks disk usage shared across a set of tables
+  private final List<DiskUsage> sharedDiskUsages;
+
+  // This tracks any shared tables (tables that have shared files)
+  private final Map<TableId,Set<TableId>> sharedTables;
+
+  public TableDiskUsageResult(final Map<TableId,AtomicLong> tableUsages,
+      final Map<TableId,Map<String,AtomicLong>> volumeUsages,
+      final SortedMap<SortedSet<String>,Long> usageMap,
+      final Map<TableId,Set<TableId>> referencedTables) {
+
+    this.tableUsages = ImmutableMap.copyOf(tableUsages);
+    this.volumeUsages = ImmutableMap.copyOf(volumeUsages);
+    this.sharedTables = ImmutableMap.copyOf(referencedTables);","[{'comment': ""These don't need to use Guava's ImmutableMap. They can use the built-in Map.copyOf"", 'commenter': 'ctubbsii'}]"
2900,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -1082,10 +1082,40 @@ Map<String,Integer> listConstraints(String tableName)
    *          a set of tables
    * @return a list of disk usage objects containing linked table names and sizes
    * @since 1.6.0
+   *
+   * @deprecated since 2.1.0 use {@link #getEstimatedDiskUsage(Set, boolean, Authorizations)}
+   *             instead.
    */
+  @Deprecated(since = ""2.1.0"")
   List<DiskUsage> getDiskUsage(Set<String> tables)
       throws AccumuloException, AccumuloSecurityException, TableNotFoundException;
 
+  /**
+   * Gets the number of bytes being used in by the files for a set of tables. This operation will
+   * use the client to scan the metadata table to compute the size metrics for the tables.
+   * Optionally shared usage information can be computed across tables.
+   *
+   * Because the metadata table is used for computing usage and not the actual files in HDFS the
+   * results will be an estimate. Older entries may exist with no file metadata (resulting in size
+   * 0) and other actions in the cluster can impact the estimated size such as flushes, tablet
+   * splits, compactions, etc.
+   *
+   * For the most accurate information a compaction should first be run on the set of tables being
+   * computed.
+   *
+   * @param tables
+   *          set of tables to compute usage across
+   * @param computeShared
+   *          whether to compute size metrics across shared files
+   * @param auths
+   *          authorizations to scan the metadata table
+   * @return The disk usage result
+   *
+   * @since 2.1.0
+   */
+  TableDiskUsageResult getEstimatedDiskUsage(Set<String> tables, boolean computeShared,
+      Authorizations auths) throws TableNotFoundException;","[{'comment': ""I don't think we need this to be a new API. We can just change the existing implementation to get the sizes from a different source. As long as the size is reliable, it doesn't matter if we get it from HDFS or the metadata."", 'commenter': 'ctubbsii'}]"
2900,core/src/main/java/org/apache/accumulo/core/util/tables/TableDiskUsage.java,"@@ -0,0 +1,180 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util.tables;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.SortedSet;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.data.TableId;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public abstract class TableDiskUsage {
+
+  private static final Logger log = LoggerFactory.getLogger(TableDiskUsage.class);
+  private int nextInternalId = 0;
+  private Map<TableId,Integer> internalIds = new HashMap<>();
+  private Map<Integer,TableId> externalIds = new HashMap<>();
+  private Map<String,Integer[]> tableFiles = new HashMap<>();
+  private Map<String,Long> fileSizes = new HashMap<>();
+
+  protected void addTableIfAbsent(TableId tableId) {
+    if (!internalIds.containsKey(tableId)) {
+      addTable(tableId);
+    }
+  }
+
+  protected void addTable(TableId tableId) {
+    if (internalIds.containsKey(tableId)) {
+      throw new IllegalArgumentException(""Already added table "" + tableId);
+    }
+
+    // Keep an internal counter for each table added
+    int iid = nextInternalId++;
+
+    // Store the table id to the internal id
+    internalIds.put(tableId, iid);
+    // Store the internal id to the table id
+    externalIds.put(iid, tableId);
+  }
+
+  protected void linkFileAndTable(TableId tableId, String file) {
+    // get the internal id for this table
+    int internalId = internalIds.get(tableId);
+
+    // Initialize a bitset for tables (internal IDs) that reference this file
+    Integer[] tables = tableFiles.get(file);
+    if (tables == null) {
+      tables = new Integer[internalIds.size()];
+      for (int i = 0; i < tables.length; i++) {
+        tables[i] = 0;
+      }
+      tableFiles.put(file, tables);
+    }
+
+    // Update the bitset to track that this table has seen this file
+    tables[internalId] = 1;
+  }
+
+  protected void addFileSize(String file, long size) {
+    fileSizes.put(file, size);
+  }
+
+  protected Map<List<TableId>,Long> calculateSharedUsage() {
+    // Bitset of tables that contain a file and total usage by all files that share that usage
+    Map<List<Integer>,Long> usage = new HashMap<>();
+
+    if (log.isTraceEnabled()) {
+      log.trace(""fileSizes {}"", fileSizes);
+    }
+    // For each file w/ referenced-table bitset
+    for (Entry<String,Integer[]> entry : tableFiles.entrySet()) {
+      if (log.isTraceEnabled()) {
+        log.trace(""file {} table bitset {}"", entry.getKey(), Arrays.toString(entry.getValue()));
+      }
+      List<Integer> key = Arrays.asList(entry.getValue());
+      Long size = fileSizes.get(entry.getKey());
+
+      Long tablesUsage = usage.getOrDefault(key, 0L);
+      tablesUsage += size;
+      usage.put(key, tablesUsage);
+    }
+
+    final Map<List<TableId>,Long> externalUsage = new HashMap<>();
+
+    for (Entry<List<Integer>,Long> entry : usage.entrySet()) {
+      List<TableId> externalKey = new ArrayList<>();
+      List<Integer> key = entry.getKey();
+      // table bitset
+      for (int i = 0; i < key.size(); i++)
+        if (key.get(i) != 0) {
+          // Convert by internal id to the table id
+          externalKey.add(externalIds.get(i));
+        }
+
+      // list of table ids and size of files shared across the tables
+      externalUsage.put(externalKey, entry.getValue());
+    }
+
+    // mapping of all enumerations of files being referenced by tables and total size of files who
+    // share the same reference
+    return externalUsage;
+  }
+
+  protected static SortedMap<SortedSet<String>,Long> buildSharedUsageMap(final TableDiskUsage tdu,
+      final ClientContext clientContext, final Set<TableId> emptyTableIds) {
+    final Map<TableId,String> reverseTableIdMap = clientContext.getTableIdToNameMap();
+
+    final SortedMap<SortedSet<String>,Long> usage = new TreeMap<>((o1, o2) -> {
+      int len1 = o1.size();
+      int len2 = o2.size();
+
+      int min = Math.min(len1, len2);
+      Iterator<String> iter1 = o1.iterator();
+      Iterator<String> iter2 = o2.iterator();
+
+      int count = 0;
+      while (count < min) {
+        String s1 = iter1.next();
+        String s2 = iter2.next();
+
+        int cmp = s1.compareTo(s2);
+        if (cmp != 0) {
+          return cmp;
+        }
+
+        count++;
+      }
+
+      return len1 - len2;
+    });","[{'comment': ""This comparator seems to be non-trivial, but has no comments explaining what it's trying to do"", 'commenter': 'ctubbsii'}, {'comment': ""The comparator already existed previously, I didn't change anything with it, I just moved it. I can look into more at what it's doing but it wasn't something I created.\r\n\r\nhttps://github.com/apache/accumulo/blob/87402dfcd8d828850ca2e4b01e8811cf506e7d4e/server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java#L231-L257"", 'commenter': 'cshannon'}]"
2900,server/base/src/main/java/org/apache/accumulo/server/util/HdfsTableDiskUsage.java,"@@ -61,94 +60,16 @@
 import io.opentelemetry.api.trace.Span;
 import io.opentelemetry.context.Scope;
 
-public class TableDiskUsage {
-
-  private static final Logger log = LoggerFactory.getLogger(TableDiskUsage.class);
-  private int nextInternalId = 0;
-  private Map<TableId,Integer> internalIds = new HashMap<>();
-  private Map<Integer,TableId> externalIds = new HashMap<>();
-  private Map<String,Integer[]> tableFiles = new HashMap<>();
-  private Map<String,Long> fileSizes = new HashMap<>();
-
-  void addTable(TableId tableId) {
-    if (internalIds.containsKey(tableId))
-      throw new IllegalArgumentException(""Already added table "" + tableId);
-
-    // Keep an internal counter for each table added
-    int iid = nextInternalId++;
-
-    // Store the table id to the internal id
-    internalIds.put(tableId, iid);
-    // Store the internal id to the table id
-    externalIds.put(iid, tableId);
-  }
-
-  void linkFileAndTable(TableId tableId, String file) {
-    // get the internal id for this table
-    int internalId = internalIds.get(tableId);
-
-    // Initialize a bitset for tables (internal IDs) that reference this file
-    Integer[] tables = tableFiles.get(file);
-    if (tables == null) {
-      tables = new Integer[internalIds.size()];
-      for (int i = 0; i < tables.length; i++)
-        tables[i] = 0;
-      tableFiles.put(file, tables);
-    }
-
-    // Update the bitset to track that this table has seen this file
-    tables[internalId] = 1;
-  }
-
-  void addFileSize(String file, long size) {
-    fileSizes.put(file, size);
-  }
-
-  Map<List<TableId>,Long> calculateUsage() {
-
-    // Bitset of tables that contain a file and total usage by all files that share that usage
-    Map<List<Integer>,Long> usage = new HashMap<>();
-
-    if (log.isTraceEnabled()) {
-      log.trace(""fileSizes {}"", fileSizes);
-    }
-    // For each file w/ referenced-table bitset
-    for (Entry<String,Integer[]> entry : tableFiles.entrySet()) {
-      if (log.isTraceEnabled()) {
-        log.trace(""file {} table bitset {}"", entry.getKey(), Arrays.toString(entry.getValue()));
-      }
-      List<Integer> key = Arrays.asList(entry.getValue());
-      Long size = fileSizes.get(entry.getKey());
-
-      Long tablesUsage = usage.get(key);
-      if (tablesUsage == null)
-        tablesUsage = 0L;
-
-      tablesUsage += size;
-
-      usage.put(key, tablesUsage);
-
-    }
-
-    Map<List<TableId>,Long> externalUsage = new HashMap<>();
-
-    for (Entry<List<Integer>,Long> entry : usage.entrySet()) {
-      List<TableId> externalKey = new ArrayList<>();
-      List<Integer> key = entry.getKey();
-      // table bitset
-      for (int i = 0; i < key.size(); i++)
-        if (key.get(i) != 0)
-          // Convert by internal id to the table id
-          externalKey.add(externalIds.get(i));
-
-      // list of table ids and size of files shared across the tables
-      externalUsage.put(externalKey, entry.getValue());
-    }
+/**
+ * Class that computes shared disk usage across tables by scanning HDFS files.
+ *
+ * @deprecated This class has been deprecated in favor of
+ *             {@link org.apache.accumulo.core.util.tables.MetadataTableDiskUsage}
+ */
+@Deprecated
+public class HdfsTableDiskUsage extends TableDiskUsage {","[{'comment': ""I don't think we need subclasses for disk usage. That seems unnecessarily complex. I think when we get the list of files, we can just keep a pair instead of a set... the pair including the filename and the size. Then, get the size from that mapping, rather than lookup in HDFS.\r\n\r\nI think a lot of the complexity of this PR is trying to preserve both old and new code... and that's not necessary."", 'commenter': 'ctubbsii'}, {'comment': ""So the reason for the subclasses and the new API was @milleruntime mentioned not wanting to break the  thrift API which makes sense. Because new version is a client side scan only and doesn't need to make a server call I kept the thrift API alone and preserved the existing implementation. The other option would be I guess to move the metadata scan to the server and replace it so the thrift API call gets the new implementation but that seems unnecessary since you can do a client scan. Or I guess you could always have both, have it server wise in the API and also make it available in the client.\n\nAlso the new API was created since more information is returned with the new call to support the new output. However this is not a big deal if we don't want to change the output as you indicated in your other comment.\n\nSo mostly the issue is what to do about the existing server side API  that is public in thrift and what that should use and if we should keep the metadata scan only client  side."", 'commenter': 'cshannon'}, {'comment': ""@ctubbsii  - Also in regards to your comment about complexity, the majority of the complexity was already there previously with the disk usage tracking since the purpose is to track shared disk usage. All I essentially did was take the existing TableDiskUsage implementation and break the common computation and data structures into a parent class and then have 2 subclasses to make it possible to switch out the source of the file sizes (HDFS or metadatable) while keeping the existing logic so the API doesn't change and the command still works the same way.\r\n\r\nIf the goal here is to make this a simple PR to simply switch out the source of the file size from being HDFS and to use metadata then I would probably just want to  revert back to what is currently there now in [TableDiskUsage](https://github.com/apache/accumulo/blob/87402dfcd8d828850ca2e4b01e8811cf506e7d4e/server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java) and get rid of the subclasses. Then I can just swap out the source of the file size from HDFS just to using the Metadata table with my code changes I've created to do that. This way we know the logic still works and would work with the current command output, the API wouldn't change and could return the same format, and the other changes would be minimal.\r\n\r\nBut again, as I said in my previous comment, I can do that but there's still the issue of the client vs server side and thrift API call and whether or not we need to keep that and if we should be doing this client side vs server side scan as the old implementation runs on a server vs my new implementation that runs client side."", 'commenter': 'cshannon'}]"
2900,shell/src/main/java/org/apache/accumulo/shell/commands/DUCommand.java,"@@ -80,13 +109,53 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
     }
 
     try {
-      String valueFormat = prettyPrint ? ""%9s"" : ""%,24d"";
-      for (DiskUsage usage : shellState.getAccumuloClient().tableOperations()
-          .getDiskUsage(tables)) {
-        Object value = prettyPrint ? NumUtil.bigNumberForSize(usage.getUsage()) : usage.getUsage();
-        shellState.getWriter()
-            .println(String.format(valueFormat + "" %s"", value, usage.getTables()));
+      final String valueFormat = prettyPrint ? ""%s"" : ""%,d"";
+      final TableDiskUsageResult usageResult = shellState.getAccumuloClient().tableOperations()
+          .getEstimatedDiskUsage(tables, verbose, auths);
+
+      // Print results for each table queried
+      for (Map.Entry<TableId,AtomicLong> entry : usageResult.getTableUsages().entrySet()) {
+        final Object usage =
+            prettyPrint ? NumUtil.bigNumberForSize(entry.getValue().get()) : entry.getValue().get();
+
+        // Print the usage for the table
+        shellState.getWriter().print(String.format(""%s  %s  used total: "" + valueFormat,
+            shellState.getContext().getTableName(entry.getKey()), entry.getKey(), usage));
+
+        // Print usage for all volume(s) that contain files for this table
+        Optional.ofNullable(usageResult.getVolumeUsages().get(entry.getKey()))
+            .ifPresent(tableVolUsage -> tableVolUsage.entrySet()
+                .forEach(vuEntry -> shellState.getWriter()
+                    .print(String.format(""  %s: "" + valueFormat, vuEntry.getKey(),
+                        prettyPrint ? NumUtil.bigNumberForSize(vuEntry.getValue().get())
+                            : vuEntry.getValue().get()))));
+
+        // Check/print if this table has any shared files
+        final Optional<Set<TableId>> sharedTables =
+            Optional.ofNullable(usageResult.getSharedTables().get(entry.getKey()));
+        final boolean hasShared = sharedTables.map(st -> st.size() > 0).orElse(false);
+
+        shellState.getWriter().print(String.format(""  has_shared: %s"", hasShared));
+        if (hasShared) {
+          shellState.getWriter().print(String.format(""  shared with:  %s"", sharedTables.get()));
+        }
+        shellState.getWriter().println();","[{'comment': ""I don't think it's necessary to change this shell command at all. The only differences should be in the API method's implementation, and maybe this command's description to emphasize it's an estimate. None of the printing of the output needs to be changed at all to achieve the desired goal of this PR, to get size info from the metadata."", 'commenter': 'ctubbsii'}]"
2900,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -1076,11 +1076,21 @@ Map<String,Integer> listConstraints(String tableName)
       throws AccumuloException, TableNotFoundException;
 
   /**
-   * Gets the number of bytes being used in the files for a set of tables
+   * Gets the number of bytes being used by the files for a set of tables. This operation will use","[{'comment': '```suggestion\r\n   * Gets the number of bytes being used by the files for a set of tables. This operation will\r\n```', 'commenter': 'dlmarion'}]"
2900,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -1076,11 +1076,21 @@ Map<String,Integer> listConstraints(String tableName)
       throws AccumuloException, TableNotFoundException;
 
   /**
-   * Gets the number of bytes being used in the files for a set of tables
+   * Gets the number of bytes being used by the files for a set of tables. This operation will use
+   * scan the metadata table for file size information to compute the size metrics for the tables.
+   *
+   * Because the metadata table is used for computing usage and not the actual files in HDFS the
+   * results will be an estimate. Older entries may exist with no file metadata (resulting in size
+   * 0) and other actions in the cluster can impact the estimated size such as flushes, tablet
+   * splits, compactions, etc.
+   *
+   * For the most accurate information a compaction should first be run on the set of tables being","[{'comment': '```suggestion\r\n   * For more accurate information a compaction should first be run on all files for the set of tables being\r\n```', 'commenter': 'dlmarion'}]"
2900,server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java,"@@ -42,25 +44,40 @@
 import org.apache.accumulo.core.data.Value;
 import org.apache.accumulo.core.dataImpl.KeyExtent;
 import org.apache.accumulo.core.metadata.MetadataTable;
-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;
+import org.apache.accumulo.core.metadata.RootTable;
+import org.apache.accumulo.core.metadata.TabletFile;
+import org.apache.accumulo.core.metadata.schema.DataFileValue;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
 import org.apache.accumulo.core.security.Authorizations;
 import org.apache.accumulo.core.trace.TraceUtil;
 import org.apache.accumulo.core.util.NumUtil;
 import org.apache.accumulo.server.cli.ServerUtilOpts;
 import org.apache.accumulo.server.fs.VolumeManager;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.LocatedFileStatus;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RemoteIterator;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import com.beust.jcommander.Parameter;
-import com.google.common.base.Joiner;
 
 import io.opentelemetry.api.trace.Span;
 import io.opentelemetry.context.Scope;
 
+/**
+ * This utility class will scan the Accumulo Metadata table to compute the disk usage for a table or
+ * table(s) by using the size value stored in columns that contain the column family
+ * {@link MetadataSchema.TabletsSection.DataFileColumnFamily}.
+ *
+ * This class will also track shared files to computed shared usage across all tables that are
+ * provided as part of the Set of tables when getting disk usage.
+ *
+ * Because the metadata table is used for computing usage and not the actual files in HDFS the
+ * results will be an estimate. Older entries may exist with no file metadata (resulting in size 0)
+ * and other actions in the cluster can impact the estimated size such as flushes, tablet splits,
+ * compactions, etc.
+ *
+ * For the most accurate information a compaction should first be run on the set of tables being","[{'comment': '```suggestion\r\n * For more accurate information a compaction should first be run on all of the files for the set of tables being\r\n```', 'commenter': 'dlmarion'}]"
2900,shell/src/main/java/org/apache/accumulo/shell/commands/DUCommand.java,"@@ -95,9 +107,14 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
 
   @Override
   public String description() {
-    return ""prints how much space, in bytes, is used by files referenced by a""
-        + "" table. When multiple tables are specified it prints how much space, in""
-        + "" bytes, is used by files shared between tables, if any."";
+    return ""Prints how much estimated space, in bytes, is used by files referenced by a ""","[{'comment': '```suggestion\r\n    return ""Prints estimated space, in bytes, used by files referenced by a ""\r\n```', 'commenter': 'dlmarion'}]"
2900,shell/src/main/java/org/apache/accumulo/shell/commands/DUCommand.java,"@@ -95,9 +107,14 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
 
   @Override
   public String description() {
-    return ""prints how much space, in bytes, is used by files referenced by a""
-        + "" table. When multiple tables are specified it prints how much space, in""
-        + "" bytes, is used by files shared between tables, if any."";
+    return ""Prints how much estimated space, in bytes, is used by files referenced by a ""
+        + ""table or tables.  When multiple tables are specified it prints how much space, in ""
+        + ""bytes, are used by files shared between tables, if any. Because the metadata table ""
+        + ""is used for the file size information and not the actual files in HDFS the results ""
+        + ""will be an estimate. Older entries may exist with no file metadata (resulting in size 0) and ""
+        + ""other actions in the cluster can impact the estimated size such as flushes, tablet splits, ""
+        + ""compactions, etc. For the most accurate information a compaction should first be run on the ""","[{'comment': '```suggestion\r\n        + ""compactions, etc. For more accurate information a compaction should first be run on all of the files for the ""\r\n```', 'commenter': 'dlmarion'}]"
2902,test/src/main/java/org/apache/accumulo/test/functional/KerberosRenewalIT.java,"@@ -200,7 +199,21 @@ private void createReadWriteDrop(AccumuloClient client) throws TableNotFoundExce
           new Key(""a"", ""b"", ""c"").compareTo(entry.getKey(), PartialKey.ROW_COLFAM_COLQUAL),
           ""Did not find the expected key"");
       assertEquals(""d"", entry.getValue().toString());
-      client.tableOperations().delete(table);
     }
+    client.tableOperations().delete(table);
+  }
+
+  private String createTableAndReturnTableName(AccumuloClient client) throws AccumuloException,
+      AccumuloSecurityException, TableNotFoundException, TableExistsException {
+    final String tableName = getUniqueNames(1)[0] + ""_table"";
+    try {
+      client.tableOperations().create(tableName);
+    } catch (TableExistsException e) {
+      log.debug(""Table {} already exists. Deleting and trying again."", tableName);
+      client.tableOperations().delete(tableName);","[{'comment': 'Do tableOperations wait for the operation to complete - or do they create a FATE op and return?  If the FATE op is launched, then this could end up with a race condition and end up throwing TableNotFound or another exception. If the FATE was in progress, the create fails because its there, the FATE completes and removes the table, then the delete fails becuase it does not exist.  Another strategy may be to pull the create / delete into functions that retry and pause a little a few times.\r\n', 'commenter': 'EdColeman'}, {'comment': ""That's a good point, and looking at the stack trace, probably how this test failed in the first place.\r\n\r\n> Another strategy may be to pull the create / delete into functions that retry and pause a little a few times.\r\n\r\nThis seems like a good idea. Also seems like a perfect place for the [`Wait.java`](https://github.com/cshannon/accumulo/blob/9d7a69d5ec4fb35c5f882a64596e7843ccbe886e/test/src/main/java/org/apache/accumulo/test/util/Wait.java#L28) test util that is part of #2799, added by @cshannon. I'm wondering if that could be broken into its own PR so it can be used elsewhere while #2799 is still under review.\r\n\r\n"", 'commenter': 'DomGarguilo'}, {'comment': '@EdColeman, this should be resolved as of [bdca3bd](https://github.com/apache/accumulo/pull/2902/commits/bdca3bde1fc9fef0034d9ebc2d1b6263f6d096d0). ', 'commenter': 'DomGarguilo'}]"
2902,test/src/main/java/org/apache/accumulo/test/functional/KerberosRenewalIT.java,"@@ -200,7 +199,21 @@ private void createReadWriteDrop(AccumuloClient client) throws TableNotFoundExce
           new Key(""a"", ""b"", ""c"").compareTo(entry.getKey(), PartialKey.ROW_COLFAM_COLQUAL),
           ""Did not find the expected key"");
       assertEquals(""d"", entry.getValue().toString());
-      client.tableOperations().delete(table);
     }
+    client.tableOperations().delete(table);","[{'comment': 'this could be in a finally block, but likely not necessary since this is a test', 'commenter': 'dlmarion'}, {'comment': 'Do you think that would be an improvement? It can be easily added if so.', 'commenter': 'DomGarguilo'}, {'comment': ""I don't know that it would be an improvement as you are getting a unique table name at the start of the test. If this test fails, it's unlikely that the table existing would cause an issue with another test or this test being repeated. If this were not test code, I would say putting it in a finally block would be needed."", 'commenter': 'dlmarion'}]"
2902,test/src/main/java/org/apache/accumulo/test/functional/KerberosRenewalIT.java,"@@ -200,7 +199,21 @@ private void createReadWriteDrop(AccumuloClient client) throws TableNotFoundExce
           new Key(""a"", ""b"", ""c"").compareTo(entry.getKey(), PartialKey.ROW_COLFAM_COLQUAL),
           ""Did not find the expected key"");
       assertEquals(""d"", entry.getValue().toString());
-      client.tableOperations().delete(table);
     }
+    client.tableOperations().delete(table);
+  }
+
+  private String createTableAndReturnTableName(AccumuloClient client) throws AccumuloException,
+      AccumuloSecurityException, TableNotFoundException, TableExistsException {
+    final String tableName = getUniqueNames(1)[0] + ""_table"";
+    try {
+      client.tableOperations().create(tableName);
+    } catch (TableExistsException e) {
+      log.debug(""Table {} already exists. Deleting and trying again."", tableName);
+      client.tableOperations().delete(tableName);
+      createTableAndReturnTableName(client);","[{'comment': '```suggestion\r\n      tableName = createTableAndReturnTableName(client);\r\n```', 'commenter': 'dlmarion'}]"
2906,core/src/main/java/org/apache/accumulo/core/util/Merge.java,"@@ -225,8 +226,9 @@ protected void merge(AccumuloClient client, String table, List<Size> sizes, int
     try {
       Text start = sizes.get(0).extent.prevEndRow();
       Text end = sizes.get(numToMerge - 1).extent.endRow();
-      message(""Merging %d tablets from (%s to %s]"", numToMerge, start == null ? ""-inf"" : start,
-          end == null ? ""+inf"" : end);
+      message(""Merging %d tablets from (%s to %s]"", numToMerge,
+          start == null ? ""-inf"" : Key.toPrintableString(start.getBytes(), 0, start.getLength(), end.getLength()),","[{'comment': 'I believe the last parameter should be `start.getLength()`.', 'commenter': 'dlmarion'}, {'comment': 'ah, definitely, thanks! Fix in flight.', 'commenter': 'skirklin'}]"
2910,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -2011,6 +2015,34 @@ public ImportDestinationArguments importDirectory(String directory) {
     return new BulkImport(directory, context);
   }
 
+  @Override
+  public TimeType getTimeType(final String tableName) throws TableNotFoundException {
+    if (tableName.equals(RootTable.NAME)) {
+      throw new IllegalArgumentException(""accumulo.root table has no TimeType"");
+    }
+    String systemTableToCheck =
+        MetadataTable.NAME.equals(tableName) ? RootTable.NAME : MetadataTable.NAME;
+    final Scanner scanner = context.createScanner(systemTableToCheck, Authorizations.EMPTY);","[{'comment': 'suggest moving this line to right before scanner is used and put in try-with-resources block.', 'commenter': 'dlmarion'}, {'comment': ""With the changes to be made with Keith's suggestion below this change will no longer be necessary."", 'commenter': 'jmark99'}]"
2910,test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java,"@@ -292,4 +295,59 @@ public void testCompactEmptyTablesWithBadIterator_FailsAndCancel() throws TableE
     }
   }
 
+  @Test
+  public void getTimeTypeTest() throws TableNotFoundException, AccumuloException,
+      TableExistsException, AccumuloSecurityException {
+    String[] tableNames = getUniqueNames(4);
+
+    // Create table with default MILLIS TimeType
+    accumuloClient.tableOperations().create(tableNames[0]);
+    TimeType timeType = accumuloClient.tableOperations().getTimeType(tableNames[0]);
+    assertEquals(TimeType.MILLIS, timeType);
+
+    // Create table with LOGICAL TimeType.
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    ntc.setTimeType(TimeType.LOGICAL);
+    accumuloClient.tableOperations().create(tableNames[1], ntc);
+    timeType = accumuloClient.tableOperations().getTimeType(tableNames[1]);
+    assertEquals(TimeType.LOGICAL, timeType);
+
+    // Create some split points
+    SortedSet<Text> splits = new TreeSet<>();
+    splits.add(new Text(""F""));
+    splits.add(new Text(""M""));
+    splits.add(new Text(""S""));
+
+    // Create table with MILLIS TimeType. Use splits to create multiple tablets
+    ntc = new NewTableConfiguration();
+    ntc.withSplits(splits);
+    accumuloClient.tableOperations().create(tableNames[2], ntc);","[{'comment': 'Maybe a note could be added to the comment about `TimeType.MILLIS` being the default for ntc and/or another test case could be added which explicitly calls `ntc.setTimeType(TimeType.MILLIS);`', 'commenter': 'DomGarguilo'}, {'comment': '@DomGarguilo I added a comment concerning the default TimeType at table creation and also created a separate case for explicitly setting the TimeType to MILLIS.', 'commenter': 'jmark99'}]"
2910,test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java,"@@ -292,4 +295,59 @@ public void testCompactEmptyTablesWithBadIterator_FailsAndCancel() throws TableE
     }
   }
 
+  @Test
+  public void getTimeTypeTest() throws TableNotFoundException, AccumuloException,
+      TableExistsException, AccumuloSecurityException {
+    String[] tableNames = getUniqueNames(4);
+
+    // Create table with default MILLIS TimeType
+    accumuloClient.tableOperations().create(tableNames[0]);
+    TimeType timeType = accumuloClient.tableOperations().getTimeType(tableNames[0]);
+    assertEquals(TimeType.MILLIS, timeType);
+
+    // Create table with LOGICAL TimeType.
+    NewTableConfiguration ntc = new NewTableConfiguration();
+    ntc.setTimeType(TimeType.LOGICAL);
+    accumuloClient.tableOperations().create(tableNames[1], ntc);
+    timeType = accumuloClient.tableOperations().getTimeType(tableNames[1]);
+    assertEquals(TimeType.LOGICAL, timeType);
+
+    // Create some split points
+    SortedSet<Text> splits = new TreeSet<>();
+    splits.add(new Text(""F""));
+    splits.add(new Text(""M""));
+    splits.add(new Text(""S""));
+
+    // Create table with MILLIS TimeType. Use splits to create multiple tablets
+    ntc = new NewTableConfiguration();
+    ntc.withSplits(splits);
+    accumuloClient.tableOperations().create(tableNames[2], ntc);
+    timeType = accumuloClient.tableOperations().getTimeType(tableNames[2]);
+    assertEquals(TimeType.MILLIS, timeType);
+
+    // Create table with LOGICAL TimeType. Use splits to create multiple tablets
+    ntc = new NewTableConfiguration();
+    ntc.setTimeType(TimeType.LOGICAL).withSplits(splits);
+    accumuloClient.tableOperations().create(tableNames[3], ntc);
+    timeType = accumuloClient.tableOperations().getTimeType(tableNames[3]);
+    assertEquals(TimeType.LOGICAL, timeType);
+
+    // check system tables
+    timeType = accumuloClient.tableOperations().getTimeType(""accumulo.metadata"");
+    assertEquals(TimeType.LOGICAL, timeType);
+
+    timeType = accumuloClient.tableOperations().getTimeType(""accumulo.replication"");
+    assertEquals(TimeType.LOGICAL, timeType);
+
+    // test non-existent table
+    assertThrows(TableNotFoundException.class,
+        () -> accumuloClient.tableOperations().getTimeType(""notatable""),
+        ""specified table that doesn't exist"");
+
+    // cannot get TimeType for root table
+    assertThrows(RuntimeException.class,","[{'comment': '```suggestion\r\n    assertThrows(IllegalArgumentException.class,\r\n```\r\nThis could be narrowed down I think', 'commenter': 'DomGarguilo'}, {'comment': 'With the changes suggested by Keith below this will no longer be necessary. All tables should return a valid TimeType and a runtime exception will be thrown if this is not the case.', 'commenter': 'jmark99'}]"
2910,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -2011,6 +2015,34 @@ public ImportDestinationArguments importDirectory(String directory) {
     return new BulkImport(directory, context);
   }
 
+  @Override
+  public TimeType getTimeType(final String tableName) throws TableNotFoundException {
+    if (tableName.equals(RootTable.NAME)) {
+      throw new IllegalArgumentException(""accumulo.root table has no TimeType"");
+    }
+    String systemTableToCheck =
+        MetadataTable.NAME.equals(tableName) ? RootTable.NAME : MetadataTable.NAME;
+    final Scanner scanner = context.createScanner(systemTableToCheck, Authorizations.EMPTY);
+    String tableId = tableIdMap().get(tableName);
+    if (tableId == null) {
+      throw new TableNotFoundException(null, tableName, ""specified table does not exist"");
+    }
+    final Text start = new Text(tableId);
+    final Text end = new Text(start);
+    start.append(new byte[] {'<'}, 0, 1);
+    end.append(new byte[] {'<'}, 0, 1);
+    scanner.setRange(new Range(start, end));
+    MetadataSchema.TabletsSection.ServerColumnFamily.TIME_COLUMN.fetch(scanner);
+    Entry<Key,Value> next = scanner.iterator().next();
+    Value val = next.getValue();
+    if (val.toString().startsWith(""L"")) {
+      return TimeType.LOGICAL;
+    } else if (val.toString().startsWith(""M"")) {
+      return TimeType.MILLIS;
+    }
+    throw new RuntimeException(""Failed to retrieve TimeType"");","[{'comment': 'May be able to use ample for this.  Also may not need a special case for root tablet w/ ample, seems like it will return the time type logical.  But not sure, this is the [code that initializes the root tablet metadata](https://github.com/apache/accumulo/blob/114704bb58a37073a0401c3b6f479761089bd1fb/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java#L189-L206) and makes me think it would return logical.\r\n\r\n```suggestion\r\n     String tableId = tableIdMap().get(tableName);\r\n    if (tableId == null) {\r\n      throw new TableNotFoundException(null, tableName, ""specified table does not exist"");\r\n    }\r\n        Optional<TabletMetadata> tabletMetadata = context.getAmple().readTablets().forTable(tableId)\r\n            .fetch(ColumnType.TIME).checkConsistency().build().stream().findFirst();\r\n    return tabletMetadata.orElseThrow(() -> new RuntimeException(""Failed to retrieve TimeType""));\r\n```', 'commenter': 'keith-turner'}, {'comment': '@keith-turner thanks for the suggestion. I forgot about the use of ample. I made a few slight tweaks. 1) forTable requires a TableID vs a String, and 2) I updated to return a TimeType rather than a TableMetadata object.', 'commenter': 'jmark99'}]"
2910,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -2011,6 +2013,20 @@ public ImportDestinationArguments importDirectory(String directory) {
     return new BulkImport(directory, context);
   }
 
+  @Override
+  public TimeType getTimeType(final String tableName) throws TableNotFoundException {
+    String tableId = tableIdMap().get(tableName);
+    if (tableId == null) {
+      throw new TableNotFoundException(null, tableName, ""specified table does not exist"");
+    }
+    Optional<TabletMetadata> tabletMetadata =
+        context.getAmple().readTablets().forTable(TableId.of(tableId))","[{'comment': 'May be able to use this function to get the table id and it throws a table not found exception.\r\n\r\n```suggestion\r\n    TableId tableId = context.getTableId(tableName);\r\n    Optional<TabletMetadata> tabletMetadata =\r\n        context.getAmple().readTablets().forTable(tableId)\r\n```', 'commenter': 'keith-turner'}]"
2910,test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java,"@@ -614,4 +615,15 @@ public void testMaxSplitsOption() throws Exception {
         ""0\n1\n2\n3\n4\n5\n6\n7\n8\n9\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\no\np\nq\nr\ns\nt\n"");
   }
 
+  @Test
+  public void testGetTimeType() throws Exception {
+    Shell.log.debug(""Starting testGetTimeType test -----------------"");
+    exec(""createtable tmtype"", true);
+    exec(""gettimetype"", true, TimeType.MILLIS.toString());
+    exec(""gettimetype -t tmtype"", true, TimeType.MILLIS.toString());","[{'comment': 'Could also try a user table with logical time to make the test cover a bit more.\r\n\r\n```suggestion\r\n    exec(""gettimetype -t tmtype"", true, TimeType.MILLIS.toString());\r\n    exec(""createtable -tl logicaltt"", true);\r\n     exec(""gettimetype -t logicaltt"", true, TimeType.LOGICAL.toString());\r\n```', 'commenter': 'keith-turner'}]"
2910,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -2011,6 +2013,16 @@ public ImportDestinationArguments importDirectory(String directory) {
     return new BulkImport(directory, context);
   }
 
+  @Override
+  public TimeType getTimeType(final String tableName) throws TableNotFoundException {
+    TableId tableId = context.getTableId(tableName);
+    Optional<TabletMetadata> tabletMetadata = context.getAmple().readTablets().forTable(tableId)
+        .fetch(TabletMetadata.ColumnType.TIME).checkConsistency().build().stream().findFirst();
+    TabletMetadata timeData =
+        tabletMetadata.orElseThrow(() -> new RuntimeException(""Failed to retrieve TimeType""));","[{'comment': 'Prefer a more specific RTE type. Perhaps IllegalStateException here?', 'commenter': 'ctubbsii'}, {'comment': 'Updated to use IllegalStateException.', 'commenter': 'jmark99'}]"
2910,shell/src/main/java/org/apache/accumulo/shell/commands/GetTimeTypeCommand.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.shell.commands;
+
+import java.util.LinkedList;
+import java.util.List;
+
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.shell.Shell;
+import org.apache.accumulo.shell.Shell.Command;
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.Options;
+
+public class GetTimeTypeCommand extends Command {","[{'comment': 'Instead of a very narrow ""gettimetype"" command, a more generic ""tableinfo"" command would be better, so that we don\'t need a thousand separate narrowly-scoped commands to get basic information about a table.', 'commenter': 'ctubbsii'}, {'comment': 'Removed shell related timetype code, leaving API changes behind.', 'commenter': 'jmark99'}]"
2917,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -133,6 +141,29 @@ public void upgradeZookeeper(ServerContext context) {
     createScanServerNodes(context);
   }
 
+  private void ensureProperAcls(ServerContext context) {
+
+    final ZooReaderWriter zrw = context.getZooReaderWriter();
+    final ZooKeeper zk = zrw.getZooKeeper();
+    final String rootPath = context.getZooKeeperRoot();
+    try {
+      ZKUtil.visitSubTreeDFS(zk, rootPath, false, (rc, path, ctx, name) -> {
+        try {
+          final Stat stat = zk.exists(path, false);
+          zk.setACL(path, ZooUtil.PUBLIC, stat.getAversion());","[{'comment': 'I believe that this would overwrite any node that had ZooUtil.PRIVATE settings and may not be what is intended.', 'commenter': 'EdColeman'}]"
2965,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsHelper.java,"@@ -56,10 +57,12 @@ public void attachIterator(String tableName, IteratorSetting setting,
     for (IteratorScope scope : scopes) {
       String root = String.format(""%s%s.%s"", Property.TABLE_ITERATOR_PREFIX,
           scope.name().toLowerCase(), setting.getName());
-      for (Entry<String,String> prop : setting.getOptions().entrySet()) {
-        this.setProperty(tableName, root + "".opt."" + prop.getKey(), prop.getValue());
-      }
-      this.setProperty(tableName, root, setting.getPriority() + "","" + setting.getIteratorClass());
+
+      Map<String,String> propsToAdd = setting.getOptions().entrySet().stream()
+          .collect(Collectors.toMap(prop -> root + "".opt."" + prop.getKey(), Entry::getValue));
+      propsToAdd.put(root, setting.getPriority() + "","" + setting.getIteratorClass());
+
+      this.modifyProperties(tableName, props -> props.putAll(propsToAdd));","[{'comment': ""ConcurrentModificationException is not being handled.  The previous code would set the props and never throw this.  Now a user could spuriously get this exception.    \r\n\r\nI didn't realize that ConcurrentModificationException was a run time exception when looking at #2799.  I don't think that is good user experience for a spurious exception that user can not take any action to avoid and must handle.  Since its a run time exception when a developer is writing code they will most likely be unaware of it and it will most likely not be thrown in their testing."", 'commenter': 'keith-turner'}, {'comment': 'If no one else is working on it or opposed to it, I would like to try creating a PR for the new modifyProperties method to make it retry automatically instead of throwing the  ConcurrentModificationException.  If that was done, then this PR could stay as is and not have to handle that condition.', 'commenter': 'keith-turner'}, {'comment': '> If no one else is working on it or opposed to it, I would like to try creating a PR for the new modifyProperties method to make it retry automatically instead of throwing the ConcurrentModificationException. If that was done, then this PR could stay as is and not have to handle that condition.\r\n\r\nSounds good to me :+1: ', 'commenter': 'DomGarguilo'}, {'comment': 'Another option could be to catch and turn the ConcurrentModificationException into a CheckedException so that callers are forced to handle it.  If two (or more) separate processes are trying to set the same properties, a retry could effectively allow the last one to always ""win"".  If the catcher just wraps the CheckedException in a retry that\'s what would happen anyway - but it would be done explicitly. \r\n\r\nIf the retry allows different, non-overlapping changes, that might be the ""best"" case.  P1 sets A, B, and P2 sets C, D then the final result should be A, B, C and D. The ""worst"" case would be if the final values were A,B or C,D \r\n\r\nIf the sets are overlapping then the result would be non-determinant P1 sets A, deletes B and P2 sets B and C, then B could be set or deleted.  Changing the same property should be much less frequent than modifying non-overlapping sets.\r\n\r\nCurrently we do not guarantee that a process will see a given set of properties anyway - say for example a scan - it will execute with whatever it happens to find - if you set a property you need to allow it to propagate to the tservers, but something could change it before the scan actually starts and we do not specify otherwise.', 'commenter': 'EdColeman'}, {'comment': '> Another option could be to catch and turn the ConcurrentModificationException into a CheckedException so that callers are forced to handle it. \r\n\r\nI am not a fan of this personally.  Looking at this PR, each call would have to handle the same case.  So that would lead to creating an internal static utility method that does the retry.\r\n\r\n> If the retry allows different, non-overlapping changes, that might be the ""best"" case. P1 sets A, B, and P2 sets C, D then the final result should be A, B, C and D. The ""worst"" case would be if the final values were A,B or C,D\r\n>\r\n>If the sets are overlapping then the result would be non-determinant P1 sets A, deletes B and P2 sets B and C, then B could be set or deleted. Changing the same property should be much less frequent than modifying non-overlapping sets.\r\n\r\nI created #2967 to do the retry and in that PR I also added a rigorous concurrency test.  The test makes lots of modifications in many threads and can detect if any single modification is lost.  The test also has some thread modify the same properties.\r\n\r\nIdeally when multiple threads are concurrently modifying properties, that should result in the same outcome as if all those modifications were done serially in some order.  Barring bugs, conceptually I think the current accumulo code should be able to achieve this.', 'commenter': 'keith-turner'}, {'comment': 'When I run the new IT added in #2967 I see evidence of many retries in the logs, so the threads are bumping into each other and retrying.', 'commenter': 'keith-turner'}]"
2965,core/src/test/java/org/apache/accumulo/core/clientImpl/TableOperationsHelperTest.java,"@@ -133,22 +133,18 @@ public void flush(String tableName, Text start, Text end, boolean wait) {}
 
     @Override
     public void setProperty(String tableName, String property, String value) {
-      if (!settings.containsKey(tableName))
-        settings.put(tableName, new TreeMap<>());
+      settings.putIfAbsent(tableName, new TreeMap<>());
       settings.get(tableName).put(property, value);
     }
 
     @Override
     public Map<String,String> modifyProperties(String tableName,
         Consumer<Map<String,String>> mapMutator)
         throws IllegalArgumentException, ConcurrentModificationException {
+      settings.putIfAbsent(tableName, new TreeMap<>());","[{'comment': '```suggestion\r\n      settings.computeIfAbsent(tableName, k -> new TreeMap<>());\r\n```', 'commenter': 'cshannon'}, {'comment': ""Small nit but if you use computeIfAbsent here then you save on the allocation each time of a new TreeMap that you don't need unless the value is absent so you reduce garbage collection since it's lazy allocated. Not a big deal since this is just a unit test but in general useful."", 'commenter': 'cshannon'}, {'comment': 'Oh cool, I did not know that, thanks!', 'commenter': 'DomGarguilo'}]"
2965,shell/src/main/java/org/apache/accumulo/shell/commands/CreateTableCommand.java,"@@ -134,22 +135,24 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
     shellState.setTableName(tableName); // switch shell to new table context
 
     if (cl.hasOption(createTableNoDefaultIters.getOpt())) {
-      for (String key : IteratorConfigUtil.generateInitialTableProperties(true).keySet()) {
-        shellState.getAccumuloClient().tableOperations().removeProperty(tableName, key);
-      }
+      Set<String> initialProps = IteratorConfigUtil.generateInitialTableProperties(true).keySet();
+      shellState.getAccumuloClient().tableOperations().modifyProperties(tableName,
+          properties -> initialProps.forEach(properties::remove));
     }
 
     // Copy options if flag was set
     if (cl.hasOption(createTableOptCopyConfig.getOpt())) {
       if (shellState.getAccumuloClient().tableOperations().exists(tableName)) {
         final Map<String,String> configuration = shellState.getAccumuloClient().tableOperations()
             .getConfiguration(cl.getOptionValue(createTableOptCopyConfig.getOpt()));
-        for (Entry<String,String> entry : configuration.entrySet()) {
-          if (Property.isValidTablePropertyKey(entry.getKey())) {
-            shellState.getAccumuloClient().tableOperations().setProperty(tableName, entry.getKey(),
-                entry.getValue());
-          }
-        }
+
+        Map<String,
+            String> propsToAdd = configuration.entrySet().stream()
+                .filter(entry -> Property.isValidTablePropertyKey(entry.getKey()))
+                .collect(Collectors.toMap(Entry::getKey, Entry::getValue));
+
+        shellState.getAccumuloClient().tableOperations().modifyProperties(tableName,
+            properties -> properties.putAll(propsToAdd));","[{'comment': '```suggestion\r\n        shellState.getAccumuloClient().tableOperations().modifyProperties(tableName,\r\n            properties -> configuration.entrySet().stream()\r\n                .filter(entry -> Property.isValidTablePropertyKey(entry.getKey()))\r\n                .forEach(entry -> properties.put(entry.getKey(), entry.getValue())));\r\n```', 'commenter': 'cshannon'}, {'comment': 'This change would make it so we only need to iterate once over the properties vs having to iterate over the map twice (once to filter into a new map and then again when putAll() is called)', 'commenter': 'cshannon'}, {'comment': 'That makes sense, ill include this change.', 'commenter': 'DomGarguilo'}, {'comment': 'Do you think I should refactor all of the spots that are similar to this to reduce the number of times we iterate?', 'commenter': 'DomGarguilo'}]"
2967,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -1035,6 +1031,30 @@ public void modifyProperties(String tableName, final Consumer<Map<String,String>
     }
   }
 
+  @Override
+  public void modifyProperties(String tableName, final Consumer<Map<String,String>> mapMutator)","[{'comment': 'Does it make sense to return a `Future<Void>` here instead? That would allow the caller to potentially perform other logic before checking the result and the caller has to handle the exceptions when calling Future.get().', 'commenter': 'dlmarion'}, {'comment': 'I would like to see Accumulo support async APIs.  I think that should be done with consideration of the entire API and looking at what APIs we want to add.  Not sure about doing that as a one off here.  Would need to create and manage a thread  or thread pool to do that here, which is another reason to consider it API wide maybe (maybe we want to allow passing in an executor for async APIs?)\r\n\r\nI would also like to explore making the read and write pipelines in the tserver and sserver asynchronous.  I think this could support more client connections per server and would be good for making them  more responsive in the case we had discussed in the past of having many sservers and few tservers.  It think it would be good to make the server pipelines async at the same time as adding async APIs to Accumulo so that the two could influence each other.', 'commenter': 'keith-turner'}, {'comment': 'Async APIs and server pipelines may be a good 3.0 project.', 'commenter': 'keith-turner'}, {'comment': '> Async APIs and server pipelines may be a good 3.0 project.\r\n\r\nI agree with this, I think adding Async APIs for the next major version would be a nice addition.', 'commenter': 'cshannon'}]"
2967,core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java,"@@ -1035,6 +1031,30 @@ public void modifyProperties(String tableName, final Consumer<Map<String,String>
     }
   }
 
+  @Override
+  public void modifyProperties(String tableName, final Consumer<Map<String,String>> mapMutator)
+      throws AccumuloException, AccumuloSecurityException, IllegalArgumentException {
+    EXISTING_TABLE_NAME.validate(tableName);
+    checkArgument(mapMutator != null, ""mapMutator is null"");
+
+    Retry retry =
+        Retry.builder().infiniteRetries().retryAfter(25, MILLISECONDS).incrementBy(25, MILLISECONDS)
+            .maxWait(30, SECONDS).backOffFactor(1.5).logInterval(3, MINUTES).createRetry();
+","[{'comment': 'Is it correct to have the log interval greater than max wait? Does this efficiently eliminate logging except on timeout?', 'commenter': 'EdColeman'}, {'comment': 'Good question.  I copied and pasted that code and modified it a bit. I went a looked at the code and logging does not happen unless an explicit method call is made.  I need to look into this some more, see if I should call the logging method or remove the config from the builder.', 'commenter': 'keith-turner'}, {'comment': 'I looked more closely at the retry code and made some updates in 3873e94.\r\n\r\nBelow are some log messages I saw while running the new IT added in this PR.\r\n\r\n```\r\n2022-09-28T14:31:47,217 [admin.TableOperations] DEBUG: Unable to modify table properties for PropStoreConfigIT_concurrentTablePropsModificationTest0 because of concurrent modification, retrying attempt 1 (suppressing retry messages for 180000ms)\r\n2022-09-28T14:31:47,217 [util.Retry] DEBUG: Sleeping for 25ms before retrying operation\r\n2022-09-28T14:31:47,252 [admin.TableOperations] DEBUG: Unable to modify table properties for PropStoreConfigIT_concurrentTablePropsModificationTest0 because of concurrent modification, retrying attempt 1 (suppressing retry messages for 180000ms)\r\n2022-09-28T14:31:47,252 [util.Retry] DEBUG: Sleeping for 25ms before retrying operation\r\n2022-09-28T14:31:47,283 [util.Retry] DEBUG: Sleeping for 62ms before retrying operation\r\n2022-09-28T14:31:47,330 [util.Retry] DEBUG: Sleeping for 150ms before retrying operation\r\n2022-09-28T14:31:47,332 [util.Retry] DEBUG: Sleeping for 158ms before retrying operation\r\n2022-09-28T14:31:47,362 [util.Retry] DEBUG: Sleeping for 79ms before retrying operation\r\n2022-09-28T14:31:47,458 [util.Retry] DEBUG: Sleeping for 111ms before retrying operation\r\n2022-09-28T14:31:47,497 [util.Retry] DEBUG: Sleeping for 217ms before retrying operation\r\n2022-09-28T14:31:47,500 [util.Retry] DEBUG: Sleeping for 215ms before retrying operation\r\n2022-09-28T14:31:47,578 [util.Retry] DEBUG: Sleeping for 153ms before retrying operation\r\n2022-09-28T14:31:47,727 [util.Retry] DEBUG: Sleeping for 302ms before retrying operation\r\n2022-09-28T14:31:47,734 [admin.TableOperations] DEBUG: Unable to modify table properties for PropStoreConfigIT_concurrentTablePropsModificationTest0 because of concurrent modification, retrying attempt 1 (suppressing retry messages for 180000ms)\r\n2022-09-28T14:31:47,735 [util.Retry] DEBUG: Sleeping for 25ms before retrying operation\r\n2022-09-28T14:31:47,746 [admin.TableOperations] DEBUG: Unable to modify table properties for PropStoreConfigIT_concurrentTablePropsModificationTest0 because of concurrent modification, retrying attempt 1 (suppressing retry messages for 180000ms)\r\n2022-09-28T14:31:47,746 [util.Retry] DEBUG: Sleeping for 25ms before retrying operation\r\n```', 'commenter': 'keith-turner'}]"
2967,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsImpl.java,"@@ -20,20 +20,11 @@
 
 import static com.google.common.base.Preconditions.checkArgument;
 import static java.nio.charset.StandardCharsets.UTF_8;
-import static java.util.concurrent.TimeUnit.SECONDS;
-import static org.apache.accumulo.core.util.Validators.EXISTING_NAMESPACE_NAME;
-import static org.apache.accumulo.core.util.Validators.NEW_NAMESPACE_NAME;
+import static java.util.concurrent.TimeUnit.*;
+import static org.apache.accumulo.core.util.Validators.*;","[{'comment': 'Star imports will cause the QA check to fail', 'commenter': 'EdColeman'}]"
2967,core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceOperationsImpl.java,"@@ -227,6 +216,34 @@ public void modifyProperties(final String namespace,
     }
   }
 
+  public void modifyProperties(final String namespace,
+      final Consumer<Map<String,String>> mapMutator)
+      throws AccumuloException, AccumuloSecurityException, NamespaceNotFoundException {
+    EXISTING_NAMESPACE_NAME.validate(namespace);
+    checkArgument(mapMutator != null, ""mapMutator is null"");
+
+    Retry retry =
+        Retry.builder().infiniteRetries().retryAfter(25, MILLISECONDS).incrementBy(25, MILLISECONDS)
+            .maxWait(30, SECONDS).backOffFactor(1.5).logInterval(3, MINUTES).createRetry();
+
+    while (true) {
+      try {
+        tryToModifyProperties(namespace, mapMutator);
+        break;
+      } catch (ConcurrentModificationException cme) {
+        try {
+          retry.logRetry(log, ""Unable to modify namespace properties for "" + namespace","[{'comment': 'Maybe it could read - ""modification attempt failed because of a concurrent modification - will retry""', 'commenter': 'EdColeman'}, {'comment': 'Agree that we should add the fact the fact that we are going to retry here and in other places where Retry.logRetry is called.', 'commenter': 'dlmarion'}, {'comment': 'I made a top level comment about this.', 'commenter': 'keith-turner'}, {'comment': 'I think this can be resolved based on the top level comment.', 'commenter': 'dlmarion'}]"
2967,test/src/main/java/org/apache/accumulo/test/conf/PropStoreConfigIT.java,"@@ -27,11 +27,17 @@
 import static org.junit.jupiter.api.Assertions.assertNull;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+import java.util.HashMap;
+import java.util.List;
 import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
 import java.util.function.Consumer;
 import java.util.function.Supplier;
 
-import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.*;","[{'comment': 'another star import', 'commenter': 'EdColeman'}]"
2967,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -57,19 +56,80 @@ void setProperty(final String property, final String value)
    * property overrides in ZooKeeper. Only properties which can be stored in ZooKeeper will be
    * accepted.
    *
+   * <P>","[{'comment': 'Please use lowercase for html tags to comply with HTML5 conventions and W3C recommendations (and are entirely disallowed in xhtml). I keep fixing these when they get added as uppercase, but they keep getting added using uppercase again, and they are really hard to grep for or use automated tooling to check, because generics also use the less than and greater than symbols, and html is notoriously difficult to parse properly. Parsing inside a javadoc comment is even trickier.', 'commenter': 'ctubbsii'}]"
2967,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -56,30 +56,36 @@ void setProperty(final String property, final String value)
    * property overrides in ZooKeeper. Only properties which can be stored in ZooKeeper will be
    * accepted.
    *
-   * <P>
+   * <p>
+   * Accumulo has multiple layers of properties that for many APIs and SPIs presented as single","[{'comment': '... APIs and SPIs **are** presented as **a** single merged view.', 'commenter': 'dlmarion'}, {'comment': 'fixed in 767ba52a6192d70ffa8b2152f12127cb7c560c42', 'commenter': 'keith-turner'}]"
2967,core/src/main/java/org/apache/accumulo/core/client/admin/NamespaceOperations.java,"@@ -175,6 +175,12 @@ void setProperty(String namespace, String property, String value)
    * {@link InstanceOperations#modifyProperties(Consumer)} which operates on a different layer of
    * properties but has the same behavior and better documentation.
    *
+   * <p>
+   * Accumulo has multiple layers of properties that for many APIs and SPIs presented as single","[{'comment': 'same comment', 'commenter': 'dlmarion'}]"
2967,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -611,6 +611,12 @@ void setProperty(String tableName, String property, String value)
    * {@link InstanceOperations#modifyProperties(Consumer)} which operates on a different layer of
    * properties but has the same behavior and better documentation.
    *
+   * <p>
+   * Accumulo has multiple layers of properties that for many APIs and SPIs presented as single","[{'comment': 'same comment', 'commenter': 'dlmarion'}]"
2967,core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java,"@@ -119,6 +127,38 @@ public void modifyProperties(final Consumer<Map<String,String>> mapMutator)
     // Send to server
     ThriftClientTypes.MANAGER.executeVoid(context, client -> client
         .modifySystemProperties(TraceUtil.traceInfo(), context.rpcCreds(), vProperties));
+
+    return vProperties.getProperties();
+  }
+
+  @Override
+  public Map<String,String> modifyProperties(final Consumer<Map<String,String>> mapMutator)
+      throws AccumuloException, AccumuloSecurityException, IllegalArgumentException {
+
+    var log = LoggerFactory.getLogger(InstanceOperationsImpl.class);
+
+    Retry retry =
+        Retry.builder().infiniteRetries().retryAfter(25, MILLISECONDS).incrementBy(25, MILLISECONDS)
+            .maxWait(30, SECONDS).backOffFactor(1.5).logInterval(3, MINUTES).createRetry();
+
+    while (true) {
+      try {
+        var props = tryToModifyProperties(mapMutator);
+        retry.logCompletion(log, ""Modifying instance properties"");
+        return props;
+      } catch (ConcurrentModificationException cme) {
+        try {
+          retry.logRetry(log,
+              ""Unable to modify instance properties for because of concurrent modification"");
+          retry.waitForNextAttempt();
+        } catch (InterruptedException e) {
+          throw new RuntimeException(e);
+        }
+      } finally {
+        retry.useRetry();","[{'comment': ""I wonder if instead of calling the method `Retry.logCompletion`, it should be called `Retry.completed(msg)` and then it disables any checks that might throw errors. For example:\r\n\r\nI don't think this is a problem here, because this Retry is configured for infiniteRetries. However, if Retry.useRetry is called elsewhere in a finally block, it could throw an exception even if the operation completed successfully (in the case where the Retry was configured for N retries, and it had retried N times but succeeding on the last one."", 'commenter': 'dlmarion'}, {'comment': 'Looking further, it looks like the issue can be avoided, if in a finally block (and maybe elsewhere where useRetry is called):\r\n\r\n```\r\n   ...\r\n   } finally {\r\n     if (retry.canRetry()) {\r\n      retry.useRetry();\r\n     }\r\n  }\r\n```', 'commenter': 'dlmarion'}, {'comment': "">  wonder if instead of calling the method Retry.logCompletion, it should be called Retry.completed(msg) and then it disables any checks that might throw errors.\r\n\r\nI was also contemplating something similar when i wrote it, was thinking of names like `close()`, `finished()` and `completed()`.  Then was thinking like you said that those would require wider changes in the Retry class. I didn't want to do those changes in the PR so decided to make it more narrow and just focused on the need that was identified for logging completion. "", 'commenter': 'keith-turner'}, {'comment': 'Using the Rety class I find myself wondering if it can be improved and then more widely used in the code. Like maybe pass an interface to a method that will do the retries, looping, logging etc.  The interface can supply the operation to run and the messages to log.', 'commenter': 'keith-turner'}, {'comment': 'Continuation of prev comment.  Wondering about something like the following.  Thinking through it a bit i think dealing with checked exceptions would be the biggest pain point in doing something like this.\r\n\r\n```java\r\nRetry retry = ..\r\nretry.execute(new Retriable(){\r\n    //whatever is needed to execute a retriable operation : Runnable, descriptive message, etc\r\n});\r\n```', 'commenter': 'keith-turner'}, {'comment': 'I did something similar when we were working on external compactions to retry thift calls - [RetryableThriftCall](https://github.com/apache/accumulo/blob/main/server/base/src/main/java/org/apache/accumulo/server/compaction/RetryableThriftCall.java)', 'commenter': 'dlmarion'}, {'comment': '> I did something similar when we were working on external compactions to retry thift calls - [RetryableThriftCall](https://github.com/apache/accumulo/blob/main/server/base/src/main/java/org/apache/accumulo/server/compaction/RetryableThriftCall.java)\r\n\r\nWe need something more general like that so we can reuse retry code.  Still scratching my head over the best way to handle checked exceptions though.  Like whats the most concise way to handle one operation that throws checked exception X and another that throws checked exception Y and these are checked exceptions that we want to pass up.', 'commenter': 'keith-turner'}, {'comment': ""I think this is a case that's similar to `Future.get()` where it throws an `ExecutionException`. As an example, we could:\r\n\r\n1. modify  [RetryableThriftFunction](https://github.com/apache/accumulo/blob/main/server/base/src/main/java/org/apache/accumulo/server/compaction/RetryableThriftFunction.java) to declare `Exception` rather than `TException'. \r\n2. Then in [RetryableThrifCall](https://github.com/apache/accumulo/blob/main/server/base/src/main/java/org/apache/accumulo/server/compaction/RetryableThriftCall.java#L106) catch `Exception`\r\n3. When retries have been exhausted, throw a RetriesExceededException [here](https://github.com/apache/accumulo/blob/main/server/base/src/main/java/org/apache/accumulo/server/compaction/RetryableThriftCall.java#L118) when there is no error and throw an ExecutionException when retries have been exhausted and there is an error."", 'commenter': 'dlmarion'}, {'comment': '@dlmarion I think it would be good to open up some sort of follow on issue about improving retry, but not sure what exactly.  I think you have more defined thoughts on it.', 'commenter': 'keith-turner'}]"
2967,core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java,"@@ -57,19 +56,86 @@ void setProperty(final String property, final String value)
    * property overrides in ZooKeeper. Only properties which can be stored in ZooKeeper will be
    * accepted.
    *
+   * <p>
+   * Accumulo has multiple layers of properties that for many APIs and SPIs are presented as a
+   * single merged view. This API does not offer that merged view, it only offers the properties set
+   * at the system layer to the mapMutator.
+   * </p>
+   *
+   * <p>
+   * This new API offers two distinct advantages over the older {@link #setProperty(String, String)}
+   * API. The older API offered the ability to unconditionally set a single property. This new API
+   * offers the following.
+   * </p>
+   *
+   * <ul>
+   * <li>Ability to unconditionally set multiple properties atomically. If five properties are
+   * mutated by this API, then eventually all of the servers will see those changes all at once.
+   * This is really important for configuring something like a scan iterator that requires setting
+   * multiple properties.</li>
+   * <li>Ability to conditionally set multiple properties atomically. With this new API a snapshot
+   * of the current instance configuration is passed in to the mapMutator. Code can inspect the
+   * current config and decide what if any changes it would like to make. If the config changes
+   * while mapMutator is doing inspection and modification, then those actions will be ignored and
+   * it will be called again with the latest snapshot of the config.</li>
+   * </ul>
+   *
+   * <p>
+   * Below is an example of using this API to conditionally set some instance properties. If while
+   * trying to set the compaction planner properties another process modifies the manager balancer
+   * properties, then it would automatically retry and call the lambda again with the latest
+   * snapshot of instance properties.
+   * </p>
+   *
+   * <pre>
+   *         {@code
+   *             AccumuloClient client = getClient();
+   *             Map<String,String> acceptedProps = client.instanceOperations().modifyProperties(currProps -> {
+   *               var planner = currProps.get(""tserver.compaction.major.service.default.planner"");
+   *               //This code will only change the compaction planner if its currently set to default settings.
+   *               //The endsWith() function was used to make the example short, would be better to use equals().
+   *               if(planner != null && planner.endsWith(""DefaultCompactionPlanner"") {
+   *                 // tservers will eventually see these compaction planner changes and when they do they will see all of the changes at once
+   *                 currProps.keySet().removeIf(
+   *                    prop -> prop.startsWith(""tserver.compaction.major.service.default.planner.opts.""));
+   *                 currProps.put(""tserver.compaction.major.service.default.planner"",""MyPlannerClassName"");
+   *                 currProps.put(""tserver.compaction.major.service.default.planner.opts.myOpt1"",""val1"");
+   *                 currProps.put(""tserver.compaction.major.service.default.planner.opts.myOpt2"",""val2"");
+   *                }
+   *             });
+   *
+   *             // Since three properties were set may want to check for the values of all
+   *             // three, just checking one in this example to keep it short.
+   *             if(""MyPlannerClassName"".equals(acceptedProps.get(""tserver.compaction.major.service.default.planner""))){
+   *                // the compaction planner change was accepted or already existed, so take action for that outcome
+   *             } else {
+   *                // the compaction planner change was not done, so take action for that outcome
+   *             }
+   *           }
+   *         }
+   * </pre>
+   *
+   * @param mapMutator
+   *          This consumer should modify the passed snapshot of instance properties to contain the
+   *          desired keys and values. It should be safe for Accumulo to call this consumer multiple
+   *          times, this may be done automatically when certain retryable errors happen. The
+   *          consumer should probably avoid accessing the Accumulo client as that could lead to
+   *          undefined behavior.
+   *
    * @throws AccumuloException
    *           if a general error occurs
    * @throws AccumuloSecurityException
    *           if the user does not have permission
    * @throws IllegalArgumentException
    *           if the Consumer alters the map by adding properties that cannot be stored in
    *           ZooKeeper
-   * @throws ConcurrentModificationException
-   *           without altering the stored properties if the server reports that the properties have
-   *           been changed by another process
+   *
+   * @return The map that became Accumulo's new properties for this table. This map is immutable and
+   *         contains the snapshot passed to mapMutator and the changes made by mapMutator.
+   * @since 2.1.0
    */
-  void modifyProperties(Consumer<Map<String,String>> mapMutator) throws AccumuloException,
-      AccumuloSecurityException, IllegalArgumentException, ConcurrentModificationException;
+  Map<String,String> modifyProperties(Consumer<Map<String,String>> mapMutator)","[{'comment': 'I think `computeProperties` might also be a decent name, since it is now returning the properties. This is analogous to the `compute*()` methods that return the resulting value in the Java Map APIs.', 'commenter': 'ctubbsii'}, {'comment': 'I think i like `modify` a bit better because it conveys something that is happening in the distributed system where `compute` feels a bit more centered on what is happening in the client.', 'commenter': 'keith-turner'}]"
2992,server/base/src/main/java/org/apache/accumulo/server/util/DeleteZooInstance.java,"@@ -103,6 +112,19 @@ private static void cleanAllOld(ServerContext context, final ZooReaderWriter zk)
     }
   }
 
+  private static boolean checkCurrentInstance(ServerContext context, String instanceName,
+      String instanceId) {
+    if (instanceId.equals(context.getInstanceID().canonical())) {
+      String prompt = String.valueOf(
+          System.console().readLine(""Warning: This is the current instance, are you sure? Y/n: ""));
+      if (prompt == null || !prompt.equals(""Y"")) {","[{'comment': ""I know in other areas (like when deleting a table or namespace with the shell) we use (yes|no) and we also ignore cases. Not really sure how consistent we are with that though. At least ignoring the case would be a reasonable expectation from the user's viewpoint. "", 'commenter': 'Manno15'}, {'comment': 'Table deletion verification is done [here](https://github.com/apache/accumulo/blob/main/shell/src/main/java/org/apache/accumulo/shell/commands/TableOperation.java#L90), as an example.', 'commenter': 'dlmarion'}, {'comment': ""I agree 100% and will change it. I didn't know if we had other examples of confirm prompts but I should have searched because I agree that it should be made consistent and match if we already have a normal pattern for prompting the user for yes/no."", 'commenter': 'cshannon'}]"
2994,core/src/main/thrift/client.thrift,"@@ -325,64 +325,76 @@ service ClientService {
     2:trace.TInfo tinfo
     3:security.TCredentials credentials
     1:ConfigurationType type
+  ) throws (
+    1:ThriftSecurityException sec
   )
 
   map<string, string> getSystemProperties(
     1:trace.TInfo tinfo
     2:security.TCredentials credentials
+  ) throws (
+    1:ThriftSecurityException sec
   )
 
   TVersionedProperties getVersionedSystemProperties(
     1:trace.TInfo tinfo
     2:security.TCredentials credentials
+  ) throws (
+    1:ThriftSecurityException sec
   )
 
   map<string, string> getTableConfiguration(
     1:trace.TInfo tinfo
     3:security.TCredentials credentials
     2:string tableName
   ) throws (
-    1:ThriftTableOperationException tope
+    1:ThriftSecurityException sec
+    2:ThriftTableOperationException tope","[{'comment': ""This (and similar occurrences) changes the ordinal for the ThriftTableOperationException. This is okay if this is new for 2.1, but I haven't checked."", 'commenter': 'ctubbsii'}, {'comment': ""Good catch, I didn't think about that. I usually think about that for input parameters. I'll evaluate and change if necessary."", 'commenter': 'dlmarion'}, {'comment': 'Addressed in 7c8df00.', 'commenter': 'dlmarion'}]"
2994,server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java,"@@ -308,6 +307,11 @@ private Map<String,String> conf(TCredentials credentials, AccumuloConfiguration
   @Override
   public Map<String,String> getConfiguration(TInfo tinfo, TCredentials credentials,
       ConfigurationType type) throws TException {
+    if (!(security.isSystemUser(credentials) || security.hasSystemPermission(credentials,
+        credentials.getPrincipal(), SystemPermission.SYSTEM))) {","[{'comment': ""I'm not sure, but curious if this OR is necessary. I'd expect the system user to have system permission already. Also, because this is likely just checking the username, this could be bypassing authentication for the system user. If this is the case, this will need to be fixed, as well as the other occurrences of doing this elsewhere in this PR."", 'commenter': 'ctubbsii'}, {'comment': 'Well, I can say that without this, things like DumpConfigIT, which use the Admin utility, failed.', 'commenter': 'dlmarion'}, {'comment': '[isSystemUser](https://github.com/apache/accumulo/blob/main/server/base/src/main/java/org/apache/accumulo/server/security/SecurityOperation.java#L148-L152) just checks that the name of the token class in the API call is the same name as the token class used by the server. There is no ""user"" in this case.', 'commenter': 'dlmarion'}, {'comment': ""Right, so that's equivalent to comparing the user principal. The class name is basically the equivalent to the system user name.\r\n\r\nBut, the system user also has a password, and the authenticate method call was removed from line 296. We need to be very careful we're not bypassing the checking of the password for any RPC request that just masquerades as the system user, but doesn't actually authenticate as the system user.\r\n\r\nThe code highlighted here seems to only check the system user principal, and proceeds regardless of whether it authenticates. What I can't tell without taking a deeper look, is if the user has already been previously authenticated before reaching this line. If it has in every case, then this is fine... but if it hasn't, we still need to do that."", 'commenter': 'ctubbsii'}, {'comment': ""> I'm not sure, but curious if this OR is necessary. I'd expect the system user to have system permission already. \r\n\r\nTo answer your original question, if I remove the `security.isSystemUser(credentials)` predicate, then I get the following error in the Admin log for DumpConfigIT.\r\n\r\n```\r\n2022-10-06T11:23:06,542 [util.Admin] ERROR: Error USER_DOESNT_EXIST for user !SYSTEM - The user does not exist\r\norg.apache.accumulo.core.client.AccumuloSecurityException: Error USER_DOESNT_EXIST for user !SYSTEM - The user does not exist\r\n        at org.apache.accumulo.core.rpc.clients.TServerClient.execute(TServerClient.java:110) ~[classes/:?]\r\n        at org.apache.accumulo.core.rpc.clients.ClientServiceThriftClient.execute(ClientServiceThriftClient.java:52) ~[classes/:?]\r\n        at org.apache.accumulo.core.clientImpl.InstanceOperationsImpl.getSiteConfiguration(InstanceOperationsImpl.java:164) ~[classes/:?]\r\n        at org.apache.accumulo.server.util.Admin.printConfig(Admin.java:604) ~[classes/:?]\r\n        at org.apache.accumulo.server.util.Admin.execute(Admin.java:378) ~[classes/:?]\r\n        at org.apache.accumulo.server.util.Admin.main(Admin.java:259) ~[classes/:?]\r\n```\r\n\r\nA warning is printed in the tablet server log, but nothing else\r\n```\r\n2022-10-06T11:23:06,519 [accumulo.audit] WARN : operation: failed; user: !SYSTEM; checking permission SYSTEM on !SYSTEM denied; exception: ThriftSecurityException(user:!SYSTEM, code:USER_DOESNT_EXIST)\r\n```\r\n\r\nI'm not quite sure of the exact code path that is raising this exception on the server side. If we are able to resolve this and remove the `security.isSystemUser(credentials)` predicate, then the authenticate call is made as part of the SecurityOperation.hasSystemPermission() call."", 'commenter': 'dlmarion'}, {'comment': ""I added a log statement in SecurityOperation.targetUserExists, this is printed in the tserver log:\r\n\r\n```\r\n2022-10-06T11:47:52,320 [security.SecurityOperation] ERROR: User does not exist.\r\njava.lang.RuntimeException: null\r\n        at org.apache.accumulo.server.security.SecurityOperation.targetUserExists(SecurityOperation.java:413) ~[classes/:?]\r\n        at org.apache.accumulo.server.security.SecurityOperation._hasSystemPermission(SecurityOperation.java:325) ~[classes/:?]\r\n        at org.apache.accumulo.server.security.SecurityOperation.hasSystemPermission(SecurityOperation.java:819) ~[classes/:?]\r\n        at org.apache.accumulo.server.security.AuditedSecurityOperation.hasSystemPermission(AuditedSecurityOperation.java:675) ~[classes/:?]\r\n        at org.apache.accumulo.server.client.ClientServiceHandler.getConfiguration(ClientServiceHandler.java:310) ~[classes/:?]\r\n ```\r\n \r\n I don't know enough about this area of the code to understand why this is being thrown. SecurityOperation.hasSystemPermission calls `casAskAboutOtherUsers` which authenticates the user, but then it calls _hasSystemPermission which ends up throwing the error that the user doesn't exist."", 'commenter': 'dlmarion'}, {'comment': 'All of the SecurityOperation._has*Permission methods have this in the javadoc: `This cannot check if a system user has permission.`. I think I have a fix, will push soon.', 'commenter': 'dlmarion'}]"
2994,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKPermHandler.java,"@@ -401,6 +401,7 @@ public void initializeSecurity(TCredentials itw, String rootuser)
         Collections.singleton(NamespacePermission.ALTER_NAMESPACE));
     namespacePerms.put(Namespace.ACCUMULO.id(),
         Collections.singleton(NamespacePermission.ALTER_TABLE));
+    namespacePerms.put(Namespace.DEFAULT.id(), Collections.singleton(NamespacePermission.READ));","[{'comment': ""The root user shouldn't have this permission by default. This namespace is for user tables, and there's no reason the root user should have read permission on the user tables by default."", 'commenter': 'ctubbsii'}, {'comment': ""Right, so I'm assuming that the SYSTEM permission is required to read the instance/system configs, NamespacePermission.READ is required to read the namespace configs, and TablePermission.READ is required to read table property / configs. "", 'commenter': 'dlmarion'}, {'comment': 'Addressed in 7c8df00. I removed this line from ZKPermHandler', 'commenter': 'dlmarion'}]"
2994,test/src/main/java/org/apache/accumulo/test/NamespacesIT.java,"@@ -755,6 +755,7 @@ public void testPermissions() throws Exception {
 
       loginAs(root);
       c.securityOperations().grantNamespacePermission(u1, n1, NamespacePermission.ALTER_TABLE);
+      c.securityOperations().grantTablePermission(u1, t3, TablePermission.READ);
       loginAs(user1);
       user1Con.tableOperations().setProperty(t3, Property.TABLE_FILE_MAX.getKey(), ""42"");","[{'comment': ""Alter table permissions should be sufficient for reading properties. It should not require READ permission. I'm also wondering if READ is even the right permission at all. Should users who merely have permission to scan a table be presumed to be able to read the configuration for that table? Or should they have `ALTER_TABLE` permission instead."", 'commenter': 'ctubbsii'}, {'comment': 'I modified the required permissions from READ to ALTER in 7c8df00, except for the system config / properties as there is no ALTER_SYSTEM property. So, the required properties are SystemPermission.SYSTEM, NamespacePermission.ALTER_NAMESPACE, and TablePermission.ALTER_TABLE.', 'commenter': 'dlmarion'}, {'comment': 'SystemPermission.SYSTEM is the equivalent to the ALTER versions, so that makes sense.', 'commenter': 'ctubbsii'}]"
3001,test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java,"@@ -419,7 +421,14 @@ private void testMissingSystemPermission(String tableNamePrefix, AccumuloClient
         break;
       case OBTAIN_DELEGATION_TOKEN:
         if (saslEnabled()) {","[{'comment': ""It doesn't appear that SASL is actually enabled in this test so this code never executes which I'm sure is why the TODO is there in the first place and why the test seems to work.\r\n\r\nSASL is enabled by setting static system properties that are checked in `AccumuloClusterHarness` so there may be some work here to only enable it for that one check or maybe it's simplest to create a new test class (or one that extends this) that sets the property to enable it so the `TestingKdc` is initialized and this can be tested."", 'commenter': 'cshannon'}, {'comment': 'Oops, good catch, guess I missed that.', 'commenter': 'DomGarguilo'}]"
3001,test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java,"@@ -592,7 +601,16 @@ private void testGrantedSystemPermission(String tableNamePrefix, AccumuloClient
         break;
       case OBTAIN_DELEGATION_TOKEN:
         if (saslEnabled()) {","[{'comment': 'Same comment as above', 'commenter': 'cshannon'}]"
3003,pom.xml,"@@ -695,6 +695,11 @@
       <artifactId>spotbugs-annotations</artifactId>
       <optional>true</optional>
     </dependency>
+    <dependency>
+      <groupId>org.apache.logging.log4j</groupId>
+      <artifactId>log4j-core</artifactId>
+      <scope>provided</scope>
+    </dependency>","[{'comment': ""We don't need to use provided scope. That is intended for shipping packages that depend on stuff provided by their containerized environment, like a Maven plugin expected to run inside Maven, or a WAR intended to run inside an application server.\r\n\r\nInstead, we need to explicitly depend on this as a `test` dependency in places where we need it, or as an optional runtime dependency. For us, the runtime dependency doesn't really matter because we only need it for the distribution tarball.\r\n\r\n```suggestion\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'This should be removed when #3000 is merged.', 'commenter': 'EdColeman'}]"
3003,server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java,"@@ -90,24 +85,33 @@ protected PropStoreKey(final InstanceId instanceId, final String path, final ID_
   public static @Nullable PropStoreKey<?> fromPath(final String path) {
     String[] tokens = path.split(""/"");
 
+    if (tokens.length < 1) {
+      return null;
+    }
+
     InstanceId instanceId;
     try {
       instanceId = InstanceId.of(tokens[IID_TOKEN_POSITION]);
     } catch (ArrayIndexOutOfBoundsException ex) {
       log.warn(""Path '{}' is an invalid path for a property cache key"", path);
       return null;
     }
-    if (tokens.length < 1 || !tokens[tokens.length - 1].equals(PROP_NODE_NAME)) {
-      // without tokens or it does not end with PROP_NAME_NAME
-      return null;
-    }
-    if (tokens[TYPE_TOKEN_POSITION].equals(TABLES_NODE_NAME)) {
+
+    String nodeName = ""/"" + tokens[tokens.length - 1];
+    if (tokens[TYPE_TOKEN_POSITION].equals(TABLES_NODE_NAME) && nodeName.equals(ZCONFIG)) {","[{'comment': 'I think that in general this method could use a bit more robust validation of the tokens length before indexing. For example, on this line the code doesn\'t make sure the tokens length is long enough before indexing TYPE_TOKEN_POSITION. Because of this if you do something like call PropStoreKey.fromPath(""/accumulo/"" + iid); then an ArrayIndexoutOfBoundsException is thrown. In theory you wouldn\'t normally call this but good to handle all error cases.\r\n\r\nOne thing that might make it easier is to just wrap all the if statements here inside the try/catch block that already exists to handle IID_TOKEN_POSITION on line 93. That way if at any part of the processing we get an ArrayIndexoutOfBoundsException it can be handled the same way with the warning logged and null returned. Something like this could work:\r\n\r\n```\r\n    try {\r\n      InstanceId instanceId = InstanceId.of(tokens[IID_TOKEN_POSITION]);\r\n      \r\n      String nodeName = ""/"" + tokens[tokens.length - 1];\r\n      if (tokens[TYPE_TOKEN_POSITION].equals(TABLES_NODE_NAME) && nodeName.equals(ZCONFIG)) {\r\n        return TablePropKey.of(instanceId, TableId.of(tokens[ID_TOKEN_POSITION]));\r\n      }\r\n\r\n      if (tokens[TYPE_TOKEN_POSITION].equals(NAMESPACE_NODE_NAME) && nodeName.equals(ZCONFIG)) {\r\n        return NamespacePropKey.of(instanceId, NamespaceId.of(tokens[ID_TOKEN_POSITION]));\r\n      }\r\n\r\n      if (nodeName.equals(ZCONFIG)) {\r\n        return SystemPropKey.of(instanceId);\r\n      }\r\n    } catch (ArrayIndexOutOfBoundsException ex) {\r\n      log.warn(""Path \'{}\' is an invalid path for a property cache key"", path);\r\n      return null;\r\n    }\r\n```\r\n\r\n', 'commenter': 'cshannon'}, {'comment': 'addressed in ac8d0b8da8', 'commenter': 'EdColeman'}]"
3003,server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java,"@@ -90,24 +85,33 @@ protected PropStoreKey(final InstanceId instanceId, final String path, final ID_
   public static @Nullable PropStoreKey<?> fromPath(final String path) {
     String[] tokens = path.split(""/"");
 
+    if (tokens.length < 1) {
+      return null;
+    }
+
     InstanceId instanceId;
     try {
       instanceId = InstanceId.of(tokens[IID_TOKEN_POSITION]);
     } catch (ArrayIndexOutOfBoundsException ex) {
       log.warn(""Path '{}' is an invalid path for a property cache key"", path);
       return null;
     }
-    if (tokens.length < 1 || !tokens[tokens.length - 1].equals(PROP_NODE_NAME)) {
-      // without tokens or it does not end with PROP_NAME_NAME
-      return null;
-    }
-    if (tokens[TYPE_TOKEN_POSITION].equals(TABLES_NODE_NAME)) {
+
+    String nodeName = ""/"" + tokens[tokens.length - 1];
+    if (tokens[TYPE_TOKEN_POSITION].equals(TABLES_NODE_NAME) && nodeName.equals(ZCONFIG)) {
       return TablePropKey.of(instanceId, TableId.of(tokens[ID_TOKEN_POSITION]));
     }
-    if (tokens[TYPE_TOKEN_POSITION].equals(NAMESPACE_NODE_NAME)) {
+
+    if (tokens[TYPE_TOKEN_POSITION].equals(NAMESPACE_NODE_NAME) && nodeName.equals(ZCONFIG)) {
       return NamespacePropKey.of(instanceId, NamespaceId.of(tokens[ID_TOKEN_POSITION]));
     }
-    return SystemPropKey.of(instanceId);
+
+    if (nodeName.equals(ZCONFIG)) {","[{'comment': ""I was wondering if it make sense to do this check first? Since it's only one condition in theory it should be a little faster if it's a system prop to process as it is only 1 string comparison vs 2 for tables/namespaces. In practice it probably won't make a noticeable difference."", 'commenter': 'cshannon'}, {'comment': 'They are ordered in the most likely occurrence  - in general the number of tables > number of namespaces > system so it seemed rational to order the check so that what would (hopefully) be more common to be checked first.', 'commenter': 'EdColeman'}]"
3003,server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoader.java,"@@ -63,6 +63,9 @@ public ZooPropLoader(final ZooReaderWriter zrw, final VersionedPropCodec propCod
 
       Stat stat = new Stat();
       byte[] bytes = zrw.getData(propStoreKey.getPath(), propStoreWatcher, stat);
+      if (bytes.length == 0) {","[{'comment': ""Can bytes be null here? I'm assuming not as I think you would just get a NoNodeException which is handled but thought I'd bring it up."", 'commenter': 'cshannon'}, {'comment': 'Addressed in ac8d0b8da8 - in this case, the ZooKeeper stat is also being used, so that should be another reliable check for the data length.  The NoNodeException likely was sufficient - but checking the stat does not seem to hurt.', 'commenter': 'EdColeman'}]"
3003,server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropStore.java,"@@ -222,6 +223,10 @@ public void create(PropStoreKey<?> propStoreKey, Map<String,String> props) {
     try {
       Stat stat = new Stat();
       byte[] bytes = zooReader.getData(propStoreKey.getPath(), watcher, stat);
+      if (bytes.length == 0) {","[{'comment': 'Same comment/question about bytes being null as above.', 'commenter': 'cshannon'}, {'comment': 'Addressed in ac8d0b8da8 and see previous comment', 'commenter': 'EdColeman'}]"
3003,server/base/src/main/java/org/apache/accumulo/server/conf/util/ConfigTransformer.java,"@@ -152,18 +152,15 @@ VersionedProperties transform(final PropStoreKey<?> propStoreKey, final Transfor
         }
       }
 
-      Set<LegacyPropNode> upgradeNodes = readLegacyProps(propStoreKey);
-      if (upgradeNodes == null) {
-        log.info(""Found existing node after reading legacy props {}, skipping conversion"",
+      Set<LegacyPropNode> upgradeNodes = readLegacyProps(legacyPath);
+      if (upgradeNodes.size() == 0) {
+        log.trace(""No existing legacy props {}, skipping conversion, writing default prop node"",
             propStoreKey);
-        results = ZooPropStore.readFromZk(propStoreKey, propStoreWatcher, zrw);
-        if (results != null) {
-          return results;
-        }
+        return writeNode(propStoreKey, Map.of());
       }
 
       upgradeNodes = convertDeprecatedProps(propStoreKey, upgradeNodes);
-
+      // todo - here","[{'comment': 'Did you mean to leave this here?', 'commenter': 'cshannon'}, {'comment': 'No', 'commenter': 'EdColeman'}, {'comment': 'Removed in ac8d0b8da8', 'commenter': 'EdColeman'}]"
3003,server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropStore.java,"@@ -438,6 +443,9 @@ private VersionedProperties readPropsFromZk(PropStoreKey<?> propStoreKey)
     try {
       Stat stat = new Stat();
       byte[] bytes = zrw.getData(propStoreKey.getPath(), stat);
+      if (bytes.length == 0) {","[{'comment': 'Same comment/question about bytes being null as above.', 'commenter': 'cshannon'}, {'comment': 'Addressed in ac8d0b8da8 and see previous comment', 'commenter': 'EdColeman'}]"
3003,server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java,"@@ -89,25 +89,44 @@ protected PropStoreKey(final InstanceId instanceId, final String path, final ID_
    */
   public static @Nullable PropStoreKey<?> fromPath(final String path) {
     String[] tokens = path.split(""/"");
+    if (tokens.length < 1) {
+      return null;
+    }
 
     InstanceId instanceId;
     try {
       instanceId = InstanceId.of(tokens[IID_TOKEN_POSITION]);
+","[{'comment': 'Alternatively, you could remove the try/catch here and check that tokens.length equals EXPECTED_CONFIG_LEN or EXPECTED_SYS_CONFIG_LEN.', 'commenter': 'dlmarion'}, {'comment': 'addressed in b9ef24d03b', 'commenter': 'EdColeman'}]"
3003,server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java,"@@ -89,25 +89,44 @@ protected PropStoreKey(final InstanceId instanceId, final String path, final ID_
    */
   public static @Nullable PropStoreKey<?> fromPath(final String path) {
     String[] tokens = path.split(""/"");
+    if (tokens.length < 1) {
+      return null;
+    }
 
     InstanceId instanceId;
     try {
       instanceId = InstanceId.of(tokens[IID_TOKEN_POSITION]);
+
+      // needs to start with /accumulo/[instanceId]
+      if (!path.startsWith(ZooUtil.getRoot(instanceId))) {
+        log.warn(
+            ""Path '{}' is invalid for a property cache key, expected to start with /accumulo/[instance_id]"",","[{'comment': 'Is there a reason not to put the actual instanceId in the warning message?', 'commenter': 'dlmarion'}, {'comment': 'addressed in b9ef24d03b', 'commenter': 'EdColeman'}]"
3011,server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKPermHandler.java,"@@ -396,11 +398,14 @@ public void initializeSecurity(TCredentials itw, String rootuser)
     tablePerms.put(RootTable.ID, Collections.singleton(TablePermission.ALTER_TABLE));
     tablePerms.put(MetadataTable.ID, Collections.singleton(TablePermission.ALTER_TABLE));
     // essentially the same but on the system namespace, the ALTER_TABLE permission is now redundant
+    // After PR #2994 which added security checks for configuration we need to also add
+    // ALTER_NAMESPACE
+    // to both Default and Accumulo Namespaces for the root user.
     Map<NamespaceId,Set<NamespacePermission>> namespacePerms = new HashMap<>();
-    namespacePerms.put(Namespace.ACCUMULO.id(),
+    namespacePerms.put(Namespace.DEFAULT.id(),","[{'comment': 'I think this needs to be removed and the comment udpated.', 'commenter': 'dlmarion'}, {'comment': ""You can't remove this or it doesn't work. You have to grant ALTER_NAMESPACE to the default namespace or else when you use the config command as the root user on a table in the default namespace you get a permission error."", 'commenter': 'cshannon'}, {'comment': ""I should add you don't need to grant READ permission or anything else but you specifically need ALTER_NAMESPACE"", 'commenter': 'cshannon'}, {'comment': 'Yes, I know. But you are handling the permission errors in part 2.', 'commenter': 'dlmarion'}, {'comment': ""Right, but I figured we'd still want to give permission to see the configs on tables in the default namespace for the root user (but maybe not). I know @ctubbsii [said](https://github.com/apache/accumulo/pull/2994#discussion_r986243402) to drop READ permission but I thought permission for the configs made sense since the root user is probably an admin trying to configure things so would want to see configs even if can't scan the table by default."", 'commenter': 'cshannon'}, {'comment': ""That's fair, hopefully @ctubbsii can chime in on this."", 'commenter': 'dlmarion'}, {'comment': ""> You can't remove this or it doesn't work. You have to grant ALTER_NAMESPACE to the default namespace or else when you use the config command as the root user on a table in the default namespace you get a permission error.\r\n\r\nYeah, that's the desired behavior. The root user does not get permissions by default to any user namespaces, including the default namespace (which is really only a legacy namespace intended for user tables, to preserve the ability to specify an unqualified table name, like before the namespace feature was added)."", 'commenter': 'ctubbsii'}]"
3014,core/src/main/java/org/apache/accumulo/core/clientImpl/bulk/ConcurrentKeyExtentCache.java,"@@ -120,12 +125,20 @@ public KeyExtent lookup(Text row) {
 
         for (Text lookupRow : lookupRows) {
           if (getFromCache(lookupRow) == null) {
-            Iterator<KeyExtent> iter = lookupExtents(lookupRow).iterator();
-            while (iter.hasNext()) {
-              KeyExtent ke2 = iter.next();
-              if (inCache(ke2))
+            while (true) {
+              try {
+                Iterator<KeyExtent> iter = lookupExtents(lookupRow).iterator();
+                while (iter.hasNext()) {
+                  KeyExtent ke2 = iter.next();
+                  if (inCache(ke2))
+                    break;
+                  updateCache(ke2);
+                }
                 break;
-              updateCache(ke2);
+              } catch (TabletDeletedException tde) {
+                // tablets were merged away in the table, start over and try again
+                log.debug(""Ignoring exception"", tde);","[{'comment': 'we may want to say something about why we are ignoring the exception. For example, ""ignoring exception as tablets were likely merged, retrying...""', 'commenter': 'dlmarion'}, {'comment': 'Good call, that was a really awful message.  I pushed a new message in 8b58d16.', 'commenter': 'keith-turner'}]"
3014,core/src/main/java/org/apache/accumulo/core/clientImpl/bulk/ConcurrentKeyExtentCache.java,"@@ -120,12 +125,24 @@ public KeyExtent lookup(Text row) {
 
         for (Text lookupRow : lookupRows) {
           if (getFromCache(lookupRow) == null) {
-            Iterator<KeyExtent> iter = lookupExtents(lookupRow).iterator();
-            while (iter.hasNext()) {
-              KeyExtent ke2 = iter.next();
-              if (inCache(ke2))
+            while (true) {
+              try {
+                Iterator<KeyExtent> iter = lookupExtents(lookupRow).iterator();
+                while (iter.hasNext()) {
+                  KeyExtent ke2 = iter.next();
+                  if (inCache(ke2))
+                    break;
+                  updateCache(ke2);
+                }
                 break;
-              updateCache(ke2);
+              } catch (TabletDeletedException tde) {
+                // tablets were merged away in the table, start over and try again
+                log.debug(
+                    ""While trying to obtain a tablet location for bulk import a tablet was ""
+                        + ""deleted.   This can be caused by a concurrent merge tablet ""
+                        + ""operation.  If so its of no concern, otherwise it could be a problem."",","[{'comment': 'Suggest getting rid of some of these extra spaces after the periods, and other small punctuation/grammar:\r\n\r\n```suggestion\r\n                    ""While trying to obtain a tablet location for bulk import, a tablet was ""\r\n                        + ""deleted. If this was caused by a concurrent merge tablet ""\r\n                        + ""operation, this is okay. Otherwise, it could be a problem."",\r\n```', 'commenter': 'ctubbsii'}]"
3015,shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java,"@@ -155,14 +155,23 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
     } else {
       // display properties
       final TreeMap<String,String> systemConfig = new TreeMap<>();
-      systemConfig
-          .putAll(shellState.getAccumuloClient().instanceOperations().getSystemConfiguration());
+      try {
+        systemConfig
+            .putAll(shellState.getAccumuloClient().instanceOperations().getSystemConfiguration());
+      } catch (AccumuloSecurityException e) {
+        Shell.log.warn(""System Configuration: "" + e.getMessage());
+      }
 
       final String outputFile = cl.getOptionValue(outputFileOpt.getOpt());
       final PrintFile printFile = outputFile == null ? null : new PrintFile(outputFile);
 
       final TreeMap<String,String> siteConfig = new TreeMap<>();
-      siteConfig.putAll(shellState.getAccumuloClient().instanceOperations().getSiteConfiguration());
+      try {
+        siteConfig
+            .putAll(shellState.getAccumuloClient().instanceOperations().getSiteConfiguration());
+      } catch (AccumuloSecurityException e) {
+        Shell.log.warn(""Site Configuration: "" + e.getMessage());","[{'comment': 'This is not a safe way to print with interpolation that logging provides. A better way is:\r\n\r\n```suggestion\r\n        Shell.log.warn(""Site Configuration: {}"", e.getMessage());\r\n```', 'commenter': 'ctubbsii'}]"
3015,core/src/main/java/org/apache/accumulo/core/client/admin/TableOperations.java,"@@ -667,12 +667,14 @@ void removeProperty(String tableName, String property)
    *          the name of the table
    * @return all properties visible by this table (system and per-table properties). Note that
    *         recently changed properties may not be visible immediately.
+   * @throws AccumuloSecurityException
+   *           if the user does not have permission
    * @throws TableNotFoundException
    *           if the table does not exist
    * @since 1.6.0
    */
   default Iterable<Entry<String,String>> getProperties(String tableName)
-      throws AccumuloException, TableNotFoundException {
+      throws AccumuloException, AccumuloSecurityException, TableNotFoundException {","[{'comment': ""This is a breaking API change, so we probably can't do this. In other places, we sometimes wrap the checked exception in an AccumuloException, in order to avoid changing the API. It's a hack, but it gets around the restriction on breaking the API."", 'commenter': 'ctubbsii'}, {'comment': 'Can we make this change in 3.0? That might be a good time to clean some of this stuff up with a major version release.', 'commenter': 'cshannon'}, {'comment': 'Backed out API changes in a38afd6', 'commenter': 'dlmarion'}]"
3025,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -62,12 +63,11 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       }
     }
     if (configuration != null) {
-      for (Entry<String,String> entry : configuration.entrySet()) {
-        if (Property.isValidTablePropertyKey(entry.getKey())) {
-          shellState.getAccumuloClient().namespaceOperations().setProperty(namespace,
-              entry.getKey(), entry.getValue());
-        }
-      }
+      var propsToAdd = configuration.entrySet().stream()
+          .filter(entry -> Property.isValidTablePropertyKey(entry.getKey()))
+          .collect(Collectors.toMap(Entry::getKey, Entry::getValue));
+      shellState.getAccumuloClient().namespaceOperations().modifyProperties(namespace,
+          properties -> properties.putAll(propsToAdd));","[{'comment': '```suggestion\r\n      final Map<String,String> config = configuration;\r\n      shellState.getAccumuloClient().namespaceOperations().modifyProperties(namespace,\r\n          properties -> config.entrySet().stream()\r\n              .filter(entry -> Property.isValidTablePropertyKey(entry.getKey()))\r\n              .forEach(entry -> properties.put(entry.getKey(), entry.getValue())));\r\n```\r\nBasically the same suggestion as in #2965', 'commenter': 'cshannon'}]"
3036,test/src/main/java/org/apache/accumulo/test/conf/PropStoreConfigIT.java,"@@ -46,23 +48,40 @@
 import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
 import org.apache.accumulo.core.rpc.clients.ThriftClientTypes;
 import org.apache.accumulo.core.trace.TraceUtil;
-import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.accumulo.harness.SharedMiniClusterBase;
 import org.apache.accumulo.server.ServerContext;
 import org.apache.accumulo.server.conf.store.NamespacePropKey;
 import org.apache.accumulo.server.conf.store.SystemPropKey;
 import org.apache.accumulo.server.conf.store.TablePropKey;
 import org.apache.accumulo.test.util.Wait;
+import org.junit.jupiter.api.AfterAll;
+import org.junit.jupiter.api.BeforeAll;
 import org.junit.jupiter.api.Tag;
 import org.junit.jupiter.api.Test;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 @Tag(MINI_CLUSTER_ONLY)
 @Tag(SUNNY_DAY)
-public class PropStoreConfigIT extends AccumuloClusterHarness {
+public class PropStoreConfigIT extends SharedMiniClusterBase {
 
   private static final Logger log = LoggerFactory.getLogger(PropStoreConfigIT.class);
 
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(1);
+  }
+
+  @BeforeAll
+  public static void setup() throws Exception {
+    SharedMiniClusterBase.startMiniCluster();
+  }
+
+  @AfterAll
+  public static void teardown() {
+    SharedMiniClusterBase.stopMiniCluster();
+  }
+","[{'comment': '```suggestion\r\n\r\n  @BeforeEach\r\n  public void clear() throws Exception {\r\n    try (var client = Accumulo.newClient().from(getClientProps()).build()) {\r\n      client.instanceOperations().modifyProperties(Map::clear);\r\n    }\r\n  }\r\n\r\n```', 'commenter': 'cshannon'}, {'comment': ""This is what I mean, I think this should be sufficient. Each test already generally does this (clears out and tries to restore the previous state). The issue is that if a test fails that may not execute or future tests may not do that so I think it's best to add this just to make sure we are in a clean state otherwise this looks good to me."", 'commenter': 'cshannon'}, {'comment': ""I think I'd still be concerned that the tests could be running concurrently. Maybe it's not a problem. I'm not sure."", 'commenter': 'ctubbsii'}]"
3067,server/base/src/main/java/org/apache/accumulo/server/compaction/FileCompactor.java,"@@ -280,7 +280,10 @@ public CompactionStats call() throws IOException, CompactionCanceledException {
           }
         }
       } catch (IOException | RuntimeException e) {
-        log.warn(""{}"", e.getMessage(), e);
+        log.warn(""{}"", e.getMessage());
+        if (log.isDebugEnabled()) {
+          log.debug(e.getMessage(), e);","[{'comment': 'This is a safer version, in case there\'s any braces in the message that could be misinterpreted as a string interpolation.\r\n\r\n```suggestion\r\n          log.debug(""{}"", e.getMessage(), e);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Good to know, I was wondering what the purpose of that was. I will keep that in mind for the future.', 'commenter': 'cshannon'}]"
3067,server/base/src/main/java/org/apache/accumulo/server/compaction/FileCompactor.java,"@@ -392,7 +395,10 @@ private void compactLocalityGroup(String lgName, Set<ByteSequence> columnFamilie
             try {
               mfw.close();
             } catch (IOException e) {
-              log.error(""{}"", e.getMessage(), e);
+              log.error(""{}"", e.getMessage());
+              if (log.isDebugEnabled()) {
+                log.debug(e.getMessage(), e);","[{'comment': '```suggestion\r\n                log.debug(""{}"", e.getMessage(), e);\r\n```', 'commenter': 'ctubbsii'}]"
3067,server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java,"@@ -578,6 +578,8 @@ protected Runnable createCompactionJob(final TExternalCompactionJob job,
         TCompactionStatusUpdate update2 = new TCompactionStatusUpdate(TCompactionState.SUCCEEDED,
             ""Compaction completed successfully"", -1, -1, -1);
         updateCompactionState(job, update2);
+      } catch (FileCompactor.CompactionCanceledException cce) {
+        LOG.debug(""Compaction canceled {} "", job.getExternalCompactionId());","[{'comment': '```suggestion\r\n        LOG.debug(""Compaction canceled {}"", job.getExternalCompactionId());\r\n```', 'commenter': 'ctubbsii'}]"
3067,server/base/src/main/java/org/apache/accumulo/server/compaction/FileCompactor.java,"@@ -280,7 +280,10 @@ public CompactionStats call() throws IOException, CompactionCanceledException {
           }
         }
       } catch (IOException | RuntimeException e) {
-        log.warn(""{}"", e.getMessage(), e);
+        log.warn(""{}"", e.getMessage());
+        if (log.isDebugEnabled()) {","[{'comment': ""You don't need these `if (log.isDebugEnabled())` conditionals either. You only need that when you're avoiding some expensive operation while constructing the log message. The only thing you'd avoid here is `e.getMessage()`, which I don't think is necessary."", 'commenter': 'ctubbsii'}, {'comment': ""Good point, mostly a force of habit when doing debugging statements as sometimes it's expensive when outputting a lot of information. I'll push an update."", 'commenter': 'cshannon'}]"
3067,server/base/src/main/java/org/apache/accumulo/server/compaction/FileCompactor.java,"@@ -280,7 +283,13 @@ public CompactionStats call() throws IOException, CompactionCanceledException {
           }
         }
       } catch (IOException | RuntimeException e) {
-        log.warn(""{}"", e.getMessage(), e);
+        // If compaction was cancelled then this may happen due to an
+        // InterruptedException etc so suppress logging
+        if (!env.isCompactionEnabled()) {","[{'comment': 'I think you have this backward.', 'commenter': 'dlmarion'}, {'comment': 'Yup, indeed I do, oops.', 'commenter': 'cshannon'}]"
3067,server/base/src/main/java/org/apache/accumulo/server/compaction/FileCompactor.java,"@@ -280,7 +283,13 @@ public CompactionStats call() throws IOException, CompactionCanceledException {
           }
         }
       } catch (IOException | RuntimeException e) {
-        log.warn(""{}"", e.getMessage(), e);
+        // If compaction was cancelled then this may happen due to an
+        // InterruptedException etc so suppress logging
+        if (!env.isCompactionEnabled()) {
+          log.debug(""{}"", e.getMessage(), e);
+        } else {
+          log.warn(""{}"", e.getMessage(), e);
+        }","[{'comment': 'I\'m very confused about the negation and order of these. Getting rid of the negation, it becomes:\r\n\r\n```suggestion\r\n        // If compaction was cancelled then this may happen due to an\r\n        // InterruptedException etc so suppress logging\r\n        if (env.isCompactionEnabled()) {\r\n          log.warn(""{}"", e.getMessage(), e);\r\n        } else {\r\n          log.debug(""{}"", e.getMessage(), e);\r\n        }\r\n```\r\n\r\nBut in addition to this, the comment is a bit confusing, because it talks about `if compaction was cancelled`, but the code says `if compaction is enabled`. It would be good to improve the comment so it connects the dots between the condition the code is checking and the phrasing of the comment. It\'s not clear to me from reading this the connection between compaction being enabled and a cancellation.', 'commenter': 'ctubbsii'}, {'comment': ""Suggestion:\r\n\r\n```\r\nThe CompactionEnv may disable the compaction for a number of reasons (e.g. user compaction cancelled, tablet split, table deletion, etc.). When this happens it's possible for an exception to be thrown when a file fails to close or some other cleanup-type issue. These error messages can be supressed.\r\n```"", 'commenter': 'dlmarion'}, {'comment': 'Ignore my last, I was making the comment at the same time you pushed a change. Your new comment is essentially the same.', 'commenter': 'dlmarion'}, {'comment': 'sounds good, your is a bit more concise but I think either is fine and as you said basically says the same thing.', 'commenter': 'cshannon'}]"
3083,server/base/src/main/java/org/apache/accumulo/server/compaction/FileCompactor.java,"@@ -212,9 +216,21 @@ public CompactionStats call() throws IOException, CompactionCanceledException {
     try {
       FileOperations fileFactory = FileOperations.getInstance();
       FileSystem ns = this.fs.getFileSystemByPath(outputFile.getPath());
-      mfw = fileFactory.newWriterBuilder()
+
+      boolean dropCacheBehindMajcOutput = false;
+      if (!RootTable.ID.equals(this.extent.tableId())
+          && !MetadataTable.ID.equals(this.extent.tableId())
+          && acuTableConf.getBoolean(Property.TABLE_MAJC_OUTPUT_DROP_CACHE)) {","[{'comment': ""@keith-turner - I tried looking through new pluggable compaction service code, I'm thinking it may be possible for someone to implement their own CompactionConfigurer such that they can set this new property based on the KeyExtent that is getting compacted. Is that possible? If not, then we may want to add that functionality in a follow-on commit. I'm thinking of the case where you have a date sorted table and you may not want to drop the files from the page cache when compacting recent data, but may want to for older data."", 'commenter': 'dlmarion'}, {'comment': ""> @keith-turner - I tried looking through new pluggable compaction service code, I'm thinking it may be possible for someone to implement their own CompactionConfigurer such that they can set this new property based on the KeyExtent that is getting compacted. Is that possible? \r\n\r\nThat would be possible and I think its a really good idea.  Doing it in 2.1.1 has the same pros and cons as were discussed with adding a new prop in 2.1.1.  The SPI definition does not prohibit that though.  If there is a compelling enough configurer that could be written with the new method, then maybe that outweighs the possible confusion.\r\n\r\nTo make the change you suggested I think that could be done by adding key extent [here](https://github.com/apache/accumulo/blob/f3d9903f8072efa50e75970d971595a66dc8affe/core/src/main/java/org/apache/accumulo/core/client/admin/compaction/CompactionConfigurer.java#L50-L56).  Adding to a new method there will not break any code written against 2.1.0, it only breaks things when a developer writes code against 2.1.1 and tries to run it against 2.1.0 where it could cause a method not found exception.  However its possible no one ever bumps into this problem."", 'commenter': 'keith-turner'}]"
3083,core/src/main/java/org/apache/accumulo/core/file/rfile/RFileOperations.java,"@@ -131,6 +135,20 @@ protected FileSKVWriter openWriter(FileOptions options) throws IOException {
       outputStream = fs.create(new Path(file), false, bufferSize, (short) rep, block);
     }
 
+    if (options.dropCacheBehind) {
+      // Tell the DataNode that the write ahead log does not need to be cached in the OS page
+      // cache","[{'comment': ""copy / paste error? This isn't a write-ahead log."", 'commenter': 'ctubbsii'}, {'comment': 'Yes, good catch. Will fix.', 'commenter': 'dlmarion'}, {'comment': 'Resolved in 4cd1525', 'commenter': 'dlmarion'}]"
3083,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -964,6 +964,11 @@ public enum Property {
       ""1.3.5""),
   TABLE_ARBITRARY_PROP_PREFIX(""table.custom."", null, PropertyType.PREFIX,
       ""Prefix to be used for user defined arbitrary properties."", ""1.7.0""),
+  TABLE_MAJC_OUTPUT_DROP_CACHE(""table.compaction.major.output.drop.cache"", ""false"",
+      PropertyType.BOOLEAN,
+      ""Setting this property to true will call""
+          + ""FSDataOutputStream.setDropBehind(true) on the major compaction output stream."",
+      ""2.1.1""),","[{'comment': 'Do we really need this? Why not just always set it, and let it be cached when read?\r\n\r\nAlso, this is a user-facing configuration addition (not strictly public API, but analogous in terms of forwards/backwards-compatibility issues and user expectations). We try to avoid those kinds of additions in patch releases. Can we justify adding it in 2.1.1?', 'commenter': 'ctubbsii'}, {'comment': '> Do we really need this? Why not just always set it, and  it  be cached when read?\r\n\r\nThere could be unforseen negative consequences with always doing dropbehind. Making it configurable give a an escape hatch for that.  However maybe if we do enough testing prior to releasing 2.1.1 we could always set it and be comfortable with that from the testing.  If it causes problems after release, then would need a 2.1.2 though.  \r\n\r\nThe following test may be useful.\r\n\r\n 1. Running concurrent query and compaction on the same table and measuring the query times\r\n 2. Running query on one table and concurrent compactions on another table and measuring query times.\r\n\r\nRun both of the above test w/ and w/o compaction input/output drop behind and compare the two.\r\n\r\nAdding new props in each bug fix release has the potential to cause confusion for anyone trying to use 2.1.X. For better or worse its something we have done in many past bug fix releases.  Its hard to assess the probability of the new feature causing problems (and not having the escape hatch) vs the new property causing confusion over time which causes its own problems.  If we do add a new prop for output, it seems like adding one for input may also be useful if its not already done.', 'commenter': 'keith-turner'}, {'comment': 'If the key extent aware configurer is a compelling feature, then it would require a new table prop for the use case @dlmarion outlined.  So I think  the decision on adding this prop hinges around whether the configurer change is made. From that perspective would it make sense to do both in the same PR?', 'commenter': 'keith-turner'}, {'comment': ""> Can we justify adding it in 2.1.1?\r\n\r\nI'm not sure we need to. There are plenty of examples of properties being added in a patch release, including 4 of them added in version 1.9.3.\r\n\r\n> Why not just always set it, and let it be cached when read?\r\n> Making it configurable give a an escape hatch for that.\r\n\r\nThis is exactly my concern.  I think it may be safe to always do this, but I'm not 100% positive. The way that Hadoop implemented this, you can tell the OS via the FADVISE_DONTNEED flag that the OS can purge the file from the page cache when the read or write is done (in this case write). I read that this is done by setting that advice on the file descriptor and that each process (the DN in this case) has a file descriptor table. I don't know if re-opening the same file shortly after would re-use the same file descriptor number and I don't know if that advice may prevent caching of the file on a subsequent read operation. My guess is that it's safe to call it on the majc output file, but I don't know for sure."", 'commenter': 'dlmarion'}, {'comment': '@dlmarion  the use case you mentioned sounds pretty compelling to me.  If the change to support it were made it would be nice to implement the use case on top of the new changes to see if it actually works as intended.  That could help with decision on whether merging is worthwhile.', 'commenter': 'keith-turner'}, {'comment': ""> If the key extent aware configurer is a compelling feature, then it would require a new table prop for the use case @dlmarion outlined.\r\n\r\nWould it require a new property if we didn't do anything with it in the default compaction configurer? For the use case that I described, I was thinking that a user would have to write their own [configurer](https://github.com/apache/accumulo/blob/main/core/src/main/java/org/apache/accumulo/core/conf/Property.java#L1007) and [override](https://github.com/apache/accumulo/blob/f3d9903f8072efa50e75970d971595a66dc8affe/core/src/main/java/org/apache/accumulo/core/client/admin/compaction/CompactionConfigurer.java#L73) the table property based on the KeyExtent."", 'commenter': 'dlmarion'}, {'comment': ""> actually works as intended.\r\n\r\nI have no issue  with the testing you suggested. However, I don't know whether we will be able to tell if the file was *actually* purged from the OS page cache as the FADVISE directive is advice."", 'commenter': 'dlmarion'}, {'comment': '> For the use case that I described, I was thinking that a user would have to write their own [configurer](https://github.com/apache/accumulo/blob/main/core/src/main/java/org/apache/accumulo/core/conf/Property.java#L1007)\r\n\r\nYeah that is what I was thinking, the changes only support someone writing their own configurer for the use case you described.', 'commenter': 'keith-turner'}, {'comment': ""> I'm not sure we need to. There are plenty of examples of properties being added in a patch release, including 4 of them added in version 1.9.3.\r\n\r\nWe should think about it and try to provide a strong justification for it. Just because we've done it before doesn't mean it's a good idea in every case (FWIW, I pushed back on at least 2 of those in 1.9.3). I'm not saying we can't do it. I'm just suggesting that we give it due consideration first. The main things to consider is risk of destabilizing changes in a patch release due to newer features, and forward-compatibility expectations when rolling back (for example, what is a reasonable user expectation when rolling back and the property configured for 2.1.1 isn't recognized under 2.1.0).\r\n\r\nIn this case, a reasonable strong justification might be measurable performance impact. If the performance impact is negligible or merely hypothetical, or could be worse as easily as it could be better, then those are reasons to hold off. But, if it's a clear benefit, and is safe to rollback to 2.1.0, minimal low-risk code changes, then that could add up to be a good justification to include it in a patch release. For me, the question about the measurable impact remains an unanswered one, which leaves me with reservations for including it."", 'commenter': 'ctubbsii'}, {'comment': 'An alternate approach would be to remove the property and make use of Property.TABLE_ARBITRARY_PROP_PREFIX (""table.custom.""). We could create a defined property in FileCompactor (e.g. table.custom.compaction.major.output.drop.cache), which defaults to false and allow the custom CompactionConfigurer to set it to true.', 'commenter': 'dlmarion'}, {'comment': 'I opened #3088 to add the extent to the configurer.  While doing that I noticed it would also be nice to add it to the selector for symmetry.  ', 'commenter': 'keith-turner'}, {'comment': ""@dlmarion The arbitrary properties are intended to be a namespace for storage of properties for user-pluggable components, not for builtin framework configurations. For example, setting the volume chooser class is a builtin configuration, but some volume choosers may make use of arbitrary properties to control their behavior. I don't think that situation applies here. My understanding is that FileCompactor is a builtin component, part of the compaction framework, and not a user-pluggable component."", 'commenter': 'ctubbsii'}, {'comment': ""That's fair, a better place for the defined Property is in [CompactionConfigurer](https://github.com/apache/accumulo/blob/main/core/src/main/java/org/apache/accumulo/core/client/admin/compaction/CompactionConfigurer.java). That's user-pluggable via a table [property](https://github.com/apache/accumulo/blob/main/core/src/main/java/org/apache/accumulo/core/conf/Property.java#L1007). I'm not sure why the CompactionConfigurer class isn't in the SPI package."", 'commenter': 'dlmarion'}, {'comment': ""> I'm not sure why the CompactionConfigurer class isn't in the SPI package.\r\n\r\n@dlmarion I think there were two reasons its ended up in API instead of SPI.  First its referenced from the user compaction APIs.  Second it can deal with user data (like deciding to compress compaction output with a certain algorithm).  I think all of the compaction interfaces in the SPI only deal with execution of compactions and do not influence the data produced.  When an interface is related to user data it may be good to place it in API rather than SPI to give more stability."", 'commenter': 'keith-turner'}, {'comment': ""In 74b0e2c I removed this property from Property.java to CompactionConfigurer. I'm going to mark this conversation as resolved as the original issue was the addition of the property."", 'commenter': 'dlmarion'}]"
3083,core/src/main/java/org/apache/accumulo/core/client/admin/compaction/CompactionConfigurer.java,"@@ -76,4 +86,20 @@ public Map<String,String> getOverrides() {
   }
 
   Overrides override(InputParameters params);
+
+  /**
+   *
+   * @param tablePropertiesWithOverrides
+   *          table properties with overrides
+   * @return true if we should call setDropBehind on majc output file
+   *
+   * @since 2.1.1
+   */
+  static boolean
+      dropCacheBehindMajcOutput(Iterable<Entry<String,String>> tablePropertiesWithOverrides) {
+    ConfigurationCopy cc = new ConfigurationCopy(tablePropertiesWithOverrides);
+    Map<String,String> customProps =
+        cc.getAllPropertiesWithPrefix(Property.TABLE_ARBITRARY_PROP_PREFIX);
+    return Boolean.valueOf(customProps.getOrDefault(TABLE_MAJC_OUTPUT_DROP_CACHE, ""false""));
+  }","[{'comment': ""This method is a public API addition and shouldn't be added in a patch release. Beyond that, I don't quite understand:\r\n\r\n1. the use of a static method in the interface (what's the relationship to the interface? why does it live here instead of some internal util class?)\r\n2. the use of a static method in the public API and how users would use this method for their code (there's no javadoc comment, so it's not clear what its purpose is)\r\n3. it also seems a little inefficient to make a copy of the props in ConfigurationCopy, make another copy of the subset of props that have the custom table prefix, then get the property. Why not just iterate over the props and match on the exact value of the property you're looking for, without storing any intermediate copies?"", 'commenter': 'ctubbsii'}, {'comment': 'Removed this method in dba224e.', 'commenter': 'dlmarion'}]"
3083,server/base/src/main/java/org/apache/accumulo/server/compaction/FileCompactor.java,"@@ -212,9 +216,21 @@ public CompactionStats call() throws IOException, CompactionCanceledException {
     try {
       FileOperations fileFactory = FileOperations.getInstance();
       FileSystem ns = this.fs.getFileSystemByPath(outputFile.getPath());
-      mfw = fileFactory.newWriterBuilder()
+
+      boolean dropCacheBehindMajcOutput = false;
+      if (!RootTable.ID.equals(this.extent.tableId())
+          && !MetadataTable.ID.equals(this.extent.tableId())
+          && CompactionConfigurer.dropCacheBehindMajcOutput(acuTableConf)) {
+        dropCacheBehindMajcOutput = true;
+      }","[{'comment': 'This is more concise.\r\n```suggestion\r\n      boolean dropCacheBehindMajcOutput = !RootTable.ID.equals(this.extent.tableId())\r\n          && !MetadataTable.ID.equals(this.extent.tableId())\r\n          && CompactionConfigurer.dropCacheBehindMajcOutput(acuTableConf);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'OBE, logic updated from changes in dba224e. Updated conditional with suggested change.', 'commenter': 'dlmarion'}]"
3083,core/src/main/java/org/apache/accumulo/core/client/admin/compaction/CompactionConfigurer.java,"@@ -31,6 +31,13 @@
  * @since 2.1.0
  */
 public interface CompactionConfigurer {
+
+  /**
+   * Property that can be set on the table, or provided by the override method below for this
+   * compaction. When true, we will call setDropBehind on the majc output files.
+   */
+  String TABLE_MAJC_OUTPUT_DROP_CACHE = ""table.custom.majc.output.drop.cache"";","[{'comment': ""This is still a public API addition that can't occur in a patch release. If anybody references this constant, they won't be able to downgrade back to 2.1.0.\r\n\r\nAlso, based on the earlier discussion about when to use custom properties (for pluggable components) vs. built-in properties (for provided frameworks/built-in code), this does seem more suitable for a built-in property in Property.java. Although we try to avoid additions in patch releases, it's probably fine here, as long as we've considered the implications for downgraders. In this case, I don't think they are seriously harmed if they set this to true, then downgrade and it is ignored... so long as it's actually ignored and doesn't cause an error if set in 2.1.0 after downgrading."", 'commenter': 'ctubbsii'}, {'comment': 'I moved the property back to Property.java and removed all changes from CompactionConfigurer.', 'commenter': 'dlmarion'}]"
3083,core/src/main/java/org/apache/accumulo/core/file/rfile/RFileOperations.java,"@@ -129,7 +136,23 @@ protected FileSKVWriter openWriter(FileOptions options) throws IOException {
       String file = options.getFilename();
       FileSystem fs = options.getFileSystem();
 
-      outputStream = fs.create(new Path(file), false, bufferSize, (short) rep, block);
+      if (options.dropCacheBehind) {
+        EnumSet<CreateFlag> set = EnumSet.of(CreateFlag.SYNC_BLOCK, CreateFlag.CREATE);
+        outputStream = fs.create(new Path(file), FsPermission.getDefault(), set, bufferSize,","[{'comment': '`block` is passed to the other call of fs.create(), but not this one.  Is there a method that supports that and the perms?\r\n\r\n', 'commenter': 'keith-turner'}, {'comment': ""I'll look."", 'commenter': 'dlmarion'}, {'comment': 'turns out that `block` will work there, there is another variable called `blockSize` that was put there by mistake. Fixed in f5aaebb.', 'commenter': 'dlmarion'}]"
3106,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -93,9 +93,8 @@ public static <U> Class<? extends U> loadClass(String className, Class<U> extens
   /**
    * Retrieve the classloader context from a table's configuration.
    */
-  @SuppressWarnings(""removal"")
   public static String tableContext(AccumuloConfiguration conf) {
-    return conf.get(conf.resolve(Property.TABLE_CLASSLOADER_CONTEXT, Property.TABLE_CLASSPATH));
+    return conf.get(conf.resolve(Property.TABLE_CLASSLOADER_CONTEXT));","[{'comment': ""You should simplify these calls to `resolve`, if there's only one property. The `resolve` method tries to select the property to use based on what the user has set in the config. However, if there's only one, then there's nothing to `resolve` and you can just call `get` on the property directly.\r\n\r\n```suggestion\r\n    return conf.get(Property.TABLE_CLASSLOADER_CONTEXT);\r\n```"", 'commenter': 'ctubbsii'}]"
3106,core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java,"@@ -470,9 +467,7 @@ public Future<?> submit(Runnable task) {
    */
   public ScheduledThreadPoolExecutor
       createGeneralScheduledExecutorService(AccumuloConfiguration conf) {
-    @SuppressWarnings(""deprecation"")
-    Property oldProp = Property.GENERAL_SIMPLETIMER_THREADPOOL_SIZE;
-    Property prop = conf.resolve(Property.GENERAL_THREADPOOL_SIZE, oldProp);
+    Property prop = conf.resolve(Property.GENERAL_THREADPOOL_SIZE);","[{'comment': '```suggestion\r\n    Property prop = Property.GENERAL_THREADPOOL_SIZE;\r\n```', 'commenter': 'ctubbsii'}]"
3106,core/src/test/java/org/apache/accumulo/core/conf/AccumuloConfigurationTest.java,"@@ -394,42 +386,17 @@ public void testScanExecutors() {
 
   // note: this is hard to test if there aren't any deprecated properties
   // if that's the case, just comment this test out or create a dummy deprecated property
-  @SuppressWarnings(""deprecation"")
   @Test
   public void testResolveDeprecated() {
     var conf = new ConfigurationCopy();
 
-    // deprecated first argument
-    var e1 = assertThrows(IllegalArgumentException.class, () -> conf
-        .resolve(Property.INSTANCE_DFS_DIR, Property.INSTANCE_DFS_URI, Property.INSTANCE_DFS_URI));
-    assertEquals(""Unexpected deprecated INSTANCE_DFS_DIR"", e1.getMessage());
-","[{'comment': ""Some of these tests are testing behavior of the `resolve` method. We may want to leave some kind of stub in place of these. Without a deprecated property to test resolution with, we can't perform these tests... but we will probably want to perform them if/when we deprecate properties in the future."", 'commenter': 'ctubbsii'}, {'comment': 'I wonder if it is better to try and create some fake properties for the tests or something to check the resolve method vs leaving in stubs for non existent properties. Seems like if we want to check the resolve method we should create a test class specifically for that.', 'commenter': 'cshannon'}, {'comment': ""Ideally, we would want to create test properties that aren't mixed in with the other properties... so we don't have to filter them out everywhere we enumerate real properties. If the method can be made generic in some way, perhaps something like `public <T> T resolve(Enumeration<T> .... propEnums);`, then it might be easy to create a test enum class for testing the method. However, the method does depend on being able to read the properties from the AccumuloConfiguration to see if they are set there, so it's not immediately obvious to me how we'd make that work without making it super convoluted."", 'commenter': 'ctubbsii'}, {'comment': ""So I came up with the idea to take two properties (I just picked two random ones) and just mark them as deprecated just for the test using reflection and then restore them at the end. This way we didn't have to do any other magic and the test will always work going forward. It seemed like the simplest/cleanest solution in this case. It's a little hacky to have to use reflection to do this but I think it's ok in this case since it's just a unit test. Let me know what you think. I also fixed/updated my PR based on your other comments."", 'commenter': 'cshannon'}, {'comment': ""I should also add I considered trying to use mocking and use EasyMock to mock the enum values but enums are final so you can't use normal mocking with EasyMock. I haven't used it before but I think the only way it could work was something like PowerMock but I checked and it is only set up for Junit 4 and not Junit 5. So it seemed like this was a simpler solution than trying to figure out a way to use PowerMock/EasyMock as getting that set up seems out of scope of this PR and if we wanted to do that should be another Issue/PR."", 'commenter': 'cshannon'}, {'comment': 'clever! I like it', 'commenter': 'ctubbsii'}]"
3106,core/src/test/java/org/apache/accumulo/core/conf/AccumuloConfigurationTest.java,"@@ -394,42 +386,17 @@ public void testScanExecutors() {
 
   // note: this is hard to test if there aren't any deprecated properties
   // if that's the case, just comment this test out or create a dummy deprecated property
-  @SuppressWarnings(""deprecation"")
   @Test
   public void testResolveDeprecated() {
     var conf = new ConfigurationCopy();
 
-    // deprecated first argument
-    var e1 = assertThrows(IllegalArgumentException.class, () -> conf
-        .resolve(Property.INSTANCE_DFS_DIR, Property.INSTANCE_DFS_URI, Property.INSTANCE_DFS_URI));
-    assertEquals(""Unexpected deprecated INSTANCE_DFS_DIR"", e1.getMessage());
-
     // non-deprecated second argument
-    var e2 = assertThrows(IllegalArgumentException.class,
-        () -> conf.resolve(Property.INSTANCE_VOLUMES, Property.INSTANCE_DFS_DIR,
-            Property.INSTANCE_SECRET, Property.INSTANCE_DFS_DIR, Property.INSTANCE_VOLUMES));
+    var e2 = assertThrows(IllegalArgumentException.class, () -> conf
+        .resolve(Property.INSTANCE_VOLUMES, Property.INSTANCE_SECRET, Property.INSTANCE_VOLUMES));
     assertEquals(""Unexpected non-deprecated [INSTANCE_SECRET, INSTANCE_VOLUMES]"", e2.getMessage());
 
     // empty second argument always resolves to non-deprecated first argument
     assertSame(Property.INSTANCE_VOLUMES, conf.resolve(Property.INSTANCE_VOLUMES));
 
-    // none are set, resolve to non-deprecated
-    assertSame(Property.INSTANCE_VOLUMES, conf.resolve(Property.INSTANCE_VOLUMES,
-        Property.INSTANCE_DFS_DIR, Property.INSTANCE_DFS_URI));
-
-    // resolve to first deprecated argument that's set; here, it's the final one
-    conf.set(Property.INSTANCE_DFS_URI, """");
-    assertSame(Property.INSTANCE_DFS_URI, conf.resolve(Property.INSTANCE_VOLUMES,
-        Property.INSTANCE_DFS_DIR, Property.INSTANCE_DFS_URI));
-
-    // resolve to first deprecated argument that's set; now, it's the first one because both are set
-    conf.set(Property.INSTANCE_DFS_DIR, """");
-    assertSame(Property.INSTANCE_DFS_DIR, conf.resolve(Property.INSTANCE_VOLUMES,
-        Property.INSTANCE_DFS_DIR, Property.INSTANCE_DFS_URI));
-
-    // every property is set, so resolve to the non-deprecated one
-    conf.set(Property.INSTANCE_VOLUMES, """");
-    assertSame(Property.INSTANCE_VOLUMES, conf.resolve(Property.INSTANCE_VOLUMES,
-        Property.INSTANCE_DFS_DIR, Property.INSTANCE_DFS_URI));","[{'comment': ""More tests that can't be tested without a deprecated property, but check the correct behavior of the `resolve` method."", 'commenter': 'ctubbsii'}]"
3106,server/base/src/test/java/org/apache/accumulo/server/master/balancer/TableLoadBalancerTest.java,"@@ -146,9 +146,7 @@ public void test() {
     TableConfiguration conf = createMock(TableConfiguration.class);
     // Eclipse might show @SuppressWarnings(""removal"") as unnecessary.
     // Eclipse is wrong. See https://bugs.eclipse.org/bugs/show_bug.cgi?id=565271
-    @SuppressWarnings(""removal"")
-    Property TABLE_CLASSPATH = Property.TABLE_CLASSPATH;
-    expect(conf.resolve(Property.TABLE_CLASSLOADER_CONTEXT, TABLE_CLASSPATH))
+    expect(conf.resolve(Property.TABLE_CLASSLOADER_CONTEXT))","[{'comment': ""This would be a weird thing to expect. Probably can be removed when the code that we're expecting is updated to avoid the resolution of a single non-deprecated property, and just uses that property directly."", 'commenter': 'ctubbsii'}]"
3106,server/manager/src/main/java/org/apache/accumulo/manager/recovery/RecoveryManager.java,"@@ -198,11 +198,9 @@ public boolean recoverLogs(KeyExtent extent, Collection<Collection<String>> walo
         synchronized (this) {
           if (!closeTasksQueued.contains(sortId) && !sortsQueued.contains(sortId)) {
             AccumuloConfiguration aconf = manager.getConfiguration();
-            @SuppressWarnings(""deprecation"")
             LogCloser closer = Property.createInstanceFromPropertyName(aconf,
-                aconf.resolve(Property.MANAGER_WAL_CLOSER_IMPLEMENTATION,
-                    Property.MANAGER_WALOG_CLOSER_IMPLEMETATION),
-                LogCloser.class, new HadoopLogCloser());
+                aconf.resolve(Property.MANAGER_WAL_CLOSER_IMPLEMENTATION), LogCloser.class,","[{'comment': ""another resolve on single property can be simplified; there's a bunch more, but I'll stop mentioning it."", 'commenter': 'ctubbsii'}]"
3106,shell/src/main/java/org/apache/accumulo/shell/Shell.java,"@@ -472,27 +472,13 @@ public ClassLoader getClassLoader(final CommandLine cl, final Shell shellState)
   }
 
   private static String getTableContextFromProps(Map<String,String> props) {
-    String tableContext = null;
     for (Entry<String,String> entry : props.entrySet()) {
-      // look for either the old property or the new one, but
-      // if the new one is set, stop looking and let it take precedence
       if (entry.getKey().equals(Property.TABLE_CLASSLOADER_CONTEXT.getKey())
           && entry.getValue() != null && !entry.getValue().isEmpty()) {
         return entry.getValue();","[{'comment': ""Since we're only looking for a single property now, I don't think this needs to be in a loop. We should be able to just directly call `props.get(Property.TABLE_CLASSLOADER_CONTEXT.getKey())` and return it."", 'commenter': 'ctubbsii'}]"
3106,server/base/src/test/java/org/apache/accumulo/server/master/balancer/TableLoadBalancerTest.java,"@@ -146,10 +146,6 @@ public void test() {
     TableConfiguration conf = createMock(TableConfiguration.class);
     // Eclipse might show @SuppressWarnings(""removal"") as unnecessary.","[{'comment': 'Can we get rid of these comments also?', 'commenter': 'dlmarion'}]"
3106,server/base/src/main/java/org/apache/accumulo/server/util/ZooZap.java,"@@ -146,15 +146,6 @@ public void execute(String[] args) throws Exception {
         }
       }
 
-      // Remove the tracers, we don't use them anymore.
-      @SuppressWarnings(""deprecation"")
-      String path = siteConf.get(Property.TRACE_ZK_PATH);
-      try {
-        zapDirectory(zoo, path, opts);","[{'comment': ""Do we need to remove this as a final step in the upgrade from 2.1 -> 3.0? I didn't look to see if this is already done."", 'commenter': 'dlmarion'}, {'comment': ""> Do we need to remove this as a final step in the upgrade from 2.1 -> 3.0? I didn't look to see if this is already done.\r\n\r\nThis should already be done in 2.1. We don't need to keep this for 3.0 upgrade, but we could suggest users remove the `/tracer` directory themselves, or run the ZooZap util with the property still set (which may be done already on `bin/accumulo-cluster stop` already."", 'commenter': 'ctubbsii'}, {'comment': ""@ctubbsii - Is there a good place to log a warning for this? I don't see an Upgrader class yet for 3.0 so is that something we should create in another PR?"", 'commenter': 'cshannon'}, {'comment': ""I don't think we need a warning for this. I meant suggest it in the release notes. Accumulo no longer has to care about the existence of this directory. It should have attempted removal on upgrading to 2.1, and if it missed it, it's not a big deal."", 'commenter': 'ctubbsii'}]"
3106,server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer1/BulkImport.java,"@@ -207,10 +207,8 @@ public static String prepareBulkImport(ServerContext manager, final VolumeManage
     final UniqueNameAllocator namer = manager.getUniqueNameAllocator();
 
     AccumuloConfiguration serverConfig = manager.getConfiguration();
-    @SuppressWarnings(""deprecation"")
     ExecutorService workers = ThreadPools.getServerThreadPools().createExecutorService(serverConfig,
-        serverConfig.resolve(Property.MANAGER_RENAME_THREADS, Property.MANAGER_BULK_RENAME_THREADS),
-        false);
+        serverConfig.resolve(Property.MANAGER_RENAME_THREADS), false);","[{'comment': '```suggestion\r\n        Property.MANAGER_RENAME_THREADS, false);\r\n```', 'commenter': 'ctubbsii'}]"
3106,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -1685,9 +1685,7 @@ public void removeInUseLogs(Set<DfsLogger> candidates) {
   public void checkIfMinorCompactionNeededForLogs(List<DfsLogger> closedLogs) {
 
     // grab this outside of tablet lock.
-    @SuppressWarnings(""deprecation"")
-    Property prop = tableConfiguration.resolve(Property.TSERV_WAL_MAX_REFERENCED,
-        Property.TSERV_WALOG_MAX_REFERENCED, Property.TABLE_MINC_LOGS_MAX);
+    Property prop = tableConfiguration.resolve(Property.TSERV_WAL_MAX_REFERENCED);
     int maxLogs = tableConfiguration.getCount(prop);","[{'comment': ""This is another where there's only one property to resolve now. Can just inline the prop variable and get rid of the resolve call."", 'commenter': 'ctubbsii'}]"
3109,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -165,8 +165,10 @@ private void validateACLs(ServerContext context) {
 ","[{'comment': 'This does not need the stat.  Could use:\r\n```\r\n          final List<ACL> acls = zk.getACL(path, null);\r\n```', 'commenter': 'EdColeman'}, {'comment': 'Do we have a minimum ZooKeeper version that is supported for Accumulo 2.1.0. Stat could not be null before ZooKeeper 3.5.0 (https://issues.apache.org/jira/browse/ZOOKEEPER-1222)', 'commenter': 'dlmarion'}]"
3109,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -165,8 +165,10 @@ private void validateACLs(ServerContext context) {
 
           if (((path.equals(Constants.ZROOT) || path.equals(Constants.ZROOT + Constants.ZINSTANCES))
               && !acls.equals(ZooDefs.Ids.OPEN_ACL_UNSAFE))
-              || (!privateWithAuth.equals(acls) && !publicWithAuth.equals(acls))) {
-            log.error(""ZNode at {} has unexpected ACL: {}"", path, acls);
+              || (!acls.containsAll(privateWithAuth) && !acls.containsAll(publicWithAuth))) {","[{'comment': ""Do we really want to change from a set-equality to a set-inclusion check? If so, we won't log an error if the node's permissions are wide-open with extraneous unexpected permissions. I'm not sure that is the intent here. Should we do an order-independent set-equality check instead?"", 'commenter': 'ctubbsii'}, {'comment': 'User ran into an issue testing the 2.1 upgrade. Manual intervention was performed and ACLs were modified before I got involved. At the point I was involved user had set acls on the znodes manually using the instructions in the user guide. The order of the ACLs in the setAcl command appeared ""correct"", but this check was still failing. User applied this patch and the upgrade got past this step. So, either the order of the ACLs in the setACL commands was incorrect, or getAcl could return them in a different order. The objects being compared are Lists, so equals checks the order. This was an attempt to perform an order independent comparison without creating new Set objects from the Lists. If having other ACLs present is an issue, then I can modify to create Sets and call `equals`.', 'commenter': 'dlmarion'}, {'comment': ""I'm looking at adding an option in zoo-info-viewer that can dump the acls in zookeeper  that could be used before running the upgrade.\r\n\r\nI was leaning towards checking that our processes have cdrw permissions - either with the accumulo digest or via wide-open, world-anyone - that ensures that we have the permissions necessary to run but does not provide any security assurance.  \r\n\r\nOtherwise, other than a few nodes like /accumulo and /accumulo/instance I think all nodes under /accumulo/[IID]/tables could be protected with the accumulo digest and not allow any other zookeeper user id access - but that could impact any uses that have external utilities that access ZooKeeper (say to read FATE status) that might not have the accumulo digest set - they probably should, but they might have been okay with world-read.\r\n"", 'commenter': 'EdColeman'}, {'comment': 'IIRC from ZooKeeperInitializer:\r\n\r\nZROOT -> ZooDefs.Ids.OPEN_ACL_UNSAFE\r\nZROOT + ZINSTANCES -> ZooDefs.Ids.OPEN_ACL_UNSAFE\r\nZROOT/IID/config -> ZooUtil.PRIVATE\r\nEverything else has ZooUtil.PUBLIC', 'commenter': 'dlmarion'}, {'comment': ""Upon second thought, I think it's probably not a good idea to check set equality, as I said earlier. Users may have customized their ACLs after installation, and that should be allowed. We only need to check that the minimum permissions we need, that the Accumulo system user has full permissions on the nodes for this instance."", 'commenter': 'ctubbsii'}]"
3109,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -143,43 +141,58 @@ public void upgradeZookeeper(ServerContext context) {
     createScanServerNodes(context);
   }
 
+  private static String extractAuthName(ACL acl) {
+    Objects.requireNonNull(acl, ""provided ACL cannot be null"");
+    try {
+      return acl.getId().getId().trim().split("":"")[0];
+    } catch (Exception ex) {
+      log.debug(""Invalid ACL passed, cannot parse id from '{}'"", acl);
+      return """";
+    }
+  }
+
+  private static boolean canWrite(List<ACL> acls) {
+    if (ZooDefs.Ids.OPEN_ACL_UNSAFE.equals(acls)) {
+      return true;
+    }
+    ;","[{'comment': '```suggestion\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in def13d1.', 'commenter': 'dlmarion'}]"
3109,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -143,43 +141,58 @@ public void upgradeZookeeper(ServerContext context) {
     createScanServerNodes(context);
   }
 
+  private static String extractAuthName(ACL acl) {
+    Objects.requireNonNull(acl, ""provided ACL cannot be null"");
+    try {
+      return acl.getId().getId().trim().split("":"")[0];
+    } catch (Exception ex) {
+      log.debug(""Invalid ACL passed, cannot parse id from '{}'"", acl);
+      return """";
+    }
+  }
+
+  private static boolean canWrite(List<ACL> acls) {
+    if (ZooDefs.Ids.OPEN_ACL_UNSAFE.equals(acls)) {
+      return true;
+    }
+    ;
+    for (ACL acl : acls) {
+      String name = extractAuthName(acl);
+      if ((""accumulo"".equals(name) || ""anyone"".equals(name))
+          && acl.getPerms() >= ZooDefs.Perms.WRITE) {
+        return true;
+      }
+    }
+    return false;","[{'comment': 'This could be more readable with `Stream.findAny(Predicate)`. The whole method becomes:\r\n\r\n```java\r\n    // true if acls are open to all or if accumulo or anyone has WRITE permission\r\n    return ZooDefs.Ids.OPEN_ACL_UNSAFE.equals(acls) || acls.stream().findAny(acl ->\r\n      Set.of(""accumulo"", ""anyone"").contains(extractAuthName(acl)) && acl.getPerms() >= ZooDefs.Perms.WRITE));\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""Addressed in def13d1. `findAny` didn't compile, used `anyMatch` instead."", 'commenter': 'dlmarion'}]"
3109,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader9to10.java,"@@ -151,19 +152,9 @@ private static String extractAuthName(ACL acl) {
     }
   }
 
-  private static boolean canWrite(List<ACL> acls) {
-    if (ZooDefs.Ids.OPEN_ACL_UNSAFE.equals(acls)) {
-      return true;
-    }
-    ;
-    for (ACL acl : acls) {
-      String name = extractAuthName(acl);
-      if ((""accumulo"".equals(name) || ""anyone"".equals(name))
-          && acl.getPerms() >= ZooDefs.Perms.WRITE) {
-        return true;
-      }
-    }
-    return false;
+  private static boolean canWrite(final Set<String> users, final List<ACL> acls) {
+    return ZooDefs.Ids.OPEN_ACL_UNSAFE.equals(acls) || acls.stream()
+        .anyMatch(a -> users.contains(extractAuthName(a)) && a.getPerms() >= ZooDefs.Perms.WRITE);","[{'comment': 'The perms are bits - so `>= ZooDefs.Perms.WRITE` will return true if any combination of write, create, delete, admin is set - so if the perm was delete only, this would still return ""true"" for can write, which seems misleading at best.\r\n\r\nI think the test might be better names hasAll and the condition was:\r\n\r\n```\r\n .anyMatch(a -> users.contains(extractAuthName(a)) && a.getPerms() == ZooDefs.Perms.ALL;\r\n```', 'commenter': 'EdColeman'}]"
3118,core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooReader.java,"@@ -121,6 +123,36 @@ public boolean exists(String zPath, Watcher watcher)
     return getStatus(zPath, watcher) != null;
   }
 
+  /**
+   * read the ZooKeeper acls for a node.
+   */
+  public List<ACL> getAcls(String zPath, Stat stat) throws KeeperException, InterruptedException {
+    requireNonNull(stat); // required for ZooKeeper 3.4
+    return retryLoop(zk -> zk.getACL(zPath, stat));
+  }
+
+  public void recursiveAclRead(String parentPath, Map<String,List<ACL>> acls) {
+    try {
+
+      Stat dummy = new Stat(); // required form ZooKeeper 3.4 compatibility, unused here.
+      acls.put(parentPath, getAcls(parentPath, dummy));
+
+      for (String child : getChildren(parentPath)) {
+        String childPath = parentPath + ""/"" + child;
+        List<ACL> acl = getAcls(childPath, dummy);
+        acls.put(childPath, acl);
+        recursiveAclRead(childPath, acls);","[{'comment': 'You could alternatively use `ZKUtil.visitSubTreeDFS`.  See Upgrader9To10.validateACLs for an example.', 'commenter': 'dlmarion'}]"
3118,core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooAclUtil.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.fate.zookeeper;
+
+import static java.util.Objects.requireNonNull;
+
+import java.util.List;
+
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.data.ACL;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ZooAclUtil {
+
+  private static final Logger log = LoggerFactory.getLogger(ZooAclUtil.class);
+
+  /**
+   * Utility class - prevent instantiation.
+   */
+  private ZooAclUtil() {}
+
+  public static String extractAuthName(ACL acl) {
+    requireNonNull(acl, ""provided ACL cannot be null"");
+    try {
+      return acl.getId().getId().trim().split("":"")[0];
+    } catch (Exception ex) {
+      log.debug(""Invalid ACL passed, cannot parse id from '{}'"", acl);
+      return """";
+    }
+  }
+
+  /**
+   * translate the ZooKeeper ACL perm bits into a string. The output order is cdrwa (when all perms
+   * are set)
+   *
+   * Mirrors `org.apache.zookeeper.ZKUtil.getPermString()` added in more recent ZooKeeper versions.
+   */
+  public static String translateZooPerm(final int perm) {
+    if (perm == ZooDefs.Perms.ALL) {
+      return ""cdrwa"";
+    }
+    StringBuilder sb = new StringBuilder();
+    if ((perm & ZooDefs.Perms.CREATE) != 0) {
+      sb.append(""c"");
+    }
+    if ((perm & ZooDefs.Perms.DELETE) != 0) {
+      sb.append(""d"");
+    }
+    if ((perm & ZooDefs.Perms.READ) != 0) {
+      sb.append(""r"");
+    }
+    if ((perm & ZooDefs.Perms.WRITE) != 0) {
+      sb.append(""w"");
+    }
+    if ((perm & ZooDefs.Perms.ADMIN) != 0) {
+      sb.append(""a"");
+    }
+    if (sb.length() == 0) {
+      return ""invalid"";
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Process the ZooKeeper acls and return a structure that shows if the accumulo user has ALL
+   * (cdrwa) access and if any other user has anything other than read access.
+   * <ul>
+   * <li>Accumulo having access is expected - anything else needs checked
+   * <li>Anyone having more than read access is unexpected and should be checked.
+   * </ul>
+   *
+   * @param acls ZooKeeper ACLs for a node
+   * @return acl status with accumulo and other accesses.
+   */
+  public static ZkAccumuloAclStatus checkWritableAuth(List<ACL> acls) {
+
+    ZkAccumuloAclStatus result = new ZkAccumuloAclStatus();
+
+    for (ACL a : acls) {
+      String name = extractAuthName(a);
+      // check accumulo has or inherits all access
+      if (""accumulo"".equals(name) || ""anyone"".equals(name)) {
+        if (a.getPerms() == ZooDefs.Perms.ALL) {
+          result.setAccumuloHasFull();
+        }
+      }
+      // check if not accumulo and any perm bit other than read is set
+      if (!""accumulo"".equals(name) && ((a.getPerms() & ~ZooDefs.Perms.READ) != 0)) {","[{'comment': 'wouldn\'t you want to check that ""anyone"" matches `name`? There could be another user that has an ACL on this node.', 'commenter': 'dlmarion'}, {'comment': 'This is checking that if it is not the accumulo user. then if other users can do anything other than read - it could be the generic ZooKeeper `anyone` of maybe a custom other - if any user other than accumulo has an access other than read they ""could"" modify the node - so it sets the any may update flag - it was intended to be access by non-accumulo. anyone or any other zk user.', 'commenter': 'EdColeman'}, {'comment': 'ok, I misunderstood, maybe a better name would be `othersCanUpdate` ?', 'commenter': 'dlmarion'}, {'comment': 'use of `anyone` here implies `world` instead of a different user.', 'commenter': 'dlmarion'}]"
3118,core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooAclUtil.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.fate.zookeeper;
+
+import static java.util.Objects.requireNonNull;
+
+import java.util.List;
+
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.data.ACL;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ZooAclUtil {
+
+  private static final Logger log = LoggerFactory.getLogger(ZooAclUtil.class);
+
+  /**
+   * Utility class - prevent instantiation.
+   */
+  private ZooAclUtil() {}
+
+  public static String extractAuthName(ACL acl) {
+    requireNonNull(acl, ""provided ACL cannot be null"");
+    try {
+      return acl.getId().getId().trim().split("":"")[0];
+    } catch (Exception ex) {
+      log.debug(""Invalid ACL passed, cannot parse id from '{}'"", acl);
+      return """";
+    }
+  }
+
+  /**
+   * translate the ZooKeeper ACL perm bits into a string. The output order is cdrwa (when all perms
+   * are set)
+   *
+   * Mirrors `org.apache.zookeeper.ZKUtil.getPermString()` added in more recent ZooKeeper versions.
+   */
+  public static String translateZooPerm(final int perm) {
+    if (perm == ZooDefs.Perms.ALL) {
+      return ""cdrwa"";
+    }
+    StringBuilder sb = new StringBuilder();
+    if ((perm & ZooDefs.Perms.CREATE) != 0) {
+      sb.append(""c"");
+    }
+    if ((perm & ZooDefs.Perms.DELETE) != 0) {
+      sb.append(""d"");
+    }
+    if ((perm & ZooDefs.Perms.READ) != 0) {
+      sb.append(""r"");
+    }
+    if ((perm & ZooDefs.Perms.WRITE) != 0) {
+      sb.append(""w"");
+    }
+    if ((perm & ZooDefs.Perms.ADMIN) != 0) {
+      sb.append(""a"");
+    }
+    if (sb.length() == 0) {
+      return ""invalid"";
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Process the ZooKeeper acls and return a structure that shows if the accumulo user has ALL
+   * (cdrwa) access and if any other user has anything other than read access.
+   * <ul>
+   * <li>Accumulo having access is expected - anything else needs checked
+   * <li>Anyone having more than read access is unexpected and should be checked.
+   * </ul>
+   *
+   * @param acls ZooKeeper ACLs for a node
+   * @return acl status with accumulo and other accesses.
+   */
+  public static ZkAccumuloAclStatus checkWritableAuth(List<ACL> acls) {
+
+    ZkAccumuloAclStatus result = new ZkAccumuloAclStatus();
+
+    for (ACL a : acls) {
+      String name = extractAuthName(a);
+      // check accumulo has or inherits all access
+      if (""accumulo"".equals(name) || ""anyone"".equals(name)) {
+        if (a.getPerms() == ZooDefs.Perms.ALL) {
+          result.setAccumuloHasFull();
+        }
+      }
+      // check if not accumulo and any perm bit other than read is set
+      if (!""accumulo"".equals(name) && ((a.getPerms() & ~ZooDefs.Perms.READ) != 0)) {
+        result.setAnyMayUpdate();
+      }
+      // check if not accumulo and any read perm is set
+      if (!""accumulo"".equals(name) && ((a.getPerms() & ZooDefs.Perms.READ) != 0)) {","[{'comment': 'wouldn\'t you want to check that ""anyone"" matches name? There could be another user that has an ACL on this node.', 'commenter': 'dlmarion'}, {'comment': 'Similar to the comment above - the intent is to flag all user read access that is not the Accumulo user - the generic ZooKeeper `anyone` or any other ZooKeeper defined user.', 'commenter': 'EdColeman'}]"
3118,core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooAclUtilTest.java,"@@ -0,0 +1,87 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.fate.zookeeper;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertThrows;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.List;
+
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.data.ACL;
+import org.apache.zookeeper.data.Id;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+class ZooAclUtilTest {
+
+  private static final Logger log = LoggerFactory.getLogger(ZooAclUtilTest.class);
+
+  @Test
+  public void extractAuthNameTest() {
+    assertThrows(NullPointerException.class, () -> ZooAclUtil.extractAuthName(null));
+    assertEquals("""",
+        ZooAclUtil.extractAuthName(new ACL(ZooDefs.Perms.ALL, new Id(""digest"", null))));
+    assertEquals(""accumulo"", ZooAclUtil
+        .extractAuthName(new ACL(ZooDefs.Perms.ALL, new Id(""digest"", ""accumulo:abcd123""))));
+    assertEquals(""noauth"",
+        ZooAclUtil.extractAuthName(new ACL(ZooDefs.Perms.ALL, new Id(""digest"", ""noauth""))));
+    assertEquals(""noscheme"",
+        ZooAclUtil.extractAuthName(new ACL(ZooDefs.Perms.ALL, new Id("""", ""noscheme""))));
+    assertEquals("""", ZooAclUtil.extractAuthName(new ACL(ZooDefs.Perms.ALL, new Id(""digest"", """"))));
+  }
+
+  @Test
+  public void translateZooPermTest() {
+    assertEquals(""invalid"", ZooAclUtil.translateZooPerm(0));
+    assertEquals(""cdrwa"", ZooAclUtil.translateZooPerm(ZooDefs.Perms.ALL));
+    assertEquals(""drw"", ZooAclUtil
+        .translateZooPerm(ZooDefs.Perms.DELETE | ZooDefs.Perms.READ | ZooDefs.Perms.WRITE));
+    assertEquals(""ca"", ZooAclUtil.translateZooPerm(ZooDefs.Perms.CREATE | ZooDefs.Perms.ADMIN));
+  }
+
+  @Test
+  public void checkWritableAuthTest() {
+    ACL a1 = new ACL(ZooDefs.Perms.ALL, new Id(""digest"", ""accumulo:abcd123""));
+    ZooAclUtil.ZkAccumuloAclStatus status = ZooAclUtil.checkWritableAuth(List.of(a1));
+    assertTrue(status.accumuloHasFull());
+    assertFalse(status.anyMayUpdate());
+    assertFalse(status.anyCanRead());
+
+    ACL a2 = new ACL(ZooDefs.Perms.ALL, new Id(""digest"", ""someone:abcd123""));
+    status = ZooAclUtil.checkWritableAuth(List.of(a2));
+    assertFalse(status.accumuloHasFull());
+    assertTrue(status.anyMayUpdate());","[{'comment': ""`anyone` can't update this znode, only `someone` can, right?"", 'commenter': 'dlmarion'}, {'comment': 'That is correct - only `someone` has permissions - this is similar to the issue seen during an upgrade where a user other than Accumulo or `anyone` had control of the node.  Accumulo could not change the node, so the upgrade failed)  So, from an Accumulo concentric view, it does not have sufficient permission to upgrade - and from a protection viewpoint someone other than Accumulo can modify the node.  So the test is exercising that option to validate it is handled correctly.', 'commenter': 'EdColeman'}, {'comment': 'I think the method name is confusing. It says ""anyMayUpdate"", but it\'s not true that anyone can update... only someone (other than accumulo) can. It would be better if the names were changed to be more intuitive. Maybe `otherMayUpdate` or similar.', 'commenter': 'ctubbsii'}, {'comment': 'changed in e2c87e0815', 'commenter': 'EdColeman'}]"
3118,core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooAclUtil.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.fate.zookeeper;","[{'comment': ""I don't think this belongs in the fate sub-package."", 'commenter': 'ctubbsii'}, {'comment': 'It seems most closely related to ZooUtil - which is currently in ../fate/zookeeper  It seems like they should be located together.  Looking ahead, this class, could also provide the check used during upgrades.', 'commenter': 'EdColeman'}, {'comment': ""The location of ZooUtil is incidental, because it was originally in a separate fate jar. It was not done with conscious planning. A lot of ZooKeeper utils are disorganized, and that's something I've been trying to nudge us towards improvements over the last few versions. We actually used to have two ZooUtil classes, in separate packages, and I was able to get rid of one in 2.1. So, we can either leave it here because that's where other utility code for ZK is located (not my preference)... or we can try to create a better precedent by not coupling it to FaTE when it's not related to FaTE (my preference) so we can be better organized with our utility code. The server-base module has a `zookeeper` package for ZK utilities. Given this is intended to support a server-side CLI utility, that seems to make the most sense to me. `org.apache.accumulo.core.util` is also another place, though it's mostly a big dumping ground for anything that doesn't fit anywhere else, same with the server-base module's util package.\r\n\r\nI don't feel strongly about this, but I think code organization is something we should at least put thought into as we maintain the project."", 'commenter': 'ctubbsii'}, {'comment': 'Moved to server-base in 34acd67757', 'commenter': 'EdColeman'}]"
3118,core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooAclUtil.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.fate.zookeeper;
+
+import static java.util.Objects.requireNonNull;
+
+import java.util.List;
+
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.data.ACL;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ZooAclUtil {
+
+  private static final Logger log = LoggerFactory.getLogger(ZooAclUtil.class);
+
+  /**
+   * Utility class - prevent instantiation.
+   */
+  private ZooAclUtil() {}
+
+  public static String extractAuthName(ACL acl) {
+    requireNonNull(acl, ""provided ACL cannot be null"");
+    try {
+      return acl.getId().getId().trim().split("":"")[0];
+    } catch (Exception ex) {
+      log.debug(""Invalid ACL passed, cannot parse id from '{}'"", acl);
+      return """";
+    }
+  }
+
+  /**
+   * translate the ZooKeeper ACL perm bits into a string. The output order is cdrwa (when all perms
+   * are set)
+   *
+   * Mirrors `org.apache.zookeeper.ZKUtil.getPermString()` added in more recent ZooKeeper versions.
+   */
+  public static String translateZooPerm(final int perm) {
+    if (perm == ZooDefs.Perms.ALL) {
+      return ""cdrwa"";
+    }
+    StringBuilder sb = new StringBuilder();
+    if ((perm & ZooDefs.Perms.CREATE) != 0) {
+      sb.append(""c"");
+    }
+    if ((perm & ZooDefs.Perms.DELETE) != 0) {
+      sb.append(""d"");
+    }
+    if ((perm & ZooDefs.Perms.READ) != 0) {
+      sb.append(""r"");
+    }
+    if ((perm & ZooDefs.Perms.WRITE) != 0) {
+      sb.append(""w"");
+    }
+    if ((perm & ZooDefs.Perms.ADMIN) != 0) {
+      sb.append(""a"");
+    }
+    if (sb.length() == 0) {
+      return ""invalid"";
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Process the ZooKeeper acls and return a structure that shows if the accumulo user has ALL
+   * (cdrwa) access and if any other user has anything other than read access.
+   * <ul>
+   * <li>Accumulo having access is expected - anything else needs checked
+   * <li>Anyone having more than read access is unexpected and should be checked.
+   * </ul>
+   *
+   * @param acls ZooKeeper ACLs for a node
+   * @return acl status with accumulo and other accesses.
+   */
+  public static ZkAccumuloAclStatus checkWritableAuth(List<ACL> acls) {
+
+    ZkAccumuloAclStatus result = new ZkAccumuloAclStatus();
+
+    for (ACL a : acls) {
+      String name = extractAuthName(a);
+      // check accumulo has or inherits all access
+      if (""accumulo"".equals(name) || ""anyone"".equals(name)) {","[{'comment': 'Is ""anyone"" a special keyword for ZooKeeper ACLs?', 'commenter': 'ctubbsii'}, {'comment': '""anyone"" is how ZooKeeper prints out the ACLs\r\n\r\n```\r\nzk> getAcl /accumulo/2a7aab5f-fca0-443d-b41e-768d3b9ca5dd \r\n\'digest,\'accumulo:X1234=\r\n: cdrwa\r\n\'world,\'anyone\r\n: r\r\n\r\n```\r\n', 'commenter': 'EdColeman'}]"
3118,core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooAclUtilTest.java,"@@ -0,0 +1,87 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.fate.zookeeper;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertThrows;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.List;
+
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.data.ACL;
+import org.apache.zookeeper.data.Id;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+class ZooAclUtilTest {","[{'comment': 'Is JUnit okay with the test class not being public?', 'commenter': 'ctubbsii'}, {'comment': ""Changed in e2c87e0815 - but junit didn't seem to care."", 'commenter': 'EdColeman'}]"
3124,server/tserver/src/main/java/org/apache/accumulo/tserver/ThriftScanClientHandler.java,"@@ -147,34 +147,40 @@ public void close() {}
     };
     return this.startScan(tinfo, credentials, extent, range, columns, batchSize, ssiList, ssio,
         authorizations, waitForWrites, isolated, readaheadThreshold, tSamplerConfig, batchTimeOut,
-        contextArg, executionHints, resolver, busyTimeout);
+        contextArg, executionHints, resolver, busyTimeout, userData);
   }
 
   public InitialScan startScan(TInfo tinfo, TCredentials credentials, KeyExtent extent,
       TRange range, List<TColumn> columns, int batchSize, List<IterInfo> ssiList,
       Map<String,Map<String,String>> ssio, List<ByteBuffer> authorizations, boolean waitForWrites,
       boolean isolated, long readaheadThreshold, TSamplerConfiguration tSamplerConfig,
       long batchTimeOut, String contextArg, Map<String,String> executionHints,
-      ScanSession.TabletResolver tabletResolver, long busyTimeout) throws NotServingTabletException,
-      ThriftSecurityException, org.apache.accumulo.core.tabletserver.thrift.TooManyFilesException,
+      ScanSession.TabletResolver tabletResolver, long busyTimeout, String userData)
+      throws NotServingTabletException, ThriftSecurityException,
+      org.apache.accumulo.core.tabletserver.thrift.TooManyFilesException,
       TSampleNotPresentException, ScanServerBusyException {
 
+    log.debug(""({}) starting scan"", userData);
+
     server.getScanMetrics().incrementStartScan(1.0D);
 
     TableId tableId = extent.tableId();
     NamespaceId namespaceId;
     try {
       namespaceId = server.getContext().getNamespaceId(tableId);
     } catch (TableNotFoundException e1) {
+      log.error(""({}) not serving tablet {}"", userData, extent.toThrift());","[{'comment': 'Thinking this would be better logged at debug or trace.  Tables being concurrently deleted while a scan is attempted can be a normal course of business.', 'commenter': 'keith-turner'}]"
3124,server/tserver/src/main/java/org/apache/accumulo/tserver/ThriftScanClientHandler.java,"@@ -147,34 +147,40 @@ public void close() {}
     };
     return this.startScan(tinfo, credentials, extent, range, columns, batchSize, ssiList, ssio,
         authorizations, waitForWrites, isolated, readaheadThreshold, tSamplerConfig, batchTimeOut,
-        contextArg, executionHints, resolver, busyTimeout);
+        contextArg, executionHints, resolver, busyTimeout, userData);
   }
 
   public InitialScan startScan(TInfo tinfo, TCredentials credentials, KeyExtent extent,
       TRange range, List<TColumn> columns, int batchSize, List<IterInfo> ssiList,
       Map<String,Map<String,String>> ssio, List<ByteBuffer> authorizations, boolean waitForWrites,
       boolean isolated, long readaheadThreshold, TSamplerConfiguration tSamplerConfig,
       long batchTimeOut, String contextArg, Map<String,String> executionHints,
-      ScanSession.TabletResolver tabletResolver, long busyTimeout) throws NotServingTabletException,
-      ThriftSecurityException, org.apache.accumulo.core.tabletserver.thrift.TooManyFilesException,
+      ScanSession.TabletResolver tabletResolver, long busyTimeout, String userData)
+      throws NotServingTabletException, ThriftSecurityException,
+      org.apache.accumulo.core.tabletserver.thrift.TooManyFilesException,
       TSampleNotPresentException, ScanServerBusyException {
 
+    log.debug(""({}) starting scan"", userData);
+
     server.getScanMetrics().incrementStartScan(1.0D);
 
     TableId tableId = extent.tableId();
     NamespaceId namespaceId;
     try {
       namespaceId = server.getContext().getNamespaceId(tableId);
     } catch (TableNotFoundException e1) {
+      log.error(""({}) not serving tablet {}"", userData, extent.toThrift());
       throw new NotServingTabletException(extent.toThrift());
     }
     if (!security.canScan(credentials, tableId, namespaceId, range, columns, ssiList, ssio,
         authorizations)) {
+      log.error(""({}) permission denied"", userData);","[{'comment': 'I am wondering if this duplicates what the audit logging is doing.  If so may be better to pass the user data to the audit logging.  Also, not sure it should be logged at error.', 'commenter': 'keith-turner'}]"
3124,server/tserver/src/main/java/org/apache/accumulo/tserver/ThriftScanClientHandler.java,"@@ -195,6 +201,7 @@ public InitialScan startScan(TInfo tinfo, TCredentials credentials, KeyExtent ex
 
     TabletBase tablet = tabletResolver.getTablet(extent);
     if (tablet == null) {
+      log.error(""({}) not serving tablet {}"", userData, extent.toThrift());","[{'comment': 'Thinking this should be logged at debug or trace, its not unexpected that tablets move around. ', 'commenter': 'keith-turner'}]"
3124,server/tserver/src/main/java/org/apache/accumulo/tserver/ThriftScanClientHandler.java,"@@ -147,34 +147,40 @@ public void close() {}
     };
     return this.startScan(tinfo, credentials, extent, range, columns, batchSize, ssiList, ssio,
         authorizations, waitForWrites, isolated, readaheadThreshold, tSamplerConfig, batchTimeOut,
-        contextArg, executionHints, resolver, busyTimeout);
+        contextArg, executionHints, resolver, busyTimeout, userData);
   }
 
   public InitialScan startScan(TInfo tinfo, TCredentials credentials, KeyExtent extent,
       TRange range, List<TColumn> columns, int batchSize, List<IterInfo> ssiList,
       Map<String,Map<String,String>> ssio, List<ByteBuffer> authorizations, boolean waitForWrites,
       boolean isolated, long readaheadThreshold, TSamplerConfiguration tSamplerConfig,
       long batchTimeOut, String contextArg, Map<String,String> executionHints,
-      ScanSession.TabletResolver tabletResolver, long busyTimeout) throws NotServingTabletException,
-      ThriftSecurityException, org.apache.accumulo.core.tabletserver.thrift.TooManyFilesException,
+      ScanSession.TabletResolver tabletResolver, long busyTimeout, String userData)
+      throws NotServingTabletException, ThriftSecurityException,
+      org.apache.accumulo.core.tabletserver.thrift.TooManyFilesException,
       TSampleNotPresentException, ScanServerBusyException {
 
+    log.debug(""({}) starting scan"", userData);","[{'comment': 'Per scan session logging is very granular.  I feel like it should be trace instead of debug.  I suspect this could contribute to a large percentage of the debug logs.', 'commenter': 'keith-turner'}]"
3124,core/src/main/java/org/apache/accumulo/core/client/ScannerBase.java,"@@ -390,4 +390,21 @@ default Stream<Entry<Key,Value>> stream() {
     return StreamSupport.stream(this.spliterator(), false);
   }
 
+  /**
+   * Set user data on the Scanner. This data will be added to server side information and logs for
+   * correlation.
+   *
+   * @param userData meaningful data that can be used to correlate server side information
+   * @since 3.0.0
+   */
+  void setUserData(String userData);","[{'comment': 'Would calling this a ""tag"" help? Also, the javadoc should mention that this should be unique for each scan, in order to be useful.', 'commenter': 'ctubbsii'}, {'comment': '""tag"" may not be the best choice - in the terms of metrics, tag has a connotation of creating a time-series. Not sure if using tag in this context would lead to that conclusion, but wanted to point it out for discussion.', 'commenter': 'EdColeman'}, {'comment': 'Metrics wasn\'t my first thought at all. The term is generically used to refer to any kind of auxiliary descriptive mechanism that enables automated processing. So, it\'s used in a lot of similar contexts. Even though the term is a generic one, I was thinking it might be more intuitive than the less familiar ""user data"" which, unlike ""tag"", doesn\'t intuitively lead one to think about auxiliary descriptive information for collating. (NIST definition is https://csrc.nist.gov/glossary/term/data_tag)', 'commenter': 'ctubbsii'}, {'comment': 'Initially I was thinking about adding this to the MDC, but not all frameworks work with that.\r\n', 'commenter': 'dlmarion'}, {'comment': 'It looks like `correlationID` is a common term, with tracing systems using the term `traceId` as their correlationID. See: https://microsoft.github.io/code-with-engineering-playbook/observability/correlation-id/. Could rename to `scanCorrelationId`.', 'commenter': 'dlmarion'}, {'comment': 'I like `setCorrelationId(String correlationId)`.  Maybe `scan` is not needed in the name since its being set on a scanner.  ', 'commenter': 'keith-turner'}, {'comment': '> Maybe scan is not needed\r\n\r\nMakes sense. It would allow us to use the same name (correlationId) on other client objects, like the BatchWriter, if we wanted to add the same feature there.', 'commenter': 'dlmarion'}, {'comment': 'I renamed to correlationId in 84b9cbe', 'commenter': 'dlmarion'}]"
3124,assemble/conf/log4j2-service.properties,"@@ -72,6 +72,21 @@ appender.monitor.name = MonitorLog
 appender.monitor.filter.threshold.type = ThresholdFilter
 appender.monitor.filter.threshold.level = warn
 
+#appender.userdata.type = RollingFile
+#appender.userdata.name = UserDataLogFiles
+#appender.userdata.fileName = ${filename}.scan-user-data
+#appender.userdata.filePattern = ${filename}-%d{yyyy-MM-dd}-%i.scan-user-data.gz
+#appender.userdata.layout.type = PatternLayout
+#appender.userdata.layout.pattern = %d{ISO8601} [%-8c{2}] %-5p: %m%n
+#appender.userdata.policies.type = Policies
+#appender.userdata.policies.time.type = TimeBasedTriggeringPolicy
+#appender.userdata.policies.time.interval = 1
+#appender.userdata.policies.time.modulate = true
+#appender.userdata.policies.size.type = SizeBasedTriggeringPolicy
+#appender.userdata.policies.size.size=512MB
+#appender.userdata.strategy.type = DefaultRolloverStrategy
+#appender.userdata.strategy.max = 10
+","[{'comment': ""In order to keep our example config file smaller, I don't think we need to create a new appender for these. We can just include them by default in the regular logs."", 'commenter': 'ctubbsii'}, {'comment': 'removed this.', 'commenter': 'dlmarion'}]"
3124,assemble/conf/log4j2-service.properties,"@@ -84,6 +99,11 @@ logger.accumulo.level = debug
 #logger.audit.additivity = false
 #logger.audit.appenderRef.audit.ref = AuditLogFiles
 
+logger.userdata.name = org.apache.accumulo.core.logging.ScanUserDataLogger
+logger.userdata.level = off","[{'comment': ""If we don't want to use a class name, we can use a logger name like `org.apache.accumulo.userdata` (or `org.apache.accumulo.tags`), similar to what we did with the audit logs. We still include `org.apache.accumulo.` as a namespace prefix, but we don't need to use an actual class name for the logger name. It can be any meaningful name."", 'commenter': 'ctubbsii'}]"
3124,core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java,"@@ -118,7 +118,7 @@ public static boolean getBatchFromServer(ClientContext context, Range range, Key
         ScanState scanState = new ScanState(context, extent.tableId(), authorizations, range,
             fetchedColumns, size, serverSideIteratorList, serverSideIteratorOptions, false,
             Constants.SCANNER_DEFAULT_READAHEAD_THRESHOLD, null, batchTimeOut, classLoaderContext,
-            null, false);
+            null, false, null);","[{'comment': 'Should we use an empty string, or some default data for here?', 'commenter': 'ctubbsii'}]"
3124,core/src/main/java/org/apache/accumulo/core/logging/ScanUserDataLogger.java,"@@ -0,0 +1,169 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.logging;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.slf4j.Marker;
+import org.slf4j.event.Level;
+import org.slf4j.helpers.AbstractLogger;
+import org.slf4j.spi.LoggingEventBuilder;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+public class ScanUserDataLogger extends AbstractLogger {","[{'comment': ""Until this PR, we've only ever depended upon `org.slf4j.Logger` and `org.slf4j.LoggerFactory` from the SLF4J API. This has kept us in a very stable position across SLF4J versions. This PR would depend upon other classes in the SLF4J API, that I'm not sure are as stable. Given the importance of our use of this dependency being widely compatible, because of its popularity as a transitive dependency from a lot of our direct dependencies, I think we'd want to try to find some other way to do this, without adding so much dependency on other SLF4J APIs beyond the main two classes that we use today."", 'commenter': 'ctubbsii'}, {'comment': 'Alternate implementation?\r\n\r\n```\r\npublic class ScanUserDataLogger {\r\n\r\n  private static final Logger LOG = LoggerFactory.getLogger(ScanUserDataLogger.class);\r\n  private static final String FORMAT = ""(%s) "";\r\n\r\n  /**\r\n   * Utility method that will log the msg and args to the calling class\' logger (classLogger)\r\n   * if the classLogger argument is supplied. The classLogger argument would be null in the case\r\n   * where we don\'t want to log the same thing twice.\r\n   * \r\n   * @param level slf4j log level\r\n   * @param classLogger calling class\' slf4j logger\r\n   * @param userData scan userData value\r\n   * @param msg log message\r\n   * @param args log message argument\r\n   */\r\n  public static void log(Level level, Logger classLogger, String userData, String msg, Object... args) {\r\n    if (classLogger != null && classLogger.isEnabledForLevel(level)) {\r\n      classLogger.atLevel(level).log(msg, args);\r\n    }\r\n    if (LOG.isEnabledForLevel(level)) {\r\n      LOG.atLevel(level).log(String.format(FORMAT, userData), args);\r\n    }\r\n  }\r\n```', 'commenter': 'dlmarion'}, {'comment': ""Elsewhere you had asked:\r\n\r\n> Why are we passing null for the logger? Under what circumstances would we want to pass null vs. something else?\r\n\r\nI was thinking that in most cases users would want to redirect this logging to a different log file to provide a more condensed view of scan related logs. Therefore, I opted to print an additional log message instead of updating the current log messages. So, in the cases where I'm adding a new log message, the classLogger argument would be null. In the case where I'm reusing an existing message, then classLogger is not null.\r\n\r\nI'm open to ideas here."", 'commenter': 'dlmarion'}, {'comment': ""That's a lot simpler, but I don't think it's as elegant as what @keith-turner did for `core/src/main/java/org/apache/accumulo/core/logging/TabletLogger.java`. I think it would be better to model it after that."", 'commenter': 'ctubbsii'}, {'comment': ""I just want to prefix any msg with the userData. I don't see how creating a new method for each and every msg I want to log makes sense."", 'commenter': 'dlmarion'}, {'comment': ""> I just want to prefix any msg with the userData. I don't see how creating a new method for each and every msg I want to log makes sense.\r\n\r\nI think what @ctubbsii  suggested could accomplish your goal, but would be out of scope for this PR.  It would need to be its own PR, a PR to gather all logging related to the lifecycle (client and server)  of scans into one place in the code.  The static methods are not the goal in TabletLogger.java, but one way of achieving some goals.  That code has the following two goals.\r\n\r\n * Use logical names for logging related to functionality instead of class names.  For example could use `org.apache.accumulo.scan` for all scan related logging, even if that functionality exist across many classes. Then a user could set org.apache.accumulo.scan to TRACE and redirect it to its own file to gather very detailed trace logging related to scans without overwhelming the general debug logs.\r\n * Gather all the code related to logging for specific functionality that user may interested in to one place in the code.  Then a user who wants to parse and search log messages can look at this one package in Accumulo and see what log messages are available to parse."", 'commenter': 'keith-turner'}]"
3124,core/src/main/java/org/apache/accumulo/core/logging/ScanUserDataLogger.java,"@@ -0,0 +1,169 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.logging;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.slf4j.Marker;
+import org.slf4j.event.Level;
+import org.slf4j.helpers.AbstractLogger;
+import org.slf4j.spi.LoggingEventBuilder;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+public class ScanUserDataLogger extends AbstractLogger {
+
+  private static final long serialVersionUID = 1L;
+  private static final Logger LOG = LoggerFactory.getLogger(ScanUserDataLogger.class);
+  private static final ThreadLocal<String> USER_DATA = new ThreadLocal<>();
+
+  public static void logTrace(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.trace(msg, objects);
+    }
+    if (!LOG.isTraceEnabled()) {
+      return;
+    }
+    USER_DATA.set(userData);
+    LOG.trace(msg, objects);
+  }
+
+  public static void logDebug(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.debug(msg, objects);
+    }
+    if (!LOG.isDebugEnabled()) {
+      return;
+    }
+    USER_DATA.set(userData);
+    LOG.debug(msg, objects);
+  }
+
+  public static void logInfo(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.info(msg, objects);
+    }
+    if (!LOG.isInfoEnabled()) {
+      return;
+    }
+    USER_DATA.set(userData);
+    LOG.info(msg, objects);
+  }
+
+  public static void logWarn(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.warn(msg, objects);
+    }
+    if (!LOG.isWarnEnabled()) {
+      return;
+    }
+    USER_DATA.set(userData);
+    LOG.warn(msg, objects);
+  }
+
+  public static void logError(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.error(msg, objects);
+    }
+    USER_DATA.set(userData);
+    LOG.error(msg, objects);
+  }
+
+  @Override
+  public boolean isTraceEnabled() {
+    return LOG.isTraceEnabled();
+  }
+
+  @Override
+  public boolean isTraceEnabled(Marker marker) {
+    return LOG.isTraceEnabled(marker);
+  }
+
+  @Override
+  public boolean isDebugEnabled() {
+    return LOG.isDebugEnabled();
+  }
+
+  @Override
+  public boolean isDebugEnabled(Marker marker) {
+    return LOG.isDebugEnabled(marker);
+  }
+
+  @Override
+  public boolean isInfoEnabled() {
+    return LOG.isInfoEnabled();
+  }
+
+  @Override
+  public boolean isInfoEnabled(Marker marker) {
+    return LOG.isInfoEnabled(marker);
+  }
+
+  @Override
+  public boolean isWarnEnabled() {
+    return LOG.isWarnEnabled();
+  }
+
+  @Override
+  public boolean isWarnEnabled(Marker marker) {
+    return LOG.isWarnEnabled(marker);
+  }
+
+  @Override
+  public boolean isErrorEnabled() {
+    return LOG.isErrorEnabled();
+  }
+
+  @Override
+  public boolean isErrorEnabled(Marker marker) {
+    return LOG.isErrorEnabled(marker);
+  }
+
+  @Override
+  protected String getFullyQualifiedCallerName() {
+    return null;
+  }
+
+  @Override
+  @SuppressFBWarnings(value = ""RV_RETURN_VALUE_IGNORED"",
+      justification = ""Return value is self, using fluent object in non-fluent manner"")","[{'comment': 'I\'m assuming the warning being suppressed here is the builder, whose methods return `this`.\r\n\r\nThis could be worded slightly better:\r\n```suggestion\r\n  @SuppressFBWarnings(value = ""RV_RETURN_VALUE_IGNORED"",\r\n      justification = ""fluent builder methods return itself, which we don\'t need to use"")\r\n```\r\n\r\nBut, given that the implementation could change to have immutable intermediates rather than return `this`, it would probably be better to just use the builder API as intended.', 'commenter': 'ctubbsii'}]"
3124,core/src/main/java/org/apache/accumulo/core/logging/ScanUserDataLogger.java,"@@ -0,0 +1,169 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.logging;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.slf4j.Marker;
+import org.slf4j.event.Level;
+import org.slf4j.helpers.AbstractLogger;
+import org.slf4j.spi.LoggingEventBuilder;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+public class ScanUserDataLogger extends AbstractLogger {
+
+  private static final long serialVersionUID = 1L;
+  private static final Logger LOG = LoggerFactory.getLogger(ScanUserDataLogger.class);
+  private static final ThreadLocal<String> USER_DATA = new ThreadLocal<>();
+
+  public static void logTrace(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.trace(msg, objects);
+    }
+    if (!LOG.isTraceEnabled()) {
+      return;
+    }
+    USER_DATA.set(userData);
+    LOG.trace(msg, objects);
+  }
+
+  public static void logDebug(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.debug(msg, objects);
+    }
+    if (!LOG.isDebugEnabled()) {
+      return;
+    }
+    USER_DATA.set(userData);
+    LOG.debug(msg, objects);
+  }
+
+  public static void logInfo(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.info(msg, objects);
+    }
+    if (!LOG.isInfoEnabled()) {
+      return;
+    }
+    USER_DATA.set(userData);
+    LOG.info(msg, objects);
+  }
+
+  public static void logWarn(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.warn(msg, objects);
+    }
+    if (!LOG.isWarnEnabled()) {
+      return;
+    }
+    USER_DATA.set(userData);
+    LOG.warn(msg, objects);
+  }
+
+  public static void logError(Logger classLogger, String userData, String msg, Object... objects) {
+    if (classLogger != null) {
+      classLogger.error(msg, objects);
+    }
+    USER_DATA.set(userData);
+    LOG.error(msg, objects);
+  }
+
+  @Override
+  public boolean isTraceEnabled() {
+    return LOG.isTraceEnabled();
+  }
+
+  @Override
+  public boolean isTraceEnabled(Marker marker) {
+    return LOG.isTraceEnabled(marker);
+  }
+
+  @Override
+  public boolean isDebugEnabled() {
+    return LOG.isDebugEnabled();
+  }
+
+  @Override
+  public boolean isDebugEnabled(Marker marker) {
+    return LOG.isDebugEnabled(marker);
+  }
+
+  @Override
+  public boolean isInfoEnabled() {
+    return LOG.isInfoEnabled();
+  }
+
+  @Override
+  public boolean isInfoEnabled(Marker marker) {
+    return LOG.isInfoEnabled(marker);
+  }
+
+  @Override
+  public boolean isWarnEnabled() {
+    return LOG.isWarnEnabled();
+  }
+
+  @Override
+  public boolean isWarnEnabled(Marker marker) {
+    return LOG.isWarnEnabled(marker);
+  }
+
+  @Override
+  public boolean isErrorEnabled() {
+    return LOG.isErrorEnabled();
+  }
+
+  @Override
+  public boolean isErrorEnabled(Marker marker) {
+    return LOG.isErrorEnabled(marker);
+  }
+
+  @Override
+  protected String getFullyQualifiedCallerName() {
+    return null;
+  }
+
+  @Override
+  @SuppressFBWarnings(value = ""RV_RETURN_VALUE_IGNORED"",
+      justification = ""Return value is self, using fluent object in non-fluent manner"")
+  protected void handleNormalizedLoggingCall(Level level, Marker marker, String messagePattern,
+      Object[] arguments, Throwable throwable) {
+
+    final String userData = USER_DATA.get();
+    USER_DATA.set(null);
+
+    LoggingEventBuilder builder = LOG.atLevel(level);
+    if (marker != null) {
+      builder.addMarker(marker);
+    }
+    if (throwable != null) {
+      builder.setCause(throwable);
+    }
+    builder.setMessage(""({}) "" + messagePattern);
+    builder.addArgument(userData);
+    if (arguments != null) {
+      for (Object arg : arguments) {
+        builder.addArgument(arg);
+      }
+    }","[{'comment': ""It's not clear the message pattern this is intending to construct. A simple example in inline comments could help with debugging this."", 'commenter': 'ctubbsii'}]"
3124,core/src/main/java/org/apache/accumulo/core/spi/scan/HintScanPrioritizer.java,"@@ -80,6 +81,8 @@ private static int getPriority(ScanInfo si, int defaultPriority, HintProblemActi
     String prio = si.getExecutionHints().get(""priority"");
     if (prio != null) {
       try {
+        ScanUserDataLogger.logTrace(null, si.getUserData(),","[{'comment': 'Why are we passing `null` for the logger? Under what circumstances would we want to pass `null` vs. something else?', 'commenter': 'ctubbsii'}]"
3124,core/src/main/java/org/apache/accumulo/core/spi/scan/ScanInfo.java,"@@ -125,4 +125,9 @@ enum Type {
    * @return Hints set by a scanner using {@link ScannerBase#setExecutionHints(Map)}
    */
   Map<String,String> getExecutionHints();
+
+  /**
+   * @return user data set by a scanner using {@link ScannerBase#setUserData(String)}
+   */
+  String getUserData();","[{'comment': ""Since this is adding a method to an API/SPI interface, it would be nice to have a default method here, so it doesn't immediately break all users who implemented this SPI in the previous release."", 'commenter': 'ctubbsii'}]"
3124,shell/src/main/java/org/apache/accumulo/shell/commands/ActiveScanIterator.java,"@@ -71,9 +71,9 @@ private void readNext() {
 
     final String header = String.format(
         "" %-21s| %-21s| %-9s| %-9s| %-7s| %-6s|""
-            + "" %-8s| %-8s| %-10s| %-20s| %-10s| %-10s | %-20s | %s"",
+            + "" %-8s| %-8s| %-10s| %-20s| %-10s| %-10s | %-20s | %s | %s"",","[{'comment': ""Because these are being printed... I'm wondering if it's better to use empty strings instead of nulls wherever null is being passed for the default here. This also makes me wonder if metadata scans internal to Accumulo should have a unique, generated user data."", 'commenter': 'ctubbsii'}]"
3124,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -95,6 +96,14 @@ protected ConsistencyLevel getConsistency(CommandLine cl) {
     }
   }
 
+  protected String getUserData(CommandLine cl) {
+    if (cl.hasOption(userDataOpt.getOpt())) {
+      return cl.getOptionValue(userDataOpt.getOpt());
+    } else {
+      return ""Accumulo Shell"";","[{'comment': 'The shell could have a unique ID it uses by default here, so individual shell instances can be distinguished from one another.', 'commenter': 'ctubbsii'}]"
3124,core/src/main/java/org/apache/accumulo/core/spi/scan/ScanServerSelector.java,"@@ -105,6 +105,12 @@ interface SelectorParameters {
      *         were set, an empty map is returned.
      */
     Map<String,String> getHints();
+
+    /**
+     * @return user data set by a scanner using {@link ScannerBase#setUserData(String)}
+     */","[{'comment': '```suggestion\r\n     * @return user data set by a scanner using {@link ScannerBase#setUserData(String)}\r\n     *\r\n     * @since 3.0.0\r\n     */\r\n```', 'commenter': 'keith-turner'}]"
3124,core/src/main/java/org/apache/accumulo/core/spi/scan/ScanInfo.java,"@@ -125,4 +125,11 @@ enum Type {
    * @return Hints set by a scanner using {@link ScannerBase#setExecutionHints(Map)}
    */
   Map<String,String> getExecutionHints();
+
+  /**
+   * @return user data set by a scanner using {@link ScannerBase#setUserData(String)}
+   */","[{'comment': '```suggestion\r\n   * @return user data set by a scanner using {@link ScannerBase#setUserData(String)}\r\n   * \r\n   * @since 3.0.0\r\n   */\r\n```', 'commenter': 'keith-turner'}]"
3129,server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java,"@@ -95,7 +97,10 @@ private static void checkTable(final ClientContext context, final boolean verbos
 
     final HashSet<KeyExtent> failures = new HashSet<>();
 
-    Map<HostAndPort,List<KeyExtent>> extentsPerServer = new TreeMap<>();
+    Comparator<HostAndPort> comparator = Comparator.nullsFirst(","[{'comment': 'You should be able to make this static and create it just one time in the class and reuse the comparator just like we were doing originally in the copied version. It might be better to even have it in a public utility class somewhere so we could re-use it elsewhere if needed.', 'commenter': 'cshannon'}, {'comment': 'Addressed in 6495b62', 'commenter': 'DomGarguilo'}]"
3134,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.apache.accumulo.harness.AccumuloITBase.SUNNY_DAY;
+import static org.apache.accumulo.test.functional.ConfigurableMacBase.configureForSsl;
+
+import java.time.Duration;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.accumulo.server.rpc.ThriftServerType;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.thrift.TConfiguration;
+import org.junit.jupiter.api.Nested;
+import org.junit.jupiter.api.Tag;
+import org.junit.jupiter.api.Test;
+
+@Tag(SUNNY_DAY)","[{'comment': ""I wouldn't consider this a SUNNY_DAY test. We don't want to add too many of those, because we want to keep those relatively fast, compared to the full IT build."", 'commenter': 'ctubbsii'}]"
3134,server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java,"@@ -221,7 +223,8 @@ private static ServerAddress createThreadedSelectorServer(HostAndPort address,
       long timeBetweenThreadChecks, long maxMessageSize) throws TTransportException {
 
     final TNonblockingServerSocket transport =
-        new TNonblockingServerSocket(new InetSocketAddress(address.getHost(), address.getPort()));
+        new TNonblockingServerSocket(new InetSocketAddress(address.getHost(), address.getPort()), 0,
+            Ints.saturatedCast(maxMessageSize));","[{'comment': ""I didn't know about saturated cast! That's nice. What about the timeout value? You're setting that to 0? What's the consequences of that?"", 'commenter': 'ctubbsii'}, {'comment': ""Yeah I've been using that for a couple years, quite handy to avoid possible buffer overflows.\r\n\r\nI'm setting it to 0 because that is the default value and what we were doing before with the previous constructor as shown here https://github.com/apache/thrift/blob/v0.17.0/lib/java/src/main/java/org/apache/thrift/transport/TNonblockingServerSocket.java#L75\r\n\r\nThe value eventually makes it's way to being set as the soTimeout() value on the socket which mean it's an infinite timeout, so basically no timeout. https://github.com/apache/thrift/blob/v0.17.0/lib/java/src/main/java/org/apache/thrift/transport/TNonblockingSocket.java#L114\r\n\r\nWe could make this configurable but for now i was just keeping the existing behavior as that would be a new config option and should be a new Issue/PR i would think."", 'commenter': 'cshannon'}, {'comment': ""I think we do have an RPC timeout configuration option. I'd be curious to know if that is being set elsewhere and needs to be set here as well. Just as with the max message size, it seems there's likely more than one place we need to set these... here, and wherever we're currently setting them before these new constructors became available to us."", 'commenter': 'ctubbsii'}, {'comment': ""I figured there may be a timeout option already and I could try and update this PR to include it but I didn't know how much refactoring that would take and didn't want to bloat the scope of this PR and thought it would be better as a separate issue to add in. I can create a new issue if you want so we don't forget to do it."", 'commenter': 'cshannon'}, {'comment': '@cshannon - were you able to look at the RPC timeout configuration or create a follow-on issue?', 'commenter': 'dlmarion'}, {'comment': 'I forgot about that, let me go ahead and create the RPC timeout configuration as a follow on thing to investigate. ', 'commenter': 'cshannon'}, {'comment': 'I opened up #3153 to investigate the RPC issue and I took a look and it looks like we should indeed to be setting the value of that property as part of the constructor argument for client timeout. We are using the property as it is returned as part of context.getclientTimeoutInMillis() and passed to the utility class [here](https://github.com/apache/accumulo/blob/c31194740080a1f12c65aec82fe46731f9da2069/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java#L198) but it appears we are only using it for the blocking servers but not for non blocking so it should be an easy fix to do it. I will update this PR to include that change as well.', 'commenter': 'cshannon'}]"
3134,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.apache.accumulo.harness.AccumuloITBase.SUNNY_DAY;
+import static org.apache.accumulo.test.functional.ConfigurableMacBase.configureForSsl;
+
+import java.time.Duration;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.harness.AccumuloClusterHarness;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.accumulo.server.rpc.ThriftServerType;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.thrift.TConfiguration;
+import org.junit.jupiter.api.Nested;
+import org.junit.jupiter.api.Tag;
+import org.junit.jupiter.api.Test;
+
+@Tag(SUNNY_DAY)
+public class ThriftMaxFrameSizeIT extends AccumuloClusterHarness {
+
+  private ThriftServerType serverType;
+
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(1);
+  }
+
+  @Override
+  public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+    cfg.setProperty(Property.GENERAL_RPC_SERVER_TYPE, serverType.name());
+    if (serverType == ThriftServerType.SSL) {
+      configureForSsl(cfg,
+          getSslDir(createTestDir(this.getClass().getName() + ""_"" + this.testName())));
+    }
+  }
+
+  @Nested
+  class TestDefault extends TestMaxFrameSize {
+    {
+      serverType = ThriftServerType.getDefault();
+    }","[{'comment': ""Interesting strategy to use nested test classes to affect the behavior of the `@BeforeEach` method that sets up the minicluster. I think we have some other Thrift tests that could benefit from a similar usage of this. I know there's some proxy tests, at least, that I think got moved to accumulo-proxy repo... but there might be others still here.\r\n\r\nHowever, I think it's a little confusing to use non-static initializer blocks to set the serverType. Can you just change those to explicit no-arg constructors instead?"", 'commenter': 'ctubbsii'}, {'comment': ""Yeah that's definitely weird and i will definitely fix it, it was probably a by product of me messing around with this test for a long time yesterday and not thinking.\r\n\r\nThis was my first time trying to do parameterized tests with JUnit 5 and I discovered yesterday there was no way to do parameters for a constructor, JUnit 4 style, which is what I was really trying to do.\r\n\r\nSo I spent a while just trying to play around with some options so i could use parameters and have them available in the configureMiniCluster method. I was trying all kinds of things before finally coming up with the nested class idea. Anyways, at one point I was messing around with some anonymous classes and had a static initializer so probably just copied it form that and wasn't thinking."", 'commenter': 'cshannon'}, {'comment': 'Also in regards to your comment about it being useful for other tests, I think so too. Curiously when I was researching online about how to solve this no one really had any answers. Everyones answer was pretty much ""you can\'t do that"". I came up with the idea for the nested classes because initially I was thinking I was going to need to write a subclass for every single test which I didn\'t want to do and then the nested idea hit me.', 'commenter': 'cshannon'}]"
3136,start/src/main/java/org/apache/accumulo/start/classloader/vfs/ContextManager.java,"@@ -19,95 +19,96 @@
 package org.apache.accumulo.start.classloader.vfs;
 
 import java.io.IOException;
+import java.net.MalformedURLException;
+import java.net.URL;
+import java.net.URLClassLoader;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
 import java.util.function.Supplier;
+import java.util.stream.Collectors;
 
-import org.apache.commons.vfs2.FileSystemException;
-import org.apache.commons.vfs2.FileSystemManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
-class ContextManager {
+//@Deprecated
+public class ContextManager {
 
   private static final Logger log = LoggerFactory.getLogger(ContextManager.class);
 
+  public static final String CONTEXT_CLASSPATH_PROPERTY = ""general.vfs.context.classpath."";","[{'comment': '```suggestion\r\n  public static final String CONTEXT_CLASSPATH_PROPERTY = ""general.context.classpath."";\r\n```', 'commenter': 'dlmarion'}]"
3136,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -1116,20 +1090,7 @@ public enum Property {
           + "" `general.vfs.context.classpath.<name>.delegation=post`, where `<name>` is""
           + "" your context name. If delegation is not specified, it defaults to loading""
           + "" from parent classloader first."",","[{'comment': 'Suggest replacing description with the following as the delegation model no longer applies:\r\n\r\nProperties in this category define a classpath. These properties start with the prefix followed by a context name. The value is a comma separated list of URIs. For example, general.context.classpath.cx1=file:///opt/accumulo/lib/cx1/*.jar,file:///usr/lib/accumulo/lib/cx1/*.jar', 'commenter': 'dlmarion'}]"
3136,start/src/main/java/org/apache/accumulo/start/classloader/AccumuloClassLoader.java,"@@ -39,7 +39,7 @@
 
 import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 
-@Deprecated
+//@Deprecated","[{'comment': 'I think we want to remove this class. The idea here is that Accumulo will use the normal JVM classloader and classpath mechanisms. ', 'commenter': 'dlmarion'}]"
3136,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -76,7 +79,11 @@ public static ClassLoader getClassLoader(String context) {
     if (context != null && !context.isEmpty()) {
       return FACTORY.getClassLoader(context);
     } else {
-      return org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.getClassLoader();
+      try {","[{'comment': 'We use the ContextClassLoaderFactory to return ClassLoaders for a table context. If a non-context classloader is requested, then we can return the application classloader. I think we can just return `ClassLoader.getSystemClassLoader()` or `ClassLoaderUtil.class.getClassLoader()` here.', 'commenter': 'dlmarion'}]"
3136,start/src/main/java/org/apache/accumulo/start/classloader/vfs/ContextManager.java,"@@ -19,95 +19,96 @@
 package org.apache.accumulo.start.classloader.vfs;
 
 import java.io.IOException;
+import java.net.MalformedURLException;
+import java.net.URL;
+import java.net.URLClassLoader;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
 import java.util.function.Supplier;
+import java.util.stream.Collectors;
 
-import org.apache.commons.vfs2.FileSystemException;
-import org.apache.commons.vfs2.FileSystemManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
-class ContextManager {
+//@Deprecated
+public class ContextManager {
 
   private static final Logger log = LoggerFactory.getLogger(ContextManager.class);
 
+  public static final String CONTEXT_CLASSPATH_PROPERTY = ""general.vfs.context.classpath."";
+
   // there is a lock per context so that one context can initialize w/o blocking another context
   private class Context {
-    AccumuloReloadingVFSClassLoader loader;
+    URLClassLoader loader;
     ContextConfig cconfig;
     boolean closed = false;
 
     Context(ContextConfig cconfig) {
       this.cconfig = cconfig;
     }
 
-    synchronized ClassLoader getClassLoader() throws FileSystemException {
+    synchronized ClassLoader getClassLoader() throws IOException {
       if (closed) {
         return null;
       }
 
       if (loader == null) {
-        log.debug(
-            ""ClassLoader not created for context {}, creating new one. uris: {}, preDelegation: {}"",
-            cconfig.name, cconfig.uris, cconfig.preDelegation);
-        loader =
-            new AccumuloReloadingVFSClassLoader(cconfig.uris, vfs, parent, cconfig.preDelegation);
+        log.debug(""ClassLoader not created for context {}, creating new one. uris: {}"",
+            cconfig.name, cconfig.uris);
+        loader = new URLClassLoader(Arrays.stream(cconfig.uris.split("","")).map(url -> {
+          try {
+            return new URL(""file://"" + url);
+          } catch (MalformedURLException e) {
+            throw new RuntimeException(e);
+          }
+        }).collect(Collectors.toList()).toArray(new URL[] {}), parent);
       }
 
-      return loader.getClassLoader();
+      return loader;
     }
 
     synchronized void close() {
       closed = true;
-      if (loader != null) {
-        loader.close();
-      }
       loader = null;
     }
   }
 
   private Map<String,Context> contexts = new HashMap<>();
 
   private volatile ContextsConfig config;
-  private FileSystemManager vfs;
-  private ReloadingClassLoader parent;
+  private ClassLoader parent;
 
-  ContextManager(FileSystemManager vfs, ReloadingClassLoader parent) {
-    this.vfs = vfs;
+  public ContextManager(ClassLoader parent) {
     this.parent = parent;","[{'comment': 'Not sure we need to the parent argument, the parent classloader will be the ClassLoader for this class.', 'commenter': 'dlmarion'}]"
3136,core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java,"@@ -106,11 +106,9 @@ public enum PropertyType {
           + ""Substitutions of the ACCUMULO_HOME environment variable can be done in the system ""
           + ""config file using '${env:ACCUMULO_HOME}' or similar.""),
 
-  // VFS_CLASSLOADER_CACHE_DIR's default value is a special case, for documentation purposes
-  @SuppressWarnings(""removal"")
   ABSOLUTEPATH(""absolute path"",
       x -> x == null || x.trim().isEmpty() || new Path(x.trim()).isAbsolute()
-          || x.equals(Property.VFS_CLASSLOADER_CACHE_DIR.getDefaultValue()),
+          || x.equals(Property.CONTEXT_CLASSPATH_PROPERTY.getDefaultValue()),","[{'comment': 'I think you can just remove line 113 with no replacement', 'commenter': 'dlmarion'}]"
3136,core/src/test/java/org/apache/accumulo/core/conf/AccumuloConfigurationTest.java,"@@ -269,13 +267,13 @@ public void testGetByPrefix() {
     assertSame(pmC, pmD);
     assertEquals(expected1, pmC);
 
-    tc.set(VFS_CONTEXT_CLASSPATH_PROPERTY.getKey() + ""ctx123"", ""hdfs://ib/p1"");
+    tc.set(CONTEXT_CLASSPATH_PROPERTY.getKey() + ""ctx123"", ""hdfs://ib/p1"");","[{'comment': 'Suggest changing these file paths to use file:///', 'commenter': 'dlmarion'}]"
3136,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -1092,21 +1079,8 @@ public enum Property {
           + ""constraint."",
       ""2.0.0""),
 
-  // VFS ClassLoader properties","[{'comment': 'GENERAL_CLASSPATHS property can be removed also.', 'commenter': 'dlmarion'}]"
3136,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java,"@@ -155,9 +155,6 @@ MiniAccumuloConfigImpl initialize() {
       @SuppressWarnings(""deprecation"")
       Property generalClasspaths = Property.GENERAL_CLASSPATHS;","[{'comment': 'This property can be removed also', 'commenter': 'dlmarion'}]"
3136,start/src/main/java/org/apache/accumulo/start/Main.java,"@@ -94,28 +94,15 @@ public static void main(final String[] args) throws Exception {
   public static synchronized ClassLoader getClassLoader() {
     if (classLoader == null) {
       try {
-        classLoader = (ClassLoader) getVFSClassLoader().getMethod(""getClassLoader"").invoke(null);
+        classLoader = org.apache.accumulo.start.classloader.AccumuloClassLoader.getClassLoader();","[{'comment': 'I think this method can be removed.', 'commenter': 'dlmarion'}]"
3136,shell/src/main/java/org/apache/accumulo/shell/commands/ClasspathCommand.java,"@@ -30,9 +30,9 @@ public class ClasspathCommand extends Command {
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState) {
 
     final PrintWriter writer = shellState.getWriter();
-    org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.printClassPath(s -> {
-      writer.print(s);
-    }, true);
+    // org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.printClassPath(s -> {
+    // writer.print(s);
+    // }, true);","[{'comment': ""I'm going to think about how we can still do this."", 'commenter': 'dlmarion'}, {'comment': ""Showing the contents of the `java.class.path` system property is probably enough here. For a configured context classloader factory, we could extend the SPI to have a method to show information about each context and call that here also. But, since this is run on the client side in the shell, it would have to be set up on the client side, and not just the server-side... unless we had a new RPC to specifically ask a random server to give us that info. And, in any case, we wouldn't be able to have much insight into the user's configured system classloader, if they set that... because that could do anything."", 'commenter': 'ctubbsii'}, {'comment': ""I think it might be sufficient to remove the classpath command. In the default scenario, the server and client VMs will use the standard JVM classpath mechanism. In the case where a users is using the new VFS classloader, the classpath is [printed](https://github.com/apache/accumulo-classloaders/blob/main/modules/vfs-class-loader/src/main/java/org/apache/accumulo/classloader/vfs/AccumuloVFSClassLoader.java#L596) to the server-side log during initialization (so it's only printed once)."", 'commenter': 'dlmarion'}, {'comment': 'Was the command previously deprecated? If not, we could at least dump `java.class.path` to make it *somewhat* useful without just removing it, and possibly causing scripts to break.', 'commenter': 'ctubbsii'}, {'comment': ""I don't think so, but could be done in 2.1.1."", 'commenter': 'dlmarion'}, {'comment': 'So, I think that it could print `java.class.path` (of the shell, not the server, unless they are the same) and then print out all of the values of the `table.class.loader.context` properties defined on the tables.', 'commenter': 'dlmarion'}]"
3136,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -1092,21 +1079,8 @@ public enum Property {
           + ""constraint."",
       ""2.0.0""),
 
-  // VFS ClassLoader properties
-
-  // this property shouldn't be used directly; it exists solely to document the default value
-  // defined by its use in AccumuloVFSClassLoader when generating the property documentation
-  @Deprecated(since = ""2.1.0"", forRemoval = true)
-  VFS_CLASSLOADER_SYSTEM_CLASSPATH_PROPERTY(
-      org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.VFS_CLASSLOADER_SYSTEM_CLASSPATH_PROPERTY,
-      """", PropertyType.STRING,
-      ""Configuration for a system level vfs classloader. Accumulo jar can be""
-          + "" configured here and loaded out of HDFS."",
-      ""1.5.0""),
-  @Deprecated(since = ""2.1.0"", forRemoval = true)
-  VFS_CONTEXT_CLASSPATH_PROPERTY(
-      org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.VFS_CONTEXT_CLASSPATH_PROPERTY,
-      null, PropertyType.PREFIX,
+  // ClassLoader properties
+  CONTEXT_CLASSPATH_PROPERTY(ContextManager.CONTEXT_CLASSPATH_PROPERTY, null, PropertyType.PREFIX,","[{'comment': ""This is a new property. I don't think we need this. We are removing this feature in favor of the system classloader, which is already configurable by Java. We never need to specify a classpath in properties, just a context name on a per-table basis, which is interpreted based on the implementation details of the classloader factory, which should be pluggable SPI."", 'commenter': 'ctubbsii'}, {'comment': ""Tables use a classloader name (context) from which to load user iterators and such. The classpaths for those classloaders need to be defined somewhere. It's been a while since we discussed the approach for this when removing VFS. What are your thoughts? I'm thinking that a URLClassLoader would be created per context classpath (it doesn't reload and it only supports what the URLClassLoader supports today)."", 'commenter': 'dlmarion'}, {'comment': ""The idea previously discussed was that the `general.context.class.loader.factory` is the singleton service for creating class loaders based on a specified context in the `table.class.loader.context` property. This is also what the descriptions of those properties say. So, the interpretation of the `table.class.loader.context` value is dependent on which implementation factory has been set in `general.context.class.loader.factory`. If you set a URLClassLoader factory, you can just set the `table.class.loader.context` property to be a set of URLs. You don't need a separate property at all. You can also have a more advanced factory that just takes a simplified name, and maps that name to a set of URLs or whatever, in its own configuration file.\r\n\r\nBasically, all we need is the factory that does `String context -> ClassLoader` mapping, and the property that stores the `String context`. That context could be a name, a number, a JSON blob, a comma-separated set of URLs (or using another delimiter than a comma), or whatever the user can imagine for their particular implementation.\r\n\r\nBy default, though, we discussed having a URLClassLoader that interprets the context field as a set of URLs. If you're concerned about misconfiguration, the implementation could require the set of URLs have some kind of prefix to validate, as in `urls:url1,url2,url3,...`, and a separate project could create a VFS classloader that had similar validation, `vfs:url1,url2,url3,...`, so mis-configurations are more detectable. But, validation like that is not a requirement of the implementing factory."", 'commenter': 'ctubbsii'}, {'comment': '> By default, though, we discussed having a URLClassLoader that interprets the context field as a set of URLs.\r\n\r\nOk, this is what I was forgetting. I do remember now discussing this. I have no issue with this, a user would set `table.class.loader.context=url1,url2,url3` and `url1,url2,url3` would be the name of the context and the set of URLs for the URLClassLoader.', 'commenter': 'dlmarion'}]"
3136,start/src/main/java/org/apache/accumulo/start/classloader/vfs/ContextManager.java,"@@ -19,95 +19,96 @@
 package org.apache.accumulo.start.classloader.vfs;
 
 import java.io.IOException;
+import java.net.MalformedURLException;
+import java.net.URL;
+import java.net.URLClassLoader;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
 import java.util.function.Supplier;
+import java.util.stream.Collectors;
 
-import org.apache.commons.vfs2.FileSystemException;
-import org.apache.commons.vfs2.FileSystemManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
-class ContextManager {
+//@Deprecated
+public class ContextManager {","[{'comment': 'This class was deprecated with the expectation to remove it entirely. What do we still need it for?', 'commenter': 'ctubbsii'}, {'comment': 'Given the discussion above about how the DefaultContextClassLoaderFactory will work now, I think this can be removed.', 'commenter': 'dlmarion'}]"
3136,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -1058,45 +1044,6 @@ public enum Property {
           + ""constraint."",
       ""2.0.0""),
 
-  // VFS ClassLoader properties
-
-  // this property shouldn't be used directly; it exists solely to document the default value
-  // defined by its use in AccumuloVFSClassLoader when generating the property documentation
-  @Deprecated(since = ""2.1.0"", forRemoval = true)
-  VFS_CLASSLOADER_SYSTEM_CLASSPATH_PROPERTY(
-      org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.VFS_CLASSLOADER_SYSTEM_CLASSPATH_PROPERTY,
-      """", PropertyType.STRING,
-      ""Configuration for a system level vfs classloader. Accumulo jar can be""
-          + "" configured here and loaded out of HDFS."",
-      ""1.5.0""),
-  @Deprecated(since = ""2.1.0"", forRemoval = true)
-  VFS_CONTEXT_CLASSPATH_PROPERTY(","[{'comment': ""I think we need a property like this, maybe called GENERAL_CONTEXT_CLASSPATH_PREFIX, so that administrators/users can define the classpaths for a context. Unless you accounted for this somewhere else, it's possible I missed it. The current way users interact with this is to define named classloader context and their paths, for example:\r\n\r\n```\r\ngeneral.vfs.context.classpath.<contextName>=<some classpath>\r\n```\r\n\r\nThen, in supported places in the code, they can pass in the `contextName` to use their custom supplied classes."", 'commenter': 'dlmarion'}, {'comment': '@cshannon reminded me that this is not needed based on earlier discussions. The context property set on the table will contain a comma separated list of URIs.', 'commenter': 'dlmarion'}]"
3136,core/src/main/java/org/apache/accumulo/core/classloader/DefaultContextClassLoaderFactory.java,"@@ -48,55 +46,49 @@ public class DefaultContextClassLoaderFactory implements ContextClassLoaderFacto
   private static final Logger LOG = LoggerFactory.getLogger(DefaultContextClassLoaderFactory.class);
   private static final String className = DefaultContextClassLoaderFactory.class.getName();
 
-  @SuppressWarnings(""removal"")
-  private static final Property VFS_CONTEXT_CLASSPATH_PROPERTY =
-      Property.VFS_CONTEXT_CLASSPATH_PROPERTY;
+  // Do we set a max size here and how long until we expire the classloader?
+  private final Cache<String,Context> contexts =
+      Caffeine.newBuilder().maximumSize(100).expireAfterAccess(1, TimeUnit.DAYS).build();
 
-  public DefaultContextClassLoaderFactory(final AccumuloConfiguration accConf) {
+  public DefaultContextClassLoaderFactory() {
     if (!isInstantiated.compareAndSet(false, true)) {
       throw new IllegalStateException(""Can only instantiate "" + className + "" once"");
     }
-    Supplier<Map<String,String>> contextConfigSupplier =
-        () -> accConf.getAllPropertiesWithPrefix(VFS_CONTEXT_CLASSPATH_PROPERTY);
-    setContextConfig(contextConfigSupplier);
-    LOG.debug(""ContextManager configuration set"");
-    startCleanupThread(accConf, contextConfigSupplier);
   }
 
-  @SuppressWarnings(""deprecation"")
-  private static void setContextConfig(Supplier<Map<String,String>> contextConfigSupplier) {
-    org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader
-        .setContextConfig(contextConfigSupplier);
-  }
+  @Override
+  public ClassLoader getClassLoader(String contextName) {","[{'comment': 'Given that we are changing the semantics of this from a name to a comma separated list of URIs, maybe we should change the name of the input parameter from `contextName` to `contextValue` or something. Both here and in the interface. Thoughts?', 'commenter': 'dlmarion'}, {'comment': ""I agree, we should probably change the name to something else. The value can actually be anything you want. By default it would be a list of URIs because that's what the default factory would use to create a classloader but if a user plugs in their own class loader factory implementation then this value can be anything the user wants as long as their factory knows how to read it. So I think `contextValue` or something like that makes a lot more sense as `contextName` doesn't apply anymore."", 'commenter': 'cshannon'}]"
3136,core/src/main/java/org/apache/accumulo/core/classloader/DefaultContextClassLoaderFactory.java,"@@ -48,55 +46,49 @@ public class DefaultContextClassLoaderFactory implements ContextClassLoaderFacto
   private static final Logger LOG = LoggerFactory.getLogger(DefaultContextClassLoaderFactory.class);
   private static final String className = DefaultContextClassLoaderFactory.class.getName();
 
-  @SuppressWarnings(""removal"")
-  private static final Property VFS_CONTEXT_CLASSPATH_PROPERTY =
-      Property.VFS_CONTEXT_CLASSPATH_PROPERTY;
+  // Do we set a max size here and how long until we expire the classloader?
+  private final Cache<String,Context> contexts =
+      Caffeine.newBuilder().maximumSize(100).expireAfterAccess(1, TimeUnit.DAYS).build();
 
-  public DefaultContextClassLoaderFactory(final AccumuloConfiguration accConf) {
+  public DefaultContextClassLoaderFactory() {
     if (!isInstantiated.compareAndSet(false, true)) {
       throw new IllegalStateException(""Can only instantiate "" + className + "" once"");
     }
-    Supplier<Map<String,String>> contextConfigSupplier =
-        () -> accConf.getAllPropertiesWithPrefix(VFS_CONTEXT_CLASSPATH_PROPERTY);
-    setContextConfig(contextConfigSupplier);
-    LOG.debug(""ContextManager configuration set"");
-    startCleanupThread(accConf, contextConfigSupplier);
   }
 
-  @SuppressWarnings(""deprecation"")
-  private static void setContextConfig(Supplier<Map<String,String>> contextConfigSupplier) {
-    org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader
-        .setContextConfig(contextConfigSupplier);
-  }
+  @Override
+  public ClassLoader getClassLoader(String contextName) {
+    if (contextName == null) {
+      throw new IllegalArgumentException(""Unknown context"");
+    }
 
-  private static void startCleanupThread(final AccumuloConfiguration conf,
-      final Supplier<Map<String,String>> contextConfigSupplier) {
-    ScheduledFuture<?> future = ThreadPools.getClientThreadPools((t, e) -> {
-      LOG.error(""context classloader cleanup thread has failed."", e);
-    }).createGeneralScheduledExecutorService(conf)
-        .scheduleWithFixedDelay(Threads.createNamedRunnable(className + ""-cleanup"", () -> {
-          LOG.trace(""{}-cleanup thread, properties: {}"", className, conf);
-          Set<String> contextsInUse = contextConfigSupplier.get().keySet().stream()
-              .map(p -> p.substring(VFS_CONTEXT_CLASSPATH_PROPERTY.getKey().length()))
-              .collect(Collectors.toSet());
-          LOG.trace(""{}-cleanup thread, contexts in use: {}"", className, contextsInUse);
-          removeUnusedContexts(contextsInUse);
-        }), 1, 1, MINUTES);
-    ThreadPools.watchNonCriticalScheduledTask(future);
-    LOG.debug(""Context cleanup timer started at 60s intervals"");
-  }
+    final ClassLoader loader = contexts.get(contextName, Context::new).getClassLoader();
 
-  @SuppressWarnings(""deprecation"")
-  private static void removeUnusedContexts(Set<String> contextsInUse) {
-    org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader
-        .removeUnusedContexts(contextsInUse);
+    LOG.debug(""Returning classloader {} for context {}"", loader.getClass().getName(), contextName);
+    return loader;
   }
 
-  @SuppressWarnings(""deprecation"")
-  @Override
-  public ClassLoader getClassLoader(String contextName) {
-    return org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader
-        .getContextClassLoader(contextName);
+  private static class Context {","[{'comment': 'Do we need this class? Could we just make the Caffeine map <String,ClassLoader> ?', 'commenter': 'dlmarion'}, {'comment': 'The purpose of Caffeine was so that the cache will remove old classloaders automatically when no longer needed but this could be re-worked if desired. \r\n\r\nIn the current version there is a thread that runs to check for no longer needed contexts and will remove them. The idea of using Caffeine was that we can cache the Classloader but if it is no longer needed anymore after some period of time (nothing reads it because maybe the config was dropped, etc) we can then just remove it from the cache automatically. This seemed simpler than maintaining a GC thread.', 'commenter': 'cshannon'}]"
3136,start/src/main/java/org/apache/accumulo/start/Main.java,"@@ -45,13 +44,11 @@ public static void main(final String[] args) throws Exception {
     // Preload classes that cause a deadlock between the ServiceLoader and the DFSClient when
     // using the VFSClassLoader with jars in HDFS.
     ClassLoader loader = getClassLoader();","[{'comment': 'Do we need the `getClassLoader()` method anymore? Can we just call `ClassLoaderUtil.getClassLoader(null);` ?', 'commenter': 'dlmarion'}, {'comment': ""I actually tried to do that but the issue is ClassLoaderUtil isn't available to use here. The main class is part of the `start` module and ClassLoaderUtil lives inside the `core` module. Core has a dependency on start so we can't use the class."", 'commenter': 'cshannon'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader10to11.java,"@@ -0,0 +1,236 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_STATE;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Set;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.client.BatchDeleter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.manager.state.tables.TableState;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class Upgrader10to11 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader10to11.class);
+
+  // Included for upgrade code usage any other usage post 3.0 should not be used.
+  private static final TableId REPLICATION_ID = TableId.of(""+rep"");
+
+  public Upgrader10to11() {
+    super();
+  }
+
+  @Override
+  public void upgradeZookeeper(final ServerContext context) {
+    log.info(""upgrade of ZooKeeper entries"");
+
+    var zrw = context.getZooReaderWriter();
+    var iid = context.getInstanceID();
+
+    // if the replication base path (../tables/+rep) assume removed or never existed.
+    if (!checkReplicationTableInZk(iid, zrw)) {
+      log.debug(""replication table root node does not exist in ZooKeeper - nothing to do"");
+      return;
+    }
+
+    // if the replication table is online - stop. There could be data in transit.
+    if (!checkReplicationOffline(iid, zrw)) {
+      throw new IllegalStateException(
+          ""Replication table is not offline. Cannot continue with upgrade that will remove replication with replication active"");
+    }
+
+    deleteReplicationConfigs(zrw, iid, context.getPropStore());
+
+    deleteReplicationTableZkEntries(zrw, iid);
+
+  }
+
+  @Override
+  public void upgradeRoot(final ServerContext context) {
+    log.info(""upgrade root - skipping, nothing to do"");
+  }
+
+  @Override
+  public void upgradeMetadata(final ServerContext context) {
+    log.info(""upgrade metadata entries"");
+    deleteReplMetadataEntries(context);
+  }
+
+  /**
+   * remove +rep entries from metadata.
+   */
+  private void deleteReplMetadataEntries(final ServerContext context) {
+    try (BatchDeleter deleter =
+        context.createBatchDeleter(MetadataTable.NAME, Authorizations.EMPTY, 10)) {
+      deleter.setRanges(Collections.singletonList(
+          new Range(REPLICATION_ID.canonical() + "";"", REPLICATION_ID.canonical() + ""<"")));
+      deleter.delete();","[{'comment': 'This deletes the `+rep` table entries, right? Do we also need to delete any replication entries within other tables? Reference: https://github.com/apache/accumulo/blob/2.1/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java#L404', 'commenter': 'dlmarion'}, {'comment': ""Maybe?  Although I configured replication I did not actually set-up and run two full instances (yet) - so I have not seen those entries. I think removing the replication iterator from the configs is the minimum required to get an upgraded instance to run - there maybe additional artifacts, but as far as I know, they do not impact functionality.\r\n\r\nAs other entries are identified, they can be added to this PR is done as a follow-on.  As-is this is sufficient to upgrade and run 3.0 code as it stands now (well, that's the goal)"", 'commenter': 'EdColeman'}, {'comment': ""I don't know what other tables would have replication entries. There's just the replication table (in its entirety) and these entries in the metadata table, as far as I can tell."", 'commenter': 'ctubbsii'}, {'comment': ""The repTableRange would be deleting the metadata entries that define the replication table. The repWalRange would be deleting the additionally tracked WALs that are preserved because they are in use, and intended to prevent the accumulo-gc from deleting the still-referenced WALs. I think that's all we're expecting."", 'commenter': 'ctubbsii'}]"
3137,server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java,"@@ -36,6 +36,11 @@
  */
 public class AccumuloDataVersion {
 
+  /**
+   * version (11) reflects removal of replication starting with 3.0
+   */
+  public static final int REMOVE_REPLICATION = 11;","[{'comment': 'I suspect that this will be just one of several things that this upgrade will perform. It might be better to rename this to something more broadly applicable, such as `REMOVE_DEPRECATIONS_FOR_VERSION_3`. Can always enumerate the details in the javadoc, if desired.', 'commenter': 'ctubbsii'}, {'comment': 'Changed in b3ca1d9b65', 'commenter': 'EdColeman'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -152,8 +155,13 @@ public synchronized void upgradeZookeeper(ServerContext context,
         abortIfFateTransactions(context);
 
         for (int v = currentVersion; v < AccumuloDataVersion.get(); v++) {
-          log.info(""Upgrading Zookeeper from data version {}"", v);
-          upgraders.get(v).upgradeZookeeper(context);
+          log.info(""Upgrading Zookeeper from data version {} target version {}"", v,
+              AccumuloDataVersion.get());","[{'comment': 'I think this log message change could be misleading. These are incremental upgrades, but the target version is just the final version. For example, the message may imply that it is upgrading from 5 to 9, but is really doing 5 to 6, then 6 to 7, etc. until it gets to 8 to 9.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed with b3ca1d9b65', 'commenter': 'EdColeman'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -152,8 +155,13 @@ public synchronized void upgradeZookeeper(ServerContext context,
         abortIfFateTransactions(context);
 
         for (int v = currentVersion; v < AccumuloDataVersion.get(); v++) {
-          log.info(""Upgrading Zookeeper from data version {}"", v);
-          upgraders.get(v).upgradeZookeeper(context);
+          log.info(""Upgrading Zookeeper from data version {} target version {}"", v,
+              AccumuloDataVersion.get());
+          var upgrader = upgraders.get(v);
+          log.info(""UPGRADER: version:{} upgrader: {}"", v, upgrader);","[{'comment': 'Is this expecting the upgrader to have a meaningful toString? If we want it to log here, in a consistent way, we should add a dedicated method for getting the description for logging. Or, we can create an upgrader base class that logs itself when run.', 'commenter': 'ctubbsii'}, {'comment': 'The was leftover from development - addressed with b3ca1d9b65', 'commenter': 'EdColeman'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -179,15 +187,21 @@ public synchronized Future<Void> upgradeMetadata(ServerContext context,
           .submit(() -> {
             try {
               for (int v = currentVersion; v < AccumuloDataVersion.get(); v++) {
-                log.info(""Upgrading Root from data version {}"", v);
-                upgraders.get(v).upgradeRoot(context);
+                log.info(""Upgrading Root from data version {} target version {}"", v,
+                    AccumuloDataVersion.get());
+                if (upgraders.get(v) != null) {
+                  upgraders.get(v).upgradeRoot(context);
+                }
               }
 
               setStatus(UpgradeStatus.UPGRADED_ROOT, eventCoordinator);
 
               for (int v = currentVersion; v < AccumuloDataVersion.get(); v++) {
-                log.info(""Upgrading Metadata from data version {}"", v);
-                upgraders.get(v).upgradeMetadata(context);
+                log.info(""Upgrading Metadata from data version {} target version {}"", v,
+                    AccumuloDataVersion.get());
+                if (upgraders.get(v) != null) {
+                  upgraders.get(v).upgradeMetadata(context);
+                }","[{'comment': ""Why the null checks here? Under what circumstances would we find an upgrader to be null? If that happens, we don't want to completely ignore it, but we should fail hard. That's not expected."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed with b3ca1d9b65', 'commenter': 'EdColeman'}, {'comment': ""I think you should replace lines 202 - 204 with `upgraders.get(v).upgradeMetadata(context);`. You added the `requireNonNull` check in the commit you mentioned, so it shouldn't be null. If it null here, then this code will hide it and move on."", 'commenter': 'dlmarion'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader10to11.java,"@@ -0,0 +1,270 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_STATE;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.RESERVED_PREFIX;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.client.BatchDeleter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.manager.state.tables.TableState;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.core.volume.Volume;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.hadoop.fs.Path;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+
+public class Upgrader10to11 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader10to11.class);
+
+  // Included for upgrade code usage any other usage post 3.0 should not be used.
+  private static final TableId REPLICATION_ID = TableId.of(""+rep"");
+
+  public Upgrader10to11() {
+    super();
+  }
+
+  @Override
+  public void upgradeZookeeper(final ServerContext context) {
+    log.info(""upgrade of ZooKeeper entries"");
+
+    var zrw = context.getZooReaderWriter();
+    var iid = context.getInstanceID();
+
+    // if the replication base path (../tables/+rep) assume removed or never existed.
+    if (!checkReplicationTableInZk(iid, zrw)) {
+      log.debug(""replication table root node does not exist in ZooKeeper - nothing to do"");
+      return;
+    }
+
+    // if the replication table is online - stop. There could be data in transit.
+    if (!checkReplicationOffline(iid, zrw)) {
+      throw new IllegalStateException(
+          ""Replication table is not offline. Cannot continue with upgrade that will remove replication with replication active"");
+    }
+
+    deleteReplicationConfigs(zrw, iid, context.getPropStore());
+
+    deleteReplicationTableZkEntries(zrw, iid);
+
+  }
+
+  @Override
+  public void upgradeRoot(final ServerContext context) {
+    log.info(""upgrade root - skipping, nothing to do"");
+  }
+
+  @Override
+  public void upgradeMetadata(final ServerContext context) {
+    log.info(""upgrade metadata entries"");","[{'comment': ""This is upgrading at the info level, which is very visible. It should have a slightly more verbose description if it's going to appear in user logs or on the console, like upgrading metadata entries to do what? or as part of what? As is, these feel like debugging log statements, with minimal information, just used in the course of development and left in unintentionally. I think they should be either meaningful for the user or removed, or reduced to the trace or debug level."", 'commenter': 'ctubbsii'}, {'comment': 'This appears once - during initialization.  Along with the classn ame in the log message highlights the upgrade steps and seems sufficient.\r\n\r\n```\r\n2022-12-23T21:01:37,482 [upgrade.Upgrader10to11] INFO : upgrade metadata entries\r\n...\r\n022-12-23T21:01:37,482 [upgrade.Upgrader10to11] INFO : upgrade root - skipping, nothing to do\r\n...\r\n2022-12-23T21:01:37,482 [upgrade.Upgrader10to11] INFO : upgrade metadata entries\r\n```\r\n\r\nHaving these stand-out beyond debug seems appropriate and would help if things do not proceed as expected.', 'commenter': 'EdColeman'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader10to11.java,"@@ -0,0 +1,270 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_STATE;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.RESERVED_PREFIX;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.client.BatchDeleter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.manager.state.tables.TableState;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.core.volume.Volume;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.hadoop.fs.Path;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+
+public class Upgrader10to11 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader10to11.class);
+
+  // Included for upgrade code usage any other usage post 3.0 should not be used.
+  private static final TableId REPLICATION_ID = TableId.of(""+rep"");
+
+  public Upgrader10to11() {
+    super();
+  }
+
+  @Override
+  public void upgradeZookeeper(final ServerContext context) {
+    log.info(""upgrade of ZooKeeper entries"");
+
+    var zrw = context.getZooReaderWriter();
+    var iid = context.getInstanceID();
+
+    // if the replication base path (../tables/+rep) assume removed or never existed.
+    if (!checkReplicationTableInZk(iid, zrw)) {
+      log.debug(""replication table root node does not exist in ZooKeeper - nothing to do"");
+      return;
+    }
+
+    // if the replication table is online - stop. There could be data in transit.
+    if (!checkReplicationOffline(iid, zrw)) {
+      throw new IllegalStateException(
+          ""Replication table is not offline. Cannot continue with upgrade that will remove replication with replication active"");
+    }
+
+    deleteReplicationConfigs(zrw, iid, context.getPropStore());
+
+    deleteReplicationTableZkEntries(zrw, iid);
+
+  }
+
+  @Override
+  public void upgradeRoot(final ServerContext context) {
+    log.info(""upgrade root - skipping, nothing to do"");
+  }
+
+  @Override
+  public void upgradeMetadata(final ServerContext context) {
+    log.info(""upgrade metadata entries"");
+    deleteReplMetadataEntries(context);
+    deleteReplHdfsFiles(context);
+  }
+
+  /**
+   * remove +rep entries from metadata.
+   */
+  private void deleteReplMetadataEntries(final ServerContext context) {
+    try (BatchDeleter deleter =
+        context.createBatchDeleter(MetadataTable.NAME, Authorizations.EMPTY, 10)) {
+
+      Range repTableRange =
+          new Range(REPLICATION_ID.canonical() + "";"", true, REPLICATION_ID.canonical() + ""<"", true);
+      // copied from MetadataSchema 2.1 (removed in 3.0)
+      Range repWalRange =
+          new Range(RESERVED_PREFIX + ""repl"", true, RESERVED_PREFIX + ""repm"", false);
+
+      deleter.setRanges(List.of(repTableRange, repWalRange));
+      deleter.delete();
+    } catch (TableNotFoundException | MutationsRejectedException ex) {
+      throw new IllegalStateException(""failed to remove replication info from metadata table"", ex);
+    }
+  }
+
+  @VisibleForTesting
+  void deleteReplHdfsFiles(final ServerContext context) {
+    try {
+      for (Volume volume : context.getVolumeManager().getVolumes()) {
+        String dirUri = volume.getBasePath() + Constants.HDFS_TABLES_DIR + Path.SEPARATOR
+            + REPLICATION_ID.canonical();","[{'comment': ""This is making assumptions that there are no other files in these directories other than files used by the replication table and not used by any other table. That doesn't seem like a safe assumption. For example, if the replication table had been cloned, some of its files might now be referenced by the clone.\r\n\r\nInstead of deleting these directories, we should instead read the file entriess in the replication table's tablets and create corresponding `~del` entries for the `accumulo-gc` to delete later when it determines they no longer need to be preserved. This also allows the actual deletion to be deferred by the user by avoiding running the `accumulo-gc` during upgrade (which might be one way they could upgrade more cautiously) or by utilizing the HDFS trash that the `accumulo-gc` is configured to use."", 'commenter': 'ctubbsii'}, {'comment': 'Will need to look at this - during the upgrade, and depending on the phase, there is limited information - what is in zookeeper, what is in the root table and what is in the metadata - other tables have not been brought online. \r\n\r\nOne way to handle this could be through instructions.\r\n\r\nThe replication table must be offline for the upgrade to proceed.  So, if someone is using replication, the instructions could include the warning that any tables that may have been cloned / shared data with the replication table, they should be compacted prior to upgrading so they do not share any files.\r\n\r\nAlternatively, the files could be left in place and leave it up to the user to delete the hdfs directory if they want.  \r\n\r\nRemoving the zookeeper and metadata entries during the upgrade process seems to be required because of the ""special"" status as a system table, but also closes off some other options.', 'commenter': 'EdColeman'}, {'comment': 'Addressed with 8490bde61d - reads the metadata table for replication table files and then deletes the files individually.  Un-referenced files and the dir structure is left alone.', 'commenter': 'EdColeman'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader10to11.java,"@@ -0,0 +1,270 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZNAMESPACES;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_STATE;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.RESERVED_PREFIX;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.Constants;
+import org.apache.accumulo.core.client.BatchDeleter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.manager.state.tables.TableState;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.core.volume.Volume;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.hadoop.fs.Path;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+
+public class Upgrader10to11 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader10to11.class);
+
+  // Included for upgrade code usage any other usage post 3.0 should not be used.
+  private static final TableId REPLICATION_ID = TableId.of(""+rep"");
+
+  public Upgrader10to11() {
+    super();
+  }
+
+  @Override
+  public void upgradeZookeeper(final ServerContext context) {
+    log.info(""upgrade of ZooKeeper entries"");
+
+    var zrw = context.getZooReaderWriter();
+    var iid = context.getInstanceID();
+
+    // if the replication base path (../tables/+rep) assume removed or never existed.
+    if (!checkReplicationTableInZk(iid, zrw)) {
+      log.debug(""replication table root node does not exist in ZooKeeper - nothing to do"");
+      return;
+    }
+
+    // if the replication table is online - stop. There could be data in transit.
+    if (!checkReplicationOffline(iid, zrw)) {
+      throw new IllegalStateException(
+          ""Replication table is not offline. Cannot continue with upgrade that will remove replication with replication active"");
+    }
+
+    deleteReplicationConfigs(zrw, iid, context.getPropStore());
+
+    deleteReplicationTableZkEntries(zrw, iid);
+
+  }
+
+  @Override
+  public void upgradeRoot(final ServerContext context) {
+    log.info(""upgrade root - skipping, nothing to do"");
+  }
+
+  @Override
+  public void upgradeMetadata(final ServerContext context) {
+    log.info(""upgrade metadata entries"");
+    deleteReplMetadataEntries(context);
+    deleteReplHdfsFiles(context);
+  }
+
+  /**
+   * remove +rep entries from metadata.
+   */
+  private void deleteReplMetadataEntries(final ServerContext context) {
+    try (BatchDeleter deleter =
+        context.createBatchDeleter(MetadataTable.NAME, Authorizations.EMPTY, 10)) {
+
+      Range repTableRange =
+          new Range(REPLICATION_ID.canonical() + "";"", true, REPLICATION_ID.canonical() + ""<"", true);
+      // copied from MetadataSchema 2.1 (removed in 3.0)
+      Range repWalRange =
+          new Range(RESERVED_PREFIX + ""repl"", true, RESERVED_PREFIX + ""repm"", false);
+
+      deleter.setRanges(List.of(repTableRange, repWalRange));
+      deleter.delete();
+    } catch (TableNotFoundException | MutationsRejectedException ex) {
+      throw new IllegalStateException(""failed to remove replication info from metadata table"", ex);
+    }
+  }
+
+  @VisibleForTesting
+  void deleteReplHdfsFiles(final ServerContext context) {
+    try {
+      for (Volume volume : context.getVolumeManager().getVolumes()) {
+        String dirUri = volume.getBasePath() + Constants.HDFS_TABLES_DIR + Path.SEPARATOR
+            + REPLICATION_ID.canonical();
+        Path replPath = new Path(dirUri);
+        if (volume.getFileSystem().exists(replPath)) {
+          try {
+            log.debug(""Removing replication dir and files in hdfs {}"", replPath);
+            volume.getFileSystem().delete(replPath, true);
+          } catch (IOException ex) {
+            log.error(""Unable to remove replication dir and files from "" + replPath + "": "" + ex);
+          }
+        }
+      }
+    } catch (IOException ex) {
+      log.error(""Unable to remove replication dir and files: "" + ex);
+    }
+  }
+
+  private boolean checkReplicationTableInZk(final InstanceId iid, final ZooReaderWriter zrw) {
+    try {
+      String path = buildRepTablePath(iid);
+      return zrw.exists(path);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""ZooKeeper error - cannot determine replication table status"",
+          ex);
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""interrupted reading replication state from ZooKeeper"", ex);
+    }
+  }
+
+  /**
+   * To protect against removing replication information if replication is being used and possible
+   * active, check the replication table state in Zookeeper to see if it is ONLINE (active) or
+   * OFFLINE (inactive). If the state node does not exist, then the status is considered as OFFLINE.
+   *
+   * @return true if the replication table state is OFFLINE, false otherwise
+   */
+  private boolean checkReplicationOffline(final InstanceId iid, final ZooReaderWriter zrw) {
+    try {
+      String path = buildRepTablePath(iid) + ZTABLE_STATE;
+      byte[] bytes = zrw.getData(path);
+      if (bytes != null && bytes.length > 0) {
+        String status = new String(bytes, UTF_8);
+        return TableState.OFFLINE.name().equals(status);
+      }
+      return false;
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(""ZooKeeper error - cannot determine replication table status"",
+          ex);
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""interrupted reading replication state from ZooKeeper"", ex);
+    }
+  }
+
+  /**
+   * Utility method to build the ZooKeeper replication table path. The path resolves to
+   * {@code /accumulo/INSTANCE_ID/tables/+rep}
+   */
+  static String buildRepTablePath(final InstanceId iid) {
+    return ZooUtil.getRoot(iid) + ZTABLES + ""/"" + REPLICATION_ID.canonical();
+  }
+
+  private void deleteReplicationTableZkEntries(ZooReaderWriter zrw, InstanceId iid) {
+    String repTablePath = buildRepTablePath(iid);
+    try {
+      zrw.recursiveDelete(repTablePath, ZooUtil.NodeMissingPolicy.SKIP);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""ZooKeeper error - failed recursive deletion on "" + repTablePath, ex);
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(""interrupted deleting "" + repTablePath + "" from ZooKeeper"",
+          ex);
+    }
+  }
+
+  private void deleteReplicationConfigs(ZooReaderWriter zrw, InstanceId iid, PropStore propStore) {
+    List<PropStoreKey<?>> ids = getPropKeysFromZkIds(zrw, iid);","[{'comment': ""Is this just removing them from the cache? It seems that these will be deleted from ZK later when we delete everything from ZK for this table. So, I'm not sure if this is strictly necessary to do separately. And, deleting the configs from ZK may cause the per-table configuration to enter a strange state, where its configuration now is the default per-table configuration or that set in the site configuration file or in the accumulo namespace config in ZK... which may not be what we want, because it could cause all sorts of things to change for the table that's supposed to be offline."", 'commenter': 'ctubbsii'}, {'comment': ""No, it is more that removing them from the cache - `propStore.removeProperties`  updates ZooKeeper.  The issue is when there are replication iterator configs specified in the the configuration - particularly the metadata table. But this is also checking the other ZooKeeper configs to make sure that if the replication iterator was set it is removed.\r\n\r\nIn 2.1, the ZooKeeper config for the metadata table has these replication entries:\r\n\r\n```\r\nroot@uno> config -t accumulo.metadata -f replc\r\n-----------+-------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------\r\nSCOPE      | NAME                                            | VALUE\r\n-----------+-------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntable      | table.iterator.majc.replcombiner .............. | 9,org.apache.accumulo.server.replication.StatusCombiner\r\ntable      | table.iterator.majc.replcombiner.opt.columns .. | stat\r\ntable      | table.iterator.minc.replcombiner .............. | 9,org.apache.accumulo.server.replication.StatusCombiner\r\ntable      | table.iterator.minc.replcombiner.opt.columns .. | stat\r\ntable      | table.iterator.scan.replcombiner .............. | 9,org.apache.accumulo.server.replication.StatusCombiner\r\ntable      | table.iterator.scan.replcombiner.opt.columns .. | stat\r\n-----------+-------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n```\r\n\r\nAnd, the StatusCombiner, if present, will cause 3.0 code to fail because they have been removed from the code base. \r\n\r\nAnd at this point, only the metadata and root tables are online.\r\n\r\nNot sure what to do if replication iterators have been set in the system config file - that's a manual removal - but if present they would likely cause the manager to go offline, if the upgrade sequence could complete.  The easiest way is for the upgrade instruction to include removing any replication configuration values with 2.1 down , but before starting 3.0.  There will likely be manual steps to completely remove replication remnants  - but functionally they will not prevent Accumulo from running. \r\n"", 'commenter': 'EdColeman'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -179,15 +186,22 @@ public synchronized Future<Void> upgradeMetadata(ServerContext context,
           .submit(() -> {
             try {
               for (int v = currentVersion; v < AccumuloDataVersion.get(); v++) {
-                log.info(""Upgrading Root from data version {}"", v);
-                upgraders.get(v).upgradeRoot(context);
+                log.info(""Upgrading Root - current version {} as step towards target version {}"", v,
+                    AccumuloDataVersion.get());
+                if (upgraders.get(v) != null) {
+                  upgraders.get(v).upgradeRoot(context);
+                }","[{'comment': ""I think you should replace lines 191-193 with `upgraders.get(v).upgradeRoot(context);`. You added the `requireNonNull` check in the commit you mentioned below, so it shouldn't be null. If it null here, then this code will hide it and move on."", 'commenter': 'dlmarion'}, {'comment': '5436f22281  changes the null check to match  the ZooKeeper  sectionn - using rquireNonNull - while the check may be redundant if the previous check(s) pass it isolates issues with execution order.  The reason behind the check is to provide a better error message - during development a misconfiguration was throwing a null pointer and having a log statement helped.  \r\n\r\nOriginally, the code skipped that section if null and it was pointed out by @ctubbsii that it would be better to fail.', 'commenter': 'EdColeman'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -105,8 +106,11 @@ public boolean isParentLevelUpgraded(KeyExtent extent) {
   private static Logger log = LoggerFactory.getLogger(UpgradeCoordinator.class);
 
   private int currentVersion;
-  private Map<Integer,Upgrader> upgraders = Map.of(AccumuloDataVersion.SHORTEN_RFILE_KEYS,
-      new Upgrader8to9(), AccumuloDataVersion.CRYPTO_CHANGES, new Upgrader9to10());
+  // map of ""current version"" -> upgrader to next version.
+  private Map<Integer,","[{'comment': ""`Map.of` returns an unmodifiable map. If you use `final` here, then the `upgraders` variable can't be reassigned. This should give you confidence in the map contents in the code below."", 'commenter': 'dlmarion'}, {'comment': 'Added final in 686aaa176d', 'commenter': 'EdColeman'}]"
3137,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader10to11.java,"@@ -0,0 +1,264 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.Constants.ZTABLES;
+import static org.apache.accumulo.core.Constants.ZTABLE_STATE;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.RESERVED_PREFIX;
+import static org.apache.accumulo.server.util.MetadataTableUtil.EMPTY_TEXT;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.client.BatchDeleter;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.manager.state.tables.TableState;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.hadoop.io.Text;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class Upgrader10to11 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader10to11.class);
+
+  // Included for upgrade code usage any other usage post 3.0 should not be used.
+  private static final TableId REPLICATION_ID = TableId.of(""+rep"");
+
+  private static final Range REP_TABLE_RANGE =
+      new Range(REPLICATION_ID.canonical() + "";"", true, REPLICATION_ID.canonical() + ""<"", true);
+
+  // copied from MetadataSchema 2.1 (removed in 3.0)
+  private static final Range REP_WAL_RANGE =
+      new Range(RESERVED_PREFIX + ""repl"", true, RESERVED_PREFIX + ""repm"", false);
+
+  public Upgrader10to11() {
+    super();
+  }
+
+  @Override
+  public void upgradeZookeeper(final ServerContext context) {
+    log.info(""upgrade of ZooKeeper entries"");
+
+    var zrw = context.getZooReaderWriter();
+    var iid = context.getInstanceID();
+
+    // if the replication base path (../tables/+rep) assume removed or never existed.
+    if (!checkReplicationTableInZk(iid, zrw)) {
+      log.debug(""replication table root node does not exist in ZooKeeper - nothing to do"");
+      return;
+    }
+
+    // if the replication table is online - stop. There could be data in transit.
+    if (!checkReplicationOffline(iid, zrw)) {
+      throw new IllegalStateException(
+          ""Replication table is not offline. Cannot continue with upgrade that will remove replication with replication active"");
+    }
+
+    cleanMetaConfig(iid, context.getPropStore());
+
+    deleteReplicationTableZkEntries(zrw, iid);
+
+  }
+
+  @Override
+  public void upgradeRoot(final ServerContext context) {
+    log.info(""upgrade root - skipping, nothing to do"");
+  }
+
+  @Override
+  public void upgradeMetadata(final ServerContext context) {
+    log.info(""upgrade metadata entries"");
+    List<String> replTableFiles = readReplFilesFromMetadata(context);
+    deleteReplMetadataEntries(context);
+    deleteReplTableFiles(context, replTableFiles);
+  }
+
+  List<String> readReplFilesFromMetadata(final ServerContext context) {
+    List<String> results = new ArrayList<>();
+    try (Scanner scanner = context.createScanner(MetadataTable.NAME, Authorizations.EMPTY)) {
+      scanner.fetchColumnFamily(MetadataSchema.TabletsSection.DataFileColumnFamily.NAME);
+      scanner.setRange(REP_TABLE_RANGE);
+      for (Map.Entry<Key,Value> entry : scanner) {
+        String f = entry.getKey()
+            .getColumnQualifier(MetadataSchema.TabletsSection.DataFileColumnFamily.NAME).toString();
+        results.add(f);
+      }
+    } catch (TableNotFoundException ex) {
+      throw new IllegalStateException(""failed to read replication files from metadata"", ex);
+    }
+    return results;
+  }
+
+  void deleteReplTableFiles(final ServerContext context, final List<String> replTableFiles) {
+    // write delete mutations","[{'comment': 'you could short-circuit here if `replTableFiles` is empty', 'commenter': 'dlmarion'}, {'comment': 'Address in 686aaa176d', 'commenter': 'EdColeman'}]"
3139,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -851,6 +851,14 @@ public enum Property {
           + "" the tablet server this duration to revive before reassigning its tablets""
           + "" to other tablet servers.""),
 
+  TABLE_LOCATION_MODE(""table.location.mode"", ""locality"", PropertyType.LAST_LOCATION_MODE,","[{'comment': 'Do we want this to be a per-table property, or a global property?', 'commenter': 'ctubbsii'}, {'comment': 'I thought a per table propery might be useful, but perhaps not.  I can go either way on this one.', 'commenter': 'ivakegg'}, {'comment': '@ctubbsii Is there a reason so make this a _general_ property versus a _tserver_ property?  We are talking about tserver locations so that would have been be my first inclination.', 'commenter': 'ivakegg'}, {'comment': 'A tserver property would make sense.', 'commenter': 'ctubbsii'}]"
3139,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -851,6 +851,14 @@ public enum Property {
           + "" the tablet server this duration to revive before reassigning its tablets""
           + "" to other tablet servers.""),
 
+  TABLE_LOCATION_MODE(""table.location.mode"", ""locality"", PropertyType.LAST_LOCATION_MODE,
+      ""Describes how the system will assign tablets initially.""
+          + "" If 'locality' is the mode, then the system will assign tablets based on the data locality (e.g. the last major compaction location).""
+          + "" If 'assignment' is the mode, then tablets will be initially assigned to the last place they were assigned which could be""
+          + "" different then where they were last compacted given balancing activities.""
+          + "" Also note that master.startup.tserver properties might need to be set as well to ensure""","[{'comment': 'Note on merging this forward into 2.1: the term `master` has changed to `manager` in newer versions, and this should be updated when this is merged forward.\r\n\r\n```suggestion\r\n          + "" Also note that master.startup.tserver properties might need to be set as well to ensure""\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'of course', 'commenter': 'ivakegg'}]"
3139,server/base/src/main/java/org/apache/accumulo/server/master/state/MetaDataStateStore.java,"@@ -139,6 +140,11 @@ public void suspend(Collection<TabletLocationState> tablets,
       for (TabletLocationState tls : tablets) {
         Mutation m = new Mutation(tls.extent.getMetadataEntry());
         if (tls.current != null) {
+          // if the location more is assignment, then preserve the current location in the last
+          // location value
+          if (""assignment"".equals(context.getConfiguration().get(Property.TABLE_LOCATION_MODE))) {
+            tls.current.putLastLocation(m);
+          }","[{'comment': 'This is definitely reading it from the system configuration, not the per-table configuration, so it should be a `general.` property, not a `table.` one.', 'commenter': 'ctubbsii'}, {'comment': 'oh, did not realize that.  ok.', 'commenter': 'ivakegg'}]"
3139,server/base/src/main/java/org/apache/accumulo/server/util/MasterMetadataUtil.java,"@@ -319,21 +324,30 @@ private static void updateRootTabletDataFile(KeyExtent extent, FileRef path, Fil
    *
    * @return A Mutation to update a tablet from the given information
    */
-  private static Mutation getUpdateForTabletDataFile(KeyExtent extent, FileRef path,
-      FileRef mergeFile, DataFileValue dfv, String time, Set<FileRef> filesInUseByScans,
-      String address, ZooLock zooLock, Set<String> unusedWalLogs, TServerInstance lastLocation,
-      long flushId) {
+  private static Mutation getUpdateForTabletDataFile(ClientContext context, KeyExtent extent,
+      FileRef path, FileRef mergeFile, DataFileValue dfv, String time,
+      Set<FileRef> filesInUseByScans, String address, ZooLock zooLock, Set<String> unusedWalLogs,
+      TServerInstance lastLocation, long flushId) {
     Mutation m = new Mutation(extent.getMetadataEntry());
 
     if (dfv.getNumEntries() > 0) {
       m.put(DataFileColumnFamily.NAME, path.meta(), new Value(dfv.encode()));
       TabletsSection.ServerColumnFamily.TIME_COLUMN.put(m, new Value(time.getBytes(UTF_8)));
-      // stuff in this location
-      TServerInstance self = getTServerInstance(address, zooLock);
-      self.putLastLocation(m);
-      // erase the old location
-      if (lastLocation != null && !lastLocation.equals(self))
-        lastLocation.clearLastLocation(m);
+
+      // if the location mode is 'locality'', then preserve the current compaction location in the
+      // last location value
+      if (""locality"".equals(context.getConfiguration().get(Property.TABLE_LOCATION_MODE))) {
+        // stuff in this location
+        TServerInstance self = getTServerInstance(address, zooLock);
+        self.putLastLocation(m);
+        // erase the old location
+        if (lastLocation != null && !lastLocation.equals(self))
+          // @TODO: this does not make any sense to me as this will simply clear out the location we
+          // just set a couple lines previously
+          // clearLastLocation will clear the ""last"" location no matter what the value is
+          lastLocation.clearLastLocation(m);","[{'comment': 'This is setting the last location on a completely different tserver. It\'s updating the mutation by putting the new tserver as the last location, and clearing the previous tserver as the last location. One is a put, and one is a delete, on two separate metadata columns. They have the same column family (""last""), but different column qualifiers (the tserver hostname). We check that they aren\'t the same, so we\'re not adding a `delete` to the same mutation that we just added a `put` for the same column.', 'commenter': 'ctubbsii'}, {'comment': 'well crud.  I did not read the code correctly.  thanks.', 'commenter': 'ivakegg'}]"
3142,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -274,6 +274,13 @@ public enum Property {
       PropertyType.BOOLEAN, ""Enables JVM metrics functionality using Micrometer"", ""2.1.0""),
   GENERAL_MICROMETER_FACTORY(""general.micrometer.factory"", """", PropertyType.CLASSNAME,
       ""Name of class that implements MeterRegistryFactory"", ""2.1.0""),
+  GENERAL_LOCATION_MODE(""general.location.mode"", ""locality"", PropertyType.LAST_LOCATION_MODE,","[{'comment': '@ctubbsii Should this be a general property or a tserver property?', 'commenter': 'ivakegg'}, {'comment': 'So it appears that a general property cannot be set in zookeeper.  I am leaning towards changing this to a tserver property unless you have a reason why not to.', 'commenter': 'ivakegg'}, {'comment': ""You're right. tserver would make more sense"", 'commenter': 'ctubbsii'}]"
3142,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -726,6 +726,14 @@ public enum Property {
       ""The number of threads on each tablet server available to retrieve""
           + "" summary data, that is not currently in cache, from RFiles."",
       ""2.0.0""),
+  TSERV_LAST_LOCATION_MODE(""tserver.last.location.mode"", ""compact"", PropertyType.LAST_LOCATION_MODE,
+      ""Describes how the system will assign tablets initially by defining how the 'last' location is updated.""","[{'comment': '```suggestion\r\n      ""Describes how the system will record the \'last\' location for tablets, which can be used for assigning them when a cluster restarts.""\r\n```', 'commenter': 'ctubbsii'}]"
3142,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -726,6 +726,14 @@ public enum Property {
       ""The number of threads on each tablet server available to retrieve""
           + "" summary data, that is not currently in cache, from RFiles."",
       ""2.0.0""),
+  TSERV_LAST_LOCATION_MODE(""tserver.last.location.mode"", ""compact"", PropertyType.LAST_LOCATION_MODE,
+      ""Describes how the system will assign tablets initially by defining how the 'last' location is updated.""
+          + "" If 'compact' is the mode, then the system will assign tablets based on the data locality (e.g. the last compaction location).""","[{'comment': '```suggestion\r\n          + "" If \'compact\' is the mode, then the system will record the location where the tablet\'s most recent compaction occurred.""\r\n```', 'commenter': 'ctubbsii'}]"
3142,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -726,6 +726,14 @@ public enum Property {
       ""The number of threads on each tablet server available to retrieve""
           + "" summary data, that is not currently in cache, from RFiles."",
       ""2.0.0""),
+  TSERV_LAST_LOCATION_MODE(""tserver.last.location.mode"", ""compact"", PropertyType.LAST_LOCATION_MODE,
+      ""Describes how the system will assign tablets initially by defining how the 'last' location is updated.""
+          + "" If 'compact' is the mode, then the system will assign tablets based on the data locality (e.g. the last compaction location).""
+          + "" If 'assign' is the mode, then tablets will be initially assigned to the last place they were assigned.""","[{'comment': '```suggestion\r\n          + "" If \'assign\' is the mode, then the most recently assigned location will be recorded.""\r\n```', 'commenter': 'ctubbsii'}]"
3142,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -726,6 +726,14 @@ public enum Property {
       ""The number of threads on each tablet server available to retrieve""
           + "" summary data, that is not currently in cache, from RFiles."",
       ""2.0.0""),
+  TSERV_LAST_LOCATION_MODE(""tserver.last.location.mode"", ""compact"", PropertyType.LAST_LOCATION_MODE,
+      ""Describes how the system will assign tablets initially by defining how the 'last' location is updated.""
+          + "" If 'compact' is the mode, then the system will assign tablets based on the data locality (e.g. the last compaction location).""
+          + "" If 'assign' is the mode, then tablets will be initially assigned to the last place they were assigned.""
+          + "" If 'unload' is the mode, then tablets will be initially assigned to the last place they were unloaded from (i.e. requires a clean shutdown).""","[{'comment': '```suggestion\r\n          + "" If \'unload\' is the mode, then tablets will be initially assigned to the last place they were unloaded from (i.e. requires a clean shutdown).""\r\n```\r\n\r\nI\'m not sure we need this one at all. This would require a clean shut down in order to freeze the current assignments. And, in the case of that clean shut down, this would be no different than the `assign` mode. If there is not a clean shutdown, then the field would merely be blank... which isn\'t helpful.', 'commenter': 'ctubbsii'}, {'comment': 'ok, that is reasonable argument.  I was briefly thinking that one would want only updating of the last location on unload.  This would allow a scan of the accumulo metadata to see what the discrepencies between the last and current locations.', 'commenter': 'ivakegg'}]"
3142,server/base/src/main/java/org/apache/accumulo/server/manager/state/MetaDataStateStore.java,"@@ -83,25 +91,30 @@ public void setFutureLocations(Collection<Assignment> assignments)
   }
 
   @Override
-  public void unassign(Collection<TabletLocationState> tablets,
+  public void unassign(ServerContext context, Collection<TabletLocationState> tablets,","[{'comment': ""I think we probably don't need to keep passing in ServerContext to each instance method if we just add it to the constructor."", 'commenter': 'ctubbsii'}, {'comment': 'I did that because the ZooTabletStateStore did not have access to the context as it was not constructed as such.', 'commenter': 'ivakegg'}, {'comment': 'Updated to construct the ZooTabletStateStore with the context.  Reset the TabletStateStore interface to what it was originally.', 'commenter': 'ivakegg'}]"
3142,server/base/src/main/java/org/apache/accumulo/server/manager/state/ZooTabletStateStore.java,"@@ -134,6 +138,18 @@ public void setLocations(Collection<Assignment> assignments) throws DistributedS
 
     TabletMutator tabletMutator = ample.mutateTablet(assignment.tablet);
     tabletMutator.putLocation(assignment.server, LocationType.CURRENT);
+    if (""assignment"".equals(context.getConfiguration().get(Property.TSERV_LAST_LOCATION_MODE))) {
+      TabletMetadata lastMetadata =","[{'comment': 'This may not be needed in a zoo tablet state store as I believe there may be and only 1 possible last location.', 'commenter': 'ivakegg'}, {'comment': 'nevermind.  It appears that we are storing all cf, cq, and values now as a json blob.', 'commenter': 'ivakegg'}]"
3142,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -726,6 +726,14 @@ public enum Property {
       ""The number of threads on each tablet server available to retrieve""
           + "" summary data, that is not currently in cache, from RFiles."",
       ""2.0.0""),
+  TSERV_LAST_LOCATION_MODE(""tserver.last.location.mode"", ""compaction"",
+      PropertyType.LAST_LOCATION_MODE,
+      ""Describes how the system will record the 'last' location for tablets, which can be used for assigning them when a cluster restarts.""
+          + "" If 'compaction' is the mode, then the system will record the location where the tablet's most recent compaction occurred.""
+          + "" If 'assignment' is the mode, then the most recently assigned location will be recorded.""
+          + "" Also note that manger.startup.tserver properties might need to be set as well to ensure""","[{'comment': '```suggestion\r\n          + "" The manager.startup.tserver properties might also need to be set to ensure""\r\n```', 'commenter': 'ctubbsii'}]"
3142,server/base/src/main/java/org/apache/accumulo/server/manager/state/MetaDataStateStore.java,"@@ -60,9 +62,24 @@ public ClosableIterator<TabletLocationState> iterator() {
   public void setLocations(Collection<Assignment> assignments) throws DistributedStoreException {
     try (var tabletsMutator = ample.mutateTablets()) {
       for (Assignment assignment : assignments) {
-        tabletsMutator.mutateTablet(assignment.tablet)
-            .putLocation(assignment.server, LocationType.CURRENT)
-            .deleteLocation(assignment.server, LocationType.FUTURE).deleteSuspension().mutate();
+        TabletMutator mutation = tabletsMutator.mutateTablet(assignment.tablet);","[{'comment': ""I would just change this variable name, so it's not confused for a `Mutation` type object, and so it's consistent with the other places we do this.\r\n\r\n```suggestion\r\n        TabletMutator tabletMutator = tabletsMutator.mutateTablet(assignment.tablet);\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'noted.  I will do that.', 'commenter': 'ivakegg'}]"
3142,server/base/src/main/java/org/apache/accumulo/server/manager/state/MetaDataStateStore.java,"@@ -102,6 +119,21 @@ private void unassign(Collection<TabletLocationState> tablets,
       for (TabletLocationState tls : tablets) {
         TabletMutator tabletMutator = tabletsMutator.mutateTablet(tls.extent);
         if (tls.current != null) {
+          // if the location mode is assignment, then preserve the current location in the last
+          // location value
+          if (""assignment""
+              .equals(context.getConfiguration().get(Property.TSERV_LAST_LOCATION_MODE))) {
+            TabletMetadata lastMetadata =
+                ample.readTablet(tls.extent, TabletMetadata.ColumnType.LAST);
+            if (lastMetadata != null && lastMetadata.getLast() != null) {
+              if (!lastMetadata.getLast().equals(tls.current)) {
+                tabletMutator.putLocation(tls.current, LocationType.LAST);
+                tabletMutator.deleteLocation(lastMetadata.getLast(), LocationType.LAST);
+              }
+            } else {
+              tabletMutator.putLocation(tls.current, LocationType.LAST);
+            }
+          }","[{'comment': ""If it's always updated when it is assigned, do you need to record it again here?"", 'commenter': 'ctubbsii'}, {'comment': 'Yes I do.  This will allow me to change from compaction to assignment on a running system and still get the benefit of the last locations being updated appropriately when the system is shutdown.', 'commenter': 'ivakegg'}]"
3142,server/base/src/main/java/org/apache/accumulo/server/manager/state/ZooTabletStateStore.java,"@@ -134,6 +138,18 @@ public void setLocations(Collection<Assignment> assignments) throws DistributedS
 
     TabletMutator tabletMutator = ample.mutateTablet(assignment.tablet);
     tabletMutator.putLocation(assignment.server, LocationType.CURRENT);
+    if (""assignment"".equals(context.getConfiguration().get(Property.TSERV_LAST_LOCATION_MODE))) {
+      TabletMetadata lastMetadata =
+          ample.readTablet(assignment.tablet, TabletMetadata.ColumnType.LAST);
+      if (lastMetadata != null && lastMetadata.getLast() != null) {
+        if (!lastMetadata.getLast().equals(assignment.server)) {
+          tabletMutator.putLocation(assignment.server, LocationType.LAST);
+          tabletMutator.deleteLocation(lastMetadata.getLast(), LocationType.LAST);
+        }
+      } else {
+        tabletMutator.putLocation(assignment.server, LocationType.LAST);
+      }
+    }","[{'comment': ""I wonder if this whole block can be put into a method that's common to both state stores. They differ very little."", 'commenter': 'ctubbsii'}, {'comment': 'That I can do.  I will work that today.', 'commenter': 'ivakegg'}, {'comment': 'I refactored the location updating into methods in the ManagerMetadataUtil class.  If you want them located elsewhere or in their own class just let me know.', 'commenter': 'ivakegg'}]"
3142,server/base/src/main/java/org/apache/accumulo/server/util/ManagerMetadataUtil.java,"@@ -205,11 +206,15 @@ public static void replaceDatafiles(ServerContext context, KeyExtent extent,
     }
 
     TServerInstance self = getTServerInstance(address, zooLock);
-    tablet.putLocation(self, LocationType.LAST);
+    // if the location mode is 'locality'', then preserve the current compaction location in the","[{'comment': ""```suggestion\r\n    // if the location mode is 'compaction', then preserve the current compaction location in the\r\n```"", 'commenter': 'ctubbsii'}]"
3142,server/base/src/main/java/org/apache/accumulo/server/util/ManagerMetadataUtil.java,"@@ -240,11 +245,15 @@ public static Optional<StoredTabletFile> updateTabletDataFile(ServerContext cont
       newFile = Optional.of(newDatafile.insert());
 
       TServerInstance self = getTServerInstance(address, zooLock);
-      tablet.putLocation(self, LocationType.LAST);
-
-      // remove the old location
-      if (lastLocation != null && !lastLocation.equals(self)) {
-        tablet.deleteLocation(lastLocation, LocationType.LAST);
+      // if the location mode is 'locality'', then preserve the current compaction location in the","[{'comment': ""```suggestion\r\n      // if the location mode is 'compaction', then preserve the current compaction location in the\r\n```"", 'commenter': 'ctubbsii'}]"
3142,test/src/main/java/org/apache/accumulo/test/functional/CompactLocationModeIT.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertNotNull;
+import static org.junit.jupiter.api.Assertions.assertNull;
+
+import java.time.Duration;
+
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.security.tokens.PasswordToken;
+import org.apache.accumulo.core.clientImpl.ClientContext;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.MetadataTable;
+import org.apache.accumulo.core.metadata.TabletLocationState;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection;
+import org.apache.accumulo.core.util.UtilWaitThread;
+import org.apache.accumulo.server.manager.state.MetaDataTableScanner;
+import org.junit.jupiter.api.Test;
+
+public class CompactLocationModeIT extends ConfigurableMacBase {","[{'comment': 'Since you\'re extending ConfigurableMacBase, you should probably implement the configure method. It would be good to set the mode to ""compaction"" explicitly in that method, in case the default ever changes.', 'commenter': 'ctubbsii'}, {'comment': 'ok, that is reasonable.  Thanks.', 'commenter': 'ivakegg'}]"
3142,server/base/src/main/java/org/apache/accumulo/server/util/ManagerMetadataUtil.java,"@@ -256,4 +247,64 @@ public static Optional<StoredTabletFile> updateTabletDataFile(ServerContext cont
     tablet.mutate();
     return newFile;
   }
+
+  /**
+   * Update the last location if the location mode is ""assignment"". This will delete the previous
+   * last location if needed and set the new last location
+   *
+   * @param context The server context
+   * @param ample The metadata persistence layer
+   * @param tabletMutator The mutator being built
+   * @param extent The tablet extent
+   * @param location The new location
+   */
+  public static void updateLastForAssignmentMode(ClientContext context, Ample ample,
+      Ample.TabletMutator tabletMutator, KeyExtent extent, TServerInstance location) {
+    // if the location mode is assignment, then preserve the current location in the last
+    // location value
+    if (""assignment"".equals(context.getConfiguration().get(Property.TSERV_LAST_LOCATION_MODE))) {
+      TabletMetadata lastMetadata = ample.readTablet(extent, TabletMetadata.ColumnType.LAST);
+      TServerInstance lastLocation = (lastMetadata == null ? null : lastMetadata.getLast());
+      ManagerMetadataUtil.updateLast(tabletMutator, lastLocation, location);
+    }
+  }
+
+  /**
+   * Update the last location if the location mode is ""compaction"". This will delete the previous
+   * last location if needed and set the new last location
+   *
+   * @param context The server context
+   * @param tabletMutator The mutator being built
+   * @param lastLocation The last location
+   * @param address The server address
+   * @param zooLock The zookeeper lock
+   */
+  public static void updateLastForCompactionMode(ClientContext context, TabletMutator tabletMutator,
+      TServerInstance lastLocation, String address, ServiceLock zooLock) {
+    // if the location mode is 'compaction', then preserve the current compaction location in the
+    // last location value
+    if (""compaction"".equals(context.getConfiguration().get(Property.TSERV_LAST_LOCATION_MODE))) {
+      TServerInstance newLocation = getTServerInstance(address, zooLock);
+      updateLast(tabletMutator, lastLocation, newLocation);
+    }
+  }
+
+  /**
+   * Update the last location, deleting the previous location if needed
+   *
+   * @param tabletMutator The mutator being built
+   * @param lastLocation The last location (may be null)
+   * @param newLocation The new location
+   */
+  public static void updateLast(TabletMutator tabletMutator, TServerInstance lastLocation,
+      TServerInstance newLocation) {
+    if (lastLocation != null) {
+      if (!lastLocation.equals(newLocation)) {
+        tabletMutator.deleteLocation(lastLocation, LocationType.LAST);
+        tabletMutator.putLocation(newLocation, LocationType.LAST);
+      }
+    } else {
+      tabletMutator.putLocation(newLocation, LocationType.LAST);
+    }","[{'comment': 'could be simplified:\r\n```suggestion\r\n    if (lastLocation != null && !lastLocation.equals(newLocation)) {\r\n      tabletMutator.deleteLocation(lastLocation, LocationType.LAST);\r\n    }\r\n    tabletMutator.putLocation(newLocation, LocationType.LAST);\r\n```', 'commenter': 'dlmarion'}, {'comment': 'But I did not want to ""putLocation"" if the lastLocation is equal to the newLocation.  This suggestion is too simple.', 'commenter': 'ivakegg'}]"
3151,core/src/main/java/org/apache/accumulo/core/data/Value.java,"@@ -181,6 +182,15 @@ public void readFields(final DataInput in) throws IOException {
     in.readFully(this.value, 0, this.value.length);
   }
 
+  public void readFields(final DataInput in, Supplier<Long> freeMemorySupplier) throws IOException {
+    int length = in.readInt();
+    while (freeMemorySupplier.get() < length) {
+      Thread.onSpinWait();
+    }
+    this.value = new byte[length];","[{'comment': ""I haven't fully thought about this approach yet in terms of the consequences of trying to wait for free memory as I've never tried to do it before but one thing that stands out to me is that there seems like there's always going to be a race condition here. Even if you wait and get the ok that there is memory to proceed by the time you continue and get to this line and go to allocate memory for the byte array there's no guarantee the JVM hasn't already allocated some of the memory you'd need for this to work in another thread. In fact, if you were right on the edge and barely had enough memory then I would think this could fail often with the JVM allocating memory in other threads. In practice though I don't know if that's true without testing but it seems a bit non deterministic since you don't have much control over the GC process and what other threads are doing."", 'commenter': 'cshannon'}, {'comment': ""I agree that it's not a perfect solution, it's an attempt at doing something. There are other options that could be added, like only check for free memory if the length is over some size. The problem we run into is that the tablet server is running N scans concurrently, JVM memory is under pressure, and it doesn't know that the next K/V it's going to read is going to create a very large (500MB, 1GB) Value object."", 'commenter': 'dlmarion'}, {'comment': ""Maybe we need to have some configurable size to give us a bit of a buffer that we add to the check that could help reduce risk of failures due to race conditions. So instead of just continuing if `free_memory` is large enough, we don't continue until `free_memory + some_configurable_size` (could be a meg, 10 megs, whatever) of memory is available to reduce the risk of failures."", 'commenter': 'cshannon'}, {'comment': ""> There are other options that could be added, like only check for free memory if the length is over some size.\r\n> So instead of just continuing if free_memory is large enough, we don't continue until free_memory + some_configurable_size\r\n\r\nI added both of these things in 0177dbd. The default size threshold to perform the free memory check is 5% of max memory. The buffer size is 2% of the Value size."", 'commenter': 'dlmarion'}]"
3151,core/src/main/java/org/apache/accumulo/core/file/rfile/RFile.java,"@@ -84,6 +95,107 @@
 
 public class RFile {
 
+  private static class RFileMemoryProtection implements NotificationListener {
+
+    private static class FreeMemoryUpdater implements Runnable {
+
+      private final ReentrantLock lock = new ReentrantLock();
+      private final AtomicReference<CountDownLatch> latchRef =
+          new AtomicReference<>(new CountDownLatch(1));
+
+      public void update() {
+        lock.lock();
+        try {
+          latchRef.get().countDown();
+        } finally {
+          lock.unlock();
+        }
+      }
+
+      @Override
+      public void run() {
+        try {
+          while (true) {
+            CountDownLatch latch = latchRef.get();
+            // TODO: Could use latch.await(long, TimeUnit) to update free memory
+            // when GC does not occur. It's probable that memory allocations
+            // will occur without GC happening, depending on the memory pool
+            // sizes.
+            latch.await();
+            // acquiring a lock here so that we don't miss a call to update() while
+            // we are getting the current free memory
+            lock.lock();
+            try {
+              FREE_MEMORY.set(Runtime.getRuntime().freeMemory());","[{'comment': '```suggestion\r\n              // Using only runtime.freeMemory() here would give you memory that is\r\n              // definitely available but may not be the amount of memory before an\r\n              // OME occurs which is really the goal. This is due to the fact that\r\n              // freeMemory() will only return the available memory from the currently\r\n              // allocated memory but the JVM may be able to allocate more memory if\r\n              // the -Xmx parameter is used so the true estimated available free memory\r\n              // is actually (maxMemory - allocatedMemory) where allocatedMemory is\r\n              // (totalMemory - freeMemory).\r\n              // See https://stackoverflow.com/questions/12807797/java-get-available-memory\r\n              // for more detailed discussion\r\n              final Runtime runtime = Runtime.getRuntime();\r\n              final long allocatedMemory = runtime.totalMemory() - runtime.freeMemory();\r\n              FREE_MEMORY.set(runtime.maxMemory() - allocatedMemory);\r\n```', 'commenter': 'cshannon'}, {'comment': ""I did a little more research into this topic today and I found some interesting discussion here: https://stackoverflow.com/questions/12807797/java-get-available-memory\r\n\r\nThe most intriguing part to me is that we should not try and use `Runtime.getRuntime().freeMemory()` but instead to use `Runtime.getRuntime().maxMemory() - allocatedMemory` where `allocatedMemory` is `Runtime.getRuntime().totalMemory() - runtime.freeMemory()`\r\n\r\nThe reasoning being if you use the `-Xmx` property then memory is allocated in chunks so using `Runtime.getRuntime().freeMemory()` may not actually return all the free memory.\r\n\r\nRegardless, no matter what it is used, it's going to just be an estimation plus there's the race condition issue so definitely keeping a buffer size to account for the estimation error, etc that you added is a good idea."", 'commenter': 'cshannon'}]"
3151,test/src/main/java/org/apache/accumulo/test/functional/ReadLargeKVIT.java,"@@ -0,0 +1,167 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.Assert.assertNull;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicReference;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.BatchWriterConfig;
+import org.apache.accumulo.core.client.IteratorSetting;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.client.admin.TableOperations;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.harness.MiniClusterConfigurationCallback;
+import org.apache.accumulo.harness.SharedMiniClusterBase;
+import org.apache.accumulo.minicluster.MemoryUnit;
+import org.apache.accumulo.minicluster.ServerType;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.accumulo.test.functional.SlowMemoryConsumingIterator.SlowMemoryConsumingWaitingIterator;
+import org.junit.jupiter.api.AfterAll;
+import org.junit.jupiter.api.BeforeAll;
+import org.junit.jupiter.api.Test;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Iterables;
+
+public class ReadLargeKVIT extends SharedMiniClusterBase {
+
+  private static class ReadLargeKVITConfiguration implements MiniClusterConfigurationCallback {
+
+    @Override
+    public void configureMiniCluster(MiniAccumuloConfigImpl cfg,
+        org.apache.hadoop.conf.Configuration coreSite) {
+      cfg.setNumTservers(1);
+      cfg.setMemory(ServerType.TABLET_SERVER, 256, MemoryUnit.MEGABYTE);
+      Map<String,String> sysProps = new HashMap<>();
+      // Enable RFileMemoryProtection for any Value
+      // over 10MB in size.
+      sysProps.put(""EnableRFileMemoryProtection"", ""true"");
+      sysProps.put(""RFileMemoryProtectionSizeThreshold"", Integer.toString(10 * 1024 * 1024));
+      cfg.setSystemProperties(sysProps);
+    }
+  }
+
+  @BeforeAll
+  public static void start() throws Exception {
+    startMiniClusterWithConfig(new ReadLargeKVITConfiguration());
+  }
+
+  @AfterAll
+  public static void stop() throws Exception {
+    stopMiniCluster();
+  }
+
+  @Test
+  public void test() throws Exception {
+
+    final int NUM_ROWS = 10;
+    byte[] EMPTY = new byte[0];
+    byte[] VALUE = new byte[11 * 1024 * 1024];
+    Arrays.fill(VALUE, (byte) '1');
+
+    final AtomicReference<Throwable> ERROR = new AtomicReference<>();
+    String table = getUniqueNames(1)[0];
+
+    Thread t = new Thread(() -> {
+      try (AccumuloClient client2 = Accumulo.newClient().from(getClientProps()).build()) {
+        try (Scanner scanner2 = client2.createScanner(table)) {
+          IteratorSetting is = new IteratorSetting(11, SlowMemoryConsumingIterator.class,
+              Map.of(""sleepTime"", ""30000""));
+          scanner2.addScanIterator(is);
+          Iterator<Entry<Key,Value>> iter2 = scanner2.iterator();
+          iter2.next();
+          if (!iter2.hasNext()) {
+            throw new RuntimeException(""hasNext == false"");
+          }
+          LoggerFactory.getLogger(ReadLargeKVIT.class).info(""Reached the end"");
+        } catch (Exception e) {
+          LoggerFactory.getLogger(ReadLargeKVIT.class).error(""Error in thread"", e);
+          e.printStackTrace();
+          ERROR.set(e);
+        }
+      }
+    });
+
+    try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {
+
+      TableOperations to = client.tableOperations();
+      to.create(table);
+
+      // Insert several K/V pairs with 11MB values
+      BatchWriterConfig bwc = new BatchWriterConfig();
+      bwc.setMaxMemory(24 * 1024 * 1024);
+      try (BatchWriter writer = client.createBatchWriter(table, bwc)) {
+        for (int i = 0; i < NUM_ROWS; i++) {
+          Mutation m = new Mutation(""test"" + i);
+          m.put(EMPTY, EMPTY, VALUE);
+          writer.addMutation(m);
+        }
+      }
+
+      // Validate NUM_ROWS in table
+      long start = System.currentTimeMillis();
+      try (Scanner scanner = client.createScanner(table)) {
+        assertEquals(10, Iterables.size(scanner));
+      }
+      long firstRunTime = System.currentTimeMillis() - start;
+
+      t.start();
+
+      long start2 = System.currentTimeMillis();
+      try (Scanner scanner3 = client.createScanner(table)) {
+        // SlowMemoryConsumingIterator is set to sleep for 30s
+        // and consume all free memory except for 10MB. In theory,
+        // getting the first K/V pair should take close to 30s.
+        scanner3.setBatchSize(1);
+        scanner3.setReadaheadThreshold(1);
+        scanner3.setBatchTimeout(1, TimeUnit.MINUTES);
+        IteratorSetting is =
+            new IteratorSetting(11, SlowMemoryConsumingWaitingIterator.class, Map.of());
+        scanner3.addScanIterator(is);
+        Iterator<Entry<Key,Value>> iter = scanner3.iterator();
+        assertTrue(iter.hasNext());
+        Entry<Key,Value> e = iter.next();
+        assertEquals(VALUE.length, e.getValue().get().length);
+      }
+      long secondRunTime = System.currentTimeMillis() - start2;
+
+      assertNull(""Error in background thread"", ERROR.get());","[{'comment': 'These parameters will probably need to be swapped. With Junits assert statements, I think the first param should be the Object that you want to make sure is null and the second param should be the error message.', 'commenter': 'DomGarguilo'}]"
3152,core/src/main/java/org/apache/accumulo/core/spi/fs/RandomVolumeChooser.java,"@@ -21,18 +21,30 @@
 import java.security.SecureRandom;
 import java.util.Set;
 
+import org.apache.accumulo.core.conf.Property;
+
 /**
+ * A {@link VolumeChooser} that selects a volume at random from the list of provided volumes. This
+ * class is currently the default volume chooser as set by {@link Property#GENERAL_VOLUME_CHOOSER}.","[{'comment': '```suggestion\r\n * A {@link VolumeChooser} that selects a volume at random from the list of provided volumes. \r\n```\r\n\r\nIf the default ever changes, then we have to remember to update this comment. Users can just check the default value of the property.', 'commenter': 'dlmarion'}, {'comment': 'Sounds good! Changes implemented', 'commenter': 'ddanielr'}]"
3160,server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java,"@@ -47,18 +47,6 @@ public class AccumuloDataVersion {
    */
   public static final int ROOT_TABLET_META_CHANGES = 10;
 
-  /**
-   * version (9) reflects changes to crypto that resulted in RFiles and WALs being serialized
-   * differently in version 2.0.0. Also RFiles in 2.0.0 may have summary data.
-   */
-  public static final int CRYPTO_CHANGES = 9;
-
-  /**
-   * version (8) reflects changes to RFile index (ACCUMULO-1124) AND the change to WAL tracking in
-   * ZK in version 1.8.0
-   */
-  public static final int SHORTEN_RFILE_KEYS = 8;
-","[{'comment': 'The javadocs should be moved to the ""historic data versions"" section below.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in a58a977761,', 'commenter': 'EdColeman'}]"
3160,server/base/src/test/java/org/apache/accumulo/server/ServerContextTest.java,"@@ -135,7 +135,8 @@ public void testCanRun() {
     // ensure this fails with older versions; the oldest supported version is hard-coded here
     // to ensure we don't unintentionally break upgrade support; changing this should be a conscious
     // decision and this check will ensure we don't overlook it
-    final int oldestSupported = 8;
+    // as of 3.0 we will only support upgrades from 2.1
+    final int oldestSupported = AccumuloDataVersion.ROOT_TABLET_META_CHANGES;","[{'comment': ""I'm not sure this comment expresses anything that the next line doesn't already express more clearly. As we move forward, the version names in this comment will get stale over time and the comment will need to be constantly updated. I'm not sure it's any more helpful than having the explicit assignment line right after."", 'commenter': 'ctubbsii'}, {'comment': 'The comment was pre-existing and I found it helpful - it stopped me in my tracks to make sure that removing the previous versions was what was intended.  Removing upgrade versions should be rare and having it explicitly stated here served as a good reminder / check.', 'commenter': 'EdColeman'}, {'comment': ""I don't mean the whole comment... most of the comment is fine... I just don't think it's useful to hard-code the version numbers into the comment, because that's not really adding anything that the rest of the comment doesn't already have... but by hard-coding these version names into the comment, it is making it more that we have to change when we move forward, in order to keep the comment current. I just think that's creating unnecessary work for ourselves, and in this case, I don't think it makes it any more clear than what was there before."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 70df59f3c7', 'commenter': 'EdColeman'}]"
3160,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -142,11 +140,14 @@ public synchronized void upgradeZookeeper(ServerContext context,
         ""Not currently in a suitable state to do zookeeper upgrade %s"", status);
 
     try {
-      int cv = context.getServerDirs()
-          .getAccumuloPersistentVersion(context.getVolumeManager().getFirst());
-      ServerContext.ensureDataVersionCompatible(cv);
+      int cv = AccumuloDataVersion.getCurrentVersion(context);
       this.currentVersion = cv;
 
+      if (cv < AccumuloDataVersion.ROOT_TABLET_META_CHANGES) {
+        throw new UnsupportedOperationException(
+            ""Upgrading from a version before 2.1 is not supported. Upgrade to 2.1 before upgrading to 3.x"");
+      }
+","[{'comment': ""Isn't this already enforced elsewhere? This seems like a duplicate check that we'll need to maintain in more places. We should already be comparing against the `oldestSupported` field somewhere."", 'commenter': 'ctubbsii'}, {'comment': 'Previously it was checked in each upgrader - the check here fails faster and provided a clearer message than if the upgradeder fail the version checks - failing with a message that upgrader for data version X is accurate, but may not translate to the user how data versions and Accumulo versions correlate.  \r\n\r\nOn the positive side - this provides a clear user message at the expense of needing to change the message if upgrades change - but that should be rare and I favored a better user experience.', 'commenter': 'EdColeman'}, {'comment': '> Previously it was checked in each upgrader - the check here fails faster and provided a clearer message than if the upgradeder fail the version checks - failing with a message that upgrader for data version X is accurate, but may not translate to the user how data versions and Accumulo versions correlate.\r\n> \r\n> On the positive side - this provides a clear user message at the expense of needing to change the message if upgrades change - but that should be rare and I favored a better user experience.\r\n\r\nOkay, but if we want to tie the message to the version number, then we should have some mapping of data version to the Accumulo version they appeared in, so we don\'t have to update this line. The number of locations where we have to made one or two character changes to keep all these messages accurate should be few, and centralized. Something like:\r\n\r\n```suggestion\r\n      if (cv < oldestVersion) {\r\n        String oldRelease = dataVersionToReleaseName.get(oldestVersion);\r\n        throw new UnsupportedOperationException(\r\n            ""Upgrading from a version before "" + oldRelease + "" is not supported. Upgrade to at least "" + oldRelease + "" before upgrading to "" + Constants.VERSION);\r\n      }\r\n\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'addressed in 70df59f3c7', 'commenter': 'EdColeman'}]"
3160,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeChecker.java,"@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import java.util.List;
+import java.util.Objects;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.manager.EventCoordinator;
+import org.apache.accumulo.server.AccumuloDataVersion;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.ZKUtil;
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.ZooKeeper;
+import org.apache.zookeeper.data.ACL;
+import org.apache.zookeeper.data.Stat;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+public class UpgradeChecker {","[{'comment': ""What is this for? I wasn't expecting new code, since this task was to remove old code. This should have a javadoc to explain what it's for, at the least."", 'commenter': 'ctubbsii'}, {'comment': 'This is not so much new code as it is moving existing code to a different place.\r\n\r\nThe code to upgrade to 2.1 included the check for ZooKeeper ACLs and with the deletion of that code, this check would not be performed.  Being that it is unclear how the invalid ACLs are occurring, it seemed like something that could be checked on each upgrade and not pegged to a specific upgrade version.', 'commenter': 'EdColeman'}, {'comment': ""Okay, checking on upgrade seems reasonable. I think maybe just adding a javadoc comment and updating names/references to it to make it clear it's a check for ZooKeeper prerequisite for the upgrade. Right now, it feels bolted on to upgrade, rather than integrated as part of the upgrade process. Polishing it up with location/naming/comments could help make it a bit more integrated into the upgrade process itself."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in a58a977761 - Renamed the class to PreUpgradeValidation and added javadoc', 'commenter': 'EdColeman'}]"
3160,server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java,"@@ -47,22 +47,12 @@ public class AccumuloDataVersion {
    */
   public static final int ROOT_TABLET_META_CHANGES = 10;
 
-  /**
-   * version (9) reflects changes to crypto that resulted in RFiles and WALs being serialized
-   * differently in version 2.0.0. Also RFiles in 2.0.0 may have summary data.
-   */
-  public static final int CRYPTO_CHANGES = 9;
-
-  /**
-   * version (8) reflects changes to RFile index (ACCUMULO-1124) AND the change to WAL tracking in
-   * ZK in version 1.8.0
-   */
-  public static final int SHORTEN_RFILE_KEYS = 8;
-
   /**
    * Historic data versions
    *
    * <ul>
+   * <li>version (9) RFiles and wal crypto serialization changes. RFile summary data in 2.0.0</li>
+   * <li>version (8) RFile index (ACCUMULO-1124) and wal tracking in ZK</li>","[{'comment': '```suggestion\r\n   * <li>version (8) RFile index (ACCUMULO-1124) and wal tracking in ZK in 1.8.0</li>\r\n```', 'commenter': 'ctubbsii'}]"
3160,server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java,"@@ -81,6 +71,28 @@ public static int get() {
     return CURRENT_VERSION;
   }
 
-  public static final Set<Integer> CAN_RUN =
-      Set.of(SHORTEN_RFILE_KEYS, CRYPTO_CHANGES, ROOT_TABLET_META_CHANGES, CURRENT_VERSION);
+  public static final Set<Integer> CAN_RUN = Set.of(ROOT_TABLET_META_CHANGES, CURRENT_VERSION);
+
+  /**
+   * Get the stored, current working version.
+   *
+   * @param context the server context
+   * @return the stored data version
+   */
+  public static int getCurrentVersion(ServerContext context) {
+    int cv =
+        context.getServerDirs().getAccumuloPersistentVersion(context.getVolumeManager().getFirst());
+    ServerContext.ensureDataVersionCompatible(cv);
+    return cv;
+  }
+
+  public static String dataVersionToReleaseName(final int version) {
+    switch (version) {
+      case ROOT_TABLET_META_CHANGES:
+        return ""2.1"";
+      case REMOVE_DEPRECATIONS_FOR_VERSION_3:
+        return ""3.0"";","[{'comment': 'It\'s highly unlikely, but possible that a data version could change in a bugfix release. Also, because data version can span multiple minor versions, it may not make sense to only show the minor release here, as that could imply it only applies to versions matching `a.b.*`, when it might also apply to version `a.b+1.*` as well. So, instead, let\'s use the full name of the version of the first appearance of the change.\r\n\r\n```suggestion\r\n        return ""2.1.0"";\r\n      case REMOVE_DEPRECATIONS_FOR_VERSION_3:\r\n        return ""3.0.0"";\r\n```', 'commenter': 'ctubbsii'}]"
3160,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -107,10 +110,8 @@ public boolean isParentLevelUpgraded(KeyExtent extent) {
 
   private int currentVersion;
   // map of ""current version"" -> upgrader to next version.
-  private final Map<Integer,
-      Upgrader> upgraders = Map.of(AccumuloDataVersion.SHORTEN_RFILE_KEYS, new Upgrader8to9(),
-          AccumuloDataVersion.CRYPTO_CHANGES, new Upgrader9to10(),
-          AccumuloDataVersion.ROOT_TABLET_META_CHANGES, new Upgrader10to11());
+  private final Map<Integer,Upgrader> upgraders =
+      Map.of(ROOT_TABLET_META_CHANGES, new Upgrader10to11());","[{'comment': ""This might be a problem with our existing upgrader code, but `Map.of` does not guarantee ordering. Taken from [the javadoc for unmodifiable maps](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Map.html#unmodifiable): `The iteration order of mappings is unspecified and is subject to change`\r\n\r\nIn this case, it's a singleton map... but in previous versions, and in the future, it may not be. We should use a map that preserves the order, because these upgraders need to be executed in order, I believe."", 'commenter': 'ctubbsii'}, {'comment': ""Addressed in 7c8ec3ad63.  I'll work a patch for 2.1 (1.10 does not have the class) with only the ordering preserving and hard-coded value removal as a separate PR."", 'commenter': 'EdColeman'}]"
3160,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -142,11 +143,17 @@ public synchronized void upgradeZookeeper(ServerContext context,
         ""Not currently in a suitable state to do zookeeper upgrade %s"", status);
 
     try {
-      int cv = context.getServerDirs()
-          .getAccumuloPersistentVersion(context.getVolumeManager().getFirst());
-      ServerContext.ensureDataVersionCompatible(cv);
+      int cv = AccumuloDataVersion.getCurrentVersion(context);
       this.currentVersion = cv;
 
+      int oldestVersion = ROOT_TABLET_META_CHANGES;","[{'comment': 'Instead of having another place where this is hard-coded, can just get the first upgrader mapping key:\r\n\r\n```suggestion\r\n      int oldestVersion = upgraders.iterator().next().getKey();\r\n```\r\n\r\nHowever, this requires a predictable ordering of the upgraders... which we might already be relying on, and which might be an existing problem. See my other comment on this.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 7c8ec3ad63', 'commenter': 'EdColeman'}]"
3160,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -142,11 +143,17 @@ public synchronized void upgradeZookeeper(ServerContext context,
         ""Not currently in a suitable state to do zookeeper upgrade %s"", status);
 
     try {
-      int cv = context.getServerDirs()
-          .getAccumuloPersistentVersion(context.getVolumeManager().getFirst());
-      ServerContext.ensureDataVersionCompatible(cv);
+      int cv = AccumuloDataVersion.getCurrentVersion(context);
       this.currentVersion = cv;
 
+      int oldestVersion = ROOT_TABLET_META_CHANGES;
+      if (cv < oldestVersion) {
+        String oldRelease = dataVersionToReleaseName(oldestVersion);
+        throw new UnsupportedOperationException(""Upgrading from a version before "" + oldRelease","[{'comment': 'I think ""less than"" makes more sense here than ""before"", since ""before"" could mean chronologically, and we might release a bugfix for an earlier release line after the `.0` version we\'re expecting. But that bugfix\'s version would still be ""less than"" the `.0` version release.\r\n\r\n```suggestion\r\n        throw new UnsupportedOperationException(""Upgrading from a version less than "" + oldRelease\r\n```', 'commenter': 'ctubbsii'}]"
3160,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java,"@@ -142,11 +145,17 @@ public synchronized void upgradeZookeeper(ServerContext context,
         ""Not currently in a suitable state to do zookeeper upgrade %s"", status);
 
     try {
-      int cv = context.getServerDirs()
-          .getAccumuloPersistentVersion(context.getVolumeManager().getFirst());
-      ServerContext.ensureDataVersionCompatible(cv);
+      int cv = AccumuloDataVersion.getCurrentVersion(context);
       this.currentVersion = cv;
 
+      int oldestVersion = upgraders.entrySet().iterator().next().getKey();
+      if (cv < oldestVersion) {
+        String oldRelease = dataVersionToReleaseName(oldestVersion);
+        throw new UnsupportedOperationException(""Upgrading from a version less than "" + oldRelease
+            + "" data version ("" + oldestVersion + "") is not supported. Upgrade to at least ""
+            + oldRelease + "" before upgrading to "" + Constants.VERSION);
+      }
+","[{'comment': ""In thinking about this some more, I don't think this added message is ever going to work... this can only execute after the `UpgradeCoordinator` is instantiated... which won't happen until after the `Manager` is constructed... but the `Manager` won't successfully get constructed because `ServerContext.init` will fail long before this, when it runs `ServerContext.ensureDataVersionCompatible`, that checks `AccumuloDataVersion.CAN_RUN`.\r\n\r\nI think if you want to have this more informative message, you're going to need to edit `ServerContext.ensureDataVersionCompatible` instead and this added check can just be removed, as it's not doing anything useful here."", 'commenter': 'ctubbsii'}, {'comment': ""I'll need to check - but pretty sure that during development I had a bad version set and the errors in the logs prompted me to put this in.  Maybe because I and the new data version implements I was able to get this far, but I did reach the check."", 'commenter': 'EdColeman'}, {'comment': ""You are correct.  Going from 1.10 to 3.0-SNAPSHOT logs\r\n\r\n```\r\n2023-02-07T21:11:03,556 [start.Main] ERROR: Thread 'manager' died.\r\njava.lang.IllegalStateException: This version of accumulo (3.0.0-SNAPSHOT) is not compatible with files stored using data version 8\r\n        at org.apache.accumulo.server.ServerContext.ensureDataVersionCompatible(ServerContext.java:302) ~[accumulo-server-base-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.server.ServerContext.init(ServerContext.java:371) ~[accumulo-server-base-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.server.AbstractServer.<init>(AbstractServer.java:50) ~[accumulo-server-base-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.manager.Manager.<init>(Manager.java:412) ~[accumulo-manager-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.manager.Manager.main(Manager.java:406) ~[accumulo-manager-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.manager.ManagerExecutable.execute(ManagerExecutable.java:45) ~[accumulo-manager-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.start.Main.lambda$execKeyword$0(Main.java:122) ~[accumulo-start-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:829) ~[?:?]\r\n\r\n```"", 'commenter': 'EdColeman'}, {'comment': '@ctubbsii would this be okay? Seems long and not adding much - might just be better to include something in the release notes or in the upgrading section of the docs.\r\n\r\n```\r\njava.lang.IllegalStateException: This version of accumulo (3.0.0-SNAPSHOT) is not compatible with files stored using data version 8 see org.apache.accumulo.server.AccumuloDataVersion javadoc for historic version mapping\r\n        at org.apache.accumulo.server.ServerContext.ensureDataVersionCompatible(ServerContext.java:302) ~[accumulo-server-base-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.server.ServerContext.init(ServerContext.java:372) ~[accumulo-server-base-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.server.AbstractServer.<init>(AbstractServer.java:50) ~[accumulo-server-base-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.manager.Manager.<init>(Manager.java:412) ~[accumulo-manager-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.manager.Manager.main(Manager.java:406) ~[accumulo-manager-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.manager.ManagerExecutable.execute(ManagerExecutable.java:45) ~[accumulo-manager-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at org.apache.accumulo.start.Main.lambda$execKeyword$0(Main.java:122) ~[accumulo-start-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:829) ~[?:?]\r\n\r\n```', 'commenter': 'EdColeman'}, {'comment': 'I think we can get what you were going for by updating that current error message to include what you were trying to include here. I can take a look and add that change to this PR if you want.', 'commenter': 'ctubbsii'}, {'comment': 'Sure - a quick look and the info I wanted is either private or in the javadoc.', 'commenter': 'EdColeman'}, {'comment': 'a5da59abc76 removes the test and leaves the message alone.  \r\n\r\nThe improvement that I was shooting for would provide guidance in the manager log when the upgrade fails.  The upgrade is using data version and while that is tied to release version, that connection is buried in our code.  Users may not be familiar with ""data version"", where it is set / coming from and how that even relates to a release. ', 'commenter': 'EdColeman'}, {'comment': 'Here\'s my proposed changes:\r\n\r\n```diff\r\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java b/server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java\r\nindex 9f523e7082..5b44913f11 100644\r\n--- a/server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java\r\n+++ b/server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java\r\n@@ -86,7 +86,11 @@ public class AccumuloDataVersion {\r\n     return cv;\r\n   }\r\n \r\n-  public static String dataVersionToReleaseName(final int version) {\r\n+  public static String oldestUpgradeableVersionName() {\r\n+    return dataVersionToReleaseName(CAN_RUN.stream().mapToInt(x -> x).min().orElseThrow());\r\n+  }\r\n+\r\n+  private static String dataVersionToReleaseName(final int version) {\r\n     switch (version) {\r\n       case ROOT_TABLET_META_CHANGES:\r\n         return ""2.1.0"";\r\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java b/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\r\nindex 03ab556923..e2286115c2 100644\r\n--- a/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\r\n+++ b/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\r\n@@ -300,7 +300,9 @@ public class ServerContext extends ClientContext {\r\n   public static void ensureDataVersionCompatible(int dataVersion) {\r\n     if (!AccumuloDataVersion.CAN_RUN.contains(dataVersion)) {\r\n       throw new IllegalStateException(""This version of accumulo ("" + Constants.VERSION\r\n-          + "") is not compatible with files stored using data version "" + dataVersion);\r\n+          + "") is not compatible with files stored using data version "" + dataVersion\r\n+          + "". Please upgrade from "" + AccumuloDataVersion.oldestUpgradeableVersionName()\r\n+          + "" or later."");\r\n     }\r\n   }\r\n \r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Added in 2aea7e1f8d9 - will run an upgrade sequence and include the output for completeness later.', 'commenter': 'EdColeman'}]"
3167,server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java,"@@ -319,39 +325,38 @@ public void run() {
       }
     }
 
+    summariesExecutor.shutdownNow();
     LOG.info(""Shutting down"");
   }
 
   private void updateSummaries() {
-    ExecutorService executor = ThreadPools.getServerThreadPools().createFixedThreadPool(10,
-        ""Compaction Summary Gatherer"", false);
-    try {
-      Set<String> queuesSeen = new ConcurrentSkipListSet<>();
 
-      tserverSet.getCurrentServers().forEach(tsi -> {
-        executor.execute(() -> updateSummaries(tsi, queuesSeen));
-      });
+    final ArrayList<Future<?>> tasks = new ArrayList<>();
+    Set<String> queuesSeen = new ConcurrentSkipListSet<>();
 
-      executor.shutdown();
+    tserverSet.getCurrentServers().forEach(tsi -> {
+      tasks.add(summariesExecutor.submit(() -> updateSummaries(tsi, queuesSeen)));
+    });
 
-      try {
-        while (!executor.awaitTermination(1, TimeUnit.MINUTES)) {}
-      } catch (InterruptedException e) {
-        Thread.currentThread().interrupt();
-        throw new RuntimeException(e);
+    // Wait for all tasks to complete
+    while (!tasks.isEmpty()) {
+      Iterator<Future<?>> iter = tasks.iterator();
+      while (iter.hasNext()) {
+        Future<?> f = iter.next();
+        if (f.isDone()) {","[{'comment': 'If there was an exception in the background task, is that something we even care about here in the foreground?', 'commenter': 'keith-turner'}, {'comment': ""We didn't handle exceptions in the original version, so I kept the same behavior."", 'commenter': 'dlmarion'}]"
3167,server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java,"@@ -319,39 +325,38 @@ public void run() {
       }
     }
 
+    summariesExecutor.shutdownNow();
     LOG.info(""Shutting down"");
   }
 
   private void updateSummaries() {
-    ExecutorService executor = ThreadPools.getServerThreadPools().createFixedThreadPool(10,
-        ""Compaction Summary Gatherer"", false);
-    try {
-      Set<String> queuesSeen = new ConcurrentSkipListSet<>();
 
-      tserverSet.getCurrentServers().forEach(tsi -> {
-        executor.execute(() -> updateSummaries(tsi, queuesSeen));
-      });
+    final ArrayList<Future<?>> tasks = new ArrayList<>();
+    Set<String> queuesSeen = new ConcurrentSkipListSet<>();
 
-      executor.shutdown();
+    tserverSet.getCurrentServers().forEach(tsi -> {
+      tasks.add(summariesExecutor.submit(() -> updateSummaries(tsi, queuesSeen)));
+    });
 
-      try {
-        while (!executor.awaitTermination(1, TimeUnit.MINUTES)) {}
-      } catch (InterruptedException e) {
-        Thread.currentThread().interrupt();
-        throw new RuntimeException(e);
+    // Wait for all tasks to complete
+    while (!tasks.isEmpty()) {","[{'comment': 'This loop will burn CPU.  Could add a sleep at the end of the loop or call get() instead of isDone() on the future because get() will block. Calling get() on the future is also useful if knowing about background exceptions is desired.\r\n\r\n\r\nI looked to see if Java had anything to wait for a bunch of futures to be done, found this: https://stackoverflow.com/a/51006865', 'commenter': 'keith-turner'}, {'comment': 'Added a sleep in 967df31', 'commenter': 'dlmarion'}]"
3167,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1012,16 +1013,19 @@ private SortedMap<TServerInstance,TabletServerStatus> gatherTableInformation(
             badServers.remove(server);
           }
         }
-      });
-    }
-    tp.shutdown();
-    try {
-      tp.awaitTermination(Math.max(10000, rpcTimeout / 3), TimeUnit.MILLISECONDS);
-    } catch (InterruptedException e) {
-      log.debug(""Interrupted while fetching status"");
+      }));
     }
 
-    tp.shutdownNow();
+    // Wait for all tasks to complete
+    while (!tasks.isEmpty()) {","[{'comment': 'This will also burn cpu while wating for all futures to be done.\r\n\r\nThe old code did not wait on background task for an unlimited amount of time.  It would wait a fixed amount of time and then call shutdownNow which interrupts the background task.  With futures that could done by canceling any that are not complete within a given time.', 'commenter': 'keith-turner'}, {'comment': 'Updated logic in 967df31 to wait 1 second between the outer loop and to cancel tasks if they have not completed in the same amount of time as the old logic used.', 'commenter': 'dlmarion'}]"
3167,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1251,6 +1258,15 @@ boolean canSuspendTablets() {
       throw new IllegalStateException(""Exception stopping status thread"", e);
     }
 
+    tp.shutdown();
+    try {
+      final long rpcTimeout = getConfiguration().getTimeInMillis(Property.GENERAL_RPC_TIMEOUT);
+      tp.awaitTermination(Math.max(10000, rpcTimeout / 3), TimeUnit.MILLISECONDS);
+    } catch (InterruptedException e) {
+      log.debug(""Interrupted while fetching status"");
+    }","[{'comment': 'Could probably only call shutdownNow here (assuming this is in the manager shutdown code)\r\n\r\n```suggestion\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Removed this code in 967df31', 'commenter': 'dlmarion'}]"
3167,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -217,6 +217,8 @@ public class Manager extends AbstractServer
   private final AtomicBoolean managerInitialized = new AtomicBoolean(false);
   private final AtomicBoolean managerUpgrading = new AtomicBoolean(false);
 
+  private ExecutorService tp = null;","[{'comment': 'Should probably give this a better name since its being promoted from a method variable to an instance variable.', 'commenter': 'keith-turner'}, {'comment': 'Addressed in 967df31', 'commenter': 'dlmarion'}]"
3167,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -950,11 +952,10 @@ private SortedMap<TServerInstance,TabletServerStatus> gatherTableInformation(
       Set<TServerInstance> currentServers, SortedMap<TabletServerId,TServerStatus> balancerMap) {
     final long rpcTimeout = getConfiguration().getTimeInMillis(Property.GENERAL_RPC_TIMEOUT);
     int threads = getConfiguration().getCount(Property.MANAGER_STATUS_THREAD_POOL_SIZE);
-    ExecutorService tp = ThreadPools.getServerThreadPools()","[{'comment': 'Its unrelated to this PR, but I think this used to be a thread pool of unlimited size.  I need to look back and see what changed.  There is a comment a bit further down in the code about rate limiting because its an unbounded thread pool.', 'commenter': 'keith-turner'}, {'comment': 'It looks like this is handled in ThreadPools. When the value of Property.MANAGER_STATUS_THREAD_POOL_SIZE is zero it returns an unbounded thread pool.', 'commenter': 'dlmarion'}]"
3167,core/src/main/java/org/apache/accumulo/core/fate/Fate.java,"@@ -235,6 +235,9 @@ public Fate(T environment, TStore<T> store, Function<Repo<T>,String> toLogStrFun
    * Launches the specified number of worker threads.
    */
   public void startTransactionRunners(AccumuloConfiguration conf) {
+    if (executor != null) {
+      throw new IllegalStateException(""Fate.startTransactionRunners called twice."");
+    }","[{'comment': ""Could rely on memozing this, so it can't be created twice, and you can also get the effect of lazily creating it when first used, so you don't need to call a start method to get it running."", 'commenter': 'ctubbsii'}, {'comment': 'Resolved in e888bdc', 'commenter': 'dlmarion'}]"
3167,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1012,17 +1014,29 @@ private SortedMap<TServerInstance,TabletServerStatus> gatherTableInformation(
             badServers.remove(server);
           }
         }
-      });
-    }
-    tp.shutdown();
-    try {
-      tp.awaitTermination(Math.max(10000, rpcTimeout / 3), TimeUnit.MILLISECONDS);
-    } catch (InterruptedException e) {
-      log.debug(""Interrupted while fetching status"");
+      }));
+    }
+    long timeToWaitForCompletion = Math.max(10000, rpcTimeout / 3);
+    long currTime = System.currentTimeMillis();","[{'comment': ""Do not rely on `System.currentTimeMillis()` for tracking duration of executions. That is wall clock time, and can be changed by NTP, arbitrarily by a user, a time zone change, daylight savings, leap time, etc. For timing duration of events inside Java, always use `System.nanoTime()`, not because you actually get nanosecond granularity (you probably don't), but because `nanoTime(t2) - nanoTime(t1)` has stronger guarantees."", 'commenter': 'ctubbsii'}, {'comment': 'Resolved in e888bdc', 'commenter': 'dlmarion'}]"
3167,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1020,17 +1022,29 @@ private SortedMap<TServerInstance,TabletServerStatus> gatherTableInformation(
             badServers.remove(server);
           }
         }
-      });
-    }
-    tp.shutdown();
-    try {
-      tp.awaitTermination(Math.max(10000, rpcTimeout / 3), MILLISECONDS);
-    } catch (InterruptedException e) {
-      log.debug(""Interrupted while fetching status"");
+      }));
+    }
+    final long timeToWaitForCompletion = Math.max(10000, rpcTimeout / 3);
+    long currTime = NANOSECONDS.toMillis(System.nanoTime());
+    final long timeToCancelTasks = currTime + timeToWaitForCompletion;
+    // Wait for all tasks to complete
+    while (!tasks.isEmpty()) {
+      boolean cancel = (currTime > timeToCancelTasks);","[{'comment': 'The time returned by `System.nanoTime()` is only valid when compared relative to another nano time.  The value is relative to some arbitrary counter in the JVM and can be negative.\r\n\r\nThis should be sometime like ```\r\nboolean cancel = ((currTime - System.nanoTime()) > MILLISECONDS.toNanos(timeToCancelTasks));\r\n``` \r\n\r\nIf may also help if all times were kept as nanos - or converted to nanos for the calculations and only converting back to millis (or seconds) for display to the user.  Using one set of units consistently may help limit conversion errors if a conversion was missed.\r\n', 'commenter': 'EdColeman'}, {'comment': 'Thx @EdColeman for the explanation. I believe I have resolved this in 860fe79.', 'commenter': 'dlmarion'}]"
3168,core/src/main/java/org/apache/accumulo/core/client/impl/TabletLocatorImpl.java,"@@ -330,8 +348,16 @@ else if (!lookupFailed)
         tabletLocations.add(tl);
       }
 
-      for (TabletLocation tl2 : tabletLocations) {
-        TabletLocatorImpl.addRange(binnedRanges, tl2.tablet_location, tl2.tablet_extent, range);
+      // Ensure the extents found are non overlapping and have no holes. When reading some extents
+      // from the cache and other from the metadata table in the loop above we may end up with
+      // non-contiguous extents. This can happen when a subset of exents are placed in the cache and
+      // then after that merges and splits happen.
+      if (isContiguous(tabletLocations)) {","[{'comment': ""I'm wondering if this check should be moved to the end of loop `l1`. Multiple disjoint ranges could be sent into this method and I'm not sure that's taken into account here. I'm assuming that you want to check that you have a contiguous list of TabletLocations for each Range."", 'commenter': 'dlmarion'}, {'comment': 'The code is checking that the extents found for each range are contiguous.  If not, the range is added to the list of failed ranges.  I think the check is in the right place.  Looking back at the code I noticed it was not setting a boolean that other places set when adding a failed range, so I added that  in ca36d92.', 'commenter': 'keith-turner'}, {'comment': 'Ok, you are right, I missed a bracket, so I thought that this was outside of loop `l1`.', 'commenter': 'dlmarion'}]"
3168,core/src/test/java/org/apache/accumulo/core/client/impl/TabletLocatorImplTest.java,"@@ -1115,6 +1116,82 @@ public void testBinRanges5() throws Exception {
         nrl(nr(""0"", ""11""), nr(""1"", ""2""), nr(""0"", ""4""), nr(""2"", ""4"")));
   }
 
+  @Test
+  public void testBinRangesNonContiguousExtents() throws Exception {
+
+    // This test exercises a bug that was seen in the tablet locator code.
+
+    KeyExtent e1 = nke(""foo"", ""05"", null);
+    KeyExtent e2 = nke(""foo"", ""1"", ""05"");
+    KeyExtent e3 = nke(""foo"", ""2"", ""05"");
+
+    Text tableName = new Text(""foo"");
+
+    TServers tservers = new TServers();
+    TabletLocatorImpl metaCache =
+        createLocators(tservers, ""tserver1"", ""tserver2"", ""foo"", e1, ""l1"", e2, ""l1"");
+
+    List<Range> ranges = nrl(nr(""01"", ""07""));
+    Map<String,Map<KeyExtent,List<Range>>> expected =
+        createExpectedBinnings(""l1"", nol(e1, nrl(nr(""01"", ""07"")), e2, nrl(nr(""01"", ""07""))));
+
+    // The following will result in extents e1 and e2 being placed in the cache.
+    runTest(tableName, ranges, metaCache, expected, nrl());
+
+    // Add e3 to the metadata table. Extent e3 could not be added earlier in the test because it
+    // overlaps e2. If e2 and e3 are seen in the same metadata read then one will be removed from
+    // the cache because the cache can never contain overlapping extents.
+    setLocation(tservers, ""tserver2"", MTE, e3, ""l1"");
+
+    // The following test reproduces a bug. Extents e1 and e2 are in the cache. Extent e3 overlaps
+    // e2 but is not in the cache. The range used by the test overlaps e1,e2,and e3. The bug was
+    // that for this situation the binRanges code in tablet locator used to return e1,e2,and e3. The
+    // desired behavior is that the range fails for this situation. This tablet locator bug caused
+    // the batch scanner to return duplicate data.
+    ranges = nrl(nr(""01"", ""17""));
+    runTest(tableName, ranges, metaCache, new HashMap<>(), nrl(nr(""01"", ""17"")));
+
+    // After the above test fails it should cause e3 to be added to the cache. Because e3 overlaps
+    // e2, when e3 is added then e2 is removed. Therefore, the following binRanges call should
+    // succeed and find the range overlaps e1 and e3.
+    expected = createExpectedBinnings(""l1"", nol(e1, nrl(nr(""01"", ""17"")), e3, nrl(nr(""01"", ""17""))));
+    runTest(tableName, ranges, metaCache, expected, nrl());
+  }
+","[{'comment': 'I think we need a test that uses multiple non-contiguous ranges.', 'commenter': 'dlmarion'}, {'comment': 'I added a new test in 1b093f9', 'commenter': 'keith-turner'}]"
3177,test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java,"@@ -470,6 +472,93 @@ public void grepTest() throws IOException {
     exec(""deletetable t -f"", true, ""Table: [t] has been deleted"");
   }
 
+  @Test
+  void configTest() throws IOException {
+    Shell.log.debug(""Starting config property type test -------------------------"");
+
+    String testTable = ""test"";
+    exec(""createtable "" + testTable, true);
+
+    for (Property property : Property.values()) {
+      PropertyType propertyType = property.getType();
+      String invalidValue, validValue = property.getDefaultValue();
+
+      // Skip test if we can't set this property via shell
+      if (!Property.isValidZooPropertyKey(property.getKey())) {
+        Shell.log.debug(""Property {} with type {} cannot be modified by shell"", property.getKey(),
+            propertyType.toString());
+        continue;
+      }
+
+      switch (propertyType) {
+        case PATH:
+        case PREFIX:
+        case STRING:
+          Shell.log.debug(""Skipping "" + propertyType + "" Property Types"");
+          continue;
+        case TIMEDURATION:
+          invalidValue = ""1h30min"";
+          break;
+        case BYTES:
+          invalidValue = ""1M500k"";
+          break;
+        case MEMORY:
+          invalidValue = ""1.5G"";
+          break;
+        case HOSTLIST:
+          invalidValue = "":1000"";
+          break;
+        case PORT:
+          invalidValue = ""65539"";
+          break;
+        case COUNT:
+          invalidValue = ""-1"";
+          break;
+        case FRACTION:
+          invalidValue = ""10Percent"";
+          break;
+        case ABSOLUTEPATH:
+          invalidValue = ""~/foo"";
+          break;
+        case CLASSNAME:
+          Shell.log.debug(""CLASSNAME properties currently fail this test"");
+          Shell.log.debug(""Regex used for CLASSNAME property types may need to be modified"");","[{'comment': 'Should we fix the regex as part of this PR?', 'commenter': 'dlmarion'}, {'comment': ""Currently, all of these changes don't alter property structure, they just enforce the previously defined types. \r\n\r\nModifying a property regex has the potential to break legitimate uses.\r\nSo, I'd rather open an issue for the regex fix and track that as a separate action. "", 'commenter': 'ddanielr'}]"
3177,test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java,"@@ -205,6 +205,17 @@ public void createTableWithTableNameLengthLimit()
     assertFalse(tableOps.exists(t2));
   }
 
+  @Test
+  public void createTableWithBadProperties()
+      throws AccumuloException, AccumuloSecurityException, TableExistsException {
+    TableOperations tableOps = accumuloClient.tableOperations();
+    String t0 = StringUtils.repeat('a', MAX_TABLE_NAME_LEN - 1);","[{'comment': 'We typically create table names with something like `String t0 = getUniqueNames(1)[0];', 'commenter': 'dlmarion'}, {'comment': 'Ah I see that on line 221. \r\nSo does this need to change?', 'commenter': 'ddanielr'}, {'comment': 'It would be nice for consistency - and the getUniqueNames uses the test and the function in the name to allow multiple tests in the same file to avoid collisions.  Not sure if it would apply here, but that would also pop up in the logs if anyone was trying to chase something down.', 'commenter': 'EdColeman'}, {'comment': ""Just pointing it out, that we have a common way to do it. I wouldn't say it needs to change if it's working. It's just not consistent with a lot of the other tests."", 'commenter': 'dlmarion'}, {'comment': ""Sorry @EdColeman  - I think I was typing and didn't see your response."", 'commenter': 'dlmarion'}, {'comment': ""Ok, I changed it and I'm running the `TableOperationsIT` now. Once it's done I'll push the commit up. "", 'commenter': 'ddanielr'}]"
3177,test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java,"@@ -470,6 +472,93 @@ public void grepTest() throws IOException {
     exec(""deletetable t -f"", true, ""Table: [t] has been deleted"");
   }
 
+  @Test
+  void configTest() throws IOException {
+    Shell.log.debug(""Starting config property type test -------------------------"");
+
+    String testTable = ""test"";
+    exec(""createtable "" + testTable, true);
+
+    for (Property property : Property.values()) {
+      PropertyType propertyType = property.getType();
+      String invalidValue, validValue = property.getDefaultValue();
+
+      // Skip test if we can't set this property via shell
+      if (!Property.isValidZooPropertyKey(property.getKey())) {
+        Shell.log.debug(""Property {} with type {} cannot be modified by shell"", property.getKey(),
+            propertyType.toString());
+        continue;
+      }
+
+      switch (propertyType) {
+        case PATH:
+        case PREFIX:
+        case STRING:
+          Shell.log.debug(""Skipping "" + propertyType + "" Property Types"");
+          continue;
+        case TIMEDURATION:
+          invalidValue = ""1h30min"";
+          break;
+        case BYTES:
+          invalidValue = ""1M500k"";
+          break;
+        case MEMORY:
+          invalidValue = ""1.5G"";
+          break;
+        case HOSTLIST:
+          invalidValue = "":1000"";
+          break;
+        case PORT:
+          invalidValue = ""65539"";
+          break;
+        case COUNT:
+          invalidValue = ""-1"";
+          break;
+        case FRACTION:
+          invalidValue = ""10Percent"";
+          break;
+        case ABSOLUTEPATH:
+          invalidValue = ""~/foo"";
+          break;
+        case CLASSNAME:
+          Shell.log.debug(""CLASSNAME properties currently fail this test"");
+          Shell.log.debug(""Regex used for CLASSNAME property types may need to be modified"");
+          continue;
+        case CLASSNAMELIST:
+          invalidValue = ""String,Object"";
+          break;
+        case DURABILITY:
+          invalidValue = ""rinse"";
+          break;
+        case GC_POST_ACTION:
+          invalidValue = ""expand"";
+          break;
+        case BOOLEAN:
+          invalidValue = ""fooFalse"";
+          break;
+        case URI:
+          invalidValue = ""12///\\{}:;123!"";
+          break;
+        default:
+          Shell.log
+              .debug(""Property Type: "" + propertyType.toString() + "" has no defined test case"");","[{'comment': 'The toString() on `propertyType.toString()` is unnecessary.', 'commenter': 'EdColeman'}]"
3177,test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java,"@@ -470,6 +472,93 @@ public void grepTest() throws IOException {
     exec(""deletetable t -f"", true, ""Table: [t] has been deleted"");
   }
 
+  @Test
+  void configTest() throws IOException {
+    Shell.log.debug(""Starting config property type test -------------------------"");
+
+    String testTable = ""test"";
+    exec(""createtable "" + testTable, true);
+
+    for (Property property : Property.values()) {
+      PropertyType propertyType = property.getType();
+      String invalidValue, validValue = property.getDefaultValue();
+
+      // Skip test if we can't set this property via shell
+      if (!Property.isValidZooPropertyKey(property.getKey())) {
+        Shell.log.debug(""Property {} with type {} cannot be modified by shell"", property.getKey(),
+            propertyType.toString());
+        continue;
+      }
+
+      switch (propertyType) {
+        case PATH:
+        case PREFIX:
+        case STRING:
+          Shell.log.debug(""Skipping "" + propertyType + "" Property Types"");
+          continue;
+        case TIMEDURATION:
+          invalidValue = ""1h30min"";
+          break;
+        case BYTES:
+          invalidValue = ""1M500k"";
+          break;
+        case MEMORY:
+          invalidValue = ""1.5G"";
+          break;
+        case HOSTLIST:
+          invalidValue = "":1000"";
+          break;
+        case PORT:
+          invalidValue = ""65539"";
+          break;
+        case COUNT:
+          invalidValue = ""-1"";
+          break;
+        case FRACTION:
+          invalidValue = ""10Percent"";
+          break;
+        case ABSOLUTEPATH:
+          invalidValue = ""~/foo"";
+          break;
+        case CLASSNAME:
+          Shell.log.debug(""CLASSNAME properties currently fail this test"");
+          Shell.log.debug(""Regex used for CLASSNAME property types may need to be modified"");
+          continue;
+        case CLASSNAMELIST:
+          invalidValue = ""String,Object"";
+          break;
+        case DURABILITY:
+          invalidValue = ""rinse"";
+          break;
+        case GC_POST_ACTION:
+          invalidValue = ""expand"";
+          break;
+        case BOOLEAN:
+          invalidValue = ""fooFalse"";
+          break;
+        case URI:
+          invalidValue = ""12///\\{}:;123!"";
+          break;
+        default:
+          Shell.log
+              .debug(""Property Type: "" + propertyType.toString() + "" has no defined test case"");
+          invalidValue = ""foo"";
+      }
+      String setCommand = ""config -s "";
+      if (Property.isValidTablePropertyKey(property.getKey())) {
+        setCommand = ""config -t "" + testTable + "" -s "";
+      }
+      Shell.log.debug(""Testing Property {} with Type {}"", property.getKey(),
+          propertyType.toString());","[{'comment': 'The toString() on `propertyType.toString()` is unnecessary.', 'commenter': 'EdColeman'}]"
3177,core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java,"@@ -88,13 +88,28 @@ public void testPorts() {
     for (Property prop : Property.values()) {
       if (prop.getType().equals(PropertyType.PORT)) {
         int port = Integer.parseInt(prop.getDefaultValue());
+        assertTrue(Property.isValidProperty(prop.getKey(), Integer.toString(port)));
         assertFalse(usedPorts.contains(port), ""Port already in use: "" + port);
         usedPorts.add(port);
         assertTrue(port > 1023 && port < 65536, ""Port out of range of valid ports: "" + port);
       }
     }
   }
 
+  @Test
+  public void testBooleans() {
+    for (Property prop : Property.values()) {","[{'comment': 'Is this loop necessary?  The test would be valid with any single boolean property.  Also, the test in ShellIT that you added is more comprehensive, testing each type - would it be better to test those conditions here?  Not sure if it needs to be in both places, but unit testing would be faster than needing to run an IT.', 'commenter': 'EdColeman'}, {'comment': ""Replaced `testBooleans` with `testPropertyValidation` that runs through the property possibilities and attempts invalid types. \r\n\r\nI left the ShellIT alone since it's a client test of the RPC calls. If the shell is ever spun out of this codebase it would remain valuable."", 'commenter': 'ddanielr'}]"
3177,core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java,"@@ -88,13 +88,80 @@ public void testPorts() {
     for (Property prop : Property.values()) {
       if (prop.getType().equals(PropertyType.PORT)) {
         int port = Integer.parseInt(prop.getDefaultValue());
+        assertTrue(Property.isValidProperty(prop.getKey(), Integer.toString(port)));
         assertFalse(usedPorts.contains(port), ""Port already in use: "" + port);
         usedPorts.add(port);
         assertTrue(port > 1023 && port < 65536, ""Port out of range of valid ports: "" + port);
       }
     }
   }
 
+  @Test
+  public void testPropertyValidation() {
+
+    for (Property property : Property.values()) {
+      PropertyType propertyType = property.getType();
+      String invalidValue, validValue = property.getDefaultValue();
+      System.out.printf(""Testing property: %s with type: %s\n"", property.getKey(), propertyType);","[{'comment': 'Should we be using a `Logger` here instead of `System.out`?', 'commenter': 'DomGarguilo'}, {'comment': 'Implemented a `Logger` and removed the default print statements.', 'commenter': 'ddanielr'}]"
3177,server/base/src/main/java/org/apache/accumulo/server/util/SystemPropUtil.java,"@@ -74,7 +76,13 @@ private static String validateSystemProperty(String property, final String value
     if (!Property.isValidZooPropertyKey(property)) {
       IllegalArgumentException iae =
           new IllegalArgumentException(""Zookeeper property is not mutable: "" + property);
-      log.debug(""Attempted to set zookeeper property.  It is not mutable"", iae);
+      log.debug(""Encountered error setting zookeeper property"", iae);","[{'comment': '`log.error` might be fitting for these', 'commenter': 'DomGarguilo'}]"
3177,test/src/main/java/org/apache/accumulo/test/NamespacesIT.java,"@@ -263,6 +263,20 @@ public void deleteNonEmptyNamespace() throws Exception {
     assertThrows(NamespaceNotEmptyException.class, () -> c.namespaceOperations().delete(namespace));
   }
 
+  @Test
+  public void setProperties() throws Exception {
+    String namespace = ""nsTest"";
+    c.namespaceOperations().create(namespace);","[{'comment': 'Any reason not to use the `namespace` String that is already created before each test?', 'commenter': 'DomGarguilo'}, {'comment': ""nope, that's an oversight on my part."", 'commenter': 'ddanielr'}]"
3177,server/base/src/main/java/org/apache/accumulo/server/util/SystemPropUtil.java,"@@ -74,7 +76,13 @@ private static String validateSystemProperty(String property, final String value
     if (!Property.isValidZooPropertyKey(property)) {
       IllegalArgumentException iae =
           new IllegalArgumentException(""Zookeeper property is not mutable: "" + property);
-      log.debug(""Attempted to set zookeeper property.  It is not mutable"", iae);
+      log.error(""Encountered error setting zookeeper property"", iae);","[{'comment': 'Did you happen to look at the exceptions as printed in the logs?  It seems likely that the method that catches the exception will also printout the stack trace.  If there was additional info to help development, maybe a trace statement?  Otherwise the logs may end up with two identical stack traces without adding additional information.\r\n\r\nAlso, error seems rather aggressive, services are going to keep running, \r\n\r\nIf this code get reworked, it may be worth considering.\r\n\r\n(I realize this has been merged, it was changed, committed before I got back to it. Just wanted to mention it if it helps going forward)', 'commenter': 'EdColeman'}, {'comment': ""@EdColeman confirmed that the calling method does in fact log the exceptions at the `error` level. \r\nI've opened https://github.com/apache/accumulo/pull/3184 to revert this change. \r\n"", 'commenter': 'ddanielr'}]"
3180,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -353,6 +362,38 @@ public short getDefaultReplication(Path path) {
     return getFileSystemByPath(path).getDefaultReplication(path);
   }
 
+  /**
+   * The Hadoop Configuration object does not currently allow for duplicate properties to be set in
+   * a single Configuration for different FileSystem URIs. Here we will look for properties in the
+   * Accumulo configuration of the form:
+   *
+   * <pre>
+   * general.custom.&lt;volume-uri&gt;.&lt;hdfs-property&gt;
+   * </pre>
+   *
+   * We will use these properties to return a new Configuration object that can be used with the
+   * FileSystem URI.
+   *
+   * @param conf AccumuloConfiguration object
+   * @param hadoopConf Hadoop Configuration object
+   * @param filesystemURI Volume Filesystem URI
+   * @return Hadoop Configuration with custom overrides for this FileSystem
+   */
+  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+      final Configuration hadoopConf, final String filesystemURI) {
+    final Configuration volumeConfig = new Configuration(hadoopConf);
+    final Map<String,String> customProps =
+        conf.getAllPropertiesWithPrefixStripped(Property.GENERAL_ARBITRARY_PROP_PREFIX);
+    customProps.forEach((key, value) -> {
+      if (key.startsWith(filesystemURI)) {
+        String property = key.substring(filesystemURI.length() + 1);
+        log.debug(""Overriding property {} to {} for volume {}"", property, value, filesystemURI);","[{'comment': 'Really like the logging.  Does this logging only happen once per volume on a server?  If its only once, maybe could promote it to info.', 'commenter': 'keith-turner'}, {'comment': 'It should only happen once, at server startup. Increased logging level to INFO in e0c12c4.', 'commenter': 'dlmarion'}]"
3180,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -353,6 +362,38 @@ public short getDefaultReplication(Path path) {
     return getFileSystemByPath(path).getDefaultReplication(path);
   }
 
+  /**
+   * The Hadoop Configuration object does not currently allow for duplicate properties to be set in
+   * a single Configuration for different FileSystem URIs. Here we will look for properties in the
+   * Accumulo configuration of the form:
+   *
+   * <pre>
+   * general.custom.&lt;volume-uri&gt;.&lt;hdfs-property&gt;
+   * </pre>
+   *
+   * We will use these properties to return a new Configuration object that can be used with the
+   * FileSystem URI.
+   *
+   * @param conf AccumuloConfiguration object
+   * @param hadoopConf Hadoop Configuration object
+   * @param filesystemURI Volume Filesystem URI
+   * @return Hadoop Configuration with custom overrides for this FileSystem
+   */
+  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+      final Configuration hadoopConf, final String filesystemURI) {
+    final Configuration volumeConfig = new Configuration(hadoopConf);
+    final Map<String,String> customProps =
+        conf.getAllPropertiesWithPrefixStripped(Property.GENERAL_ARBITRARY_PROP_PREFIX);","[{'comment': 'I would use a different prefix for these properties.  I think of the general arbitrary prefix as something plugins use.  This config is not related to a plugin, its for core functionality.  Could do something like the following.\r\n\r\n```\r\ninstance.volumes.dfs.config.<volume-uri>.<property-name>=<property-value>\r\n```\r\n\r\nThis config prefix also makes the config sort together with the instance.volumes config.', 'commenter': 'keith-turner'}, {'comment': 'Created new property in e0c12c4.', 'commenter': 'dlmarion'}]"
3180,test/src/main/java/org/apache/accumulo/test/VolumeManagerIT.java,"@@ -0,0 +1,148 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertNotNull;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.NewTableConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.accumulo.test.functional.ConfigurableMacBase;
+import org.apache.accumulo.test.functional.ReadWriteIT;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocatedFileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.RemoteIterator;
+import org.junit.jupiter.api.Test;
+
+public class VolumeManagerIT extends ConfigurableMacBase {
+
+  private String vol1;
+  private String vol2;
+
+  @Override
+  protected void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+    cfg.useMiniDFS(true);
+    cfg.setFinalSiteConfigUpdater((config) -> {
+      vol1 = config.getSiteConfig().get(Property.INSTANCE_VOLUMES.getKey());
+      assertTrue(vol1.contains(""localhost""));
+      vol2 = vol1.replace(""localhost"", ""127.0.0.1"");
+      config.setProperty(Property.INSTANCE_VOLUMES.getKey(), String.join("","", vol2, vol1));
+
+      // Set Volume specific HDFS overrides
+      config.setProperty(""general.volume.chooser"",
+          ""org.apache.accumulo.core.spi.fs.PreferredVolumeChooser"");
+      config.setProperty(""general.custom.volume.preferred.default"", vol1);
+      config.setProperty(""general.custom.volume.preferred.logger"", vol2);
+
+      // Need to escape the colons in the custom property volume because it's part of the key. It's
+      // being
+      // written out to a file and read in using the Properties object.
+      String vol1FileOutput = vol1.replaceAll("":"", ""\\\\:"");
+      String vol2FileOutput = vol2.replaceAll("":"", ""\\\\:"");
+      config.setProperty(
+          Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol1FileOutput + "".dfs.blocksize"",
+          ""10485760"");
+      config.setProperty(
+          Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol2FileOutput + "".dfs.blocksize"",
+          ""104857600"");
+      config.setProperty(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol1FileOutput
+          + "".dfs.client.use.datanode.hostname"", ""true"");
+      config.setProperty(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol2FileOutput
+          + "".dfs.client.use.datanode.hostname"", ""false"");
+      config.setProperty(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol1FileOutput
+          + "".dfs.client.hedged.read.threadpool.size"", ""0"");
+      config.setProperty(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol2FileOutput
+          + "".dfs.client.hedged.read.threadpool.size"", ""1"");
+    });
+  }
+
+  @Test
+  public void testHdfsConfigOverrides() throws Exception {
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
+
+      Map<String,String> siteConfig = c.instanceOperations().getSiteConfiguration();
+      assertEquals(""10485760"", siteConfig
+          .get(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol1 + "".dfs.blocksize""));
+      assertEquals(""104857600"", siteConfig","[{'comment': '`104857600` is so similar to `10485760` that at first glance I thought both were the same.  Could replace `104857600` with something more visually distinct like `51200000`', 'commenter': 'keith-turner'}, {'comment': 'Updated test to use proposed value in e0c12c4.', 'commenter': 'dlmarion'}]"
3180,server/base/src/test/java/org/apache/accumulo/server/fs/VolumeManagerImplTest.java,"@@ -104,4 +111,88 @@ public Scope getChooserScope() {
       assertThrows(RuntimeException.class, () -> vm.choose(chooserEnv, volumes));
     }
   }
+
+  @Test
+  public void testConfigurationOverrides() throws Exception {
+
+    final String vol1 = ""file://127.0.0.1/vol1/"";
+    final String vol2 = ""file://localhost/vol2/"";
+    final String vol3 = ""hdfs://127.0.0.1/accumulo"";
+    final String vol4 = ""hdfs://localhost/accumulo"";
+
+    ConfigurationCopy conf = new ConfigurationCopy();
+    conf.set(Property.INSTANCE_VOLUMES, String.join("","", vol1, vol2, vol3, vol4));
+    conf.set(Property.GENERAL_VOLUME_CHOOSER, Property.GENERAL_VOLUME_CHOOSER.getDefaultValue());
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol1 + "".""
+        + HdfsClientConfigKeys.HedgedRead.THREADPOOL_SIZE_KEY, ""10"");
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol1 + "".""
+        + HdfsClientConfigKeys.DFS_CLIENT_CACHE_DROP_BEHIND_READS, ""true"");
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol2 + "".""
+        + HdfsClientConfigKeys.HedgedRead.THREADPOOL_SIZE_KEY, ""20"");
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol2 + "".""
+        + HdfsClientConfigKeys.DFS_CLIENT_CACHE_DROP_BEHIND_READS, ""false"");
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol3 + "".""
+        + HdfsClientConfigKeys.HedgedRead.THREADPOOL_SIZE_KEY, ""30"");
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol3 + "".""
+        + HdfsClientConfigKeys.DFS_CLIENT_CACHE_DROP_BEHIND_READS, ""TRUE"");
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol4 + "".""
+        + HdfsClientConfigKeys.HedgedRead.THREADPOOL_SIZE_KEY, ""40"");
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol4 + "".""
+        + HdfsClientConfigKeys.DFS_CLIENT_CACHE_DROP_BEHIND_READS, ""FALSE"");
+
+    VolumeManager vm = VolumeManagerImpl.get(conf, hadoopConf);
+
+    FileSystem fs1 = vm.getFileSystemByPath(new Path(vol1));
+    Configuration conf1 = fs1.getConf();
+
+    FileSystem fs2 = vm.getFileSystemByPath(new Path(vol2));
+    Configuration conf2 = fs2.getConf();
+
+    FileSystem fs3 = vm.getFileSystemByPath(new Path(vol3));
+    Configuration conf3 = fs3.getConf();
+
+    FileSystem fs4 = vm.getFileSystemByPath(new Path(vol4));
+    Configuration conf4 = fs4.getConf();","[{'comment': 'Could add a 5th volume that does not set any overrides to ensure there is no interference.', 'commenter': 'keith-turner'}, {'comment': 'Added a fifth volume to the test in e0c12c4.', 'commenter': 'dlmarion'}]"
3180,server/base/src/test/java/org/apache/accumulo/server/fs/VolumeManagerImplTest.java,"@@ -104,4 +111,88 @@ public Scope getChooserScope() {
       assertThrows(RuntimeException.class, () -> vm.choose(chooserEnv, volumes));
     }
   }
+
+  @Test
+  public void testConfigurationOverrides() throws Exception {
+
+    final String vol1 = ""file://127.0.0.1/vol1/"";
+    final String vol2 = ""file://localhost/vol2/"";
+    final String vol3 = ""hdfs://127.0.0.1/accumulo"";
+    final String vol4 = ""hdfs://localhost/accumulo"";
+
+    ConfigurationCopy conf = new ConfigurationCopy();
+    conf.set(Property.INSTANCE_VOLUMES, String.join("","", vol1, vol2, vol3, vol4));
+    conf.set(Property.GENERAL_VOLUME_CHOOSER, Property.GENERAL_VOLUME_CHOOSER.getDefaultValue());
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol1 + "".""
+        + HdfsClientConfigKeys.HedgedRead.THREADPOOL_SIZE_KEY, ""10"");
+    conf.set(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey() + vol1 + "".""","[{'comment': 'Could set a third override on one volume that no others set to ensure there is no interference for this case.  For example could set a third blocksize prop for only vol1. ', 'commenter': 'keith-turner'}, {'comment': 'Added blocksize property to one volume in test in e0c12c4.', 'commenter': 'dlmarion'}]"
3180,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -120,6 +120,16 @@ public enum Property {
           + "" a comma or other reserved characters in a URI use standard URI hex""
           + "" encoding. For example replace commas with %2C."",
       ""1.6.0""),
+  INSTANCE_VOLUMES_CONFIG(""instance.volumes.config."", null, PropertyType.PREFIX,
+      ""Properties in this category are used to provide volume specific overrides to ""
+          + ""the general filesystem client configuration. Properties using this prefix ""
+          + ""should be in the form ""
+          + ""'instance.volumes.config.<volume-uri>.<property-name>=<property-value>. An ""
+          + ""example: ""
+          + ""'instance.volume.config.hdfs://namespace-a:8020/accumulo.dfs.client.hedged.read.threadpool.size=10'. ""
+          + ""Note that when specifying property names that contain colons in the properties ""
+          + ""files that the colons need to be escaped with a backslash."",
+      ""2.1.1""),","[{'comment': 'Throughout this PR, you are inconsistently switching between `instance.volumes.config` and `instance.volume.config`. Is this singular or plural? Is it intended to apply to one volume only or more than one? I think this needs to be cleaned up throughout.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in f34eb45.', 'commenter': 'dlmarion'}]"
3180,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java,"@@ -812,4 +820,29 @@ public int getNumCompactors() {
   public void setNumCompactors(int numCompactors) {
     this.numCompactors = numCompactors;
   }
+
+  /**
+   * Set the FinalSiteConfigUpdater instance that will be used to modify the site configuration
+   * right before it's written out a file. This would be useful in the case where the configuration
+   * needs to be updated based on a property that is set in MiniAccumuloClusterImpl like
+   * instance.volumes
+   *
+   * @param updater FinalSiteConfigUpdater instance
+   * @since 2.1.1
+   */
+  public void setFinalSiteConfigUpdater(FinalSiteConfigUpdater updater) {
+    this.updater = updater;
+  }
+
+  /**
+   * Called by MiniAccumuloClusterImpl after all modifications are done to the configuration and
+   * right before it's written out to a file.
+   *
+   * @since 2.1.1","[{'comment': ""Don't need `@since` for these internal Impl methods. If we start doing that everywhere that's not public API, it will get unwieldy fast."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in f34eb45.', 'commenter': 'dlmarion'}]"
3180,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -353,11 +363,63 @@ public short getDefaultReplication(Path path) {
     return getFileSystemByPath(path).getDefaultReplication(path);
   }
 
+  /**
+   * The Hadoop Configuration object does not currently allow for duplicate properties to be set in
+   * a single Configuration for different FileSystem URIs. Here we will look for properties in the
+   * Accumulo configuration of the form:
+   *
+   * <pre>
+   * instance.volume.config.&lt;volume-uri&gt;.&lt;hdfs-property&gt;
+   * </pre>
+   *
+   * We will use these properties to return a new Configuration object that can be used with the
+   * FileSystem URI.
+   *
+   * @param conf AccumuloConfiguration object
+   * @param hadoopConf Hadoop Configuration object
+   * @param filesystemURI Volume Filesystem URI
+   * @return Hadoop Configuration with custom overrides for this FileSystem
+   */
+  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+      final Configuration hadoopConf, final String filesystemURI) {
+    final Configuration volumeConfig = new Configuration(hadoopConf);
+    final Map<String,String> customProps =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG);
+    customProps.forEach((key, value) -> {
+      if (key.startsWith(filesystemURI)) {
+        String property = key.substring(filesystemURI.length() + 1);
+        log.info(""Overriding property {} to {} for volume {}"", property, value, filesystemURI);
+        volumeConfig.set(property, value);
+      }
+    });
+    return volumeConfig;
+  }
+
+  private static void warnVolumeOverridesMissingVolume(AccumuloConfiguration conf,
+      Set<String> definedVolumes) {
+    final Map<String,String> overrideProperties = new ConcurrentHashMap<>(
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG));
+
+    definedVolumes.forEach(vol -> {
+      log.debug(""Looking for defined volume: {}"", vol);
+      overrideProperties.keySet().forEach(override -> {
+        if (override.startsWith(vol)) {
+          log.debug(""Found volume {}, removing property {}"", vol, override);
+          overrideProperties.remove(override);
+        }
+      });
+    });","[{'comment': 'This can be written much more succinctly. Plus, you can get false positives if you don\'t include the dot separator (for example, an override for `hdfs://host:port/path/to/accumulo2` will incorrectly be removed as a match against a volume named `hdfs://host:port/path/to/accumulo` if you don\'t include the dot to ensure the volume name is terminated).\r\n\r\n```suggestion\r\n    definedVolumes.forEach(\r\n        vol -> overrideProperties.entrySet().removeIf(entry -> entry.getKey().startsWith(vol + ""."")));\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in f34eb45.', 'commenter': 'dlmarion'}]"
3180,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -353,11 +363,63 @@ public short getDefaultReplication(Path path) {
     return getFileSystemByPath(path).getDefaultReplication(path);
   }
 
+  /**
+   * The Hadoop Configuration object does not currently allow for duplicate properties to be set in
+   * a single Configuration for different FileSystem URIs. Here we will look for properties in the
+   * Accumulo configuration of the form:
+   *
+   * <pre>
+   * instance.volume.config.&lt;volume-uri&gt;.&lt;hdfs-property&gt;
+   * </pre>
+   *
+   * We will use these properties to return a new Configuration object that can be used with the
+   * FileSystem URI.
+   *
+   * @param conf AccumuloConfiguration object
+   * @param hadoopConf Hadoop Configuration object
+   * @param filesystemURI Volume Filesystem URI
+   * @return Hadoop Configuration with custom overrides for this FileSystem
+   */
+  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+      final Configuration hadoopConf, final String filesystemURI) {
+    final Configuration volumeConfig = new Configuration(hadoopConf);
+    final Map<String,String> customProps =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG);
+    customProps.forEach((key, value) -> {
+      if (key.startsWith(filesystemURI)) {
+        String property = key.substring(filesystemURI.length() + 1);
+        log.info(""Overriding property {} to {} for volume {}"", property, value, filesystemURI);
+        volumeConfig.set(property, value);
+      }
+    });
+    return volumeConfig;
+  }
+
+  private static void warnVolumeOverridesMissingVolume(AccumuloConfiguration conf,
+      Set<String> definedVolumes) {
+    final Map<String,String> overrideProperties = new ConcurrentHashMap<>(
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG));
+
+    definedVolumes.forEach(vol -> {
+      log.debug(""Looking for defined volume: {}"", vol);
+      overrideProperties.keySet().forEach(override -> {
+        if (override.startsWith(vol)) {
+          log.debug(""Found volume {}, removing property {}"", vol, override);
+          overrideProperties.remove(override);
+        }
+      });
+    });
+
+    overrideProperties.forEach((k, v) -> log
+        .warn(""Found no matching volume for volume config override property {} = {}"", k, v));
+  }","[{'comment': 'On second thought, the entire method can be written without an intermediate map or dealing with any concurrency... just grab the properties you want to log a warning about.\r\n\r\n```suggestion\r\n  private static void warnVolumeOverridesMissingVolume(AccumuloConfiguration conf,\r\n      Set<String> definedVolumes) {\r\n    conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG).entrySet().stream()\r\n        // log only configs where none of the volumes (with a dot) prefix its key\r\n        .filter(e -> definedVolumes.stream().noneMatch(vol -> e.getKey().startsWith(vol + ""."")))\r\n        .forEach(\r\n            e -> log.warn(""Found no matching volume for volume config override property {}"", e));\r\n  }\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in f34eb45.', 'commenter': 'dlmarion'}]"
3180,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -353,11 +363,63 @@ public short getDefaultReplication(Path path) {
     return getFileSystemByPath(path).getDefaultReplication(path);
   }
 
+  /**
+   * The Hadoop Configuration object does not currently allow for duplicate properties to be set in
+   * a single Configuration for different FileSystem URIs. Here we will look for properties in the
+   * Accumulo configuration of the form:
+   *
+   * <pre>
+   * instance.volume.config.&lt;volume-uri&gt;.&lt;hdfs-property&gt;
+   * </pre>
+   *
+   * We will use these properties to return a new Configuration object that can be used with the
+   * FileSystem URI.
+   *
+   * @param conf AccumuloConfiguration object
+   * @param hadoopConf Hadoop Configuration object
+   * @param filesystemURI Volume Filesystem URI
+   * @return Hadoop Configuration with custom overrides for this FileSystem
+   */
+  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+      final Configuration hadoopConf, final String filesystemURI) {
+    final Configuration volumeConfig = new Configuration(hadoopConf);
+    final Map<String,String> customProps =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG);
+    customProps.forEach((key, value) -> {
+      if (key.startsWith(filesystemURI)) {
+        String property = key.substring(filesystemURI.length() + 1);
+        log.info(""Overriding property {} to {} for volume {}"", property, value, filesystemURI);
+        volumeConfig.set(property, value);
+      }
+    });","[{'comment': 'This can be written slightly more succinctly without an intermediate map variable. This doesn\'t save much, but I do think there\'s some expressive value in separating the Predicate `if` clause from the `forEach` action:\r\n\r\n```suggestion\r\n    conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG).entrySet().stream()\r\n        .filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {\r\n          String key = e.getKey().substring(filesystemURI.length() + 1);\r\n          String value = e.getValue();\r\n          log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);\r\n          volumeConfig.set(key, value);\r\n        });\r\n```\r\n\r\nAnd again, you need to include the dot when checking for the startsWith... or you\'ll get false positives from similarly named paths.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in f34eb45.', 'commenter': 'dlmarion'}]"
3180,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java,"@@ -47,6 +47,12 @@
  * @since 1.6.0
  */
 public class MiniAccumuloConfigImpl {
+
+  @FunctionalInterface
+  public static interface FinalSiteConfigUpdater {
+    void update(MiniAccumuloConfigImpl cfg);
+  }","[{'comment': 'You don\'t need this interface. Just use `Consumer<MiniAccumuloConfigImpl>`. While it probably doesn\'t hurt to have an extra interface to select a more expressive name for ease of use, I think the name here, with the word ""Final"" here is a bit presumptive (what if we need a subsequent modification to support other tests? would we call it ""FinalFinal""?). We can just avoid coming up with a name by just using the existing Consumer... also, Consumer has the helpful `andThen` method, which we don\'t need to reinvent and could be useful.', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in f34eb45.', 'commenter': 'dlmarion'}]"
3180,minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java,"@@ -812,4 +820,29 @@ public int getNumCompactors() {
   public void setNumCompactors(int numCompactors) {
     this.numCompactors = numCompactors;
   }
+
+  /**
+   * Set the FinalSiteConfigUpdater instance that will be used to modify the site configuration
+   * right before it's written out a file. This would be useful in the case where the configuration
+   * needs to be updated based on a property that is set in MiniAccumuloClusterImpl like
+   * instance.volumes
+   *
+   * @param updater FinalSiteConfigUpdater instance
+   * @since 2.1.1
+   */
+  public void setFinalSiteConfigUpdater(FinalSiteConfigUpdater updater) {","[{'comment': ""Even if the parameter is changed to a Consumer, this method still needs a decent name. I still have reservations about this one. One thing it has going for it is that it's not public API... it's only on the Impl class.\r\n\r\nStill, maybe something like:\r\n```suggestion\r\n  public void preStartProcessor(Consumer<MiniAccumuloConfigImpl> prestartConfigProcessor) {\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in f34eb45.', 'commenter': 'dlmarion'}]"
3180,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -353,11 +363,63 @@ public short getDefaultReplication(Path path) {
     return getFileSystemByPath(path).getDefaultReplication(path);
   }
 
+  /**
+   * The Hadoop Configuration object does not currently allow for duplicate properties to be set in
+   * a single Configuration for different FileSystem URIs. Here we will look for properties in the
+   * Accumulo configuration of the form:
+   *
+   * <pre>
+   * instance.volume.config.&lt;volume-uri&gt;.&lt;hdfs-property&gt;
+   * </pre>
+   *
+   * We will use these properties to return a new Configuration object that can be used with the
+   * FileSystem URI.
+   *
+   * @param conf AccumuloConfiguration object
+   * @param hadoopConf Hadoop Configuration object
+   * @param filesystemURI Volume Filesystem URI
+   * @return Hadoop Configuration with custom overrides for this FileSystem
+   */
+  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+      final Configuration hadoopConf, final String filesystemURI) {
+    final Configuration volumeConfig = new Configuration(hadoopConf);
+    final Map<String,String> customProps =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG);
+    customProps.forEach((key, value) -> {
+      if (key.startsWith(filesystemURI)) {
+        String property = key.substring(filesystemURI.length() + 1);
+        log.info(""Overriding property {} to {} for volume {}"", property, value, filesystemURI);
+        volumeConfig.set(property, value);
+      }
+    });
+    return volumeConfig;
+  }
+
+  private static void warnVolumeOverridesMissingVolume(AccumuloConfiguration conf,
+      Set<String> definedVolumes) {
+    final Map<String,String> overrideProperties = new ConcurrentHashMap<>(
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG));
+
+    definedVolumes.forEach(vol -> {
+      log.debug(""Looking for defined volume: {}"", vol);
+      overrideProperties.keySet().forEach(override -> {
+        if (override.startsWith(vol)) {
+          log.debug(""Found volume {}, removing property {}"", vol, override);
+          overrideProperties.remove(override);
+        }
+      });
+    });
+
+    overrideProperties.forEach((k, v) -> log
+        .warn(""Found no matching volume for volume config override property {} = {}"", k, v));","[{'comment': 'Would be really nice to unit test this case, however since it just logs a message not sure of the best way to do that.', 'commenter': 'keith-turner'}, {'comment': 'Implemented test in f34eb45.', 'commenter': 'dlmarion'}]"
3180,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -120,11 +120,11 @@ public enum Property {
           + "" a comma or other reserved characters in a URI use standard URI hex""
           + "" encoding. For example replace commas with %2C."",
       ""1.6.0""),
-  INSTANCE_VOLUMES_CONFIG(""instance.volumes.config."", null, PropertyType.PREFIX,
+  INSTANCE_VOLUMES_CONFIG(""instance.volume.config."", null, PropertyType.PREFIX,","[{'comment': '```suggestion\r\n  INSTANCE_VOLUME_CONFIG_PREFIX(""instance.volume.config."", null, PropertyType.PREFIX,\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Changes applied in 55fe4f3', 'commenter': 'dlmarion'}]"
3180,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -382,44 +382,37 @@ public short getDefaultReplication(Path path) {
    */
   private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
       final Configuration hadoopConf, final String filesystemURI) {
+
     final Configuration volumeConfig = new Configuration(hadoopConf);
-    final Map<String,String> customProps =
-        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG);
-    customProps.forEach((key, value) -> {
-      if (key.startsWith(filesystemURI)) {
-        String property = key.substring(filesystemURI.length() + 1);
-        log.info(""Overriding property {} to {} for volume {}"", property, value, filesystemURI);
-        volumeConfig.set(property, value);
-      }
-    });
-    return volumeConfig;
-  }
 
-  private static void warnVolumeOverridesMissingVolume(AccumuloConfiguration conf,
-      Set<String> definedVolumes) {
-    final Map<String,String> overrideProperties = new ConcurrentHashMap<>(
-        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG));
+    conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG).entrySet().stream()
+        .filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {
+          String key = e.getKey().substring(filesystemURI.length() + 1);
+          String value = e.getValue();
+          log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);
+          volumeConfig.set(key, value);
+        });
 
-    definedVolumes.forEach(vol -> {
-      log.debug(""Looking for defined volume: {}"", vol);
-      overrideProperties.keySet().forEach(override -> {
-        if (override.startsWith(vol)) {
-          log.debug(""Found volume {}, removing property {}"", vol, override);
-          overrideProperties.remove(override);
-        }
-      });
-    });
+    return volumeConfig;
+  }
 
-    overrideProperties.forEach((k, v) -> log
-        .warn(""Found no matching volume for volume config override property {} = {}"", k, v));
+  protected static List<Entry<String,String>>
+      findVolumeOverridesMissingVolume(AccumuloConfiguration conf, Set<String> definedVolumes) {
+    return conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG).entrySet()
+        .stream()
+        // log only configs where none of the volumes (with a dot) prefix its key
+        .filter(e -> definedVolumes.stream().noneMatch(vol -> e.getKey().startsWith(vol + ""."")))
+        .collect(Collectors.toList());","[{'comment': 'I see you needed this as a separate method for verification in the test. However, it\'s a bit weird to return a list of entries, rather than a map. But, there\'s also a performance hit by collecting it into a data structure in the first place. I don\'t think you need that at all for the test or for logging the warning. Just return the Stream, which you can call forEach on directly for the warning... and for the test you can check it as you stream, or call `.collect` on it there to do whatever verification you need.\r\n\r\n```suggestion\r\n  protected static Stream<Entry<String,String>>\r\n      findVolumeOverridesMissingVolume(AccumuloConfiguration conf, Set<String> definedVolumes) {\r\n    return conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUMES_CONFIG).entrySet()\r\n        .stream()\r\n        // log only configs where none of the volumes (with a dot) prefix its key\r\n        .filter(e -> definedVolumes.stream().noneMatch(vol -> e.getKey().startsWith(vol + ""."")));\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Changes applied in 55fe4f3', 'commenter': 'dlmarion'}]"
3187,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -419,6 +419,9 @@ public enum Property {
       PropertyType.COUNT, ""Max number of files a major compaction thread can open at once. ""),
   TSERV_SCAN_MAX_OPENFILES(""tserver.scan.files.open.max"", ""100"", PropertyType.COUNT,
       ""Maximum total files that all tablets in a tablet server can open for scans. ""),
+  @Experimental
+  TSERV_SCAN_INITIAL_WAIT_ENABLED(""tserver.scan.initial.wait.enabled"", ""true"", PropertyType.BOOLEAN,
+      ""Determines if the client waits for writes before scanning a table for the first time""),","[{'comment': 'Might want to include why.  Something along the lines of:\r\n\r\n`Determines if the client waits for writes in progress before scanning a table for the first time to ensure the client sees all data previously written.`', 'commenter': 'EdColeman'}]"
3187,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -931,9 +934,7 @@ public enum Property {
       ""The sampling percentage to use for replication traces""),
   REPLICATION_RPC_TIMEOUT(""replication.rpc.timeout"", ""2m"", PropertyType.TIMEDURATION,
       ""Amount of time for a single replication RPC call to last before failing""
-          + "" the attempt. See replication.work.attempts.""),
-
-  ;
+          + "" the attempt. See replication.work.attempts.""),;","[{'comment': 'Should be able to remove trailing comma before the semi-colon.', 'commenter': 'dlmarion'}]"
3187,core/src/main/java/org/apache/accumulo/core/client/impl/ThriftScanner.java,"@@ -116,7 +116,10 @@ public static boolean getBatchFromServer(ClientContext context, Range range, Key
             Constants.SCANNER_DEFAULT_READAHEAD_THRESHOLD, null, batchTimeOut, classLoaderContext);
 
         TabletType ttype = TabletType.type(extent);
-        boolean waitForWrites = !serversWaitedForWrites.get(ttype).contains(server);
+        boolean waitForWrites = false;
+        if (context.getConfiguration().getBoolean(Property.TSERV_SCAN_INITIAL_WAIT_ENABLED)) {","[{'comment': 'It should only be configurable where TabletType == TabletType.USER.', 'commenter': 'dlmarion'}]"
3189,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -402,11 +404,10 @@ public Map<String,Pair<UUID,String>> getScanServers() {
       try {
         final var zLockPath = ServiceLock.path(root + ""/"" + addr);
         ZcStat stat = new ZcStat();
-        byte[] lockData = ServiceLock.getLockData(getZooCache(), zLockPath, stat);
-        if (lockData != null) {
-          String[] fields = new String(lockData, UTF_8).split("","", 2);
-          UUID uuid = UUID.fromString(fields[0]);
-          String group = fields[1];
+        ServiceLockData sld = ServiceLock.getLockData(getZooCache(), zLockPath, stat);","[{'comment': 'I like the idea of creating the ServiceLockData to abstract away some of this implementation with more expressive APIs. I think that could be done independently of adding the group concept. Doing that first may make it easier to see just the changes required to add a tserver group feature, separately from the code organization work.', 'commenter': 'ctubbsii'}]"
3189,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -512,24 +513,28 @@ public List<String> getManagerLocations() {
     OpTimer timer = null;
 
     if (log.isTraceEnabled()) {
-      log.trace(""tid={} Looking up manager location in zookeeper."", Thread.currentThread().getId());
+      log.trace(""tid={} Looking up manager location in zookeeper at {}."",
+          Thread.currentThread().getId(), zLockManagerPath);
       timer = new OpTimer().start();
     }
 
-    byte[] loc = zooCache.getLockData(zLockManagerPath);
+    ServiceLockData sld = zooCache.getLockData(zLockManagerPath);
+    String location = null;
+    if (sld != null) {
+      location = sld.getAddressString(ThriftService.MANAGER);
+    }","[{'comment': 'You could make it so `sld` is never `null`, which would clean up some of these `null` checks, which seems to be a lot of boilerplate. You could return `Optional<ServiceLockData>` or a dummy `ServiceLockData` that represents the ""not found in ZK"" case (like `ServiceLockData.MISSING` that has a `null` address string).', 'commenter': 'ctubbsii'}, {'comment': 'Updated to use Optional', 'commenter': 'dlmarion'}]"
3189,core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java,"@@ -512,24 +513,28 @@ public List<String> getManagerLocations() {
     OpTimer timer = null;
 
     if (log.isTraceEnabled()) {
-      log.trace(""tid={} Looking up manager location in zookeeper."", Thread.currentThread().getId());
+      log.trace(""tid={} Looking up manager location in zookeeper at {}."",
+          Thread.currentThread().getId(), zLockManagerPath);","[{'comment': ""Some of these log message changes seem unrelated to this PR. Might be useful to do those separately, first, so they don't detract from reviews on the main changes intended in this PR."", 'commenter': 'ctubbsii'}]"
3189,core/src/main/java/org/apache/accumulo/core/util/ServiceLockData.java,"@@ -0,0 +1,223 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util;
+
+import java.util.Collections;
+import java.util.EnumMap;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+import java.util.UUID;
+
+import com.google.common.net.HostAndPort;
+import com.google.gson.Gson;
+
+public class ServiceLockData implements Comparable<ServiceLockData> {
+
+  private static final Gson gson = new Gson();
+
+  /**
+   * Thrift Service list
+   */
+  public static enum ThriftService {
+    CLIENT,
+    COORDINATOR,
+    COMPACTOR,
+    FATE,
+    GC,
+    MANAGER,
+    NONE,
+    TABLET_INGEST,
+    TABLET_MANAGEMENT,
+    TABLET_SCAN,
+    TSERV
+  }
+
+  /**
+   * An object that describes a process, the group assigned to that process, the Thrift service and
+   * the address to use to communicate with that service.
+   */
+  public static class ServiceDescriptor {
+
+    /**
+     * The group name that will be used when one is not specified.
+     */
+    public static final String DEFAULT_GROUP_NAME = ""default"";
+
+    private final UUID uuid;
+    private final ThriftService service;
+    private final String address;
+    private final String group;
+
+    public ServiceDescriptor(UUID uuid, ThriftService service, String address, String group) {
+      super();","[{'comment': ""Pretty sure you don't need to explicitly call the super constructor here... especially since the super-class is just Object.\r\n```suggestion\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Implemented suggestion in 17df5d6.', 'commenter': 'dlmarion'}]"
3189,core/src/main/java/org/apache/accumulo/core/util/ServiceLockData.java,"@@ -0,0 +1,223 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util;
+
+import java.util.Collections;
+import java.util.EnumMap;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+import java.util.UUID;
+
+import com.google.common.net.HostAndPort;
+import com.google.gson.Gson;
+
+public class ServiceLockData implements Comparable<ServiceLockData> {
+
+  private static final Gson gson = new Gson();
+
+  /**
+   * Thrift Service list
+   */
+  public static enum ThriftService {
+    CLIENT,
+    COORDINATOR,
+    COMPACTOR,
+    FATE,
+    GC,
+    MANAGER,
+    NONE,
+    TABLET_INGEST,
+    TABLET_MANAGEMENT,
+    TABLET_SCAN,
+    TSERV
+  }
+
+  /**
+   * An object that describes a process, the group assigned to that process, the Thrift service and
+   * the address to use to communicate with that service.
+   */
+  public static class ServiceDescriptor {
+
+    /**
+     * The group name that will be used when one is not specified.
+     */
+    public static final String DEFAULT_GROUP_NAME = ""default"";
+
+    private final UUID uuid;
+    private final ThriftService service;
+    private final String address;
+    private final String group;
+
+    public ServiceDescriptor(UUID uuid, ThriftService service, String address, String group) {
+      super();
+      this.uuid = uuid;
+      this.service = service;
+      this.address = address;
+      this.group = Optional.ofNullable(group).orElse(DEFAULT_GROUP_NAME);
+    }
+
+    public UUID getUUID() {
+      return uuid;
+    }
+
+    public ThriftService getService() {
+      return service;
+    }
+
+    public String getAddress() {
+      return address;
+    }
+
+    public String getGroup() {
+      return group;
+    }
+
+    @Override
+    public int hashCode() {
+      return toString().hashCode();
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (o instanceof ServiceDescriptor) {","[{'comment': 'I think `instanceof` has gotten us in trouble before with `equals`. It will return true for subclasses, but the subclass may not have a reciprocal equals method implemented. Might want to instead check class same-ness rather than `instanceof`.', 'commenter': 'ctubbsii'}, {'comment': 'This came over as part of the rename from ServerServices to ServiceLockData.', 'commenter': 'dlmarion'}, {'comment': ""Gotcha. Still, it's something that we can fix now, since the rename effectively made it a new class, and it'd be good to fix it before any subclasses are created that would be affected by the change."", 'commenter': 'ctubbsii'}, {'comment': 'Implemented suggestion in 17df5d6.', 'commenter': 'dlmarion'}]"
3189,core/src/main/java/org/apache/accumulo/core/util/ServiceLockData.java,"@@ -0,0 +1,223 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util;
+
+import java.util.Collections;
+import java.util.EnumMap;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+import java.util.UUID;
+
+import com.google.common.net.HostAndPort;
+import com.google.gson.Gson;
+
+public class ServiceLockData implements Comparable<ServiceLockData> {
+
+  private static final Gson gson = new Gson();
+
+  /**
+   * Thrift Service list
+   */
+  public static enum ThriftService {
+    CLIENT,
+    COORDINATOR,
+    COMPACTOR,
+    FATE,
+    GC,
+    MANAGER,
+    NONE,
+    TABLET_INGEST,
+    TABLET_MANAGEMENT,
+    TABLET_SCAN,
+    TSERV
+  }
+
+  /**
+   * An object that describes a process, the group assigned to that process, the Thrift service and
+   * the address to use to communicate with that service.
+   */
+  public static class ServiceDescriptor {
+
+    /**
+     * The group name that will be used when one is not specified.
+     */
+    public static final String DEFAULT_GROUP_NAME = ""default"";
+
+    private final UUID uuid;
+    private final ThriftService service;
+    private final String address;
+    private final String group;
+
+    public ServiceDescriptor(UUID uuid, ThriftService service, String address, String group) {
+      super();
+      this.uuid = uuid;
+      this.service = service;
+      this.address = address;
+      this.group = Optional.ofNullable(group).orElse(DEFAULT_GROUP_NAME);
+    }
+
+    public UUID getUUID() {
+      return uuid;
+    }
+
+    public ThriftService getService() {
+      return service;
+    }
+
+    public String getAddress() {
+      return address;
+    }
+
+    public String getGroup() {
+      return group;
+    }
+
+    @Override
+    public int hashCode() {
+      return toString().hashCode();
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (o instanceof ServiceDescriptor) {
+        return toString().equals(o.toString());
+      }
+      return false;
+    }
+
+    @Override
+    public String toString() {
+      return gson.toJson(this);
+    }
+
+  }
+
+  /**
+   * A set of ServiceDescriptor's
+   */
+  public static class ServiceDescriptors {
+    private final Set<ServiceDescriptor> descriptors;
+
+    public ServiceDescriptors() {
+      descriptors = new HashSet<>();
+    }
+
+    public ServiceDescriptors(HashSet<ServiceDescriptor> descriptors) {
+      this.descriptors = descriptors;
+    }
+
+    public void addService(ServiceDescriptor sd) {
+      this.descriptors.add(sd);
+    }
+
+    public Set<ServiceDescriptor> getServices() {
+      return descriptors;
+    }
+
+    @Override
+    public String toString() {
+      return gson.toJson(this);
+    }
+
+    public static ServiceDescriptors parse(String data) {
+      return gson.fromJson(data, ServiceDescriptors.class);
+    }
+  }
+
+  private EnumMap<ThriftService,ServiceDescriptor> services;
+
+  public ServiceLockData(ServiceDescriptors sds) {
+    this.services = new EnumMap<>(ThriftService.class);
+    sds.getServices().forEach(sd -> {
+      this.services.put(sd.getService(), sd);
+    });","[{'comment': '```suggestion\r\n    sds.getServices().forEach(sd -> this.services.put(sd.getService(), sd));\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Implemented suggestion in 17df5d6.', 'commenter': 'dlmarion'}]"
3189,core/src/main/java/org/apache/accumulo/core/util/ServiceLockData.java,"@@ -0,0 +1,223 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util;
+
+import java.util.Collections;
+import java.util.EnumMap;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+import java.util.UUID;
+
+import com.google.common.net.HostAndPort;
+import com.google.gson.Gson;
+
+public class ServiceLockData implements Comparable<ServiceLockData> {
+
+  private static final Gson gson = new Gson();
+
+  /**
+   * Thrift Service list
+   */
+  public static enum ThriftService {
+    CLIENT,
+    COORDINATOR,
+    COMPACTOR,
+    FATE,
+    GC,
+    MANAGER,
+    NONE,
+    TABLET_INGEST,
+    TABLET_MANAGEMENT,
+    TABLET_SCAN,
+    TSERV
+  }
+
+  /**
+   * An object that describes a process, the group assigned to that process, the Thrift service and
+   * the address to use to communicate with that service.
+   */
+  public static class ServiceDescriptor {
+
+    /**
+     * The group name that will be used when one is not specified.
+     */
+    public static final String DEFAULT_GROUP_NAME = ""default"";
+
+    private final UUID uuid;
+    private final ThriftService service;
+    private final String address;
+    private final String group;
+
+    public ServiceDescriptor(UUID uuid, ThriftService service, String address, String group) {
+      super();
+      this.uuid = uuid;
+      this.service = service;
+      this.address = address;
+      this.group = Optional.ofNullable(group).orElse(DEFAULT_GROUP_NAME);
+    }
+
+    public UUID getUUID() {
+      return uuid;
+    }
+
+    public ThriftService getService() {
+      return service;
+    }
+
+    public String getAddress() {
+      return address;
+    }
+
+    public String getGroup() {
+      return group;
+    }
+
+    @Override
+    public int hashCode() {
+      return toString().hashCode();
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (o instanceof ServiceDescriptor) {
+        return toString().equals(o.toString());
+      }
+      return false;
+    }
+
+    @Override
+    public String toString() {
+      return gson.toJson(this);
+    }
+
+  }
+
+  /**
+   * A set of ServiceDescriptor's
+   */
+  public static class ServiceDescriptors {
+    private final Set<ServiceDescriptor> descriptors;
+
+    public ServiceDescriptors() {
+      descriptors = new HashSet<>();
+    }
+
+    public ServiceDescriptors(HashSet<ServiceDescriptor> descriptors) {
+      this.descriptors = descriptors;
+    }
+
+    public void addService(ServiceDescriptor sd) {
+      this.descriptors.add(sd);
+    }
+
+    public Set<ServiceDescriptor> getServices() {
+      return descriptors;
+    }
+
+    @Override
+    public String toString() {
+      return gson.toJson(this);
+    }
+
+    public static ServiceDescriptors parse(String data) {
+      return gson.fromJson(data, ServiceDescriptors.class);
+    }
+  }
+
+  private EnumMap<ThriftService,ServiceDescriptor> services;
+
+  public ServiceLockData(ServiceDescriptors sds) {
+    this.services = new EnumMap<>(ThriftService.class);
+    sds.getServices().forEach(sd -> {
+      this.services.put(sd.getService(), sd);
+    });
+  }
+
+  public ServiceLockData(UUID uuid, String address, ThriftService service, String group) {
+    this(new ServiceDescriptors(new HashSet<>(
+        Collections.singleton(new ServiceDescriptor(uuid, service, address, group)))));","[{'comment': '```suggestion\r\n    this(new ServiceDescriptors(Set.of(new ServiceDescriptor(uuid, service, address, group))));\r\n```', 'commenter': 'ctubbsii'}]"
3189,minicluster/src/test/resources/log4j2-test.properties,"@@ -33,6 +33,6 @@ logger.01.level = error
 logger.02.name = org.apache.zookeeper
 logger.02.level = error
 
-rootLogger.level = info
+rootLogger.level = debug","[{'comment': ""Is this temporary? I hope this doesn't make our tests during the build too spammy."", 'commenter': 'ctubbsii'}, {'comment': 'I just removed this in a follow-on commit', 'commenter': 'dlmarion'}]"
3189,server/base/src/main/java/org/apache/accumulo/server/ServerOpts.java,"@@ -27,6 +28,17 @@ public class ServerOpts extends ConfigOpts {
   @Parameter(names = {""-a"", ""--address""}, description = ""address to bind to"")
   private String address = null;
 
+  @Parameter(required = false, names = {""-g"", ""--group""},
+      description = ""Optional group name that will be made available to the client (e.g. ScanServerSelector) ""
+          + ""and server (e.g. Balancers) plugins.  If not specified will be set to '""
+          + ServiceLockData.ServiceDescriptor.DEFAULT_GROUP_NAME
+          + ""'. Assigning servers to groups support dedicating server resources for specific purposes, where supported."")
+  private String groupName = ServiceLockData.ServiceDescriptor.DEFAULT_GROUP_NAME;","[{'comment': 'I\'ve never been a fan of these command-line options separate from our config file settings. The `-a` was added as a fix, but the semantics of it have always been confusing, as in... is it the ""bind address"" or is it the ""service advertisement"" address? It has always been conflated as both, with a special case for `0.0.0.0` and `localhost`, but it should always have been two.\r\n\r\nI\'m mentioning this now, because I don\'t think we should continue the precedent of adding new command-line options. Instead, there should be a `tserver.group` attribute that can be put in the config file or specified on the command-line, like any other config file option can be, with `-o tserver.group=X`. (Similarly, we should have `tserver.address.listen` and `tserver.address.advertise` as separate properties that default to `0.0.0.0` and `AUTO` by default, but that\'s a separate issue. My main point here is that we shouldn\'t continue the previous precedent... we should do it via config).', 'commenter': 'ctubbsii'}, {'comment': ""Does the current configuration mechanism support shell interpolation for configuration values? For example, could you put the following in `accumulo.properties`?\r\n```\r\ntserver.address.listen=$(hostname -i)`\r\n```\r\n\r\nIf so, then this makes sense to me. If not, then I don't think anyone would want to put the bind address in a config file, as that would cause different `accumulo.properties` files on every host. At least by putting in on the command line you can use CLI commands to get the IP address and pass that in as the argument value. It even works in [k8s](https://github.com/dlmarion/accumulo-k8s/blob/main/accumulo-tserver.yaml#L23)"", 'commenter': 'dlmarion'}, {'comment': ""We use commons-configuration2, which does support property interpolation, but not exactly like your example. You can do something like `tserver.address.listen=${env:HOSTNAME}` and in in your environment, you can do `HOSTNAME=$(hostname -i)` (for example, in `accumulo-env.sh` or `.bashrc`).\r\n\r\nWe also support setting any of our configuration properties on the command-line to override what's in the config file. So, instead of `-a $(hostname -i)`, you could do `-o tserver.address.listen=$(hostname -i)`, so you don't need to have the address hard-coded in the config file, and can set it dynamically on the command-line, even though it's a configuration file property. This feature was specifically added in 2.0 to support container deployments.\r\n\r\nFor the address, that would depend on us actually having a configuration property for the listen/bind address to replace `-a`, which we currently do not have (`tserver.address.listen` was merely an example). But, that's the direction I think we should go; for the purposes of this issue, I think the addition of the group should follow the direction of making it a configuration property, instead of the old precedent of `-a`. We have a complete configuration mechanism with a lot of flexibility already, so we shouldn't need to add one-off command-line options outside that configuration mechanism, like `-g` (and we should eventually get rid of the other one-off command-line options like `-a`, but that's for another ticket)."", 'commenter': 'ctubbsii'}]"
3189,server/base/src/main/java/org/apache/accumulo/server/ServerOpts.java,"@@ -27,6 +28,17 @@ public class ServerOpts extends ConfigOpts {
   @Parameter(names = {""-a"", ""--address""}, description = ""address to bind to"")
   private String address = null;
 
+  @Parameter(required = false, names = {""-g"", ""--group""},","[{'comment': 'Adding this option to server types where it does not do anything seems confusing to me.  Like I can start a manager with the option, but it will not have any effect.  Scoping it to only the tserver and scan server seems less confusing to me.', 'commenter': 'keith-turner'}, {'comment': 'My comment above regarding using our normal configuration property would make this a non-issue. Only server types where it makes sense would have that configuration property.', 'commenter': 'ctubbsii'}]"
3189,core/src/main/java/org/apache/accumulo/core/util/ServiceLockData.java,"@@ -0,0 +1,224 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.util;
+
+import static java.util.Objects.requireNonNull;
+
+import java.util.Collections;
+import java.util.EnumMap;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+import java.util.UUID;
+
+import com.google.common.net.HostAndPort;
+import com.google.gson.Gson;
+
+public class ServiceLockData implements Comparable<ServiceLockData> {
+
+  private static final Gson gson = new Gson();
+
+  /**
+   * Thrift Service list
+   */
+  public static enum ThriftService {
+    CLIENT,
+    COORDINATOR,
+    COMPACTOR,
+    FATE,
+    GC,
+    MANAGER,
+    NONE,
+    TABLET_INGEST,
+    TABLET_MANAGEMENT,
+    TABLET_SCAN,
+    TSERV
+  }
+
+  /**
+   * An object that describes a process, the group assigned to that process, the Thrift service and
+   * the address to use to communicate with that service.
+   */
+  public static class ServiceDescriptor {
+
+    /**
+     * The group name that will be used when one is not specified.
+     */
+    public static final String DEFAULT_GROUP_NAME = ""default"";
+
+    private final UUID uuid;
+    private final ThriftService service;
+    private final String address;
+    private final String group;
+
+    public ServiceDescriptor(UUID uuid, ThriftService service, String address) {
+      this(uuid, service, address, DEFAULT_GROUP_NAME);
+    }
+
+    public ServiceDescriptor(UUID uuid, ThriftService service, String address, String group) {
+      super();
+      this.uuid = requireNonNull(uuid);
+      this.service = requireNonNull(service);
+      this.address = requireNonNull(address);
+      this.group = requireNonNull(group);
+    }
+
+    public UUID getUUID() {
+      return uuid;
+    }
+
+    public ThriftService getService() {
+      return service;
+    }
+
+    public String getAddress() {
+      return address;
+    }
+
+    public String getGroup() {
+      return group;
+    }
+
+    @Override
+    public int hashCode() {
+      return toString().hashCode();
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (o instanceof ServiceDescriptor) {
+        return toString().equals(o.toString());
+      }
+      return false;
+    }
+
+    @Override
+    public String toString() {
+      return gson.toJson(this);
+    }
+
+  }
+
+  /**
+   * A set of ServiceDescriptor's
+   */
+  public static class ServiceDescriptors {
+    private final Set<ServiceDescriptor> descriptors;
+
+    public ServiceDescriptors() {
+      descriptors = new HashSet<>();
+    }
+
+    public ServiceDescriptors(HashSet<ServiceDescriptor> descriptors) {
+      this.descriptors = descriptors;
+    }
+
+    public void addService(ServiceDescriptor sd) {
+      this.descriptors.add(sd);
+    }
+
+    public Set<ServiceDescriptor> getServices() {
+      return descriptors;
+    }
+  }
+
+  private EnumMap<ThriftService,ServiceDescriptor> services;
+
+  public ServiceLockData(ServiceDescriptors sds) {
+    this.services = new EnumMap<>(ThriftService.class);
+    sds.getServices().forEach(sd -> {
+      this.services.put(sd.getService(), sd);
+    });
+  }
+
+  public ServiceLockData(UUID uuid, String address, ThriftService service, String group) {
+    this(new ServiceDescriptors(new HashSet<>(
+        Collections.singleton(new ServiceDescriptor(uuid, service, address, group)))));
+  }
+
+  public ServiceLockData(UUID uuid, String address, ThriftService service) {
+    this(new ServiceDescriptors(
+        new HashSet<>(Collections.singleton(new ServiceDescriptor(uuid, service, address)))));
+  }
+
+  public String getAddressString(ThriftService service) {
+    ServiceDescriptor sd = services.get(service);
+    if (sd == null) {
+      return null;
+    }
+    return sd.getAddress();
+  }
+
+  public HostAndPort getAddress(ThriftService service) {
+    return AddressUtil.parseAddress(getAddressString(service), false);
+  }
+
+  public String getGroup(ThriftService service) {
+    ServiceDescriptor sd = services.get(service);
+    if (sd == null) {
+      return null;
+    }
+    return sd.getGroup();
+  }
+
+  public UUID getServerUUID(ThriftService service) {
+    ServiceDescriptor sd = services.get(service);
+    if (sd == null) {
+      return null;
+    }
+    return sd.getUUID();
+  }
+
+  public String serialize() {
+    ServiceDescriptors sd = new ServiceDescriptors();
+    services.values().forEach(s -> sd.addService(s));
+    return gson.toJson(sd);
+  }
+
+  @Override
+  public String toString() {
+    return serialize();
+  }
+
+  @Override
+  public int hashCode() {
+    return toString().hashCode();
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (o instanceof ServiceLockData) {
+      return toString().equals(o.toString());
+    }
+    return false;
+  }
+
+  @Override
+  public int compareTo(ServiceLockData other) {
+    return toString().compareTo(other.toString());
+  }
+
+  public static Optional<ServiceLockData> parse(String lockData) {","[{'comment': 'Would this be more general if it took `byte[]` and did the string conversion internally to the method?', 'commenter': 'EdColeman'}]"
3192,assemble/bin/accumulo,"@@ -89,4 +89,21 @@ function main() {
   exec ""${JAVA[@]}"" ""${JAVA_OPTS[@]}"" org.apache.accumulo.start.Main ""$@""
 }
 
+args=(""$@"")","[{'comment': ""I'm not sure what this is doing. It doesn't look like this assignment is used."", 'commenter': 'ctubbsii'}, {'comment': 'should be resolved now', 'commenter': 'dlmarion'}]"
3192,assemble/bin/accumulo,"@@ -89,4 +89,21 @@ function main() {
   exec ""${JAVA[@]}"" ""${JAVA_OPTS[@]}"" org.apache.accumulo.start.Main ""$@""
 }
 
+args=(""$@"")
+
+if [ -n ""$ACCUMULO_BIND_ADDR"" ]; then
+  args+=(""-o"")
+  args+=(""general.process.bind.addr=${env:ACCUMULO_BIND_ADDR}"")
+fi
+
+if [ -n ""$ACCUMULO_COMPACTOR_QUEUE"" ]; then
+  args+=(""-o"")
+  args+=(""compactor.queue=${env:ACCUMULO_COMPACTOR_QUEUE}"")
+fi
+
+if [ -n ""$ACCUMULO_SSERVER_GROUP"" ]; then
+  args+=(""-o"")
+  args+=(""sserver.group=${env:ACCUMULO_SSERVER_GROUP}"")
+fi","[{'comment': 'You can combine these into one line. Bash arrays are space delimited.\r\n\r\nHowever, I don\'t think this will work in the bash script. The `${env:XXX}` syntax works inside the configuration file. Here in bash, it should just be `$XXX`.\r\n\r\nAlso, I think we\'ve standardized on the double square braces `[[`, so we\'re not potentially executing an external process, `/usr/bin/[` (yes, that\'s an actual process and how POSIX shells process the contents of the square braces; the double brace makes it clear that it\'s done inside bash as a language feature, not as an external process).\r\n\r\n```suggestion\r\nif [[ -n ""$ACCUMULO_BIND_ADDR"" ]]; then\r\n  args+=(""-o"" ""general.process.bind.addr=$ACCUMULO_BIND_ADDR"")\r\nfi\r\n\r\nif [[ -n ""$ACCUMULO_COMPACTOR_QUEUE"" ]]; then\r\n  args+=(""-o"" ""compactor.queue=$ACCUMULO_COMPACTOR_QUEUE"")\r\nfi\r\n\r\nif [[ -n ""$ACCUMULO_SSERVER_GROUP"" ]; then\r\n  args+=(""-o"" ""sserver.group=$ACCUMULO_SSERVER_GROUP"")\r\nfi\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'resolved in later commit', 'commenter': 'dlmarion'}]"
3192,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -284,6 +285,9 @@ public enum Property {
       PropertyType.BOOLEAN, ""Enables JVM metrics functionality using Micrometer"", ""2.1.0""),
   GENERAL_MICROMETER_FACTORY(""general.micrometer.factory"", """", PropertyType.CLASSNAME,
       ""Name of class that implements MeterRegistryFactory"", ""2.1.0""),
+  GENERAL_PROCESS_BIND_ADDRESS(""general.process.bind.addr"", ""0.0.0.0"", PropertyType.STRING,
+      ""The local IP address to which this server should bind for sending and receiving network traffic"",
+      ""3.0.0""),","[{'comment': 'Should the bind address include the bind port, or do we still want to keep the port option separate?', 'commenter': 'ctubbsii'}, {'comment': ""No, we don't want to specify the port. Each server process has a port property already and some have the ability to do port search."", 'commenter': 'dlmarion'}]"
3192,assemble/conf/accumulo-env.sh,"@@ -144,3 +144,6 @@ esac
 
 ## Specifies command that will be placed before calls to Java in accumulo script
 # export ACCUMULO_JAVA_PREFIX=""""
+# export ACCUMULO_BIND_ADDR=""""
+# export ACCUMULO_COMPACTOR_QUEUE=""""
+# export ACCUMULO_SSERVER_GROUP=""""","[{'comment': 'We should not be adding new environment variables. One of the design goals of the scripts starting in 2.0 was to be less tightly coupled to environment variables baked in to our startup scripts. These lock users in to features of `bin/accumulo` and makes `bin/accumulo` do more than merely ""run java main class with provided args"".\r\n\r\nRather than add these environment variables, and then bake in the options into the `bin/accumulo` script, users should just specify `-o <whatever>` themselves, as needed. We do not need to, and should not, be selectively picking a subset of our configuration options, and providing yet-another-way to configure those same options. This creates confusion and makes it hard for users to know the ""right"" or ""best"" way to configure things.', 'commenter': 'ctubbsii'}, {'comment': 'all changes have been removed from accumulo-env.sh', 'commenter': 'dlmarion'}]"
3192,server/base/src/main/java/org/apache/accumulo/server/ServerOpts.java,"@@ -20,17 +20,11 @@
 
 import org.apache.accumulo.core.cli.ConfigOpts;
 
-import com.beust.jcommander.Parameter;
+public final class ServerOpts extends ConfigOpts {
 
-public class ServerOpts extends ConfigOpts {
+  // This class is empty on purpose. The intent here is that
+  // the Accumulo server processes will only ConfigOpts. Can't
+  // make ConfigOpts final as it's used by utility classes
+  // that subclass it.","[{'comment': 'Can we just delete this class?', 'commenter': 'ctubbsii'}, {'comment': 'Yes, but that would enable someone to reintroduce a subclass of ConfigOpts for a server process at a later time.', 'commenter': 'dlmarion'}, {'comment': ""If we need to re-add it in future, that's fine. If not, it stays deleted. It's internal code organization only, not public API, so it's okay to delete if we don't need it."", 'commenter': 'ctubbsii'}, {'comment': 'removed in fae4135', 'commenter': 'dlmarion'}]"
3192,assemble/bin/accumulo,"@@ -89,4 +89,21 @@ function main() {
   exec ""${JAVA[@]}"" ""${JAVA_OPTS[@]}"" org.apache.accumulo.start.Main ""$@""
 }
 
+args=(""$@"")
+
+if [ -n ""$ACCUMULO_BIND_ADDR"" ]; then
+  args+=(""-o"")
+  args+=(""general.process.bind.addr=${env:ACCUMULO_BIND_ADDR}"")
+fi
+
+if [ -n ""$ACCUMULO_COMPACTOR_QUEUE"" ]; then
+  args+=(""-o"")
+  args+=(""compactor.queue=${env:ACCUMULO_COMPACTOR_QUEUE}"")
+fi
+
+if [ -n ""$ACCUMULO_SSERVER_GROUP"" ]; then
+  args+=(""-o"")
+  args+=(""sserver.group=${env:ACCUMULO_SSERVER_GROUP}"")
+fi
+
 main ""$@""","[{'comment': 'I meant to put ""$args"" here...', 'commenter': 'dlmarion'}, {'comment': ""Gotcha. In any case, I don't think we should be hard-coding these options into `bin/accumulo` or creating env variables as proxies for them at all."", 'commenter': 'ctubbsii'}, {'comment': ""so you don't think I should make *any* changes to `bin/accumulo` ?"", 'commenter': 'dlmarion'}, {'comment': 'all changes have been removed from bin/accumulo', 'commenter': 'dlmarion'}]"
3195,core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooUtilTest.java,"@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.fate.zookeeper;
+
+import static org.junit.jupiter.api.Assertions.assertThrows;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.KeeperException.InvalidACLException;
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.data.ACL;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+class ZooUtilTest {
+  Logger log = LoggerFactory.getLogger(ZooUtilTest.class);
+
+  @Test
+  void checkUnmodifiable() throws Exception {
+    assertTrue(validateACL(ZooUtil.PRIVATE));
+    assertTrue(validateACL(ZooUtil.PUBLIC));
+  }
+
+  /**
+   * This test replicates the acl check in ZooKeeper.java to show ZooKeeper will not accept an
+   * ImmutableCollection for the ACL list. Callers need to use Collections.unmodifiableList()
+   * instead of List.of() or List.copyOf(), because ImmutableCollections.contains() doesn't handle
+   * nulls properly (JDK-8265905) and ZooKeeper (as of 3.8.1) calls acl.contains((Object) null)
+   * which throws a NPE when passed an immutable collection
+   */
+  @Test
+  public void checkImmutableAcl() throws Exception {
+
+    final List<ACL> mutable = new ArrayList<>(ZooDefs.Ids.CREATOR_ALL_ACL);
+    assertTrue(validateACL(mutable));
+
+    try {
+      final List<ACL> immutable = List.copyOf(ZooDefs.Ids.CREATOR_ALL_ACL);
+      assertThrows(NullPointerException.class, () -> validateACL(immutable));
+    } catch (Exception ex) {
+      log.warn(""validateAcls failed with exception"", ex);
+    }
+  }
+
+  /**
+   * Copied from ZooKeeper 3.8.1,(ZooKeeper.validateACL())[] for stand-alone testing,
+   *
+   * @see <a
+   *      href=""https://github.com/apache/zookeeper/blob/2e9c3f3ceda90aeb9380acc87b253bf7661b7794/zookeeper-server/src/main/java/org/apache/zookeeper/ZooKeeper.java#L3075/>
+   */
+  boolean validateACL(List<ACL> acl) throws KeeperException.InvalidACLException {","[{'comment': ""You know, we don't actually need the comments to be javadoc comments for tests. We're not generating javadocs for test code, so you're only going to see these comments looking at the source code. And, in that case, it's easier to omit the HTML and javadoc tags. Also, this method can be private, even if the original wasn't.\r\n\r\n```suggestion\r\n   // Copied from ZooKeeper 3.8.1 for stand-alone testing here\r\n   // https://github.com/apache/zookeeper/blob/2e9c3f3ceda90aeb9380acc87b253bf7661b7794/zookeeper-server/src/main/java/org/apache/zookeeper/ZooKeeper.java#L3075/\r\n  private boolean validateACL(List<ACL> acl) throws KeeperException.InvalidACLException {\r\n```\r\n\r\nAlso, the HTML was malformed in yours anyway... you forgot the closing double quote around the URL."", 'commenter': 'ctubbsii'}]"
3195,core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooUtilTest.java,"@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.fate.zookeeper;
+
+import static org.junit.jupiter.api.Assertions.assertThrows;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.KeeperException.InvalidACLException;
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.data.ACL;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+class ZooUtilTest {
+  Logger log = LoggerFactory.getLogger(ZooUtilTest.class);
+
+  @Test
+  void checkUnmodifiable() throws Exception {
+    assertTrue(validateACL(ZooUtil.PRIVATE));
+    assertTrue(validateACL(ZooUtil.PUBLIC));
+  }
+
+  /**","[{'comment': ""Here also, this doesn't need to be a javadoc comment, especially since the classes mentioned in the body of this comment don't even use links, so this is just a freeform comment. It's easier to avoid breaking javadoc rules/conventions/expectations if we don't bother making it a javadoc when we don't need to in the first place, and we don't need to here."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in c50465c65df', 'commenter': 'EdColeman'}]"
3202,core/src/main/java/org/apache/accumulo/core/util/UtilWaitThread.java,"@@ -18,17 +18,45 @@
  */
 package org.apache.accumulo.core.util;
 
+import static java.util.concurrent.TimeUnit.MICROSECONDS;
+
+import java.util.concurrent.TimeUnit;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 public class UtilWaitThread {
   private static final Logger log = LoggerFactory.getLogger(UtilWaitThread.class);
 
+  /**
+   * Sleep for specified duration in milliseconds. If the sleep is interrupted, a message is logged
+   * and the interrupt flag is reset, but otherwise the interrupt is ignored.
+   * <p>
+   *
+   * @deprecated Use {@link UtilWaitThread#sleep(long, TimeUnit)} instead.
+   */
+  @Deprecated(since = ""3.0.0"")
   public static void sleep(long millis) {
+    sleep(millis, MICROSECONDS);","[{'comment': 'Should the time unit here be MILLISECONDS?', 'commenter': 'dlmarion'}, {'comment': 'Resolved with 5f2c1cb00aed8d8', 'commenter': 'EdColeman'}]"
3202,core/src/main/java/org/apache/accumulo/core/client/IsolatedScanner.java,"@@ -114,7 +114,8 @@ private void readRow() {
           }
 
           // wait a moment before retrying
-          sleepUninterruptibly(100, MILLISECONDS);
+          // ignore interrupt status
+          UtilWaitThread.sleep(100, MILLISECONDS);","[{'comment': 'There are a lot of places in this PR where we are using this new method but intentionally ignoring the return value and the thread interrupt status. For example:\r\n```\r\n          // ignore interrupt status\r\n          UtilWaitThread.sleep(100, MILLISECONDS);\r\n```\r\n\r\nI think this may be incorrect. If we are really going to ignore the thread interrupt status, then we should be resetting it back to `false` if it is `true`. I think, in the cases like this where we are ignoring the interrupt and return value, calling `Uninterruptibles.sleepUninterruptibly` is the right thing to do.', 'commenter': 'dlmarion'}, {'comment': 'Over time, we should look to improving the interrupt handling and this is a step in getting there.  Other than allowing an earlier return on interrupt, this preserves the current behavior.\r\n\r\nThe only difference between using `UtilWaitThread.sleep` and ignoring the interrupt as done here and using `sleepUninterruptibly` is that we will now exit the sleep on an interrupt (and log a message).  sleepUninterruptibly catches the interrupt and then goes back to sleep for the remaining time.  If an interrupt occurred, sleepUninterruptibly also sets the interrupted flag on return if an interrupt occurred, but we currently never look at it.\r\n\r\n', 'commenter': 'EdColeman'}, {'comment': ""> sleepUninterruptibly also sets the interrupted flag on return if an interrupt occurred\r\n\r\nok, I was lazy and didn't look. Thanks. Have you built this with `errorprone`? I'm wondering if it complains about the return value being unchecked. Is there a reason to return a `boolean` if the pattern is for the user to check if the Thread was interrupted after calling this method?\r\n\r\nI would think that it would be more explicit to throw the InterruptedException when it's thrown in the current thread (e.g. Thread.sleep() ) and check the interrupt status on another thread. "", 'commenter': 'dlmarion'}, {'comment': 'There are places where testing the return value seemed more explicit than testing the flag (see CompactionCoordinator, line 219)\r\n```\r\n if (!UtilWaitThread.sleep(1000, MILLISECONDS)) {\r\n        throw new InterruptedException(""Interrupted during pause trying to get coordinator lock"");\r\n      }\r\n```\r\nIn some places, this was used to throw an exiting exception (IOException, AccumuloException) \r\n\r\nI avoided throwing the InterruptedException because there are so many places where we currently do nothing and that seemed like we would need to add a lot of empty try / catches.  Not advocating that\'s the ""best"" way to handle things, but it is what we have now and this is trying to ease us into handling them, but that should be tackled over time.', 'commenter': 'EdColeman'}, {'comment': ""> there are so many places where we currently do nothing and that seemed like we would need to add a lot of empty try / catches\r\n\r\nThat's why I was thinking that continuing to use Uninterruptibles in these places makes sense. Ignore where you really want to ignore, handle where you really want to handle. I'm still curious what errorprone has to say."", 'commenter': 'dlmarion'}, {'comment': 'Running error prone explicitly (at the current level) says nothing.  I also was under the impression that we now run error prone by default, but I just did \r\n```\r\nmvn verify -Perrorprone -Dtest=blah -Dit.test=blah\r\n```\r\nto double check.', 'commenter': 'EdColeman'}, {'comment': 'If we elect to continue to use Uninterruptibles I would like to hear the need that we *must* sleep for the entire time, ignoring the interrupt.\r\n\r\nRunning error prone at the next effort level, I do not see anything - it may be seeing that the interrupt flag is set as a side effect - but we fail pretty fast in core because of other (known) issues, so not really a strong test.\r\n\r\nIf it does get flagged, I would advocate that we create a similar method `UtilWaitThread.sleepQuitely` or something so that we still break out of the sleep on interrupts.', 'commenter': 'EdColeman'}, {'comment': ""> If we elect to continue to use Uninterruptibles I would like to hear the need that we must sleep for the entire time, ignoring the interrupt.\r\n\r\nTypically when you call `Thread.sleep` and ignore the InterruptedException, you are waiting on some other condition to be met that is outside the control of the code. Many times, this is done inside of a loop, and you don't want to exit the loop, you just want to wait."", 'commenter': 'dlmarion'}]"
3202,core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java,"@@ -679,7 +679,7 @@ private void invalidateSession(SessionID sessionId, HostAndPort location)
 
     LockID lid = new LockID(context.getZooKeeperRoot() + Constants.ZTSERVERS, sessionId.lockId);
 
-    while (true) {
+    while (true && !Thread.currentThread().isInterrupted()) {","[{'comment': ""I think that prior to this change this loop would exit either when invalidation was successful or if an error was thrown. I think this introduces a new condition where the loop exits but it's not successful nor is an error thrown.  I'm assuming here (because I didn't look) that the calling code will assume success when this returns, even though it may not have succeeded. This would be a good place to throw InterruptedException if we want to handle it."", 'commenter': 'dlmarion'}, {'comment': 'See 60f14d86ef8b - the other option would be to remove the interrupt flag check and ignore as done before.', 'commenter': 'EdColeman'}]"
3202,core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java,"@@ -704,11 +704,15 @@ private void invalidateSession(SessionID sessionId, HostAndPort location)
         throw new TimedOutException(Collections.singleton(location.toString()));
       }
 
-      sleepUninterruptibly(sleepTime, MILLISECONDS);
+      // interrupt checked in while condition
+      UtilWaitThread.sleep(sleepTime, MILLISECONDS);
       sleepTime = Math.min(2 * sleepTime, MAX_SLEEP);
 
     }
-
+    if (Thread.currentThread().isInterrupted()) {
+      throw new TimedOutException(
+          ""Interrupted during sleep: "" + Collections.singleton(location.toString()));
+    }","[{'comment': 'Why not just call Thread.sleep() and throw InterruptedException then? It has the same effect with less code.', 'commenter': 'dlmarion'}, {'comment': 'This logs and resets the interrupt flag.  Throwing an existing exception delegates to exception handling in place. Throwing InterruptException would require more changes.', 'commenter': 'EdColeman'}]"
3203,.github/workflows/maven.yaml,"@@ -110,6 +109,12 @@ jobs:
       run: mvn -B -V -e -ntp ""-Dstyle.color=always"" -DskipFormat ${{ matrix.profile.args }}
       env:
         MAVEN_OPTS: -Djansi.force=true
+    - name: Build with Maven (ZooKeeper compatibility build)
+      if: ${{ github.base_ref != 'main' }}","[{'comment': 'Why exclude main?', 'commenter': 'ctubbsii'}, {'comment': ""The reason for this change is that #3200 uses newer ZooKeeper API objects, so this check fails. #3200 isn't merged, but if it is, then this is an issue."", 'commenter': 'dlmarion'}, {'comment': ""Okay, that's a compelling reason to bump the minimum. What's the minimum for it to work? Can we just modify the one existing line, rather than do the changes proposed in this PR that jumps through hoops to check for main?"", 'commenter': 'ctubbsii'}, {'comment': ""I'll look."", 'commenter': 'dlmarion'}, {'comment': 'According to [this](https://github.com/apache/zookeeper/commit/553639378d5cf86c2488afff4586e5e4cce38061), it looks like 3.6.0 might be the minimum.', 'commenter': 'dlmarion'}, {'comment': 'I will modify the GitHub action file in my branch and see if that resolves it', 'commenter': 'dlmarion'}, {'comment': ""If that's the case, then changing it to 3.6.4 (the latest 3.6 bugfix) would probably be appropriate."", 'commenter': 'ctubbsii'}, {'comment': ""3.6.1 was the minimum. I updated it in my PR, I'm going to close this."", 'commenter': 'dlmarion'}, {'comment': '3.6.1 is the minimum from an API perspective. Do you think the API changed between 3.6.1 and 3.6.4 that would cause an issue, but still works with 3.8.1?', 'commenter': 'dlmarion'}, {'comment': 'I hope not. That would be uncharacteristically unstable of the ZK team.', 'commenter': 'ctubbsii'}]"
3203,.github/workflows/maven.yaml,"@@ -110,6 +109,12 @@ jobs:
       run: mvn -B -V -e -ntp ""-Dstyle.color=always"" -DskipFormat ${{ matrix.profile.args }}
       env:
         MAVEN_OPTS: -Djansi.force=true
+    - name: Build with Maven (ZooKeeper compatibility build)
+      if: ${{ github.base_ref != 'main' }}
+      timeout-minutes: 60
+      run: mvn -B -V -e -ntp ""-Dstyle.color=always"" -DskipFormat package -DskipTests -Dhadoop.version=3.0.3 -Dzookeeper.version=3.5.10
+      env:","[{'comment': ""This just puts inline what we have previously defined elsewhere. It seems the reason to do this is to exclude the main branch, and not use the normal args, but I'm not sure why we're excluding the main branch."", 'commenter': 'ctubbsii'}]"
3215,assemble/pom.xml,"@@ -297,11 +297,6 @@
       <artifactId>log4j-api</artifactId>
       <optional>true</optional>
     </dependency>
-    <dependency>
-      <groupId>org.apache.logging.log4j</groupId>
-      <artifactId>log4j-core</artifactId>
-      <optional>true</optional>
-    </dependency>
     <dependency>","[{'comment': ""We still need log4j-core in the assembly for runtime. That's why this one didn't have the same comment as the others."", 'commenter': 'ctubbsii'}, {'comment': '@ctubbsii - I was literally typing the same thing as you replied to this :)', 'commenter': 'cshannon'}, {'comment': 'Fixed in 42a4fa2', 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -42,13 +43,13 @@ public class SeekableByteArrayInputStream extends InputStream {
   @SuppressFBWarnings(value = ""VO_VOLATILE_REFERENCE_TO_ARRAY"",
       justification = ""see explanation above"")
   private volatile byte[] buffer;
-  private int cur;
-  private int max;
+  private final AtomicInteger cur;
+  private final AtomicInteger max;","[{'comment': ""Looking back at this, I think that `max` can just be modified to be `final` instead of making it an `AtomicInteger`. It's set in the constructor and never changes."", 'commenter': 'dlmarion'}, {'comment': ""I was going off of the comment above these variables describing why `buffer` needs to be volatile and it seems like `max` would fall into the same boat. Both `max` and `buffer` **could** be final as they are both only set in the  constructor however I'm not sure if they should be or not."", 'commenter': 'DomGarguilo'}, {'comment': ""I think that `max` can be final, which means it doesn't need to be `volatile` or `AtomicInteger`.  There are performance impacts with `volatile` and `AtomicInteger` so we only want to use them where we need to. I think making `cur` either `volatile` or `AtomicInteger` is fine."", 'commenter': 'dlmarion'}, {'comment': 'In 0dacfdc I changed `max` to `final int` and kept `AtomicInteger cur`  since it needs to perform non atomic operations like `++`.', 'commenter': 'DomGarguilo'}, {'comment': 'Also im not sure what going on with the build. Its passing locally for me.', 'commenter': 'DomGarguilo'}, {'comment': '> Also im not sure what going on with the build. Its passing locally for me.\r\n\r\nDoes not appear to be local to this branch. I see the same build failure in the 3 most recent PRs', 'commenter': 'DomGarguilo'}, {'comment': 'The build kicked off with maven 3.9.0 - there is an issue with that maven version that removed a dependency (plex) that used to be provided.  There are more details in the Accumulo slack channel, but the bottom line is that plugins need updated to use maven 3.9.0, or we force the actions to use an older maven version.  The fix for the plugins is in progress, but not sure when they will be released.', 'commenter': 'EdColeman'}, {'comment': ""Thanks, @EdColeman. I guess I'll wait for that fix until I merge this in."", 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -42,13 +43,13 @@ public class SeekableByteArrayInputStream extends InputStream {
   @SuppressFBWarnings(value = ""VO_VOLATILE_REFERENCE_TO_ARRAY"",
       justification = ""see explanation above"")
   private volatile byte[] buffer;
-  private int cur;
-  private int max;
+  private final AtomicInteger cur;","[{'comment': ""In both constructors, you're initializing it to 0. You can just do that here once.\r\n\r\n```suggestion\r\n  private final AtomicInteger cur = new AtomicInteger(0);\r\n```"", 'commenter': 'ctubbsii'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -42,13 +43,13 @@ public class SeekableByteArrayInputStream extends InputStream {
   @SuppressFBWarnings(value = ""VO_VOLATILE_REFERENCE_TO_ARRAY"",
       justification = ""see explanation above"")
   private volatile byte[] buffer;
-  private int cur;
-  private int max;
+  private final AtomicInteger cur;
+  private final int max;
 
   @Override
   public int read() {
-    if (cur < max) {
-      return buffer[cur++] & 0xff;
+    if (cur.get() < max) {
+      return buffer[cur.getAndIncrement()] & 0xff;
     } else {
       return -1;
     }","[{'comment': ""I don't know if this method needs to be thread-safe, but it does seem odd to call `get` and then `getAndIncrement` in the next line, when the two gets could be different in this construction, if multiple threads called this method (or another one that changed the pointer).\r\n\r\nI think you could change this to do the get and the increment atomically. Maybe something like this:\r\n\r\n```suggestion\r\n    int currentValue = cur.getAndAccumulate(1, (x, i) -> x < max ? x + i : x);\r\n    if (currentValue < max) {\r\n      return buffer[currentValue] & 0xff;\r\n    } else {\r\n      return -1;\r\n    }\r\n```\r\n\r\nHowever, I don't think that it would be enough to make this class thread-safe."", 'commenter': 'ctubbsii'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -78,14 +80,14 @@ public int read(byte[] b, int offset, int length) {
       length = avail;
     }
 
-    System.arraycopy(buffer, cur, b, offset, length);
-    cur += length;
+    System.arraycopy(buffer, cur.get(), b, offset, length);
+    cur.getAndAdd(length);","[{'comment': ""This section is another section where the value could change, and it's not thread-safe, between the various non-atomic calls to `cur.get()`. There's one hidden in the `available()` call, and then one in the array copy, then the subsequent advancement of the pointer.\r\n\r\nIf we want this to be thread-safe, it should probably get the current value and advance the pointer atomically, so that outside threads can't change the pointer during the same call, similar to my previous suggestion."", 'commenter': 'ctubbsii'}, {'comment': 'addressed in f251078', 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -94,13 +96,13 @@ public long skip(long requestedSkip) {
       }
     }
 
-    cur += actualSkip;
+    cur.getAndAdd(actualSkip);","[{'comment': 'The skip method is another one where the get and the add are disjointed, and should be atomic.', 'commenter': 'ctubbsii'}, {'comment': 'addressed in 67d4fa4', 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -41,14 +44,15 @@ public class SeekableByteArrayInputStream extends InputStream {
   // thread 2 sees all of thread 1 changes before setting the volatile.
   @SuppressFBWarnings(value = ""VO_VOLATILE_REFERENCE_TO_ARRAY"",
       justification = ""see explanation above"")
-  private volatile byte[] buffer;
-  private int cur;
-  private int max;
+  private final byte[] buffer;","[{'comment': 'The comment above discusses the reason of the variable being volatile was because SpotBugs highlighted an issue. I would suggest removing the `SuppressFBWarnings` to see if making the variable `final` resolves the issue.', 'commenter': 'dlmarion'}, {'comment': ""Yeah, if it's not volatile, then it shouldn't need the warnings suppression. My concern, though, is that there's an attempt to explain that it was necessary to have it volatile (which can also be removed if it's not volatile)... but the explanation doesn't make sense to me, and I can't see a reason why it should be volatile. I think making it final is fine... but I still have reservations, and might want to try to do some digging through the history to see if whoever wrote that can offer a better explanation."", 'commenter': 'ctubbsii'}, {'comment': 'Removed the `SuppressFBWarnings` and the old comments about volatile in 3658005', 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -78,29 +94,34 @@ public int read(byte[] b, int offset, int length) {
       length = avail;
     }
 
-    System.arraycopy(buffer, cur, b, offset, length);
-    cur += length;
+    System.arraycopy(buffer, currentValue, b, offset, length);
     return length;
   }
 
   @Override
   public long skip(long requestedSkip) {
-    int actualSkip = max - cur;
-    if (requestedSkip < actualSkip) {
-      if (requestedSkip < 0) {
-        actualSkip = 0;
-      } else {
-        actualSkip = (int) requestedSkip;
+
+    BiFunction<Integer,Integer,Integer> skipValue = (current, skip) -> {
+      int actual = max - current;
+      if (skip < actual) {
+        actual = Math.max(skip, 0);
       }
-    }
+      return actual;
+    };
+
+    IntBinaryOperator add = (cur1, skip) -> {
+      int actual = skipValue.apply(cur1, skip);
+      return cur1 + actual;
+    };","[{'comment': '```suggestion\r\n    // compute how much to advance, based on actual amount skipped\r\n    IntBinaryOperator add = (cur1, skip) -> cur1 + skipValue.apply(cur1, skip);\r\n```', 'commenter': 'ctubbsii'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -78,29 +94,34 @@ public int read(byte[] b, int offset, int length) {
       length = avail;
     }
 
-    System.arraycopy(buffer, cur, b, offset, length);
-    cur += length;
+    System.arraycopy(buffer, currentValue, b, offset, length);
     return length;
   }
 
   @Override
   public long skip(long requestedSkip) {
-    int actualSkip = max - cur;
-    if (requestedSkip < actualSkip) {
-      if (requestedSkip < 0) {
-        actualSkip = 0;
-      } else {
-        actualSkip = (int) requestedSkip;
+","[{'comment': ""```suggestion\r\n    // actual skip is at least 0, but no more than what's available\r\n    BiFunction<Integer,Integer,Integer> skipValue = (current, skip) -> Math.max(0, Math.min(max - current, skip));\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in eb92e96', 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -78,29 +94,34 @@ public int read(byte[] b, int offset, int length) {
       length = avail;
     }
 
-    System.arraycopy(buffer, cur, b, offset, length);
-    cur += length;
+    System.arraycopy(buffer, currentValue, b, offset, length);
     return length;
   }
 
   @Override
   public long skip(long requestedSkip) {
-    int actualSkip = max - cur;
-    if (requestedSkip < actualSkip) {
-      if (requestedSkip < 0) {
-        actualSkip = 0;
-      } else {
-        actualSkip = (int) requestedSkip;
+
+    BiFunction<Integer,Integer,Integer> skipValue = (current, skip) -> {
+      int actual = max - current;
+      if (skip < actual) {
+        actual = Math.max(skip, 0);","[{'comment': '```suggestion\r\n```', 'commenter': 'ctubbsii'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -78,29 +94,34 @@ public int read(byte[] b, int offset, int length) {
       length = avail;
     }
 
-    System.arraycopy(buffer, cur, b, offset, length);
-    cur += length;
+    System.arraycopy(buffer, currentValue, b, offset, length);
     return length;
   }
 
   @Override
   public long skip(long requestedSkip) {
-    int actualSkip = max - cur;
-    if (requestedSkip < actualSkip) {
-      if (requestedSkip < 0) {
-        actualSkip = 0;
-      } else {
-        actualSkip = (int) requestedSkip;
+
+    BiFunction<Integer,Integer,Integer> skipValue = (current, skip) -> {
+      int actual = max - current;
+      if (skip < actual) {
+        actual = Math.max(skip, 0);
       }
-    }
+      return actual;
+    };","[{'comment': '```suggestion\r\n```', 'commenter': 'ctubbsii'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -78,29 +94,34 @@ public int read(byte[] b, int offset, int length) {
       length = avail;
     }
 
-    System.arraycopy(buffer, cur, b, offset, length);
-    cur += length;
+    System.arraycopy(buffer, currentValue, b, offset, length);
     return length;
   }
 
   @Override
   public long skip(long requestedSkip) {
-    int actualSkip = max - cur;
-    if (requestedSkip < actualSkip) {
-      if (requestedSkip < 0) {
-        actualSkip = 0;
-      } else {
-        actualSkip = (int) requestedSkip;
+
+    BiFunction<Integer,Integer,Integer> skipValue = (current, skip) -> {
+      int actual = max - current;
+      if (skip < actual) {
+        actual = Math.max(skip, 0);
       }
-    }
+      return actual;
+    };
+
+    IntBinaryOperator add = (cur1, skip) -> {
+      int actual = skipValue.apply(cur1, skip);
+      return cur1 + actual;
+    };
+
+    int currentValue = cur.getAndAccumulate((int) requestedSkip, add);","[{'comment': '```suggestion\r\n    // advance the pointer and return the actual amount skipped\r\n    int currentValue = cur.getAndAccumulate((int) requestedSkip, add);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in eb92e96', 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -68,7 +72,19 @@ public int read(byte[] b, int offset, int length) {
       return 0;
     }
 
-    int avail = max - cur;
+    IntBinaryOperator add = (cur1, length1) -> {","[{'comment': ""```suggestion\r\n    // compute how much to read, based on what's left available\r\n    IntBinaryOperator add = (cur1, length1) -> {\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in eb92e96', 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -41,14 +44,15 @@ public class SeekableByteArrayInputStream extends InputStream {
   // thread 2 sees all of thread 1 changes before setting the volatile.
   @SuppressFBWarnings(value = ""VO_VOLATILE_REFERENCE_TO_ARRAY"",
       justification = ""see explanation above"")
-  private volatile byte[] buffer;
-  private int cur;
-  private int max;
+  private final byte[] buffer;
+  private final AtomicInteger cur = new AtomicInteger(0);
+  private final int max;
 
   @Override
   public int read() {
-    if (cur < max) {
-      return buffer[cur++] & 0xff;
+    final int currentValue = cur.getAndAccumulate(1, (v, x) -> v < max ? v + x : v);","[{'comment': ""```suggestion\r\n    // advance the pointer by 1 if we haven't reached the end\r\n    final int currentValue = cur.getAndAccumulate(1, (v, x) -> v < max ? v + x : v);\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in eb92e96', 'commenter': 'DomGarguilo'}]"
3219,core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java,"@@ -124,26 +145,24 @@ public void close() throws IOException {}
   public SeekableByteArrayInputStream(byte[] buf) {
     requireNonNull(buf, ""bug argument was null"");
     this.buffer = buf;
-    this.cur = 0;
     this.max = buf.length;
   }
 
   public SeekableByteArrayInputStream(byte[] buf, int maxOffset) {
     requireNonNull(buf, ""bug argument was null"");
     this.buffer = buf;
-    this.cur = 0;
     this.max = maxOffset;
   }
 
   public void seek(int position) {
     if (position < 0 || position >= max) {
       throw new IllegalArgumentException(""position = "" + position + "" maxOffset = "" + max);
     }
-    this.cur = position;
+    this.cur.set(position);
   }
 
   public int getPosition() {
-    return this.cur;
+    return this.cur.get();
   }
 
   byte[] getBuffer() {","[{'comment': ""This method looks dangerous. We can definitely get into trouble if another thread is altering the buffer returned by this. However... I don't think marking it volatile would have protected us from that. If we don't need this method, it should be removed."", 'commenter': 'ctubbsii'}, {'comment': ""It looks like this is only used in one place.  `SeekableByteArrayInputStream.getBuffer()` is used in `CachableBlockFile.CachedBlockRead.getBuffer()` which is only called in `MultiLevelIndex.IndexBlock.readFields()`. There is a comment that says `this block is cached, so avoid copy` in `readFields()` so making a copy to avoid altering the original buffer might not be an option here. As far as I can tell though, the byte[] returned by `getBuffer()` is never altered, only read, so it might not be an issue in the current code. Would be nice to make it immutable but there is no immutable byte array in java.\r\n\r\nI'm not too sure how your concern should be addressed @ctubbsii. Do you have any ideas? If not do you think this PR is ready to merge?"", 'commenter': 'DomGarguilo'}]"
3244,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1024,12 +1024,11 @@ private SortedMap<TServerInstance,TabletServerStatus> gatherTableInformation(
         }
       }));
     }
-    final long timeToWaitForCompletion = Math.max(10000, rpcTimeout / 3);
+    final long timeToWaitNanos = MILLISECONDS.toNanos(Math.max(10_000, rpcTimeout / 3));","[{'comment': 'To avoid the loss of precision when dividing by 3, could convert to nanos before dividing. Could also convert from seconds instead of millis for the numeric literal:\r\n\r\n```suggestion\r\n    // wait at least 10 seconds\r\n    final long timeToWaitNanos = Math.max(SECONDS.toNanos(10), MILLISECONDS.toNanos(rpcTimeout) / 3);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 4295d305ce', 'commenter': 'EdColeman'}, {'comment': ""I don't think you applied my change. It looks like a different, similar change was applied. The result is an unnecessary extra conversion that is incorrect."", 'commenter': 'ctubbsii'}]"
3244,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1024,12 +1024,13 @@ private SortedMap<TServerInstance,TabletServerStatus> gatherTableInformation(
         }
       }));
     }
-    final long timeToWaitForCompletion = Math.max(10000, rpcTimeout / 3);
+    // wait at least 10 seconds
+    final long timeToWaitNanos =
+        MILLISECONDS.toNanos(Math.max(SECONDS.toNanos(10), MILLISECONDS.toNanos(rpcTimeout) / 3));","[{'comment': 'These are already nanos.\r\n\r\n```suggestion\r\n    final long timeToWaitNanos = Math.max(SECONDS.toNanos(10), MILLISECONDS.toNanos(rpcTimeout) / 3);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'fixed in fa538b0b3e', 'commenter': 'EdColeman'}, {'comment': ""As you predicted, my formatting wasn't quite right. One thing that might work, to make the line shorter, is to rename the variable from `timeToWaitNanos` to `nanosToWait`. That might be enough to make it fit on one line."", 'commenter': 'ctubbsii'}]"
3288,server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java,"@@ -87,6 +93,24 @@ public void runServer() throws Exception {
     }
   }
 
+  @Override
+  public void registerMetrics(MeterRegistry registry) {
+    lowMemoryMetricGuage =
+        Gauge
+            .builder(METRICS_APP_PREFIX + applicationName + ""."" + hostname + "".""","[{'comment': 'I\'m not super familiar with Micrometer, but the way this is currently implemented, wouldn\'t\r\nthe metrics be created in the following structure?\r\n`accumulo.app.app1.server1.detected.low.memory = 1|0` \r\n`accumulo.app.app1.server2.detected.low.memory = 1|0`\r\n`accumulo.app.app2.server1.detected.low.memory = 1|0`\r\n\r\nI\'m pretty sure these would all be considered unique metrics.\r\n\r\nDoes Micrometer support tags or metric labels? \r\nIf the application name and host name were added as tags on the metric, then the metric would look like\r\n`accumulo.app.detected.low.memory{""application Name"": ""app1"", ""hostname"": ""server1""} = 1|0` \r\n\r\nThis drops the unique metric definition down to `accumulo.app.detected.low.memory` and allows sorting and filtering based on `application name` or `hostname`. \r\n\r\nAlerting still works by firing on `accumulo.app.detected.low.memory =1` but better granularity is allowed on the visualization and alerting ends.\r\n', 'commenter': 'ddanielr'}, {'comment': ""There are tags - I'm trying to confirm that they are being set correctly as I look at Dave's other comments."", 'commenter': 'EdColeman'}, {'comment': 'I agree, I think we want something like `accumulo.server.detected.low.memory` and the application name should be in a tag.', 'commenter': 'dlmarion'}, {'comment': 'Leveraged tags in 674a9da956', 'commenter': 'EdColeman'}]"
3288,core/src/main/java/org/apache/accumulo/core/metrics/MetricsProducer.java,"@@ -594,6 +601,8 @@ public interface MetricsProducer {
 
   Logger LOG = LoggerFactory.getLogger(MetricsProducer.class);
 
+  String METRICS_APP_PREFIX = ""accumulo.app."";","[{'comment': '""app"" sounds like ""mobile app"", and sounds a bit ""trendy"" and less serious/scientific. Maybe ""server""? Or just ""accumulo.""? Or ""accumulo.application""?', 'commenter': 'ctubbsii'}, {'comment': 'addressed in 674a9da956', 'commenter': 'EdColeman'}]"
3288,server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java,"@@ -35,21 +36,25 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public abstract class AbstractServer implements AutoCloseable, Runnable {
+import io.micrometer.core.instrument.Gauge;
+import io.micrometer.core.instrument.MeterRegistry;
+
+public abstract class AbstractServer implements AutoCloseable, MetricsProducer, Runnable {
 
   private final ServerContext context;
   protected final String applicationName;
   private final String hostname;
-  private final Logger log;
+
+  private Gauge lowMemoryMetricGuage = null;
 
   protected AbstractServer(String appName, ConfigOpts opts, String[] args) {
-    this.log = LoggerFactory.getLogger(getClass().getName());
     this.applicationName = appName;
     opts.parseArgs(appName, args);
     var siteConfig = opts.getSiteConfiguration();
     this.hostname = siteConfig.get(Property.GENERAL_PROCESS_BIND_ADDRESS);
     SecurityUtil.serverLogin(siteConfig);
     context = new ServerContext(siteConfig);
+    Logger log = LoggerFactory.getLogger(getClass().getName());","[{'comment': 'I think you can just use the class, rather than explicitly call `.getName()`.\r\n```suggestion\r\n    Logger log = LoggerFactory.getLogger(getClass());\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'fixed in 674a9da956', 'commenter': 'EdColeman'}]"
3288,server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java,"@@ -87,6 +93,24 @@ public void runServer() throws Exception {
     }
   }
 
+  @Override
+  public void registerMetrics(MeterRegistry registry) {
+    lowMemoryMetricGuage =
+        Gauge
+            .builder(METRICS_APP_PREFIX + applicationName + ""."" + hostname + "".""
+                + METRICS_APP_LOW_MEMORY, this, this::lowMemDetected)
+            .description(
+                ""reports 1 when process memory usage is above threshold, 0 when memory is okay"") // optional
+            .register(registry);
+  }
+
+  private int lowMemDetected(AbstractServer abstractServer) {
+    if (abstractServer.context.getLowMemoryDetector().isRunningLowOnMemory()) {
+      return 1;
+    }
+    return 0;","[{'comment': '```suggestion\r\n    return abstractServer.context.getLowMemoryDetector().isRunningLowOnMemory() ? 1 : 0;\r\n```\r\n\r\nOr, this looks cleaner, because it seems to be passing the current object to itself:\r\n\r\n```suggestion\r\n    return context.getLowMemoryDetector().isRunningLowOnMemory() ? 1 : 0;\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'fixed in 674a9da956 (method moved to ProcessMetrics)', 'commenter': 'EdColeman'}]"
3288,server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java,"@@ -602,7 +602,7 @@ public void run() {
       LOG.error(""Error initializing metrics, metrics will not be emitted."", e1);
     }
     pausedMetrics = new PausedCompactionMetrics();
-    MetricsUtil.initializeProducers(this, pausedMetrics);
+    initServerMetrics(pausedMetrics);","[{'comment': ""I think `this` was dropped accidentally. `Compactor` implements `MetricsProducer` too, so it's `registerMetrics` method is not getting called."", 'commenter': 'dlmarion'}, {'comment': 'fixed in 49b93b349e', 'commenter': 'EdColeman'}]"
3288,server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java,"@@ -87,6 +94,16 @@ public void runServer() throws Exception {
     }
   }
 
+  @Override
+  public void registerMetrics(MeterRegistry registry) {
+    processMetrics.registerMetrics(registry);
+  }
+
+  public void initServerMetrics(MetricsProducer... producers) {","[{'comment': ""I don't think this method is necessary, and it's initializing the metrics for AbstractServer differently than everything else. Since you have modified `AbstractServer` to implement `MetricsProducer`, then:\r\n\r\n  1. You should have any subclasses of `AbstractServer` call `super.registerMetrics` in their respective `registerMetrics` method. For example, [Compactor.registerMetrics](https://github.com/apache/accumulo/blob/15c9f55280f4ce28a80b800f1e05128f33c85db3/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java#L170).\r\n  2. You should pass `this` in subclasses of `AbstractServer` in the call to `MetricsUtil.initializeProducers`."", 'commenter': 'dlmarion'}, {'comment': ""@dlmarion What do you think is easier to understand?  Classes that extended abstract server must call `super.registerMetrics` or call a method 'initServerMetrics' ?  Either way requires something that may not be obvious.  \r\n\r\nBecause of the tagging, the registration / initialization needs to occur in the run method - and after certain things have been initialized (host and port) \r\n\r\nI'm not thrilled with either approach, but went with a specific `initServerMetrics` method - if relying on calling `super.registerMetrics' and the needing to call the static `MetricsUtil.initializeProducers` which results in an indirect call to `registerMetrics`?\r\n\r\nI was thinking about moving the static `initializeMetrics` out of MetricsUtil and put the functionality into AbstractServer - I think that functionality only applies to services extending AbstractServer.  \r\n\r\nThe `initializeProducers` functionality needs to exist to register metrics producers for things that are created outside of the AbstarctServer context (thrift metrics, cache,....) - but those are created / exist within a service context that has already been expected to initialize the registry.\r\n"", 'commenter': 'EdColeman'}, {'comment': ""> @dlmarion What do you think is easier to understand? Classes that extended abstract server must call `super.registerMetrics` or call a method 'initServerMetrics' ? Either way requires something that may not be obvious.\r\n\r\nWith Java, I think it's standard practice, or should be, for developers to  evaluate whether or not the superclass method needs to be called when overriding a method. \r\n\r\n> \r\n> Because of the tagging, the registration / initialization needs to occur in the run method - and after certain things have been initialized (host and port)\r\n> \r\nagreed\r\n\r\n> I'm not thrilled with either approach, but went with a specific `initServerMetrics` method - if relying on calling `super.registerMetrics' and the needing to call the static `MetricsUtil.initializeProducers`which results in an indirect call to`registerMetrics`?\r\n> \r\n> I was thinking about moving the static `initializeMetrics` out of MetricsUtil and put the functionality into AbstractServer - I think that functionality only applies to services extending AbstractServer.\r\n> \r\n\r\nI think that might be fine iff only AbstractServer subclasses call that method.\r\n\r\n> The `initializeProducers` functionality needs to exist to register metrics producers for things that are created outside of the AbstarctServer context (thrift metrics, cache,....) - but those are created / exist within a service context that has already been expected to initialize the registry.\r\n\r\nagreed"", 'commenter': 'dlmarion'}, {'comment': 'Addressed in 49b93b349e - backed of removal of MetricsUtil for now to minimize changes.  May look into additional changes in future PRs, but this code works as expected.', 'commenter': 'EdColeman'}]"
3321,core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java,"@@ -401,6 +401,23 @@ public static String getRowPrefix() {
 
   }
 
+  /**
+   * Holds error message processing flags
+   */
+  public static class ProblemSection {
+    private static final Section section =
+        new Section(RESERVED_PREFIX + ""err_"", true, RESERVED_PREFIX + ""err`"", false);","[{'comment': 'This section definition includes the underscore as a mandatory part of the section. That\'s the most narrowly scoped definition, and completely fine. However, I\'m a bit surprised that the section wasn\'t defined more widely as everything starting with `err`, as in something like:\r\n\r\n```java\r\n      new Section(RESERVED_PREFIX + ""err"", true, RESERVED_PREFIX + ""ers"", false);\r\n```\r\n\r\nI am not suggesting changing it... just noting the distinction between the narrow section definition and the wider definition.', 'commenter': 'ctubbsii'}]"
3321,server/base/src/main/java/org/apache/accumulo/server/problems/ProblemReports.java,"@@ -217,9 +218,9 @@ private Iterator<Entry<Key,Value>> getIter2() {
                 scanner.setTimeout(3, TimeUnit.SECONDS);
 
                 if (table == null) {
-                  scanner.setRange(new Range(new Text(""~err_""), false, new Text(""~err`""), false));
+                  scanner.setRange(ProblemSection.getRange());","[{'comment': ""One difference is that the new version has the start key inclusive, and the old range has it exclusive. I don't think this matters, since we don't expect any rows to just include the prefix without a tableId after the underscore, so this isn't an important change. I'm just noting it here as a very slight difference in behavior, in case there's a surprise later, and this comment serves to help debug something."", 'commenter': 'ctubbsii'}]"
3321,server/base/src/main/java/org/apache/accumulo/server/problems/ProblemReports.java,"@@ -217,9 +218,9 @@ private Iterator<Entry<Key,Value>> getIter2() {
                 scanner.setTimeout(3, TimeUnit.SECONDS);
 
                 if (table == null) {
-                  scanner.setRange(new Range(new Text(""~err_""), false, new Text(""~err`""), false));
+                  scanner.setRange(ProblemSection.getRange());
                 } else {
-                  scanner.setRange(new Range(new Text(""~err_"" + table)));
+                  scanner.setRange(new Range(new Text(ProblemSection.getRowPrefix() + table)));","[{'comment': 'This can avoid a Text:\r\n\r\n```suggestion\r\n                  scanner.setRange(new Range(ProblemSection.getRowPrefix() + table));\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'I found two more instances of `new Text` use in ProblemReports and was able to remove all three and the import `Text` statement. \r\n', 'commenter': 'ddanielr'}]"
3364,server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java,"@@ -224,8 +225,15 @@ public List<Short> check(Environment env, Mutation mutation) {
         try {
           TabletHostingGoalUtil.fromValue(new Value(columnUpdate.getValue()));
         } catch (IllegalArgumentException e) {
-          violations = addViolation(violations, 4);
+          violations = addViolation(violations, 10);
         }
+      } else if (ServerColumnFamily.OPID_COLUMN.equals(columnFamily, columnQualifier)) {","[{'comment': 'I think you need to update the check on line 207 that checks that the Value is not empty.', 'commenter': 'dlmarion'}]"
3384,test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedScanIT.java,"@@ -307,13 +308,16 @@ public void testBatchScanReturnsEarlyDueToLowMemory() throws Exception {
         consumeServerMemory(scanner);
 
         // Wait for longer than the memory check interval
-        Thread.sleep(6000);
+        Thread.sleep(3000);","[{'comment': ""Why do you still have a sleep here? The main point of using the Wait.waitFor() api is you don't need to include anymore artificial sleeps as it will progress when ready."", 'commenter': 'cshannon'}, {'comment': 'The metrics collection is inherently asynchronous - this was to insert some delay and then use the polling in Wait so that the back off could start small and then start backing off, hopefully reducing the over-all time required for the test by minimizing the total time spent waiting for the metrics to update.\r\n\r\nThis test in particular seems to have a race condition with the low memory detector that was recently added - so it was the only one addressed here.  The others were left alone for now', 'commenter': 'EdColeman'}]"
3384,test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedScanIT.java,"@@ -377,31 +381,37 @@ public void testBatchScanPauses() throws Exception {
         // Grab the current paused count, wait two seconds and then confirm that
         // the number of rows fetched by the memoryConsumingScanner has not increased
         // and that the scan delay counter has increased.
-        double returned = SCAN_RETURNED_EARLY.doubleValue();
-        double paused = SCAN_START_DELAYED.doubleValue();
+        final double returned = SCAN_RETURNED_EARLY.doubleValue();
+        final double paused = SCAN_START_DELAYED.doubleValue();
         Thread.sleep(1500);
         assertEquals(currentCount, fetched.get());
-        assertTrue(SCAN_START_DELAYED.doubleValue() >= paused);
-        assertTrue(SCAN_RETURNED_EARLY.doubleValue() >= returned);
-        assertEquals(1, LOW_MEM_DETECTED.get());
+
+        assertTrue(
+            Wait.waitFor(() -> (SCAN_START_DELAYED.doubleValue() >= paused), 20_000L, 1000L));
+        assertTrue(
+            Wait.waitFor(() -> (SCAN_RETURNED_EARLY.doubleValue() >= returned), 20_000L, 1000L));
+        assertTrue(Wait.waitFor(() -> (LOW_MEM_DETECTED.get() == 1), 20_000L, 1000L));
 
         // Perform the check again
-        paused = SCAN_START_DELAYED.doubleValue();
-        returned = SCAN_RETURNED_EARLY.doubleValue();
+        final double paused2 = SCAN_START_DELAYED.doubleValue();
+        final double returned2 = SCAN_RETURNED_EARLY.doubleValue();
         Thread.sleep(1500);
         assertEquals(currentCount, fetched.get());
-        assertTrue(SCAN_START_DELAYED.doubleValue() >= paused);
-        assertEquals(returned, SCAN_RETURNED_EARLY.doubleValue());
-        assertEquals(1, LOW_MEM_DETECTED.get());
+
+        assertTrue(
+            Wait.waitFor(() -> (SCAN_START_DELAYED.doubleValue() >= paused2), 20_000L, 1000L));
+        assertTrue(
+            Wait.waitFor(() -> (SCAN_RETURNED_EARLY.doubleValue() == returned2), 20_000L, 1000L));
+        assertTrue(Wait.waitFor(() -> (LOW_MEM_DETECTED.get() == 1), 20_000L, 1000L));
 
         // Free the memory which will allow the pausing scanner to continue
         freeServerMemory(client, table);
 
         t.join();
         assertEquals(30, fetched.get());
         // allow metic collection to cycle.
-        Thread.sleep(6_000);
-        assertEquals(0, LOW_MEM_DETECTED.get());
+        Thread.sleep(3_000);","[{'comment': 'Another case where the sleep can be removed', 'commenter': 'cshannon'}]"
3384,test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedScanIT.java,"@@ -272,7 +273,7 @@ public void testScanPauses() throws Exception {
         Thread.sleep(1500);
         assertEquals(currentCount, fetched.get());
 
-        assertEquals(1, LOW_MEM_DETECTED.get());
+        assertTrue(Wait.waitFor(() -> (LOW_MEM_DETECTED.get() == 1), 20_000L, 1000L));","[{'comment': ""All of these lines use the same max wait time and delay, and all of them assert that the condition is true (that assertion could probably be moved into the Wait class.. but I didn't check to see where all it was used). All of them also seem to have an unnecessary parens around the condition in the lambda. You should remove all this boilerplate by creating a local method:\r\n\r\n```suggestion\r\n        waitForTrue(() -> LOW_MEM_DETECTED.get() == 1);\r\n```\r\n\r\nThe method would look like:\r\n\r\n```java\r\n  private static void waitForTrue(Wait.Condition condition) throws Exception {\r\n    // could specify 20 seconds, but the default is 30, and that's fine. The default delay is 1.\r\n    assertTrue(Wait.waitFor(condition));\r\n  }\r\n```"", 'commenter': 'ctubbsii'}]"
3389,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/manager.js,"@@ -25,29 +25,52 @@
 
 var managerStatusTable, recoveryListTable;
 
-function refreshManagerBanner() {
+function refreshManagerBanners() {
   getStatus().then(function () {
-    var managerStatus = JSON.parse(sessionStorage.status).managerStatus;
+    const managerStatus = JSON.parse(sessionStorage.status).managerStatus;
 
     // If manager status is error
     if (managerStatus === 'ERROR') {
-      // show banner and hide table
-      $('#managerBanner').show();
+      // show the manager error banner and hide table
+      $('#managerRunningBanner').show();
       $('#managerStatus_wrapper').hide();
     } else {
-      // otherwise, hide banner and show table
-      $('#managerBanner').hide();
+      // otherwise, hide the error banner and show manager table
+      $('#managerRunningBanner').hide();
       $('#managerStatus_wrapper').show();
     }
   });
+
+  getManager().then(function () {
+    const managerData = JSON.parse(sessionStorage.manager);
+    const managerState = managerData.managerState;
+    const managerGoalState = managerData.managerGoalState;
+
+    const isStateGoalDifferent = managerState !== managerGoalState;","[{'comment': 'I think some of the logic might read better if there were fewer negations.\r\n\r\n```suggestion\r\n    const goalStateIsCurrent = managerState == managerGoalState;\r\n```\r\n\r\nAlso, what happens if these are the same, but both are null?\r\nWhat happens if `getManager()` errors?', 'commenter': 'ctubbsii'}, {'comment': ""> I think some of the logic might read better if there were fewer negations.\r\n\r\nI inverted this boolean in a new commit.\r\n \r\n> Also, what happens if these are the same, but both are null?\r\n\r\nThe banner will be hidden when `managerState === null` is checked.\r\n\r\n> What happens if `getManager()` errors?\r\n\r\nWe don't have much error handling anywhere in the monitor code but I could add a catch block in this case to `console.error` the error."", 'commenter': 'DomGarguilo'}, {'comment': ""> We don't have much error handling anywhere in the monitor code but I could add a catch block in this case to `console.error` the error.\r\n\r\nIf there's an error, I think it would show up on the console anyway. I was mostly thinking about making sure the rendered view was sensible in those edge cases. I think if it errors, then it will keep whatever the current message is, whether it's hidden or shown on the page. That could lead to user confusion if it contradicts the message in the other banner about the manager not running."", 'commenter': 'ctubbsii'}, {'comment': ""I think we don't need to worry about this edge case too much unless we start seeing it a lot."", 'commenter': 'ctubbsii'}]"
3389,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/manager.js,"@@ -25,29 +25,52 @@
 
 var managerStatusTable, recoveryListTable;
 
-function refreshManagerBanner() {
+function refreshManagerBanners() {
   getStatus().then(function () {
-    var managerStatus = JSON.parse(sessionStorage.status).managerStatus;
+    const managerStatus = JSON.parse(sessionStorage.status).managerStatus;
 
     // If manager status is error
     if (managerStatus === 'ERROR') {
-      // show banner and hide table
-      $('#managerBanner').show();
+      // show the manager error banner and hide table
+      $('#managerRunningBanner').show();
       $('#managerStatus_wrapper').hide();","[{'comment': ""Is `#managerStatus_wrapper` still valid? I can't find any element named that. Looking through the code for `_wrapper`, I'm wondering if this is a DataTables-specific thing that gets auto-created by DataTables."", 'commenter': 'ctubbsii'}, {'comment': ""> Looking through the code for `_wrapper`, I'm wondering if this is a DataTables-specific thing that gets auto-created by DataTables.\r\n\r\nThis is correct. It is a way to hide or show an entire DataTable."", 'commenter': 'DomGarguilo'}]"
3389,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/manager.js,"@@ -25,29 +25,52 @@
 
 var managerStatusTable, recoveryListTable;
 
-function refreshManagerBanner() {
+function refreshManagerBanners() {
   getStatus().then(function () {
-    var managerStatus = JSON.parse(sessionStorage.status).managerStatus;
+    const managerStatus = JSON.parse(sessionStorage.status).managerStatus;
 
     // If manager status is error
     if (managerStatus === 'ERROR') {
-      // show banner and hide table
-      $('#managerBanner').show();
+      // show the manager error banner and hide table
+      $('#managerRunningBanner').show();
       $('#managerStatus_wrapper').hide();
     } else {
-      // otherwise, hide banner and show table
-      $('#managerBanner').hide();
+      // otherwise, hide the error banner and show manager table
+      $('#managerRunningBanner').hide();
       $('#managerStatus_wrapper').show();
     }
   });
+
+  getManager().then(function () {
+    const managerData = JSON.parse(sessionStorage.manager);
+    const managerState = managerData.managerState;
+    const managerGoalState = managerData.managerGoalState;
+
+    const isStateGoalDifferent = managerState !== managerGoalState;
+
+    // if the manager state is normal and the goal is the same as the state or manager is not running, return early
+    if ((managerState === 'NORMAL' && !isStateGoalDifferent) || managerState === null) {
+      $('#managerStateBanner').hide();
+      return;
+    }
+
+    // update the manager state banner message and show it
+    let bannerMessage = 'Manager state: ' + managerState;
+    if (isStateGoalDifferent) {
+      bannerMessage += '. Manager goal state: ' + managerGoalState;
+    }
+    $('#managerStateBanner .alert.alert-warning').text(bannerMessage);","[{'comment': ""It seems weird to query the inner div by the alert class labels on it. Why not just give the inner-div a name? Or just select the child element?\r\n\r\nFor example, either of these should work:\r\n\r\n(select all `<div>` elements inside the element named `#managerStateBanner`)\r\n```suggestion\r\n    $('#managerStateBanner div').text(bannerMessage);\r\n```\r\n\r\n(select all `<div>` elements that are one level inside the element named `#managerStateBanner`)\r\n```suggestion\r\n    $('#managerStateBanner > div').text(bannerMessage);\r\n```\r\n\r\nUsing the classes to select seems fragile. If we change the presentation style in the template, we'll have to change the classes in multiple places (will have to update the JavaScript to match). Better to give a specific name, or navigate using structural elements, rather than depend on class styles."", 'commenter': 'ctubbsii'}, {'comment': 'Added an inner class.', 'commenter': 'DomGarguilo'}]"
3389,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/manager.js,"@@ -25,29 +25,52 @@
 
 var managerStatusTable, recoveryListTable;
 
-function refreshManagerBanner() {
+function refreshManagerBanners() {
   getStatus().then(function () {
-    var managerStatus = JSON.parse(sessionStorage.status).managerStatus;
+    const managerStatus = JSON.parse(sessionStorage.status).managerStatus;
 
     // If manager status is error
     if (managerStatus === 'ERROR') {
-      // show banner and hide table
-      $('#managerBanner').show();
+      // show the manager error banner and hide table
+      $('#managerRunningBanner').show();
       $('#managerStatus_wrapper').hide();
     } else {
-      // otherwise, hide banner and show table
-      $('#managerBanner').hide();
+      // otherwise, hide the error banner and show manager table
+      $('#managerRunningBanner').hide();
       $('#managerStatus_wrapper').show();
     }
   });
+
+  getManager().then(function () {
+    const managerData = JSON.parse(sessionStorage.manager);
+    const managerState = managerData.managerState;
+    const managerGoalState = managerData.managerGoalState;
+
+    const isStateGoalDifferent = managerState !== managerGoalState;
+
+    // if the manager state is normal and the goal is the same as the state or manager is not running, return early
+    if ((managerState === 'NORMAL' && !isStateGoalDifferent) || managerState === null) {","[{'comment': ""Is `managerState` always `null` when the `managerStatus` is `ERROR`, like in the previous block? It seems like that's not always the case."", 'commenter': 'ctubbsii'}, {'comment': 'Yea I think so. If managerStatus (which is pulled from `/rest/status`) is ERROR, the manager is down which makes most of the stats at `/rest/manager` become `null`.', 'commenter': 'DomGarguilo'}, {'comment': 'The issue is that they come from two different endpoints, and are therefore fetched separately. There is no reason to think they are always in sync.', 'commenter': 'ctubbsii'}, {'comment': 'The new code does not rely on any data from `/rest/status`. The two blocks in `refreshManagerBanners()` each pull from different endpoints and act independently of each other.', 'commenter': 'DomGarguilo'}, {'comment': ""> The new code does not rely on any data from `/rest/status`. The two blocks in `refreshManagerBanners()` each pull from different endpoints and act independently of each other.\r\n\r\nRight, that's my point. There are two banners. The other banner pulls from the other endpoint. The end result is that it could say something in one banner that contradicts the other banner. For example, it could say that it it's in safe mode *and* that it's not running at the same time. But, maybe that's not really a problem. I just wanted to make sure we thought about those edge cases."", 'commenter': 'ctubbsii'}, {'comment': ""I think we don't need to worry about this edge case too much unless we start seeing it a lot."", 'commenter': 'ctubbsii'}]"
3389,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/manager.ftl,"@@ -23,9 +23,12 @@
           <h3>Manager Server Overview</h3>
        </div>
     </div>
-    <div id=""managerBanner"" style=""display: none;"">
+    <div id=""managerRunningBanner"" style=""display: none;"">
         <div class=""alert alert-danger"" role=""alert"">Manager Server Not Running</div>
     </div>
+    <div id=""managerStateBanner"" style=""display: none;"">
+        <div class=""alert alert-warning manager-banner-message"" role=""alert""></div>","[{'comment': 'This is a unique item, and should thus be an ID rather than a CLASS.\r\n```suggestion\r\n        <div id=""manager-banner-message"" class=""alert alert-warning"" role=""alert""></div>\r\n```', 'commenter': 'ctubbsii'}]"
3389,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/manager.js,"@@ -25,29 +25,54 @@
 
 var managerStatusTable, recoveryListTable;
 
-function refreshManagerBanner() {
+function refreshManagerBanners() {
   getStatus().then(function () {
-    var managerStatus = JSON.parse(sessionStorage.status).managerStatus;
+    const managerStatus = JSON.parse(sessionStorage.status).managerStatus;
 
     // If manager status is error
     if (managerStatus === 'ERROR') {
-      // show banner and hide table
-      $('#managerBanner').show();
+      // show the manager error banner and hide table
+      $('#managerRunningBanner').show();
       $('#managerStatus_wrapper').hide();
     } else {
-      // otherwise, hide banner and show table
-      $('#managerBanner').hide();
+      // otherwise, hide the error banner and show manager table
+      $('#managerRunningBanner').hide();
       $('#managerStatus_wrapper').show();
     }
   });
+
+  getManager().then(function () {
+    const managerData = JSON.parse(sessionStorage.manager);
+    const managerState = managerData.managerState;
+    const managerGoalState = managerData.managerGoalState;
+
+    const isStateGoalSame = managerState === managerGoalState;
+
+    // if the manager state is normal and the goal state is the same as the current state,
+    // or of the manager is not running, hide the state banner and return early
+    if ((managerState === 'NORMAL' && isStateGoalSame) || managerState === null) {
+      $('#managerStateBanner').hide();
+      return;
+    }
+
+    // update the manager state banner message and show it
+    let bannerMessage = 'Manager state: ' + managerState;
+    if (!isStateGoalSame) {
+      // only show the goal state if it differs from the manager's current state
+      bannerMessage += '. Manager goal state: ' + managerGoalState;
+    }
+    $('.manager-banner-message').text(bannerMessage);","[{'comment': ""```suggestion\r\n    $('#manager-banner-message').text(bannerMessage);\r\n```"", 'commenter': 'ctubbsii'}]"
3398,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -984,7 +984,7 @@ synchronized void completeClose(boolean saveState, boolean completeClose) throws
             activeScans.size());
         this.wait(50);
       } catch (InterruptedException e) {
-        log.error(e.toString());
+        log.error(""{}"", e, e);","[{'comment': '```suggestion\r\n        log.error(""Interrupted waiting to completeClose for extent {}, {}"", extent, e, e);\r\n```', 'commenter': 'EdColeman'}, {'comment': 'This is fine with me, it provides more context.', 'commenter': 'cshannon'}]"
3398,server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java,"@@ -984,7 +984,7 @@ synchronized void completeClose(boolean saveState, boolean completeClose) throws
             activeScans.size());
         this.wait(50);
       } catch (InterruptedException e) {
-        log.error(e.toString());
+        log.error(""Interrupted waiting to completeClose for extent {}, {}"", extent, e, e);","[{'comment': 'With a custom message, it\'s no longer necessary to force the exception.toString to be part of the message. That\'s mostly a hack to satisfy the logging API when you don\'t have a custom message to satisfy the API\'s need for a string as the first parameter. Having the stack trace is sufficient, as it will already display the message.\r\n\r\n```suggestion\r\n        log.error(""Interrupted waiting to completeClose for extent {}"", extent, e);\r\n```', 'commenter': 'ctubbsii'}]"
3400,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -36,25 +39,30 @@ private ClassLoaderUtil() {
   /**
    * Initialize the ContextClassLoaderFactory
    */
-  public static synchronized void initContextFactory(AccumuloConfiguration conf) {
+  public static synchronized void initContextFactory(final AccumuloConfiguration conf) {
     if (FACTORY == null) {
       LOG.debug(""Creating {}"", ContextClassLoaderFactory.class.getName());
       String factoryName = conf.get(Property.GENERAL_CONTEXT_CLASSLOADER_FACTORY);
       if (factoryName == null || factoryName.isEmpty()) {
-        // load the default implementation
         LOG.info(""Using default {}, which is subject to change in a future release"",
             ContextClassLoaderFactory.class.getName());
-        FACTORY = new DefaultContextClassLoaderFactory(conf);
-      } else {
-        // load user's selected implementation
-        try {
-          var factoryClass = Class.forName(factoryName).asSubclass(ContextClassLoaderFactory.class);
-          LOG.info(""Creating {}: {}"", ContextClassLoaderFactory.class.getName(), factoryName);
-          FACTORY = factoryClass.getDeclaredConstructor().newInstance();
-        } catch (ReflectiveOperationException e) {
-          throw new IllegalStateException(""Unable to load and initialize class: "" + factoryName, e);
-        }
+        factoryName = DefaultContextClassLoaderFactory.class.getName();
+      }
+
+      try {
+        var factoryClass = Class.forName(factoryName).asSubclass(ContextClassLoaderFactory.class);
+        LOG.info(""Creating {}: {}"", ContextClassLoaderFactory.class.getName(), factoryName);
+        FACTORY = factoryClass.getDeclaredConstructor().newInstance();
+      } catch (ReflectiveOperationException e) {
+        throw new IllegalStateException(""Unable to load and initialize class: "" + factoryName, e);
       }
+
+      FACTORY.setEnvironment(new ContextClassLoaderEnvironment() {
+        @Override
+        public ServiceEnvironment.Configuration getConfiguration() {
+          return new ConfigurationImpl(conf);
+        }
+      });","[{'comment': 'This can be a lambda instead of an anonymous class.', 'commenter': 'ctubbsii'}, {'comment': 'got it', 'commenter': 'ivakegg'}]"
3400,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -36,25 +39,30 @@ private ClassLoaderUtil() {
   /**
    * Initialize the ContextClassLoaderFactory
    */
-  public static synchronized void initContextFactory(AccumuloConfiguration conf) {
+  public static synchronized void initContextFactory(final AccumuloConfiguration conf) {
     if (FACTORY == null) {
       LOG.debug(""Creating {}"", ContextClassLoaderFactory.class.getName());
       String factoryName = conf.get(Property.GENERAL_CONTEXT_CLASSLOADER_FACTORY);
       if (factoryName == null || factoryName.isEmpty()) {
-        // load the default implementation
         LOG.info(""Using default {}, which is subject to change in a future release"",
             ContextClassLoaderFactory.class.getName());
-        FACTORY = new DefaultContextClassLoaderFactory(conf);
-      } else {
-        // load user's selected implementation
-        try {
-          var factoryClass = Class.forName(factoryName).asSubclass(ContextClassLoaderFactory.class);
-          LOG.info(""Creating {}: {}"", ContextClassLoaderFactory.class.getName(), factoryName);
-          FACTORY = factoryClass.getDeclaredConstructor().newInstance();
-        } catch (ReflectiveOperationException e) {
-          throw new IllegalStateException(""Unable to load and initialize class: "" + factoryName, e);
-        }
+        factoryName = DefaultContextClassLoaderFactory.class.getName();","[{'comment': ""The default implementation is intentionally treated differently, so users cannot configure it or instantiate it directly. It's for internal use only."", 'commenter': 'ctubbsii'}, {'comment': 'ok', 'commenter': 'ivakegg'}]"
3400,core/src/main/java/org/apache/accumulo/core/classloader/DefaultContextClassLoaderFactory.java,,"[{'comment': 'You might consider adding a comment in the file what makes it ""bad""', 'commenter': 'EdColeman'}, {'comment': 'added a comment in 347d6c3', 'commenter': 'keith-turner'}]"
3400,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -57,4 +57,11 @@ public interface ContextClassLoaderFactory {
    */
   ClassLoader getClassLoader(String contextName);
 
+  /**
+   * Pass the service environment to allow for additional class loader configuration
+   *
+   * @param env the class loader environment
+   */
+  default void setEnvironment(ContextClassLoaderEnvironment env) {}","[{'comment': 'A lot of the spi interfaces have a common pattern of having an init method that a takes a single InitParameters interface like [here](https://github.com/apache/accumulo/blob/2f27dbd188ea5cee32f486599b97181b136fa787/core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionPlanner.java#L72) \r\n\r\n```suggestion\r\n  default void init(InitParameters params) {}\r\n```\r\n\r\nThis could follow that pattern.  Also the init method name is good if its expected be called before anything else and once.', 'commenter': 'keith-turner'}, {'comment': ""I don't mind the name change for the method, but the `InitParameters` doesn't seem to be as accurate as the `...Environment` name. `InitParameters` implies to me that there are fixed params at initialization. I think that's a bit misleading. What's actually being passed is essentially `Supplier<ServiceEnvironment.Configuration>`, which isn't a fixed set of parameters, but a way to peek into the current system configuration, as needed.\r\n\r\nIf it's called `InitParameters`, and the method is still `getConfiguration()`, then that seems to imply a fixed configuration at initialization, when really the only thing that's fixed is the mechanism to get the configuration contained within the environment, not the configuration itself. So, if it's called `InitParameters`, I think the method would need to be changed to something like `.environmentConfigurationSupplier()`, so it's clear that the *supplier* is the fixed parameter, and the configuration is part of the environmental state, not just the state at initialization.\r\n\r\nIn putting together my suggested changes in ivakegg/accumulo#5, I considered suggesting getting rid of the ContextClassLoaderEnvironment entirely, and replace it with a `Supplier<ServiceEnvironment.Configuration>`, but that seemed too limiting, in case we want to expose more of the environment. Ideally, we'd pass the entire `ServiceEnvironment`, instead of just the configuration, but I explained why that was a problem in ivakegg/accumulo#5."", 'commenter': 'ctubbsii'}, {'comment': ""That's a long way of saying, I think `init(ContextClassLoaderEnvironment env)` might be the best option, with the configuration retrievable via `env.getConfiguration()`."", 'commenter': 'ctubbsii'}, {'comment': 'I tried to make a comment on this and it kept failing.  So I tried it at the top level and that worked.  I think GH was having problems. ', 'commenter': 'keith-turner'}, {'comment': ""@keith-turner I see it. I don't mind the name to follow the same pattern. If we're using a level of indirection to get the configuration, do you think it would be better if InitParameters return a ContextClassLoaderEnvironment which returns the ServiceEnvironment.Configuration, or should it just replace the environment, and return the ServiceEnvironment.Configuration? Or should it return a `Supplier<ServiceEnvironment.Configuration` (this last option is what I was thinking, so we don't need to have both a new Environment class and a new InitParameters class, and we can just have the latter)?"", 'commenter': 'ctubbsii'}]"
3400,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderEnvironment.java,"@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.spi.common;
+
+/**
+ * The environment provided to the context class loader factory for its use
+ *
+ * @since 2.1.1
+ */
+public interface ContextClassLoaderEnvironment {
+
+  /**
+   * Get the service environment configuration
+   *
+   * @return The configuration
+   * @since 2.1.1","[{'comment': 'Should be able to drop the since tag on the method because its same as the class since tag.\r\n\r\n```suggestion\r\n```', 'commenter': 'keith-turner'}]"
3428,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -873,8 +873,13 @@ public enum Property {
       ""The listening port for the garbage collector's monitor service"", ""1.3.5""),
   GC_DELETE_THREADS(""gc.threads.delete"", ""16"", PropertyType.COUNT,
       ""The number of threads used to delete RFiles and write-ahead logs"", ""1.3.5""),
+  @Deprecated(since = ""2.1.1"", forRemoval = true)
   GC_TRASH_IGNORE(""gc.trash.ignore"", ""false"", PropertyType.BOOLEAN,
       ""Do not use the Trash, even if it is configured."", ""1.5.0""),
+  GC_USE_TRASH(""gc.use.trash"", ""true"", PropertyType.STRING,
+      ""Moves a file to the Trash (if available). Valid values are true, false, and bulk_imports_only.""
+          + "" Mutually exclusive with gc.trash.ignore."",
+      ""2.1.1""),","[{'comment': 'I like the fix of the negation. But, I\'m wondering if there are use cases for use trash for `everything_except_bulk_imports`  or something different for WALs and RFiles. I am thinking we should have a more robust TrashPolicy SPI, where we give it everything we know about a file, and it returns ""Trash"" or ""Delete"". The default implementation would be ""AlwaysTrash"", but we could also provide a ""NeverTrash"" implementation, and anything else more complicated would be up to the user to provide.', 'commenter': 'ctubbsii'}]"
3428,server/gc/src/main/java/org/apache/accumulo/gc/FileJanitor.java,"@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.gc;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.fs.VolumeManager;
+import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class FileJanitor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(FileJanitor.class);
+
+  enum SendFilesToTrash {
+
+    TRUE(""true""), FALSE(""false""), IMPORTS_ONLY(""bulk_imports_only"");
+
+    final String val;
+
+    SendFilesToTrash(String val) {
+      this.val = val;
+    }
+
+    public static SendFilesToTrash fromBoolean(boolean b) {
+      if (b) {
+        return TRUE;
+      }
+      return FALSE;
+    }
+
+    public static SendFilesToTrash fromString(String val) {
+      switch (val.toLowerCase()) {","[{'comment': '```suggestion\r\n      switch (val.toLowerCase().strip()) {\r\n```', 'commenter': 'ctubbsii'}]"
3428,server/gc/src/main/java/org/apache/accumulo/gc/FileJanitor.java,"@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.gc;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.fs.VolumeManager;
+import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class FileJanitor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(FileJanitor.class);
+
+  enum SendFilesToTrash {
+
+    TRUE(""true""), FALSE(""false""), IMPORTS_ONLY(""bulk_imports_only"");
+
+    final String val;
+
+    SendFilesToTrash(String val) {
+      this.val = val;
+    }
+
+    public static SendFilesToTrash fromBoolean(boolean b) {
+      if (b) {
+        return TRUE;
+      }
+      return FALSE;
+    }
+
+    public static SendFilesToTrash fromString(String val) {
+      switch (val.toLowerCase()) {
+        case ""true"":
+          return TRUE;
+        case ""false"":
+          return FALSE;
+        case ""bulk_imports_only"":
+          return IMPORTS_ONLY;
+        default:
+          throw new IllegalArgumentException(""Unknown value: "" + val);
+      }
+    }
+  }
+
+  private final ServerContext context;
+  private final SendFilesToTrash usingTrash;
+
+  @SuppressWarnings(""removal"")","[{'comment': 'This warning could be more narrowly placed on the property name, as a local variable:\r\n\r\n```java\r\n@SuppressWarnings(""removal"")\r\nProperty GC_TRASH_IGNORE = Property.GC_TRASH_IGNORE;\r\n```\r\n\r\nNow, you have a local variable that is not deprecated, that you can use everywhere you are currently using the deprecated one, and you don\'t need to suppress warnings on the entire method.', 'commenter': 'ctubbsii'}]"
3428,server/gc/src/main/java/org/apache/accumulo/gc/FileJanitor.java,"@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.gc;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.fs.VolumeManager;
+import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class FileJanitor {","[{'comment': ""Class needs a javadoc to explain its purpose, so we don't have to infer it by trying to read all the code that uses it."", 'commenter': 'ctubbsii'}]"
3428,server/gc/src/main/java/org/apache/accumulo/gc/FileJanitor.java,"@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.gc;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.fs.VolumeManager;
+import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class FileJanitor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(FileJanitor.class);
+
+  enum SendFilesToTrash {
+
+    TRUE(""true""), FALSE(""false""), IMPORTS_ONLY(""bulk_imports_only"");
+
+    final String val;
+
+    SendFilesToTrash(String val) {
+      this.val = val;
+    }
+
+    public static SendFilesToTrash fromBoolean(boolean b) {
+      if (b) {
+        return TRUE;
+      }
+      return FALSE;
+    }
+
+    public static SendFilesToTrash fromString(String val) {
+      switch (val.toLowerCase()) {
+        case ""true"":
+          return TRUE;
+        case ""false"":
+          return FALSE;
+        case ""bulk_imports_only"":
+          return IMPORTS_ONLY;
+        default:
+          throw new IllegalArgumentException(""Unknown value: "" + val);
+      }
+    }
+  }
+
+  private final ServerContext context;
+  private final SendFilesToTrash usingTrash;
+
+  @SuppressWarnings(""removal"")
+  public FileJanitor(ServerContext context) {
+    this.context = context;
+    final AccumuloConfiguration conf = this.context.getConfiguration();
+    if (conf.isPropertySet(Property.GC_TRASH_IGNORE) && conf.isPropertySet(Property.GC_USE_TRASH)) {
+      throw new IllegalStateException(""Cannot specify both "" + Property.GC_TRASH_IGNORE.getKey()
+          + "" and "" + Property.GC_USE_TRASH.getKey() + "" properties."");
+    }","[{'comment': ""It's okay to set them both as long as they are consistent. They should be opposite values, though. So, instead of checking that they are set, we should check that they are not equal (assuming they are boolean, and the bulk imports option isn't used). As long as they aren't equal to each other, that's fine. Some users are very pedantic, and they set everything, because they aren't sure which property to set, or because they have set an old value in ZK, and the new one in the config file, or something similar. That's not a problem, as long as its consistent."", 'commenter': 'ctubbsii'}]"
3428,server/gc/src/main/java/org/apache/accumulo/gc/FileJanitor.java,"@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.gc;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.fs.VolumeManager;
+import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class FileJanitor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(FileJanitor.class);
+
+  enum SendFilesToTrash {
+
+    TRUE(""true""), FALSE(""false""), IMPORTS_ONLY(""bulk_imports_only"");
+
+    final String val;
+
+    SendFilesToTrash(String val) {
+      this.val = val;
+    }
+
+    public static SendFilesToTrash fromBoolean(boolean b) {
+      if (b) {
+        return TRUE;
+      }
+      return FALSE;
+    }
+
+    public static SendFilesToTrash fromString(String val) {
+      switch (val.toLowerCase()) {
+        case ""true"":
+          return TRUE;
+        case ""false"":
+          return FALSE;
+        case ""bulk_imports_only"":
+          return IMPORTS_ONLY;
+        default:
+          throw new IllegalArgumentException(""Unknown value: "" + val);
+      }
+    }
+  }
+
+  private final ServerContext context;
+  private final SendFilesToTrash usingTrash;
+
+  @SuppressWarnings(""removal"")
+  public FileJanitor(ServerContext context) {
+    this.context = context;
+    final AccumuloConfiguration conf = this.context.getConfiguration();
+    if (conf.isPropertySet(Property.GC_TRASH_IGNORE) && conf.isPropertySet(Property.GC_USE_TRASH)) {
+      throw new IllegalStateException(""Cannot specify both "" + Property.GC_TRASH_IGNORE.getKey()
+          + "" and "" + Property.GC_USE_TRASH.getKey() + "" properties."");
+    }
+    if (conf.isPropertySet(Property.GC_TRASH_IGNORE)) {
+      this.usingTrash = SendFilesToTrash.fromBoolean(!conf.getBoolean(Property.GC_TRASH_IGNORE));
+    } else if (conf.isPropertySet(Property.GC_USE_TRASH)) {
+      this.usingTrash = SendFilesToTrash.fromString(conf.get(Property.GC_USE_TRASH));
+    } else {
+      LOG.warn(
+          ""Neither GC trash property was set ({} or {}). Defaulting to using trash for all files (if available)."",
+          Property.GC_TRASH_IGNORE.getKey(), Property.GC_USE_TRASH.getKey());
+      this.usingTrash = SendFilesToTrash.TRUE;
+    }
+  }
+
+  public ServerContext getContext() {
+    return this.context;
+  }
+","[{'comment': ""Don't think we need this getter. If not, it should be removed. Any caller should already have a context."", 'commenter': 'ctubbsii'}]"
3428,server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java,"@@ -436,4 +410,9 @@ public GcCycleMetrics getGcCycleMetrics() {
     return gcCycleMetrics;
   }
 
+  @VisibleForTesting
+  public FileJanitor getFileJanitor() {
+    return this.janitor;
+  }
+","[{'comment': ""This can be package-private. It doesn't need to be public for unit tests in the same package."", 'commenter': 'ctubbsii'}]"
3428,server/gc/src/test/java/org/apache/accumulo/gc/SimpleGarbageCollectorNewPropertyTest.java,"@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.gc;
+
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.partialMockBuilder;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.gc.FileJanitor.SendFilesToTrash;
+import org.apache.hadoop.fs.Path;
+import org.junit.jupiter.api.Test;
+
+public class SimpleGarbageCollectorNewPropertyTest extends SimpleGarbageCollectorTest {
+
+  @Override
+  protected ConfigurationCopy createSystemConfig() {
+    Map<String,String> conf = new HashMap<>();
+    conf.put(Property.INSTANCE_RPC_SASL_ENABLED.getKey(), ""false"");
+    conf.put(Property.GC_CYCLE_START.getKey(), ""1"");
+    conf.put(Property.GC_CYCLE_DELAY.getKey(), ""20"");
+    conf.put(Property.GC_DELETE_THREADS.getKey(), ""2"");
+    conf.put(Property.GC_USE_TRASH.getKey(), ""true"");
+
+    return new ConfigurationCopy(conf);
+  }
+
+  @Test
+  public void testMoveToTrash_NotUsingTrash_importsOnlyEnabled() throws Exception {
+    systemConfig.set(Property.GC_USE_TRASH.getKey(), ""bulk_imports_only"");
+
+    gc = partialMockBuilder(SimpleGarbageCollector.class).addMockedMethod(""getContext"")
+        .addMockedMethod(""getFileJanitor"").createMock();","[{'comment': ""I feel like you could just call `new FileJanitor()` with a mocked context to test it, rather than trying to get it from a partially mocked SimpleGarbageCollector to rely on that object to construct it for you. You're not actually testing anything from SimpleGarbageCollector here. It's just extra noisy test code that is a long-way around getting to the code you're actually testing."", 'commenter': 'ctubbsii'}]"
3428,server/gc/src/test/java/org/apache/accumulo/gc/SimpleGarbageCollectorNewPropertyTest.java,"@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.gc;
+
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.partialMockBuilder;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.gc.FileJanitor.SendFilesToTrash;
+import org.apache.hadoop.fs.Path;
+import org.junit.jupiter.api.Test;
+
+public class SimpleGarbageCollectorNewPropertyTest extends SimpleGarbageCollectorTest {","[{'comment': ""I don't think you need to extend SimpleGarbageCollectorTest to test this."", 'commenter': 'ctubbsii'}]"
3428,server/manager/src/test/java/org/apache/accumulo/manager/upgrade/RootFilesUpgradeTest.java,"@@ -78,6 +78,7 @@ public void renameReplacement(VolumeManager fs, Path tmpDatafile, Path newDatafi
       rename(fs, tmpDatafile, newDatafile);
     }
 
+    @SuppressWarnings(""removal"")","[{'comment': 'Instead of suppressing the old property, can the test be updated to use the new?', 'commenter': 'ctubbsii'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashEnabledIT.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashEnabledIT extends GarbageCollectorTrashBase {
+
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(5);
+  }
+
+  @Override
+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+
+    Map<String,String> hadoopOverrides = new HashMap<>();
+    hadoopOverrides.put(CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY, ""5"");
+    cfg.setHadoopConfOverrides(hadoopOverrides);
+    cfg.useMiniDFS(true);
+
+    cfg.setProperty(Property.GC_CYCLE_START, ""1"");
+    cfg.setProperty(Property.GC_CYCLE_DELAY, ""1"");
+    @SuppressWarnings(""removal"")
+    Property p = Property.GC_TRASH_IGNORE;
+    cfg.setProperty(p, ""false""); // default, use trash if configured
+    cfg.setProperty(Property.GC_PORT, ""0"");
+    cfg.setProperty(Property.TSERV_MAXMEM, ""5K"");
+    cfg.setProperty(Property.TABLE_MAJC_RATIO, ""5.0"");
+    cfg.setProperty(Property.TSERV_MAJC_DELAY, ""180s"");
+  }
+
+  @Test
+  public void testTrashHadoopEnabledAccumuloEnabled() throws Exception {
+    String table = this.getUniqueNames(1)[0];
+    final FileSystem fs = super.getCluster().getFileSystem();
+    super.makeTrashDir(fs);
+    ArrayList<StoredTabletFile> files = null;
+    TableId tid = null;
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
+      files = super.loadData(super.getServerContext(), c, table);
+      assertFalse(files.isEmpty());
+      c.tableOperations().compact(table, new CompactionConfig());
+      tid = TableId.of(c.tableOperations().tableIdMap().get(table));
+    }
+    Thread.sleep(10000);
+    assertEquals(files.size(), super.countFilesInTrash(fs, tid));","[{'comment': 'This could loop waiting for the condition to be true.', 'commenter': 'keith-turner'}, {'comment': 'Addressed in 5ec3ca8.', 'commenter': 'dlmarion'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashEnabledIT.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashEnabledIT extends GarbageCollectorTrashBase {
+
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(5);
+  }
+
+  @Override
+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+
+    Map<String,String> hadoopOverrides = new HashMap<>();
+    hadoopOverrides.put(CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY, ""5"");
+    cfg.setHadoopConfOverrides(hadoopOverrides);
+    cfg.useMiniDFS(true);
+
+    cfg.setProperty(Property.GC_CYCLE_START, ""1"");
+    cfg.setProperty(Property.GC_CYCLE_DELAY, ""1"");
+    @SuppressWarnings(""removal"")
+    Property p = Property.GC_TRASH_IGNORE;
+    cfg.setProperty(p, ""false""); // default, use trash if configured
+    cfg.setProperty(Property.GC_PORT, ""0"");
+    cfg.setProperty(Property.TSERV_MAXMEM, ""5K"");
+    cfg.setProperty(Property.TABLE_MAJC_RATIO, ""5.0"");
+    cfg.setProperty(Property.TSERV_MAJC_DELAY, ""180s"");
+  }
+
+  @Test
+  public void testTrashHadoopEnabledAccumuloEnabled() throws Exception {
+    String table = this.getUniqueNames(1)[0];
+    final FileSystem fs = super.getCluster().getFileSystem();
+    super.makeTrashDir(fs);
+    ArrayList<StoredTabletFile> files = null;
+    TableId tid = null;
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
+      files = super.loadData(super.getServerContext(), c, table);
+      assertFalse(files.isEmpty());
+      c.tableOperations().compact(table, new CompactionConfig());","[{'comment': 'Could wait for the compaction to finish\r\n\r\n```suggestion\r\n      c.tableOperations().compact(table, new CompactionConfig().setWait(true));\r\n```', 'commenter': 'keith-turner'}, {'comment': 'it defaults to true', 'commenter': 'dlmarion'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashEnabledCustomPolicyIT.java,"@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.TrashPolicyDefault;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashEnabledCustomPolicyIT extends GarbageCollectorTrashBase {
+
+  public static class NoFlushFilesInTrashPolicy extends TrashPolicyDefault {
+
+    @Override
+    public boolean moveToTrash(Path path) throws IOException {
+      // Don't put flush files in the Trash
+      if (!path.getName().startsWith(""F"")) {
+        return super.moveToTrash(path);
+      }
+      return false;
+    }
+
+  }
+
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(5);
+  }
+
+  @Override
+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+
+    Map<String,String> hadoopOverrides = new HashMap<>();
+    hadoopOverrides.put(CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY, ""5"");
+    hadoopOverrides.put(""fs.trash.classname"", NoFlushFilesInTrashPolicy.class.getName());
+    cfg.setHadoopConfOverrides(hadoopOverrides);
+    cfg.useMiniDFS(true);
+
+    cfg.setProperty(Property.GC_CYCLE_START, ""1"");
+    cfg.setProperty(Property.GC_CYCLE_DELAY, ""1"");
+    @SuppressWarnings(""removal"")
+    Property p = Property.GC_TRASH_IGNORE;
+    cfg.setProperty(p, ""false""); // default, use trash if configured
+    cfg.setProperty(Property.GC_PORT, ""0"");
+    cfg.setProperty(Property.TSERV_MAXMEM, ""5K"");
+    cfg.setProperty(Property.TABLE_MAJC_RATIO, ""5.0"");
+    cfg.setProperty(Property.TSERV_MAJC_DELAY, ""180s"");
+  }
+
+  @Test
+  public void testTrashHadoopEnabledAccumuloEnabled() throws Exception {
+    String table = this.getUniqueNames(1)[0];
+    final FileSystem fs = super.getCluster().getFileSystem();
+    super.makeTrashDir(fs);
+    ArrayList<StoredTabletFile> files = null;
+    TableId tid = null;
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
+      files = super.loadData(super.getServerContext(), c, table);
+      assertFalse(files.isEmpty());
+      c.tableOperations().compact(table, new CompactionConfig());
+      tid = TableId.of(c.tableOperations().tableIdMap().get(table));
+    }
+    Thread.sleep(10000);","[{'comment': 'Could make this wait until the flush files in the original location are gone/GCed and then count the files in trash instead of sleeping.', 'commenter': 'keith-turner'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashDisabledIT.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashDisabledIT extends GarbageCollectorTrashBase {
+
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(5);
+  }
+
+  @Override
+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+
+    Map<String,String> hadoopOverrides = new HashMap<>();
+    hadoopOverrides.put(CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY, ""5"");
+    cfg.setHadoopConfOverrides(hadoopOverrides);
+    cfg.useMiniDFS(true);
+
+    cfg.setProperty(Property.GC_CYCLE_START, ""1"");
+    cfg.setProperty(Property.GC_CYCLE_DELAY, ""1"");
+    @SuppressWarnings(""removal"")
+    Property p = Property.GC_TRASH_IGNORE;
+    cfg.setProperty(p, ""true""); // don't use trash if configured
+    cfg.setProperty(Property.GC_PORT, ""0"");
+    cfg.setProperty(Property.TSERV_MAXMEM, ""5K"");
+    cfg.setProperty(Property.TABLE_MAJC_RATIO, ""5.0"");
+    cfg.setProperty(Property.TSERV_MAJC_DELAY, ""180s"");
+  }
+
+  @Test
+  public void testTrashHadoopEnabledAccumuloDisabled() throws Exception {
+    String table = this.getUniqueNames(1)[0];
+    final FileSystem fs = super.getCluster().getFileSystem();
+    super.makeTrashDir(fs);
+    ArrayList<StoredTabletFile> files = null;
+    TableId tid = null;
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
+      files = super.loadData(super.getServerContext(), c, table);
+      assertFalse(files.isEmpty());
+      c.tableOperations().compact(table, new CompactionConfig());
+      tid = TableId.of(c.tableOperations().tableIdMap().get(table));
+    }
+    Thread.sleep(10000);","[{'comment': 'Could wait for compaction and then GC (by waiting for the files to be deleted in orig location) here also instead of sleeping.  Could put a new method in the parent class that waits for files to be gone from a table.', 'commenter': 'keith-turner'}, {'comment': 'Addressed in 5ec3ca8.', 'commenter': 'dlmarion'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashDefaultIT.java,"@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.time.Duration;
+import java.util.ArrayList;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashDefaultIT extends GarbageCollectorTrashBase {
+
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(5);
+  }
+
+  @Override
+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+    // By default Hadoop trash is disabled - fs.trash.interval defaults to 0
+    cfg.useMiniDFS(true);
+
+    cfg.setProperty(Property.GC_CYCLE_START, ""1"");
+    cfg.setProperty(Property.GC_CYCLE_DELAY, ""1"");
+    @SuppressWarnings(""removal"")
+    Property p = Property.GC_TRASH_IGNORE;
+    cfg.setProperty(p, ""false""); // default, use trash if configured
+    cfg.setProperty(Property.GC_PORT, ""0"");
+    cfg.setProperty(Property.TSERV_MAXMEM, ""5K"");
+    cfg.setProperty(Property.TABLE_MAJC_RATIO, ""5.0"");
+    cfg.setProperty(Property.TSERV_MAJC_DELAY, ""1"");
+  }
+
+  @Test
+  public void testTrashHadoopDisabledAccumuloEnabled() throws Exception {
+    String table = this.getUniqueNames(1)[0];
+    final FileSystem fs = super.getCluster().getFileSystem();
+    super.makeTrashDir(fs);
+    TableId tid = null;
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
+      ArrayList<StoredTabletFile> files = super.loadData(super.getServerContext(), c, table);
+      assertFalse(files.isEmpty());
+      c.tableOperations().compact(table, new CompactionConfig());
+      tid = TableId.of(c.tableOperations().tableIdMap().get(table));
+    }
+
+    // The default value for fs.trash.interval is 0, which means that
+    // trash is disabled in the Hadoop configuration. Enabling trash in
+    // Accumulo (GC_TRASH_IGNORE = false) still requires enabling trash in Hadoop
+    Thread.sleep(10000);","[{'comment': 'could also wait for compaction and files to be gone here.', 'commenter': 'keith-turner'}, {'comment': 'Addressed in 5ec3ca8.', 'commenter': 'dlmarion'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashDefaultIT.java,"@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.time.Duration;
+import java.util.ArrayList;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashDefaultIT extends GarbageCollectorTrashBase {
+
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(5);
+  }
+
+  @Override
+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+    // By default Hadoop trash is disabled - fs.trash.interval defaults to 0","[{'comment': 'Could explicitly set this to zero in the test even thought its the default.', 'commenter': 'keith-turner'}, {'comment': 'Addressed in 5ec3ca8.', 'commenter': 'dlmarion'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashEnabledCustomPolicyIT.java,"@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.TrashPolicyDefault;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashEnabledCustomPolicyIT extends GarbageCollectorTrashBase {
+
+  public static class NoFlushFilesInTrashPolicy extends TrashPolicyDefault {
+
+    @Override
+    public boolean moveToTrash(Path path) throws IOException {
+      // Don't put flush files in the Trash
+      if (!path.getName().startsWith(""F"")) {","[{'comment': 'The test could be restructured a bit so that there are F and C or A files to GC.  Then we could got more positive confirmation in the test that this class is being called (we could have zero files in trash for other reasons than this class being called).  Thinking of something like the following.\r\n\r\n 1. ingest data\r\n 2. flush table creating a F file\r\n 3. compact table creating an A file and marking the F file for GC\r\n 4. compact table again creating another A file and marking prev A file for GC\r\n 5. wait for the F and A files marked for GC to be gone\r\n 6. assert trash has one file, should be the A file', 'commenter': 'keith-turner'}, {'comment': 'Addressed in 5ec3ca8.', 'commenter': 'dlmarion'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashEnabledCustomPolicyIT.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.TrashPolicyDefault;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashEnabledCustomPolicyIT extends GarbageCollectorTrashBase {
+
+  public static class NoFlushFilesInTrashPolicy extends TrashPolicyDefault {
+
+    @Override
+    public boolean moveToTrash(Path path) throws IOException {
+      // Don't put flush files in the Trash
+      if (!path.getName().startsWith(""F"")) {
+        return super.moveToTrash(path);
+      }
+      return false;
+    }
+
+  }
+
+  @Override
+  protected Duration defaultTimeout() {
+    return Duration.ofMinutes(5);
+  }
+
+  @Override
+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+
+    Map<String,String> hadoopOverrides = new HashMap<>();
+    hadoopOverrides.put(CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY, ""5"");
+    hadoopOverrides.put(""fs.trash.classname"", NoFlushFilesInTrashPolicy.class.getName());
+    cfg.setHadoopConfOverrides(hadoopOverrides);
+    cfg.useMiniDFS(true);
+
+    cfg.setProperty(Property.GC_CYCLE_START, ""1"");
+    cfg.setProperty(Property.GC_CYCLE_DELAY, ""1"");
+    @SuppressWarnings(""removal"")
+    Property p = Property.GC_TRASH_IGNORE;
+    cfg.setProperty(p, ""false""); // default, use trash if configured
+    cfg.setProperty(Property.GC_PORT, ""0"");
+    cfg.setProperty(Property.TSERV_MAXMEM, ""5K"");
+    cfg.setProperty(Property.TABLE_MAJC_RATIO, ""5.0"");
+    cfg.setProperty(Property.TSERV_MAJC_DELAY, ""180s"");
+  }
+
+  @Test
+  public void testTrashHadoopEnabledAccumuloEnabled() throws Exception {
+    String table = this.getUniqueNames(1)[0];
+    final FileSystem fs = super.getCluster().getFileSystem();
+    super.makeTrashDir(fs);
+    try (AccumuloClient c = Accumulo.newClient().from(getClientProperties()).build()) {
+      ReadWriteIT.ingest(c, 10, 10, 10, 0, table);
+      c.tableOperations().flush(table);
+      ArrayList<StoredTabletFile> files1 = getFilesForTable(super.getServerContext(), c, table);
+      assertFalse(files1.isEmpty());
+      assertTrue(files1.stream().allMatch(stf -> stf.getPath().getName().startsWith(""F"")));
+      c.tableOperations().compact(table, new CompactionConfig());
+      super.waitForFilesToBeGCd(files1);
+      ArrayList<StoredTabletFile> files2 = getFilesForTable(super.getServerContext(), c, table);
+      assertTrue(files2.stream().noneMatch(stf -> stf.getPath().getName().startsWith(""F"")));","[{'comment': 'A check like the one above would be good here.  Could the not empty check be more specific, do you know how many files to expect?\r\n\r\n```suggestion\r\n      assertFalse(files2.isEmpty());\r\n      assertTrue(files2.stream().noneMatch(stf -> stf.getPath().getName().startsWith(""F"")));\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Added in ef414e1', 'commenter': 'dlmarion'}]"
3436,server/gc/src/test/java/org/apache/accumulo/gc/SimpleGarbageCollectorTest.java,"@@ -65,6 +65,8 @@ public class SimpleGarbageCollectorTest {
   private SimpleGarbageCollector gc;
   private ConfigurationCopy systemConfig;
   private static SiteConfiguration siteConfig = SiteConfiguration.empty().build();
+  @SuppressWarnings(""removal"")
+  private final Property p = Property.GC_TRASH_IGNORE;","[{'comment': 'Can keep this name:\r\n```suggestion\r\n  private final Property GC_TRASH_IGNORE = Property.GC_TRASH_IGNORE;\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Implemented in 2aa689f', 'commenter': 'dlmarion'}]"
3436,server/gc/src/test/java/org/apache/accumulo/gc/SimpleGarbageCollectorTest.java,"@@ -97,7 +99,7 @@ private ConfigurationCopy createSystemConfig() {
     conf.put(Property.GC_CYCLE_START.getKey(), ""1"");
     conf.put(Property.GC_CYCLE_DELAY.getKey(), ""20"");
     conf.put(Property.GC_DELETE_THREADS.getKey(), ""2"");
-    conf.put(Property.GC_TRASH_IGNORE.getKey(), ""false"");
+    conf.put(p.getKey(), ""false"");","[{'comment': '```suggestion\r\n    conf.put(GC_TRASH_IGNORE.getKey(), ""false"");\r\n```', 'commenter': 'ctubbsii'}]"
3436,server/gc/src/test/java/org/apache/accumulo/gc/SimpleGarbageCollectorTest.java,"@@ -132,7 +134,7 @@ public void testMoveToTrash_UsingTrash_VolMgrFailure() throws Exception {
 
   @Test
   public void testMoveToTrash_NotUsingTrash() throws Exception {
-    systemConfig.set(Property.GC_TRASH_IGNORE.getKey(), ""true"");
+    systemConfig.set(p.getKey(), ""true"");","[{'comment': '```suggestion\r\n    systemConfig.set(GC_TRASH_IGNORE.getKey(), ""true"");\r\n```', 'commenter': 'ctubbsii'}]"
3436,server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java,"@@ -463,11 +463,15 @@ boolean inSafeMode() {
   boolean moveToTrash(Path path) throws IOException {
     final VolumeManager fs = context.getVolumeManager();
     if (!isUsingTrash()) {
+      log.trace(""Accumulo Trash is disabled."");
       return false;
     }
     try {
-      return fs.moveToTrash(path);
+      boolean success = fs.moveToTrash(path);
+      log.trace(""Accumulo Trash enabled, moving to trash succeeded?: {}"", success);
+      return success;
     } catch (FileNotFoundException ex) {
+      log.error(""Error moving to trash"", ex);","[{'comment': '```suggestion\r\n      log.error(""Error moving {} to trash"", path, ex);\r\n```', 'commenter': 'ctubbsii'}]"
3436,server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java,"@@ -463,11 +463,15 @@ boolean inSafeMode() {
   boolean moveToTrash(Path path) throws IOException {
     final VolumeManager fs = context.getVolumeManager();
     if (!isUsingTrash()) {
+      log.trace(""Accumulo Trash is disabled."");","[{'comment': '```suggestion\r\n      log.trace(""Accumulo Trash is disabled. Skipped for {}"", path);\r\n```', 'commenter': 'ctubbsii'}]"
3436,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -354,7 +355,10 @@ public void bulkRename(Map<Path,Path> oldToNewPathMap, int poolSize, String pool
   @Override
   public boolean moveToTrash(Path path) throws IOException {
     FileSystem fs = getFileSystemByPath(path);
+    log.trace(""fs.trash.interval: {}"",
+        fs.getConf().get(CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY));","[{'comment': '```suggestion\r\n    String key = CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY;\r\n    log.trace(""{}: {}"", key, fs.getConf().get(key));\r\n```', 'commenter': 'ctubbsii'}]"
3436,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -354,7 +355,10 @@ public void bulkRename(Map<Path,Path> oldToNewPathMap, int poolSize, String pool
   @Override
   public boolean moveToTrash(Path path) throws IOException {
     FileSystem fs = getFileSystemByPath(path);
+    log.trace(""fs.trash.interval: {}"",
+        fs.getConf().get(CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY));
     Trash trash = new Trash(fs, fs.getConf());
+    log.trace(""Hadoop Trash is enabled: {}"", trash.isEnabled());","[{'comment': '```suggestion\r\n    log.trace(""Hadoop Trash is enabled for {}: {}"", path, trash.isEnabled());\r\n```', 'commenter': 'ctubbsii'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashBase.java,"@@ -0,0 +1,124 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.UncheckedIOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Iterator;
+
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.TabletFile;
+import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;
+import org.apache.accumulo.core.metadata.schema.TabletsMetadata;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.test.util.Wait;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocatedFileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.RemoteIterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class GarbageCollectorTrashBase extends ConfigurableMacBase {
+
+  private static final Logger LOG = LoggerFactory.getLogger(GarbageCollectorTrashBase.class);
+
+  public ArrayList<StoredTabletFile> getFilesForTable(ServerContext ctx, AccumuloClient client,
+      String tableName) {
+    String tid = client.tableOperations().tableIdMap().get(tableName);
+    TabletsMetadata tms =
+        ctx.getAmple().readTablets().forTable(TableId.of(tid)).fetch(ColumnType.FILES).build();
+    ArrayList<StoredTabletFile> files = new ArrayList<>();
+    tms.forEach(tm -> {
+      files.addAll(tm.getFiles());
+    });
+    LOG.debug(""Tablet files: {}"", files);
+    return files;
+  }
+
+  public ArrayList<StoredTabletFile> loadData(ServerContext ctx, AccumuloClient client,
+      String tableName) throws Exception {
+    // create some files
+    for (int i = 0; i < 5; i++) {
+      ReadWriteIT.ingest(client, 10, 10, 10, 0, tableName);
+      client.tableOperations().flush(tableName);
+    }
+    return getFilesForTable(ctx, client, tableName);
+  }
+
+  public boolean userTrashDirExists(FileSystem fs) {
+    return !fs.getTrashRoots(false).isEmpty();
+  }
+
+  public void makeTrashDir(FileSystem fs) throws IOException {
+    if (!userTrashDirExists(fs)) {
+      Path homeDir = fs.getHomeDirectory();
+      Path trashDir = new Path(homeDir, "".Trash"");","[{'comment': ""This is okay so long as miniDFS is used. If LocalFileSystem or RawLocalFileSystem is used, this is probably going to pollute the user's `$HOME` instead of stay in the `/target/` directory."", 'commenter': 'ctubbsii'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashBase.java,"@@ -0,0 +1,124 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.UncheckedIOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Iterator;
+
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.TabletFile;
+import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;
+import org.apache.accumulo.core.metadata.schema.TabletsMetadata;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.test.util.Wait;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocatedFileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.RemoteIterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class GarbageCollectorTrashBase extends ConfigurableMacBase {","[{'comment': ""```suggestion\r\n// base class for ITs that test our legacy trash flag and adoop's trash policy with accumulo-gc\r\npublic class GarbageCollectorTrashBase extends ConfigurableMacBase {\r\n```"", 'commenter': 'ctubbsii'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashDefaultIT.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashDefaultIT extends GarbageCollectorTrashBase {","[{'comment': ""```suggestion\r\n// verify trash is not used using by Hadoop default when Hadoop isn't configured\r\n// to use it (even though our legacy prop set to use it, if configured)\r\npublic class GarbageCollectorTrashDefaultIT extends GarbageCollectorTrashBase {\r\n```"", 'commenter': 'ctubbsii'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashDisabledIT.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashDisabledIT extends GarbageCollectorTrashBase {","[{'comment': '```suggestion\r\n// verify trash is ignored when Hadoop is configured to use it, because our\r\n// legacy prop is set to ignore it\r\npublic class GarbageCollectorTrashDisabledIT extends GarbageCollectorTrashBase {\r\n```', 'commenter': 'ctubbsii'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashEnabledCustomPolicyIT.java,"@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.TrashPolicyDefault;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashEnabledCustomPolicyIT extends GarbageCollectorTrashBase {","[{'comment': '```suggestion\r\n// verify trash is used if our legacy prop is set to not ignore it (the default)\r\n// and Hadoop is also configured to use a custom trash policy\r\npublic class GarbageCollectorTrashEnabledCustomPolicyIT extends GarbageCollectorTrashBase {\r\n```', 'commenter': 'ctubbsii'}]"
3436,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorTrashEnabledIT.java,"@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.functional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.admin.CompactionConfig;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
+import org.apache.hadoop.fs.FileSystem;
+import org.junit.jupiter.api.Test;
+
+public class GarbageCollectorTrashEnabledIT extends GarbageCollectorTrashBase {","[{'comment': ""```suggestion\r\n// verify trash is used with Hadoop's default trash policy when our legacy prop\r\n// is set to not ignore\r\npublic class GarbageCollectorTrashEnabledIT extends GarbageCollectorTrashBase {\r\n```"", 'commenter': 'ctubbsii'}]"
3445,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropSetTool.java,"@@ -0,0 +1,388 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getInstanceId;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getNamespaceIdToNameMap;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getTableIdToName;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReader;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.server.util.PropUtil;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+@AutoService(KeywordExecutable.class)
+public class ZooPropSetTool implements KeywordExecutable {
+
+  public static final String INDENT = ""  "";
+  private static final Logger LOG = LoggerFactory.getLogger(ZooPropSetTool.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooPropSetTool() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooPropSetTool().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-prop-set-tool"";
+  }
+
+  @Override
+  public String description() {
+    return ""Emergency tool to modify properties stored in ZooKeeper without a cluster.""
+        + "" Prefer using the shell if it is available"";
+  }
+
+  @Override
+  public void execute(String[] args) throws Exception {
+    ZooPropSetTool.Opts opts = new ZooPropSetTool.Opts();
+    opts.parseArgs(ZooPropSetTool.class.getName(), args);
+
+    ZooReaderWriter zrw = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zrw, opts.instanceId, opts.instanceName);
+    if (iid == null) {
+      throw new IllegalArgumentException(""Cannot continue without a valid instance."");
+    }
+
+    PropStoreKey<?> propKey = getPropKey(iid, opts, zrw);
+    switch (opts.getCmdMode()) {
+      case SET:
+        setProperty(propKey, opts);
+        break;
+      case DELETE:
+        deleteProperty(propKey, readPropNode(propKey, zrw), opts);
+        break;
+      case PRINT:
+        printProperties(propKey, readPropNode(propKey, zrw), opts);
+        break;
+      case ERROR:
+      default:
+        throw new IllegalArgumentException(""Invalid operation requested"");
+    }
+  }
+
+  private void setProperty(final PropStoreKey<?> propKey, final Opts opts) {
+    LOG.trace(""set {}"", propKey);
+
+    if (!opts.setOpt.contains(""="")) {
+      throw new IllegalArgumentException(
+          ""Invalid set property format. Requires name=value, received "" + opts.setOpt);
+    }
+    String[] tokens = opts.setOpt.split(""="");
+    Map<String,String> propValue = Map.of(tokens[0].trim(), tokens[1].trim());
+    var siteConfig = opts.getSiteConfiguration();
+    ServerContext context = new ServerContext(siteConfig);
+    PropUtil.setProperties(context, propKey, propValue);
+  }
+
+  private void deleteProperty(final PropStoreKey<?> propKey,
+      VersionedProperties versionedProperties, final Opts opts) {
+    LOG.trace(""delete {} - {}"", propKey, opts.deleteOpt);
+    String p = opts.deleteOpt.trim();
+    if (p.isEmpty() || !Property.isValidPropertyKey(p)) {
+      throw new IllegalArgumentException(""Invalid property name, Received: '"" + p + ""'"");
+    }
+    // warn, but not throwing an error. If this was run in a script, allow the script to continue.
+    if (!versionedProperties.asMap().containsKey(p)) {
+      LOG.warn(""skipping delete: property '{}' is not set for: {}- delete would have no effect"", p,
+          propKey);
+      return;
+    }
+    var siteConfig = opts.getSiteConfiguration();
+    ServerContext context = new ServerContext(siteConfig);
+    PropUtil.removeProperties(context, propKey, List.of(p));
+  }
+
+  private void printProperties(final PropStoreKey<?> propKey, final VersionedProperties props,
+      final Opts opts) {
+    LOG.trace(""print {}"", propKey);
+
+    OutputStream outStream = System.out;
+
+    String scope;
+    if (propKey instanceof SystemPropKey) {
+      scope = ""SYSTEM"";
+    } else if (propKey instanceof NamespacePropKey) {
+      scope = ""NAMESPACE"";
+    } else if (propKey instanceof TablePropKey) {
+      scope = ""TABLE"";
+    } else {
+      scope = ""unknown"";
+    }
+
+    try (PrintWriter writer =
+        new PrintWriter(new BufferedWriter(new OutputStreamWriter(outStream, UTF_8)))) {
+      // header
+      writer.printf(""%s - custom properties\nid: %s, data version: %d, timestamp: %s\n"", scope,
+          propKey.getId(), props.getDataVersion(), props.getTimestampISO());
+
+      // skip filtering if no props
+      if (props.asMap().isEmpty()) {
+        writer.printf(""%snone\n"", INDENT);
+        return;
+      }
+
+      SortedMap<String,String> sortedMap = filterProps(props, opts);
+      // skip print if all filtered out
+      if (sortedMap.isEmpty()) {
+        writer.printf(""%snone\n"", INDENT);
+        return;
+      }
+      sortedMap.forEach((name, value) -> writer.printf(""%s%s=%s\n"", INDENT, name, value));
+    }
+  }
+
+  /**
+   * If a filter is provided, filter the properties and return the map sorted for consistent
+   * presentation. If no filter is provided, all properties for the property node are returned.
+   */
+  private SortedMap<String,String> filterProps(VersionedProperties props, Opts opts) {
+    var propsMap = props.asMap();
+
+    Filter filter = opts.getFilter();
+    switch (filter.getMode()) {
+      case NAME:
+        String nameFilter = filter.getString();
+        return new TreeMap<>(
+            propsMap.entrySet().stream().filter(e -> e.getKey().contains(nameFilter))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+      case NAME_VAL:
+        String nvFilter = filter.getString();
+        return new TreeMap<>(propsMap.entrySet().stream()
+            .filter(e -> (e.getKey().contains(nvFilter) || e.getValue().contains(nvFilter)))
+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+      default:
+        return new TreeMap<>(propsMap);
+    }
+  }
+
+  private VersionedProperties readPropNode(final PropStoreKey<?> propKey,
+      final ZooReader zooReader) {
+    try {
+      return ZooPropStore.readFromZk(propKey, nullWatcher, zooReader);
+    } catch (IOException | KeeperException | InterruptedException ex) {
+      throw new IllegalStateException(ex);
+    }
+  }
+
+  private PropStoreKey<?> getPropKey(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+
+    // either tid or table name option provided, get the table id
+    if (!opts.tableOpt.isEmpty() || !opts.tableIdOpt.isEmpty()) {
+      TableId tid = getTableId(iid, opts, zooReader);
+      return TablePropKey.of(iid, tid);
+    }
+
+    // either nid of namespace name provided, get the namespace id.
+    if (!opts.namespaceOpt.isEmpty() || !opts.namespaceIdOpt.isEmpty()) {
+      NamespaceId nid = getNamespaceId(iid, opts, zooReader);
+      return NamespacePropKey.of(iid, nid);
+    }
+
+    // no table or namespace, assume system.
+    return SystemPropKey.of(iid);
+  }
+
+  private TableId getTableId(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+    if (!opts.tableIdOpt.isEmpty()) {
+      return TableId.of(opts.tableIdOpt);
+    }
+    Map<NamespaceId,String> nids = getNamespaceIdToNameMap(iid, zooReader);
+
+    Map<TableId,String> tids = getTableIdToName(iid, nids, zooReader);
+    return tids.entrySet().stream().filter(entry -> opts.tableOpt.equals(entry.getValue()))
+        .map(Map.Entry::getKey).findAny()
+        .orElseThrow(() -> new IllegalArgumentException(""Could not find table "" + opts.tableOpt));
+  }
+
+  private NamespaceId getNamespaceId(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+    if (!opts.namespaceIdOpt.isEmpty()) {
+      return NamespaceId.of(opts.namespaceIdOpt);
+    }
+    Map<NamespaceId,String> nids = getNamespaceIdToNameMap(iid, zooReader);
+    return nids.entrySet().stream().filter(entry -> opts.namespaceOpt.equals(entry.getValue()))
+        .map(Map.Entry::getKey).findAny().orElseThrow(
+            () -> new IllegalArgumentException(""Could not find namespace "" + opts.namespaceOpt));
+  }
+
+  static class Opts extends ConfigOpts {
+
+    @Parameter(names = {""-d"", ""--delete""}, description = ""delete a property"")
+    public String deleteOpt = """";
+    @Parameter(names = {""-f"", ""--filter""},
+        description = ""show only properties that contain this string in their name."")
+    public String filterOpt = """";
+    @Parameter(names = {""-fv"", ""--filter-with-values""},
+        description = ""show only properties that contain this string in their name."")
+    public String filterWithValuesOpt = """";
+    @Parameter(names = {""--instanceName""},
+        description = ""Specify the instance name to use. If instance name or id are not provided, determined from configuration (requires a running hdfs instance)"")
+    public String instanceName = """";
+    @Parameter(names = {""--instanceId""},
+        description = ""Specify the instance id to use. If instance name or id are not provided, determined from configuration (requires a running hdfs instance)"")
+    public String instanceId = """";
+    @Parameter(names = {""-ns"", ""--namespace""},
+        description = ""namespace to display/set/delete properties for"")
+    public String namespaceOpt = """";
+    @Parameter(names = {""-nid"", ""--namespace-id""},
+        description = ""namespace id to display/set/delete properties for"")
+    public String namespaceIdOpt = """";
+    @Parameter(names = {""-s"", ""--set""}, description = ""set a property"")
+    public String setOpt = """";
+    @Parameter(names = {""-t"", ""--table""},
+        description = ""table to display/set/delete properties for"")
+    public String tableOpt = """";
+    @Parameter(names = {""-tid"", ""--table-id""},
+        description = ""table id to display/set/delete properties for"")
+    public String tableIdOpt = """";
+
+    private Filter filter = null;
+
+    @Override
+    public void parseArgs(String programName, String[] args, Object... others) {
+      super.parseArgs(programName, args, others);","[{'comment': 'If `ConfigOpts` had `protected boolean skipLegacyProcessing`, then you could set it to `true` before this call so that the `-s` parameter did not throw an exception.', 'commenter': 'dlmarion'}, {'comment': 'I look at making that change tomorrow - thanks', 'commenter': 'EdColeman'}]"
3445,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropSetTool.java,"@@ -0,0 +1,391 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getInstanceId;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getNamespaceIdToNameMap;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getTableIdToName;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReader;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.server.util.PropUtil;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+@AutoService(KeywordExecutable.class)
+public class ZooPropSetTool implements KeywordExecutable {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ZooPropSetTool.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooPropSetTool() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooPropSetTool().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-prop-set-tool"";","[{'comment': 'Will revisit and review this later, but wanted to make this one initial suggestion:\r\n\r\n```suggestion\r\n    return ""zoo-prop-editor"";\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""I don't care to fight over the name - but...  I think I considered editor over set, but editor seemed to connotate something that would be more interactive.  What is provided is a one-shot, basically brute force writer.  `zoo-prop-set-tool` seems unwieldy, and I don't like it.  But I didn't find anything that fit either.\r\n\r\nIf you think that editor does not imply additional functionality, then it is more work-able, but just wondering if there are other suggestions.\r\n"", 'commenter': 'EdColeman'}, {'comment': 'I don\'t think the term ""editor"" implies interactivity... just that it\'s a tool to make edits. There are plenty of editors that aren\'t interactive (`sed` comes immediately to mind, among a few others). Its function is to edit. Its help/usage will quickly resolve any confusion over *how* it performs the editing function.\r\n\r\nMainly, I agree with you about `zoo-prop-set-tool` being unwieldy, but also, the current name is misleading, because it can do more kinds of edits than merely setting a property. It can modify a property, and delete one, too. The term ""editor"" implies any kind of edit, and seems like the most appropriate here.', 'commenter': 'ctubbsii'}, {'comment': ""I'll push up a change with it called editor soon.  Your suggestion is a trap ;-)  Need to update KeywordStartIT too."", 'commenter': 'EdColeman'}, {'comment': 'Addressed in e2d827213b', 'commenter': 'EdColeman'}]"
3445,test/src/main/java/org/apache/accumulo/test/start/KeywordStartIT.java,"@@ -201,6 +203,8 @@ public void checkHasMain() {
     expectSet.add(Shell.class);
     expectSet.add(SimpleGarbageCollector.class);
     expectSet.add(TabletServer.class);
+    expectSet.add(ZooInfoViewer.class);
+    expectSet.add(ZooPropSetTool.class);","[{'comment': ""The purpose of this test was mainly to ensure that existing tools that already had a main method, or tools that require a main method, didn't lose it and break compatibility or critical behavior.\r\n\r\nWe don't actually need to add new main methods to new KeywordExecutable tools we create. They probably don't need to have a main method, and don't need to be here. If ZooInfoViewer had a main method in 2.1.0, though, it should stay.\r\n\r\nDoes this new class really need a main method, though?"", 'commenter': 'ctubbsii'}, {'comment': 'What determines when a tool should have a main method?  The javadoc in `KeywordExecutable` states how, if it is desired to have a ""redundant main method"" what it should look like.  Certainly using `accumulo tool-name` is convenient, but I do not know when someone might prefer using a main method, or which tools ""require"" a main method, other than the list in KeywordStartIT, which I think over time may be testing tools that ""have a main method"" vs. test tools that actually ""require"" a main main, but have one for convenience (or in my case added because of ignorance of the difference.)\r\n\r\nThe Keyword and main support seems to mainly be boiler-plate between utilities and I was trying to maintain similar functionality between various tools so that would provide similar experiences rather than the user needing know tool-by-tool which options it would support,\r\n', 'commenter': 'EdColeman'}, {'comment': 'I removed main in d11c27ea95 and I think the KeywordStartIT was updated correctly - the test passes, but unclear if this is how the test should be used for non-main tools.', 'commenter': 'EdColeman'}, {'comment': 'Tools typically don\'t have a main method. Everything generally gets launched through our Start entry point. However, many existing tools happened to have a main method. So, the test was an attempt to make sure we didn\'t accidentally remove a main method that users might be using in scripts.\r\n\r\nOther than Start, and tools that don\'t have a keyword, we don\'t have a requirement for any tools to have a ""main"" method at all. I think you updated the IT correctly.\r\n\r\nThis is out of scope for this PR, but we may want to add a test to check that we don\'t introduce new main classes unintentionally. Currently we only have a test that checks that we\'re preserving the existing ones... but a better test would be to check through all our classes, and check each one for having a main method, and failing if it finds any we haven\'t identified as needing one.', 'commenter': 'ctubbsii'}]"
3445,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropSetTool.java,"@@ -0,0 +1,391 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getInstanceId;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getNamespaceIdToNameMap;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getTableIdToName;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReader;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.server.util.PropUtil;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+@AutoService(KeywordExecutable.class)
+public class ZooPropSetTool implements KeywordExecutable {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ZooPropSetTool.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooPropSetTool() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooPropSetTool().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-prop-set-tool"";
+  }
+
+  @Override
+  public String description() {
+    return ""Emergency tool to modify properties stored in ZooKeeper without a cluster.""
+        + "" Prefer using the shell if it is available"";
+  }
+
+  @Override
+  public void execute(String[] args) throws Exception {
+    ZooPropSetTool.Opts opts = new ZooPropSetTool.Opts();
+    opts.parseArgs(ZooPropSetTool.class.getName(), args);
+
+    ZooReaderWriter zrw = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zrw, opts.instanceId, opts.instanceName);
+    if (iid == null) {
+      throw new IllegalArgumentException(""Cannot continue without a valid instance."");
+    }
+
+    var siteConfig = opts.getSiteConfiguration();
+    try (ServerContext context = new ServerContext(siteConfig)) {
+
+      PropStoreKey<?> propKey = getPropKey(iid, opts, zrw);
+      switch (opts.getCmdMode()) {
+        case SET:
+          setProperty(context, propKey, opts);
+          break;
+        case DELETE:
+          deleteProperty(context, propKey, readPropNode(propKey, zrw), opts);
+          break;
+        case PRINT:
+          printProperties(context, propKey, readPropNode(propKey, zrw), opts);
+          break;
+        case ERROR:
+        default:
+          throw new IllegalArgumentException(""Invalid operation requested"");
+      }
+    }
+  }
+
+  private void setProperty(final ServerContext context, final PropStoreKey<?> propKey,
+      final Opts opts) {
+    LOG.trace(""set {}"", propKey);
+
+    if (!opts.setOpt.contains(""="")) {
+      throw new IllegalArgumentException(
+          ""Invalid set property format. Requires name=value, received "" + opts.setOpt);
+    }
+    String[] tokens = opts.setOpt.split(""="");
+    Map<String,String> propValue = Map.of(tokens[0].trim(), tokens[1].trim());
+    PropUtil.setProperties(context, propKey, propValue);
+  }
+
+  private void deleteProperty(final ServerContext context, final PropStoreKey<?> propKey,
+      VersionedProperties versionedProperties, final Opts opts) {
+    LOG.trace(""delete {} - {}"", propKey, opts.deleteOpt);
+    String p = opts.deleteOpt.trim();
+    if (p.isEmpty() || !Property.isValidPropertyKey(p)) {
+      throw new IllegalArgumentException(""Invalid property name, Received: '"" + p + ""'"");
+    }
+    // warn, but not throwing an error. If this was run in a script, allow the script to continue.
+    if (!versionedProperties.asMap().containsKey(p)) {
+      LOG.warn(""skipping delete: property '{}' is not set for: {}- delete would have no effect"", p,
+          propKey);
+      return;
+    }
+    PropUtil.removeProperties(context, propKey, List.of(p));
+  }
+
+  private void printProperties(final ServerContext context, final PropStoreKey<?> propKey,
+      final VersionedProperties props, final Opts opts) {
+    LOG.trace(""print {}"", propKey);
+
+    OutputStream outStream = System.out;
+
+    String scope;
+    if (propKey instanceof SystemPropKey) {
+      scope = ""SYSTEM"";
+    } else if (propKey instanceof NamespacePropKey) {
+      scope = ""NAMESPACE"";
+    } else if (propKey instanceof TablePropKey) {
+      scope = ""TABLE"";
+    } else {
+      scope = ""unknown"";
+    }
+
+    try (PrintWriter writer =
+        new PrintWriter(new BufferedWriter(new OutputStreamWriter(outStream, UTF_8)))) {
+      // header
+      writer.printf(""- Instance name: %s\n"", context.getInstanceName());
+      writer.printf(""- Instance id: %s\n"", context.getInstanceID());
+      writer.printf(""- Property scope: - %s\n"", scope);
+      writer.printf(""- id: %s, data version: %d, timestamp: %s\n"", propKey.getId(),
+          props.getDataVersion(), props.getTimestampISO());
+
+      // skip filtering if no props
+      if (props.asMap().isEmpty()) {
+        writer.println(""none"");
+        return;
+      }
+
+      SortedMap<String,String> sortedMap = filterProps(props, opts);
+      // skip print if all filtered out
+      if (sortedMap.isEmpty()) {
+        writer.println(""none"");
+        return;
+      }
+      sortedMap.forEach((name, value) -> writer.printf(""%s=%s\n"", name, value));
+    }
+  }
+
+  /**
+   * If a filter is provided, filter the properties and return the map sorted for consistent
+   * presentation. If no filter is provided, all properties for the property node are returned.
+   */
+  private SortedMap<String,String> filterProps(VersionedProperties props, Opts opts) {
+    var propsMap = props.asMap();
+
+    Filter filter = opts.getFilter();
+    switch (filter.getMode()) {
+      case NAME:
+        String nameFilter = filter.getString();
+        return new TreeMap<>(
+            propsMap.entrySet().stream().filter(e -> e.getKey().contains(nameFilter))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+      case NAME_VAL:
+        String nvFilter = filter.getString();
+        return new TreeMap<>(propsMap.entrySet().stream()
+            .filter(e -> (e.getKey().contains(nvFilter) || e.getValue().contains(nvFilter)))
+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+      default:
+        return new TreeMap<>(propsMap);
+    }
+  }
+
+  private VersionedProperties readPropNode(final PropStoreKey<?> propKey,
+      final ZooReader zooReader) {
+    try {
+      return ZooPropStore.readFromZk(propKey, nullWatcher, zooReader);
+    } catch (IOException | KeeperException | InterruptedException ex) {
+      throw new IllegalStateException(ex);
+    }
+  }
+
+  private PropStoreKey<?> getPropKey(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+
+    // either tid or table name option provided, get the table id
+    if (!opts.tableOpt.isEmpty() || !opts.tableIdOpt.isEmpty()) {
+      TableId tid = getTableId(iid, opts, zooReader);
+      return TablePropKey.of(iid, tid);
+    }
+
+    // either nid of namespace name provided, get the namespace id.
+    if (!opts.namespaceOpt.isEmpty() || !opts.namespaceIdOpt.isEmpty()) {
+      NamespaceId nid = getNamespaceId(iid, opts, zooReader);
+      return NamespacePropKey.of(iid, nid);
+    }
+
+    // no table or namespace, assume system.
+    return SystemPropKey.of(iid);
+  }
+
+  private TableId getTableId(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+    if (!opts.tableIdOpt.isEmpty()) {
+      return TableId.of(opts.tableIdOpt);
+    }
+    Map<NamespaceId,String> nids = getNamespaceIdToNameMap(iid, zooReader);
+
+    Map<TableId,String> tids = getTableIdToName(iid, nids, zooReader);
+    return tids.entrySet().stream().filter(entry -> opts.tableOpt.equals(entry.getValue()))
+        .map(Map.Entry::getKey).findAny()
+        .orElseThrow(() -> new IllegalArgumentException(""Could not find table "" + opts.tableOpt));
+  }
+
+  private NamespaceId getNamespaceId(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+    if (!opts.namespaceIdOpt.isEmpty()) {
+      return NamespaceId.of(opts.namespaceIdOpt);
+    }
+    Map<NamespaceId,String> nids = getNamespaceIdToNameMap(iid, zooReader);
+    return nids.entrySet().stream().filter(entry -> opts.namespaceOpt.equals(entry.getValue()))
+        .map(Map.Entry::getKey).findAny().orElseThrow(
+            () -> new IllegalArgumentException(""Could not find namespace "" + opts.namespaceOpt));
+  }
+
+  static class Opts extends ConfigOpts {
+
+    @Parameter(names = {""-d"", ""--delete""}, description = ""delete a property"")
+    public String deleteOpt = """";
+    @Parameter(names = {""-f"", ""--filter""},
+        description = ""show only properties that contain this string in their name."")
+    public String filterOpt = """";
+    @Parameter(names = {""-fv"", ""--filter-with-values""},
+        description = ""show only properties that contain this string in their name."")
+    public String filterWithValuesOpt = """";
+    @Parameter(names = {""--instanceName""},
+        description = ""Specify the instance name to use. If instance name or id are not provided, determined from configuration (requires a running hdfs instance)"")
+    public String instanceName = """";
+    @Parameter(names = {""--instanceId""},
+        description = ""Specify the instance id to use. If instance name or id are not provided, determined from configuration (requires a running hdfs instance)"")
+    public String instanceId = """";","[{'comment': ""We don't need these. We previously dropped a lot of these complicated CLI options for these kind of server-side utils in favor of just setting the path to the server-side config file, which already has everything that is needed. I didn't realize you had added similar options to the ZooInfoViewer tool, and would recommend they be dropped from there as well, to keep things simple. Anybody using a server-side util should already have a server-side config file with all the needed connectionString information."", 'commenter': 'ctubbsii'}, {'comment': 'Removed in d11c27ea95', 'commenter': 'EdColeman'}]"
3445,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropSetTool.java,"@@ -0,0 +1,391 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getInstanceId;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getNamespaceIdToNameMap;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getTableIdToName;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReader;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.server.util.PropUtil;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+@AutoService(KeywordExecutable.class)
+public class ZooPropSetTool implements KeywordExecutable {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ZooPropSetTool.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooPropSetTool() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooPropSetTool().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-prop-set-tool"";
+  }
+
+  @Override
+  public String description() {
+    return ""Emergency tool to modify properties stored in ZooKeeper without a cluster.""
+        + "" Prefer using the shell if it is available"";
+  }
+
+  @Override
+  public void execute(String[] args) throws Exception {
+    ZooPropSetTool.Opts opts = new ZooPropSetTool.Opts();
+    opts.parseArgs(ZooPropSetTool.class.getName(), args);
+
+    ZooReaderWriter zrw = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zrw, opts.instanceId, opts.instanceName);
+    if (iid == null) {
+      throw new IllegalArgumentException(""Cannot continue without a valid instance."");
+    }
+
+    var siteConfig = opts.getSiteConfiguration();
+    try (ServerContext context = new ServerContext(siteConfig)) {
+
+      PropStoreKey<?> propKey = getPropKey(iid, opts, zrw);
+      switch (opts.getCmdMode()) {
+        case SET:
+          setProperty(context, propKey, opts);
+          break;
+        case DELETE:
+          deleteProperty(context, propKey, readPropNode(propKey, zrw), opts);
+          break;
+        case PRINT:
+          printProperties(context, propKey, readPropNode(propKey, zrw), opts);
+          break;
+        case ERROR:
+        default:
+          throw new IllegalArgumentException(""Invalid operation requested"");
+      }
+    }
+  }
+
+  private void setProperty(final ServerContext context, final PropStoreKey<?> propKey,
+      final Opts opts) {
+    LOG.trace(""set {}"", propKey);
+
+    if (!opts.setOpt.contains(""="")) {
+      throw new IllegalArgumentException(
+          ""Invalid set property format. Requires name=value, received "" + opts.setOpt);
+    }
+    String[] tokens = opts.setOpt.split(""="");
+    Map<String,String> propValue = Map.of(tokens[0].trim(), tokens[1].trim());
+    PropUtil.setProperties(context, propKey, propValue);
+  }
+
+  private void deleteProperty(final ServerContext context, final PropStoreKey<?> propKey,
+      VersionedProperties versionedProperties, final Opts opts) {
+    LOG.trace(""delete {} - {}"", propKey, opts.deleteOpt);
+    String p = opts.deleteOpt.trim();
+    if (p.isEmpty() || !Property.isValidPropertyKey(p)) {
+      throw new IllegalArgumentException(""Invalid property name, Received: '"" + p + ""'"");
+    }
+    // warn, but not throwing an error. If this was run in a script, allow the script to continue.
+    if (!versionedProperties.asMap().containsKey(p)) {
+      LOG.warn(""skipping delete: property '{}' is not set for: {}- delete would have no effect"", p,
+          propKey);
+      return;
+    }
+    PropUtil.removeProperties(context, propKey, List.of(p));
+  }
+
+  private void printProperties(final ServerContext context, final PropStoreKey<?> propKey,
+      final VersionedProperties props, final Opts opts) {
+    LOG.trace(""print {}"", propKey);
+
+    OutputStream outStream = System.out;
+
+    String scope;
+    if (propKey instanceof SystemPropKey) {
+      scope = ""SYSTEM"";
+    } else if (propKey instanceof NamespacePropKey) {
+      scope = ""NAMESPACE"";
+    } else if (propKey instanceof TablePropKey) {
+      scope = ""TABLE"";
+    } else {
+      scope = ""unknown"";
+    }
+
+    try (PrintWriter writer =
+        new PrintWriter(new BufferedWriter(new OutputStreamWriter(outStream, UTF_8)))) {
+      // header
+      writer.printf(""- Instance name: %s\n"", context.getInstanceName());
+      writer.printf(""- Instance id: %s\n"", context.getInstanceID());
+      writer.printf(""- Property scope: - %s\n"", scope);
+      writer.printf(""- id: %s, data version: %d, timestamp: %s\n"", propKey.getId(),
+          props.getDataVersion(), props.getTimestampISO());
+
+      // skip filtering if no props
+      if (props.asMap().isEmpty()) {
+        writer.println(""none"");
+        return;
+      }
+
+      SortedMap<String,String> sortedMap = filterProps(props, opts);
+      // skip print if all filtered out
+      if (sortedMap.isEmpty()) {
+        writer.println(""none"");
+        return;
+      }
+      sortedMap.forEach((name, value) -> writer.printf(""%s=%s\n"", name, value));
+    }
+  }
+
+  /**
+   * If a filter is provided, filter the properties and return the map sorted for consistent
+   * presentation. If no filter is provided, all properties for the property node are returned.
+   */
+  private SortedMap<String,String> filterProps(VersionedProperties props, Opts opts) {
+    var propsMap = props.asMap();
+
+    Filter filter = opts.getFilter();
+    switch (filter.getMode()) {
+      case NAME:
+        String nameFilter = filter.getString();
+        return new TreeMap<>(
+            propsMap.entrySet().stream().filter(e -> e.getKey().contains(nameFilter))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+      case NAME_VAL:
+        String nvFilter = filter.getString();
+        return new TreeMap<>(propsMap.entrySet().stream()
+            .filter(e -> (e.getKey().contains(nvFilter) || e.getValue().contains(nvFilter)))
+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+      default:
+        return new TreeMap<>(propsMap);
+    }
+  }
+
+  private VersionedProperties readPropNode(final PropStoreKey<?> propKey,
+      final ZooReader zooReader) {
+    try {
+      return ZooPropStore.readFromZk(propKey, nullWatcher, zooReader);
+    } catch (IOException | KeeperException | InterruptedException ex) {
+      throw new IllegalStateException(ex);
+    }
+  }
+
+  private PropStoreKey<?> getPropKey(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+
+    // either tid or table name option provided, get the table id
+    if (!opts.tableOpt.isEmpty() || !opts.tableIdOpt.isEmpty()) {
+      TableId tid = getTableId(iid, opts, zooReader);
+      return TablePropKey.of(iid, tid);
+    }
+
+    // either nid of namespace name provided, get the namespace id.
+    if (!opts.namespaceOpt.isEmpty() || !opts.namespaceIdOpt.isEmpty()) {
+      NamespaceId nid = getNamespaceId(iid, opts, zooReader);
+      return NamespacePropKey.of(iid, nid);
+    }
+
+    // no table or namespace, assume system.
+    return SystemPropKey.of(iid);
+  }
+
+  private TableId getTableId(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+    if (!opts.tableIdOpt.isEmpty()) {
+      return TableId.of(opts.tableIdOpt);
+    }
+    Map<NamespaceId,String> nids = getNamespaceIdToNameMap(iid, zooReader);
+
+    Map<TableId,String> tids = getTableIdToName(iid, nids, zooReader);
+    return tids.entrySet().stream().filter(entry -> opts.tableOpt.equals(entry.getValue()))
+        .map(Map.Entry::getKey).findAny()
+        .orElseThrow(() -> new IllegalArgumentException(""Could not find table "" + opts.tableOpt));
+  }
+
+  private NamespaceId getNamespaceId(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+    if (!opts.namespaceIdOpt.isEmpty()) {
+      return NamespaceId.of(opts.namespaceIdOpt);
+    }
+    Map<NamespaceId,String> nids = getNamespaceIdToNameMap(iid, zooReader);
+    return nids.entrySet().stream().filter(entry -> opts.namespaceOpt.equals(entry.getValue()))
+        .map(Map.Entry::getKey).findAny().orElseThrow(
+            () -> new IllegalArgumentException(""Could not find namespace "" + opts.namespaceOpt));
+  }
+
+  static class Opts extends ConfigOpts {
+
+    @Parameter(names = {""-d"", ""--delete""}, description = ""delete a property"")
+    public String deleteOpt = """";
+    @Parameter(names = {""-f"", ""--filter""},
+        description = ""show only properties that contain this string in their name."")
+    public String filterOpt = """";
+    @Parameter(names = {""-fv"", ""--filter-with-values""},
+        description = ""show only properties that contain this string in their name."")
+    public String filterWithValuesOpt = """";
+    @Parameter(names = {""--instanceName""},
+        description = ""Specify the instance name to use. If instance name or id are not provided, determined from configuration (requires a running hdfs instance)"")
+    public String instanceName = """";
+    @Parameter(names = {""--instanceId""},
+        description = ""Specify the instance id to use. If instance name or id are not provided, determined from configuration (requires a running hdfs instance)"")
+    public String instanceId = """";
+    @Parameter(names = {""-ns"", ""--namespace""},
+        description = ""namespace to display/set/delete properties for"")
+    public String namespaceOpt = """";
+    @Parameter(names = {""-nid"", ""--namespace-id""},
+        description = ""namespace id to display/set/delete properties for"")","[{'comment': 'I would keep it simple and only have the namespace name. We can add an ""are you sure"" (and a `-f, --force` bypass option) prompt that displays the namespace ID that was resolved for the given name.\r\n\r\nSame comment for table name and table ID. We only need the name, just like in the shell.\r\n\r\nAny view options should display the name and id for these, though.', 'commenter': 'ctubbsii'}, {'comment': ""Allowing and id or a name was to ease using information if it was coming from a log file.  Often, the logs use id, not name.  Making the user dig out the name, potentially without the shell being available seemed like an unnecessary step when the tool could use either.  Internally, the code uses id and if given a name it takes effort to lookup the id, so just accepting the id that is needed anyway didn't seem to add more complexity."", 'commenter': 'EdColeman'}, {'comment': ""I'm convinced. Having both is good. You're right that it doesn't really add complexity."", 'commenter': 'ctubbsii'}]"
3445,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropSetTool.java,"@@ -0,0 +1,391 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getInstanceId;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getNamespaceIdToNameMap;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getTableIdToName;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReader;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.server.util.PropUtil;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+@AutoService(KeywordExecutable.class)
+public class ZooPropSetTool implements KeywordExecutable {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ZooPropSetTool.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooPropSetTool() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooPropSetTool().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-prop-set-tool"";
+  }
+
+  @Override
+  public String description() {
+    return ""Emergency tool to modify properties stored in ZooKeeper without a cluster.""
+        + "" Prefer using the shell if it is available"";
+  }
+
+  @Override
+  public void execute(String[] args) throws Exception {
+    ZooPropSetTool.Opts opts = new ZooPropSetTool.Opts();
+    opts.parseArgs(ZooPropSetTool.class.getName(), args);
+
+    ZooReaderWriter zrw = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zrw, opts.instanceId, opts.instanceName);
+    if (iid == null) {
+      throw new IllegalArgumentException(""Cannot continue without a valid instance."");
+    }
+
+    var siteConfig = opts.getSiteConfiguration();
+    try (ServerContext context = new ServerContext(siteConfig)) {
+
+      PropStoreKey<?> propKey = getPropKey(iid, opts, zrw);
+      switch (opts.getCmdMode()) {
+        case SET:
+          setProperty(context, propKey, opts);
+          break;
+        case DELETE:
+          deleteProperty(context, propKey, readPropNode(propKey, zrw), opts);
+          break;
+        case PRINT:
+          printProperties(context, propKey, readPropNode(propKey, zrw), opts);
+          break;
+        case ERROR:
+        default:
+          throw new IllegalArgumentException(""Invalid operation requested"");
+      }
+    }
+  }
+
+  private void setProperty(final ServerContext context, final PropStoreKey<?> propKey,
+      final Opts opts) {
+    LOG.trace(""set {}"", propKey);
+
+    if (!opts.setOpt.contains(""="")) {
+      throw new IllegalArgumentException(
+          ""Invalid set property format. Requires name=value, received "" + opts.setOpt);
+    }
+    String[] tokens = opts.setOpt.split(""="");
+    Map<String,String> propValue = Map.of(tokens[0].trim(), tokens[1].trim());
+    PropUtil.setProperties(context, propKey, propValue);
+  }
+
+  private void deleteProperty(final ServerContext context, final PropStoreKey<?> propKey,
+      VersionedProperties versionedProperties, final Opts opts) {
+    LOG.trace(""delete {} - {}"", propKey, opts.deleteOpt);
+    String p = opts.deleteOpt.trim();
+    if (p.isEmpty() || !Property.isValidPropertyKey(p)) {
+      throw new IllegalArgumentException(""Invalid property name, Received: '"" + p + ""'"");
+    }
+    // warn, but not throwing an error. If this was run in a script, allow the script to continue.
+    if (!versionedProperties.asMap().containsKey(p)) {
+      LOG.warn(""skipping delete: property '{}' is not set for: {}- delete would have no effect"", p,
+          propKey);
+      return;
+    }
+    PropUtil.removeProperties(context, propKey, List.of(p));
+  }
+
+  private void printProperties(final ServerContext context, final PropStoreKey<?> propKey,
+      final VersionedProperties props, final Opts opts) {
+    LOG.trace(""print {}"", propKey);
+
+    OutputStream outStream = System.out;
+
+    String scope;
+    if (propKey instanceof SystemPropKey) {
+      scope = ""SYSTEM"";
+    } else if (propKey instanceof NamespacePropKey) {
+      scope = ""NAMESPACE"";
+    } else if (propKey instanceof TablePropKey) {
+      scope = ""TABLE"";
+    } else {
+      scope = ""unknown"";
+    }
+
+    try (PrintWriter writer =
+        new PrintWriter(new BufferedWriter(new OutputStreamWriter(outStream, UTF_8)))) {
+      // header
+      writer.printf(""- Instance name: %s\n"", context.getInstanceName());
+      writer.printf(""- Instance id: %s\n"", context.getInstanceID());
+      writer.printf(""- Property scope: - %s\n"", scope);
+      writer.printf(""- id: %s, data version: %d, timestamp: %s\n"", propKey.getId(),
+          props.getDataVersion(), props.getTimestampISO());","[{'comment': 'There should be a quiet mode to suppress this. Can also consider putting it in STDERR. So people can pipe the output and not catch all this, which is useful for listing props and grepping.', 'commenter': 'ctubbsii'}, {'comment': 'The `-` for the header info was to support grep -v to strip those if desired.', 'commenter': 'EdColeman'}, {'comment': ""Okay, that makes sense, but grepping out a dash is unfriendly. You can't do `grep -v -`, you have to do `grep -v '[-]'` or `grep -v -- -`\r\nMaybe instead, just `log.info()` this stuff, so it stands out and can be suppressed any number of ways, based on the user's logging config?"", 'commenter': 'ctubbsii'}, {'comment': 'If you redirect the output to a file (say `| tee` ) - having the metadata / header along with the values is something that I (thinking as a user) could find handy for reference.  For example, take a snapshot ""before"", make the change and then get a second snapshot, having the date stamp and version number as metadata along with the values could help distinguish between the snapshots and persist with the file or act a a record of the users actions.\r\n\r\nWould an alternate character, `+`, `=` be friendlier for grep? I think that `#` has the same issue as `-`', 'commenter': 'EdColeman'}, {'comment': ""Changed in d11c27ea95 to use `+` to make 'grep -v` more user friendly"", 'commenter': 'EdColeman'}, {'comment': ""`+` is better. I would prefer `:`, personally. It looks less intrusive, and is more commonly used as a delimiter for parsing output with scripts (thinking of `gpg --with-colons`, `$PATH`, and a few others), which is a similar situation here. `:` is also not a commonly used grep or regex pattern character. Either is fine, but I like the `:` much better. I suggested logging these, though, because our default log4j colorizes the output and makes it stand out separately, and it's very easy to grep out the `[INFO]` stuff, or merely tweak log4j to use STDERR instead, according to the user's preferences. This also aligns with the shell's practice of logging information that's supplemental to the output of the command you're executing.\r\n\r\nThere are pros and cons to lots of different alternatives. I don't want to engage in bike-shedding, though. So, anything that's not `-` is probably fine."", 'commenter': 'ctubbsii'}]"
3445,server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropSetTool.java,"@@ -0,0 +1,391 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf.util;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getInstanceId;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getNamespaceIdToNameMap;
+import static org.apache.accumulo.server.conf.util.ZooPropUtils.getTableIdToName;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.cli.ConfigOpts;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReader;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStoreKey;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.PropStoreWatcher;
+import org.apache.accumulo.server.conf.store.impl.ReadyMonitor;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.apache.accumulo.server.util.PropUtil;
+import org.apache.accumulo.start.spi.KeywordExecutable;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.beust.jcommander.Parameter;
+import com.google.auto.service.AutoService;
+
+@AutoService(KeywordExecutable.class)
+public class ZooPropSetTool implements KeywordExecutable {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ZooPropSetTool.class);
+  private final NullWatcher nullWatcher =
+      new NullWatcher(new ReadyMonitor(ZooInfoViewer.class.getSimpleName(), 20_000L));
+
+  /**
+   * No-op constructor - provided so ServiceLoader autoload does not consume resources.
+   */
+  public ZooPropSetTool() {}
+
+  public static void main(String[] args) throws Exception {
+    new ZooPropSetTool().execute(args);
+  }
+
+  @Override
+  public String keyword() {
+    return ""zoo-prop-set-tool"";
+  }
+
+  @Override
+  public String description() {
+    return ""Emergency tool to modify properties stored in ZooKeeper without a cluster.""
+        + "" Prefer using the shell if it is available"";
+  }
+
+  @Override
+  public void execute(String[] args) throws Exception {
+    ZooPropSetTool.Opts opts = new ZooPropSetTool.Opts();
+    opts.parseArgs(ZooPropSetTool.class.getName(), args);
+
+    ZooReaderWriter zrw = new ZooReaderWriter(opts.getSiteConfiguration());
+
+    InstanceId iid = getInstanceId(zrw, opts.instanceId, opts.instanceName);
+    if (iid == null) {
+      throw new IllegalArgumentException(""Cannot continue without a valid instance."");
+    }
+
+    var siteConfig = opts.getSiteConfiguration();
+    try (ServerContext context = new ServerContext(siteConfig)) {
+
+      PropStoreKey<?> propKey = getPropKey(iid, opts, zrw);
+      switch (opts.getCmdMode()) {
+        case SET:
+          setProperty(context, propKey, opts);
+          break;
+        case DELETE:
+          deleteProperty(context, propKey, readPropNode(propKey, zrw), opts);
+          break;
+        case PRINT:
+          printProperties(context, propKey, readPropNode(propKey, zrw), opts);
+          break;
+        case ERROR:
+        default:
+          throw new IllegalArgumentException(""Invalid operation requested"");
+      }
+    }
+  }
+
+  private void setProperty(final ServerContext context, final PropStoreKey<?> propKey,
+      final Opts opts) {
+    LOG.trace(""set {}"", propKey);
+
+    if (!opts.setOpt.contains(""="")) {
+      throw new IllegalArgumentException(
+          ""Invalid set property format. Requires name=value, received "" + opts.setOpt);
+    }
+    String[] tokens = opts.setOpt.split(""="");
+    Map<String,String> propValue = Map.of(tokens[0].trim(), tokens[1].trim());
+    PropUtil.setProperties(context, propKey, propValue);
+  }
+
+  private void deleteProperty(final ServerContext context, final PropStoreKey<?> propKey,
+      VersionedProperties versionedProperties, final Opts opts) {
+    LOG.trace(""delete {} - {}"", propKey, opts.deleteOpt);
+    String p = opts.deleteOpt.trim();
+    if (p.isEmpty() || !Property.isValidPropertyKey(p)) {
+      throw new IllegalArgumentException(""Invalid property name, Received: '"" + p + ""'"");
+    }
+    // warn, but not throwing an error. If this was run in a script, allow the script to continue.
+    if (!versionedProperties.asMap().containsKey(p)) {
+      LOG.warn(""skipping delete: property '{}' is not set for: {}- delete would have no effect"", p,
+          propKey);
+      return;
+    }
+    PropUtil.removeProperties(context, propKey, List.of(p));
+  }
+
+  private void printProperties(final ServerContext context, final PropStoreKey<?> propKey,
+      final VersionedProperties props, final Opts opts) {
+    LOG.trace(""print {}"", propKey);
+
+    OutputStream outStream = System.out;
+
+    String scope;
+    if (propKey instanceof SystemPropKey) {
+      scope = ""SYSTEM"";
+    } else if (propKey instanceof NamespacePropKey) {
+      scope = ""NAMESPACE"";
+    } else if (propKey instanceof TablePropKey) {
+      scope = ""TABLE"";
+    } else {
+      scope = ""unknown"";
+    }
+
+    try (PrintWriter writer =
+        new PrintWriter(new BufferedWriter(new OutputStreamWriter(outStream, UTF_8)))) {
+      // header
+      writer.printf(""- Instance name: %s\n"", context.getInstanceName());
+      writer.printf(""- Instance id: %s\n"", context.getInstanceID());
+      writer.printf(""- Property scope: - %s\n"", scope);
+      writer.printf(""- id: %s, data version: %d, timestamp: %s\n"", propKey.getId(),
+          props.getDataVersion(), props.getTimestampISO());
+
+      // skip filtering if no props
+      if (props.asMap().isEmpty()) {
+        writer.println(""none"");
+        return;
+      }
+
+      SortedMap<String,String> sortedMap = filterProps(props, opts);
+      // skip print if all filtered out
+      if (sortedMap.isEmpty()) {
+        writer.println(""none"");
+        return;
+      }
+      sortedMap.forEach((name, value) -> writer.printf(""%s=%s\n"", name, value));
+    }
+  }
+
+  /**
+   * If a filter is provided, filter the properties and return the map sorted for consistent
+   * presentation. If no filter is provided, all properties for the property node are returned.
+   */
+  private SortedMap<String,String> filterProps(VersionedProperties props, Opts opts) {
+    var propsMap = props.asMap();
+
+    Filter filter = opts.getFilter();
+    switch (filter.getMode()) {
+      case NAME:
+        String nameFilter = filter.getString();
+        return new TreeMap<>(
+            propsMap.entrySet().stream().filter(e -> e.getKey().contains(nameFilter))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+      case NAME_VAL:
+        String nvFilter = filter.getString();
+        return new TreeMap<>(propsMap.entrySet().stream()
+            .filter(e -> (e.getKey().contains(nvFilter) || e.getValue().contains(nvFilter)))
+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+      default:
+        return new TreeMap<>(propsMap);
+    }
+  }
+
+  private VersionedProperties readPropNode(final PropStoreKey<?> propKey,
+      final ZooReader zooReader) {
+    try {
+      return ZooPropStore.readFromZk(propKey, nullWatcher, zooReader);
+    } catch (IOException | KeeperException | InterruptedException ex) {
+      throw new IllegalStateException(ex);
+    }
+  }
+
+  private PropStoreKey<?> getPropKey(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+
+    // either tid or table name option provided, get the table id
+    if (!opts.tableOpt.isEmpty() || !opts.tableIdOpt.isEmpty()) {
+      TableId tid = getTableId(iid, opts, zooReader);
+      return TablePropKey.of(iid, tid);
+    }
+
+    // either nid of namespace name provided, get the namespace id.
+    if (!opts.namespaceOpt.isEmpty() || !opts.namespaceIdOpt.isEmpty()) {
+      NamespaceId nid = getNamespaceId(iid, opts, zooReader);
+      return NamespacePropKey.of(iid, nid);
+    }
+
+    // no table or namespace, assume system.
+    return SystemPropKey.of(iid);
+  }
+
+  private TableId getTableId(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+    if (!opts.tableIdOpt.isEmpty()) {
+      return TableId.of(opts.tableIdOpt);
+    }
+    Map<NamespaceId,String> nids = getNamespaceIdToNameMap(iid, zooReader);
+
+    Map<TableId,String> tids = getTableIdToName(iid, nids, zooReader);
+    return tids.entrySet().stream().filter(entry -> opts.tableOpt.equals(entry.getValue()))
+        .map(Map.Entry::getKey).findAny()
+        .orElseThrow(() -> new IllegalArgumentException(""Could not find table "" + opts.tableOpt));
+  }
+
+  private NamespaceId getNamespaceId(final InstanceId iid, final ZooPropSetTool.Opts opts,
+      final ZooReader zooReader) {
+    if (!opts.namespaceIdOpt.isEmpty()) {
+      return NamespaceId.of(opts.namespaceIdOpt);
+    }
+    Map<NamespaceId,String> nids = getNamespaceIdToNameMap(iid, zooReader);
+    return nids.entrySet().stream().filter(entry -> opts.namespaceOpt.equals(entry.getValue()))
+        .map(Map.Entry::getKey).findAny().orElseThrow(
+            () -> new IllegalArgumentException(""Could not find namespace "" + opts.namespaceOpt));
+  }
+
+  static class Opts extends ConfigOpts {
+
+    @Parameter(names = {""-d"", ""--delete""}, description = ""delete a property"")
+    public String deleteOpt = """";
+    @Parameter(names = {""-f"", ""--filter""},
+        description = ""show only properties that contain this string in their name."")
+    public String filterOpt = """";
+    @Parameter(names = {""-fv"", ""--filter-with-values""},
+        description = ""show only properties that contain this string in their name."")
+    public String filterWithValuesOpt = """";
+    @Parameter(names = {""--instanceName""},
+        description = ""Specify the instance name to use. If instance name or id are not provided, determined from configuration (requires a running hdfs instance)"")
+    public String instanceName = """";
+    @Parameter(names = {""--instanceId""},
+        description = ""Specify the instance id to use. If instance name or id are not provided, determined from configuration (requires a running hdfs instance)"")
+    public String instanceId = """";
+    @Parameter(names = {""-ns"", ""--namespace""},
+        description = ""namespace to display/set/delete properties for"")
+    public String namespaceOpt = """";
+    @Parameter(names = {""-nid"", ""--namespace-id""},
+        description = ""namespace id to display/set/delete properties for"")
+    public String namespaceIdOpt = """";
+    @Parameter(names = {""-s"", ""--set""}, description = ""set a property"")
+    public String setOpt = """";
+    @Parameter(names = {""-t"", ""--table""},
+        description = ""table to display/set/delete properties for"")
+    public String tableOpt = """";
+    @Parameter(names = {""-tid"", ""--table-id""},
+        description = ""table id to display/set/delete properties for"")
+    public String tableIdOpt = """";
+
+    private Filter filter = null;
+
+    @Override
+    public void parseArgs(String programName, String[] args, Object... others) {
+      super.parseArgs(programName, args, others);
+      var cmdMode = getCmdMode();
+      if (cmdMode == Opts.CmdMode.ERROR) {
+        throw new IllegalArgumentException(""Cannot use set and delete in one command"");
+      }
+      filter = new Filter(this);
+      if (!filter.getString().isEmpty()
+          && (cmdMode == Opts.CmdMode.SET || cmdMode == CmdMode.DELETE)) {
+        throw new IllegalArgumentException(""Cannot use filter with set or delete"");
+      }
+    }
+
+    Filter getFilter() {
+      return filter;
+    }
+
+    CmdMode getCmdMode() {
+      if (!deleteOpt.isEmpty() && !setOpt.isEmpty()) {
+        return CmdMode.ERROR;
+      }
+      if (!deleteOpt.isEmpty()) {
+        return CmdMode.DELETE;
+      }
+      if (!setOpt.isEmpty()) {
+        return CmdMode.SET;
+      }
+      return CmdMode.PRINT;
+    }
+
+    enum CmdMode {
+      ERROR, PRINT, SET, DELETE
+    }
+  }
+
+  /**
+   * The filter type and the filter string specified by command line options. Filter types are - no
+   * filter, filter on name or filter by name or value.
+   */
+  private static class Filter {","[{'comment': ""I don't think you need any of this complexity for filtering the output of properties. You're reading all the properties anyway, so this isn't adding much value for the user. They can already pipe the output to grep. Can we try to minimize the feature and complexity of this tool with that fact in mind by getting rid of all the filtering stuff? It's overkill."", 'commenter': 'ctubbsii'}, {'comment': 'It can be dropped - I was trying to mirror the same functionality that the shell provided.', 'commenter': 'EdColeman'}, {'comment': ""That makes sense, but I think we can target a subset of the functions... like for the ones we keep, we use the same syntax, but not add anything that they can already do themselves because of the fact that they're running it as a command-line tool instead of a shell command."", 'commenter': 'ctubbsii'}, {'comment': 'Filtering removed in d11c27ea95', 'commenter': 'EdColeman'}]"
3452,core/src/test/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParserTest.java,"@@ -242,4 +244,23 @@ public void testShellOutputWithOptionalComponents() throws Exception {
     assertEquals(expected, actual);
   }
 
+  @Test
+  public void testFileWithUnknownSections() throws Exception {
+    URL configFile = ClusterConfigParserTest.class
+        .getResource(""/org/apache/accumulo/core/conf/cluster/bad-cluster.yaml"");
+    assertNotNull(configFile);
+
+    Map<String,String> contents =
+        ClusterConfigParser.parseConfiguration(new File(configFile.toURI()).getAbsolutePath());
+
+    ByteArrayOutputStream baos = new ByteArrayOutputStream();
+    PrintStream ps = new PrintStream(baos);
+
+    var exception = assertThrows(IllegalArgumentException.class,
+        () -> ClusterConfigParser.outputShellVariables(contents, System.out));
+
+    assertTrue(exception.getMessage().contains(""vserver""));
+
+    ps.close();","[{'comment': 'try-with-resources is shorter:\r\n\r\n```suggestion\r\n    try (var baos = new ByteArrayOutputStream(); var ps = new PrintStream(baos)) {\r\n      var exception = assertThrows(IllegalArgumentException.class,\r\n          () -> ClusterConfigParser.outputShellVariables(contents, System.out));\r\n      assertTrue(exception.getMessage().contains(""vserver""));\r\n    }\r\n```', 'commenter': 'ctubbsii'}]"
3452,core/src/main/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParser.java,"@@ -40,6 +40,12 @@ public class ClusterConfigParser {
   private static final String PROPERTY_FORMAT = ""%s=\""%s\""%n"";
   private static final String[] SECTIONS = new String[] {""manager"", ""monitor"", ""gc"", ""tserver""};
 
+  private static final Set<String> VALID_CONFIG_KEYS = Set.of(""manager"", ""monitor"", ""gc"", ""tserver"",
+      ""tservers_per_host"", ""sservers_per_host"", ""compaction.coordinator"");
+
+  private static final String[] VALID_CONFIG_PREFIXES =
+      new String[] {""compaction.compactor."", ""sserver.""};","[{'comment': 'If you make these into a Predicate, this makes the code that streams it look a lot cleaner. The Predicate itself can be simplified, too, if you stream the prefixes, rather than for loop over it.\r\n\r\nHere\'s the Predicate (and making the array into a Set):\r\n\r\n```suggestion\r\n  private static final Set<String> VALID_CONFIG_PREFIXES =\r\n      Set.of(""compaction.compactor."", ""sserver."");\r\n\r\n  private static final Predicate<String> VALID_CONFIG_SECTIONS =\r\n      section -> VALID_CONFIG_KEYS.contains(section)\r\n          || VALID_CONFIG_PREFIXES.stream().anyMatch(section::startsWith);\r\n```\r\n\r\nNote: this requires `import java.util.function.Predicate;`', 'commenter': 'ctubbsii'}]"
3452,core/src/main/java/org/apache/accumulo/core/conf/cluster/ClusterConfigParser.java,"@@ -87,6 +93,20 @@ private static void flatten(String parentKey, String key, Object value,
 
   public static void outputShellVariables(Map<String,String> config, PrintStream out) {
 
+    config.keySet().forEach(section -> {
+      if (VALID_CONFIG_KEYS.contains(section)) {
+        return;
+      }
+
+      for (String validPrefix : VALID_CONFIG_PREFIXES) {
+        if (section.startsWith(validPrefix)) {
+          return;
+        }
+      }
+
+      throw new IllegalArgumentException(""Unknown configuration section : "" + section);
+    });","[{'comment': 'Here\'s the stream simplification to replace the forEach. I preserved behavior here, but this is a lot simpler if you\'re not trying to extract the invalid config section to put in the message. In that case, you could just do: `if (config.keySet().stream().anyMatch(VALID_CONFIG_SECTIONS.negate()) { throw ...` or negate the check instead of the Predicate, as in `if (!config.keySet().stream().allMatch(VALID_CONFIG_SECTIONS) { throw ...`\r\n\r\n```suggestion\r\n    // find invalid config sections and point the user to the first one\r\n    config.keySet().stream().filter(VALID_CONFIG_SECTIONS.negate()).findFirst()\r\n        .ifPresent(section -> {\r\n          throw new IllegalArgumentException(""Unknown configuration section : "" + section);\r\n        });\r\n```\r\n\r\nThere are also variants of this that would collect the entire set of invalid sections and put them all in the error message, or to put each one into their own suppressed exception, reduced into a single one to throw. But, those are probably overkill.', 'commenter': 'ctubbsii'}]"
3481,test/src/main/java/org/apache/accumulo/test/start/KeywordStartIT.java,"@@ -135,7 +137,7 @@ public void testExpectedClasses() {
     expectSet.put(""init"", Initialize.class);
     expectSet.put(""login-info"", LoginProperties.class);
     expectSet.put(""manager"", ManagerExecutable.class);
-    expectSet.put(""master"", org.apache.accumulo.manager.MasterExecutable.class);
+    expectSet.put(""master"", MasterExecutable.class);","[{'comment': 'This previously used the fully qualified class name because you can\'t suppress the deprecation warning in the imports. It should be changed back.\r\n\r\n```suggestion\r\n    expectSet.put(""master"", MasterExecutable.class);\r\n```', 'commenter': 'ctubbsii'}]"
3481,test/src/main/java/org/apache/accumulo/test/start/KeywordStartIT.java,"@@ -203,10 +208,41 @@ public void checkHasMain() {
     expectSet.add(TabletServer.class);
     expectSet.add(ZooKeeperMain.class);
 
+    expectSet.add(ConvertConfig.class);
+    expectSet.add(ConfigPropertyUpgrader.class);
+    expectSet.add(CheckServerConfig.class);
+    expectSet.add(CreateEmpty.class);
+    expectSet.add(ECAdmin.class);
+    expectSet.add(GenerateSplits.class);
+    expectSet.add(SplitLarge.class);
+    expectSet.add(ZooZap.class);
+
+    // check that classes in the expected set contain a main
     for (Class<?> c : expectSet) {
       assertTrue(hasMain(c), ""Class "" + c.getName() + "" is missing a main method!"");
     }
 
+    // build a list of all classed that implement KeywordExecutable
+    TreeMap<String,KeywordExecutable> foundExecutables =
+        new TreeMap<>(Main.getExecutables(getClass().getClassLoader()));","[{'comment': ""This only checks for KeywordExecutables that have a main method that we didn't expect. My comment was to check all classes, not just KeywordExecutable classes. So, this is a start, but incomplete (relative to the suggestion)."", 'commenter': 'ctubbsii'}, {'comment': 'My suggestion can be done for 3.x', 'commenter': 'ctubbsii'}]"
3508,server/base/src/main/java/org/apache/accumulo/server/mem/LowMemoryDetector.java,"@@ -105,68 +101,50 @@ public void logGCInfo(AccumuloConfiguration conf) {
     try {
       final long now = TimeUnit.NANOSECONDS.toMillis(System.nanoTime());
 
-      List<GarbageCollectorMXBean> gcmBeans = ManagementFactory.getGarbageCollectorMXBeans();
       Runtime rt = Runtime.getRuntime();
 
       StringBuilder sb = new StringBuilder(""gc"");
 
       boolean sawChange = false;
 
-      long maxIncreaseInCollectionTime = 0;
-
-      for (GarbageCollectorMXBean gcBean : gcmBeans) {
-        Long prevTime = prevGcTime.get(gcBean.getName());
-        long pt = 0;
-        if (prevTime != null) {
-          pt = prevTime;
-        }
-
-        long time = gcBean.getCollectionTime();
-
-        if (time - pt != 0) {
-          sawChange = true;
+      final long maxConfiguredMemory = rt.maxMemory();
+      final long allocatedMemory = rt.totalMemory();
+      final long allocatedFreeMemory = rt.freeMemory();
+      final long freeMemory = maxConfiguredMemory - (allocatedMemory - allocatedFreeMemory);
+      final double lowMemoryThreshold = maxConfiguredMemory * freeMemoryPercentage;
+      log.trace(""Memory info: max={}, allocated={}, free={}, free threshold={}"",
+          maxConfiguredMemory, allocatedMemory, freeMemory, (long) lowMemoryThreshold);
+
+      if (freeMemory < (long) lowMemoryThreshold) {
+        lowMemCount++;
+        if (lowMemCount > 3 && !runningLowOnMemory) {
+          runningLowOnMemory = true;
+          log.info(""Running low on memory: max={}, allocated={}, free={}, free threshold={}"",","[{'comment': 'should this be warn?', 'commenter': 'dlmarion'}, {'comment': 'Fixed in e58cfec34c', 'commenter': 'EdColeman'}]"
3508,server/base/src/main/java/org/apache/accumulo/server/mem/LowMemoryDetector.java,"@@ -185,11 +163,11 @@ public void logGCInfo(AccumuloConfiguration conf) {
         return;
       }
 
-      if (maxIncreaseInCollectionTime > keepAliveTimeout) {
-        Halt.halt(""Garbage collection may be interfering with lock keep-alive.  Halting."", -1);
-      }
+      // if (maxIncreaseInCollectionTime > keepAliveTimeout) {","[{'comment': 'I think this should be uncommented.', 'commenter': 'dlmarion'}, {'comment': 'Fixed in e58cfec34c - required other code to be restored too.', 'commenter': 'EdColeman'}]"
3508,test/src/main/java/org/apache/accumulo/test/functional/MemoryFreeingIterator.java,"@@ -28,23 +28,51 @@
 import org.apache.accumulo.core.iterators.IteratorEnvironment;
 import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
 import org.apache.accumulo.core.iterators.WrappingIterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 
 public class MemoryFreeingIterator extends WrappingIterator {
 
-  @Override
-  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,
-      IteratorEnvironment env) throws IOException {
-    super.init(source, options, env);
+  private static final Logger LOG = LoggerFactory.getLogger(MemoryFreeingIterator.class);
+
+  @SuppressFBWarnings(value = ""DM_GC"", justification = ""gc is okay for test"")
+  public MemoryFreeingIterator() {
+    super();
+    LOG.info(""Freeing consumed memory"");
     MemoryConsumingIterator.freeBuffers();
     while (this.isRunningLowOnMemory()) {
+      System.gc();
       // wait for LowMemoryDetector to recognize the memory is free.
       try {
         Thread.sleep(SECONDS.toMillis(1));
       } catch (InterruptedException ex) {
         Thread.currentThread().interrupt();
-        throw new IOException(""wait for low memory detector interrupted"", ex);
+        throw new RuntimeException(""wait for low memory detector interrupted"", ex);
       }
     }
+    LOG.info(""Consumed memory freed"");
+  }
+
+  @Override
+  @SuppressFBWarnings(value = ""DM_GC"", justification = ""gc is okay for test"")
+  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,
+      IteratorEnvironment env) throws IOException {
+    super.init(source, options, env);
+    // LOG.info(""Freeing consumed memory"");","[{'comment': 'this commented code can be removed. This method might be able to be removed entirely.', 'commenter': 'dlmarion'}, {'comment': 'Addressed in e58cfec34c.  Left call and added log statement to so init shows up in test logs.', 'commenter': 'EdColeman'}]"
3508,server/base/src/main/java/org/apache/accumulo/server/mem/LowMemoryDetector.java,"@@ -36,20 +36,21 @@
 
 public class LowMemoryDetector {
 
+  private static final Logger LOG = LoggerFactory.getLogger(LowMemoryDetector.class);
+
   @FunctionalInterface
-  public static interface Action {
+  public interface Action {
     void execute();
   }","[{'comment': ""For what it's worth, it seems like Runnable would suffice here. No need for a new interface."", 'commenter': 'ctubbsii'}]"
3508,server/base/src/main/java/org/apache/accumulo/server/mem/LowMemoryDetector.java,"@@ -135,48 +134,55 @@ public void logGCInfo(AccumuloConfiguration conf) {
         prevGcTime.put(gcBean.getName(), time);
       }
 
-      long mem = rt.freeMemory();
-      if (maxIncreaseInCollectionTime == 0) {
-        gcTimeIncreasedCount = 0;
-      } else {
-        gcTimeIncreasedCount++;
-        if (gcTimeIncreasedCount > 3 && mem < rt.maxMemory() * freeMemoryPercentage) {
+      Runtime rt = Runtime.getRuntime();
+      final long maxConfiguredMemory = rt.maxMemory();
+      final long allocatedMemory = rt.totalMemory();
+      final long allocatedFreeMemory = rt.freeMemory();
+      final long freeMemory = maxConfiguredMemory - (allocatedMemory - allocatedFreeMemory);
+      final double lowMemoryThreshold = maxConfiguredMemory * freeMemoryPercentage;","[{'comment': ""Should cast this to long here, so it doesn't need to be done three times later."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 3ed08fc601', 'commenter': 'EdColeman'}]"
3508,server/base/src/main/java/org/apache/accumulo/server/mem/LowMemoryDetector.java,"@@ -135,48 +134,55 @@ public void logGCInfo(AccumuloConfiguration conf) {
         prevGcTime.put(gcBean.getName(), time);
       }
 
-      long mem = rt.freeMemory();
-      if (maxIncreaseInCollectionTime == 0) {
-        gcTimeIncreasedCount = 0;
-      } else {
-        gcTimeIncreasedCount++;
-        if (gcTimeIncreasedCount > 3 && mem < rt.maxMemory() * freeMemoryPercentage) {
+      Runtime rt = Runtime.getRuntime();
+      final long maxConfiguredMemory = rt.maxMemory();
+      final long allocatedMemory = rt.totalMemory();
+      final long allocatedFreeMemory = rt.freeMemory();
+      final long freeMemory = maxConfiguredMemory - (allocatedMemory - allocatedFreeMemory);
+      final double lowMemoryThreshold = maxConfiguredMemory * freeMemoryPercentage;
+      LOG.trace(""Memory info: max={}, allocated={}, free={}, free threshold={}"",
+          maxConfiguredMemory, allocatedMemory, freeMemory, (long) lowMemoryThreshold);
+
+      if (freeMemory < (long) lowMemoryThreshold) {
+        lowMemCount++;
+        if (lowMemCount > 3 && !runningLowOnMemory) {
           runningLowOnMemory = true;
-          log.warn(""Running low on memory"");
-          gcTimeIncreasedCount = 0;
+          LOG.warn(""Running low on memory: max={}, allocated={}, free={}, free threshold={}"",
+              maxConfiguredMemory, allocatedMemory, freeMemory, (long) lowMemoryThreshold);
+        }
+      } else {
+        // If we were running low on memory, but are not any longer, than log at warn
+        // so that it shows up in the logs
+        if (runningLowOnMemory) {
+          LOG.warn(""Recovered from low memory condition"");
         } else {
-          // If we were running low on memory, but are not any longer, than log at warn
-          // so that it shows up in the logs
-          if (runningLowOnMemory) {
-            log.warn(""Recovered from low memory condition"");
-          } else {
-            log.trace(""Not running low on memory"");
-          }
-          runningLowOnMemory = false;
+          LOG.trace(""Not running low on memory"");
         }
+        runningLowOnMemory = false;
+        lowMemCount = 0;
       }
 
-      if (mem != lastMemorySize) {
+      if (freeMemory != lastMemorySize) {
         sawChange = true;
       }
 
       String sign = ""+"";
-      if (mem - lastMemorySize <= 0) {
+      if (freeMemory - lastMemorySize <= 0) {
         sign = """";
       }
 
-      sb.append(String.format("" freemem=%,d(%s%,d) totalmem=%,d"", mem, sign, (mem - lastMemorySize),
-          rt.totalMemory()));
+      sb.append(String.format("" freemem=%,d(%s%,d) totalmem=%,d"", freeMemory, sign,
+          (freeMemory - lastMemorySize), rt.totalMemory()));","[{'comment': ""You don't need the sign variable. Java String format already supports always showing a sign, even if it's positive. Just use `%+,d` as the format string."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed in 3ed08fc601', 'commenter': 'EdColeman'}]"
3508,test/src/main/java/org/apache/accumulo/test/functional/MemoryFreeingIterator.java,"@@ -28,23 +28,39 @@
 import org.apache.accumulo.core.iterators.IteratorEnvironment;
 import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
 import org.apache.accumulo.core.iterators.WrappingIterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 
 public class MemoryFreeingIterator extends WrappingIterator {
 
-  @Override
-  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,
-      IteratorEnvironment env) throws IOException {
-    super.init(source, options, env);
+  private static final Logger LOG = LoggerFactory.getLogger(MemoryFreeingIterator.class);
+
+  @SuppressFBWarnings(value = ""DM_GC"", justification = ""gc is okay for test"")
+  public MemoryFreeingIterator() {
+    super();
+    LOG.info(""Freeing consumed memory"");
     MemoryConsumingIterator.freeBuffers();
     while (this.isRunningLowOnMemory()) {
+      System.gc();
       // wait for LowMemoryDetector to recognize the memory is free.
       try {
         Thread.sleep(SECONDS.toMillis(1));
       } catch (InterruptedException ex) {
         Thread.currentThread().interrupt();
-        throw new IOException(""wait for low memory detector interrupted"", ex);
+        throw new RuntimeException(""wait for low memory detector interrupted"", ex);
       }","[{'comment': ""For the test code, I feel like you could just throw the InterruptedException, instead of trying to convert it to some other type. It doesn't matter for the test. Just throwing what's there already is shorter, and the less code the better, especially when trying to maintain complicated tests. It's probably not even going to be thrown in the test anyway, unless we kill our own test... in which case, we would expect it."", 'commenter': 'ctubbsii'}, {'comment': 'Was already corrected?  (Change is reflected in 3ed08fc601)', 'commenter': 'EdColeman'}, {'comment': ""No. I'm still seeing the IOException changed to RuntimeException in this PR. I'm suggesting just get rid of the catch block entirely, and put `throws InterruptedException` on the method, because that's good enough for the test. The only thing this catch block is doing is adding noise to the test code to add an unnecessary conversion of an interrupt to a different exception type and adding a message to that new exception that doesn't provide more useful details than the stack trace would already have. The interrupt isn't even likely to happen anyway, unless the user kills the test on purpose, in which case, they probably don't care which exception type gets thrown."", 'commenter': 'ctubbsii'}, {'comment': 'Fixed in 55265e8199', 'commenter': 'EdColeman'}]"
3508,test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedScanIT.java,"@@ -392,21 +489,28 @@ public void testBatchScanPauses() throws Exception {
         assertEquals(currentCount, fetched.get());
         assertTrue(SCAN_START_DELAYED.doubleValue() >= paused);
         assertEquals(returned, SCAN_RETURNED_EARLY.doubleValue());
-        assertEquals(1, LOW_MEM_DETECTED.get());
 
+        // check across multiple low memory checks and metric updates that low memory detected
+        // remains set
+        int checkCount = 6;
+        while (checkCount-- > 0) {","[{'comment': ""This currently runs 6 times (5, 4, 3, 2, 1, 0). It seems like you intended it to run 5 times (5, 4, 3, 2, 1), or maybe (4, 3, 2, 1, 0)?.\r\n\r\n```suggestion\r\n        while (--checkCount > 0) {\r\n```\r\n\r\nYou could also be more explicit about the range you want to iterate over by using IntStream and forEach:\r\n\r\n```java\r\nIntStream.rangeClosed(1, 5).map(i -> 6-i).forEach(System.out::println); // or\r\nIntStream.rangeClosed(-5, -1).map(i -> -i).forEach(System.out::println);\r\n```\r\n\r\nThat example goes backwards, like yours, but it's easier to go forwards."", 'commenter': 'ctubbsii'}, {'comment': 'There is no requirement for number of iterations. The more iterations the greater the confidence that the value is not flapping. So, there was a trade-off of greater confidence vs increased test time. \r\n\r\nAt a minimum, for the test to be valid the metrics do need to update at least once.\r\n\r\nChanged it to count up in 3ed08fc601, mainly for logging. Chose not to use IntStream here because it seems harder to understand for a common loop construct in a test,', 'commenter': 'EdColeman'}]"
3508,test/src/main/java/org/apache/accumulo/test/functional/MemoryFreeingIterator.java,"@@ -28,23 +28,39 @@
 import org.apache.accumulo.core.iterators.IteratorEnvironment;
 import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
 import org.apache.accumulo.core.iterators.WrappingIterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 
 public class MemoryFreeingIterator extends WrappingIterator {
 
-  @Override
-  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,
-      IteratorEnvironment env) throws IOException {
-    super.init(source, options, env);
+  private static final Logger LOG = LoggerFactory.getLogger(MemoryFreeingIterator.class);
+
+  @SuppressFBWarnings(value = ""DM_GC"", justification = ""gc is okay for test"")
+  public MemoryFreeingIterator() {
+    super();","[{'comment': ""Don't need this\r\n\r\n```suggestion\r\n```"", 'commenter': 'ctubbsii'}, {'comment': 'Removed in 55265e8199', 'commenter': 'EdColeman'}]"
3508,test/src/main/java/org/apache/accumulo/test/functional/MemoryFreeingIterator.java,"@@ -28,23 +28,39 @@
 import org.apache.accumulo.core.iterators.IteratorEnvironment;
 import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
 import org.apache.accumulo.core.iterators.WrappingIterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 
 public class MemoryFreeingIterator extends WrappingIterator {
 
-  @Override
-  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,
-      IteratorEnvironment env) throws IOException {
-    super.init(source, options, env);
+  private static final Logger LOG = LoggerFactory.getLogger(MemoryFreeingIterator.class);
+
+  @SuppressFBWarnings(value = ""DM_GC"", justification = ""gc is okay for test"")
+  public MemoryFreeingIterator() {
+    super();
+    LOG.info(""Freeing consumed memory"");
     MemoryConsumingIterator.freeBuffers();
     while (this.isRunningLowOnMemory()) {
+      System.gc();
       // wait for LowMemoryDetector to recognize the memory is free.
       try {
         Thread.sleep(SECONDS.toMillis(1));
       } catch (InterruptedException ex) {
         Thread.currentThread().interrupt();
-        throw new IOException(""wait for low memory detector interrupted"", ex);
+        throw new RuntimeException(""wait for low memory detector interrupted"", ex);
       }
     }
+    LOG.info(""Consumed memory freed"");
+  }
+
+  @Override
+  @SuppressFBWarnings(value = ""DM_GC"", justification = ""gc is okay for test"")
+  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,
+      IteratorEnvironment env) throws IOException {
+    super.init(source, options, env);
+    LOG.info(""init call - should free consumed memory"");","[{'comment': ""What's the reasoning for moving the content of this method into the constructor? If you do need to move stuff to the constructor, you probably don't need this method at all.\r\n\r\nEven if you keep it for some kind of log message, you don't need the warnings suppression... there is no garbage collection happening in this method anymore. Also, the log message itself seems incorrect, since it's not freeing anything."", 'commenter': 'ctubbsii'}, {'comment': ""The reason for moving it is that the scan was not running because the server was low on memory. Instead, I moved the code to the constructor and then call `client.instanceOperations().testClassLoad(MemoryFreeingIterator.class.getName(),  WrappingIterator.class.getCanonicalName());` to load the class on the TabletServer and free the consumed memory. I don't think this method is needed."", 'commenter': 'dlmarion'}, {'comment': 'My only concern is that it still may not run... because calling `System.gc()` may do nothing. Is that going to result in a flaky test, or just flaky coverage but consistently passing test?', 'commenter': 'ctubbsii'}, {'comment': ""MemoryFreeingIterator or a scan using it doesn't need to run. `MemoryConsumingIterator.freeBuffers()` just needs to get called on the server side."", 'commenter': 'dlmarion'}]"
3508,test/src/main/java/org/apache/accumulo/test/functional/MemoryFreeingIterator.java,"@@ -28,23 +28,39 @@
 import org.apache.accumulo.core.iterators.IteratorEnvironment;
 import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
 import org.apache.accumulo.core.iterators.WrappingIterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 
 public class MemoryFreeingIterator extends WrappingIterator {
 
-  @Override
-  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,
-      IteratorEnvironment env) throws IOException {
-    super.init(source, options, env);
+  private static final Logger LOG = LoggerFactory.getLogger(MemoryFreeingIterator.class);
+
+  @SuppressFBWarnings(value = ""DM_GC"", justification = ""gc is okay for test"")
+  public MemoryFreeingIterator() {
+    super();
+    LOG.info(""Freeing consumed memory"");
     MemoryConsumingIterator.freeBuffers();
     while (this.isRunningLowOnMemory()) {
+      System.gc();","[{'comment': 'Calling this may do nothing at all. Is that okay in this test?', 'commenter': 'ctubbsii'}, {'comment': 'I agree that System.gc is a hint and may do nothing. The real thing that needs to occur is the call to `MemoryConsumingIterator.freeBuffers()`. The JVM GC should eventually run as the system tries to use more memory. Will it cause a flaky test? Unknown.', 'commenter': 'dlmarion'}]"
3508,test/src/main/java/org/apache/accumulo/test/functional/MemoryFreeingIterator.java,"@@ -28,23 +28,39 @@
 import org.apache.accumulo.core.iterators.IteratorEnvironment;
 import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
 import org.apache.accumulo.core.iterators.WrappingIterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 
 public class MemoryFreeingIterator extends WrappingIterator {
 
-  @Override
-  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,
-      IteratorEnvironment env) throws IOException {
-    super.init(source, options, env);
+  private static final Logger LOG = LoggerFactory.getLogger(MemoryFreeingIterator.class);
+
+  @SuppressFBWarnings(value = ""DM_GC"", justification = ""gc is okay for test"")
+  public MemoryFreeingIterator() {
+    super();
+    LOG.info(""Freeing consumed memory"");","[{'comment': 'Well, it\'s attempting to anyway... there\'s no guarantee here.\r\n\r\n```suggestion\r\n    LOG.info(""Attempting to free consumed memory"");\r\n```', 'commenter': 'ctubbsii'}]"
3508,test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedScanIT.java,"@@ -162,13 +173,11 @@ private void consumeServerMemory(BatchScanner scanner) {
     assertTrue(iter.hasNext());
   }
 
-  static void freeServerMemory(AccumuloClient client, String table) throws Exception {
-    try (Scanner scanner = client.createScanner(table)) {
-      scanner.addScanIterator(new IteratorSetting(11, MemoryFreeingIterator.class, Map.of()));
-      @SuppressWarnings(""unused"")
-      Iterator<Entry<Key,Value>> iter = scanner.iterator(); // init'ing the iterator should be
-                                                            // enough to free the memory
-    }
+  static void freeServerMemory(AccumuloClient client) throws Exception {
+    // Instantiating this class on the TabletServer will free the memory as it
+    // frees the buffers created by the MemoryConsumingIterator in its constructor.
+    client.instanceOperations().testClassLoad(MemoryFreeingIterator.class.getName(),
+        WrappingIterator.class.getCanonicalName());","[{'comment': ""In this case, it doesn't matter, but you can't use canonical name to specify the type in the general case.\r\n\r\n```suggestion\r\n        WrappingIterator.class.getName());\r\n```"", 'commenter': 'ctubbsii'}]"
3508,test/src/main/java/org/apache/accumulo/test/util/Wait.java,"@@ -48,4 +50,13 @@ public static boolean waitFor(final Condition condition, final long duration,
     }
     return conditionSatisfied;
   }
+
+  /**
+   * A retry for use in junit tests when testing asynchronous conditions that are expected to
+   * eventually meet the condition or fail the test. Using this method should be used instead of an
+   * arbitrary sleep and hoping to catch the condition in the expected state.
+   */
+  public static void assertTrueWithRetry(final Wait.Condition condition) throws Exception {
+    assertTrue(waitFor(condition));
+  }","[{'comment': ""Using this method isn't any better than not using it. I don't think this is really adding much value here.\r\n\r\n```java\r\n  assertTrueWithRetry(x);\r\n  assertTrue(waitFor(x));\r\n```\r\n\r\nAlso, it seems like somebody was trying to avoid adding JUnit assertions into this class directly, otherwise, I'd expect the waitFor methods to already do this assertion... because everywhere we use it, we already assertTrue on the returned value... except in a few places where it looks like we forgot to check the return value on the assumption that something gets thrown if the timeout is exceeded... but that's not true.\r\n\r\nSo, if we want to do anything here in this Wait class, it'd be better to just bake in the assertTrue at the end of the existing waitFor method."", 'commenter': 'ctubbsii'}, {'comment': ""Originally @ctubbsii I thought the suggestion in PR #3384 to use a static method in the test to replace the assertTrue(waitFor(....)\r\n\r\n```\r\nThe method would look like:\r\n  private static void waitForTrue(Wait.Condition condition) throws Exception {\r\n    // could specify 20 seconds, but the default is 30, and that's fine. The default delay is 1.\r\n    assertTrue(Wait.waitFor(condition));\r\n  }\r\n```\r\nSo that it could be consistent across tests, I though the Wait Util might me a good home.  I though the original `assertTrue(waitFor(x))` was clear enough, but tried to be accommodating to your request.  Maybe I didn't understand the original request?  At this point I do not know what you want.\r\n\r\nIt seems to be remove the method from Wait to eliminate the junit dependency and just use `assertTrue(waifFor(` in the tests? as it was originally?"", 'commenter': 'EdColeman'}, {'comment': ""I guess I have few thoughts here that superseded my previous comment:\r\n\r\n1. My suggestion was mainly about reducing boilerplate. Without the custom timeouts, there's already a lot less boilerplate and the method might not be needed.\r\n2. Your name is longer than the one I suggested, making it less beneficial in my mind.\r\n3. My original suggestion was to create a local method in the test, not in the Wait test utility class, because there seemed to be a convention to avoid using the assertions in the Wait class, for some reason. I'm not opposed to putting the assertions directly in the Wait class, but it wasn't done before. If you're willing to put the assertions in the Wait class, then it could be done with the existing methods.\r\n4. After looking more closely at the Wait class, and how it is used, I think all uses of it would benefit from having the assertions checked inside its waitFor methods. If that's done (can be a separate PR), then adding this new method becomes completely unnecessary... it's merely checking the same condition that all the other waitFor uses are... the only difference is that this new method checks it inside the Wait class, and all the other uses check it just outside... it seems kind of pointless to add a method that does the same thing as the existing methods, but is used slightly differently. It's like having a methods for commutative operations like: `multiplyAB(A, B)` and a separate `multiplyBA(B, A)`. We only need the one.\r\n\r\nIf assertions are added to the Wait method, they could have names that make it clear they are doing an internal assert, like `waitForAndAssertTrue` or `assertTrueEventually`, or `assertTrueWithRetry` instead of just `waitFor`."", 'commenter': 'ctubbsii'}]"
3510,core/src/main/java/org/apache/accumulo/core/metadata/ValidationUtil.java,"@@ -53,6 +53,7 @@ public static Path validate(Path path) {
 
   public static void validateRFileName(String fileName) {
     Objects.requireNonNull(fileName);
+    // TODO: In 3.0.0 validate that filename starts with FilePrefix","[{'comment': 'You should be able to implement this todo here and remove the comment. Something like `FilePrefix.fromPrefix(fileName.substring(0,1));`', 'commenter': 'dlmarion'}]"
3542,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -1101,7 +1107,7 @@ public void run() {
       log.error(""Error initializing metrics, metrics will not be emitted."", e1);
     }
 
-    recoveryManager = new RecoveryManager(this, TIME_TO_CACHE_RECOVERY_WAL_EXISTENCE);
+    recoveryManager = new RecoveryManager(this, timeToCacheRecoveryWalExistence);","[{'comment': 'This should pass a LongSupplier to compute the value dynamically from the current value of the configurable interval time.', 'commenter': 'ctubbsii'}, {'comment': ""I'm not sure that makes sense here. This is done during Manager startup before clients can connect to it. The RecoveryManager is created and the value of `timeToCacheRecoveryWalExistence` is used in the RecoveryManager constructor to set the expiration time on the Caffeine CacheBuilder. This is only called once, not in a loop, and I don't think the Caffeine cache can be updated after construction."", 'commenter': 'dlmarion'}]"
3542,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -449,6 +448,13 @@ public static void main(String[] args) throws Exception {
       log.info(""SASL is not enabled, delegation tokens will not be available"");
       delegationTokensAvailable = false;
     }
+    this.waitTimeBetweenScans =
+        aconf.getTimeInMillis(Property.MANAGER_TABLET_GROUP_WATCHER_INTERVAL);","[{'comment': ""Computing it once at the time of the Manager startup makes this a fixed property, only changeable at restarts. You can do that, of course, but should add it to the `Property.fixedProperties`. However, a better idea is to have the field be a LongSupplier, and compute it from the config dynamically. You can use Guava's `Suppliers.memoize` to create the supplier. You can configure the memoizing supplier to cache the value for a certain amount of time, if you don't want it to do the lookup from the config too frequently."", 'commenter': 'ctubbsii'}, {'comment': 'I think I can do this without the supplier. TabletGroupWatcher calls `manager.getWaitTImeBetweenScans()`. That method can just get the current value of the property from the configuration and return it.', 'commenter': 'dlmarion'}, {'comment': 'baea8d7 provides the TabletGroupWatcher with the current value of the property.', 'commenter': 'dlmarion'}]"
3542,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -197,7 +197,7 @@ public void run() {
         }
 
         if (currentTServers.isEmpty()) {
-          eventListener.waitForEvents(Manager.TIME_TO_WAIT_BETWEEN_SCANS);
+          eventListener.waitForEvents(manager.getWaitTimeBetweenScans());","[{'comment': ""Using the Supplier idea I suggested, the value can be obtained once at the top of this loop, and reused for the rest of the loop iteration, using a final local variable, so it doesn't change in the middle of the iteration."", 'commenter': 'ctubbsii'}, {'comment': 'baea8d7 provides the TabletGroupWatcher with the current value of the property.', 'commenter': 'dlmarion'}]"
3542,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -311,6 +311,9 @@ public enum Property {
       ""The balancer class that accumulo will use to make tablet assignment and ""
           + ""migration decisions."",
       ""1.3.5""),
+  MANAGER_TABLET_GROUP_WATCHER_INTERVAL(""manager.tablet.watcher.interval"", ""60s"",
+      PropertyType.TIMEDURATION,
+      ""Time to wait between scanning tablet states to determine migrations, etc."", ""2.1.2""),","[{'comment': 'I\'m not sure `etc.` is doing much to help the reader understand the category of things this affects. The migrations is only one example. If there were a second example in the list, then `etc.` could make sense, because one might be able to infer a category of tasks from two examples. If there\'s no second example to add to the list, you could just phrase it as ""states to determine outstanding tasks, such as migrations.""\r\n\r\n```suggestion\r\n      ""Time to wait between scanning tablet states to identify outstanding tasks to perform, such as migrations."", ""2.1.2""),\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Updated property description in ff6923e', 'commenter': 'dlmarion'}]"
3542,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -347,8 +347,8 @@ public void run() {
         }
         if (manager.tserverSet.getCurrentServers().equals(currentTServers.keySet())) {
           Manager.log.debug(String.format(""[%s] sleeping for %.2f seconds"", store.name(),
-              Manager.TIME_TO_WAIT_BETWEEN_SCANS / 1000.));
-          eventListener.waitForEvents(Manager.TIME_TO_WAIT_BETWEEN_SCANS);
+              manager.getWaitTimeBetweenScans() / 1000.));
+          eventListener.waitForEvents(manager.getWaitTimeBetweenScans());","[{'comment': ""I still think you need to use a local variable in here, so the log message isn't logging a different value than what's actually being used, which makes debugging a pain.\r\n\r\nAnd, this, as well as the occurrence above are still in the same `while (manager.stillManager())` loop. It would be weird if the value changed from one operation to another in that same loop. They should share the same local value."", 'commenter': 'ctubbsii'}, {'comment': 'Created a local variable at the top of the loop so that the value for the property is the same for the entire iteration, in ff6923e', 'commenter': 'dlmarion'}]"
3542,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -449,6 +447,13 @@ public static void main(String[] args) throws Exception {
       log.info(""SASL is not enabled, delegation tokens will not be available"");
       delegationTokensAvailable = false;
     }
+    long waitTimeBetweenScans =
+        aconf.getTimeInMillis(Property.MANAGER_TABLET_GROUP_WATCHER_INTERVAL);
+    this.timeToCacheRecoveryWalExistence = waitTimeBetweenScans / 4;
+  }
+
+  public long getWaitTimeBetweenScans() {
+    return this.getConfiguration().getTimeInMillis(Property.MANAGER_TABLET_GROUP_WATCHER_INTERVAL);","[{'comment': ""The `getWaitTimeBetweenScans()` method returns the current value, but `this.timeToCacheRecoveryWalExistence` is still based on, and fixed to, the initial value. It doesn't seem like a good idea to have a property be runtime-mutable, but also be the basis for a runtime-immutable characteristic, at the same time. Should these be separate properties, one that is a fixedProperty and one that is mutable at runtime?"", 'commenter': 'ctubbsii'}, {'comment': 'Created a separate property in ff6923e', 'commenter': 'dlmarion'}]"
3542,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -449,6 +447,12 @@ public static void main(String[] args) throws Exception {
       log.info(""SASL is not enabled, delegation tokens will not be available"");
       delegationTokensAvailable = false;
     }
+    this.timeToCacheRecoveryWalExistence =
+        aconf.getTimeInMillis(Property.MANAGER_RECOVERY_WAL_EXISTENCE_CACHE_TIME);
+  }
+
+  public long getWaitTimeBetweenScans() {
+    return this.getConfiguration().getTimeInMillis(Property.MANAGER_TABLET_GROUP_WATCHER_INTERVAL);","[{'comment': ""Since this is now only used once, and the implementation is trivial, this should now be inline'd, rather than add a new API method to Manager."", 'commenter': 'ctubbsii'}]"
3542,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -311,6 +311,13 @@ public enum Property {
       ""The balancer class that accumulo will use to make tablet assignment and ""
           + ""migration decisions."",
       ""1.3.5""),
+  MANAGER_TABLET_GROUP_WATCHER_INTERVAL(""manager.tablet.watcher.interval"", ""60s"",
+      PropertyType.TIMEDURATION,
+      ""Time to wait between scanning tablet states to identify tablets that need to be assigned, un-assigned, migrated, etc."",
+      ""2.1.2""),
+  MANAGER_RECOVERY_WAL_EXISTENCE_CACHE_TIME(""manager.recovery.wal.cache.time"", ""15s"",
+      PropertyType.TIMEDURATION,
+      ""Amount of time that the existence of recovery write-ahead logs is cached."", ""2.1.2""),","[{'comment': 'This should be grouped adjacent to the other `manager.recovery.*` property.', 'commenter': 'ctubbsii'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -37,33 +37,42 @@
 
 public class CreateNamespaceCommand extends Command {
   private Option createNamespaceOptCopyConfig;
+  private Option createNamespaceOptCopyProperties;
 
   @Override
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws AccumuloException, AccumuloSecurityException, TableExistsException,
       TableNotFoundException, IOException, ClassNotFoundException, NamespaceExistsException,
       NamespaceNotFoundException {
 
-    if (createNamespaceOptCopyConfig == null) {
-      getOptions();
-    }
+    getOptions();","[{'comment': 'I\'m not sure what this is doing at all. `getOptions()` will be called by the framework later. It should also be idempotent and lightweight, so it doesn\'t matter if the private members are set or not yet. But by the time this method is called, they certainly will be... so unless I\'m mistaken, this line is redundant and useless.\r\n\r\nI looked around and no other shell commands do this. The only other two that are ""weird"" in some way is the GrantCommand and RevokeCommand. In those, their getOptions() calls super.getOptions(), but just ignores the returned object, it for some reason. So, they are either depending on side-effects, are pointlessly calling super.getOptions() that does nothing, or they are broken and not correctly inheriting options from their super class. Regardless, those are a separate issue from here.\r\n', 'commenter': 'ctubbsii'}, {'comment': 'removed in c05206a036', 'commenter': 'EdColeman'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -37,33 +37,42 @@
 
 public class CreateNamespaceCommand extends Command {
   private Option createNamespaceOptCopyConfig;
+  private Option createNamespaceOptCopyProperties;
 
   @Override
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws AccumuloException, AccumuloSecurityException, TableExistsException,
       TableNotFoundException, IOException, ClassNotFoundException, NamespaceExistsException,
       NamespaceNotFoundException {
 
-    if (createNamespaceOptCopyConfig == null) {
-      getOptions();
-    }
+    getOptions();
 
     String namespace = cl.getArgs()[0];
 
     shellState.getAccumuloClient().namespaceOperations().create(namespace);
 
-    // Copy options if flag was set
-    Map<String,String> configuration = null;
+    Map<String,String> propsToSet = null;
+
+    // Copy configuration options if flag was set
     if (cl.hasOption(createNamespaceOptCopyConfig.getOpt())) {
-      String copy = cl.getOptionValue(createNamespaceOptCopyConfig.getOpt());
+      String srcNs = cl.getOptionValue(createNamespaceOptCopyConfig.getOpt());
       if (shellState.getAccumuloClient().namespaceOperations().exists(namespace)) {
-        configuration = shellState.getAccumuloClient().namespaceOperations().getConfiguration(copy);
+        propsToSet = shellState.getAccumuloClient().namespaceOperations().getConfiguration(srcNs);
       }
     }
-    if (configuration != null) {
-      final Map<String,String> config = configuration;
+
+    if (cl.hasOption(createNamespaceOptCopyProperties.getOpt())) {
+      String srcNs = cl.getOptionValue(createNamespaceOptCopyProperties.getOpt());
+      if (shellState.getAccumuloClient().namespaceOperations().exists(namespace)) {
+        propsToSet =
+            shellState.getAccumuloClient().namespaceOperations().getNamespaceProperties(srcNs);
+      }
+    }","[{'comment': ""I didn't realize we already had an API that got only the namespace properties. I thought the new API showed the effective properties, just like the old API."", 'commenter': 'ctubbsii'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -37,33 +37,42 @@
 
 public class CreateNamespaceCommand extends Command {
   private Option createNamespaceOptCopyConfig;
+  private Option createNamespaceOptCopyProperties;
 
   @Override
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws AccumuloException, AccumuloSecurityException, TableExistsException,
       TableNotFoundException, IOException, ClassNotFoundException, NamespaceExistsException,
       NamespaceNotFoundException {
 
-    if (createNamespaceOptCopyConfig == null) {
-      getOptions();
-    }
+    getOptions();
 
     String namespace = cl.getArgs()[0];
 
     shellState.getAccumuloClient().namespaceOperations().create(namespace);
 
-    // Copy options if flag was set
-    Map<String,String> configuration = null;
+    Map<String,String> propsToSet = null;
+
+    // Copy configuration options if flag was set
     if (cl.hasOption(createNamespaceOptCopyConfig.getOpt())) {
-      String copy = cl.getOptionValue(createNamespaceOptCopyConfig.getOpt());
+      String srcNs = cl.getOptionValue(createNamespaceOptCopyConfig.getOpt());
       if (shellState.getAccumuloClient().namespaceOperations().exists(namespace)) {
-        configuration = shellState.getAccumuloClient().namespaceOperations().getConfiguration(copy);
+        propsToSet = shellState.getAccumuloClient().namespaceOperations().getConfiguration(srcNs);
       }
     }
-    if (configuration != null) {
-      final Map<String,String> config = configuration;
+
+    if (cl.hasOption(createNamespaceOptCopyProperties.getOpt())) {
+      String srcNs = cl.getOptionValue(createNamespaceOptCopyProperties.getOpt());
+      if (shellState.getAccumuloClient().namespaceOperations().exists(namespace)) {
+        propsToSet =
+            shellState.getAccumuloClient().namespaceOperations().getNamespaceProperties(srcNs);
+      }
+    }
+
+    if (propsToSet != null) {
+      final Map<String,String> props = propsToSet;
       shellState.getAccumuloClient().namespaceOperations().modifyProperties(namespace,","[{'comment': ""It'd be nice if we didn't have to mutate the properties, but could set them at creation time, similar to NewTableConfiguration. This could fail after the namespace is created, but without modifying the properties."", 'commenter': 'ctubbsii'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -37,33 +37,42 @@
 
 public class CreateNamespaceCommand extends Command {
   private Option createNamespaceOptCopyConfig;
+  private Option createNamespaceOptCopyProperties;
 
   @Override
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws AccumuloException, AccumuloSecurityException, TableExistsException,
       TableNotFoundException, IOException, ClassNotFoundException, NamespaceExistsException,
       NamespaceNotFoundException {
 
-    if (createNamespaceOptCopyConfig == null) {
-      getOptions();
-    }
+    getOptions();
 
     String namespace = cl.getArgs()[0];
 
     shellState.getAccumuloClient().namespaceOperations().create(namespace);
 
-    // Copy options if flag was set
-    Map<String,String> configuration = null;
+    Map<String,String> propsToSet = null;
+
+    // Copy configuration options if flag was set
     if (cl.hasOption(createNamespaceOptCopyConfig.getOpt())) {
-      String copy = cl.getOptionValue(createNamespaceOptCopyConfig.getOpt());
+      String srcNs = cl.getOptionValue(createNamespaceOptCopyConfig.getOpt());
       if (shellState.getAccumuloClient().namespaceOperations().exists(namespace)) {
-        configuration = shellState.getAccumuloClient().namespaceOperations().getConfiguration(copy);
+        propsToSet = shellState.getAccumuloClient().namespaceOperations().getConfiguration(srcNs);
       }
     }
-    if (configuration != null) {
-      final Map<String,String> config = configuration;
+
+    if (cl.hasOption(createNamespaceOptCopyProperties.getOpt())) {
+      String srcNs = cl.getOptionValue(createNamespaceOptCopyProperties.getOpt());
+      if (shellState.getAccumuloClient().namespaceOperations().exists(namespace)) {","[{'comment': ""This existence check will hide the errors if the table doesn't exist. That looks like it's a pre-existing problem. If the user specifies to copy from a namespace that doesn't exist (probably most often from a typo), I'd expect a NamespaceNotFoundException, not silent success without copying anything. Even though it's a pre-existing problem, it shouldn't be carried forward into this new implementation, and the old implementation should be fixed, too."", 'commenter': 'ctubbsii'}, {'comment': 'Addressed with c05206a036', 'commenter': 'EdColeman'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -87,11 +96,13 @@ public Options getOptions() {
 
     createNamespaceOptCopyConfig =
         new Option(""cc"", ""copy-config"", true, ""namespace to copy configuration from"");
+    createNamespaceOptCopyProperties = new Option(""cp"", ""copy-properties"", true,
+        ""namespace to copy properties from (only namespace properties are copied)"");","[{'comment': ""This is so similarly named. Although the naming similarity affects the Java API also, the Java API has the benefit of a decent Javadoc. This could use a better name to avoid getting too verbose help documentation.\r\n\r\nFor example, instead of creating a separate property (and allowing them both to be used, which could get confusing), just add a `--copy-config-exclude-parent` or just `--exclude-parent` that's used in conjunction with `--copy-config`."", 'commenter': 'ctubbsii'}, {'comment': 'c05206a036 changed to use --exclude-parent ', 'commenter': 'EdColeman'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -32,42 +32,55 @@
 import org.apache.accumulo.shell.Shell.Command;
 import org.apache.commons.cli.CommandLine;
 import org.apache.commons.cli.Option;
-import org.apache.commons.cli.OptionGroup;
 import org.apache.commons.cli.Options;
 
 public class CreateNamespaceCommand extends Command {
   private Option createNamespaceOptCopyConfig;
+  private Option createNamesapceOptExcludeParentProps;
 
   @Override
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws AccumuloException, AccumuloSecurityException, TableExistsException,
       TableNotFoundException, IOException, ClassNotFoundException, NamespaceExistsException,
       NamespaceNotFoundException {
 
-    if (createNamespaceOptCopyConfig == null) {
-      getOptions();
+    // validate that copy config and copy properties options are mutually exclusive.
+    if (cl.hasOption(createNamesapceOptExcludeParentProps.getLongOpt())
+        && !cl.hasOption(createNamespaceOptCopyConfig.getOpt())) {
+      throw new IllegalArgumentException(createNamesapceOptExcludeParentProps.getLongOpt()
+          + "" only valid when using "" + createNamespaceOptCopyConfig.getLongOpt());
     }
 
     String namespace = cl.getArgs()[0];
 
     shellState.getAccumuloClient().namespaceOperations().create(namespace);
+    if (!shellState.getAccumuloClient().namespaceOperations().exists(namespace)) {
+      throw new IllegalArgumentException(""Could not create namespace `"" + namespace + ""`"");
+    }
 
-    // Copy options if flag was set
-    Map<String,String> configuration = null;
+    // Copy configuration options if flag was set
     if (cl.hasOption(createNamespaceOptCopyConfig.getOpt())) {
-      String copy = cl.getOptionValue(createNamespaceOptCopyConfig.getOpt());
-      if (shellState.getAccumuloClient().namespaceOperations().exists(namespace)) {
-        configuration = shellState.getAccumuloClient().namespaceOperations().getConfiguration(copy);
+      String srcNs = cl.getOptionValue(createNamespaceOptCopyConfig.getOpt());
+      if (!srcNs.isEmpty() && !shellState.getAccumuloClient().namespaceOperations().exists(srcNs)) {
+        throw new NamespaceNotFoundException(null, srcNs, null);
+      }
+      Map<String,String> userProps;
+      if (cl.hasOption(createNamesapceOptExcludeParentProps.getLongOpt())) {
+        // use namespace props - excludes parents in configuration
+        userProps =
+            shellState.getAccumuloClient().namespaceOperations().getNamespaceProperties(srcNs);
+      } else {
+        // use namespace config - include parents in configuration hierarchy
+        userProps = shellState.getAccumuloClient().namespaceOperations().getConfiguration(srcNs);
+      }
+      if (userProps != null) {","[{'comment': 'can userProps actually be null here?', 'commenter': 'ivakegg'}, {'comment': 'Currently, probably not, but unless we use a not null annotation, that is not guaranteed to always be the case.', 'commenter': 'EdColeman'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -32,42 +32,55 @@
 import org.apache.accumulo.shell.Shell.Command;
 import org.apache.commons.cli.CommandLine;
 import org.apache.commons.cli.Option;
-import org.apache.commons.cli.OptionGroup;
 import org.apache.commons.cli.Options;
 
 public class CreateNamespaceCommand extends Command {
   private Option createNamespaceOptCopyConfig;
+  private Option createNamesapceOptExcludeParentProps;
 
   @Override
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws AccumuloException, AccumuloSecurityException, TableExistsException,
       TableNotFoundException, IOException, ClassNotFoundException, NamespaceExistsException,
       NamespaceNotFoundException {
 
-    if (createNamespaceOptCopyConfig == null) {
-      getOptions();
+    // validate that copy config and copy properties options are mutually exclusive.","[{'comment': 'This comment is out of date with the current code and refers to a previous iteration of this change.', 'commenter': 'ctubbsii'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateNamespaceCommand.java,"@@ -89,10 +102,11 @@ public Options getOptions() {
         new Option(""cc"", ""copy-config"", true, ""namespace to copy configuration from"");
     createNamespaceOptCopyConfig.setArgName(""namespace"");
 
-    OptionGroup ogp = new OptionGroup();
-    ogp.addOption(createNamespaceOptCopyConfig);
+    createNamesapceOptExcludeParentProps = new Option(null, ""exclude-parent"", false,
+        ""exclude parent(s) properties when copying configuration"");","[{'comment': ""s/parent(s)/parent's/"", 'commenter': 'ctubbsii'}]"
3562,shell/src/main/java/org/apache/accumulo/shell/commands/CreateTableCommand.java,"@@ -292,7 +310,9 @@ public Options getOptions() {
     final Options o = new Options();
 
     createTableOptCopyConfig =
-        new Option(""cc"", ""copy-config"", true, ""table to copy configuration from"");
+        new Option(""cc"", ""copy-config"", true, ""table to copy effective configuration from"");
+    createTableOptExcludeParentProps = new Option(null, ""exclude-parent"", false,
+        ""exclude parent(s) properties when copying configuration"");","[{'comment': ""s/parent(s)/parent's/"", 'commenter': 'ctubbsii'}]"
3568,test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java,"@@ -520,10 +520,12 @@ public void setIterOptionPrompt() throws Exception {
 
   protected void checkTableForProperty(final AccumuloClient client, final String tableName,
       final String expectedKey, final String expectedValue) throws Exception {
-    assertTrue(
-        Wait.waitFor(() -> client.tableOperations().getConfiguration(tableName).get(expectedKey)
-            .equals(expectedValue), 5000, 500),
-        ""Failed to find expected value for key: "" + expectedKey);
+    try {","[{'comment': 'This seems like a step backwards in terms of usage. The original issue commented on making it easier to use the Wait api but having to now use a try/catch makes this pretty ugly. Maybe instead of replacing the behavior that returns false we could have an additional method (or flag) that supports throwing the exception so we support both ways depending on what the unit test writer wants to do.', 'commenter': 'cshannon'}, {'comment': 'This one place had an exception message that I thought would be nice to keep - all other occurrences where just failing when they used assertTrue and there were some that did not test the return condition.\r\n\r\nKeeping the exception message in this single case can be removed if its not adding much.', 'commenter': 'EdColeman'}, {'comment': 'Maybe we could provide an option to provide an error message for the IllegalStateException then? I feel like this is going to come up again where we want to have a custom error message that is thrown. So we should just allow providing the message for the IllegalStateException as an option with an overloaded method.', 'commenter': 'cshannon'}, {'comment': 'Could just add a method that is something like the following and the existing method can just have a default.\r\n\r\n```Java\r\npublic static void waitFor(final Condition condition, final long duration, final long sleepMillis, String errorMsg)\r\n      throws Exception {\r\n//pass error message to new IllegalStateException()\r\n}\r\n```', 'commenter': 'cshannon'}, {'comment': 'Added in 0031099059', 'commenter': 'EdColeman'}]"
3568,test/src/main/java/org/apache/accumulo/test/util/Wait.java,"@@ -29,23 +29,37 @@ public interface Condition {
     boolean isSatisfied() throws Exception;
   }
 
-  public static boolean waitFor(Condition condition) throws Exception {
-    return waitFor(condition, MAX_WAIT_MILLIS);
+  /**
+   * Wait for the provided condition - will throw an IllegalStateException is the wait exceeds the
+   * wait period.
+   */
+  public static void waitFor(Condition condition) throws Exception {
+    waitFor(condition, MAX_WAIT_MILLIS);
   }
 
-  public static boolean waitFor(final Condition condition, final long duration) throws Exception {
-    return waitFor(condition, duration, SLEEP_MILLIS);
+  /**
+   * Wait for the provided condition - will throw an IllegalStateException is the wait exceeds the
+   * wait period.
+   */
+  public static void waitFor(final Condition condition, final long duration) throws Exception {
+    waitFor(condition, duration, SLEEP_MILLIS);
   }
 
-  public static boolean waitFor(final Condition condition, final long duration,
-      final long sleepMillis) throws Exception {
+  /**
+   * Wait for the provided condition - will throw an IllegalStateException is the wait exceeds the
+   * wait period.
+   */
+  public static void waitFor(final Condition condition, final long duration, final long sleepMillis)
+      throws Exception {","[{'comment': 'Also I think you should be able to get rid of the throws Exception here if just throwing a runtime exception only', 'commenter': 'cshannon'}, {'comment': 'The exception from `condition.isSatisfied()` needs to be declared, or turned into a Runtime exception - otherwise it is bubbling up through the waitFor calls - and the InterruptedException is not being specifically handled.', 'commenter': 'EdColeman'}, {'comment': 'Addressed in 0031099059', 'commenter': 'EdColeman'}]"
3612,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tables/TableInformation.java,"@@ -60,12 +60,12 @@ public class TableInformation {
   // running scans with queued in parenthesis
   public String scansCombo;
 
-  private int queuedMajorCompactions;
-  private int runningMajorCompactions;
-  private int queuedMinorCompactions;
-  private int runningMinorCompactions;
-  private int queuedScans;
-  private int runningScans;
+  public int queuedMajorCompactions;
+  public int runningMajorCompactions;
+  public int queuedMinorCompactions;
+  public int runningMinorCompactions;
+  public int queuedScans;
+  public int runningScans;","[{'comment': ""Why change these to public? It doesn't look like they are used outside of this class."", 'commenter': 'DomGarguilo'}, {'comment': 'Never mind, I now see that these are new values being sent to the monitor so they need to be public.', 'commenter': 'DomGarguilo'}]"
3612,server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tservers/TabletServerInformation.java,"@@ -64,17 +64,17 @@ public class TabletServerInformation {
   // New variables
 
   public String ip;
-  private Integer scansRunning;
-  private Integer scansQueued;
+  public Integer scansRunning;
+  public Integer scansQueued;
   // combo string with running value and number queued in parenthesis
   public String minorCombo;
   public String majorCombo;
   public String scansCombo;
-  private Integer minorRunning;
-  private Integer minorQueued;
+  public Integer minorRunning;
+  public Integer minorQueued;
 
-  private Integer majorRunning;
-  private Integer majorQueued;
+  public Integer majorRunning;
+  public Integer majorQueued;","[{'comment': 'Same here. Do they need to be changed to public for some reason?', 'commenter': 'DomGarguilo'}]"
3612,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/tables.ftl,"@@ -91,7 +114,7 @@
       </script>
       <div class=""row"">
         <div class=""col-xs-12"">
-          <h3>Table Overview</h3>
+          <h3>Table OVERVIEW</h3>","[{'comment': ""Was this changed due to preference? It doesn't look like other parts of the monitor use all caps like this."", 'commenter': 'DomGarguilo'}, {'comment': ""Was typo. Just did that to ensure changes were taking effect. Reverted to 'Overview'."", 'commenter': 'AlbertWhitlock'}]"
3612,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/table.js,"@@ -133,10 +148,30 @@ function initTableServerTable(tableID) {
       },
       {
         ""data"": ""osload""
+      },
+      {
+        ""data"": ""scansRunning""","[{'comment': '```suggestion\r\n        ""data"": ""scansRunning"",\r\n        ""visible"": false\r\n```\r\nI think it makes things easier to follow if the visibility is defined here, along with the `data`, instead of all at once with the line below: `tableServersTable.columns([13, 14, 15, 16, 17, 18]).visible(false);`.', 'commenter': 'DomGarguilo'}, {'comment': ""I like this suggestion. I specified the hidden columns' visibility was false right where the column is defined."", 'commenter': 'AlbertWhitlock'}]"
3612,server/monitor/src/main/resources/org/apache/accumulo/monitor/resources/js/table.js,"@@ -80,6 +80,21 @@ function initTableServerTable(tableID) {
           }
           return data;
         }
+      },
+      {
+        ""targets"": [7],
+        ""type"": ""numeric"",
+        ""orderData"": [13, 14]
+      },","[{'comment': 'I think it would be helpful to add comments to clear up what this new logic does. Maybe a general one and then a specific one per-column like in this example:\r\n```suggestion\r\n      // ensure these columns are sorted by the numeric values that comprise the combined string instead of sorting them alphabetically by the string itself\r\n      \r\n      // sort scans first by number of running, then by number of queued\r\n      {\r\n        ""targets"": [7],\r\n        ""type"": ""numeric"",\r\n        ""orderData"": [13, 14]\r\n      },\r\n```', 'commenter': 'DomGarguilo'}, {'comment': 'I added some comments describing the functionality of those blocks of code.', 'commenter': 'AlbertWhitlock'}]"
3612,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/table.ftl,"@@ -50,6 +50,12 @@
                 <th class=""percent"" title=""The recent index cache hit rate."">Index Cache<br />Hit Rate&nbsp;</th>
                 <th class=""percent"" title=""The recent data cache hit rate."">Data Cache<br />Hit Rate&nbsp;</th>
                 <th class=""big-num"" title=""The Unix one minute load average. The average number of processes in the run queue over a one minute interval."">OS&nbsp;Load&nbsp;</th>
+                <th title=""Running Scans"">Running Scans</th>
+                <th title=""Queued Scans"">Queued Scans</th>
+                <th title=""Running MinC"">Running MinC</th>
+                <th title=""Queued MinC"">Queued MinC</th>
+                <th title=""Running MajC"">Running MajC</th>
+                <th title=""Queued MajC"">Queued MajC</th>","[{'comment': '```suggestion\r\n```\r\nI did some testing and it seems that these columns do not need to be added to the html since they are made invisible anyways in the js. DataTables can still use the values as expected without there being columns present in the html.', 'commenter': 'DomGarguilo'}, {'comment': 'Removed the (hidden) columns from the .ftl files.', 'commenter': 'AlbertWhitlock'}]"
3676,server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.conf.Property.GC_PORT;
+import static org.apache.accumulo.core.conf.Property.INSTANCE_SECRET;
+import static org.apache.accumulo.core.conf.Property.INSTANCE_ZK_HOST;
+import static org.apache.accumulo.core.conf.Property.MANAGER_BULK_TSERVER_REGEX;
+import static org.apache.accumulo.core.conf.Property.TABLE_BLOOM_ENABLED;
+import static org.apache.accumulo.core.conf.Property.TABLE_BLOOM_SIZE;
+import static org.apache.accumulo.core.conf.Property.TABLE_DURABILITY;
+import static org.apache.accumulo.core.conf.Property.TABLE_FILE_MAX;
+import static org.apache.accumulo.core.conf.Property.TSERV_SCAN_MAX_OPENFILES;
+import static org.apache.accumulo.server.MockServerContext.getMockContextWithPropStore;
+import static org.easymock.EasyMock.anyObject;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.eq;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.expectLastCall;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.reset;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.time.Instant;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.DefaultConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.WithTestNames;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.io.TempDir;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+/**
+ * Ensure that each implementation of AccumuloConfiguration has a working implementation of
+ * isPropertySet()
+ */
+public class AccumuloConfigurationIsPropertySetTest extends WithTestNames {
+
+  private static final Set<Property> ALL_PROPERTIES =
+      Arrays.stream(Property.values()).collect(Collectors.toSet());
+  private static final Logger log =
+      LoggerFactory.getLogger(AccumuloConfigurationIsPropertySetTest.class);
+
+  @TempDir
+  private static File tempDir;
+
+  /**
+   * Test that the provided properties are set or not set using
+   * {@link AccumuloConfiguration#isPropertySet(Property)}
+   *
+   * @param accumuloConfiguration impl to test against
+   * @param expectIsSet set of props that should be set
+   * @param expectNotSet set of props that should not be set
+   */
+  private static void testPropertyIsSetImpl(AccumuloConfiguration accumuloConfiguration,
+      Set<Property> expectIsSet, Set<Property> expectNotSet) {
+    for (Property prop : expectIsSet) {
+      assertTrue(accumuloConfiguration.isPropertySet(prop), ""Expected "" + prop + "" to be set"");
+    }
+    for (Property prop : expectNotSet) {
+      assertFalse(accumuloConfiguration.isPropertySet(prop), ""Expected "" + prop + "" to NOT be set"");
+    }
+  }
+
+  /**
+   * Verifies that the expected properties are in the given config object without using
+   * isPropertySet
+   */
+  private static void verifyProps(AccumuloConfiguration accumuloConfiguration,
+      Set<Property> shouldBeSet, Set<Property> shouldNotBeSet) {
+    Map<String,String> propsMap = new HashMap<>();
+    accumuloConfiguration.getProperties(propsMap, x -> true);
+    Predicate<Property> mapContainsProp = property -> propsMap.containsKey(property.getKey());
+    assertTrue(shouldBeSet.stream().allMatch(mapContainsProp));
+    assertTrue(shouldNotBeSet.stream().noneMatch(mapContainsProp));
+  }
+
+  @Test
+  public void configurationCopy() {
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, GC_PORT);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);","[{'comment': 'I think you have the parameters to `Sets.difference` backwards. This will show `shouldBeSet - ALL_PROPERTIES = EMPTY_SET`.\r\n\r\nYou should probably add asserts to make sure the difference is not empty, to avoid that kind of bug in the test code.', 'commenter': 'ctubbsii'}]"
3676,server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.conf.Property.GC_PORT;
+import static org.apache.accumulo.core.conf.Property.INSTANCE_SECRET;
+import static org.apache.accumulo.core.conf.Property.INSTANCE_ZK_HOST;
+import static org.apache.accumulo.core.conf.Property.MANAGER_BULK_TSERVER_REGEX;
+import static org.apache.accumulo.core.conf.Property.TABLE_BLOOM_ENABLED;
+import static org.apache.accumulo.core.conf.Property.TABLE_BLOOM_SIZE;
+import static org.apache.accumulo.core.conf.Property.TABLE_DURABILITY;
+import static org.apache.accumulo.core.conf.Property.TABLE_FILE_MAX;
+import static org.apache.accumulo.core.conf.Property.TSERV_SCAN_MAX_OPENFILES;
+import static org.apache.accumulo.server.MockServerContext.getMockContextWithPropStore;
+import static org.easymock.EasyMock.anyObject;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.eq;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.expectLastCall;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.reset;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.time.Instant;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.DefaultConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.WithTestNames;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.io.TempDir;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+/**
+ * Ensure that each implementation of AccumuloConfiguration has a working implementation of
+ * isPropertySet()
+ */
+public class AccumuloConfigurationIsPropertySetTest extends WithTestNames {
+
+  private static final Set<Property> ALL_PROPERTIES =
+      Arrays.stream(Property.values()).collect(Collectors.toSet());
+  private static final Logger log =
+      LoggerFactory.getLogger(AccumuloConfigurationIsPropertySetTest.class);
+
+  @TempDir
+  private static File tempDir;
+
+  /**
+   * Test that the provided properties are set or not set using
+   * {@link AccumuloConfiguration#isPropertySet(Property)}
+   *
+   * @param accumuloConfiguration impl to test against
+   * @param expectIsSet set of props that should be set
+   * @param expectNotSet set of props that should not be set
+   */
+  private static void testPropertyIsSetImpl(AccumuloConfiguration accumuloConfiguration,
+      Set<Property> expectIsSet, Set<Property> expectNotSet) {
+    for (Property prop : expectIsSet) {
+      assertTrue(accumuloConfiguration.isPropertySet(prop), ""Expected "" + prop + "" to be set"");
+    }
+    for (Property prop : expectNotSet) {
+      assertFalse(accumuloConfiguration.isPropertySet(prop), ""Expected "" + prop + "" to NOT be set"");
+    }
+  }
+
+  /**
+   * Verifies that the expected properties are in the given config object without using
+   * isPropertySet
+   */
+  private static void verifyProps(AccumuloConfiguration accumuloConfiguration,
+      Set<Property> shouldBeSet, Set<Property> shouldNotBeSet) {
+    Map<String,String> propsMap = new HashMap<>();
+    accumuloConfiguration.getProperties(propsMap, x -> true);
+    Predicate<Property> mapContainsProp = property -> propsMap.containsKey(property.getKey());
+    assertTrue(shouldBeSet.stream().allMatch(mapContainsProp));
+    assertTrue(shouldNotBeSet.stream().noneMatch(mapContainsProp));
+  }
+
+  @Test
+  public void configurationCopy() {
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, GC_PORT);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    // set up object
+    ConfigurationCopy configurationCopy = new ConfigurationCopy();
+    shouldBeSet.forEach(property -> configurationCopy.set(property, ""foo""));
+
+    verifyProps(configurationCopy, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(configurationCopy, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void defaultConfiguration() {
+    Set<Property> shouldBeSet = Collections.emptySet();
+    Set<Property> shouldNotBeSet = new HashSet<>(ALL_PROPERTIES);
+
+    DefaultConfiguration defaultConfiguration = DefaultConfiguration.getInstance();
+
+    // we don't expect isPropertySet to be true for any prop since props cannot be set by the user
+    // for DefaultConfiguration
+    testPropertyIsSetImpl(defaultConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void namespaceConfiguration() {
+    InstanceId iid = InstanceId.of(UUID.randomUUID());
+    ZooPropStore propStore = createMock(ZooPropStore.class);
+    propStore.registerAsListener(anyObject(), anyObject());
+    expectLastCall().anyTimes();
+    ZooReaderWriter zrw = createMock(ZooReaderWriter.class);
+    ServerContext context = getMockContextWithPropStore(iid, zrw, propStore);
+    ConfigurationCopy parent = new ConfigurationCopy(
+        Map.of(TABLE_BLOOM_SIZE.getKey(), TABLE_BLOOM_SIZE.getDefaultValue()));
+    reset(propStore);
+    NamespaceId NSID = NamespaceId.of(""namespace"");
+    var nsPropStoreKey = NamespacePropKey.of(iid, NSID);
+    expect(propStore.get(eq(nsPropStoreKey))).andReturn(new VersionedProperties(123, Instant.now(),
+        Map.of(Property.INSTANCE_SECRET.getKey(), ""sekrit""))).anyTimes();
+    propStore.registerAsListener(eq(nsPropStoreKey), anyObject());
+    expectLastCall().anyTimes();
+    replay(propStore, context);
+
+    NamespaceConfiguration namespaceConfiguration =
+        new NamespaceConfiguration(context, NSID, parent);
+
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, INSTANCE_SECRET);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    verifyProps(namespaceConfiguration, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(namespaceConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @SuppressFBWarnings(value = {""PATH_TRAVERSAL_IN""}, justification = ""path provided by test"")
+  @Test
+  public void siteConfiguration() throws IOException {
+
+    Set<Property> shouldBeSet =
+        Set.of(INSTANCE_ZK_HOST, INSTANCE_SECRET, MANAGER_BULK_TSERVER_REGEX);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);","[{'comment': 'This needs the same change as mentioned elsewhere\r\n\r\n```suggestion\r\n    Set<Property> shouldNotBeSet = Sets.difference(ALL_PROPERTIES, shouldBeSet);\r\n```', 'commenter': 'keith-turner'}]"
3676,server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.conf.Property.GC_PORT;
+import static org.apache.accumulo.core.conf.Property.INSTANCE_SECRET;
+import static org.apache.accumulo.core.conf.Property.INSTANCE_ZK_HOST;
+import static org.apache.accumulo.core.conf.Property.MANAGER_BULK_TSERVER_REGEX;
+import static org.apache.accumulo.core.conf.Property.TABLE_BLOOM_ENABLED;
+import static org.apache.accumulo.core.conf.Property.TABLE_BLOOM_SIZE;
+import static org.apache.accumulo.core.conf.Property.TABLE_DURABILITY;
+import static org.apache.accumulo.core.conf.Property.TABLE_FILE_MAX;
+import static org.apache.accumulo.core.conf.Property.TSERV_SCAN_MAX_OPENFILES;
+import static org.apache.accumulo.server.MockServerContext.getMockContextWithPropStore;
+import static org.easymock.EasyMock.anyObject;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.eq;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.expectLastCall;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.reset;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.time.Instant;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.DefaultConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.WithTestNames;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.io.TempDir;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+/**
+ * Ensure that each implementation of AccumuloConfiguration has a working implementation of
+ * isPropertySet()
+ */
+public class AccumuloConfigurationIsPropertySetTest extends WithTestNames {
+
+  private static final Set<Property> ALL_PROPERTIES =
+      Arrays.stream(Property.values()).collect(Collectors.toSet());
+  private static final Logger log =
+      LoggerFactory.getLogger(AccumuloConfigurationIsPropertySetTest.class);
+
+  @TempDir
+  private static File tempDir;
+
+  /**
+   * Test that the provided properties are set or not set using
+   * {@link AccumuloConfiguration#isPropertySet(Property)}
+   *
+   * @param accumuloConfiguration impl to test against
+   * @param expectIsSet set of props that should be set
+   * @param expectNotSet set of props that should not be set
+   */
+  private static void testPropertyIsSetImpl(AccumuloConfiguration accumuloConfiguration,
+      Set<Property> expectIsSet, Set<Property> expectNotSet) {
+    for (Property prop : expectIsSet) {
+      assertTrue(accumuloConfiguration.isPropertySet(prop), ""Expected "" + prop + "" to be set"");
+    }
+    for (Property prop : expectNotSet) {
+      assertFalse(accumuloConfiguration.isPropertySet(prop), ""Expected "" + prop + "" to NOT be set"");
+    }
+  }
+
+  /**
+   * Verifies that the expected properties are in the given config object without using
+   * isPropertySet
+   */
+  private static void verifyProps(AccumuloConfiguration accumuloConfiguration,
+      Set<Property> shouldBeSet, Set<Property> shouldNotBeSet) {
+    Map<String,String> propsMap = new HashMap<>();
+    accumuloConfiguration.getProperties(propsMap, x -> true);
+    Predicate<Property> mapContainsProp = property -> propsMap.containsKey(property.getKey());
+    assertTrue(shouldBeSet.stream().allMatch(mapContainsProp));
+    assertTrue(shouldNotBeSet.stream().noneMatch(mapContainsProp));
+  }
+
+  @Test
+  public void configurationCopy() {
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, GC_PORT);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    // set up object
+    ConfigurationCopy configurationCopy = new ConfigurationCopy();
+    shouldBeSet.forEach(property -> configurationCopy.set(property, ""foo""));
+
+    verifyProps(configurationCopy, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(configurationCopy, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void defaultConfiguration() {
+    Set<Property> shouldBeSet = Collections.emptySet();
+    Set<Property> shouldNotBeSet = new HashSet<>(ALL_PROPERTIES);
+
+    DefaultConfiguration defaultConfiguration = DefaultConfiguration.getInstance();
+
+    // we don't expect isPropertySet to be true for any prop since props cannot be set by the user
+    // for DefaultConfiguration
+    testPropertyIsSetImpl(defaultConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void namespaceConfiguration() {
+    InstanceId iid = InstanceId.of(UUID.randomUUID());
+    ZooPropStore propStore = createMock(ZooPropStore.class);
+    propStore.registerAsListener(anyObject(), anyObject());
+    expectLastCall().anyTimes();
+    ZooReaderWriter zrw = createMock(ZooReaderWriter.class);
+    ServerContext context = getMockContextWithPropStore(iid, zrw, propStore);
+    ConfigurationCopy parent = new ConfigurationCopy(
+        Map.of(TABLE_BLOOM_SIZE.getKey(), TABLE_BLOOM_SIZE.getDefaultValue()));
+    reset(propStore);
+    NamespaceId NSID = NamespaceId.of(""namespace"");
+    var nsPropStoreKey = NamespacePropKey.of(iid, NSID);
+    expect(propStore.get(eq(nsPropStoreKey))).andReturn(new VersionedProperties(123, Instant.now(),
+        Map.of(Property.INSTANCE_SECRET.getKey(), ""sekrit""))).anyTimes();
+    propStore.registerAsListener(eq(nsPropStoreKey), anyObject());
+    expectLastCall().anyTimes();
+    replay(propStore, context);
+
+    NamespaceConfiguration namespaceConfiguration =
+        new NamespaceConfiguration(context, NSID, parent);
+
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, INSTANCE_SECRET);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    verifyProps(namespaceConfiguration, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(namespaceConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @SuppressFBWarnings(value = {""PATH_TRAVERSAL_IN""}, justification = ""path provided by test"")
+  @Test
+  public void siteConfiguration() throws IOException {
+
+    Set<Property> shouldBeSet =
+        Set.of(INSTANCE_ZK_HOST, INSTANCE_SECRET, MANAGER_BULK_TSERVER_REGEX);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    // create a properties file contents
+    StringBuilder sb = new StringBuilder();
+    for (Property p : shouldBeSet) {
+      sb.append(p.getKey()).append(""=foo\n"");
+    }
+    String propsFileContents = sb.toString();
+
+    // create a new file and write the properties to it
+    File propsDir = new File(tempDir, testName());
+    assertTrue(propsDir.mkdirs());
+    File propsFile = new File(propsDir, ""accumulo2.properties"");
+    assertTrue(propsFile.exists() || propsFile.createNewFile());
+    try (FileWriter writer = new FileWriter(propsFile, UTF_8)) {
+      log.info(""Writing the following properties to {}:\n{}"", propsFile, propsFileContents);
+      writer.write(propsFileContents);
+    }
+
+    // create the object
+    SiteConfiguration siteConfiguration = SiteConfiguration.fromFile(propsFile).build();
+
+    verifyProps(siteConfiguration, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(siteConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void systemConfiguration() {
+    InstanceId instanceId = InstanceId.of(UUID.randomUUID());
+    ServerContext context = createMock(ServerContext.class);
+    expect(context.getInstanceID()).andReturn(instanceId).anyTimes();
+    PropStore propStore = createMock(ZooPropStore.class);
+    expect(context.getPropStore()).andReturn(propStore).anyTimes();
+    SiteConfiguration siteConfig = SiteConfiguration.empty().build();
+    expect(context.getSiteConfiguration()).andReturn(siteConfig).anyTimes();
+    replay(context);
+    propStore.registerAsListener(anyObject(), anyObject());
+    expectLastCall().anyTimes();
+    SystemPropKey sysPropKey = SystemPropKey.of(instanceId);
+    VersionedProperties sysProps = new VersionedProperties(1, Instant.now(),
+        Map.of(GC_PORT.getKey(), ""1234"", TSERV_SCAN_MAX_OPENFILES.getKey(), ""19""));
+    expect(propStore.get(eq(sysPropKey))).andReturn(sysProps).times(2);
+    replay(propStore);
+    ConfigurationCopy defaultConfig = new ConfigurationCopy(
+        Map.of(TABLE_BLOOM_SIZE.getKey(), TABLE_BLOOM_SIZE.getDefaultValue()));
+
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, GC_PORT, TSERV_SCAN_MAX_OPENFILES);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    // create SystemConfiguration object
+    SystemConfiguration systemConfiguration =
+        new SystemConfiguration(context, sysPropKey, defaultConfig);
+
+    verifyProps(systemConfiguration, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(systemConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void tableConfiguration() {
+    InstanceId instanceId = InstanceId.of(UUID.randomUUID());
+    ServerContext context = createMock(ServerContext.class);
+    expect(context.getInstanceID()).andReturn(instanceId).anyTimes();
+    PropStore propStore = createMock(PropStore.class);
+    expect(context.getPropStore()).andReturn(propStore).anyTimes();
+    var siteConfig = SiteConfiguration.empty().build();
+    expect(context.getSiteConfiguration()).andReturn(siteConfig).anyTimes();
+    replay(context); // prop store is read from context.
+    propStore.registerAsListener(anyObject(), anyObject());
+    expectLastCall().anyTimes();
+    var sysPropKey = SystemPropKey.of(instanceId);
+    VersionedProperties sysProps =
+        new VersionedProperties(1, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), ""true""));
+    expect(propStore.get(eq(sysPropKey))).andReturn(sysProps).times(2);
+    NamespaceId NID = NamespaceId.of(""namespace"");
+    var nsPropKey = NamespacePropKey.of(instanceId, NID);
+    VersionedProperties nsProps = new VersionedProperties(2, Instant.now(),
+        Map.of(TABLE_FILE_MAX.getKey(), ""21"", TABLE_BLOOM_ENABLED.getKey(), ""false""));
+    expect(propStore.get(eq(nsPropKey))).andReturn(nsProps).once();
+    var TID = TableId.of(""3"");
+    var tablePropKey = TablePropKey.of(instanceId, TID);
+    VersionedProperties tableProps =
+        new VersionedProperties(3, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), ""true""));
+    expect(propStore.get(eq(tablePropKey))).andReturn(tableProps).once();
+    ConfigurationCopy parentConfig =
+        new ConfigurationCopy(Map.of(TABLE_BLOOM_SIZE.getKey(), TABLE_BLOOM_SIZE.getDefaultValue(),
+            TABLE_DURABILITY.getKey(), TABLE_DURABILITY.getDefaultValue()));
+    replay(propStore);
+
+    NamespaceId nsid = nsPropKey.getId();
+
+    NamespaceConfiguration namespaceConfig =
+        new NamespaceConfiguration(context, nsid, parentConfig);
+
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE);","[{'comment': 'Seems like this set should be the following based on what is set for the parent and table conf object.\r\n\r\n```suggestion\r\n    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, TABLE_DURABILITY, TABLE_BLOOM_ENABLED);\r\n```', 'commenter': 'keith-turner'}]"
3676,server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.server.conf;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.conf.Property.GC_PORT;
+import static org.apache.accumulo.core.conf.Property.INSTANCE_SECRET;
+import static org.apache.accumulo.core.conf.Property.INSTANCE_ZK_HOST;
+import static org.apache.accumulo.core.conf.Property.MANAGER_BULK_TSERVER_REGEX;
+import static org.apache.accumulo.core.conf.Property.TABLE_BLOOM_ENABLED;
+import static org.apache.accumulo.core.conf.Property.TABLE_BLOOM_SIZE;
+import static org.apache.accumulo.core.conf.Property.TABLE_DURABILITY;
+import static org.apache.accumulo.core.conf.Property.TABLE_FILE_MAX;
+import static org.apache.accumulo.core.conf.Property.TSERV_SCAN_MAX_OPENFILES;
+import static org.apache.accumulo.server.MockServerContext.getMockContextWithPropStore;
+import static org.easymock.EasyMock.anyObject;
+import static org.easymock.EasyMock.createMock;
+import static org.easymock.EasyMock.eq;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.expectLastCall;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.reset;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.time.Instant;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import org.apache.accumulo.core.conf.AccumuloConfiguration;
+import org.apache.accumulo.core.conf.ConfigurationCopy;
+import org.apache.accumulo.core.conf.DefaultConfiguration;
+import org.apache.accumulo.core.conf.Property;
+import org.apache.accumulo.core.conf.SiteConfiguration;
+import org.apache.accumulo.core.data.InstanceId;
+import org.apache.accumulo.core.data.NamespaceId;
+import org.apache.accumulo.core.data.TableId;
+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.accumulo.server.WithTestNames;
+import org.apache.accumulo.server.conf.codec.VersionedProperties;
+import org.apache.accumulo.server.conf.store.NamespacePropKey;
+import org.apache.accumulo.server.conf.store.PropStore;
+import org.apache.accumulo.server.conf.store.SystemPropKey;
+import org.apache.accumulo.server.conf.store.TablePropKey;
+import org.apache.accumulo.server.conf.store.impl.ZooPropStore;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.io.TempDir;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+
+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+
+/**
+ * Ensure that each implementation of AccumuloConfiguration has a working implementation of
+ * isPropertySet()
+ */
+public class AccumuloConfigurationIsPropertySetTest extends WithTestNames {
+
+  private static final Set<Property> ALL_PROPERTIES =
+      Arrays.stream(Property.values()).collect(Collectors.toSet());
+  private static final Logger log =
+      LoggerFactory.getLogger(AccumuloConfigurationIsPropertySetTest.class);
+
+  @TempDir
+  private static File tempDir;
+
+  /**
+   * Test that the provided properties are set or not set using
+   * {@link AccumuloConfiguration#isPropertySet(Property)}
+   *
+   * @param accumuloConfiguration impl to test against
+   * @param expectIsSet set of props that should be set
+   * @param expectNotSet set of props that should not be set
+   */
+  private static void testPropertyIsSetImpl(AccumuloConfiguration accumuloConfiguration,
+      Set<Property> expectIsSet, Set<Property> expectNotSet) {
+    for (Property prop : expectIsSet) {
+      assertTrue(accumuloConfiguration.isPropertySet(prop), ""Expected "" + prop + "" to be set"");
+    }
+    for (Property prop : expectNotSet) {
+      assertFalse(accumuloConfiguration.isPropertySet(prop), ""Expected "" + prop + "" to NOT be set"");
+    }
+  }
+
+  /**
+   * Verifies that the expected properties are in the given config object without using
+   * isPropertySet
+   */
+  private static void verifyProps(AccumuloConfiguration accumuloConfiguration,
+      Set<Property> shouldBeSet, Set<Property> shouldNotBeSet) {
+    Map<String,String> propsMap = new HashMap<>();
+    accumuloConfiguration.getProperties(propsMap, x -> true);
+    Predicate<Property> mapContainsProp = property -> propsMap.containsKey(property.getKey());
+    assertTrue(shouldBeSet.stream().allMatch(mapContainsProp));
+    assertTrue(shouldNotBeSet.stream().noneMatch(mapContainsProp));
+  }
+
+  @Test
+  public void configurationCopy() {
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, GC_PORT);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    // set up object
+    ConfigurationCopy configurationCopy = new ConfigurationCopy();
+    shouldBeSet.forEach(property -> configurationCopy.set(property, ""foo""));
+
+    verifyProps(configurationCopy, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(configurationCopy, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void defaultConfiguration() {
+    Set<Property> shouldBeSet = Collections.emptySet();
+    Set<Property> shouldNotBeSet = new HashSet<>(ALL_PROPERTIES);
+
+    DefaultConfiguration defaultConfiguration = DefaultConfiguration.getInstance();
+
+    // we don't expect isPropertySet to be true for any prop since props cannot be set by the user
+    // for DefaultConfiguration
+    testPropertyIsSetImpl(defaultConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void namespaceConfiguration() {
+    InstanceId iid = InstanceId.of(UUID.randomUUID());
+    ZooPropStore propStore = createMock(ZooPropStore.class);
+    propStore.registerAsListener(anyObject(), anyObject());
+    expectLastCall().anyTimes();
+    ZooReaderWriter zrw = createMock(ZooReaderWriter.class);
+    ServerContext context = getMockContextWithPropStore(iid, zrw, propStore);
+    ConfigurationCopy parent = new ConfigurationCopy(
+        Map.of(TABLE_BLOOM_SIZE.getKey(), TABLE_BLOOM_SIZE.getDefaultValue()));
+    reset(propStore);
+    NamespaceId NSID = NamespaceId.of(""namespace"");
+    var nsPropStoreKey = NamespacePropKey.of(iid, NSID);
+    expect(propStore.get(eq(nsPropStoreKey))).andReturn(new VersionedProperties(123, Instant.now(),
+        Map.of(Property.INSTANCE_SECRET.getKey(), ""sekrit""))).anyTimes();
+    propStore.registerAsListener(eq(nsPropStoreKey), anyObject());
+    expectLastCall().anyTimes();
+    replay(propStore, context);
+
+    NamespaceConfiguration namespaceConfiguration =
+        new NamespaceConfiguration(context, NSID, parent);
+
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, INSTANCE_SECRET);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    verifyProps(namespaceConfiguration, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(namespaceConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @SuppressFBWarnings(value = {""PATH_TRAVERSAL_IN""}, justification = ""path provided by test"")
+  @Test
+  public void siteConfiguration() throws IOException {
+
+    Set<Property> shouldBeSet =
+        Set.of(INSTANCE_ZK_HOST, INSTANCE_SECRET, MANAGER_BULK_TSERVER_REGEX);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    // create a properties file contents
+    StringBuilder sb = new StringBuilder();
+    for (Property p : shouldBeSet) {
+      sb.append(p.getKey()).append(""=foo\n"");
+    }
+    String propsFileContents = sb.toString();
+
+    // create a new file and write the properties to it
+    File propsDir = new File(tempDir, testName());
+    assertTrue(propsDir.mkdirs());
+    File propsFile = new File(propsDir, ""accumulo2.properties"");
+    assertTrue(propsFile.exists() || propsFile.createNewFile());
+    try (FileWriter writer = new FileWriter(propsFile, UTF_8)) {
+      log.info(""Writing the following properties to {}:\n{}"", propsFile, propsFileContents);
+      writer.write(propsFileContents);
+    }
+
+    // create the object
+    SiteConfiguration siteConfiguration = SiteConfiguration.fromFile(propsFile).build();
+
+    verifyProps(siteConfiguration, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(siteConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void systemConfiguration() {
+    InstanceId instanceId = InstanceId.of(UUID.randomUUID());
+    ServerContext context = createMock(ServerContext.class);
+    expect(context.getInstanceID()).andReturn(instanceId).anyTimes();
+    PropStore propStore = createMock(ZooPropStore.class);
+    expect(context.getPropStore()).andReturn(propStore).anyTimes();
+    SiteConfiguration siteConfig = SiteConfiguration.empty().build();
+    expect(context.getSiteConfiguration()).andReturn(siteConfig).anyTimes();
+    replay(context);
+    propStore.registerAsListener(anyObject(), anyObject());
+    expectLastCall().anyTimes();
+    SystemPropKey sysPropKey = SystemPropKey.of(instanceId);
+    VersionedProperties sysProps = new VersionedProperties(1, Instant.now(),
+        Map.of(GC_PORT.getKey(), ""1234"", TSERV_SCAN_MAX_OPENFILES.getKey(), ""19""));
+    expect(propStore.get(eq(sysPropKey))).andReturn(sysProps).times(2);
+    replay(propStore);
+    ConfigurationCopy defaultConfig = new ConfigurationCopy(
+        Map.of(TABLE_BLOOM_SIZE.getKey(), TABLE_BLOOM_SIZE.getDefaultValue()));
+
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE, GC_PORT, TSERV_SCAN_MAX_OPENFILES);
+    Set<Property> shouldNotBeSet = Sets.difference(shouldBeSet, ALL_PROPERTIES);
+
+    // create SystemConfiguration object
+    SystemConfiguration systemConfiguration =
+        new SystemConfiguration(context, sysPropKey, defaultConfig);
+
+    verifyProps(systemConfiguration, shouldBeSet, shouldNotBeSet);
+
+    testPropertyIsSetImpl(systemConfiguration, shouldBeSet, shouldNotBeSet);
+  }
+
+  @Test
+  public void tableConfiguration() {
+    InstanceId instanceId = InstanceId.of(UUID.randomUUID());
+    ServerContext context = createMock(ServerContext.class);
+    expect(context.getInstanceID()).andReturn(instanceId).anyTimes();
+    PropStore propStore = createMock(PropStore.class);
+    expect(context.getPropStore()).andReturn(propStore).anyTimes();
+    var siteConfig = SiteConfiguration.empty().build();
+    expect(context.getSiteConfiguration()).andReturn(siteConfig).anyTimes();
+    replay(context); // prop store is read from context.
+    propStore.registerAsListener(anyObject(), anyObject());
+    expectLastCall().anyTimes();
+    var sysPropKey = SystemPropKey.of(instanceId);
+    VersionedProperties sysProps =
+        new VersionedProperties(1, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), ""true""));
+    expect(propStore.get(eq(sysPropKey))).andReturn(sysProps).times(2);
+    NamespaceId NID = NamespaceId.of(""namespace"");
+    var nsPropKey = NamespacePropKey.of(instanceId, NID);
+    VersionedProperties nsProps = new VersionedProperties(2, Instant.now(),
+        Map.of(TABLE_FILE_MAX.getKey(), ""21"", TABLE_BLOOM_ENABLED.getKey(), ""false""));
+    expect(propStore.get(eq(nsPropKey))).andReturn(nsProps).once();
+    var TID = TableId.of(""3"");
+    var tablePropKey = TablePropKey.of(instanceId, TID);
+    VersionedProperties tableProps =
+        new VersionedProperties(3, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), ""true""));
+    expect(propStore.get(eq(tablePropKey))).andReturn(tableProps).once();
+    ConfigurationCopy parentConfig =
+        new ConfigurationCopy(Map.of(TABLE_BLOOM_SIZE.getKey(), TABLE_BLOOM_SIZE.getDefaultValue(),
+            TABLE_DURABILITY.getKey(), TABLE_DURABILITY.getDefaultValue()));
+    replay(propStore);
+
+    NamespaceId nsid = nsPropKey.getId();
+
+    NamespaceConfiguration namespaceConfig =
+        new NamespaceConfiguration(context, nsid, parentConfig);
+
+    Set<Property> shouldBeSet = Set.of(TABLE_BLOOM_SIZE);
+    Set<Property> shouldNotBeSet = Set.of(TSERV_SCAN_MAX_OPENFILES);","[{'comment': 'Could this do the set difference with all props that other test have done?', 'commenter': 'keith-turner'}]"
3678,server/base/src/main/java/org/apache/accumulo/server/util/PropUtil.java,"@@ -65,6 +65,19 @@ protected static void validateProperties(final ServerContext context,
         }
         throw new IllegalArgumentException(exceptionMessage + propStoreKey + "" name: ""
             + prop.getKey() + "", value: "" + prop.getValue());
+      } else if (prop.getKey().equals(Property.TABLE_CLASSLOADER_CONTEXT.getKey())) {
+        ClassLoader cl;
+        try {
+          // If the context cannot be resolved, then this call should throw an error
+          cl = ClassLoaderUtil.getClassLoader(prop.getValue());
+        } catch (RuntimeException re) {","[{'comment': ""I think you're looking for:\r\n\r\n```suggestion\r\n        } catch (ReflectiveOperationException re) {\r\n```\r\n\r\nOr maybe:\r\n\r\n```suggestion\r\n        } catch (ReflectiveOperationException | NullPointerException re) {\r\n```\r\n\r\nOther RTEs could be entirely unrelated.\r\nnit: Also, can we just stick with `e` as the exception name?"", 'commenter': 'ctubbsii'}, {'comment': ""`ClassLoaderUtil.getClassLoader(contextName)` calls `ContextClassLoaderFactory.getClassLoader(contextName)`. The javadoc there says to throw a RuntimeException - see [here](https://github.com/apache/accumulo/blob/2.1/core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java#L60). If I'm able to add the method you suggested above, then this might be moot."", 'commenter': 'dlmarion'}]"
3678,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -82,6 +82,14 @@ public static ClassLoader getClassLoader(String context) {
     }
   }
 
+  public static boolean isValidContext(String context) {
+    if (context != null && !context.isEmpty()) {","[{'comment': ""This can be decided by the factory as well. There's no need to constrain what the factory considers valid."", 'commenter': 'ctubbsii'}, {'comment': ""I don't believe that an empty or null context name is valid. I don't think that a user can set that in the shell. I don't think that it can be set via the API either, but I would have to look to be sure. A null or empty context here could indicate a bug."", 'commenter': 'dlmarion'}, {'comment': 'Setting a table property via the shell or API ends up [here](https://github.com/apache/accumulo/blob/2.1/server/manager/src/main/java/org/apache/accumulo/manager/ManagerClientServiceHandler.java#L558). If `context` is null or empty, then there could be an issue with the Accumulo code.', 'commenter': 'dlmarion'}, {'comment': 'If the context property is not set for a particular table, the factory should still get consulted. The empty string is the default value for `table.class.loader.context`, so it most definitely should be valid. The factory should get passed an empty string if the user hasn\'t overridden it with anything else. It should be up to the factory to handle that according to its own implementation... whether it passes through to the system classloader, or contains a built-in default classloader to return is up to the implementation.\r\n\r\nAs for the part of the code you point to, I\'m not exactly sure what it\'s doing there... I would think that it should be possible to override a value of `""default""` with a value of `""""`, but that code you pointed to seemed to interpret `""""` as `unset()`. I\'m not sure it\'s doing the right thing... but that would be a separate issue. Right now, I\'m just concerned about what values the factory should allow... and I don\'t think we need to constrain it to only see non-empty String values as valid, when an empty String is the default, and should be perfectly valid. I\'m also of the opinion that `null` should be valid as well. For example, if we had a context classloader factory implementation called `AlwaysReturnSystemClassLoader`, then all context values are valid, and would result in returning the system class loader, with the actual value ignored. That trivial implementation seems like it should be allowed, but it won\'t work if constraints about validity are imposed outside the SPI implementation.\r\n', 'commenter': 'ctubbsii'}, {'comment': ""> The empty string is the default value for `table.class.loader.context`, so it most definitely should be valid.\r\n\r\nI'm not sure about that. There are no properties of PropertyType.STRING in Property.java whose default value is `null`. They all either have a value, or an empty string. I don't think that empty string is a valid String property value in all cases. I'm sure that in some cases it means the property is not set.\r\n\r\n> If the context property is not set for a particular table, the factory should still get consulted.\r\n\r\nIt's never worked like this. Even in `main` where the VFS stuff has been ripped out ClassLoaderUtil only consults the ContextClassLoaderFactory when the context name is set to a non-empty String. However, I think I have a solution - I pushed the null and empty checks into the factory in a4750ab."", 'commenter': 'dlmarion'}, {'comment': '> > The empty string is the default value for `table.class.loader.context`, so it most definitely should be valid.\r\n> \r\n> I\'m not sure about that. There are no properties of PropertyType.STRING in Property.java whose default value is `null`. They all either have a value, or an empty string. I don\'t think that empty string is a valid String property value in all cases. I\'m sure that in some cases it means the property is not set.\r\n\r\nI don\'t think we\'ve done a good job in the internal config code of having a way to distinguish between set or not set, or allowing a value at one scope to be overridden by unsetting it at a different scope. This is a shortcoming in the current implementation, but far out of scope to fix here. The fact that `null` isn\'t used is really more of a byproduct of how commons-configuration works (it\'s really hard to set it as a value, because it either looks like an empty string, or looks like no entry). In theory, it could be a value, but in practice, you can\'t really set it. But empty string certainly can be set, and is set as the default in many properties. I don\'t think we have anything enforcing that `null` can\'t be the default value, though. If we\'re going to rely on that, we should probably enforce it.\r\n\r\n> \r\n> > If the context property is not set for a particular table, the factory should still get consulted.\r\n> \r\n> It\'s never worked like this. Even in `main` where the VFS stuff has been ripped out ClassLoaderUtil only consults the ContextClassLoaderFactory when the context name is set to a non-empty String.\r\n\r\nThat is surprising to me. I was thinking it should always be consulted where there is a context possible. But now I\'m not so sure what the right behavior should be, because there are too many edge cases and failure scenarios. Trying to put myself in the shoes of a user who has implemented a custom factory, it\'s getting harder to imagine how they might expect things to behave with respect to their implementation under all these scenarios. I\'m leaning towards always consulting the factory, and leaving the ""return null"" mean ""defer to system classloader"", but I can also see an argument for ""use context classloader factory only if a non-empty context is provided"" (which is the current behavior). I\'m fine with leaving the current behavior while I think about it some more.\r\n\r\n> However, I think I have a solution - I pushed the null and empty checks into the factory in [a4750ab](https://github.com/apache/accumulo/commit/a4750ab12a9694c8b2c0a5590451bdaefa311ed5).\r\n\r\nI think pushing these checks into the factory makes sense, and allowing the factory to return null, which causes Accumulo to use its system class loader. However, it\'s still a little unclear whether contexts that result in returning `null`, and deferring to the system class loader, are ""valid"" or not. Does ""valid"" mean ""I won\'t cause an error"" or does valid mean ""I won\'t return null""?', 'commenter': 'ctubbsii'}, {'comment': '> Does ""valid"" mean ""I won\'t cause an error"" or does valid mean ""I won\'t return null""?\r\n\r\nI think it should only return `true` if the ContextClassLoaderFactory implementation can return a ClassLoader for the context name. Currently what is happening is that an exception is being thrown because the context name is not known to be valid.', 'commenter': 'dlmarion'}]"
3678,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -82,6 +82,14 @@ public static ClassLoader getClassLoader(String context) {
     }
   }
 
+  public static boolean isValidContext(String context) {
+    if (context != null && !context.isEmpty()) {
+      return FACTORY.isValid(context);","[{'comment': 'I think this is the only line needed here.', 'commenter': 'ctubbsii'}, {'comment': 'This is OBE, the context value is no longer checked in this method. It has been pushed into the factory.', 'commenter': 'dlmarion'}]"
3678,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -65,4 +65,15 @@ default void init(ContextClassLoaderEnvironment env) {}
    * @return the class loader for the given contextName
    */
   ClassLoader getClassLoader(String contextName);
+
+  /**
+   * Validate that the contextName is supported by the ContextClassLoaderFactory implementation
+   *
+   * @param contextName the name of the context that represents a class loader that is managed by
+   *        this factory (can be null)
+   * @return true if valid, false otherwise
+   */
+  default boolean isValid(String contextName) {
+    return false;","[{'comment': 'Should the default be true to maintain existing behavior, or is existing behavior so harmful as to warrant making all contexts invalid by default?', 'commenter': 'ctubbsii'}, {'comment': ""Based on my analysis in #3677 , an invalid context name could prevent any classes from being loaded, which could lead to the minor compaction thread for a tablet to die. From what I found, a minor compaction for a tablet will not occur again if the minor compaction thread dies, meaning that it can't be closed normally. I think the safest action when this occurs is a total restart of Accumulo.\r\n\r\nThe changes in #3677 try to prevent the minor compaction thread from dying, but there could be other areas within Accumulo where something similar happens when an invalid context name is used. Given that, and that the recovery for this situation is a total restart, I would suggest that `true` not be the default answer. In fact, I don't think this should be a default method, I think we should force all implementations to provide an answer."", 'commenter': 'dlmarion'}, {'comment': ""I see your point about making it stricter for safety... but making this default to `false` or making it a mandatory method would break things in a patch release on an LTM branch... and I don't think the benefits you describe outweigh the downsides of that breakage."", 'commenter': 'ctubbsii'}, {'comment': ""I'm fine with bumping the minor version as part of this patch."", 'commenter': 'dlmarion'}, {'comment': 'That would be a non-LTM release, not part of the LTM release we are supporting as stable, though.', 'commenter': 'ctubbsii'}, {'comment': 'So, we can\'t change the ""current"" LTM release?', 'commenter': 'dlmarion'}, {'comment': ""No, that wouldn't make sense to do. The 2.1 version was marked LTM, specifically because 2.1 is the version we wanted to communicate that we would invest our time patching to keep stable. That's what it means to be an LTM version. LTM is an attribute of a specific major.minor release series. If LTM follows us and moves across major/minor releases, then the whole LTM concept makes no sense and stops being useful to communicate that 2.1 is the version we're invested in."", 'commenter': 'ctubbsii'}, {'comment': ""This is moot if we follow @keith-turner 's suggestion to use `getClassLoader(contextName)` as the default implementation, which I think is a good idea. If this returns something not null without throwing an exception, then this method should return true. If an implementation needs to avoid constructing the classloaders for some reason, then it can override the default method."", 'commenter': 'ctubbsii'}]"
3678,server/base/src/main/java/org/apache/accumulo/server/util/PropUtil.java,"@@ -65,6 +65,11 @@ protected static void validateProperties(final ServerContext context,
         }
         throw new IllegalArgumentException(exceptionMessage + propStoreKey + "" name: ""
             + prop.getKey() + "", value: "" + prop.getValue());
+      } else if (prop.getKey().equals(Property.TABLE_CLASSLOADER_CONTEXT.getKey())) {
+        if (!ClassLoaderUtil.isValidContext(prop.getValue())) {","[{'comment': 'The else block can be removed and the indent promoted up one level to simplify this. Also the two if conditions can be combined with `&&`', 'commenter': 'ctubbsii'}, {'comment': 'Completed in 9272c0f', 'commenter': 'dlmarion'}]"
3678,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -75,11 +75,28 @@ static synchronized void resetContextFactoryForTests() {
 
   @SuppressWarnings(""deprecation"")
   public static ClassLoader getClassLoader(String context) {
-    if (context != null && !context.isEmpty()) {
-      return FACTORY.getClassLoader(context);
-    } else {
+    if (FACTORY == null) {
       return org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.getClassLoader();
     }","[{'comment': ""FACTORY isn't supposed to be null here when this is called, or below where a similar check is made. I'm not sure I understand this part of these changes.\r\n\r\nIf these methods can be called prior to the FACTORY being instantiated, then the FACTORY instantiation should be done lazily with a `Suppliers.memoize()`."", 'commenter': 'ctubbsii'}, {'comment': ""I added this because I noticed that referencing FACTORY without calling `initContextFactory` would cause a NPE.  Looking at where this is called, I think the probability of it happening is low. I'm going to back this change out. "", 'commenter': 'dlmarion'}, {'comment': 'Removed null checks in in 9272c0f', 'commenter': 'dlmarion'}]"
3678,core/src/main/java/org/apache/accumulo/core/classloader/ClassLoaderUtil.java,"@@ -75,11 +75,28 @@ static synchronized void resetContextFactoryForTests() {
 
   @SuppressWarnings(""deprecation"")
   public static ClassLoader getClassLoader(String context) {
-    if (context != null && !context.isEmpty()) {
-      return FACTORY.getClassLoader(context);
-    } else {
+    if (FACTORY == null) {
       return org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.getClassLoader();
     }
+    ClassLoader cl;
+    try {
+      cl = FACTORY.getClassLoader(context);
+      return (cl == null)","[{'comment': 'I do like the idea that the factory can return null, which just means, defer to the system classloader. That behavior should be added to the javadoc, though.', 'commenter': 'ctubbsii'}, {'comment': ""I'm going to modify this. The javadoc already says that `ContextClassLoaderFactory.getClassLoader` should throw a RuntimeException."", 'commenter': 'dlmarion'}, {'comment': 'Modified this in in 9272c0f such that a null classloader return variable is no longer checked in accordance with the existing javadoc. The method should either return a classloader or throw a RTE.', 'commenter': 'dlmarion'}]"
3678,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -65,4 +65,15 @@ default void init(ContextClassLoaderEnvironment env) {}
    * @return the class loader for the given contextName
    */
   ClassLoader getClassLoader(String contextName);
+
+  /**
+   * Validate that the contextName is supported by the ContextClassLoaderFactory implementation
+   *
+   * @param contextName the name of the context that represents a class loader that is managed by
+   *        this factory (can be null)
+   * @return true if valid, false otherwise
+   */
+  default boolean isValid(String contextName) {
+    return false;
+  }","[{'comment': 'Had a discussion with @ctubbsii and @EdColeman about this PR.  This new methods default impl could call the existing getClassLoader() method.  This gives the opportunity to override if calling that for validation is problematic, but does not require it.\r\n\r\n```suggestion\r\n  default boolean isValid(String contextName) {\r\n     try{\r\n        var loader = getClassLoader(contextName);\r\n        if(loader == null){\r\n           log.debug(""Classloader context name {} is not valid, a null class loader was returned"", contextName);\r\n           return false;\r\n        }\r\n        return true;\r\n     } catch(RuntimeException e){\r\n        // TODO what log level should this be?\r\n        log.debug(""Classloader context name {} is not valid"", contextName, e);\r\n        return false;\r\n     }\r\n    \r\n  }\r\n```', 'commenter': 'keith-turner'}, {'comment': '@keith-turner - Thanks for the suggestion, I think it satisfies my concerns.', 'commenter': 'dlmarion'}]"
3678,core/src/main/java/org/apache/accumulo/core/spi/common/ContextClassLoaderFactory.java,"@@ -65,4 +65,15 @@ default void init(ContextClassLoaderEnvironment env) {}
    * @return the class loader for the given contextName
    */
   ClassLoader getClassLoader(String contextName);
+
+  /**
+   * Validate that the contextName is supported by the ContextClassLoaderFactory implementation
+   *
+   * @param contextName the name of the context that represents a class loader that is managed by
+   *        this factory (can be null)
+   * @return true if valid, false otherwise
+   */","[{'comment': 'Needs a since tag.\r\n\r\n```suggestion\r\n   * @since 2.1.2\r\n   */\r\n```', 'commenter': 'keith-turner'}]"
3680,shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java,"@@ -436,4 +449,18 @@ public Options getOptions() {
   public int numArgs() {
     return 0;
   }
+
+  /**
+   * Determine is force is set as an option or user enters (y | yes) on the shell prompt.
+   *
+   * @return true if force is set as opt or y | yes entered at command line.
+   */
+  private boolean forceSet(final Shell shellState, final CommandLine cl, final String prompt) {
+    if (cl.hasOption(forceOpt)) {
+      return true;
+    }
+    shellState.getWriter().flush();
+    String line = shellState.getReader().readLine(prompt + "" (yes|no)? "");
+    return line != null && (line.equalsIgnoreCase(""y"") || line.equalsIgnoreCase(""yes""));","[{'comment': ""This prompt feels very custom. I suspect there are other places in the shell where we have a similar prompt. It'd be good to maintain consistency across the shell for yes/no prompts. I suggest putting this in the Shell class, or moving any existing similar method into the Shell class, and calling it there, as in:\r\n\r\n```java\r\n    if (cl.hasOption(forceOpt) || shellState.yesNo(prompt)) {\r\n      // ...\r\n    }\r\n```\r\n"", 'commenter': 'ctubbsii'}, {'comment': 'I would prefer to do that as a separate PR. The code was copied / modified table operation.  One reason that code could not be used directly is that table operations use ""-f"" and ""--force"" as the command option.  For the config command ""-f"" is filter.\r\n\r\nI will do this as a follow-on,\r\n\r\n', 'commenter': 'EdColeman'}, {'comment': 'Addressed with PR #3684 ', 'commenter': 'EdColeman'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/Shell.java,"@@ -1263,4 +1263,20 @@ public boolean hasExited() {
     return exit;
   }
 
+  /**
+   * Prompt user for yes/no using the shell prompt.
+   *
+   * @param prompt the string printed to user, with (yes|no)? appended as the prompt.
+   * @return true if user enters y | yes.
+   */
+  public boolean yorn(final String prompt) {","[{'comment': '""yorn"" is a weird name. ""yesno"" is more common for this sort of thing. Here\'s [an example in Lua](https://dev.fandom.com/wiki/Global_Lua_Modules/Yesno). ""confirm"" is also a good option.', 'commenter': 'ctubbsii'}, {'comment': 'Switched to `confirm` as `yorn` is also very similar to `yarn`', 'commenter': 'ddanielr'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java,"@@ -117,7 +118,8 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       }
     } else if (cl.hasOption(setOpt.getOpt())) {
       // set property on table
-      String property = cl.getOptionValue(setOpt.getOpt()), value = null;
+      String property = cl.getOptionValue(setOpt.getOpt());
+      String value;","[{'comment': 'Thank you for that!\r\nWe should sweep through our code in a separate PR and fix all those multi-assignments (except the ones in try-with-resources or for loop initialization, of course).', 'commenter': 'ctubbsii'}, {'comment': 'By fix do you mean replace with multiple assignments, like in this case?', 'commenter': 'DomGarguilo'}, {'comment': '> By fix do you mean replace with multiple assignments, like in this case?\r\n\r\nYes, where it makes sense to do it, anyway.', 'commenter': 'ctubbsii'}, {'comment': 'Created #3689 to do as follow-on.', 'commenter': 'ctubbsii'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java,"@@ -129,8 +131,7 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       // check for deprecation
       var theProp = Property.getPropertyByKey(property);
       if (theProp != null && theProp.isDeprecated()) {
-        if (!forceSet(shellState, cl,
-            ""Trying to set deprecated property `"" + property + ""` continue"")) {
+        if (!shellState.yorn(""Trying to set deprecated property `"" + property + ""` continue"")) {","[{'comment': ""The old method handled the force option. This change only does the prompt. You've got to do something like: `!isForceSet() && !shellstate.yorn(...)`. You could keep the existing `forceSet()` method, but change it's behavior to let the prompt happen on `shellState` instead of inside that method."", 'commenter': 'ctubbsii'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/commands/DeleteNamespaceCommand.java,"@@ -38,17 +38,14 @@ public class DeleteNamespaceCommand extends Command {
   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)
       throws Exception {
     boolean force = false;
-    boolean operate = true;
     if (cl.hasOption(forceOpt.getOpt())) {
       force = true;
     }
     String namespace = cl.getArgs()[0];
 
+    boolean operate = true;
     if (!force) {
-      shellState.getWriter().flush();
-      String line =
-          shellState.getReader().readLine(getName() + "" { "" + namespace + "" } (yes|no)? "");
-      operate = line != null && (line.equalsIgnoreCase(""y"") || line.equalsIgnoreCase(""yes""));
+      operate = shellState.yorn(getName() + "" { "" + namespace + "" }"");
     }
     if (operate) {","[{'comment': 'This can be simplified to `if (force || shellState.yorn(...)) {`', 'commenter': 'ctubbsii'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/commands/DropUserCommand.java,"@@ -51,9 +51,7 @@ private void doDropUser(final Shell shellState, final String user, final boolean
     boolean operate = true;
 
     if (!force) {
-      shellState.getWriter().flush();
-      String line = shellState.getReader().readLine(getName() + "" { "" + user + "" } (yes|no)? "");
-      operate = line != null && (line.equalsIgnoreCase(""y"") || line.equalsIgnoreCase(""yes""));
+      operate = shellState.yorn(getName() + "" { "" + user + "" }"");
     }
     if (operate) {","[{'comment': 'Same opportunity to simplify the expression without the need for the intermediate `operate` variable.', 'commenter': 'ctubbsii'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/commands/MergeCommand.java,"@@ -53,13 +53,7 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       size = ConfigurationTypeHelper.getFixedMemoryAsBytes(cl.getOptionValue(sizeOpt.getOpt()));
     }
     if (startRow == null && endRow == null && size < 0 && !all) {
-      shellState.getWriter().flush();
-      String line = shellState.getReader()
-          .readLine(""Merge the entire table { "" + tableName + "" } into one tablet (yes|no)? "");
-      if (line == null) {
-        return 0;
-      }
-      if (!line.equalsIgnoreCase(""y"") && !line.equalsIgnoreCase(""yes"")) {
+      if (!shellState.yorn(""Merge the entire table { "" + tableName + "" } into one tablet"")) {","[{'comment': 'Not related to your PR, but until no-chop merges are complete, I wonder if this prompt should convey a bit more panic, as in:\r\n\r\n`""Are you *REALLY* sure you want to merge the entire table { "" + tableName + "" } into one tablet?!?!?!""`', 'commenter': 'ctubbsii'}, {'comment': 'Updated the prompt wording.', 'commenter': 'ddanielr'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java,"@@ -78,6 +79,8 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       NamespaceNotFoundException {
     reader = shellState.getReader();
 
+    Boolean force = cl.hasOption(forceOpt);","[{'comment': 'Could this be the primitive `boolean` instead of `Boolean`?', 'commenter': 'DomGarguilo'}, {'comment': 'I agree it should be:\r\n\r\n```suggestion\r\n    boolean force = cl.hasOption(forceOpt);\r\n```', 'commenter': 'ctubbsii'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/commands/MergeCommand.java,"@@ -53,13 +53,9 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       size = ConfigurationTypeHelper.getFixedMemoryAsBytes(cl.getOptionValue(sizeOpt.getOpt()));
     }
     if (startRow == null && endRow == null && size < 0 && !all) {
-      shellState.getWriter().flush();
-      String line = shellState.getReader()
-          .readLine(""Merge the entire table { "" + tableName + "" } into one tablet (yes|no)? "");
-      if (line == null) {
-        return 0;
-      }
-      if (!line.equalsIgnoreCase(""y"") && !line.equalsIgnoreCase(""yes"")) {
+      if (!shellState","[{'comment': 'Do we not want `force` to take affect here for this command? It would be a change in behavior if added, but just curious.', 'commenter': 'DomGarguilo'}, {'comment': ""This is actually a bit confusing, because the force flag was only here to ensure that we would force merging when the merged tablets would trigger a subsequent split. The value of the force flag is actually passed to the underlying implementation, rather than used here. There isn't a force option to bypass this prompt, as this prompt was added later, based on bad user experiences merging an entire table.\r\n\r\nI think maybe this should be left alone for now, and not allow the force flag to bypass this question, because it's too important to ensure the user really wants to trigger the chop compactions that come with merging an entire table. No automation or force flag should allow that to happen... a decision to merge an entire table really does need to have user interaction. If somebody really wants to do it without a prompt, they can automate it in JShell or compile some Java code to use the API directly.\r\n\r\nThis prompt will be less important once merging an entire table isn't going to trigger a bunch of chop compactions, when the no-chop merge feature is complete."", 'commenter': 'ctubbsii'}]"
3684,shell/src/main/java/org/apache/accumulo/shell/commands/TableOperation.java,"@@ -73,24 +73,12 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       Shell.log.warn(""No tables found that match your criteria"");
     }
 
-    boolean more = true;
-    // flush the tables
+    // do op if forced or user answers prompt with yes
     for (String tableName : tableSet) {
-      if (!more) {
-        break;
-      }","[{'comment': 'The removal of this logic is a slight change in behavior. That\'s not necessarily a bad thing, but in this case, I think it\'s worth preserving the current behavior. The previous code would interpret a EOF (triggered by Ctrl-C, or perhaps Ctrl-D ? to abort the prompt) as a ""stop asking for the rest"" instruction. With the behavior in this PR, the confirm method will return false, and continue asking about the rest of the tables.\r\n\r\nTo solve this, instead of having the confirm method return a boolean, it could return an `Optional<Boolean>`, with not present generally meaning ""no response"" or ""abort the action that triggered the question"", which could be used to support this case, or other similar cases.', 'commenter': 'ctubbsii'}]"
3706,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -387,17 +387,25 @@ public short getDefaultReplication(Path path) {
   private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
       final Configuration hadoopConf, final String filesystemURI) {
 
-    final Configuration volumeConfig = new Configuration(hadoopConf);
-
-    conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
-        .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {
-          String key = e.getKey().substring(filesystemURI.length() + 1);
-          String value = e.getValue();
-          log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);
-          volumeConfig.set(key, value);
-        });
-
-    return volumeConfig;
+    Map<String,String> volumeHdfsConfigOverrides =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX);
+
+    // Calling new Configuration(Configuration) will synchronize on the constructor parameter
+    // and can cause issues if many threads call this method concurrently.
+    if (!volumeHdfsConfigOverrides.isEmpty()) {
+      final Configuration volumeConfig = new Configuration(hadoopConf);
+
+      volumeHdfsConfigOverrides.entrySet().stream()
+          .filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {","[{'comment': 'Would it be worthwhile to avoid creating the hadoop config copy in the case where filter on the stream drops everything?', 'commenter': 'keith-turner'}, {'comment': ""Yeah, so I'm trying to add that right now and running into compilation issues. It's turning the resulting map into Map<Object,Object> vs Map<String,String>. Do you have something that works?"", 'commenter': 'dlmarion'}, {'comment': '```\r\n    Map<String,String> volumeHdfsConfigOverrides = \r\n        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()\r\n            .stream().filter(e -> e.getKey().startsWith(filesystemURI + "".""))\r\n            .collect(Collectors.toUnmodifiableMap(Function.identity(), Function.identity()));\r\n```\r\n\r\ndoesn\'t compile.', 'commenter': 'dlmarion'}, {'comment': 'I got it:\r\n\r\n```\r\n    Map<String,String> volumeHdfsConfigOverrides = \r\n        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()\r\n            .stream().filter(e -> e.getKey().startsWith(filesystemURI + "".""))\r\n            .collect(Collectors.toUnmodifiableMap(e -> e.getKey(), e -> e.getValue()));\r\n```', 'commenter': 'dlmarion'}, {'comment': 'Could do something like\r\n\r\n```java\r\n    List<Entry<String, String>> matching = conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()\r\n            .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).collect(Collectors.toList());\r\n    \r\n    if(!matching.isEmpty()) {\r\n      Configuration volumeConfig = new Configuration(hadoopConf);\r\n      matching.forEach(e->{\r\n        String key = e.getKey().substring(filesystemURI.length() + 1);\r\n        String value = e.getValue();\r\n        log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);\r\n        volumeConfig.set(key, value);\r\n      });\r\n    }\r\n```', 'commenter': 'keith-turner'}, {'comment': 'This works: \r\n\r\n> I got it:\r\n> \r\n> ```\r\n>     Map<String,String> volumeHdfsConfigOverrides = \r\n>         conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()\r\n>             .stream().filter(e -> e.getKey().startsWith(filesystemURI + "".""))\r\n>             .collect(Collectors.toUnmodifiableMap(e -> e.getKey(), e -> e.getValue()));\r\n> ```\r\n\r\nI was just about to post it but a little different:\r\n\r\n```\r\n    Map<String,String> volumeHdfsConfigOverrides =\r\n            conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()\r\n                    .stream().filter(e -> e.getKey().startsWith(filesystemURI + "".""))\r\n                    .collect(Collectors.toUnmodifiableMap(Entry::getKey, Entry::getValue));\r\n```', 'commenter': 'cshannon'}]"
3706,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -387,17 +387,25 @@ public short getDefaultReplication(Path path) {
   private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
       final Configuration hadoopConf, final String filesystemURI) {
 
-    final Configuration volumeConfig = new Configuration(hadoopConf);
-
-    conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
-        .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {
-          String key = e.getKey().substring(filesystemURI.length() + 1);
-          String value = e.getValue();
-          log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);
-          volumeConfig.set(key, value);
-        });
-
-    return volumeConfig;
+    Map<String,String> volumeHdfsConfigOverrides =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX);
+
+    // Calling new Configuration(Configuration) will synchronize on the constructor parameter
+    // and can cause issues if many threads call this method concurrently.
+    if (!volumeHdfsConfigOverrides.isEmpty()) {
+      final Configuration volumeConfig = new Configuration(hadoopConf);
+
+      volumeHdfsConfigOverrides.entrySet().stream()
+          .filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {
+            String key = e.getKey().substring(filesystemURI.length() + 1);
+            String value = e.getValue();
+            log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);","[{'comment': 'This seems to be missing a {}', 'commenter': 'matthpeterson'}, {'comment': 'Yep, noticed and fixed in latest commit.', 'commenter': 'dlmarion'}]"
3706,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -388,20 +389,22 @@ private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration
       final Configuration hadoopConf, final String filesystemURI) {
 
     Map<String,String> volumeHdfsConfigOverrides =
-        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX);
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
+            .stream().filter(e -> e.getKey().startsWith(filesystemURI + "".""))
+            .collect(Collectors.toUnmodifiableMap(Entry::getKey, Entry::getValue));
 
     // Calling new Configuration(Configuration) will synchronize on the constructor parameter
     // and can cause issues if many threads call this method concurrently.
     if (!volumeHdfsConfigOverrides.isEmpty()) {
+
       final Configuration volumeConfig = new Configuration(hadoopConf);
 
-      volumeHdfsConfigOverrides.entrySet().stream()
-          .filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {
-            String key = e.getKey().substring(filesystemURI.length() + 1);
-            String value = e.getValue();
-            log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);
-            volumeConfig.set(key, value);
-          });
+      volumeHdfsConfigOverrides.entrySet().forEach(e -> {","[{'comment': 'You don\'t have to make this change but I noticed this could be simplified slightly with something like:\r\n\r\n```\r\nvolumeHdfsConfigOverrides.forEach((k, v) -> {\r\n    String key = k.substring(filesystemURI.length() + 1);\r\n    log.info(""Overriding property {}={} for volume {}"", key, v, filesystemURI);\r\n    volumeConfig.set(key, v);\r\n});\r\n```\r\n', 'commenter': 'cshannon'}, {'comment': ""Other than this small nit (which you can ignore if you want as I said) the PR looks good to me. It would be nice if we could write a test for this specific `getVolumeManagerConfiguration()` method but it's private. However, it looks like the  testConfigurationOverrides() test inside of the VolumeManagerImplTest class provides coverage on the prefix usage already so I think it's ok."", 'commenter': 'cshannon'}, {'comment': 'I made your suggested change, and went ahead and moved the substring call up too in cfb3047. Yes, that test should be sufficient I think.', 'commenter': 'dlmarion'}]"
3706,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -384,20 +411,61 @@ public short getDefaultReplication(Path path) {
    * @param filesystemURI Volume Filesystem URI
    * @return Hadoop Configuration with custom overrides for this FileSystem
    */
-  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+  protected static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
       final Configuration hadoopConf, final String filesystemURI) {
 
-    final Configuration volumeConfig = new Configuration(hadoopConf);
-
-    conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
-        .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {
-          String key = e.getKey().substring(filesystemURI.length() + 1);
-          String value = e.getValue();
-          log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);
-          volumeConfig.set(key, value);
-        });
+    Map<String,String> volumeHdfsConfigOverrides =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
+            .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).collect(
+                Collectors.toUnmodifiableMap(e -> e.getKey().substring(filesystemURI.length() + 1),
+                    Entry::getValue));
 
-    return volumeConfig;
+    if (volumeHdfsConfigOverrides.isEmpty()) {
+      // There are no overrides in the AccumuloConfiguration for this volume, return the Hadoop
+      // Configuration input parameter.
+      return hadoopConf;
+    } else {
+      while (true) {
+
+        // Get the ConfigurationOverride object from the Cache. If no object exists, then create
+        // one - it's comprised of the volume overrides map from above and a Hadoop Configuration
+        // object with the overrides applied.
+        final AtomicBoolean createdNew = new AtomicBoolean(false);
+        ConfigurationOverride cachedVolumeConfig =
+            HDFS_CONFIGS_FOR_VOLUME.get(filesystemURI, (fs) -> {
+              createdNew.set(true);
+              log.debug(""Caching a new configuration for volume: {} with overrides: {}"",
+                  filesystemURI, volumeHdfsConfigOverrides);
+              Configuration volumeConfig = new Configuration(hadoopConf);
+              volumeHdfsConfigOverrides.forEach((k, v) -> {
+                log.info(""Overriding property {}={} for volume {}"", k, v, filesystemURI);
+                volumeConfig.set(k, v);
+              });
+              return new ConfigurationOverride(volumeConfig, volumeHdfsConfigOverrides);
+            });
+
+        if (createdNew.get()) {
+          // return the Hadoop Configuration that we just created with the overrides for this volume
+          return cachedVolumeConfig.getHadoopConfigWithOverrides();
+        } else {
+          // We are using an older version of the Configuration, ensure that the instance volume
+          // overrides did not change. If they did, invalidate the cache and try again.
+          if (volumeHdfsConfigOverrides.equals(cachedVolumeConfig.getVolumeOverrides())) {
+            // return the cached config, properties are the same
+            log.debug(
+                ""Volume override properties are the same for volume: {}, returning cached configuration"",
+                filesystemURI);
+            return cachedVolumeConfig.getHadoopConfigWithOverrides();
+          } else {
+            log.debug(
+                ""Cached volume override properties are not the same for volume: {}, invalidating cache and retrying"",
+                filesystemURI);
+            HDFS_CONFIGS_FOR_VOLUME.invalidate(filesystemURI);
+            continue;","[{'comment': 'I think the `continue` can be deleted.', 'commenter': 'cshannon'}]"
3706,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -384,20 +411,61 @@ public short getDefaultReplication(Path path) {
    * @param filesystemURI Volume Filesystem URI
    * @return Hadoop Configuration with custom overrides for this FileSystem
    */
-  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+  protected static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
       final Configuration hadoopConf, final String filesystemURI) {
 
-    final Configuration volumeConfig = new Configuration(hadoopConf);
-
-    conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
-        .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {
-          String key = e.getKey().substring(filesystemURI.length() + 1);
-          String value = e.getValue();
-          log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);
-          volumeConfig.set(key, value);
-        });
+    Map<String,String> volumeHdfsConfigOverrides =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
+            .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).collect(
+                Collectors.toUnmodifiableMap(e -> e.getKey().substring(filesystemURI.length() + 1),
+                    Entry::getValue));
 
-    return volumeConfig;
+    if (volumeHdfsConfigOverrides.isEmpty()) {
+      // There are no overrides in the AccumuloConfiguration for this volume, return the Hadoop
+      // Configuration input parameter.
+      return hadoopConf;
+    } else {
+      while (true) {
+
+        // Get the ConfigurationOverride object from the Cache. If no object exists, then create
+        // one - it's comprised of the volume overrides map from above and a Hadoop Configuration
+        // object with the overrides applied.
+        final AtomicBoolean createdNew = new AtomicBoolean(false);
+        ConfigurationOverride cachedVolumeConfig =
+            HDFS_CONFIGS_FOR_VOLUME.get(filesystemURI, (fs) -> {
+              createdNew.set(true);
+              log.debug(""Caching a new configuration for volume: {} with overrides: {}"",
+                  filesystemURI, volumeHdfsConfigOverrides);
+              Configuration volumeConfig = new Configuration(hadoopConf);","[{'comment': 'This caching something derived from a method parameter, the hadoopConfig. Need to account for different hadoop configs somehow in the cache key or refactor the code to not pass in a hadoopConfig.', 'commenter': 'keith-turner'}]"
3706,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -384,20 +390,34 @@ public short getDefaultReplication(Path path) {
    * @param filesystemURI Volume Filesystem URI
    * @return Hadoop Configuration with custom overrides for this FileSystem
    */
-  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+  protected static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
       final Configuration hadoopConf, final String filesystemURI) {
 
-    final Configuration volumeConfig = new Configuration(hadoopConf);
+    Map<String,String> volumeHdfsConfigOverrides =
+        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
+            .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).collect(
+                Collectors.toUnmodifiableMap(e -> e.getKey().substring(filesystemURI.length() + 1),
+                    Entry::getValue));
 
-    conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
-        .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).forEach(e -> {
-          String key = e.getKey().substring(filesystemURI.length() + 1);
-          String value = e.getValue();
-          log.info(""Overriding property {} for volume {}"", key, value, filesystemURI);
-          volumeConfig.set(key, value);
+    if (volumeHdfsConfigOverrides.isEmpty()) {
+      // There are no overrides in the AccumuloConfiguration for this volume, return the Hadoop
+      // Configuration input parameter.
+      return hadoopConf;
+    } else {
+      // Get the Hadoop Configuration object from the Cache for this filesystemURI. If no object
+      // exists, then create one, apply the overrides and put it in the cache. The Volume config
+      // overrides cannot change, so we only need to cache this once
+      return HDFS_CONFIGS_FOR_VOLUME.get(filesystemURI, (fs) -> {","[{'comment': 'This is a bit hacky, but it seems like it could work to support different hadoopConfig instances being passed in.  When the exact same object is passed in should use the cached entry.\r\n\r\n```suggestion\r\n    /**\r\n     * Hadoop does not implement hashCode for the configuration class, so the following gets the\r\n     *  objects hashcode.  The concept is to make the cached object have 1:1 correspondence with\r\n     *  the hadoop config object from which it was derived.\r\n     */\r\n     String key = filesystemURI+hadoopConf.hashCode();\r\n\r\n      return HDFS_CONFIGS_FOR_VOLUME.get(key, (fs) -> {\r\n```', 'commenter': 'keith-turner'}, {'comment': ""I think there is only one Hadoop configuration object in use. I didn't track this down though and instead just implemented your suggestion in 1b2fb52."", 'commenter': 'dlmarion'}]"
3706,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -384,20 +390,36 @@ public short getDefaultReplication(Path path) {
    * @param filesystemURI Volume Filesystem URI
    * @return Hadoop Configuration with custom overrides for this FileSystem
    */
-  private static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
+  protected static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
       final Configuration hadoopConf, final String filesystemURI) {
 
-    final Configuration volumeConfig = new Configuration(hadoopConf);
+    Map<String,String> volumeHdfsConfigOverrides =","[{'comment': 'Is it ok to do the following and avoid the computation of volumeHdfsConfigOverrides on each call?\r\n\r\n```java\r\n    final String key = hadoopConf.hashCode() + filesystemURI;\r\n    return HDFS_CONFIGS_FOR_VOLUME.get(key, (fs) -> {\r\n\r\n      Map<String,String> volumeHdfsConfigOverrides =\r\n              conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()\r\n                      .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).collect(\r\n                              Collectors.toUnmodifiableMap(e -> e.getKey().substring(filesystemURI.length() + 1),\r\n                                      Entry::getValue));\r\n      if (volumeHdfsConfigOverrides.isEmpty()) {\r\n        return hadoopConf;\r\n      }else {\r\n        log.debug(""Caching a new configuration for volume: {} with overrides: {}"", filesystemURI,\r\n                volumeHdfsConfigOverrides);\r\n        Configuration volumeConfig = new Configuration(hadoopConf);\r\n        volumeHdfsConfigOverrides.forEach((k, v) -> {\r\n          log.info(""Overriding property {}={} for volume {}"", k, v, filesystemURI);\r\n          volumeConfig.set(k, v);\r\n        });\r\n        return new Configuration(volumeConfig);\r\n      }\r\n    });\r\n```', 'commenter': 'keith-turner'}, {'comment': 'My only concern with putting more into the `Cache.get` call is this in the javadoc:\r\n```\r\n   * Some attempted update operations on this cache by other threads may be blocked while the\r\n   * computation is in progress, so the computation should be short and simple, and must not attempt\r\n   * to update any other mappings of this cache.\r\n```', 'commenter': 'dlmarion'}, {'comment': 'Could consider this change (or something like it) later if the computation of volumeHdfsConfigOverrides actually shows up in any profiling.', 'commenter': 'keith-turner'}, {'comment': 'I included this suggestion in my code review commit 8f1ee93431999e673a878be134f947270eb0dd17', 'commenter': 'ctubbsii'}]"
3706,server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java,"@@ -393,33 +398,29 @@ public short getDefaultReplication(Path path) {
   protected static Configuration getVolumeManagerConfiguration(AccumuloConfiguration conf,
       final Configuration hadoopConf, final String filesystemURI) {
 
-    Map<String,String> volumeHdfsConfigOverrides =
-        conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
-            .stream().filter(e -> e.getKey().startsWith(filesystemURI + ""."")).collect(
-                Collectors.toUnmodifiableMap(e -> e.getKey().substring(filesystemURI.length() + 1),
-                    Entry::getValue));
+    final var cacheKey = new Pair<>(hadoopConf, filesystemURI);
+    return HDFS_CONFIGS_FOR_VOLUME.get(cacheKey, (key) -> {
 
-    if (volumeHdfsConfigOverrides.isEmpty()) {
-      // There are no overrides in the AccumuloConfiguration for this volume, return the Hadoop
-      // Configuration input parameter.
-      return hadoopConf;
-    } else {
-      // Get the Hadoop Configuration object from the Cache for this filesystemURI. If no object
-      // exists, then create one, apply the overrides and put it in the cache. The Volume config
-      // overrides cannot change, so we only need to cache this once. Construct a Key that is
-      // unique to the Hadoop Configuration input parameter in case different objects are passed in.
-      final String key = hadoopConf.hashCode() + filesystemURI;
-      return HDFS_CONFIGS_FOR_VOLUME.get(key, (fs) -> {
-        log.debug(""Caching a new configuration for volume: {} with overrides: {}"", filesystemURI,
-            volumeHdfsConfigOverrides);
-        Configuration volumeConfig = new Configuration(hadoopConf);
-        volumeHdfsConfigOverrides.forEach((k, v) -> {
-          log.info(""Overriding property {}={} for volume {}"", k, v, filesystemURI);
-          volumeConfig.set(k, v);
-        });
-        return new Configuration(volumeConfig);
+      Map<String,String> volumeHdfsConfigOverrides =
+          conf.getAllPropertiesWithPrefixStripped(Property.INSTANCE_VOLUME_CONFIG_PREFIX).entrySet()
+              .stream().filter(e -> e.getKey().startsWith(filesystemURI + "".""))
+              .collect(Collectors.toUnmodifiableMap(
+                  e -> e.getKey().substring(filesystemURI.length() + 1), Entry::getValue));
+
+      // use the original if no overrides exist
+      if (volumeHdfsConfigOverrides.isEmpty()) {
+        return hadoopConf;
+      }
+
+      log.debug(""Caching a new configuration for volume: {} with overrides: {}"", filesystemURI,","[{'comment': 'This debug logging seems to duplicate the information that will be logged in the later log.info().  Seems like it could be removed', 'commenter': 'keith-turner'}]"
3725,server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java,"@@ -101,6 +101,9 @@ public GCRun(Ample.DataLevel level, ServerContext context) {
     this.level = level;
     this.context = context;
     this.config = context.getConfiguration();
+    log.info(""GC candidate batch size = {} bytes ({} of GC memory)"",
+        config.getAsBytes(Property.GC_CANDIDATE_BATCH_SIZE),
+        config.get(Property.GC_CANDIDATE_BATCH_SIZE));","[{'comment': ""Probably shouldn't log here, as it will log 3x every cycle."", 'commenter': 'ctubbsii'}]"
3725,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -867,8 +867,8 @@ public enum Property {
   GC_PREFIX(""gc."", null, PropertyType.PREFIX,
       ""Properties in this category affect the behavior of the accumulo garbage collector."",
       ""1.3.5""),
-  GC_CANDIDATE_BATCH_SIZE(""gc.candidate.batch.size"", ""8M"", PropertyType.BYTES,
-      ""The batch size used for garbage collection."", ""2.1.0""),
+  GC_CANDIDATE_BATCH_SIZE(""gc.candidate.batch.size"", ""50%"", PropertyType.MEMORY,
+      ""The amount of memory used to calculate batch size for garbage collection."", ""2.1.0""),","[{'comment': '```suggestion\r\n      ""The amount of memory used as the batch size for garbage collection."", ""2.1.0""),\r\n```', 'commenter': 'ctubbsii'}]"
3725,server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java,"@@ -90,7 +90,8 @@ public class SimpleGarbageCollector extends AbstractServer implements Iface {
     log.info(""start delay: {} milliseconds"", getStartDelay());
     log.info(""time delay: {} milliseconds"", gcDelay);
     log.info(""safemode: {}"", inSafeMode());
-    log.info(""candidate batch size: {} bytes"", getCandidateBatchSize());
+    log.info(""GC candidate batch size: {} bytes ({} of GC memory)"", getCandidateBatchSize(),","[{'comment': 'GC is redundant.\r\n\r\n```suggestion\r\n    log.info(""candidate batch size: {} bytes ({} of memory)"", getCandidateBatchSize(),\r\n```\r\n\r\nI\'m not sure the parenthetical addition is adding much either. The site configuration is already logged on startup, so they can already see the String value in the logs.', 'commenter': 'ctubbsii'}, {'comment': 'For a long running cluster, the logs will likely roll and that may be a consideration for keeping this - maybe a debug level?', 'commenter': 'EdColeman'}, {'comment': ""I think that's an unrelated change. We can add an independent thing if we want to report the config on a regular basis, or when something changes, or some other mechanism. I don't think that needs to be conflated with this PR."", 'commenter': 'ctubbsii'}, {'comment': ""To be clear, I'm not suggesting we remove this log statement... just that the original message was sufficient. I'm trying to minimize unrelated changes to the core task of changing the property type. This is for a patch release, after all... minimizing changes to the stable version is the default goal."", 'commenter': 'ctubbsii'}]"
3725,test/src/main/java/org/apache/accumulo/test/functional/GarbageCollectorIT.java,"@@ -310,6 +311,33 @@ public void testProperPortAdvertisement() throws Exception {
     }
   }
 
+  // Verify that the GC_CANDIDATE_BATCH_SIZE can use percentage rather than a specific byte value.
+  @Test
+  public void gcTestSetGCBatchSizeAsMemoryValue() throws Exception {
+    killMacGc();
+    Thread.sleep(SECONDS.toMillis(15));","[{'comment': ""Instead of doing arbitrary sleep durations, should use Wait.waitFor, which can wait up to a max duration (scaled by timeout.factor) for the condition being tested to happen.\r\n\r\nBut actually, I don't think this test is needed at all. It's just testing the ConfigurationTypeHelper.getMemoryAsBytes, which already has test coverage.\r\n\r\nI think it can just be deleted."", 'commenter': 'ctubbsii'}]"
3728,server/base/src/main/java/org/apache/accumulo/server/manager/state/MergeInfo.java,"@@ -81,25 +81,22 @@ public boolean isDelete() {
     return this.operation.equals(Operation.DELETE);
   }
 
-  public boolean needsToBeChopped(KeyExtent otherExtent) {
-    // TODO: For now only Deletes still need chops
-    // During a delete, the block after the merge will be stretched to cover the deleted area.
-    // Therefore, it needs to be chopped
-    if (isDelete() && otherExtent.tableId().equals(extent.tableId())) {
-      return otherExtent.prevEndRow() != null && otherExtent.prevEndRow().equals(extent.endRow());
-    } else {
-      return false;
-    }
-  }
-
   public boolean overlaps(KeyExtent otherExtent) {
     boolean result = this.extent.overlaps(otherExtent);
-    if (!result && needsToBeChopped(otherExtent)) {
+    if (!result && deleteOverlaps(otherExtent)) {
       return true;
     }
     return result;
   }
 
+  public boolean deleteOverlaps(KeyExtent otherExtent) {
+    if (isDelete() && otherExtent.tableId().equals(extent.tableId())) {
+      return otherExtent.prevEndRow() != null && otherExtent.prevEndRow().equals(extent.endRow());
+    } else {
+      return false;
+    }
+  }
+","[{'comment': 'You don\'t need to rename this method. A ""chop"" can be a synonym for ""apply fencing"" instead of a ""chop compaction"".\r\n\r\nAt the very least, it\'d be helpful not to relocate the method, if the body didn\'t change, so it makes it easier to review code changes and see that only the method name changed.', 'commenter': 'ctubbsii'}, {'comment': 'Fixed in latest commit', 'commenter': 'cshannon'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/Manager.java,"@@ -649,16 +648,6 @@ TabletGoalState getGoalState(TabletLocationState tls, MergeInfo mergeInfo) {
               break;
             case STARTED:
             case SPLITTING:
-              return TabletGoalState.HOSTED;
-            case WAITING_FOR_CHOPPED:
-              if (tls.getState(tserverSet.getCurrentServers()).equals(TabletState.HOSTED)) {
-                if (tls.chopped) {
-                  return TabletGoalState.UNASSIGNED;
-                }
-              } else if (tls.chopped && tls.walogs.isEmpty()) {
-                return TabletGoalState.UNASSIGNED;
-              }
-","[{'comment': ""Removing this creates a warning about lack of full coverage in the switch statement. There is no case for this instance or a default case. Unfortunately, we (inconveniently) chose to use the ordinal to serialize the merge info for fate operations, so we have to be very careful not to cause a problem by changing the ordinals and interpreting them incorrectly across an upgrade. If we hadn't done that (which, in general, is a bad idea), then we could have just removed it entirely, since it's only used internally, and we expect all the jars we use to be upgraded at the same time.\r\n\r\nThe best option for now is to just leave the `WAITING_FOR_CHOPPED` enum that is now should no longer be used, and mark it deprecated in the enum class. (Note, it is still in use in the MergeStats class, but that can/should be updated with these changes that discontinue its use as a valid state.) In this switch statement, just add a default case that throws an exception about an unrecognized state."", 'commenter': 'ctubbsii'}, {'comment': ""One of the things I wasn't sure about was if we were just going to leave all the chop compaction related stuff around as deprecated even if not used anymore or just delete it all. I figured removing may cause issues with upgrades or other problems but I wasn't sure since chop compaction code should mostly be internal in the servers and I wouldn't expect to touch public API a lot (but I haven't checked), although there is a Thrift api for running a chop compaction. In the elasticity branch I figure we will delete it all but in main I guess we can keep it and mark as deprecated.\r\n\r\nI was going to tackle the deprecation/removal in a follow on PR (upgrading metadata would also be a follow on PR) so I can revert back the things I removed such as this switch statement for now. We also need to handle the upgrade case in a follow on PR to upgrade existing metadata to the new format."", 'commenter': 'cshannon'}, {'comment': ""Definitely delete what isn't needed, if it's internal code. The only thing you need to be careful about keeping around is stuff related to serializing (if it's persisted anywhere needed across upgrades), or if it's public API. Then, we need to be more careful."", 'commenter': 'ctubbsii'}, {'comment': ""> Unfortunately, we (inconveniently) chose to use the ordinal to serialize the merge info for fate operations, so we have to be very careful not to cause a problem by changing the ordinals and interpreting them incorrectly across an upgrade. If we hadn't done that (which, in general, is a bad idea), then we could have just removed it entirely, since it's only used internally, and we expect all the jars we use to be upgraded at the same time.\r\n\r\nAccumulo used to not upgrade if there are outstanding FATE ops. Reasoning about FATEs operations running across an upgrade is not easy to do, its also really difficult to test.  So we should be able to remove the WAITING_FOR_CHOPPED enum.  If Accumulo is no longer preventing upgrade when there are outstanding FATE ops, then I would consider that a bug."", 'commenter': 'keith-turner'}, {'comment': 'Found the code that does the check for outstanding fate ops in the upgrade code.\r\n\r\nhttps://github.com/apache/accumulo/blob/770c05b46a4e3c135c85d0772ebb2b02d9db5414/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/UpgradeCoordinator.java#L157', 'commenter': 'keith-turner'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -924,6 +1015,135 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
     }
   }
 
+  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {
+    return needsFencingForSplit(info, keyExtent) || info.deleteOverlaps(keyExtent);
+  }","[{'comment': ""This is the only method that calls needsFencingForSplit. needsFencingForSplit also double-checks that the operation `isDelete()`, but that's redundant if this method is being called.\r\n\r\nPutting the contents of that method into this one, and simplifying it dramatically, I get:\r\n\r\n```suggestion\r\n  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {\r\n    // Does this extent cover the end points of the delete?\r\n    Predicate<Text> isWithin = r -> r != null && keyExtent.contains(r);\r\n    Predicate<Text> isNotBoundary =\r\n        r -> !r.equals(keyExtent.endRow()) && !r.equals(keyExtent.prevEndRow());\r\n    KeyExtent deleteRange = info.getExtent();\r\n    return (keyExtent.overlaps(deleteRange) && Stream\r\n        .of(deleteRange.prevEndRow(), deleteRange.endRow()).anyMatch(isWithin.and(isNotBoundary)))\r\n        || info.deleteOverlaps(keyExtent);\r\n  }\r\n```\r\n\r\nNow, I'm still not exactly sure about the edge cases this is checking for... there are too many overlaps, contains, and boundary checks happening in different methods to really intuitively get it. But this simplification should help. I selected predicate names that suggest what is being checked here, so that it would be easier to understand why you might need fencing in some cases, but not in others. You don't need fencing, for example, if the delete range's endRow is within the current keyExtent, but falls exactly on its boundary... you can just delete all the files. But you would need to fence if the delete range is within the current keyExtent, but doesn't reach all the way at the boundary.\r\n\r\nI'm really not sure about the correctness of this when it comes to the edge cases, though, like the first or last tablet (where endRow or prevEndRow are null), or if the bounds of the deleted range are not inclusive. I'm hoping there's already good coverage of all this, so you don't have to do much... but simplification like what I am suggesting here could make it much more readable if done in more places."", 'commenter': 'ctubbsii'}, {'comment': ""Thanks I will take a look when I get a chance and try and see if it's correct and the changes make sense. It's tricky as the logic combines checks for both when we need to do chop compactions and also if we need to split in order to see if we need to fence the files instead. Also once things are working and we have good test coverage we can always then clean up or improve the code and business logic. I figure some of this will be improved a bit over time as we test and find ways to optimize."", 'commenter': 'cshannon'}, {'comment': 'My latest commit uses this suggestion, it seems to be equivalent after looking at it and is a lot more concise. All the tests still pass.', 'commenter': 'cshannon'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -557,6 +560,35 @@ private int assignedOrHosted(Map<TableId,TableCounts> last) {
     return result;
   }
 
+  // This method is used to detect if a tablet needs to be split for a delete
+  // Instead of performing a split, the tablet will have it's files fenced.
+  private boolean needsFencingForSplit(MergeInfo info, KeyExtent extent) {
+    // Merges don't split
+    if (!info.isDelete()) {
+      return false;
+    }
+    // Does this extent cover the end points of the delete?
+    KeyExtent range = info.getExtent();
+    if (extent.overlaps(range)) {
+      for (Text splitPoint : new Text[] {range.prevEndRow(), range.endRow()}) {
+        if (splitPoint == null) {
+          continue;
+        }
+        if (!extent.contains(splitPoint)) {
+          continue;
+        }
+        if (splitPoint.equals(extent.endRow())) {
+          continue;
+        }
+        if (splitPoint.equals(extent.prevEndRow())) {
+          continue;
+        }
+        return true;
+      }
+    }
+    return false;
+  }
+","[{'comment': 'This method appears to only be used in one place, and can be consolidated into that method. I do that in my other comment, so this method can just be deleted.\r\n\r\n```suggestion\r\n```', 'commenter': 'ctubbsii'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/state/MergeStats.java,"@@ -136,17 +128,10 @@ public MergeState nextMergeState(AccumuloClient accumuloClient, CurrentState man
         log.info(""Merge range is already contained in a single tablet {}"", info.getExtent());
         state = MergeState.COMPLETE;
       } else if (hosted == total) {
-        if (info.isDelete()) {
-          if (!lowerSplit) {
-            log.info(""Waiting for {} lower split to occur {}"", info, info.getExtent());
-          } else if (!upperSplit) {
-            log.info(""Waiting for {} upper split to occur {}"", info, info.getExtent());
-          } else {
-            state = MergeState.WAITING_FOR_CHOPPED;
-          }
-        } else {
-          state = MergeState.WAITING_FOR_CHOPPED;
-        }
+        // Todo: Clean up references to WAITING_FOR_CHOPPED and SPLITTING and remove
+        // from enum in a future PR as both are going away. for now just change","[{'comment': 'See my other comment about serialization of the ordinals, and the risks of removing these enums.', 'commenter': 'ctubbsii'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -669,8 +671,97 @@ private void updateMergeState(Map<TableId,MergeStats> mergeStatsCache) {
     }
   }
 
+  private Text getDeletionStartRow(MergeInfo info) throws AccumuloException {
+    // Find the first tablet in the deletion range and check if it was fenced
+    // If the tablet has been fenced we need to keep it as there is still valid
+    // data and we don't want to delete it
+    Text deletionStartRow = info.getExtent().prevEndRow();
+    if (deletionStartRow != null) {
+      Manager.log.trace(""Finding tablet that contains the start row {} for deletion"",
+          deletionStartRow);
+      KeyExtent firstTablet =
+          loadTabletMetadata(info.getExtent().tableId(), deletionStartRow, ColumnType.PREV_ROW)
+              .map(TabletMetadata::getExtent).orElse(null);
+      // If needsFencingForDeletion() is true then this tablet was previously fenced
+      // so we need to keep it and not delete the tablet. Skip to the next highest
+      // tablet after this one if it exists
+      if (firstTablet != null && needsFencingForDeletion(info, firstTablet)) {","[{'comment': 'firstTablet being null at this point seems very problematic and should probably not be ignored.  If a table has tablets, then one should be found.', 'commenter': 'keith-turner'}, {'comment': ""You are right, this check is not needed, it was just force of habit of checking for null and I will get rid of it. I can also get rid of it for looking up the last tablet, something should always return since it's using the overlapping() method for looking up tablets so rows should always map to something."", 'commenter': 'cshannon'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -924,6 +1015,135 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
     }
   }
 
+  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {
+    return needsFencingForSplit(info, keyExtent) || info.deleteOverlaps(keyExtent);
+  }
+
+  // Instead of splitting or chopping tablets for a delete we instead create ranges
+  // to exclude the portion of the tablet that should be deleted
+  private List<Range> createRangesForDeletion(TabletMetadata tabletMetadata,
+      final KeyExtent deleteRange) {
+    final KeyExtent keyExtent = tabletMetadata.getExtent();
+    final Text prevTabletEndRow =
+        tabletMetadata.getPrevEndRow() != null ? tabletMetadata.getPrevEndRow() : null;
+    final Text tabletEndRow = keyExtent.endRow();
+    final Range tabletRange = tabletMetadata.getExtent().toDataRange();
+    final Key firstKey =
+        prevTabletEndRow != null ? new Key(prevTabletEndRow).followingKey(PartialKey.ROW) : null;
+    final List<Range> ranges = new ArrayList<>();
+
+    // This covers the case of when a deletion range overlaps the first tablet
+    // We need to create a range that excludes the deletion
+    if ((firstKey == null || !firstKey.equals(deleteRange.toDataRange().getStartKey()))
+        && deleteRange.toDataRange().getStartKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getStartKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at start of deletion range with end row {}, Start deletion row {}"",
+          prevTabletEndRow, deleteRange.toDataRange().getStartKey().getRow());
+      ranges.add(new Range(prevTabletEndRow, false,
+          deleteRange.toDataRange().getStartKey().getRow(), false));
+    }
+
+    // This covers the case of when a deletion range overlaps the last tablet
+    // We need to create a range that excludes the deletion.
+    if ((tabletEndRow == null
+        || !tabletEndRow.equals(deleteRange.toDataRange().getEndKey().getRow()))
+        && deleteRange.toDataRange().getEndKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getEndKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at end of deletion range with end row {}, End deletion row {}"",
+          tabletEndRow, deleteRange.toDataRange().getEndKey().getRow());
+      ranges.add(new Range(deleteRange.toDataRange().getEndKey().getRow(), tabletEndRow));
+    }
+
+    return ranges;
+  }
+
+  private void updateMetadataRecordsForDelete(MergeInfo info) throws AccumuloException {
+    final KeyExtent range = info.getExtent();
+
+    String targetSystemTable = MetadataTable.NAME;
+    if (range.isMeta()) {
+      targetSystemTable = RootTable.NAME;
+    }
+
+    final AccumuloClient client = manager.getContext();
+
+    try (BatchWriter bw = client.createBatchWriter(targetSystemTable)) {
+      final Text startRow = range.prevEndRow();
+      final Text lastRow = range.endRow() != null
+          ? new Key(range.endRow()).followingKey(PartialKey.ROW).getRow() : null;
+
+      // Find the tablets that overlap the start and end of the deletion range
+      final List<TabletMetadata> tabletMetadatas = new ArrayList<>();
+      loadTabletMetadata(range.tableId(), startRow, ColumnType.PREV_ROW, ColumnType.FILES)
+          .ifPresent(tabletMetadatas::add);
+      loadTabletMetadata(range.tableId(), lastRow, ColumnType.PREV_ROW, ColumnType.FILES)
+          .ifPresent(tabletMetadatas::add);","[{'comment': 'If the delete range falls within in a single tablet, would the same tablet end up tabletMetadatas twice?  If so would that cause any problems in the loop below?', 'commenter': 'keith-turner'}, {'comment': 'If the delete range is in the same tablet you end up with two metadata files, we have to fence twice. So you end up deleting the previous file metadata and replacing it with two new files. The first file would have a range that covers the start of the tablet to the start of the delete range and the second file would have a range that covers the end of the delete range to the end of the tablet so that the entire delete range in the middle of the tablet is fenced off and not visible.', 'commenter': 'cshannon'}, {'comment': 'Will the code end up deleting the original files twice in the metadata table?  That should be ok I think, tyring to make sure I understand the code. Would it be ok to deduplicate tablets before the loop and if there is the same tablet twice only go through the loop once?', 'commenter': 'keith-turner'}, {'comment': ""Hmm I wonder if I can use a set or a map to track the TabletMetadata, that way if we have the same thing twice it won't run twice."", 'commenter': 'cshannon'}, {'comment': ""The latest commit fixes this to make sure we don't process a tablet twice."", 'commenter': 'cshannon'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -924,6 +1015,135 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
     }
   }
 
+  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {
+    return needsFencingForSplit(info, keyExtent) || info.deleteOverlaps(keyExtent);
+  }
+
+  // Instead of splitting or chopping tablets for a delete we instead create ranges
+  // to exclude the portion of the tablet that should be deleted
+  private List<Range> createRangesForDeletion(TabletMetadata tabletMetadata,
+      final KeyExtent deleteRange) {
+    final KeyExtent keyExtent = tabletMetadata.getExtent();
+    final Text prevTabletEndRow =
+        tabletMetadata.getPrevEndRow() != null ? tabletMetadata.getPrevEndRow() : null;
+    final Text tabletEndRow = keyExtent.endRow();
+    final Range tabletRange = tabletMetadata.getExtent().toDataRange();
+    final Key firstKey =
+        prevTabletEndRow != null ? new Key(prevTabletEndRow).followingKey(PartialKey.ROW) : null;
+    final List<Range> ranges = new ArrayList<>();
+
+    // This covers the case of when a deletion range overlaps the first tablet
+    // We need to create a range that excludes the deletion
+    if ((firstKey == null || !firstKey.equals(deleteRange.toDataRange().getStartKey()))
+        && deleteRange.toDataRange().getStartKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getStartKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at start of deletion range with end row {}, Start deletion row {}"",
+          prevTabletEndRow, deleteRange.toDataRange().getStartKey().getRow());
+      ranges.add(new Range(prevTabletEndRow, false,
+          deleteRange.toDataRange().getStartKey().getRow(), false));
+    }
+
+    // This covers the case of when a deletion range overlaps the last tablet
+    // We need to create a range that excludes the deletion.
+    if ((tabletEndRow == null
+        || !tabletEndRow.equals(deleteRange.toDataRange().getEndKey().getRow()))
+        && deleteRange.toDataRange().getEndKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getEndKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at end of deletion range with end row {}, End deletion row {}"",
+          tabletEndRow, deleteRange.toDataRange().getEndKey().getRow());
+      ranges.add(new Range(deleteRange.toDataRange().getEndKey().getRow(), tabletEndRow));
+    }
+
+    return ranges;
+  }
+
+  private void updateMetadataRecordsForDelete(MergeInfo info) throws AccumuloException {
+    final KeyExtent range = info.getExtent();
+
+    String targetSystemTable = MetadataTable.NAME;
+    if (range.isMeta()) {
+      targetSystemTable = RootTable.NAME;
+    }
+
+    final AccumuloClient client = manager.getContext();
+
+    try (BatchWriter bw = client.createBatchWriter(targetSystemTable)) {
+      final Text startRow = range.prevEndRow();
+      final Text lastRow = range.endRow() != null
+          ? new Key(range.endRow()).followingKey(PartialKey.ROW).getRow() : null;
+
+      // Find the tablets that overlap the start and end of the deletion range
+      final List<TabletMetadata> tabletMetadatas = new ArrayList<>();
+      loadTabletMetadata(range.tableId(), startRow, ColumnType.PREV_ROW, ColumnType.FILES)
+          .ifPresent(tabletMetadatas::add);
+      loadTabletMetadata(range.tableId(), lastRow, ColumnType.PREV_ROW, ColumnType.FILES)
+          .ifPresent(tabletMetadatas::add);
+
+      for (TabletMetadata tabletMetadata : tabletMetadatas) {
+        final KeyExtent keyExtent = tabletMetadata.getExtent();
+
+        // Check if this tablet needs to have its files fenced for the deletion
+        if (needsFencingForDeletion(info, keyExtent)) {
+          Manager.log.debug(""Found overlapping keyExtent {} for delete, fencing files."", keyExtent);
+
+          // Create the ranges for fencing the files, this takes the place of
+          // chop compactions and splits
+          final List<Range> ranges = createRangesForDeletion(tabletMetadata, range);
+
+          // Go through and fence each of the files that are part of the tablet
+          for (Entry<StoredTabletFile,DataFileValue> entry : tabletMetadata.getFilesMap()
+              .entrySet()) {
+            StoredTabletFile existing = entry.getKey();
+            Value value = entry.getValue().encodeAsValue();
+
+            Mutation m = new Mutation(keyExtent.toMetaRow());
+
+            // Go through each range that was created and modify the metadata for the file
+            // The end row should be inclusive for the current tablet and the previous end row
+            // should be exclusive for the start row.
+            for (Range fenced : ranges) {
+              // Clip range with the tablet range if the range already exists
+              fenced = existing.hasRange() ? existing.getRange().clip(fenced) : fenced;","[{'comment': 'The ranges could be disjoint which would throw an exception,  thinking if they are disjoint then just drop the file w/o adding anything?  It would be cleaner to put the rest of the code in an else instead of doing a continue like the code below, was just trying to make the suggestion short.\r\n\r\n```suggestion\r\n              fenced = existing.hasRange() ? existing.getRange().clip(fenced, true) : fenced;\r\n\r\n              if(fenced == null) {\r\n                m.putDelete(DataFileColumnFamily.NAME, existing.getMetadataText());\r\n                continue;\r\n              }\r\n```', 'commenter': 'keith-turner'}, {'comment': 'I actually just ran into this exact situation as I was testing deleting with tablets that already have ranged files so I will be making sure to include this with my next testing updates.', 'commenter': 'cshannon'}, {'comment': 'This has been fixed and includes new tests in https://github.com/apache/accumulo/pull/3728/commits/0e6c5d70bc1fad6b234206e103d1851d52e752df\r\n\r\nIf a file is disjoint for all the new range(s) then it will be dropped. There maybe 2 ranges created if the deletion range is contained within 1 tablet so we need to check both. the ITs include tests for deleting existing ranged files', 'commenter': 'cshannon'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -924,6 +1015,135 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
     }
   }
 
+  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {
+    return needsFencingForSplit(info, keyExtent) || info.deleteOverlaps(keyExtent);
+  }
+
+  // Instead of splitting or chopping tablets for a delete we instead create ranges
+  // to exclude the portion of the tablet that should be deleted
+  private List<Range> createRangesForDeletion(TabletMetadata tabletMetadata,
+      final KeyExtent deleteRange) {
+    final KeyExtent keyExtent = tabletMetadata.getExtent();","[{'comment': 'Not completely sure the following assumption is true, but think it might be.  Still looking through the code. If the assumption is true it would be good to add the validation.\r\n\r\n```suggestion\r\n   // its assumed that the delete range overlaps this range but does not contain it.\r\n    Preconditions.checkArgument(!deleteRange.contains(tabletMetadata.getExtent()));\r\n\r\n    final KeyExtent keyExtent = tabletMetadata.getExtent();\r\n```', 'commenter': 'keith-turner'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -924,6 +1015,135 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
     }
   }
 
+  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {
+    return needsFencingForSplit(info, keyExtent) || info.deleteOverlaps(keyExtent);
+  }
+
+  // Instead of splitting or chopping tablets for a delete we instead create ranges
+  // to exclude the portion of the tablet that should be deleted
+  private List<Range> createRangesForDeletion(TabletMetadata tabletMetadata,
+      final KeyExtent deleteRange) {
+    final KeyExtent keyExtent = tabletMetadata.getExtent();
+    final Text prevTabletEndRow =
+        tabletMetadata.getPrevEndRow() != null ? tabletMetadata.getPrevEndRow() : null;
+    final Text tabletEndRow = keyExtent.endRow();
+    final Range tabletRange = tabletMetadata.getExtent().toDataRange();
+    final Key firstKey =
+        prevTabletEndRow != null ? new Key(prevTabletEndRow).followingKey(PartialKey.ROW) : null;
+    final List<Range> ranges = new ArrayList<>();
+
+    // This covers the case of when a deletion range overlaps the first tablet
+    // We need to create a range that excludes the deletion
+    if ((firstKey == null || !firstKey.equals(deleteRange.toDataRange().getStartKey()))
+        && deleteRange.toDataRange().getStartKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getStartKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at start of deletion range with end row {}, Start deletion row {}"",
+          prevTabletEndRow, deleteRange.toDataRange().getStartKey().getRow());
+      ranges.add(new Range(prevTabletEndRow, false,
+          deleteRange.toDataRange().getStartKey().getRow(), false));
+    }","[{'comment': 'Assuming that for the case where the first row of the delete range is null that it means everything in the tablet would be deleted from the perspective of the prev row.  If that is true then would not want to act when firstKey is null and could simplify the code to the following.  Not completely about this, also I did not adjust the log message.\r\n\r\n```suggestion\r\n    if (firstKey != null && tabletMetadata.getExtent().contains(firstKey.getRow())) {\r\n      // In the case where firstKey is null, there is no data to keep in the tablet.\r\n      Manager.log.trace(\r\n          ""Fencing tablet at start of deletion range with end row {}, Start deletion row {}"",\r\n          prevTabletEndRow, deleteRange.toDataRange().getStartKey().getRow());\r\n      ranges.add(new Range(prevTabletEndRow, false, deleteRange.prevEndRow(), true));\r\n    }\r\n```', 'commenter': 'keith-turner'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -924,6 +1015,135 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
     }
   }
 
+  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {
+    return needsFencingForSplit(info, keyExtent) || info.deleteOverlaps(keyExtent);
+  }
+
+  // Instead of splitting or chopping tablets for a delete we instead create ranges
+  // to exclude the portion of the tablet that should be deleted
+  private List<Range> createRangesForDeletion(TabletMetadata tabletMetadata,
+      final KeyExtent deleteRange) {
+    final KeyExtent keyExtent = tabletMetadata.getExtent();
+    final Text prevTabletEndRow =
+        tabletMetadata.getPrevEndRow() != null ? tabletMetadata.getPrevEndRow() : null;
+    final Text tabletEndRow = keyExtent.endRow();
+    final Range tabletRange = tabletMetadata.getExtent().toDataRange();
+    final Key firstKey =
+        prevTabletEndRow != null ? new Key(prevTabletEndRow).followingKey(PartialKey.ROW) : null;
+    final List<Range> ranges = new ArrayList<>();
+
+    // This covers the case of when a deletion range overlaps the first tablet
+    // We need to create a range that excludes the deletion
+    if ((firstKey == null || !firstKey.equals(deleteRange.toDataRange().getStartKey()))
+        && deleteRange.toDataRange().getStartKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getStartKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at start of deletion range with end row {}, Start deletion row {}"",
+          prevTabletEndRow, deleteRange.toDataRange().getStartKey().getRow());
+      ranges.add(new Range(prevTabletEndRow, false,
+          deleteRange.toDataRange().getStartKey().getRow(), false));
+    }
+
+    // This covers the case of when a deletion range overlaps the last tablet
+    // We need to create a range that excludes the deletion.
+    if ((tabletEndRow == null
+        || !tabletEndRow.equals(deleteRange.toDataRange().getEndKey().getRow()))
+        && deleteRange.toDataRange().getEndKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getEndKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at end of deletion range with end row {}, End deletion row {}"",
+          tabletEndRow, deleteRange.toDataRange().getEndKey().getRow());
+      ranges.add(new Range(deleteRange.toDataRange().getEndKey().getRow(), tabletEndRow));
+    }","[{'comment': 'The following does nothing for a null end row and as a result can simplify the code.  Also did not adjust the log message.\r\n\r\n```suggestion\r\n    if (deleteRange.endRow() != null && tabletMetadata.getExtent().contains(deleteRange.endRow())) {\r\n      // in the case where the delete range end row is null, there not data to keep in the tablet.\r\n      Manager.log.trace(\r\n          ""Fencing tablet at end of deletion range with end row {}, End deletion row {}"",\r\n          tabletEndRow, deleteRange.toDataRange().getEndKey().getRow());\r\n      ranges.add(new Range(deleteRange.endRow(), false, tabletEndRow, true));\r\n    }\r\n```', 'commenter': 'keith-turner'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -924,6 +1015,135 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
     }
   }
 
+  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {
+    return needsFencingForSplit(info, keyExtent) || info.deleteOverlaps(keyExtent);
+  }
+
+  // Instead of splitting or chopping tablets for a delete we instead create ranges
+  // to exclude the portion of the tablet that should be deleted
+  private List<Range> createRangesForDeletion(TabletMetadata tabletMetadata,
+      final KeyExtent deleteRange) {
+    final KeyExtent keyExtent = tabletMetadata.getExtent();
+    final Text prevTabletEndRow =
+        tabletMetadata.getPrevEndRow() != null ? tabletMetadata.getPrevEndRow() : null;
+    final Text tabletEndRow = keyExtent.endRow();
+    final Range tabletRange = tabletMetadata.getExtent().toDataRange();
+    final Key firstKey =
+        prevTabletEndRow != null ? new Key(prevTabletEndRow).followingKey(PartialKey.ROW) : null;
+    final List<Range> ranges = new ArrayList<>();
+
+    // This covers the case of when a deletion range overlaps the first tablet
+    // We need to create a range that excludes the deletion
+    if ((firstKey == null || !firstKey.equals(deleteRange.toDataRange().getStartKey()))
+        && deleteRange.toDataRange().getStartKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getStartKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at start of deletion range with end row {}, Start deletion row {}"",
+          prevTabletEndRow, deleteRange.toDataRange().getStartKey().getRow());
+      ranges.add(new Range(prevTabletEndRow, false,
+          deleteRange.toDataRange().getStartKey().getRow(), false));
+    }
+
+    // This covers the case of when a deletion range overlaps the last tablet
+    // We need to create a range that excludes the deletion.
+    if ((tabletEndRow == null
+        || !tabletEndRow.equals(deleteRange.toDataRange().getEndKey().getRow()))
+        && deleteRange.toDataRange().getEndKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getEndKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at end of deletion range with end row {}, End deletion row {}"",
+          tabletEndRow, deleteRange.toDataRange().getEndKey().getRow());
+      ranges.add(new Range(deleteRange.toDataRange().getEndKey().getRow(), tabletEndRow));
+    }
+
+    return ranges;
+  }
+
+  private void updateMetadataRecordsForDelete(MergeInfo info) throws AccumuloException {
+    final KeyExtent range = info.getExtent();
+
+    String targetSystemTable = MetadataTable.NAME;
+    if (range.isMeta()) {
+      targetSystemTable = RootTable.NAME;
+    }
+
+    final AccumuloClient client = manager.getContext();
+
+    try (BatchWriter bw = client.createBatchWriter(targetSystemTable)) {
+      final Text startRow = range.prevEndRow();
+      final Text lastRow = range.endRow() != null
+          ? new Key(range.endRow()).followingKey(PartialKey.ROW).getRow() : null;
+
+      // Find the tablets that overlap the start and end of the deletion range
+      final List<TabletMetadata> tabletMetadatas = new ArrayList<>();
+      loadTabletMetadata(range.tableId(), startRow, ColumnType.PREV_ROW, ColumnType.FILES)","[{'comment': 'For this what is the case where the tablet is not present?', 'commenter': 'keith-turner'}, {'comment': ""I don't think there is a case where it wouldn't find a tablet. the loadTabletMetadata() method calls off to Ample and uses the overlapp() method so it should always map and return a matching tablet."", 'commenter': 'cshannon'}]"
3728,server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java,"@@ -924,6 +967,135 @@ private void mergeMetadataRecords(MergeInfo info) throws AccumuloException {
     }
   }
 
+  private boolean needsFencingForDeletion(MergeInfo info, KeyExtent keyExtent) {
+    return needsFencingForSplit(info, keyExtent) || info.deleteOverlaps(keyExtent);
+  }
+
+  // Instead of splitting or chopping tablets for a delete we instead create ranges
+  // to exclude the portion of the tablet that should be deleted
+  private List<Range> createRangesForDeletion(TabletMetadata tabletMetadata,
+      final KeyExtent deleteRange) {
+    final KeyExtent keyExtent = tabletMetadata.getExtent();
+    final Text prevTabletEndRow =
+        tabletMetadata.getPrevEndRow() != null ? tabletMetadata.getPrevEndRow() : null;
+    final Text tabletEndRow = keyExtent.endRow();
+    final Range tabletRange = tabletMetadata.getExtent().toDataRange();
+    final Key firstKey =
+        prevTabletEndRow != null ? new Key(prevTabletEndRow).followingKey(PartialKey.ROW) : null;
+    final List<Range> ranges = new ArrayList<>();
+
+    // This covers the case of when a deletion range overlaps the first tablet
+    // We need to create a range that excludes the deletion
+    if ((firstKey == null || !firstKey.equals(deleteRange.toDataRange().getStartKey()))
+        && deleteRange.toDataRange().getStartKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getStartKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at start of deletion range with end row {}, Start deletion row {}"",
+          prevTabletEndRow, deleteRange.toDataRange().getStartKey().getRow());
+      ranges.add(new Range(prevTabletEndRow, false,
+          deleteRange.toDataRange().getStartKey().getRow(), false));
+    }
+
+    // This covers the case of when a deletion range overlaps the last tablet
+    // We need to create a range that excludes the deletion.
+    if ((tabletEndRow == null
+        || !tabletEndRow.equals(deleteRange.toDataRange().getEndKey().getRow()))
+        && deleteRange.toDataRange().getEndKey() != null
+        && tabletRange.contains(deleteRange.toDataRange().getEndKey())) {
+      Manager.log.trace(
+          ""Fencing tablet at end of deletion range with end row {}, End deletion row {}"",
+          tabletEndRow, deleteRange.toDataRange().getEndKey().getRow());
+      ranges.add(new Range(deleteRange.toDataRange().getEndKey().getRow(), tabletEndRow));
+    }
+
+    return ranges;
+  }
+
+  private void updateMetadataRecordsForDelete(MergeInfo info) throws AccumuloException {
+    final KeyExtent range = info.getExtent();
+
+    String targetSystemTable = MetadataTable.NAME;
+    if (range.isMeta()) {
+      targetSystemTable = RootTable.NAME;
+    }
+
+    final AccumuloClient client = manager.getContext();
+
+    try (BatchWriter bw = client.createBatchWriter(targetSystemTable)) {
+      final Text startRow = range.prevEndRow();
+      final Text lastRow = range.endRow() != null
+          ? new Key(range.endRow()).followingKey(PartialKey.ROW).getRow() : null;
+
+      // Find the tablets that overlap the start and end of the deletion range
+      final List<TabletMetadata> tabletMetadatas = new ArrayList<>();
+      loadTabletMetadata(range.tableId(), startRow, ColumnType.PREV_ROW, ColumnType.FILES)
+          .ifPresent(tabletMetadatas::add);
+      loadTabletMetadata(range.tableId(), lastRow, ColumnType.PREV_ROW, ColumnType.FILES)
+          .ifPresent(tabletMetadatas::add);
+
+      for (TabletMetadata tabletMetadata : tabletMetadatas) {
+        final KeyExtent keyExtent = tabletMetadata.getExtent();
+
+        // Check if this tablet needs to have its files fenced for the deletion
+        if (needsFencingForDeletion(info, keyExtent)) {
+          Manager.log.debug(""Found overlapping keyExtent {} for delete, fencing files."", keyExtent);
+
+          // Create the ranges for fencing the files, this takes the place of
+          // chop compactions and splits
+          final List<Range> ranges = createRangesForDeletion(tabletMetadata, range);","[{'comment': 'Can this return an empty list and if so what should happen?', 'commenter': 'keith-turner'}, {'comment': ""Hmm, I don't think it would ever be empty because we are only iterating over the 1 or 2 tablets that overlap the deletion range by looking up the tablets previously. I would need to think about it but at least in the existing tests it's never empty and should always be at least 1 range or 2 (if the deletion is contained in 1 tablet). We may want to add a check for this to throw an error as I don't think it would be valid if it was empty."", 'commenter': 'cshannon'}, {'comment': 'One case I was unsure about was what happens when deleting null,null, basically all data.   Was not sure if this code gets involved in that case.', 'commenter': 'keith-turner'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -1028,14 +1022,133 @@ private MutationSet sendMutationsToTabletServer(String location,
         timeoutTracker.errorOccured();
         throw new IOException(e);
       } catch (TApplicationException tae) {
+        // no need to bother closing session in this case
+        usid = null;
         updateServerErrors(location, tae);
         throw new AccumuloServerException(location, tae);
       } catch (ThriftSecurityException e) {
+        // no need to bother closing session in this case
+        usid = null;
         updateAuthorizationFailures(
             tabMuts.keySet().stream().collect(toMap(identity(), ke -> e.code)));
         throw new AccumuloSecurityException(e.user, e.code, e);
       } catch (TException e) {
         throw new IOException(e);
+      } finally {
+        if (usid != null) {
+          // There is an open session, must close it before the batchwriter closes or writes could
+          // happen after the batch writer closes. See #3721. Queuing a task instead of executing
+          // the code here because throwing exceptions in a finally block makes the code hard to
+          // reason about. Queue is less likely to throw an exception.
+          sendThreadPool.execute(new CloseSessionTask(location, usid));
+        }
+      }
+    }
+
+    class CloseSessionTask implements Runnable {
+
+      private final String location;
+      private final long usid;
+
+      CloseSessionTask(String location, Long usid) {
+        this.location = location;
+        this.usid = usid;
+        synchronized (TabletServerBatchWriter.this) {
+          if (!failedSessions.add(new Pair<>(usid, location))) {
+            throw new IllegalStateException(""Duplicate session "" + location + "" "" + usid);
+          }
+        }
+
+      }
+
+      @Override
+      public void run() {
+        try {
+          closeSession();
+        } catch (InterruptedException | RuntimeException | ThriftSecurityException e) {
+          updateUnknownErrors(""Failed to close session "" + location + "" "" + usid, e);
+        } finally {
+          synchronized (TabletServerBatchWriter.this) {
+            if (!failedSessions.remove(new Pair<>(usid, location))) {
+              throw new IllegalStateException(""Session missing "" + location + "" "" + usid);
+            }
+            TabletServerBatchWriter.this.notifyAll();
+          }
+        }
+      }
+
+      /**
+       * Checks if there is a lock held by a tserver at a specific host and port.
+       */
+      private boolean isALockHeld(String tserver) {
+        var root = context.getZooKeeperRoot() + Constants.ZTSERVERS;
+        var zLockPath = ServiceLock.path(root + ""/"" + tserver);
+        return ServiceLock.getSessionId(context.getZooCache(), zLockPath) != 0;
+      }
+
+      private void closeSession() throws InterruptedException, ThriftSecurityException {
+
+        Retry retry = Retry.builder().infiniteRetries().retryAfter(100, MILLISECONDS)","[{'comment': 'Not sure if it will help, but I created RetryableThriftCall as part of the external compaction code. Example [here](https://github.com/apache/accumulo/blob/main/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java#L359)', 'commenter': 'dlmarion'}, {'comment': 'I think the way this code is creating the client with different timeouts and responding differently to various thrift exceptions that it might be tricky to use that.', 'commenter': 'keith-turner'}, {'comment': 'Yeah, I think the RetryableThriftCall I created is meant to continue retrying even in the midst of errors. For example, if the CompactionCoordinator is restarting and the Compactor needs to report status.', 'commenter': 'dlmarion'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -160,8 +165,9 @@ public class TabletServerBatchWriter implements AutoCloseable {
   private final HashSet<String> serverSideErrors = new HashSet<>();
   private final FailedMutations failedMutations;
   private int unknownErrors = 0;
-  private boolean somethingFailed = false;
+  private volatile boolean somethingFailed = false;","[{'comment': ""This doesn't need to be changed in this PR, but I think there's a good case to be made to prefer the Java 5 atomic classes over manually making variables volatile.\r\n\r\n* Atomic classes force correct usage\r\n* Usually the extra object creation overhead doesn't matter (but obviously, don't use them when it does matter)\r\n* final Atomic references allow the object's contained value to be updated inside a lambda or anonymous class\r\n* One can immediately infer the memory characteristics by merely looking at the type of the variable in the IDE, and not needing to reference any initializing keywords, which makes for less IDE juggling back and forth, and doesn't require the developer to juggle that information in their head about which fields are declared volatile and which aren't."", 'commenter': 'ctubbsii'}, {'comment': 'I agree AtomicBoolean is better here because its not frequently used.  If it were frequently used then using the Atomics introduces a layer of memory indirection causing even more pointer chasing.  Changed in a513d75', 'commenter': 'keith-turner'}, {'comment': ""I try to always declare Atomics as final, so there's no additional concern about pointer chasing. But yeah, I get what you mean if that's not certain. Not really relevant here, but I remembered another good use of Atomics that you don't get directly with volatile: compareAndSet."", 'commenter': 'ctubbsii'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -901,8 +907,8 @@ public void send(TabletServerMutations<Mutation> tsm)
             span.end();
           }
         } catch (IOException e) {
-          if (log.isTraceEnabled()) {
-            log.trace(""failed to send mutations to {} : {}"", location, e.getMessage());
+          if (log.isDebugEnabled()) {
+            log.debug(""failed to send mutations to {} : {}"", location, e.getMessage());
           }","[{'comment': 'The guarding based on the log level is unnecessary.\r\n\r\n```suggestion\r\n          log.debug(""failed to send mutations to {} : {}"", location, e.getMessage());\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'changed in a513d75', 'commenter': 'keith-turner'}]"
3733,test/src/main/java/org/apache/accumulo/test/MetaConstraintRetryIT.java,"@@ -52,15 +51,14 @@ public void test() throws Exception {
           TablePermission.WRITE);
 
       ServerContext context = getServerContext();
-      Writer w = new Writer(context, MetadataTable.ID);
       KeyExtent extent = new KeyExtent(TableId.of(""5""), null, null);
 
       Mutation m = new Mutation(extent.toMetaRow());
       // unknown columns should cause constraint violation
       m.put(""badcolfam"", ""badcolqual"", ""3"");
-      var e = assertThrows(RuntimeException.class,
-          () -> MetadataTableUtil.update(context, w, null, m, extent));
-      assertEquals(ConstraintViolationException.class, e.getCause().getClass());
+      var e = assertThrows(IllegalArgumentException.class,
+          () -> MetadataTableUtil.update(context, null, m, extent));
+      assertEquals(MutationsRejectedException.class, e.getCause().getClass());","[{'comment': 'I think the idea here was to verify that the rejection happened because the constraint was violated. I think you need to inspect the MutationsRejectedException a bit deeper in order to get the same verification.', 'commenter': 'ctubbsii'}, {'comment': 'changed in a513d75', 'commenter': 'keith-turner'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/Writer.java,"@@ -59,62 +42,18 @@ public Writer(ClientContext context, TableId tableId) {
     this.tableId = tableId;
   }
 
-  private static void updateServer(ClientContext context, Mutation m, KeyExtent extent,
-      HostAndPort server) throws TException, NotServingTabletException,
-      ConstraintViolationException, AccumuloSecurityException {
-    checkArgument(m != null, ""m is null"");
-    checkArgument(extent != null, ""extent is null"");
-    checkArgument(server != null, ""server is null"");
-    checkArgument(context != null, ""context is null"");
-
-    TabletClientService.Iface client = null;
-    try {
-      client = ThriftUtil.getClient(ThriftClientTypes.TABLET_SERVER, server, context);
-      client.update(TraceUtil.traceInfo(), context.rpcCreds(), extent.toThrift(), m.toThrift(),
-          TDurability.DEFAULT);
-    } catch (ThriftSecurityException e) {
-      throw new AccumuloSecurityException(e.user, e.code);
-    } finally {
-      ThriftUtil.returnClient((TServiceClient) client, context);
-    }
-  }
-
-  public void update(Mutation m) throws AccumuloException, AccumuloSecurityException,
-      ConstraintViolationException, TableNotFoundException {
+  public void update(Mutation m) throws AccumuloException, TableNotFoundException {
     checkArgument(m != null, ""m is null"");
 
     if (m.size() == 0) {
       throw new IllegalArgumentException(""Can not add empty mutations"");
     }
 
-    while (true) {
-      TabletLocation tabLoc = TabletLocator.getLocator(context, tableId).locateTablet(context,
-          new Text(m.getRow()), false, true);
-
-      if (tabLoc == null) {
-        log.trace(""No tablet location found for row {}"", new String(m.getRow(), UTF_8));
-        sleepUninterruptibly(500, MILLISECONDS);
-        continue;
-      }
-
-      final HostAndPort parsedLocation = HostAndPort.fromString(tabLoc.tablet_location);
-      try {
-        updateServer(context, m, tabLoc.tablet_extent, parsedLocation);
-        return;
-      } catch (NotServingTabletException e) {
-        log.trace(""Not serving tablet, server = {}"", parsedLocation);
-        TabletLocator.getLocator(context, tableId).invalidateCache(tabLoc.tablet_extent);
-      } catch (ConstraintViolationException cve) {
-        log.error(""error sending update to {}"", parsedLocation, cve);
-        // probably do not need to invalidate cache, but it does not hurt
-        TabletLocator.getLocator(context, tableId).invalidateCache(tabLoc.tablet_extent);
-        throw cve;
-      } catch (TException e) {
-        log.error(""error sending update to {}"", parsedLocation, e);
-        TabletLocator.getLocator(context, tableId).invalidateCache(tabLoc.tablet_extent);
-      }
+    String table = Ample.DataLevel.of(tableId).metaTable();
 
-      sleepUninterruptibly(500, MILLISECONDS);
+    try (var writer = context.createBatchWriter(table)) {
+      writer.addMutation(m);
+      writer.flush();","[{'comment': ""```suggestion\r\n```\r\n\r\nShould flush on close anyway. Not much point in calling it manually. The other places this was changed didn't. So, for consistency, I suggest not putting it here either."", 'commenter': 'ctubbsii'}, {'comment': 'fixed in a513d75. I think that was vestigial code  (I had a loop or somethnig inside the try block that needed that in past iterations)', 'commenter': 'keith-turner'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -931,6 +937,8 @@ private MutationSet sendMutationsToTabletServer(String location,
 
       timeoutTracker.startingWrite();
 
+      Long usid = null;","[{'comment': ""OptionalLong is a better style here, as it's more explicit than null checking and implicit reliance on auto-boxing."", 'commenter': 'ctubbsii'}, {'comment': ""changed in a513d75.  Didn't see much harm or benefit in this change after making it."", 'commenter': 'keith-turner'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -1028,14 +1020,139 @@ private MutationSet sendMutationsToTabletServer(String location,
         timeoutTracker.errorOccured();
         throw new IOException(e);
       } catch (TApplicationException tae) {
+        // no need to bother closing session in this case
+        usid = null;
         updateServerErrors(location, tae);
         throw new AccumuloServerException(location, tae);
       } catch (ThriftSecurityException e) {
+        // no need to bother closing session in this case
+        usid = null;
         updateAuthorizationFailures(
             tabMuts.keySet().stream().collect(toMap(identity(), ke -> e.code)));
         throw new AccumuloSecurityException(e.user, e.code, e);
       } catch (TException e) {
         throw new IOException(e);
+      } finally {
+        if (usid != null) {
+          // There is an open session, must close it before the batchwriter closes or writes could
+          // happen after the batch writer closes. See #3721. Queuing a task instead of executing
+          // the code here because throwing exceptions in a finally block makes the code hard to
+          // reason about. Queue is less likely to throw an exception.
+          //
+          // When the task is created below it adds to the failedSessions map, this is done while
+          // the mutations for this task are still incremented in totalMemUsed. It is important that
+          // these overlap in time so that there is no gap where the client would not wait for the
+          // session to close. Overlap in times means the session is added to failedSessions before
+          // the mutations are decremented from totalMemUsed.
+          sendThreadPool.execute(new CloseSessionTask(location, usid));
+        }
+      }
+    }
+
+    class CloseSessionTask implements Runnable {
+
+      private final String location;
+      private final long usid;
+
+      CloseSessionTask(String location, Long usid) {
+        this.location = location;
+        this.usid = usid;
+        synchronized (TabletServerBatchWriter.this) {
+          if (!failedSessions.add(new Pair<>(usid, location))) {
+            throw new IllegalStateException(""Duplicate session "" + location + "" "" + usid);
+          }
+        }
+
+      }
+
+      @Override
+      public void run() {
+        try {
+          closeSession();
+        } catch (InterruptedException | RuntimeException | ThriftSecurityException e) {
+          updateUnknownErrors(""Failed to close session "" + location + "" "" + usid, e);
+        } finally {
+          synchronized (TabletServerBatchWriter.this) {
+            if (!failedSessions.remove(new Pair<>(usid, location))) {
+              throw new IllegalStateException(""Session missing "" + location + "" "" + usid);
+            }
+            TabletServerBatchWriter.this.notifyAll();
+          }
+        }
+      }
+
+      /**
+       * Checks if there is a lock held by a tserver at a specific host and port.
+       */
+      private boolean isALockHeld(String tserver) {
+        var root = context.getZooKeeperRoot() + Constants.ZTSERVERS;
+        var zLockPath = ServiceLock.path(root + ""/"" + tserver);
+        return ServiceLock.getSessionId(context.getZooCache(), zLockPath) != 0;
+      }
+
+      private void closeSession() throws InterruptedException, ThriftSecurityException {
+
+        Retry retry = Retry.builder().infiniteRetries().retryAfter(100, MILLISECONDS)
+            .incrementBy(100, MILLISECONDS).maxWait(60, SECONDS).backOffFactor(1.5)
+            .logInterval(3, MINUTES).createRetry();
+
+        final HostAndPort parsedServer = HostAndPort.fromString(location);
+
+        long startTime = System.currentTimeMillis();
+
+        // If somethingFailed is true then the batch writer will throw an exception on close or
+        // flush, so no need to close this session. Only want to close the session for retryable
+        // exceptions.
+        while (!somethingFailed) {
+
+          TabletClientService.Client client = null;
+
+          // Check if a lock is held by any tserver at the host and port. It does not need to be the
+          // exact tserver instance that existed when the session was created because if a new
+          // tserver instance comes up then the session will not exist there. Trying to get the
+          // exact tserver instance that created the session would require changes to the RPC that
+          // creates the session and this is not needed.
+          if (!isALockHeld(location)) {
+            retry.logCompletion(log,
+                ""No tserver for failed write session "" + location + "" "" + usid);
+            break;
+          }
+
+          try {
+            if (timeout < context.getClientTimeoutInMillis()) {
+              client = ThriftUtil.getClient(ThriftClientTypes.TABLET_SERVER, parsedServer, context,
+                  timeout);
+            } else {
+              client = ThriftUtil.getClient(ThriftClientTypes.TABLET_SERVER, parsedServer, context);
+            }
+
+            client.closeUpdate(TraceUtil.traceInfo(), usid);
+            retry.logCompletion(log, ""Closed failed write session "" + location + "" "" + usid);
+            break;
+          } catch (NoSuchScanIDException e) {
+            retry.logCompletion(log,
+                ""Failed write session no longer exists "" + location + "" "" + usid);
+            // The session no longer exists, so done
+            break;
+          } catch (TApplicationException tae) {
+            // no need to bother closing session in this case
+            updateServerErrors(location, tae);
+            break;
+          } catch (ThriftSecurityException e) {
+            throw e;
+          } catch (TException e) {
+            retry.waitForNextAttempt(log, ""Attempting to close failed write session "" + location
+                + "" "" + usid + "" "" + e.getMessage());
+          } finally {
+            ThriftUtil.returnClient(client, context);
+          }
+
+          // if a timeout is set on the batch writer, then do not retry longer than the timeout
+          if ((System.currentTimeMillis() - startTime) > timeout) {","[{'comment': 'This should be using System.nanoTime() to check the timeout, which tracks the process execution duration, and should not be using the system clock, which can be changed arbitrarily by NTP, a user, another process.', 'commenter': 'ctubbsii'}, {'comment': 'The rest of the class is using System.currentTimeMillis(), so I chose to use it here for consistency.  If a change were to be made to use nano time it should be done for the whole class.  That could be a follow on issue.', 'commenter': 'keith-turner'}, {'comment': ""I would prioritize correctness over consistency. If we avoid incorrect implementations, it avoids creating more work for somebody else to fix later. This one is independent of any others in this very large class, so it's easy to get it correct now, and will create more work for somebody later if deferred."", 'commenter': 'ctubbsii'}, {'comment': ""> I would prioritize correctness over consistency.\r\n\r\nThat's a good reason to update.  It was irksome to make it inconsistent, but I made the change in 66c6849 to make it correct.  Don't want to fix all the other places in this PR though."", 'commenter': 'keith-turner'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -1028,14 +1020,139 @@ private MutationSet sendMutationsToTabletServer(String location,
         timeoutTracker.errorOccured();
         throw new IOException(e);
       } catch (TApplicationException tae) {
+        // no need to bother closing session in this case
+        usid = null;
         updateServerErrors(location, tae);
         throw new AccumuloServerException(location, tae);
       } catch (ThriftSecurityException e) {
+        // no need to bother closing session in this case
+        usid = null;
         updateAuthorizationFailures(
             tabMuts.keySet().stream().collect(toMap(identity(), ke -> e.code)));
         throw new AccumuloSecurityException(e.user, e.code, e);
       } catch (TException e) {
         throw new IOException(e);
+      } finally {
+        if (usid != null) {
+          // There is an open session, must close it before the batchwriter closes or writes could
+          // happen after the batch writer closes. See #3721. Queuing a task instead of executing
+          // the code here because throwing exceptions in a finally block makes the code hard to
+          // reason about. Queue is less likely to throw an exception.
+          //
+          // When the task is created below it adds to the failedSessions map, this is done while
+          // the mutations for this task are still incremented in totalMemUsed. It is important that
+          // these overlap in time so that there is no gap where the client would not wait for the
+          // session to close. Overlap in times means the session is added to failedSessions before
+          // the mutations are decremented from totalMemUsed.
+          sendThreadPool.execute(new CloseSessionTask(location, usid));
+        }
+      }
+    }
+
+    class CloseSessionTask implements Runnable {
+
+      private final String location;
+      private final long usid;
+
+      CloseSessionTask(String location, Long usid) {
+        this.location = location;
+        this.usid = usid;
+        synchronized (TabletServerBatchWriter.this) {
+          if (!failedSessions.add(new Pair<>(usid, location))) {
+            throw new IllegalStateException(""Duplicate session "" + location + "" "" + usid);
+          }
+        }
+
+      }
+
+      @Override
+      public void run() {
+        try {
+          closeSession();
+        } catch (InterruptedException | RuntimeException | ThriftSecurityException e) {
+          updateUnknownErrors(""Failed to close session "" + location + "" "" + usid, e);
+        } finally {
+          synchronized (TabletServerBatchWriter.this) {
+            if (!failedSessions.remove(new Pair<>(usid, location))) {
+              throw new IllegalStateException(""Session missing "" + location + "" "" + usid);
+            }
+            TabletServerBatchWriter.this.notifyAll();
+          }
+        }
+      }
+
+      /**
+       * Checks if there is a lock held by a tserver at a specific host and port.
+       */
+      private boolean isALockHeld(String tserver) {
+        var root = context.getZooKeeperRoot() + Constants.ZTSERVERS;
+        var zLockPath = ServiceLock.path(root + ""/"" + tserver);
+        return ServiceLock.getSessionId(context.getZooCache(), zLockPath) != 0;
+      }
+
+      private void closeSession() throws InterruptedException, ThriftSecurityException {
+
+        Retry retry = Retry.builder().infiniteRetries().retryAfter(100, MILLISECONDS)
+            .incrementBy(100, MILLISECONDS).maxWait(60, SECONDS).backOffFactor(1.5)
+            .logInterval(3, MINUTES).createRetry();
+
+        final HostAndPort parsedServer = HostAndPort.fromString(location);
+
+        long startTime = System.currentTimeMillis();","[{'comment': 'Should use nanoTime.', 'commenter': 'ctubbsii'}, {'comment': 'Fixed in 66c6849', 'commenter': 'keith-turner'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -1028,14 +1020,139 @@ private MutationSet sendMutationsToTabletServer(String location,
         timeoutTracker.errorOccured();
         throw new IOException(e);
       } catch (TApplicationException tae) {
+        // no need to bother closing session in this case
+        usid = null;
         updateServerErrors(location, tae);
         throw new AccumuloServerException(location, tae);
       } catch (ThriftSecurityException e) {
+        // no need to bother closing session in this case
+        usid = null;
         updateAuthorizationFailures(
             tabMuts.keySet().stream().collect(toMap(identity(), ke -> e.code)));
         throw new AccumuloSecurityException(e.user, e.code, e);
       } catch (TException e) {
         throw new IOException(e);
+      } finally {
+        if (usid != null) {
+          // There is an open session, must close it before the batchwriter closes or writes could
+          // happen after the batch writer closes. See #3721. Queuing a task instead of executing
+          // the code here because throwing exceptions in a finally block makes the code hard to
+          // reason about. Queue is less likely to throw an exception.
+          //
+          // When the task is created below it adds to the failedSessions map, this is done while
+          // the mutations for this task are still incremented in totalMemUsed. It is important that
+          // these overlap in time so that there is no gap where the client would not wait for the
+          // session to close. Overlap in times means the session is added to failedSessions before
+          // the mutations are decremented from totalMemUsed.
+          sendThreadPool.execute(new CloseSessionTask(location, usid));
+        }
+      }
+    }
+
+    class CloseSessionTask implements Runnable {","[{'comment': ""There's a lot going on in this. This type should be in its own file. If you want it to have access to TabletServerBatchWriter internals, you can pass `this` as a parameter."", 'commenter': 'ctubbsii'}, {'comment': 'The class is very tightly coupled with the internal implementation and accesses private functions and instance variable of the containing class.  So to push it out would also require changing things to be non private.   I do not think that internal classes that are very tightly coupled with internal  implementation of the containing class need to be moved out.   Its only a class because its internal code that needs to be run in a another thread.', 'commenter': 'keith-turner'}, {'comment': ""I think that the tight coupling might be the problem. It strains the principle of encapsulation by making everything accessible to everything else within a very large class. I think a more modular approach would make this very large class a bit more comprehensible by forcing clear APIs. We've been in this situation before, and it has benefited us to separate things out. I don't think this is that different. I'm not suggesting it all be done here... I'm just suggesting the new class doesn't need to add to the size and complexity of the already large containing class."", 'commenter': 'ctubbsii'}, {'comment': 'I think the tight coupling is nice when analyzing the code. I like when I can examine references to a batch writer variable in an IDE and see all the functions that directly use it. Makes it easy to analyze the code for correctness w/ multithreading.  In  other parts of the Accumulo code we have layers of evolved indirection in the code that make it much more difficult to analyze, understand, and maintain.  I think this code could benefit from being made more modular, but making it more modular should be done with an overall strategy in mind.  I think a strategy of simply extracting inner classes does not evolve to any particular goal and can make the code harder to understand and maintain when iterating that strategy. Maybe a plan like separating  the RPC code from the in in memory state of the batch writer would be a better way to achieve a modular batch writer, not sure.', 'commenter': 'keith-turner'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -1028,14 +1022,139 @@ private MutationSet sendMutationsToTabletServer(String location,
         timeoutTracker.errorOccured();
         throw new IOException(e);
       } catch (TApplicationException tae) {
+        // no need to bother closing session in this case
+        usid = OptionalLong.empty();
         updateServerErrors(location, tae);
         throw new AccumuloServerException(location, tae);
       } catch (ThriftSecurityException e) {
+        // no need to bother closing session in this case
+        usid = OptionalLong.empty();
         updateAuthorizationFailures(
             tabMuts.keySet().stream().collect(toMap(identity(), ke -> e.code)));
         throw new AccumuloSecurityException(e.user, e.code, e);
       } catch (TException e) {
         throw new IOException(e);
+      } finally {
+        if (usid.isPresent()) {
+          // There is an open session, must close it before the batchwriter closes or writes could
+          // happen after the batch writer closes. See #3721. Queuing a task instead of executing
+          // the code here because throwing exceptions in a finally block makes the code hard to
+          // reason about. Queue is less likely to throw an exception.
+          //
+          // When the task is created below it adds to the failedSessions map, this is done while
+          // the mutations for this task are still incremented in totalMemUsed. It is important that
+          // these overlap in time so that there is no gap where the client would not wait for the
+          // session to close. Overlap in times means the session is added to failedSessions before
+          // the mutations are decremented from totalMemUsed.
+          sendThreadPool.execute(new CloseSessionTask(location, usid.getAsLong()));","[{'comment': 'The CloseSessionTask constructor can throw an IllegalStateException when there is a duplicate failed session id. Would this hide or suppress another exception that is being thrown above?', 'commenter': 'dlmarion'}, {'comment': '> Would this hide or suppress another exception that is being thrown above?\r\n\r\nYeah it could. I tried to minimize what was done in the finally block because of that. I would only expect the illegalstate exception to be thrown when there is a bug in the accumulo code, so thinking its ok.  Maybe if I refactored it to use closeable that would handle exceptions better and avoid the need for another thread.  Exceptions that happen with try-with-resources closeable will be appended to any exceptions unlike finally which drops them.', 'commenter': 'keith-turner'}, {'comment': 'I updated the code to use closeable and moved the code out of the finally block in https://github.com/apache/accumulo/commit/cf624372ca15e5992b36003be96123c0595e703d which simplified these changes a bit', 'commenter': 'keith-turner'}]"
3733,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java,"@@ -1028,14 +1022,139 @@ private MutationSet sendMutationsToTabletServer(String location,
         timeoutTracker.errorOccured();
         throw new IOException(e);
       } catch (TApplicationException tae) {
+        // no need to bother closing session in this case
+        usid = OptionalLong.empty();
         updateServerErrors(location, tae);
         throw new AccumuloServerException(location, tae);
       } catch (ThriftSecurityException e) {
+        // no need to bother closing session in this case
+        usid = OptionalLong.empty();
         updateAuthorizationFailures(
             tabMuts.keySet().stream().collect(toMap(identity(), ke -> e.code)));
         throw new AccumuloSecurityException(e.user, e.code, e);
       } catch (TException e) {
         throw new IOException(e);
+      } finally {
+        if (usid.isPresent()) {
+          // There is an open session, must close it before the batchwriter closes or writes could
+          // happen after the batch writer closes. See #3721. Queuing a task instead of executing
+          // the code here because throwing exceptions in a finally block makes the code hard to
+          // reason about. Queue is less likely to throw an exception.
+          //
+          // When the task is created below it adds to the failedSessions map, this is done while
+          // the mutations for this task are still incremented in totalMemUsed. It is important that
+          // these overlap in time so that there is no gap where the client would not wait for the
+          // session to close. Overlap in times means the session is added to failedSessions before
+          // the mutations are decremented from totalMemUsed.
+          sendThreadPool.execute(new CloseSessionTask(location, usid.getAsLong()));
+        }
+      }
+    }
+
+    class CloseSessionTask implements Runnable {
+
+      private final String location;
+      private final long usid;
+
+      CloseSessionTask(String location, Long usid) {
+        this.location = location;
+        this.usid = usid;
+        synchronized (TabletServerBatchWriter.this) {
+          if (!failedSessions.add(new Pair<>(usid, location))) {
+            throw new IllegalStateException(""Duplicate session "" + location + "" "" + usid);
+          }
+        }
+
+      }
+
+      @Override
+      public void run() {
+        try {
+          closeSession();
+        } catch (InterruptedException | RuntimeException | ThriftSecurityException e) {
+          updateUnknownErrors(""Failed to close session "" + location + "" "" + usid, e);
+        } finally {
+          synchronized (TabletServerBatchWriter.this) {
+            if (!failedSessions.remove(new Pair<>(usid, location))) {
+              throw new IllegalStateException(""Session missing "" + location + "" "" + usid);
+            }
+            TabletServerBatchWriter.this.notifyAll();","[{'comment': 'Do we need to call `notifyAll` in all cases? If so, I think throwing the exception might defeat that.', 'commenter': 'dlmarion'}, {'comment': 'In this case its ok not to call it because the set was not changed, however I could reorder because it does not hurt to call it and can be called anywhere in the sync block.', 'commenter': 'keith-turner'}]"
3737,core/src/test/java/org/apache/accumulo/core/rpc/ThriftUtilTest.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.rpc;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertThrows;
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+
+import org.apache.thrift.transport.TByteBuffer;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+import org.apache.thrift.transport.layered.TFramedTransport;
+import org.junit.jupiter.api.Test;
+
+public class ThriftUtilTest {
+
+  public static final int FRAME_HDR_SIZE = 4;
+  public static final int MB1 = 100 * 1024 * 1024;
+  public static final int MB10 = 100 * 1024 * 1024;
+  public static final int MB100 = 100 * 1024 * 1024;
+  public static final int GB = 1 * 1024 * 1024 * 1024;
+
+  @Test
+  public void testDefaultTFramedTransportFactory() throws TTransportException {
+
+    // This test confirms that the default maxMessageSize in Thrift is 100MB
+    // even when we set the frame size to be 1GB
+
+    TByteBuffer underlyingTransport = new TByteBuffer(ByteBuffer.allocate(1024));
+
+    TFramedTransport.Factory factory = new TFramedTransport.Factory(GB);
+    TTransport framedTransport = factory.getTransport(underlyingTransport);
+
+    assertEquals(framedTransport.getConfiguration().getMaxFrameSize(), GB);
+    assertEquals(framedTransport.getConfiguration().getMaxMessageSize(), MB100);
+  }
+
+  @Test
+  public void testAccumuloTFramedTransportFactory() throws TTransportException {
+
+    // This test confirms that our custom FramedTransportFactory sets the max
+    // message size and max frame size to the value that we want.
+
+    TByteBuffer underlyingTransport = new TByteBuffer(ByteBuffer.allocate(1024));
+
+    AccumuloTFramedTransportFactory factory = new AccumuloTFramedTransportFactory(GB);
+    TTransport framedTransport = factory.getTransport(underlyingTransport);
+
+    assertEquals(framedTransport.getConfiguration().getMaxFrameSize(), GB);
+    assertEquals(framedTransport.getConfiguration().getMaxMessageSize(), GB);
+  }
+
+  @Test
+  public void testMessageSizeReadWriteSuccess() throws Exception {
+
+    // This test creates an 10MB buffer in memory as the underlying transport, then
+    // creates a TFramedTransport with a 1MB maxFrameSize and maxMessageSize. It then
+    // writes 1MB - 4 bytes (to account for the frame header) to the transport and
+    // reads the data back out.
+
+    TByteBuffer underlyingTransport = new TByteBuffer(ByteBuffer.allocate(MB10));
+    AccumuloTFramedTransportFactory factory = new AccumuloTFramedTransportFactory(MB1);
+    TTransport framedTransport = factory.getTransport(underlyingTransport);
+    assertEquals(framedTransport.getConfiguration().getMaxFrameSize(), MB1);
+    assertEquals(framedTransport.getConfiguration().getMaxMessageSize(), MB1);
+
+    byte[] writeBuf = new byte[MB1 - FRAME_HDR_SIZE];
+    Arrays.fill(writeBuf, (byte) 1);
+    framedTransport.write(writeBuf);
+    framedTransport.flush();
+
+    assertEquals(MB1, underlyingTransport.getByteBuffer().position());
+    underlyingTransport.flip();
+    assertEquals(0, underlyingTransport.getByteBuffer().position());
+    assertEquals(MB1, underlyingTransport.getByteBuffer().limit());
+
+    byte[] readBuf = new byte[MB1];
+    framedTransport.read(readBuf, 0, MB1);
+  }
+
+  @Test
+  public void testMessageSizeWriteFailure() throws Exception {
+
+    // This test creates an 10MB buffer in memory as the underlying transport, then
+    // creates a TFramedTransport with a 1MB maxFrameSize and maxMessageSize. It then
+    // writes 1MB + 100 bytes to the transport, which fails as it's larger than the
+    // configured frame and message size.
+
+    TByteBuffer underlyingTransport = new TByteBuffer(ByteBuffer.allocate(MB10));
+    AccumuloTFramedTransportFactory factory = new AccumuloTFramedTransportFactory(MB1);
+    TTransport framedTransport = factory.getTransport(underlyingTransport);
+    assertEquals(framedTransport.getConfiguration().getMaxFrameSize(), MB1);
+    assertEquals(framedTransport.getConfiguration().getMaxMessageSize(), MB1);
+
+    // Unable to write more than 1MB to the TByteBuffer
+    byte[] writeBuf = new byte[MB1 + 100];
+    Arrays.fill(writeBuf, (byte) 1);
+    framedTransport.write(writeBuf);
+    assertThrows(TTransportException.class, () -> framedTransport.flush());","[{'comment': ""This method isn't actually testing the frame size... the exception being thrown is one about the read buffer being too small. It doesn't seem related to the frame size. So, I'm not sure this test is testing what it needs to test."", 'commenter': 'ctubbsii'}, {'comment': 'I realized that my MB1, MB10, and MB100 variables were the same value due to copy/paste. I have fixed this and modified the test to confirm that the read does fail. Will push commit shortly.', 'commenter': 'dlmarion'}]"
3737,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -91,14 +91,16 @@ protected abstract class TestMaxFrameSize {
     @Test
     public void testMaxFrameSizeLargerThanDefault() throws Exception {
 
+      int maxSize = TConfiguration.DEFAULT_MAX_FRAME_SIZE;
+      // make sure we go even bigger than that","[{'comment': ""I tried to modify this test to use the maxSize of the defaults (so it should end up being the 100MB value), as the basis for this test.\r\n\r\nHowever, that didn't work. I couldn't get past the memory errors."", 'commenter': 'ctubbsii'}, {'comment': ""So, on the client side, `ThriftUtil.transportFactory()` is using a hardcoded size on `Integer.MAX_VALUE`. On the server side, with no other properties specified, it's using `GENERAL_MAX_MESSAGE_SIZE` which defaults to `1GB`. I modified the test to set `GENERAL_MAX_MESSAGE_SIZE` to `TConfiguration.DEFAULT_MAX_FRAME_SIZE` and changed `ourSize` to twice that. The tests fails and the following is in the tserver log:\r\n\r\n```\r\n2023-08-31T16:44:36,001 [server.TThreadPoolServer] DEBUG: Error processing request\r\norg.apache.thrift.transport.TTransportException: Socket is closed by peer.\r\n        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:176) ~[libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:100) ~[libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.thrift.transport.layered.TFramedTransport.readFrame(TFramedTransport.java:132) ~[libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.thrift.transport.layered.TFramedTransport.read(TFramedTransport.java:100) ~[libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:100) ~[libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.thrift.protocol.TCompactProtocol.readByte(TCompactProtocol.java:622) ~[libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.thrift.protocol.TCompactProtocol.readMessageBegin(TCompactProtocol.java:479) ~[libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.thrift.TMultiplexedProcessor.process(TMultiplexedProcessor.java:104) ~[libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.accumulo.server.rpc.TimedProcessor.process(TimedProcessor.java:50) ~[classes/:?]\r\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:254) [libthrift-0.17.0.jar:0.17.0]\r\n        at org.apache.accumulo.core.trace.TraceWrappedRunnable.run(TraceWrappedRunnable.java:52) [classes/:?]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\r\n        at org.apache.accumulo.core.trace.TraceWrappedRunnable.run(TraceWrappedRunnable.java:52) [classes/:?]\r\n        at java.lang.Thread.run(Thread.java:829) [?:?]\r\n```\r\n\r\n\r\nI'm going to push the changes with this test broken so you can see it."", 'commenter': 'dlmarion'}, {'comment': 'Upon further testing, setting `GENERAL_MAX_MESSAGE_SIZE` is not enough. The other max message size properties have default values. All max message size properties will need to be specified by the user, likely to the same value.', 'commenter': 'dlmarion'}, {'comment': 'IT fixed!', 'commenter': 'dlmarion'}]"
3737,core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchReaderIterator.java,"@@ -407,7 +408,8 @@ public void run() {
             failures.putAll(tsFailures);
           }
         }
-
+      } catch (EOFException e) {
+        fatalException = e;","[{'comment': ""There's another place, in MetadataLocationObtainer, where this can propagate up, that isn't being handled as a fatal exception. I'm not sure it matters there, though."", 'commenter': 'ctubbsii'}, {'comment': ""Eh, this is really just one of many places where the code may not handle the error thrown from the RPC layer well. I don't think it uniquely stands out. I only noticed the MetadataLocationObtainer one because it receives the new EOFException, but then just treats it the same as other IOExceptions."", 'commenter': 'ctubbsii'}]"
3737,core/src/main/java/org/apache/accumulo/core/rpc/AccumuloTFramedTransportFactory.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.rpc;
+
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+import org.apache.thrift.transport.layered.TFramedTransport;
+
+public class AccumuloTFramedTransportFactory extends TFramedTransport.Factory {
+
+  private final int maxMessageSize;
+
+  public AccumuloTFramedTransportFactory(int maxMessageSize) {
+    super(maxMessageSize);
+    this.maxMessageSize = maxMessageSize;
+  }
+
+  @Override
+  public TTransport getTransport(TTransport base) throws TTransportException {
+    // The input parameter ""base"" is typically going to be a TSocket implementation
+    // that represents a connection between two Accumulo endpoints (client-server,
+    // or server-server). The base transport has a maxMessageSize which defaults to
+    // 100MB. The FramedTransport that is created by this factory adds a header to
+    // the message with payload size information. The FramedTransport has a default
+    // frame size of 16MB, but the TFramedTransport constructor sets the frame size
+    // to the frame size set on the underlying transport (""base"" in this case"").
+    // According to current Thrift docs, a message has to fit into 1 frame, so the
+    // frame size will be set to the value that is lower. Prior to this class being
+    // created, we were only setting the frame size, so messages were capped at 100MB
+    // because that's the default maxMessageSize. Here we are setting the maxMessageSize
+    // and maxFrameSize to the same value on the ""base"" transport so that when the
+    // TFramedTransport object is created, it ends up using the values that we want.
+    base.getConfiguration().setMaxFrameSize(maxMessageSize);
+    base.getConfiguration().setMaxMessageSize(maxMessageSize);","[{'comment': ""I spent some time looking at this, and I think it's a good workaround. However, I think the bug should be fixed upstream. I created https://issues.apache.org/jira/browse/THRIFT-5732\r\nWe can leave a note in this class that it can be removed once that issue is fixed and we move to a release with the fix included."", 'commenter': 'ctubbsii'}]"
3737,core/src/test/java/org/apache/accumulo/core/rpc/ThriftUtilTest.java,"@@ -0,0 +1,129 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.rpc;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertThrows;
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+
+import org.apache.thrift.transport.TByteBuffer;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+import org.apache.thrift.transport.layered.TFramedTransport;
+import org.junit.jupiter.api.Test;
+
+public class ThriftUtilTest {
+
+  public static final int FRAME_HDR_SIZE = 4;
+  public static final int MB1 = 1 * 1024 * 1024;
+  public static final int MB10 = 10 * 1024 * 1024;
+  public static final int MB100 = 100 * 1024 * 1024;
+  public static final int GB = 1 * 1024 * 1024 * 1024;
+
+  @Test
+  public void testDefaultTFramedTransportFactory() throws TTransportException {
+
+    // This test confirms that the default maxMessageSize in Thrift is 100MB
+    // even when we set the frame size to be 1GB
+
+    TByteBuffer underlyingTransport = new TByteBuffer(ByteBuffer.allocate(1024));
+
+    TFramedTransport.Factory factory = new TFramedTransport.Factory(GB);
+    TTransport framedTransport = factory.getTransport(underlyingTransport);
+
+    assertEquals(framedTransport.getConfiguration().getMaxFrameSize(), GB);
+    assertEquals(framedTransport.getConfiguration().getMaxMessageSize(), MB100);
+  }
+
+  @Test
+  public void testAccumuloTFramedTransportFactory() throws TTransportException {
+
+    // This test confirms that our custom FramedTransportFactory sets the max
+    // message size and max frame size to the value that we want.
+
+    TByteBuffer underlyingTransport = new TByteBuffer(ByteBuffer.allocate(1024));
+
+    AccumuloTFramedTransportFactory factory = new AccumuloTFramedTransportFactory(GB);
+    TTransport framedTransport = factory.getTransport(underlyingTransport);
+
+    assertEquals(framedTransport.getConfiguration().getMaxFrameSize(), GB);
+    assertEquals(framedTransport.getConfiguration().getMaxMessageSize(), GB);
+  }
+
+  @Test
+  public void testMessageSizeReadWriteSuccess() throws Exception {
+
+    // This test creates an 10MB buffer in memory as the underlying transport, then
+    // creates a TFramedTransport with a 1MB maxFrameSize and maxMessageSize. It then
+    // writes 1MB - 4 bytes (to account for the frame header) to the transport and
+    // reads the data back out.
+
+    TByteBuffer underlyingTransport = new TByteBuffer(ByteBuffer.allocate(MB10));
+    AccumuloTFramedTransportFactory factory = new AccumuloTFramedTransportFactory(MB1);
+    TTransport framedTransport = factory.getTransport(underlyingTransport);
+    assertEquals(framedTransport.getConfiguration().getMaxFrameSize(), MB1);
+    assertEquals(framedTransport.getConfiguration().getMaxMessageSize(), MB1);
+
+    byte[] writeBuf = new byte[MB1 - FRAME_HDR_SIZE];
+    Arrays.fill(writeBuf, (byte) 1);
+    framedTransport.write(writeBuf);
+    framedTransport.flush();
+
+    assertEquals(MB1, underlyingTransport.getByteBuffer().position());
+    underlyingTransport.flip();
+    assertEquals(0, underlyingTransport.getByteBuffer().position());
+    assertEquals(MB1, underlyingTransport.getByteBuffer().limit());
+
+    byte[] readBuf = new byte[MB1];
+    framedTransport.read(readBuf, 0, MB1);
+  }
+
+  @Test
+  public void testMessageSizeWriteFailure() throws Exception {
+
+    // This test creates an 10MB buffer in memory as the underlying transport, then
+    // creates a TFramedTransport with a 1MB maxFrameSize and maxMessageSize. It then
+    // writes 1MB + 100 bytes to the transport, which fails as it's larger than the
+    // configured frame and message size.
+
+    TByteBuffer underlyingTransport = new TByteBuffer(ByteBuffer.allocate(MB10));
+    AccumuloTFramedTransportFactory factory = new AccumuloTFramedTransportFactory(MB1);
+    TTransport framedTransport = factory.getTransport(underlyingTransport);
+    assertEquals(framedTransport.getConfiguration().getMaxFrameSize(), MB1);
+    assertEquals(framedTransport.getConfiguration().getMaxMessageSize(), MB1);
+
+    // Write more than 1MB to the TByteBuffer, it's possible to write more data
+    // than allowed by the frame, it's enforced on the read.
+    byte[] writeBuf = new byte[MB1 + 100];
+    Arrays.fill(writeBuf, (byte) 1);
+    framedTransport.write(writeBuf);
+    framedTransport.flush();
+
+    assertEquals(MB1 + 100 + FRAME_HDR_SIZE, underlyingTransport.getByteBuffer().position());
+    underlyingTransport.flip();
+    assertEquals(0, underlyingTransport.getByteBuffer().position());
+    assertEquals(MB1 + 100 + FRAME_HDR_SIZE, underlyingTransport.getByteBuffer().limit());
+
+    byte[] readBuf = new byte[MB1 + 100];
+    assertThrows(TTransportException.class, () -> framedTransport.read(readBuf, 0, MB1 + 100));","[{'comment': '```suggestion\r\n    var e =\r\n        assertThrows(TTransportException.class, () -> framedTransport.read(readBuf, 0, MB1 + 100));\r\n    assertEquals(""Frame size (1048676) larger than max length (1048576)!"", e.getMessage());\r\n```', 'commenter': 'ctubbsii'}]"
3737,test/pom.xml,"@@ -206,6 +206,11 @@
       <groupId>org.junit.jupiter</groupId>
       <artifactId>junit-jupiter-engine</artifactId>
     </dependency>
+    <dependency>
+      <groupId>org.opentest4j</groupId>
+      <artifactId>opentest4j</artifactId>
+      <version>1.2.0</version>
+    </dependency>","[{'comment': ""You don't need this. You can just expect AssertionError, it's parent type."", 'commenter': 'ctubbsii'}]"
3737,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -32,19 +37,20 @@
 import org.apache.thrift.TConfiguration;
 import org.junit.jupiter.api.Nested;
 import org.junit.jupiter.api.Test;
+import org.opentest4j.AssertionFailedError;
 
 public class ThriftMaxFrameSizeIT extends AccumuloClusterHarness {
 
   private ThriftServerType serverType;
 
-  @Override
-  protected Duration defaultTimeout() {
-    return Duration.ofMinutes(1);
-  }
-","[{'comment': 'I still think we should have a timeout on the ITs.', 'commenter': 'ctubbsii'}]"
3737,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -32,19 +37,20 @@
 import org.apache.thrift.TConfiguration;
 import org.junit.jupiter.api.Nested;
 import org.junit.jupiter.api.Test;
+import org.opentest4j.AssertionFailedError;
 
 public class ThriftMaxFrameSizeIT extends AccumuloClusterHarness {
 
   private ThriftServerType serverType;
 
-  @Override
-  protected Duration defaultTimeout() {
-    return Duration.ofMinutes(1);
-  }
-
   @Override
   public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+    cfg.setNumTservers(1);
     cfg.setProperty(Property.GENERAL_RPC_SERVER_TYPE, serverType.name());
+    cfg.setProperty(Property.GENERAL_MAX_MESSAGE_SIZE,
+        Integer.toString(TConfiguration.DEFAULT_MAX_FRAME_SIZE));
+    cfg.setProperty(Property.TSERV_MAX_MESSAGE_SIZE,
+        Integer.toString(TConfiguration.DEFAULT_MAX_FRAME_SIZE));","[{'comment': ""So, you're fundamentally altering the purpose of this test.\r\n\r\nThe purpose of this test was to make sure that we could pass messages larger than the 16MB max frame size that was the default. We are inherently relying on the fact that our default values for these on the server side are greater than this max, and we are passing messages that exceed the 16MB, to make sure that the configuration on the server-side worked as expected.\r\n\r\nYour changes, however, make it so the server side is configured back to the default, making it indistinguishable from our server-side configuration being ignored, and verifying that the client hangs (except for SSL, which is weird), because it can *not* pass messages larger than the 16MB.\r\n\r\nI don't think the changes to this IT are good here.\r\n\r\nThe situation I ran into when I was looking at this IT, was that I didn't just want to try to pass messages larger than 16MB... I wanted to try to pass messages larger than 100MB, to make sure that the changes in this PR correctly set the max message size when it set the max frame size. I was unable to modify the test to do that, because 100MB is huge, and I kept getting out of memory errors.\r\n\r\nRather than test that we can set it larger than 100MB, we can just test that we're overriding the default of 100MB to something like 20MB. We can then test that we can pass messages over 16MB, but less than 20MB, and we can verify that messages larger than 20MB won't work. But that doesn't tell us anything about whether they failed because they hit the max frame size or because they hit the max message size.\r\n\r\nWe also still might not be able to easily distinguish in the IT the reason why messages larger than 20MB won't work... it could be because of an OutOfMemoryError on the server, rather than exceeding the max message size. What I found interesting in these changes you made, was that you were basically testing that the client hung, and not that the client failed. Based on the EOFException change in TabletServerBatchReaderIterator, I had expected to see a hard failure, rather than a hang due to retries, but I guess we're hitting this max message issue in more places than that iterator and that change was a very specific change to address a related bug, rather than a general fix for retrying Thrift messages that are too big at the RPC layer."", 'commenter': 'ctubbsii'}, {'comment': ""My test refactoring addresses pretty much all this. The cause of the larger messages not working is still not easily distinguishable, though. But, at least there's coverage for both cases now. I did spend quite a bit of time trying to find a good configuration size and sizes of messages to send, that would make sure the test passed okay. I'm still not sure why some smaller values don't work."", 'commenter': 'ctubbsii'}]"
3737,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -56,50 +62,95 @@ class TestDefault extends TestMaxFrameSize {
     TestDefault() {
       serverType = ThriftServerType.getDefault();
     }
+
+    @Test
+    public void testDefaultServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }","[{'comment': ""The succeeded boolean is only ever false when the method that sets it is interrupted... which also would throw the assertion failure. So, adding that boolean doesn't actually add more information... it's always false on the interrupt case, and it's always true on the non-interrupt case. So, it's just a proxy for whether or not we used the assertTimeoutPreemptively method. It can be removed."", 'commenter': 'ctubbsii'}]"
3737,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -56,50 +62,95 @@ class TestDefault extends TestMaxFrameSize {
     TestDefault() {
       serverType = ThriftServerType.getDefault();
     }
+
+    @Test
+    public void testDefaultServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestThreadedSelector extends TestMaxFrameSize {
     TestThreadedSelector() {
       serverType = ThriftServerType.THREADED_SELECTOR;
     }
+
+    @Test
+    public void testThreadedSelectorServerFrameSize() throws Exception {","[{'comment': ""Making these separate tests in each separate `@Nested` class starts defeating the reason these are nested. Plus, all of them except the Ssl one (which I don't understand why it works) has the same implementation. So it can just stay in the base class. The Ssl one could just be made to use a different base class that doesn't have the same method."", 'commenter': 'ctubbsii'}, {'comment': 'I pushed another commit that refactors this test further to address this and other issues.', 'commenter': 'ctubbsii'}]"
3737,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -56,50 +62,95 @@ class TestDefault extends TestMaxFrameSize {
     TestDefault() {
       serverType = ThriftServerType.getDefault();
     }
+
+    @Test
+    public void testDefaultServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestThreadedSelector extends TestMaxFrameSize {
     TestThreadedSelector() {
       serverType = ThriftServerType.THREADED_SELECTOR;
     }
+
+    @Test
+    public void testThreadedSelectorServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestCustomHsHa extends TestMaxFrameSize {
     TestCustomHsHa() {
       serverType = ThriftServerType.CUSTOM_HS_HA;
     }
+
+    @Test
+    public void testCustomHsHaServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestThreadPool extends TestMaxFrameSize {
     TestThreadPool() {
       serverType = ThriftServerType.THREADPOOL;
     }
+
+    @Test
+    public void testThreadPoolServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestSsl extends TestMaxFrameSize {
     TestSsl() {
-      serverType = ThriftServerType.THREADPOOL;
+      serverType = ThriftServerType.SSL;","[{'comment': ""Well, that one was an interesting typo. We'll want to keep that fix."", 'commenter': 'ctubbsii'}]"
3737,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -56,50 +62,95 @@ class TestDefault extends TestMaxFrameSize {
     TestDefault() {
       serverType = ThriftServerType.getDefault();
     }
+
+    @Test
+    public void testDefaultServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestThreadedSelector extends TestMaxFrameSize {
     TestThreadedSelector() {
       serverType = ThriftServerType.THREADED_SELECTOR;
     }
+
+    @Test
+    public void testThreadedSelectorServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestCustomHsHa extends TestMaxFrameSize {
     TestCustomHsHa() {
       serverType = ThriftServerType.CUSTOM_HS_HA;
     }
+
+    @Test
+    public void testCustomHsHaServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestThreadPool extends TestMaxFrameSize {
     TestThreadPool() {
       serverType = ThriftServerType.THREADPOOL;
     }
+
+    @Test
+    public void testThreadPoolServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestSsl extends TestMaxFrameSize {
     TestSsl() {
-      serverType = ThriftServerType.THREADPOOL;
+      serverType = ThriftServerType.SSL;
+    }
+
+    @Test
+    public void testSslServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      testMaxFrameSizeLargerThanDefault(succeeded);
+      assertTrue(succeeded.get());","[{'comment': ""I'm really not sure why this is still working. I guess Ssl transports don't care about the max message size for some reason? That's weird."", 'commenter': 'ctubbsii'}, {'comment': ""After refactoring the test, I don't see the ssl case being an exception... though it does seem to fail with slightly different messages."", 'commenter': 'ctubbsii'}]"
3737,test/src/main/java/org/apache/accumulo/test/functional/ThriftMaxFrameSizeIT.java,"@@ -56,50 +62,95 @@ class TestDefault extends TestMaxFrameSize {
     TestDefault() {
       serverType = ThriftServerType.getDefault();
     }
+
+    @Test
+    public void testDefaultServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestThreadedSelector extends TestMaxFrameSize {
     TestThreadedSelector() {
       serverType = ThriftServerType.THREADED_SELECTOR;
     }
+
+    @Test
+    public void testThreadedSelectorServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestCustomHsHa extends TestMaxFrameSize {
     TestCustomHsHa() {
       serverType = ThriftServerType.CUSTOM_HS_HA;
     }
+
+    @Test
+    public void testCustomHsHaServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestThreadPool extends TestMaxFrameSize {
     TestThreadPool() {
       serverType = ThriftServerType.THREADPOOL;
     }
+
+    @Test
+    public void testThreadPoolServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      assertThrows(AssertionFailedError.class,
+          () -> assertTimeoutPreemptively(Duration.ofSeconds(30),
+              () -> testMaxFrameSizeLargerThanDefault(succeeded)));
+      assertFalse(succeeded.get());
+    }
   }
 
   @Nested
   class TestSsl extends TestMaxFrameSize {
     TestSsl() {
-      serverType = ThriftServerType.THREADPOOL;
+      serverType = ThriftServerType.SSL;
+    }
+
+    @Test
+    public void testSslServerFrameSize() throws Exception {
+      AtomicBoolean succeeded = new AtomicBoolean(false);
+      testMaxFrameSizeLargerThanDefault(succeeded);
+      assertTrue(succeeded.get());
     }
   }
 
   protected abstract class TestMaxFrameSize {
 
-    @Test
-    public void testMaxFrameSizeLargerThanDefault() throws Exception {
+    public void testMaxFrameSizeLargerThanDefault(AtomicBoolean success) throws Exception {
+
+      int maxSize = TConfiguration.DEFAULT_MAX_FRAME_SIZE;
+      // make sure we go even bigger than that
+      int ourSize = maxSize * 2;
 
       // Ingest with a value width greater than the thrift default size to verify our setting works
-      // for max frame wize
+      // for max frame size
       try (AccumuloClient accumuloClient = Accumulo.newClient().from(getClientProps()).build()) {
-        String table = getUniqueNames(1)[0];
-        ReadWriteIT.ingest(accumuloClient, 1, 1, TConfiguration.DEFAULT_MAX_FRAME_SIZE + 1, 0,
-            table);
-        ReadWriteIT.verify(accumuloClient, 1, 1, TConfiguration.DEFAULT_MAX_FRAME_SIZE + 1, 0,
-            table);
+        String table = getUniqueNames(1)[0] + serverType.name();","[{'comment': 'I am not sure why this was modified to add the server type name to the table name. If the `@Nested` naming is working correctly, these already have unique names per test.', 'commenter': 'ctubbsii'}, {'comment': ""As it turns out, the naming wasn't unique, so this was still needed. It's actually worse than that, though... minicluster reuses directories from other tests, because the test naming for nested classes are not unique enough for it to use a separate test directory. I'm not exactly sure how to fix that, as I'm not super familiar with the nested test concept, but as long as the test is passing, and they aren't being run concurrently, it's not really a big deal."", 'commenter': 'ctubbsii'}]"
3754,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/default.ftl,"@@ -30,7 +30,8 @@
         ${val}
       </#list>
     <#else>
-      <script src=""/resources/external/jquery/jquery-3.6.1.js""></script>
+      <script src=""/resources/external/jquery/jquery-3.7.1.js""></script>
+      <script src=""/resources/external/popper/popper.js""></script>
       <script src=""/resources/external/bootstrap/js/bootstrap.js""></script>","[{'comment': 'We don\'t really need popper separate. I\'m not even sure why they made it possible to separate out. It\'s just used internally in bootstrap, as part of bootstrap.\r\n\r\nIf you use the bootstrap bundle, the only thing you need to do is update the filename:\r\n\r\n```suggestion\r\n      <script src=""/resources/external/bootstrap/js/bootstrap.bundle.js""></script>\r\n```\r\n\r\nAlso, version 5.3.2 is the latest available, as of this comment.\r\n', 'commenter': 'ctubbsii'}]"
3770,server/monitor/src/main/resources/org/apache/accumulo/monitor/templates/default.ftl,"@@ -45,6 +45,7 @@
       <script src=""/resources/external/flot/jquery.flot.time.js""></script>
       <script src=""/resources/external/flot/jquery.flot.resize.js""></script>
       <link rel=""stylesheet"" href=""/resources/external/bootstrap/css/bootstrap.css"" />
+      <link rel=""stylesheet"" href=""/resources/external/bootstrap/css/bootstrap-icons.css"" />","[{'comment': ""This new dependency (bootstrap icons are a separate dependency than the bootstrap bundle) requires an update to the LICENSE files.\r\n\r\nFrom the top of the css file, I see this info to use:\r\n```\r\n/*!\r\n * Bootstrap Icons v1.11.1 (https://icons.getbootstrap.com/)\r\n * Copyright 2019-2023 The Bootstrap Authors\r\n * Licensed under MIT (https://github.com/twbs/icons/blob/main/LICENSE)\r\n */\r\n```\r\n\r\nRelated: I noticed flot and jquery have license info in all three of:\r\n\r\n* LICENSE\r\n* server/monitor/src/main/appended-resources/META-INF/LICENSE\r\n* assemble/src/main/resources/LICENSE\r\n\r\nBut bootstrap and datatables are only in the first two, but should probably be in the third as well.\r\n\r\nThe first one is for the source tarball. The last one is for the binary tarball, and the middle one is for the monitor's jar, which is also contained in the binary tarball, but should still list things at the root of the tarball."", 'commenter': 'ctubbsii'}, {'comment': 'Should be fixed in 80a743c', 'commenter': 'DomGarguilo'}]"
3770,server/monitor/src/main/appended-resources/META-INF/LICENSE,"@@ -75,14 +75,23 @@ Files:
      * Dual licensed under the MIT and GPL licenses.
      * http://benalman.com/about/license/
 
-## Bootstrap v5.3.1 (https://getbootstrap.com/)
+## Bootstrap v5.3.2 (https://getbootstrap.com/)","[{'comment': 'I must be going crazy, because I thought I had checked these already. But nice catch.', 'commenter': 'ctubbsii'}, {'comment': 'I think we had discussed who would fix this on the bootstrap ticket but neither of us ever got around to saying that we would do it.', 'commenter': 'DomGarguilo'}, {'comment': 'I definitely remember checking. I definitely re-downloaded the 5.3.2 versions, to make sure they matched bit-for-bit. I thought I checked the version in the LICENSE files at the same time, but maybe I saw 5.3.1 and my brain just replaced the 1 with a 2 in my head. :shrug:', 'commenter': 'ctubbsii'}]"
3776,test/src/main/java/org/apache/accumulo/test/ManagerRepairsDualAssignmentIT.java,"@@ -99,12 +102,21 @@ public void test() throws Exception {
         }
       }
       assertEquals(2, states.size());
+      int deadCount = getZkDeadCount(cluster.getServerContext().getZooReader(),
+          cluster.getServerContext().getInstanceID());
       // Kill a tablet server... we don't care which one... wait for everything to be reassigned
       cluster.killProcess(ServerType.TABLET_SERVER,
           cluster.getProcesses().get(ServerType.TABLET_SERVER).iterator().next());
       Set<TabletMetadata.Location> replStates = new HashSet<>();
       @SuppressWarnings(""deprecation"")
       TableId repTable = org.apache.accumulo.core.replication.ReplicationTable.ID;
+
+      while (getZkDeadCount(cluster.getServerContext().getZooReader(),
+          cluster.getServerContext().getInstanceID()) <= deadCount) {
+        log.debug(""Waiting for dead server to be reported in ZooKeeper. Waiting for next check..."");
+        UtilWaitThread.sleep(5000);
+      }","[{'comment': 'I think you could use `Wait.waitFor` to watch for this case.', 'commenter': 'ctubbsii'}, {'comment': 'changed in 4994f630bf', 'commenter': 'EdColeman'}]"
3776,test/src/main/java/org/apache/accumulo/test/ManagerRepairsDualAssignmentIT.java,"@@ -99,12 +102,21 @@ public void test() throws Exception {
         }
       }
       assertEquals(2, states.size());
+      int deadCount = getZkDeadCount(cluster.getServerContext().getZooReader(),
+          cluster.getServerContext().getInstanceID());","[{'comment': 'This should always be 0 at the start of the test.', 'commenter': 'ctubbsii'}, {'comment': 'It should - but I wanted to avoid the assumption in case there was any chance of a concurrent test had / left something there - a very remote possibility. It just seemed more general to get the count. kill a server and wait for that count to increase.', 'commenter': 'EdColeman'}, {'comment': ""If a concurrent test was somehow using this test's instance of MiniAccumuloCluster, that'd be a big problem, likely to be the cause of other test flakes. So, we'd want to detect that, not obscure it by progressing with a different count that was detected. I would just add a simple `assertEquals(0, actual)` here instead of trying to record and track the previous number."", 'commenter': 'ctubbsii'}]"
3776,test/src/main/java/org/apache/accumulo/test/ManagerRepairsDualAssignmentIT.java,"@@ -99,12 +102,21 @@ public void test() throws Exception {
         }
       }
       assertEquals(2, states.size());
+      int deadCount = getZkDeadCount(cluster.getServerContext().getZooReader(),
+          cluster.getServerContext().getInstanceID());
       // Kill a tablet server... we don't care which one... wait for everything to be reassigned
       cluster.killProcess(ServerType.TABLET_SERVER,
           cluster.getProcesses().get(ServerType.TABLET_SERVER).iterator().next());
       Set<TabletMetadata.Location> replStates = new HashSet<>();
       @SuppressWarnings(""deprecation"")
       TableId repTable = org.apache.accumulo.core.replication.ReplicationTable.ID;
+
+      while (getZkDeadCount(cluster.getServerContext().getZooReader(),
+          cluster.getServerContext().getInstanceID()) <= deadCount) {","[{'comment': 'Does this count ever go back down after increasing? If so, this check could be flaky if it went up and then back down before we checked it here.', 'commenter': 'ctubbsii'}, {'comment': 'Within the context of the test it should not decrease - whatever would clean the dead server list would only have a narrow window (5 sec).  If it is shown to be a problem, then the check would need to be more complicated.  \r\n\r\nIf somehow the ""same"" server was brought back to life, the remainder of the test might have issue too.', 'commenter': 'EdColeman'}]"
3776,test/src/main/java/org/apache/accumulo/test/ManagerRepairsDualAssignmentIT.java,"@@ -150,12 +164,24 @@ public void test() throws Exception {
     }
   }
 
+  private int getZkDeadCount(ZooReader zooReader, final InstanceId iid) {
+    String tPath = Constants.ZROOT + ""/"" + iid + Constants.ZDEADTSERVERS;
+    try {
+      int count = zooReader.getChildren(tPath).size();
+      log.debug(""Current dead server count: {}"", count);
+      return count;
+    } catch (Exception ex) {
+      throw new IllegalStateException(
+          ""Failed to read the number of dead tservers reported in ZooKeeper"", ex);
+    }","[{'comment': ""You don't need to catch and rethrow the exception in the test code... just let it be thrown out of the test method and JUnit will ensure we see the stack trace. It doesn't need to be converted to a RTE."", 'commenter': 'ctubbsii'}, {'comment': 'Changed in 4994f630bf', 'commenter': 'EdColeman'}]"
3776,test/src/main/java/org/apache/accumulo/test/ManagerRepairsDualAssignmentIT.java,"@@ -150,12 +164,24 @@ public void test() throws Exception {
     }
   }
 
+  private int getZkDeadCount(ZooReader zooReader, final InstanceId iid) {
+    String tPath = Constants.ZROOT + ""/"" + iid + Constants.ZDEADTSERVERS;
+    try {
+      int count = zooReader.getChildren(tPath).size();
+      log.debug(""Current dead server count: {}"", count);
+      return count;
+    } catch (Exception ex) {
+      throw new IllegalStateException(
+          ""Failed to read the number of dead tservers reported in ZooKeeper"", ex);
+    }
+  }
+
   private void waitForCleanStore(TabletStateStore store) {
     while (true) {
       try (ClosableIterator<TabletLocationState> iter = store.iterator()) {
         iter.forEachRemaining(t -> {});
       } catch (Exception ex) {
-        System.out.println(ex);
+        log.debug(""Exception waiting fro clean store"", ex);","[{'comment': '```suggestion\r\n        log.debug(""Exception waiting for clean store"", ex);\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'Changed in 4994f630bf', 'commenter': 'EdColeman'}]"
3779,server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java,"@@ -71,7 +77,11 @@ public static int get() {
     return CURRENT_VERSION;
   }
 
-  public static final Set<Integer> CAN_RUN = Set.of(ROOT_TABLET_META_CHANGES, CURRENT_VERSION);
+  // TODO - this is currently set to disable upgrades until metadata file json conversion
+  // implemented
+  // public static final Set<Integer> CAN_RUN = Set.of(REMOVE_DEPRECATIONS_FOR_VERSION_3,
+  // CURRENT_VERSION);","[{'comment': '```suggestion\r\n  // TODO - this disables upgrades until https://github.com/apache/accumulo/issues/3768 is done\r\n  // public static final Set<Integer> CAN_RUN = Set.of(REMOVE_DEPRECATIONS_FOR_VERSION_3,\r\n  // CURRENT_VERSION);\r\n```', 'commenter': 'ctubbsii'}]"
3779,server/base/src/test/java/org/apache/accumulo/server/ServerContextTest.java,"@@ -135,7 +135,9 @@ public void testCanRun() {
     // ensure this fails with older versions; the oldest supported version is hard-coded here
     // to ensure we don't unintentionally break upgrade support; changing this should be a conscious
     // decision and this check will ensure we don't overlook it
-    final int oldestSupported = AccumuloDataVersion.ROOT_TABLET_META_CHANGES;
+
+    // TODO basically disable check until upgrade to 3.1 is supported.
+    final int oldestSupported = AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;","[{'comment': 'Instead of doing this, you should use `@Disabled`\r\n\r\n```suggestion\r\n    final int oldestSupported = AccumuloDataVersion.ROOT_TABLET_META_CHANGES;\r\n```\r\n\r\nOn the method, add:\r\n\r\n\r\n```java\r\n@Disabled(""must fix https://github.com/apache/accumulo/issues/3768 to re-enable"")\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'I thought of using disabled - opted to make this test fail instead.  This way, as the code is changed to support the necessary version, this test will fail and be impossible to overlook.  ', 'commenter': 'EdColeman'}, {'comment': ""When you say you opted to make this test fail instead, do you mean when the fix for #3768 is eventually done? It looks like you have it set to pass right now, but should fail after the upgrade is done on line 146 with:\r\n\r\n\r\n```java\r\n    IntStream.of(oldestSupported - 1, currentVersion + 1).forEach(shouldFail);\r\n```\r\n\r\nIs that what you mean?\r\nIf so, I think that's fine. Otherwise, I'm confused."", 'commenter': 'ctubbsii'}, {'comment': 'Yes - when adding the supporting versions to `UpgradeCoordinator` the line \r\n```\r\n IntStream.of(oldestSupported - 1, currentVersion + 1).forEach(shouldFail);\r\n```\r\ncauses a hard fail.  (My line numbers may be off by one.)\r\n\r\n```\r\norg.opentest4j.AssertionFailedError: Expected java.lang.IllegalStateException to be thrown, but nothing was thrown.\r\n\r\n\tat org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:152)\r\n\tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:73)\r\n\tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:35)\r\n\tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3083)\r\n\tat org.apache.accumulo.server.ServerContextTest.lambda$testCanRun$3(ServerContextTest.java:144)\r\n\tat java.base/java.util.Spliterators$IntArraySpliterator.forEachRemaining(Spliterators.java:1032)\r\n\tat java.base/java.util.stream.IntPipeline$Head.forEach(IntPipeline.java:593)\r\n\tat org.apache.accumulo.server.ServerContextTest.testCanRun(ServerContextTest.java:147)\r\n```\r\n\r\n', 'commenter': 'EdColeman'}, {'comment': 'Okay, in that case, I think it would be more clear that it is disabled by referencing the current version instead of the name of the version. Also, can add the commented out version that it needs to go back to when it is fixed later.\r\n\r\n```suggestion\r\n    // TODO basically disable check until upgrade to 3.1 is supported.\r\n    // final int oldestSupported = AccumuloDataVersion.ROOT_TABLET_META_CHANGES;\r\n    final int oldestSupported = AccumuloDataVersion.CURRENT_VERSION;\r\n```', 'commenter': 'ctubbsii'}, {'comment': ""I don't think you included my suggestion to make the test point directly to the `CURRENT_VERSION` alias. You left it pointing to `METADATA_FILE_JSON_ENCODING`. I know they are the same value, but I think that leaving it pointing to the specific named version implies there's some reason that specific version was picked. But the reason it was picked is because it's the current version. So, it's more clear to just directly point to `CURRENT_VERSION`. But it doesn't matter that much, because this should all get backed out once the upgrade code is added, so I don't think it's worth changing now. I just wanted to explain my reasoning a bit more."", 'commenter': 'ctubbsii'}]"
3779,server/base/src/main/java/org/apache/accumulo/server/AccumuloDataVersion.java,"@@ -71,7 +77,10 @@ public static int get() {
     return CURRENT_VERSION;
   }
 
-  public static final Set<Integer> CAN_RUN = Set.of(ROOT_TABLET_META_CHANGES, CURRENT_VERSION);
+  // TODO - this disables upgrades until https://github.com/apache/accumulo/issues/3768 is done
+  // public static final Set<Integer> CAN_RUN = Set.of(REMOVE_DEPRECATIONS_FOR_VERSION_3,","[{'comment': 'If I correctly remember how this works, this should include the data version from 2.1 and 3.0, when it is uncommented later.\r\n\r\n```suggestion\r\n  // public static final Set<Integer> CAN_RUN = Set.of(ROOT_TABLET_META_CHANGES, REMOVE_DEPRECATIONS_FOR_VERSION_3,\r\n```', 'commenter': 'ctubbsii'}, {'comment': 'I will make the change.  As luck would have it I already have the correct code in another branch ;-)', 'commenter': 'EdColeman'}]"
3792,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -77,9 +77,10 @@ public enum Property {
       ""1.6.0""),
   RPC_SSL_CIPHER_SUITES(""rpc.ssl.cipher.suites"", """", PropertyType.STRING,
       ""Comma separated list of cipher suites that can be used by accepted connections"", ""1.6.1""),
-  RPC_SSL_ENABLED_PROTOCOLS(""rpc.ssl.server.enabled.protocols"", ""TLSv1.2"", PropertyType.STRING,
+  RPC_SSL_ENABLED_PROTOCOLS(""rpc.ssl.server.enabled.protocols"", ""TLSv1.2,TLSv1.3"",","[{'comment': 'We can just select TLSv1.3 by default. If users have need for both, they can enable that themselves (for example, to support a mix of different versions of Accumulo clients configured differently).\r\n\r\n```suggestion\r\n  RPC_SSL_ENABLED_PROTOCOLS(""rpc.ssl.server.enabled.protocols"", ""TLSv1.3"",\r\n```', 'commenter': 'ctubbsii'}]"
3792,core/src/main/java/org/apache/accumulo/core/conf/Property.java,"@@ -797,8 +798,8 @@ public enum Property {
       ""A comma-separated list of disallowed SSL Ciphers, see""
           + "" monitor.ssl.include.ciphers to allow ciphers"",
       ""1.6.1""),
-  MONITOR_SSL_INCLUDE_PROTOCOLS(""monitor.ssl.include.protocols"", ""TLSv1.2"", PropertyType.STRING,
-      ""A comma-separate list of allowed SSL protocols"", ""1.5.3""),
+  MONITOR_SSL_INCLUDE_PROTOCOLS(""monitor.ssl.include.protocols"", ""TLSv1.2,TLSv1.3"",","[{'comment': 'Same comment as above. Moderately up-to-date browsers will be able to use TLS 1.3, so this shouldn\'t be an issue as a default config. Users can change it if they want.\r\n\r\n```suggestion\r\n  MONITOR_SSL_INCLUDE_PROTOCOLS(""monitor.ssl.include.protocols"", ""TLSv1.3"",\r\n```', 'commenter': 'ctubbsii'}]"
3792,test/src/main/java/org/apache/accumulo/test/functional/ReadWriteIT.java,"@@ -154,7 +154,8 @@ public void sunnyDay() throws Exception {
         if (monitorSslKeystore != null && !monitorSslKeystore.isEmpty()) {
           log.info(
               ""Using HTTPS since monitor ssl keystore configuration was observed in accumulo configuration"");
-          SSLContext ctx = SSLContext.getInstance(""TLSv1.2"");
+          SSLContext ctx =
+              SSLContext.getInstance(Property.RPC_SSL_CLIENT_PROTOCOL.getDefaultValue());","[{'comment': ""I'm not sure, but this might still fit on one line if you use `var`. Could also statically import the property name, so it doesn't take up as much space on the line."", 'commenter': 'ctubbsii'}]"
3813,core/src/main/java/org/apache/accumulo/core/util/compaction/ExternalCompactionUtil.java,"@@ -98,17 +98,12 @@ public static String getHostPortString(HostAndPort address) {
    */
   public static Optional<HostAndPort> findCompactionCoordinator(ClientContext context) {
     final String lockPath = context.getZooKeeperRoot() + Constants.ZCOORDINATOR_LOCK;
-    try {
-      var zk = ZooSession.getAnonymousSession(context.getZooKeepers(),
-          context.getZooKeepersSessionTimeOut());
-      byte[] address = ServiceLock.getLockData(zk, ServiceLock.path(lockPath));
-      if (null == address) {
-        return Optional.empty();
-      }
-      return Optional.of(HostAndPort.fromString(new String(address)));
-    } catch (KeeperException | InterruptedException e) {
-      throw new RuntimeException(e);
+    byte[] address =
+        ServiceLock.getLockData(context.getZooCache(), ServiceLock.path(lockPath), new ZcStat());
+    if (null == address) {
+      return Optional.empty();
     }
+    return Optional.of(HostAndPort.fromString(new String(address)));","[{'comment': ""There's a couple of different variants of this that are slightly more concise, some could inline the lockPath as well. I couldn't get it more concise than 3 lines. But, there is a variant of this that would inline the lockPath and still be only 3 lines total. I'm not sure if you want to go with any of them.\r\n\r\nBut, at a minimum, the String should be interpreting the address as UTF-8, because that's how it should have been persisted.\r\n\r\n```suggestion\r\n    Optional<byte[]> address = Optional.ofNullable(\r\n        ServiceLock.getLockData(context.getZooCache(), ServiceLock.path(lockPath), new ZcStat()));\r\n    return address.map(bytes -> new String(bytes, UTF_8)).map(HostAndPort::fromString);\r\n```\r\n"", 'commenter': 'ctubbsii'}, {'comment': ""I think I'm just going to add the `UTF_8` argument to the String constructor. By modifying this to call `Optional.map` twice we are incurring 2 calls to `Objects.requireNonNull`, `Optional.isPresent`, and `Optional.empty` vs one null-check and one call to `Optional.empty`. Your suggestion may reduce the lines of code, but it's not as efficient in achieving the same result."", 'commenter': 'dlmarion'}]"
3817,pom.xml,"@@ -981,8 +954,6 @@
                 <unused>org.apache.logging.log4j:log4j-1.2-api:jar:*</unused>
                 <unused>org.apache.logging.log4j:log4j-slf4j2-impl:jar:*</unused>
                 <unused>org.apache.logging.log4j:log4j-web:jar:*</unused>
-                <!-- This should be removed upon completion of migrating junit 4 to 5 -->
-                <unused>org.junit.vintage:junit-vintage-engine:jar:*</unused>","[{'comment': 'I think there is also an entry in the `ignoredUsedUndeclaredDependencies` section that can be removed like this one.', 'commenter': 'DomGarguilo'}, {'comment': 'Additionally in the pom, in the `maven-checkstyle-plugin` section, we might be able to remove some rules that refference junit4 imports or add one/some that ensure only junit jupiter imports are used.', 'commenter': 'DomGarguilo'}, {'comment': 'So removing powermock mean we can remove usage of all old junit apis?  If so that is nice, it would be nice if the old stuff was no longer there in autocomplete in IDE.', 'commenter': 'keith-turner'}]"
3817,server/compaction-coordinator/src/test/java/org/apache/accumulo/coordinator/CompactionCoordinatorTest.java,"@@ -1,657 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * ""License""); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   https://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.accumulo.coordinator;
-
-import static org.easymock.EasyMock.anyObject;
-import static org.easymock.EasyMock.expect;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
-import static org.junit.Assert.assertTrue;
-
-import java.net.UnknownHostException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Set;
-import java.util.TreeMap;
-import java.util.TreeSet;
-import java.util.UUID;
-import java.util.concurrent.ScheduledThreadPoolExecutor;
-
-import org.apache.accumulo.core.cli.ConfigOpts;
-import org.apache.accumulo.core.clientImpl.thrift.TInfo;
-import org.apache.accumulo.core.clientImpl.thrift.ThriftSecurityException;
-import org.apache.accumulo.core.compaction.thrift.TExternalCompaction;
-import org.apache.accumulo.core.conf.DefaultConfiguration;
-import org.apache.accumulo.core.dataImpl.thrift.TKeyExtent;
-import org.apache.accumulo.core.metadata.TServerInstance;
-import org.apache.accumulo.core.metadata.schema.ExternalCompactionId;
-import org.apache.accumulo.core.rpc.ThriftUtil;
-import org.apache.accumulo.core.securityImpl.thrift.TCredentials;
-import org.apache.accumulo.core.tabletserver.thrift.TCompactionQueueSummary;
-import org.apache.accumulo.core.tabletserver.thrift.TCompactionStats;
-import org.apache.accumulo.core.tabletserver.thrift.TExternalCompactionJob;
-import org.apache.accumulo.core.tabletserver.thrift.TabletServerClientService;
-import org.apache.accumulo.core.tabletserver.thrift.TabletServerClientService.Client;
-import org.apache.accumulo.core.trace.TraceUtil;
-import org.apache.accumulo.core.util.compaction.ExternalCompactionUtil;
-import org.apache.accumulo.core.util.compaction.RunningCompaction;
-import org.apache.accumulo.server.AbstractServer;
-import org.apache.accumulo.server.ServerContext;
-import org.apache.accumulo.server.manager.LiveTServerSet;
-import org.apache.accumulo.server.rpc.ServerAddress;
-import org.apache.accumulo.server.security.AuditedSecurityOperation;
-import org.apache.thrift.transport.TTransportException;
-import org.apache.zookeeper.KeeperException;
-import org.junit.Test;
-import org.junit.runner.RunWith;
-import org.powermock.api.easymock.PowerMock;
-import org.powermock.core.classloader.annotations.PowerMockIgnore;
-import org.powermock.core.classloader.annotations.PrepareForTest;
-import org.powermock.core.classloader.annotations.SuppressStaticInitializationFor;
-import org.powermock.modules.junit4.PowerMockRunner;
-
-import com.google.common.collect.Sets;
-import com.google.common.net.HostAndPort;
-
-@RunWith(PowerMockRunner.class)
-@PrepareForTest({CompactionCoordinator.class, DeadCompactionDetector.class, ThriftUtil.class,
-    ExternalCompactionUtil.class})
-@SuppressStaticInitializationFor({""org.apache.log4j.LogManager""})
-@PowerMockIgnore({""org.slf4j.*"", ""org.apache.logging.*"", ""org.apache.log4j.*"",
-    ""org.apache.commons.logging.*"", ""org.xml.*"", ""javax.xml.*"", ""org.w3c.dom.*"",
-    ""com.sun.org.apache.xerces.*""})
-public class CompactionCoordinatorTest {
-
-  public class TestCoordinator extends CompactionCoordinator {
-
-    private final ServerContext context;
-    private final ServerAddress client;
-    private final Client tabletServerClient;
-
-    private Set<ExternalCompactionId> metadataCompactionIds = null;
-
-    protected TestCoordinator(CompactionFinalizer finalizer, LiveTServerSet tservers,
-        ServerAddress client, Client tabletServerClient, ServerContext context,
-        AuditedSecurityOperation security) {
-      super(new ConfigOpts(), new String[] {}, context.getConfiguration());
-      this.compactionFinalizer = finalizer;
-      this.tserverSet = tservers;
-      this.client = client;
-      this.tabletServerClient = tabletServerClient;
-      this.context = context;
-      this.security = security;
-    }
-
-    @Override
-    protected void startDeadCompactionDetector() {}
-
-    @Override
-    protected long getTServerCheckInterval() {
-      this.shutdown = true;
-      return 0L;
-    }
-
-    @Override
-    protected void startCompactionCleaner(ScheduledThreadPoolExecutor schedExecutor) {}
-
-    @Override
-    protected CompactionFinalizer createCompactionFinalizer(ScheduledThreadPoolExecutor stpe) {
-      return null;
-    }
-
-    @Override
-    protected LiveTServerSet createLiveTServerSet() {
-      return null;
-    }
-
-    @Override
-    protected void setupSecurity() {}
-
-    @Override
-    protected void printStartupMsg() {}
-
-    @Override
-    public ServerContext getContext() {
-      return this.context;
-    }
-
-    @Override
-    protected void getCoordinatorLock(HostAndPort clientAddress)
-        throws KeeperException, InterruptedException {}
-
-    @Override
-    protected ServerAddress startCoordinatorClientService() throws UnknownHostException {
-      return client;
-    }
-
-    @Override
-    protected Client getTabletServerConnection(TServerInstance tserver) throws TTransportException {
-      return tabletServerClient;
-    }
-
-    @Override
-    public void compactionCompleted(TInfo tinfo, TCredentials credentials,
-        String externalCompactionId, TKeyExtent textent, TCompactionStats stats)
-        throws ThriftSecurityException {}
-
-    @Override
-    public void compactionFailed(TInfo tinfo, TCredentials credentials, String externalCompactionId,
-        TKeyExtent extent) throws ThriftSecurityException {}
-
-    void setMetadataCompactionIds(Set<ExternalCompactionId> mci) {
-      metadataCompactionIds = mci;
-    }
-
-    @Override
-    protected Set<ExternalCompactionId> readExternalCompactionIds() {
-      if (metadataCompactionIds == null) {
-        return RUNNING_CACHE.keySet();
-      } else {
-        return metadataCompactionIds;
-      }
-    }
-
-    public Map<String,TreeMap<Short,TreeSet<TServerInstance>>> getQueues() {
-      return CompactionCoordinator.QUEUE_SUMMARIES.QUEUES;
-    }
-
-    public Map<TServerInstance,Set<QueueAndPriority>> getIndex() {
-      return CompactionCoordinator.QUEUE_SUMMARIES.INDEX;
-    }
-
-    public Map<ExternalCompactionId,RunningCompaction> getRunning() {
-      return RUNNING_CACHE;
-    }
-
-    public void resetInternals() {
-      getQueues().clear();
-      getIndex().clear();
-      getRunning().clear();
-      metadataCompactionIds = null;
-    }
-
-  }
-
-  @Test
-  public void testCoordinatorColdStartNoCompactions() throws Exception {
-    PowerMock.resetAll();
-    PowerMock.suppress(PowerMock.constructor(AbstractServer.class));
-    PowerMock.suppress(PowerMock.methods(ThriftUtil.class, ""returnClient""));
-    PowerMock.suppress(PowerMock.methods(DeadCompactionDetector.class, ""detectDeadCompactions"",
-        ""detectDanglingFinalStateMarkers""));
-
-    ServerContext context = PowerMock.createNiceMock(ServerContext.class);
-    expect(context.getConfiguration()).andReturn(DefaultConfiguration.getInstance()).anyTimes();
-
-    PowerMock.mockStatic(ExternalCompactionUtil.class);
-    List<RunningCompaction> runningCompactions = new ArrayList<>();
-    expect(ExternalCompactionUtil.getCompactionsRunningOnCompactors(context))
-        .andReturn(runningCompactions);
-
-    CompactionFinalizer finalizer = PowerMock.createNiceMock(CompactionFinalizer.class);
-    LiveTServerSet tservers = PowerMock.createNiceMock(LiveTServerSet.class);
-    expect(tservers.getCurrentServers()).andReturn(Collections.emptySet()).anyTimes();
-
-    ServerAddress client = PowerMock.createNiceMock(ServerAddress.class);
-    HostAndPort address = HostAndPort.fromString(""localhost:10240"");
-    expect(client.getAddress()).andReturn(address).anyTimes();
-
-    TServerInstance tsi = PowerMock.createNiceMock(TServerInstance.class);
-    expect(tsi.getHostPort()).andReturn(""localhost:9997"").anyTimes();
-
-    TabletServerClientService.Client tsc =
-        PowerMock.createNiceMock(TabletServerClientService.Client.class);
-    expect(tsc.getCompactionQueueInfo(anyObject(), anyObject())).andReturn(Collections.emptyList())
-        .anyTimes();
-
-    AuditedSecurityOperation security = PowerMock.createNiceMock(AuditedSecurityOperation.class);
-
-    PowerMock.replayAll();
-
-    var coordinator = new TestCoordinator(finalizer, tservers, client, tsc, context, security);
-    coordinator.resetInternals();
-    assertEquals(0, coordinator.getQueues().size());
-    assertEquals(0, coordinator.getIndex().size());
-    assertEquals(0, coordinator.getRunning().size());
-    coordinator.run();
-    assertEquals(0, coordinator.getQueues().size());
-    assertEquals(0, coordinator.getIndex().size());
-    assertEquals(0, coordinator.getRunning().size());
-
-    PowerMock.verifyAll();
-    coordinator.resetInternals();
-    coordinator.close();
-  }
-
-  @Test
-  public void testCoordinatorColdStart() throws Exception {
-    PowerMock.resetAll();
-    PowerMock.suppress(PowerMock.constructor(AbstractServer.class));
-    PowerMock.suppress(PowerMock.methods(ThriftUtil.class, ""returnClient""));
-    PowerMock.suppress(PowerMock.methods(DeadCompactionDetector.class, ""detectDeadCompactions"",
-        ""detectDanglingFinalStateMarkers""));
-
-    ServerContext context = PowerMock.createNiceMock(ServerContext.class);
-    expect(context.getConfiguration()).andReturn(DefaultConfiguration.getInstance()).anyTimes();
-
-    TCredentials creds = PowerMock.createNiceMock(TCredentials.class);
-    expect(context.rpcCreds()).andReturn(creds);
-
-    PowerMock.mockStatic(ExternalCompactionUtil.class);
-    List<RunningCompaction> runningCompactions = new ArrayList<>();
-    expect(ExternalCompactionUtil.getCompactionsRunningOnCompactors(context))
-        .andReturn(runningCompactions);
-
-    CompactionFinalizer finalizer = PowerMock.createNiceMock(CompactionFinalizer.class);
-    LiveTServerSet tservers = PowerMock.createNiceMock(LiveTServerSet.class);
-    TServerInstance instance = PowerMock.createNiceMock(TServerInstance.class);
-    expect(tservers.getCurrentServers()).andReturn(Collections.singleton(instance)).once();
-
-    ServerAddress client = PowerMock.createNiceMock(ServerAddress.class);
-    HostAndPort address = HostAndPort.fromString(""localhost:10240"");
-    expect(client.getAddress()).andReturn(address).anyTimes();
-
-    TServerInstance tsi = PowerMock.createNiceMock(TServerInstance.class);
-    expect(tsi.getHostPort()).andReturn(""localhost:9997"").anyTimes();
-
-    TabletServerClientService.Client tsc =
-        PowerMock.createNiceMock(TabletServerClientService.Client.class);
-    TCompactionQueueSummary queueSummary = PowerMock.createNiceMock(TCompactionQueueSummary.class);
-    expect(tsc.getCompactionQueueInfo(anyObject(), anyObject()))
-        .andReturn(Collections.singletonList(queueSummary)).anyTimes();
-    expect(queueSummary.getQueue()).andReturn(""R2DQ"").anyTimes();
-    expect(queueSummary.getPriority()).andReturn((short) 1).anyTimes();
-
-    AuditedSecurityOperation security = PowerMock.createNiceMock(AuditedSecurityOperation.class);
-
-    PowerMock.replayAll();
-
-    var coordinator = new TestCoordinator(finalizer, tservers, client, tsc, context, security);
-    coordinator.resetInternals();
-    assertEquals(0, coordinator.getQueues().size());
-    assertEquals(0, coordinator.getIndex().size());
-    assertEquals(0, coordinator.getRunning().size());
-    coordinator.run();
-    assertEquals(1, coordinator.getQueues().size());
-    QueueAndPriority qp = QueueAndPriority.get(""R2DQ"".intern(), (short) 1);","[{'comment': 'we are losing `R2DQ`', 'commenter': 'keith-turner'}]"
3817,src/build/ci/find-unapproved-junit.sh,"@@ -38,8 +36,10 @@ function findalljunitproblems() {
   fi
   # find any new classes using something other than the jupiter API, except those allowed
   grep ""$opts"" --include='*.java' 'org[.]junit[.](?!jupiter)' | grep -Pv ""^(${ALLOWED_PIPE_SEP//./[.]})\$""
-  # find any uses of the jupiter API in the allowed vintage classes
-  grep ""$opts"" 'org[.]junit[.]jupiter' ""${ALLOWED[@]}""
+  if ((${#ALLOWED[@]} != 0)); then","[{'comment': 'Could be a follow on issue, wondering if the ALLOWED set should just be removed from the script since its empty.  Would simplify the script.', 'commenter': 'keith-turner'}, {'comment': 'I left it there just in case we needed it in the future for some reason. We have the same thing in accumulo-access with this script.', 'commenter': 'dlmarion'}]"
3817,pom.xml,"@@ -1092,26 +1055,14 @@
                   <property name=""format"" value=""org[.]apache[.]commons[.]math[.]"" />
                   <property name=""message"" value=""Use commons-math3 (org.apache.commons.math3.*)"" />
                 </module>
-                <module name=""RegexpSinglelineJava"">
-                  <property name=""format"" value=""junit[.]framework[.]TestCase"" />
-                  <property name=""message"" value=""Use JUnit4+ @Test annotation instead of TestCase"" />
-                </module>","[{'comment': ""I don't think this line should have been removed. This was checking to make sure we were *not* using JUnit 3. Instead of removing this, we should have added the checkstyle to make sure we were *not* using JUnit 3 **or** 4. Then, we could get rid of the junit script that was being run during GitHub Actions, as checkstyle would have covered it."", 'commenter': 'ctubbsii'}]"
3876,core/src/main/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadata.java,"@@ -59,11 +60,14 @@ public class RootTabletMetadata {
   };
 
   // JSON Mapping Version 1. Released with Accumulo version 2.1.0
-  private static final int VERSION = 1;
+  private static final int VERSION_1 = 1;
+  // JSON Mapping Version 1. Released with Accumulo version 3,1","[{'comment': '```suggestion\r\n  // JSON Mapping Version 2. Released with Accumulo version 3,1\r\n```', 'commenter': 'cshannon'}, {'comment': 'Addressed in e8ae63227647', 'commenter': 'EdColeman'}]"
3876,core/src/main/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadata.java,"@@ -73,10 +77,27 @@ private static class Data {
      */
     private final TreeMap<String,TreeMap<String,String>> columnValues;
 
-    public Data(int version, TreeMap<String,TreeMap<String,String>> columnValues) {
+    private Data(int version, TreeMap<String,TreeMap<String,String>> columnValues) {
       this.version = version;
       this.columnValues = columnValues;
     }
+
+    public int getVersion() {
+      return version;
+    }
+
+    /**
+     * For external use, return a copy so the original remains immutable.
+     */
+    public TreeMap<String,TreeMap<String,String>> getColumnValues() {
+      return new TreeMap<>(columnValues);","[{'comment': ""Instead of making a copy and using TreeMap I would recommend using an Immutable map and then we don't need to worry about copying data (assuming the values are supposed to be immutable). Since it's a SortedMap you could look at using Guava's `ImmutableSortedMap`. It might be a little tricky because the map contains another map so you may need to make them both immutable depending on how it's used. You also could just wrap the map(s) using `UnmodifiableMap`"", 'commenter': 'cshannon'}, {'comment': 'With refactoring, this method was not needed outside of the class and the method was removed in e8ae63227647', 'commenter': 'EdColeman'}]"
3876,core/src/test/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadataTest.java,"@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.metadata.schema;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class RootTabletMetadataTest {
+  private static final Logger LOG = LoggerFactory.getLogger(RootTabletMetadataTest.class);
+
+  @Test
+  public void convertRoot1File() {
+    String root21ZkData =
+        ""{\""version\"":1,\""columnValues\"":{\""file\"":{\""hdfs://localhost:8020/accumulo/tables/+r/root_tablet/A000000v.rf\"":\""1368,61\""},\""last\"":{\""100025091780006\"":\""localhost:9997\""},\""loc\"":{\""100025091780006\"":\""localhost:9997\""},\""srv\"":{\""dir\"":\""root_tablet\"",\""flush\"":\""3\"",\""lock\"":\""tservers/localhost:9997/zlock#9db8961a-4ee9-400e-8e80-3353148baadd#0000000000$100025091780006\"",\""time\"":\""L53\""},\""~tab\"":{\""~pr\"":\""\\u0000\""}}}"";
+
+    RootTabletMetadata rtm = RootTabletMetadata.upgrade(root21ZkData);
+    LOG.debug(""converted column values: {}"", rtm.toTabletMetadata().getFiles());
+    assertEquals(1, rtm.toTabletMetadata().getFiles().size());
+
+    LOG.info(""FILES: {}"", rtm.toTabletMetadata().getFilesMap());
+  }
+
+  @Test
+  public void convertRoot2Files() {
+    String root212ZkData2Files =
+        ""{\""version\"":1,\""columnValues\"":{\""file\"":{\""hdfs://localhost:8020/accumulo/tables/+r/root_tablet/00000_00000.rf\"":\""0,0\"",\""hdfs://localhost:8020/accumulo/tables/+r/root_tablet/F000000c.rf\"":\""926,18\""},\""last\"":{\""10001a84d7d0005\"":\""localhost:9997\""},\""loc\"":{\""10001a84d7d0005\"":\""localhost:9997\""},\""srv\"":{\""dir\"":\""root_tablet\"",\""flush\"":\""2\"",\""lock\"":\""tservers/localhost:9997/zlock#d21adaa4-0f97-4004-9ff8-cce9dbb6687f#0000000000$10001a84d7d0005\"",\""time\"":\""L6\""},\""~tab\"":{\""~pr\"":\""\\u0000\""}}}\n"";
+
+    RootTabletMetadata rtm = RootTabletMetadata.upgrade(root212ZkData2Files);
+    LOG.debug(""converted column values: {}"", rtm.toTabletMetadata());
+    assertEquals(2, rtm.toTabletMetadata().getFiles().size());","[{'comment': 'Besides just checking size we should also verify correctness. It would be good to verify the files match the previous path after upgrade and also the ranges are infinite, etc.', 'commenter': 'cshannon'}, {'comment': 'Added contains checks in e8ae63227647', 'commenter': 'EdColeman'}]"
3876,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java,"@@ -0,0 +1,259 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.metadata.RootTable.ZROOT_TABLET;
+import static org.apache.accumulo.server.AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;
+
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.schema.Ample;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;
+import org.apache.accumulo.core.metadata.schema.UpgraderDeprecatedConstants;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.core.util.ComparablePair;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.hadoop.io.Text;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.gson.JsonSyntaxException;
+
+public class Upgrader11to12 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader11to12.class);
+
+  @Override
+  public void upgradeZookeeper(@NonNull ServerContext context) {
+    log.debug(""Upgrade ZooKeeper: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootBase = ZooUtil.getRoot(context.getInstanceID()) + ZROOT_TABLET;
+
+    try {
+      var zrw = context.getZooReaderWriter();
+      Stat stat = new Stat();
+      byte[] rootData = zrw.getData(rootBase, stat);
+
+      String json = new String(rootData, UTF_8);
+      if (RootTabletMetadata.Data.needsConversion(json)) {
+        RootTabletMetadata rtm = RootTabletMetadata.upgrade(json);
+        zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper due to interrupt"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper because of ZooKeeper exception"", ex);
+    }
+  }
+
+  @Override
+  public void upgradeRoot(@NonNull ServerContext context) {
+    log.debug(""Upgrade root: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+
+    var rootName = Ample.DataLevel.METADATA.metaTable();
+    var filesToConvert = getFileReferences(context, rootName);
+    convertFileReferences(context, rootName, filesToConvert);
+    deleteObsoleteReferences(context, rootName);
+  }
+
+  @Override
+  public void upgradeMetadata(@NonNull ServerContext context) {
+    log.debug(""Upgrade metadata: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+
+    var metaName = Ample.DataLevel.USER.metaTable();
+    var filesToConvert = getFileReferences(context, metaName);
+    convertFileReferences(context, metaName, filesToConvert);
+    deleteObsoleteReferences(context, metaName);
+  }
+
+  private Map<ComparablePair<KeyExtent,String>,Value>
+      getFileReferences(@NonNull final ServerContext context, @NonNull final String tableName) {
+
+    log.trace(""gather file references for table: {}"", tableName);
+
+    Map<ComparablePair<KeyExtent,String>,Value> filesToConvert = new TreeMap<>();
+
+    try (Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {
+      scanner.fetchColumnFamily(MetadataSchema.TabletsSection.DataFileColumnFamily.STR_NAME);
+      scanner.forEach((k, v) -> {
+        KeyExtent ke = KeyExtent.fromMetaRow(k.getRow());
+        String file = k.getColumnQualifier().toString();
+        // filter out references that are in the correct format already.
+        if (fileNeedsConversion(file)) {
+          var prev = filesToConvert.put(new ComparablePair<>(ke, file), v);
+          if (prev != null) {
+            throw new IllegalStateException(
+                ""upgrade for table: "" + tableName + "" aborted, would have missed: "" + prev);
+          }
+        }
+      });
+    } catch (TableNotFoundException ex) {
+      throw new IllegalStateException(""failed to read metadata table for upgrade"", ex);
+    }
+    log.debug(""Number of files to convert for table: {}, number of files: {}"", tableName,
+        filesToConvert.size());
+    return filesToConvert;
+  }
+
+  private void convertFileReferences(final ServerContext context, final String tableName,
+      Map<ComparablePair<KeyExtent,String>,Value> filesToConvert) {
+
+    // not using ample to avoid StoredTabletFile because old file ref is incompatible
+    try (AccumuloClient c = Accumulo.newClient().from(context.getProperties()).build();
+        BatchWriter batchWriter = c.createBatchWriter(tableName)) {
+      filesToConvert.forEach((refPair, value) -> {
+        try {
+          log.trace(""update file reference for table: {}. row: {} to: {} value: {}"", tableName,
+              refPair.getFirst().toMetaRow(), refPair.getSecond(), value);
+
+          var row = refPair.getFirst().toMetaRow();
+          var fileJson = new Text(StoredTabletFile.serialize(refPair.getSecond(), new Range()));","[{'comment': '```suggestion\r\n          var fileJson = StoredTabletFile.of(new Path(refPair.getSecond())).getMetadataText();\r\n```\r\nBoth ways are equivalent so this is optional but this is a little nicer not having to create the empty range.', 'commenter': 'cshannon'}, {'comment': 'Changed in e8ae63227647 (along with other changes)', 'commenter': 'EdColeman'}]"
3876,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java,"@@ -0,0 +1,259 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.metadata.RootTable.ZROOT_TABLET;
+import static org.apache.accumulo.server.AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;
+
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.schema.Ample;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;
+import org.apache.accumulo.core.metadata.schema.UpgraderDeprecatedConstants;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.core.util.ComparablePair;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.hadoop.io.Text;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.gson.JsonSyntaxException;
+
+public class Upgrader11to12 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader11to12.class);
+
+  @Override
+  public void upgradeZookeeper(@NonNull ServerContext context) {
+    log.debug(""Upgrade ZooKeeper: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootBase = ZooUtil.getRoot(context.getInstanceID()) + ZROOT_TABLET;
+
+    try {
+      var zrw = context.getZooReaderWriter();
+      Stat stat = new Stat();
+      byte[] rootData = zrw.getData(rootBase, stat);
+
+      String json = new String(rootData, UTF_8);
+      if (RootTabletMetadata.Data.needsConversion(json)) {
+        RootTabletMetadata rtm = RootTabletMetadata.upgrade(json);
+        zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper due to interrupt"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper because of ZooKeeper exception"", ex);
+    }
+  }
+
+  @Override
+  public void upgradeRoot(@NonNull ServerContext context) {
+    log.debug(""Upgrade root: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+
+    var rootName = Ample.DataLevel.METADATA.metaTable();
+    var filesToConvert = getFileReferences(context, rootName);
+    convertFileReferences(context, rootName, filesToConvert);
+    deleteObsoleteReferences(context, rootName);
+  }
+
+  @Override
+  public void upgradeMetadata(@NonNull ServerContext context) {
+    log.debug(""Upgrade metadata: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+
+    var metaName = Ample.DataLevel.USER.metaTable();
+    var filesToConvert = getFileReferences(context, metaName);
+    convertFileReferences(context, metaName, filesToConvert);
+    deleteObsoleteReferences(context, metaName);
+  }
+
+  private Map<ComparablePair<KeyExtent,String>,Value>
+      getFileReferences(@NonNull final ServerContext context, @NonNull final String tableName) {
+
+    log.trace(""gather file references for table: {}"", tableName);
+
+    Map<ComparablePair<KeyExtent,String>,Value> filesToConvert = new TreeMap<>();
+
+    try (Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {
+      scanner.fetchColumnFamily(MetadataSchema.TabletsSection.DataFileColumnFamily.STR_NAME);
+      scanner.forEach((k, v) -> {
+        KeyExtent ke = KeyExtent.fromMetaRow(k.getRow());
+        String file = k.getColumnQualifier().toString();
+        // filter out references that are in the correct format already.
+        if (fileNeedsConversion(file)) {
+          var prev = filesToConvert.put(new ComparablePair<>(ke, file), v);
+          if (prev != null) {
+            throw new IllegalStateException(
+                ""upgrade for table: "" + tableName + "" aborted, would have missed: "" + prev);
+          }
+        }
+      });
+    } catch (TableNotFoundException ex) {
+      throw new IllegalStateException(""failed to read metadata table for upgrade"", ex);
+    }
+    log.debug(""Number of files to convert for table: {}, number of files: {}"", tableName,
+        filesToConvert.size());
+    return filesToConvert;
+  }
+
+  private void convertFileReferences(final ServerContext context, final String tableName,
+      Map<ComparablePair<KeyExtent,String>,Value> filesToConvert) {
+
+    // not using ample to avoid StoredTabletFile because old file ref is incompatible
+    try (AccumuloClient c = Accumulo.newClient().from(context.getProperties()).build();
+        BatchWriter batchWriter = c.createBatchWriter(tableName)) {
+      filesToConvert.forEach((refPair, value) -> {
+        try {
+          log.trace(""update file reference for table: {}. row: {} to: {} value: {}"", tableName,
+              refPair.getFirst().toMetaRow(), refPair.getSecond(), value);
+
+          var row = refPair.getFirst().toMetaRow();
+          var fileJson = new Text(StoredTabletFile.serialize(refPair.getSecond(), new Range()));
+
+          Mutation m = new Mutation(row);
+          m.at().family(MetadataSchema.TabletsSection.DataFileColumnFamily.STR_NAME)
+              .qualifier(fileJson).put(value);
+
+          log.trace(""table: {}, adding: {}"", tableName, m.prettyPrint());
+          batchWriter.addMutation(m);
+
+          Mutation delete = new Mutation(row);
+          delete.at().family(MetadataSchema.TabletsSection.DataFileColumnFamily.STR_NAME)
+              .qualifier(refPair.getSecond()).delete();
+
+          log.trace(""table {}: deleting: {}"", tableName, delete.prettyPrint());
+          batchWriter.addMutation(delete);
+        } catch (MutationsRejectedException ex) {
+          throw new IllegalStateException(""Failed to update file entries for table: "" + tableName,
+              ex);
+        }
+      });
+      batchWriter.flush();
+    } catch (Exception ex) {
+      throw new IllegalStateException(ex);
+    }
+  }
+
+  /**
+   * Removes chopped and external compaction references that have obsolete encoding that is
+   * incompatible with StoredTabletFile json encoding. These references should be rare. If they are
+   * present, some operations likely terminated abnormally under the old version before shutdown.
+   * The deletions are logged so those operations may be re-run if desired.
+   */
+  private void deleteObsoleteReferences(ServerContext context, String tableName) {
+    log.debug(""processing obsolete references for table: {}"", tableName);
+    try (AccumuloClient c = Accumulo.newClient().from(context.getProperties()).build();
+        BatchWriter batchWriter = c.createBatchWriter(tableName)) {
+
+      try (Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {
+        scanner.fetchColumnFamily(UpgraderDeprecatedConstants.ChoppedColumnFamily.STR_NAME);
+        scanner
+            .fetchColumnFamily(MetadataSchema.TabletsSection.ExternalCompactionColumnFamily.NAME);
+        scanner.forEach((k, v) -> {
+          Mutation delete;
+          var family = k.getColumnFamily();
+          if (family.equals(UpgraderDeprecatedConstants.ChoppedColumnFamily.NAME)) {
+            delete = buildChoppedDeleteMutation(k, tableName);
+          } else if (family
+              .equals(MetadataSchema.TabletsSection.ExternalCompactionColumnFamily.NAME)) {
+            delete = buildExternalCompactionDelete(k, tableName);
+          } else {
+            throw new IllegalStateException(
+                ""unexpected column Family: '{}' seen processing obsolete references"");
+          }
+          try {
+            batchWriter.addMutation(delete);
+          } catch (MutationsRejectedException ex) {
+            log.warn(""Failed to delete obsolete reference for table: "" + tableName + "". Ref: ""
+                + delete.prettyPrint()
+                + "". Will try to continue. Ref may need to be manually removed"");
+            log.warn(""Constraint violations: {}"", ex.getConstraintViolationSummaries());
+          }
+        });
+      }
+    } catch (MutationsRejectedException ex) {
+      log.warn(""Failed to delete obsolete reference for table: "" + tableName + "" on close"");
+      log.warn(""Constraint violations: {}"", ex.getConstraintViolationSummaries());
+      throw new IllegalStateException(ex);
+    } catch (Exception ex) {
+      throw new IllegalStateException(
+          ""Processing obsolete referenced for table: "" + tableName + "" failed. Upgrade aborting"",
+          ex);
+    }
+  }
+
+  private Mutation buildChoppedDeleteMutation(final Key k, final String tableName) {
+    Mutation delete = new Mutation(k.getRow()).at()
+        .family(UpgraderDeprecatedConstants.ChoppedColumnFamily.STR_NAME)
+        .qualifier(UpgraderDeprecatedConstants.ChoppedColumnFamily.STR_NAME).delete();
+    log.warn(
+        ""Deleting chopped reference from:{}. Previous split or delete may not have completed cleanly. Ref: {}"",
+        tableName, delete.prettyPrint());
+    return delete;
+  }
+
+  private Mutation buildExternalCompactionDelete(Key k, String tableName) {
+    Mutation delete = new Mutation(k.getRow()).at()
+        .family(MetadataSchema.TabletsSection.ExternalCompactionColumnFamily.NAME)
+        .qualifier(k.getColumnQualifier()).delete();
+    log.warn(
+        ""Deleting external compaction reference from:{}. Previous compaction may not have completed. Ref: {}"",
+        tableName, delete.prettyPrint());
+    return delete;
+  }
+
+  /**
+   * Quick sanity check to see if value has been converted by checking the candidate can be parsed
+   * into a StoredTabletFile. If parsing does not throw a parsing exception. then assume it has been
+   * converted. If validate cannot parse the candidate a JSON, then it is assumed that it has not
+   * been converted. Other parsing errors will propagate as IllegalArgumentExceptions.
+   *
+   * @param candidate a possible file: reference.
+   * @return false if a valid StoredTabletFile, true if it cannot be parsed as JSON. Otherwise,
+   *         propagate an IllegalArgumentException
+   */
+  private boolean fileNeedsConversion(@NonNull final String candidate) {
+    try {
+      StoredTabletFile.validate(candidate);","[{'comment': 'Some trace logging might be good here when validating just in case we need to troubleshoot something weird going on with the metadata not converting, etc.', 'commenter': 'cshannon'}, {'comment': 'Added in e8ae63227647', 'commenter': 'EdColeman'}]"
3876,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java,"@@ -0,0 +1,259 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.metadata.RootTable.ZROOT_TABLET;
+import static org.apache.accumulo.server.AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;
+
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.client.TableNotFoundException;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Range;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.dataImpl.KeyExtent;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.schema.Ample;
+import org.apache.accumulo.core.metadata.schema.MetadataSchema;
+import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;
+import org.apache.accumulo.core.metadata.schema.UpgraderDeprecatedConstants;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.core.util.ComparablePair;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.hadoop.io.Text;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.gson.JsonSyntaxException;
+
+public class Upgrader11to12 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader11to12.class);
+
+  @Override
+  public void upgradeZookeeper(@NonNull ServerContext context) {
+    log.debug(""Upgrade ZooKeeper: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootBase = ZooUtil.getRoot(context.getInstanceID()) + ZROOT_TABLET;
+
+    try {
+      var zrw = context.getZooReaderWriter();
+      Stat stat = new Stat();
+      byte[] rootData = zrw.getData(rootBase, stat);
+
+      String json = new String(rootData, UTF_8);
+      if (RootTabletMetadata.Data.needsConversion(json)) {
+        RootTabletMetadata rtm = RootTabletMetadata.upgrade(json);
+        zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper due to interrupt"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper because of ZooKeeper exception"", ex);
+    }
+  }
+
+  @Override
+  public void upgradeRoot(@NonNull ServerContext context) {
+    log.debug(""Upgrade root: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+
+    var rootName = Ample.DataLevel.METADATA.metaTable();
+    var filesToConvert = getFileReferences(context, rootName);
+    convertFileReferences(context, rootName, filesToConvert);
+    deleteObsoleteReferences(context, rootName);
+  }
+
+  @Override
+  public void upgradeMetadata(@NonNull ServerContext context) {
+    log.debug(""Upgrade metadata: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+
+    var metaName = Ample.DataLevel.USER.metaTable();
+    var filesToConvert = getFileReferences(context, metaName);
+    convertFileReferences(context, metaName, filesToConvert);
+    deleteObsoleteReferences(context, metaName);
+  }
+
+  private Map<ComparablePair<KeyExtent,String>,Value>
+      getFileReferences(@NonNull final ServerContext context, @NonNull final String tableName) {
+
+    log.trace(""gather file references for table: {}"", tableName);
+
+    Map<ComparablePair<KeyExtent,String>,Value> filesToConvert = new TreeMap<>();
+
+    try (Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {
+      scanner.fetchColumnFamily(MetadataSchema.TabletsSection.DataFileColumnFamily.STR_NAME);
+      scanner.forEach((k, v) -> {
+        KeyExtent ke = KeyExtent.fromMetaRow(k.getRow());
+        String file = k.getColumnQualifier().toString();
+        // filter out references that are in the correct format already.
+        if (fileNeedsConversion(file)) {
+          var prev = filesToConvert.put(new ComparablePair<>(ke, file), v);
+          if (prev != null) {
+            throw new IllegalStateException(
+                ""upgrade for table: "" + tableName + "" aborted, would have missed: "" + prev);
+          }
+        }
+      });
+    } catch (TableNotFoundException ex) {
+      throw new IllegalStateException(""failed to read metadata table for upgrade"", ex);
+    }
+    log.debug(""Number of files to convert for table: {}, number of files: {}"", tableName,
+        filesToConvert.size());
+    return filesToConvert;
+  }
+
+  private void convertFileReferences(final ServerContext context, final String tableName,
+      Map<ComparablePair<KeyExtent,String>,Value> filesToConvert) {
+
+    // not using ample to avoid StoredTabletFile because old file ref is incompatible
+    try (AccumuloClient c = Accumulo.newClient().from(context.getProperties()).build();","[{'comment': ""Right now the conversion is happening in two steps where the metadata is first scanned to load the file references into a map and then that is iterated over and updated. One thing I am wondering is if this is a problem if there's a lot of files in the system. I'm not sure what a practical upper bounds would be since depends on a lot of factors but if it's say hundreds of thousands or millions that would be a lot to load at once. We could try and break it up maybe into batches (maybe a tablet at a time or some sort of limit on files at one time and keep track of where you left off). \r\n\r\nI haven't really tried this before also thinking maybe we just modify the files while scanning in the same loop to avoid having to load into a map. The main thing I am not sure of is whether or not that would cause issues with the updates and we'd need to use an isolated scanner but that also buffers the entire row into memory as well. \r\n\r\n@keith-turner any thoughts on this?"", 'commenter': 'cshannon'}, {'comment': 'Changed so that the upgrade file conversion, chop and external compaction removals are done in a single pass through the metadata without creating a map of the file references in e8ae63227647. This reduced the three passes through the metadata that was previous done on first implementation to a single pass and no intermediate storage is used.\r\n\r\nDid not use an IsolatedScanner - not sure if that is necessary or not.', 'commenter': 'EdColeman'}, {'comment': ""I like these changes a lot, this will be much more efficient in terms of having to make a single pass and also with memory usage as the batch writer will flush as needed. \r\n\r\nThe main thing left would be is if we need to use an `IsolatedScanner` or not to buffer the entire row into memory so the changes in place don't cause issues. The code already handles things if already upgraded so the main thing I'd want to avoid is processing the same columns in the row more than once and I am not sure if that could happen or not."", 'commenter': 'cshannon'}, {'comment': '> Did not use an IsolatedScanner - not sure if that is necessary or not.\r\n\r\nIts good to use an isolated scanner for metadata as it ensure that partial changes from a mutation are not seen.', 'commenter': 'keith-turner'}]"
3876,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java,"@@ -0,0 +1,208 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.metadata.RootTable.ZROOT_TABLET;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.UpgraderDeprecatedConstants.ChoppedColumnFamily;
+import static org.apache.accumulo.server.AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.schema.Ample;
+import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.gson.JsonSyntaxException;
+
+public class Upgrader11to12 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader11to12.class);
+
+  @Override
+  public void upgradeZookeeper(@NonNull ServerContext context) {
+    log.debug(""Upgrade ZooKeeper: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootBase = ZooUtil.getRoot(context.getInstanceID()) + ZROOT_TABLET;
+
+    try {
+      var zrw = context.getZooReaderWriter();
+      Stat stat = new Stat();
+      byte[] rootData = zrw.getData(rootBase, stat);
+
+      String json = new String(rootData, UTF_8);
+      if (RootTabletMetadata.Data.needsConversion(json)) {
+        RootTabletMetadata rtm = RootTabletMetadata.upgrade(json);
+        zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper due to interrupt"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper because of ZooKeeper exception"", ex);
+    }
+  }
+
+  @Override
+  public void upgradeRoot(@NonNull ServerContext context) {
+    log.debug(""Upgrade root: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootName = Ample.DataLevel.METADATA.metaTable();
+    processReferences(context, rootName);
+  }
+
+  @Override
+  public void upgradeMetadata(@NonNull ServerContext context) {
+    log.debug(""Upgrade metadata: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var metaName = Ample.DataLevel.USER.metaTable();
+    processReferences(context, metaName);
+  }
+
+  private void processReferences(ServerContext context, String tableName) {
+    // not using ample to avoid StoredTabletFile because old file ref is incompatible
+    try (AccumuloClient c = Accumulo.newClient().from(context.getProperties()).build();
+        BatchWriter batchWriter = c.createBatchWriter(tableName);
+        Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {
+
+      scanner.fetchColumnFamily(DataFileColumnFamily.NAME);
+      scanner.fetchColumnFamily(ChoppedColumnFamily.NAME);
+      scanner.fetchColumnFamily(ExternalCompactionColumnFamily.NAME);
+      scanner.forEach((k, v) -> {
+        var family = k.getColumnFamily();
+        if (family.equals(DataFileColumnFamily.NAME)) {
+          upgradeDataFileCF(k, v, batchWriter, tableName);
+        } else if (family.equals(ChoppedColumnFamily.NAME)) {
+          removeChoppedCF(k, batchWriter, tableName);
+        } else if (family.equals(ExternalCompactionColumnFamily.NAME)) {
+          removeExternalCompactionCF(k, batchWriter, tableName);
+        } else {
+          log.warn(""Received unexpected column family processing references: "" + family);
+        }
+      });
+    } catch (MutationsRejectedException mex) {
+      log.warn(""Failed to update reference for table: "" + tableName);
+      log.warn(""Constraint violations: {}"", mex.getConstraintViolationSummaries());
+      throw new IllegalStateException(""Failed to process table: "" + tableName, mex);
+    } catch (Exception ex) {
+      throw new IllegalStateException(""Failed to process table: "" + tableName, ex);
+    }
+  }
+
+  @VisibleForTesting
+  void upgradeDataFileCF(final Key key, final Value value, final BatchWriter batchWriter,
+      final String tableName) {
+    String file = key.getColumnQualifier().toString();
+    // filter out references if they are in the correct format already.
+    if (fileNeedsConversion(file)) {
+      var fileJson = StoredTabletFile.of(new Path(file)).getMetadataText();
+      try {
+        Mutation update = new Mutation(key.getRow());
+        update.at().family(DataFileColumnFamily.STR_NAME).qualifier(fileJson).put(value);
+        log.trace(""table: {}, adding: {}"", tableName, update.prettyPrint());
+        batchWriter.addMutation(update);
+
+        Mutation delete = new Mutation(key.getRow());
+        delete.at().family(DataFileColumnFamily.STR_NAME).qualifier(file).delete();
+        log.trace(""table {}: deleting: {}"", tableName, delete.prettyPrint());
+        batchWriter.addMutation(delete);
+      } catch (MutationsRejectedException ex) {
+        // include constraint violation info in log - but stop upgrade
+        log.warn(
+            ""Failed to update file reference for table: "" + tableName + "". row: "" + key.getRow());
+        log.warn(""Constraint violations: {}"", ex.getConstraintViolationSummaries());
+        throw new IllegalStateException(""File conversion failed. Aborting upgrade"", ex);
+      }
+    }
+  }
+
+  @VisibleForTesting
+  void removeChoppedCF(final Key key, final BatchWriter batchWriter, final String tableName) {
+    Mutation delete = null;
+    try {
+      delete = new Mutation(key.getRow()).at().family(ChoppedColumnFamily.STR_NAME)
+          .qualifier(ChoppedColumnFamily.STR_NAME).delete();
+      log.warn(
+          ""Deleting chopped reference from:{}. Previous split or delete may not have completed cleanly. Ref: {}"",
+          tableName, delete.prettyPrint());
+      batchWriter.addMutation(delete);
+    } catch (MutationsRejectedException ex) {
+      log.warn(""Failed to delete obsolete chopped CF reference for table: "" + tableName + "". Ref: ""
+          + delete.prettyPrint() + "". Will try to continue. Ref may need to be manually removed"");
+      log.warn(""Constraint violations: {}"", ex.getConstraintViolationSummaries());
+    }
+  }
+
+  @VisibleForTesting
+  void removeExternalCompactionCF(final Key key, final BatchWriter batchWriter,
+      final String tableName) {
+    Mutation delete = null;
+    try {
+      delete = new Mutation(key.getRow()).at().family(ExternalCompactionColumnFamily.NAME)
+          .qualifier(key.getColumnQualifier()).delete();
+      log.warn(","[{'comment': 'Its not unexpected rf problematic to have these external compactions, so the warn level does not seem necessary.\r\n\r\n```suggestion\r\n      log.debug(\r\n```', 'commenter': 'keith-turner'}, {'comment': 'The warn was so that that would stand out in the logs.  This will run once, and debug statements could be easier lost / ignored.  The removal of external compaction markers is because they would likely cause problems because they do not include the fence parameters.', 'commenter': 'EdColeman'}]"
3876,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java,"@@ -0,0 +1,208 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.metadata.RootTable.ZROOT_TABLET;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.UpgraderDeprecatedConstants.ChoppedColumnFamily;
+import static org.apache.accumulo.server.AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.schema.Ample;
+import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.gson.JsonSyntaxException;
+
+public class Upgrader11to12 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader11to12.class);
+
+  @Override
+  public void upgradeZookeeper(@NonNull ServerContext context) {
+    log.debug(""Upgrade ZooKeeper: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootBase = ZooUtil.getRoot(context.getInstanceID()) + ZROOT_TABLET;
+
+    try {
+      var zrw = context.getZooReaderWriter();
+      Stat stat = new Stat();
+      byte[] rootData = zrw.getData(rootBase, stat);
+
+      String json = new String(rootData, UTF_8);
+      if (RootTabletMetadata.Data.needsConversion(json)) {
+        RootTabletMetadata rtm = RootTabletMetadata.upgrade(json);
+        zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());","[{'comment': 'Since the root tablet metadata is so important, may be good to log the root tablet metadata before and after.', 'commenter': 'keith-turner'}, {'comment': 'Added in aa2c468a5b. Use IsolatedScanner and ZooKeeper root dump before / after.', 'commenter': 'EdColeman'}]"
3876,core/src/main/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadata.java,"@@ -73,10 +77,20 @@ private static class Data {
      */
     private final TreeMap<String,TreeMap<String,String>> columnValues;
 
-    public Data(int version, TreeMap<String,TreeMap<String,String>> columnValues) {
+    private Data(int version, TreeMap<String,TreeMap<String,String>> columnValues) {
       this.version = version;
       this.columnValues = columnValues;
     }
+
+    public int getVersion() {
+      return version;
+    }
+
+    public static boolean needsConversion(final String json) {","[{'comment': '```suggestion\r\n    public static boolean needsUpgrade(final String json) {\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Changed in bceac27d1c', 'commenter': 'EdColeman'}]"
3876,core/src/test/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadataTest.java,"@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.metadata.schema;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.hadoop.fs.Path;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class RootTabletMetadataTest {
+  private static final Logger LOG = LoggerFactory.getLogger(RootTabletMetadataTest.class);
+
+  @Test
+  public void convertRoot1File() {
+    String root21ZkData =
+        ""{\""version\"":1,\""columnValues\"":{\""file\"":{\""hdfs://localhost:8020/accumulo/tables/+r/root_tablet/A000000v.rf\"":\""1368,61\""},\""last\"":{\""100025091780006\"":\""localhost:9997\""},\""loc\"":{\""100025091780006\"":\""localhost:9997\""},\""srv\"":{\""dir\"":\""root_tablet\"",\""flush\"":\""3\"",\""lock\"":\""tservers/localhost:9997/zlock#9db8961a-4ee9-400e-8e80-3353148baadd#0000000000$100025091780006\"",\""time\"":\""L53\""},\""~tab\"":{\""~pr\"":\""\\u0000\""}}}"";
+
+    RootTabletMetadata rtm = RootTabletMetadata.upgrade(root21ZkData);
+    LOG.debug(""converted column values: {}"", rtm.toTabletMetadata().getFiles());
+
+    var files = rtm.toTabletMetadata().getFiles();
+    LOG.info(""FILES: {}"", rtm.toTabletMetadata().getFilesMap());
+
+    assertEquals(1, files.size());
+    assertTrue(files.contains(StoredTabletFile
+        .of(new Path(""hdfs://localhost:8020/accumulo/tables/+r/root_tablet/A000000v.rf""))));
+  }
+
+  @Test
+  public void convertRoot2Files() {
+    String root212ZkData2Files =
+        ""{\""version\"":1,\""columnValues\"":{\""file\"":{\""hdfs://localhost:8020/accumulo/tables/+r/root_tablet/00000_00000.rf\"":\""0,0\"",\""hdfs://localhost:8020/accumulo/tables/+r/root_tablet/F000000c.rf\"":\""926,18\""},\""last\"":{\""10001a84d7d0005\"":\""localhost:9997\""},\""loc\"":{\""10001a84d7d0005\"":\""localhost:9997\""},\""srv\"":{\""dir\"":\""root_tablet\"",\""flush\"":\""2\"",\""lock\"":\""tservers/localhost:9997/zlock#d21adaa4-0f97-4004-9ff8-cce9dbb6687f#0000000000$10001a84d7d0005\"",\""time\"":\""L6\""},\""~tab\"":{\""~pr\"":\""\\u0000\""}}}\n"";
+
+    RootTabletMetadata rtm = RootTabletMetadata.upgrade(root212ZkData2Files);
+    LOG.debug(""converted column values: {}"", rtm.toTabletMetadata());
+","[{'comment': 'Can test the needsConversion function.\r\n\r\n```suggestion\r\nassertTrue(RootTabletMetadata.needsConversion(root212ZkData2Files));\r\n```', 'commenter': 'keith-turner'}, {'comment': 'Added tests in bceac27d1c', 'commenter': 'EdColeman'}]"
3876,core/src/test/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadataTest.java,"@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.core.metadata.schema;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.hadoop.fs.Path;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class RootTabletMetadataTest {","[{'comment': 'Could add the following test also\r\n\r\n * Pass version 5 data and ensure an exception is thrown\r\n * Pass version 2 data to upgrade and ensure it behaves as expected', 'commenter': 'keith-turner'}, {'comment': 'Added test in bceac27d1c (used -1 as version so it should always be invalid)', 'commenter': 'EdColeman'}]"
3876,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java,"@@ -0,0 +1,208 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.metadata.RootTable.ZROOT_TABLET;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.UpgraderDeprecatedConstants.ChoppedColumnFamily;
+import static org.apache.accumulo.server.AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.schema.Ample;
+import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.gson.JsonSyntaxException;
+
+public class Upgrader11to12 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader11to12.class);
+
+  @Override
+  public void upgradeZookeeper(@NonNull ServerContext context) {
+    log.debug(""Upgrade ZooKeeper: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootBase = ZooUtil.getRoot(context.getInstanceID()) + ZROOT_TABLET;
+
+    try {
+      var zrw = context.getZooReaderWriter();
+      Stat stat = new Stat();
+      byte[] rootData = zrw.getData(rootBase, stat);
+
+      String json = new String(rootData, UTF_8);
+      if (RootTabletMetadata.Data.needsConversion(json)) {
+        RootTabletMetadata rtm = RootTabletMetadata.upgrade(json);
+        zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper due to interrupt"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper because of ZooKeeper exception"", ex);
+    }
+  }
+
+  @Override
+  public void upgradeRoot(@NonNull ServerContext context) {
+    log.debug(""Upgrade root: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootName = Ample.DataLevel.METADATA.metaTable();
+    processReferences(context, rootName);
+  }
+
+  @Override
+  public void upgradeMetadata(@NonNull ServerContext context) {
+    log.debug(""Upgrade metadata: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var metaName = Ample.DataLevel.USER.metaTable();
+    processReferences(context, metaName);
+  }
+
+  private void processReferences(ServerContext context, String tableName) {
+    // not using ample to avoid StoredTabletFile because old file ref is incompatible
+    try (AccumuloClient c = Accumulo.newClient().from(context.getProperties()).build();
+        BatchWriter batchWriter = c.createBatchWriter(tableName);
+        Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {
+
+      scanner.fetchColumnFamily(DataFileColumnFamily.NAME);
+      scanner.fetchColumnFamily(ChoppedColumnFamily.NAME);
+      scanner.fetchColumnFamily(ExternalCompactionColumnFamily.NAME);
+      scanner.forEach((k, v) -> {
+        var family = k.getColumnFamily();
+        if (family.equals(DataFileColumnFamily.NAME)) {
+          upgradeDataFileCF(k, v, batchWriter, tableName);
+        } else if (family.equals(ChoppedColumnFamily.NAME)) {
+          removeChoppedCF(k, batchWriter, tableName);
+        } else if (family.equals(ExternalCompactionColumnFamily.NAME)) {
+          removeExternalCompactionCF(k, batchWriter, tableName);
+        } else {
+          log.warn(""Received unexpected column family processing references: "" + family);","[{'comment': 'I would throw an exception here instead of logging a warning because its completely unexpected.', 'commenter': 'keith-turner'}, {'comment': 'Added in bceac27d1c. ', 'commenter': 'EdColeman'}]"
3876,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java,"@@ -0,0 +1,208 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.metadata.RootTable.ZROOT_TABLET;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.UpgraderDeprecatedConstants.ChoppedColumnFamily;
+import static org.apache.accumulo.server.AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.schema.Ample;
+import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.gson.JsonSyntaxException;
+
+public class Upgrader11to12 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader11to12.class);
+
+  @Override
+  public void upgradeZookeeper(@NonNull ServerContext context) {
+    log.debug(""Upgrade ZooKeeper: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootBase = ZooUtil.getRoot(context.getInstanceID()) + ZROOT_TABLET;
+
+    try {
+      var zrw = context.getZooReaderWriter();
+      Stat stat = new Stat();
+      byte[] rootData = zrw.getData(rootBase, stat);
+
+      String json = new String(rootData, UTF_8);
+      if (RootTabletMetadata.Data.needsConversion(json)) {
+        RootTabletMetadata rtm = RootTabletMetadata.upgrade(json);
+        zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper due to interrupt"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper because of ZooKeeper exception"", ex);
+    }
+  }
+
+  @Override
+  public void upgradeRoot(@NonNull ServerContext context) {
+    log.debug(""Upgrade root: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootName = Ample.DataLevel.METADATA.metaTable();
+    processReferences(context, rootName);
+  }
+
+  @Override
+  public void upgradeMetadata(@NonNull ServerContext context) {
+    log.debug(""Upgrade metadata: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var metaName = Ample.DataLevel.USER.metaTable();
+    processReferences(context, metaName);
+  }
+
+  private void processReferences(ServerContext context, String tableName) {
+    // not using ample to avoid StoredTabletFile because old file ref is incompatible
+    try (AccumuloClient c = Accumulo.newClient().from(context.getProperties()).build();
+        BatchWriter batchWriter = c.createBatchWriter(tableName);
+        Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {
+
+      scanner.fetchColumnFamily(DataFileColumnFamily.NAME);
+      scanner.fetchColumnFamily(ChoppedColumnFamily.NAME);
+      scanner.fetchColumnFamily(ExternalCompactionColumnFamily.NAME);
+      scanner.forEach((k, v) -> {
+        var family = k.getColumnFamily();
+        if (family.equals(DataFileColumnFamily.NAME)) {
+          upgradeDataFileCF(k, v, batchWriter, tableName);
+        } else if (family.equals(ChoppedColumnFamily.NAME)) {
+          removeChoppedCF(k, batchWriter, tableName);
+        } else if (family.equals(ExternalCompactionColumnFamily.NAME)) {
+          removeExternalCompactionCF(k, batchWriter, tableName);
+        } else {
+          log.warn(""Received unexpected column family processing references: "" + family);
+        }
+      });
+    } catch (MutationsRejectedException mex) {
+      log.warn(""Failed to update reference for table: "" + tableName);
+      log.warn(""Constraint violations: {}"", mex.getConstraintViolationSummaries());
+      throw new IllegalStateException(""Failed to process table: "" + tableName, mex);
+    } catch (Exception ex) {
+      throw new IllegalStateException(""Failed to process table: "" + tableName, ex);
+    }
+  }
+
+  @VisibleForTesting
+  void upgradeDataFileCF(final Key key, final Value value, final BatchWriter batchWriter,
+      final String tableName) {
+    String file = key.getColumnQualifier().toString();
+    // filter out references if they are in the correct format already.
+    if (fileNeedsConversion(file)) {
+      var fileJson = StoredTabletFile.of(new Path(file)).getMetadataText();
+      try {
+        Mutation update = new Mutation(key.getRow());
+        update.at().family(DataFileColumnFamily.STR_NAME).qualifier(fileJson).put(value);
+        log.trace(""table: {}, adding: {}"", tableName, update.prettyPrint());
+        batchWriter.addMutation(update);
+
+        Mutation delete = new Mutation(key.getRow());
+        delete.at().family(DataFileColumnFamily.STR_NAME).qualifier(file).delete();
+        log.trace(""table {}: deleting: {}"", tableName, delete.prettyPrint());
+        batchWriter.addMutation(delete);
+      } catch (MutationsRejectedException ex) {
+        // include constraint violation info in log - but stop upgrade
+        log.warn(
+            ""Failed to update file reference for table: "" + tableName + "". row: "" + key.getRow());
+        log.warn(""Constraint violations: {}"", ex.getConstraintViolationSummaries());
+        throw new IllegalStateException(""File conversion failed. Aborting upgrade"", ex);
+      }
+    }
+  }
+
+  @VisibleForTesting
+  void removeChoppedCF(final Key key, final BatchWriter batchWriter, final String tableName) {
+    Mutation delete = null;
+    try {
+      delete = new Mutation(key.getRow()).at().family(ChoppedColumnFamily.STR_NAME)
+          .qualifier(ChoppedColumnFamily.STR_NAME).delete();
+      log.warn(
+          ""Deleting chopped reference from:{}. Previous split or delete may not have completed cleanly. Ref: {}"",
+          tableName, delete.prettyPrint());
+      batchWriter.addMutation(delete);
+    } catch (MutationsRejectedException ex) {
+      log.warn(""Failed to delete obsolete chopped CF reference for table: "" + tableName + "". Ref: ""","[{'comment': 'Need to rethrow this exception.  The batch writer queues up mutations and does not send them immediately.  When addMuation throws an exception it may not have been caused by the most recently added mutation.  Could be from a mutation for changing a file.', 'commenter': 'keith-turner'}, {'comment': 'Added in bceac27d1c', 'commenter': 'EdColeman'}]"
3876,server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java,"@@ -0,0 +1,208 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.manager.upgrade;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.accumulo.core.metadata.RootTable.ZROOT_TABLET;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;
+import static org.apache.accumulo.core.metadata.schema.UpgraderDeprecatedConstants.ChoppedColumnFamily;
+import static org.apache.accumulo.server.AccumuloDataVersion.METADATA_FILE_JSON_ENCODING;
+
+import org.apache.accumulo.core.client.Accumulo;
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.BatchWriter;
+import org.apache.accumulo.core.client.MutationsRejectedException;
+import org.apache.accumulo.core.client.Scanner;
+import org.apache.accumulo.core.data.Key;
+import org.apache.accumulo.core.data.Mutation;
+import org.apache.accumulo.core.data.Value;
+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;
+import org.apache.accumulo.core.metadata.StoredTabletFile;
+import org.apache.accumulo.core.metadata.schema.Ample;
+import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;
+import org.apache.accumulo.core.security.Authorizations;
+import org.apache.accumulo.server.ServerContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.data.Stat;
+import org.checkerframework.checker.nullness.qual.NonNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.gson.JsonSyntaxException;
+
+public class Upgrader11to12 implements Upgrader {
+
+  private static final Logger log = LoggerFactory.getLogger(Upgrader11to12.class);
+
+  @Override
+  public void upgradeZookeeper(@NonNull ServerContext context) {
+    log.debug(""Upgrade ZooKeeper: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootBase = ZooUtil.getRoot(context.getInstanceID()) + ZROOT_TABLET;
+
+    try {
+      var zrw = context.getZooReaderWriter();
+      Stat stat = new Stat();
+      byte[] rootData = zrw.getData(rootBase, stat);
+
+      String json = new String(rootData, UTF_8);
+      if (RootTabletMetadata.Data.needsConversion(json)) {
+        RootTabletMetadata rtm = RootTabletMetadata.upgrade(json);
+        zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());
+      }
+    } catch (InterruptedException ex) {
+      Thread.currentThread().interrupt();
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper due to interrupt"", ex);
+    } catch (KeeperException ex) {
+      throw new IllegalStateException(
+          ""Could not read root metadata from ZooKeeper because of ZooKeeper exception"", ex);
+    }
+  }
+
+  @Override
+  public void upgradeRoot(@NonNull ServerContext context) {
+    log.debug(""Upgrade root: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var rootName = Ample.DataLevel.METADATA.metaTable();
+    processReferences(context, rootName);
+  }
+
+  @Override
+  public void upgradeMetadata(@NonNull ServerContext context) {
+    log.debug(""Upgrade metadata: upgrading to data version {}"", METADATA_FILE_JSON_ENCODING);
+    var metaName = Ample.DataLevel.USER.metaTable();
+    processReferences(context, metaName);
+  }
+
+  private void processReferences(ServerContext context, String tableName) {
+    // not using ample to avoid StoredTabletFile because old file ref is incompatible
+    try (AccumuloClient c = Accumulo.newClient().from(context.getProperties()).build();
+        BatchWriter batchWriter = c.createBatchWriter(tableName);
+        Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {
+
+      scanner.fetchColumnFamily(DataFileColumnFamily.NAME);
+      scanner.fetchColumnFamily(ChoppedColumnFamily.NAME);
+      scanner.fetchColumnFamily(ExternalCompactionColumnFamily.NAME);
+      scanner.forEach((k, v) -> {
+        var family = k.getColumnFamily();
+        if (family.equals(DataFileColumnFamily.NAME)) {
+          upgradeDataFileCF(k, v, batchWriter, tableName);
+        } else if (family.equals(ChoppedColumnFamily.NAME)) {
+          removeChoppedCF(k, batchWriter, tableName);
+        } else if (family.equals(ExternalCompactionColumnFamily.NAME)) {
+          removeExternalCompactionCF(k, batchWriter, tableName);
+        } else {
+          log.warn(""Received unexpected column family processing references: "" + family);
+        }
+      });
+    } catch (MutationsRejectedException mex) {
+      log.warn(""Failed to update reference for table: "" + tableName);
+      log.warn(""Constraint violations: {}"", mex.getConstraintViolationSummaries());
+      throw new IllegalStateException(""Failed to process table: "" + tableName, mex);
+    } catch (Exception ex) {
+      throw new IllegalStateException(""Failed to process table: "" + tableName, ex);
+    }
+  }
+
+  @VisibleForTesting
+  void upgradeDataFileCF(final Key key, final Value value, final BatchWriter batchWriter,
+      final String tableName) {
+    String file = key.getColumnQualifier().toString();
+    // filter out references if they are in the correct format already.
+    if (fileNeedsConversion(file)) {
+      var fileJson = StoredTabletFile.of(new Path(file)).getMetadataText();
+      try {
+        Mutation update = new Mutation(key.getRow());
+        update.at().family(DataFileColumnFamily.STR_NAME).qualifier(fileJson).put(value);
+        log.trace(""table: {}, adding: {}"", tableName, update.prettyPrint());
+        batchWriter.addMutation(update);
+
+        Mutation delete = new Mutation(key.getRow());
+        delete.at().family(DataFileColumnFamily.STR_NAME).qualifier(file).delete();
+        log.trace(""table {}: deleting: {}"", tableName, delete.prettyPrint());
+        batchWriter.addMutation(delete);
+      } catch (MutationsRejectedException ex) {
+        // include constraint violation info in log - but stop upgrade
+        log.warn(
+            ""Failed to update file reference for table: "" + tableName + "". row: "" + key.getRow());
+        log.warn(""Constraint violations: {}"", ex.getConstraintViolationSummaries());
+        throw new IllegalStateException(""File conversion failed. Aborting upgrade"", ex);
+      }
+    }
+  }
+
+  @VisibleForTesting
+  void removeChoppedCF(final Key key, final BatchWriter batchWriter, final String tableName) {
+    Mutation delete = null;
+    try {
+      delete = new Mutation(key.getRow()).at().family(ChoppedColumnFamily.STR_NAME)
+          .qualifier(ChoppedColumnFamily.STR_NAME).delete();
+      log.warn(
+          ""Deleting chopped reference from:{}. Previous split or delete may not have completed cleanly. Ref: {}"",
+          tableName, delete.prettyPrint());
+      batchWriter.addMutation(delete);
+    } catch (MutationsRejectedException ex) {
+      log.warn(""Failed to delete obsolete chopped CF reference for table: "" + tableName + "". Ref: ""
+          + delete.prettyPrint() + "". Will try to continue. Ref may need to be manually removed"");
+      log.warn(""Constraint violations: {}"", ex.getConstraintViolationSummaries());
+    }
+  }
+
+  @VisibleForTesting
+  void removeExternalCompactionCF(final Key key, final BatchWriter batchWriter,
+      final String tableName) {
+    Mutation delete = null;
+    try {
+      delete = new Mutation(key.getRow()).at().family(ExternalCompactionColumnFamily.NAME)
+          .qualifier(key.getColumnQualifier()).delete();
+      log.warn(
+          ""Deleting external compaction reference from:{}. Previous compaction may not have completed. Ref: {}"",
+          tableName, delete.prettyPrint());
+      batchWriter.addMutation(delete);
+    } catch (MutationsRejectedException ex) {
+      log.warn(""Failed to delete obsolete external compaction CF reference for table: "" + tableName","[{'comment': 'Also need to throw an exception here.', 'commenter': 'keith-turner'}, {'comment': 'Added in bceac27d1c', 'commenter': 'EdColeman'}]"
3895,pom.xml,"@@ -632,7 +632,7 @@
         <!-- stay on 3.21.0 for now due to https://github.com/apache/accumulo/issues/3446 -->","[{'comment': '```suggestion\r\n```', 'commenter': 'ctubbsii'}]"
3905,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -49,6 +49,20 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       if (cl.getArgList().isEmpty()) {
         throw new MissingArgumentException(""No terms specified"");
       }
+      // Configure formatting options
+      final FormatterConfig config = new FormatterConfig();
+      config.setPrintTimestamps(cl.hasOption(timestampOpt.getOpt()));
+      if (cl.hasOption(showFewOpt.getOpt())) {
+        final String showLength = cl.getOptionValue(showFewOpt.getOpt());
+        try {
+          final int length = Integer.parseInt(showLength);
+          config.setShownLength(length);
+        } catch (NumberFormatException nfe) {
+          Shell.log.error(""Arg must be an integer."", nfe);
+        } catch (IllegalArgumentException iae) {
+          Shell.log.error(""Arg must be greater than one."", iae);","[{'comment': ""This error handling logic copied from ScanCommand does not look correct. The IllegalArgumentException is thrown when the number is less 0, not less than 2, which this message implies. It should probably just grab `iae.getMessage()` to show here, so it is more correct. That should be fixed in ScanCommand as well. This doesn't have to be part of this PR, though. It can be fixed in a subsequent change, as it's not related to what you're trying to fix here. It's up to you if you want to make a separate PR to fix that or fix it in this PR."", 'commenter': 'ctubbsii'}, {'comment': 'I will open a new pull request.', 'commenter': 'rsingh433'}, {'comment': 'Pull request #3952', 'commenter': 'rsingh433'}]"
3905,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -123,6 +136,7 @@ public Options getOptions() {
     negateOpt = new Option(""v"", ""negate"", false, ""only include rows without search term"");
     opts.addOption(numThreadsOpt);
     opts.addOption(negateOpt);
+    opts.addOption(showFewOpt);","[{'comment': ""I don't think this is needed, since it's done in the parent class, ScanCommand."", 'commenter': 'ctubbsii'}]"
3905,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -49,6 +49,20 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       if (cl.getArgList().isEmpty()) {
         throw new MissingArgumentException(""No terms specified"");
       }
+      // Configure formatting options
+      final FormatterConfig config = new FormatterConfig();
+      config.setPrintTimestamps(cl.hasOption(timestampOpt.getOpt()));
+      if (cl.hasOption(showFewOpt.getOpt())) {
+        final String showLength = cl.getOptionValue(showFewOpt.getOpt());","[{'comment': ':+1:  for fixing this for the grep commands.', 'commenter': 'ctubbsii'}]"
3905,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -70,6 +70,8 @@ public class ScanCommand extends Command {
   private Option contextOpt;
   private Option executionHintsOpt;
   private Option scanServerOpt;
+  protected Option showFewOpt;
+  private Option formatterOpt;","[{'comment': ""I think this `formatterOpt` can stay in the list above. Only `showFewOpt` needed to be moved out to its own line, to make it `protected`.\r\n\r\nUltimately, we shouldn't have comma-separated lists of variables like these at all, so eventually, it'd be better if they were all in their own separate lines. But, to keep this PR minimal, it'd be good to move it back."", 'commenter': 'ctubbsii'}]"
3905,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -447,6 +448,10 @@ public Options getOptions() {
     executionHintsOpt = new Option(null, ""execution-hints"", true, ""Execution hints map"");
     scanServerOpt =
         new Option(""cl"", ""consistency-level"", true, ""set consistency level (experimental)"");
+    // Options specific to ScanCommand
+    showFewOpt = new Option(""f"", ""show-few"", true, ""show only a specified number of characters"");
+    formatterOpt =
+        new Option(""fm"", ""formatter"", true, ""fully qualified name of the formatter class to use"");","[{'comment': ""I don't think these are needed, since they are already defined above on line 435."", 'commenter': 'ctubbsii'}]"
3905,shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java,"@@ -447,7 +447,6 @@ public Options getOptions() {
     executionHintsOpt = new Option(null, ""execution-hints"", true, ""Execution hints map"");
     scanServerOpt =
         new Option(""cl"", ""consistency-level"", true, ""set consistency level (experimental)"");","[{'comment': 'Just going to re-add this blank line, because I think it\'s better here, to separate the option initializations and the arg names.\r\n\r\n```suggestion\r\n        new Option(""cl"", ""consistency-level"", true, ""set consistency level (experimental)"");\r\n\r\n```', 'commenter': 'ctubbsii'}]"
3905,shell/src/main/java/org/apache/accumulo/shell/commands/GrepCommand.java,"@@ -49,6 +49,20 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s
       if (cl.getArgList().isEmpty()) {
         throw new MissingArgumentException(""No terms specified"");
       }
+      // Configure formatting options
+      final FormatterConfig config = new FormatterConfig();
+      config.setPrintTimestamps(cl.hasOption(timestampOpt.getOpt()));","[{'comment': '@rsingh433 I just noticed that this print timestamps feature is already enabled on line 104, so this is redundant here.', 'commenter': 'ctubbsii'}]"
3927,core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java,"@@ -105,6 +106,20 @@ public void testPorts() {
     }
   }
 
+  @Test
+  public void testJson() {
+    // JsonParser.parseString(""not json"");","[{'comment': 'This comment can be removed\r\n```suggestion\r\n```\r\n\r\n', 'commenter': 'ddanielr'}, {'comment': 'Removed in 7fa8acabb7', 'commenter': 'EdColeman'}]"
3927,core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java,"@@ -105,6 +106,20 @@ public void testPorts() {
     }
   }
 
+  @Test
+  public void testJson() {
+    // JsonParser.parseString(""not json"");
+
+    var p1 = TSERV_COMPACTION_SERVICE_META_EXECUTORS;
+    assertFalse(TSERV_COMPACTION_SERVICE_META_EXECUTORS.getType().isValidFormat(""notJson""));
+
+    String json =
+        ""[{'name':'small','type':'internal','maxSize':'32M','numThreads':2},{'name':'huge','type':'internal','numThreads':2}]""
+            .replaceAll(""'"", ""\"""");
+    assertTrue(Property.isValidProperty(p1.getKey(), json));
+
+  }
+","[{'comment': 'This test should probably follow the other property test patterns and test all properties with the JSON property type. \r\nSomething like:\r\n```suggestion\r\n   for (Property prop : Property.values()) {\r\n      if (prop.getType().equals(PropertyType.JSON)) {\r\n        assertFalse(prop.getType().isValidFormat(""notJson""));\r\n        String json =\r\n            ""[{\'foo\':\'bar\',\'type\':\'test\',\'fooBar\':\'32\'},{\'foo\':\'bar\',\'type\':\'test\',\'fooBar\':32}]"".replaceAll(\r\n                ""\'"", ""\\"""");\r\n        assertTrue(Property.isValidProperty(prop.getKey(), json));\r\n      }\r\n    }\r\n```\r\n\r\n', 'commenter': 'ddanielr'}, {'comment': 'I took this suggestion and modified it further in 7fa8acabb7', 'commenter': 'EdColeman'}, {'comment': ""Changes look good. There's two separate follow up comments on your changes. "", 'commenter': 'ddanielr'}]"
3927,test/src/main/java/org/apache/accumulo/test/shell/ConfigSetIT.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.shell;
+
+import static org.apache.accumulo.core.conf.Property.MONITOR_RESOURCES_EXTERNAL;
+import static org.apache.accumulo.core.conf.Property.TSERV_COMPACTION_SERVICE_ROOT_EXECUTORS;
+import static org.apache.accumulo.harness.AccumuloITBase.MINI_CLUSTER_ONLY;
+import static org.apache.accumulo.harness.AccumuloITBase.SUNNY_DAY;
+import static org.junit.jupiter.api.Assertions.assertThrows;
+
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.security.tokens.PasswordToken;
+import org.apache.accumulo.harness.SharedMiniClusterBase;
+import org.junit.jupiter.api.AfterAll;
+import org.junit.jupiter.api.BeforeAll;
+import org.junit.jupiter.api.Tag;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Tag(MINI_CLUSTER_ONLY)
+@Tag(SUNNY_DAY)
+public class ConfigSetIT extends SharedMiniClusterBase {
+  @BeforeAll
+  public static void setup() throws Exception {
+    SharedMiniClusterBase.startMiniCluster();
+  }
+
+  @AfterAll
+  public static void teardown() {
+    SharedMiniClusterBase.stopMiniCluster();
+  }
+
+  private static final Logger log = LoggerFactory.getLogger(ConfigSetIT.class);
+
+  @Test
+  public void setInvalidJson() throws Exception {","[{'comment': 'This question is probably out of scope of the ""simple validation"" of this PR.\r\n \r\nIs there a way to test setting a custom, user-defined property (created under the `general.custom` prefix that\'s of type JSON with the shell?', 'commenter': 'ddanielr'}, {'comment': 'As it is now, `general.custom` is a prefix and does not specify a type.\r\n\r\nI think it would take something like a new PREFIX type, maybe something like `general.custom.json` that could then be used to trigger validation.  There may be issues with precedence with `general.custom` that would short circuit `general.custom.json`.  \r\n\r\nCertainly something to discuss as a follow-on.', 'commenter': 'EdColeman'}]"
3927,core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java,"@@ -136,6 +141,9 @@ public enum PropertyType {
       ""An arbitrary string of characters whose format is unspecified and""
           + "" interpreted based on the context of the property to which it applies.""),
 
+  JSON(""json"", x -> new ValidJson().test(x),","[{'comment': 'Would this create a new `ValidJson` object each time this is used? If so, is there a way to create a single instance for reuse?', 'commenter': 'DomGarguilo'}, {'comment': ""The test is fairly light weight, so I don't know how we could evaluate if it made a difference.  With 37112e7e0e, the ObjectMapper is shared, so there should be even less impact on object creation."", 'commenter': 'EdColeman'}, {'comment': 'It doesn\'t make sense to make the object mapper static, and instantiate the ValidJson object each time in order to call it\'s test method. It\'s already a Predicate, so you don\'t need the `x -> ... .test(x)` stuff, you can just do:\r\n\r\n```suggestion\r\n  JSON(""json"", new ValidJson(),\r\n```\r\n\r\nThen, you don\'t have to worry about making the object mapper a static singleton... the PropertyType.JSON enum will just re-use the singleton `new ValidJson()` instance passed in for its predicate every time.', 'commenter': 'ctubbsii'}, {'comment': 'It should be just `new ValidJson()`, my mistake.', 'commenter': 'ctubbsii'}, {'comment': 'fixed in 33bc47d2a1', 'commenter': 'EdColeman'}]"
3927,core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java,"@@ -186,6 +195,39 @@ public boolean isValidFormat(String value) {
     return predicate.test(value);
   }
 
+  /**
+   * Validate that the provided string can be parsed into a json object. This implementation uses
+   * jackson databind because it is less permissive that GSON for what is considered valid. This
+   * implementation cannot guarantee that the json is valid for the target usage. That would require
+   * something like a json schema or a check specific to the use-case. This is only trying to
+   * provide a generic, minimal check that at least the json is valid.
+   */
+  private static class ValidJson implements Predicate<String> {
+    private static final Logger log = LoggerFactory.getLogger(ValidJson.class);
+
+    // set a limit of 1 million characters on the string as rough sanity check
+    private static final int ONE_MILLION = 1024 * 1024;
+
+    @Override
+    public boolean test(String value) {
+      try {
+        if (value.length() > ONE_MILLION) {
+          log.info(""provided json string length {} is greater than limit of {} for parsing"",
+              value.length(), ONE_MILLION);
+          return false;
+        }
+        ObjectMapper mapper =
+            new ObjectMapper().enable(DeserializationFeature.FAIL_ON_READING_DUP_TREE_KEY)
+                .enable(DeserializationFeature.FAIL_ON_TRAILING_TOKENS);","[{'comment': 'Similar comment here. Not sure how expensive this object is to create but maybe a `private static final ObjectMapper` could be created within this class for reuse.', 'commenter': 'DomGarguilo'}, {'comment': 'Made static in 37112e7e0e.  Also, left documentation that we could consider using ThreadLocal.  The ObjectMapper is thread safe, but apparently uses synchronization, so if we see contention then there are alternatives.', 'commenter': 'EdColeman'}, {'comment': 'Unnecessary. Just throw it inside the ValidJson class as a private instance field inside that. See my previous comment about passing `ValidJson::new` as the predicate.', 'commenter': 'ctubbsii'}, {'comment': 'Changed in 33bc47d2a1', 'commenter': 'EdColeman'}]"
3927,core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java,"@@ -105,6 +107,35 @@ public void testPorts() {
     }
   }
 
+  @Test
+  public void testJson() {
+    assertFalse(TSERV_COMPACTION_SERVICE_META_EXECUTORS.getType().isValidFormat(""notJson""));
+
+    // use ""real"" example
+    String json1 =
+        ""[{'name':'small','type':'internal','maxSize':'32M','numThreads':2},{'name':'huge','type':'internal','numThreads':2}]""
+            .replaceAll(""'"", ""\"""");
+    // use synthetic, but valid json
+    String json2 =","[{'comment': 'If you want to have two separate json test values, we might want to remove the top level list brackets on one of these so the test values are different forms of valid json.', 'commenter': 'ddanielr'}, {'comment': 'Added another sample in 37112e7e0e - the test requires a single json object, so removing the brackets allow fails.', 'commenter': 'EdColeman'}]"
3927,core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java,"@@ -105,6 +107,35 @@ public void testPorts() {
     }
   }
 
+  @Test
+  public void testJson() {
+    assertFalse(TSERV_COMPACTION_SERVICE_META_EXECUTORS.getType().isValidFormat(""notJson""));","[{'comment': 'Do we still need this test for this specific property since this is tested on line 134?\r\n`invalids.forEach(j -> assertFalse(prop.getType().isValidFormat(j))); `', 'commenter': 'ddanielr'}, {'comment': 'removed with 37112e7e0e (and some additional cases added)', 'commenter': 'EdColeman'}]"
3927,test/src/main/java/org/apache/accumulo/test/shell/ConfigSetIT.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.accumulo.test.shell;
+
+import static org.apache.accumulo.core.conf.Property.MONITOR_RESOURCES_EXTERNAL;
+import static org.apache.accumulo.core.conf.Property.TSERV_COMPACTION_SERVICE_ROOT_EXECUTORS;
+import static org.apache.accumulo.harness.AccumuloITBase.MINI_CLUSTER_ONLY;
+import static org.apache.accumulo.harness.AccumuloITBase.SUNNY_DAY;
+import static org.junit.jupiter.api.Assertions.assertThrows;
+
+import org.apache.accumulo.core.client.AccumuloClient;
+import org.apache.accumulo.core.client.AccumuloException;
+import org.apache.accumulo.core.client.security.tokens.PasswordToken;
+import org.apache.accumulo.harness.SharedMiniClusterBase;
+import org.junit.jupiter.api.AfterAll;
+import org.junit.jupiter.api.BeforeAll;
+import org.junit.jupiter.api.Tag;
+import org.junit.jupiter.api.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Tag(MINI_CLUSTER_ONLY)
+@Tag(SUNNY_DAY)","[{'comment': ""```suggestion\r\n```\r\n\r\nI'm against making this a new sunny test."", 'commenter': 'ctubbsii'}, {'comment': 'removed sunny tag in 33bc47d2a1', 'commenter': 'EdColeman'}]"
3934,server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/ReserveTablets.java,"@@ -53,17 +58,25 @@ public long isReady(long tid, Manager env) throws Exception {
     log.debug(""{} reserving tablets in range {}"", FateTxId.formatTid(tid), range);
     var opid = TabletOperationId.from(TabletOperationType.MERGING, tid);
 
+    AtomicLong opsAccepted = new AtomicLong(0);
+    BiConsumer<KeyExtent,Ample.ConditionalResult> resultConsumer = (extent, result) -> {","[{'comment': ""I'm wondering if we should have some shared Abstract class or utility for `ReserveTablets` for things like this BiConsumer that are basically identical between delete/merge"", 'commenter': 'cshannon'}]"
3960,server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueues.java,"@@ -91,6 +91,11 @@ public long getQueueCount() {
     return priorityQueues.mappingCount();
   }
 
+  public long getQueuedJobCount() {
+    return priorityQueues.values().stream().map(CompactionJobPriorityQueue::getQueuedJobs)
+        .reduce(Long::sum).orElseThrow();","[{'comment': 'Instead of throwing an NoSuchElement exception, can we just return zero instead?\r\n```suggestion\r\n        .reduce(Long.valueOf(0), (Long::sum));\r\n```', 'commenter': 'ddanielr'}, {'comment': ""Good catch, I didn't realize there was a two-arg version of `reduce`. On second thought, I'm going to get rid of the stream and replace it with a simple for-loop as it's likely faster."", 'commenter': 'dlmarion'}]"
