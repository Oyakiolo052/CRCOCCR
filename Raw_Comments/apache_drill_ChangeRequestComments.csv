Pull,Path,Diff_hunk,Comment
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -17,36 +17,161 @@
  */
 package org.apache.drill.exec.fn.impl;
 
-import org.apache.drill.test.BaseTestQuery;
 import org.apache.drill.categories.SqlFunctionTest;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+
 @Category(SqlFunctionTest.class)
-public class TestNewSimpleRepeatedFunctions extends BaseTestQuery {
+public class TestNewSimpleRepeatedFunctions extends ClusterTest {
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+  }
+
   @Test
   public void testRepeatedContainsForWildCards() throws Exception {
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
-
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(false)
-       .baselineValues(false)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, true, true, true, false)
+        .go();
+
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, false, false, true, false)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedMap() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1}, {""field1"": 2}]}
+      {""mapArray"": [{""field2"": ""val2""}, {""field1"": 2}, {""field1"": 2}]}
+      {""mapArray"": []}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val4""}, {""field1"": 2}, {""field1"": 2}, {""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}, {""field1"": 2}]}
+    */
+    String fileName = ""repeated_count_map.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""{\""field1\"": 1, \""field2\"": \""val1\""}, {\""field1\"": 2}"",
+          ""{\""field1\"": 1}, {\""field1\"": 2}"",
+          ""{\""field2\"": \""val2\""}, {\""field1\"": 2}, {\""field1\"": 2}"",
+          """",
+          ""{\""field1\"": 1, \""field2\"": \""val3\""}"",
+          ""{\""field1\"": 1, \""field2\"": \""val4\""}, {\""field1\"": 2}, {\""field1\"": 2}, {\""field2\"": \""val1\""}, {\""field1\"": 2}"",
+          ""{\""field1\"": 1, \""field2\"": \""val3\""}, {\""field1\"": 2}""
+      };
+      for (String value : arrayElements) {
+        String entry = String.format(""{\""mapArray\"": [%s]}\n"", value);
+        writer.write(entry);
+      }
+    }
+
+    String selectQuery = ""select repeated_count(mapArray) from dfs.`%s`"";
+    testBuilder()
+        .sqlQuery(selectQuery, fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 2, 3, 0, 1, 5, 2)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" where repeated_count(mapArray) > 2"", fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(3, 5)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 3, 0, 1, 5)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1 having repeated_count(mapArray) < 3"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 0, 1)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedList() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""id"": 1, ""array"": [[1, 2], [1, 3], [2, 3]]}
+      {""id"": 2, ""array"": []}
+      {""id"": 3, ""array"": [[2, 3], [1, 3, 4]]}
+      {""id"": 4, ""array"": [[1], [2], [3, 4], [5], [6]]}
+      {""id"": 5, ""array"": [[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]]}
+      {""id"": 6, ""array"": [[1, 2], [3], [4], [5]]}
+      {""id"": 7, ""array"": []}
+      {""id"": 8, ""array"": [[1], [2], [3]]}
+    */
+    String fileName = ""repeated_count_list.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""[1, 2], [1, 3], [2, 3]"",
+          """",
+          ""[2, 3], [1, 3, 4]"",
+          ""[1], [2], [3, 4], [5], [6]"",
+          ""[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]"",
+          ""[1, 2], [3], [4], [5]"",
+          """",
+          ""[1], [2], [3]""};
+      int elementId = 1;
+      for (String value : arrayElements) {
+        String entry = String.format(""{\""id\"": %d, \""array\"": [%s]}\n"", elementId++, value);
+        writer.write(entry);
+      }
+    }
+
+    String selectQuery = ""select repeated_count(array) from dfs.`%s`"";
+    testBuilder()
+        .sqlQuery(selectQuery, fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(3, 0, 2, 5, 9, 4, 0, 3)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" where repeated_count(array) > 4"", fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(5, 9)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(3, 0, 2, 5, 9, 4)
+        .go();
 
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1 having repeated_count(array) < 4"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(3, 0, 2)
+        .go();
   }
 }","[{'comment': '```suggestion\r\n}\r\n\r\n```', 'commenter': 'vdiravka'}]"
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -17,36 +17,161 @@
  */
 package org.apache.drill.exec.fn.impl;
 
-import org.apache.drill.test.BaseTestQuery;
 import org.apache.drill.categories.SqlFunctionTest;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+
 @Category(SqlFunctionTest.class)
-public class TestNewSimpleRepeatedFunctions extends BaseTestQuery {
+public class TestNewSimpleRepeatedFunctions extends ClusterTest {
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+  }
+
   @Test
   public void testRepeatedContainsForWildCards() throws Exception {
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
-
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(false)
-       .baselineValues(false)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, true, true, true, false)
+        .go();
+
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, false, false, true, false)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedMap() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1}, {""field1"": 2}]}
+      {""mapArray"": [{""field2"": ""val2""}, {""field1"": 2}, {""field1"": 2}]}
+      {""mapArray"": []}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val4""}, {""field1"": 2}, {""field1"": 2}, {""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}, {""field1"": 2}]}
+    */
+    String fileName = ""repeated_count_map.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""{\""field1\"": 1, \""field2\"": \""val1\""}, {\""field1\"": 2}"",
+          ""{\""field1\"": 1}, {\""field1\"": 2}"",
+          ""{\""field2\"": \""val2\""}, {\""field1\"": 2}, {\""field1\"": 2}"",
+          """",
+          ""{\""field1\"": 1, \""field2\"": \""val3\""}"",
+          ""{\""field1\"": 1, \""field2\"": \""val4\""}, {\""field1\"": 2}, {\""field1\"": 2}, {\""field2\"": \""val1\""}, {\""field1\"": 2}"",
+          ""{\""field1\"": 1, \""field2\"": \""val3\""}, {\""field1\"": 2}""
+      };
+      for (String value : arrayElements) {
+        String entry = String.format(""{\""mapArray\"": [%s]}\n"", value);
+        writer.write(entry);
+      }
+    }
+
+    String selectQuery = ""select repeated_count(mapArray) from dfs.`%s`"";
+    testBuilder()
+        .sqlQuery(selectQuery, fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 2, 3, 0, 1, 5, 2)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" where repeated_count(mapArray) > 2"", fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(3, 5)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 3, 0, 1, 5)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1 having repeated_count(mapArray) < 3"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 0, 1)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedList() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""id"": 1, ""array"": [[1, 2], [1, 3], [2, 3]]}
+      {""id"": 2, ""array"": []}
+      {""id"": 3, ""array"": [[2, 3], [1, 3, 4]]}
+      {""id"": 4, ""array"": [[1], [2], [3, 4], [5], [6]]}
+      {""id"": 5, ""array"": [[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]]}
+      {""id"": 6, ""array"": [[1, 2], [3], [4], [5]]}
+      {""id"": 7, ""array"": []}
+      {""id"": 8, ""array"": [[1], [2], [3]]}
+    */
+    String fileName = ""repeated_count_list.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""[1, 2], [1, 3], [2, 3]"",
+          """",
+          ""[2, 3], [1, 3, 4]"",
+          ""[1], [2], [3, 4], [5], [6]"",
+          ""[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]"",
+          ""[1, 2], [3], [4], [5]"",
+          """",
+          ""[1], [2], [3]""};
+      int elementId = 1;
+      for (String value : arrayElements) {
+        String entry = String.format(""{\""id\"": %d, \""array\"": [%s]}\n"", elementId++, value);
+        writer.write(entry);
+      }
+    }
+
+    String selectQuery = ""select repeated_count(array) from dfs.`%s`"";
+    testBuilder()
+        .sqlQuery(selectQuery, fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(3, 0, 2, 5, 9, 4, 0, 3)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" where repeated_count(array) > 4"", fileName)","[{'comment': 'Use other `sqlQuery` method with varargs', 'commenter': 'vdiravka'}]"
1641,exec/vector/src/main/java/org/apache/drill/exec/vector/complex/impl/RepeatedListReaderImpl.java,"@@ -80,7 +79,10 @@ public void reset() {
 
   @Override
   public int size() {
-    return maxOffset - currentOffset;
+    if (currentOffset == NO_VALUES) {
+      return 0;
+    }
+    return maxOffset - currentOffset - 1;","[{'comment': 'Consider:\r\n`return currentOffset == NO_VALUES ? 0 : maxOffset - currentOffset - 1;`', 'commenter': 'vdiravka'}, {'comment': 'Introduce also `isNull()` method like in `RepeatedMapReaderImpl` class', 'commenter': 'vdiravka'}, {'comment': 'Not sure whether this `isNull()` is actually needed. In `ReapetedMapReaderImpl` it is used in `SingleLikeRepeatedMapReaderImpl` (to check if it `isSet()`) which has  `ReapetedMapReaderImpl` as delegate.\r\n\r\nRegarding using ternary operator - it is OK, but I find current implementation cleaner (with respect to readability), because second value (`maxOffset - currentOffset - 1`) uses 3 operands.', 'commenter': 'KazydubB'}, {'comment': ""In my opinion usage ternary or full conditional construct doesn't impact code readability here. I'm confused by unclear naming of the fields and necessity to use 'magic number' along with the fields. @KazydubB  could you please rename or provide descriptive javadocs and constant for 1 to make offsets usage look clear ? "", 'commenter': 'ihuzenko'}, {'comment': '@KazydubB It will be good to have it for consistency with `ReapetedMapReaderImpl`.\r\nYou can use parentheses. \r\n`return isNull() ? 0 : (maxOffset - currentOffset - 1);`\r\nThe final decision is yours\r\n\r\n@ihuzenko Good note. I think it will be good to have a comment, why `-1` is needed. ', 'commenter': 'vdiravka'}, {'comment': 'I agree that this is not clear why the `-1` is added, so decided to decrement `maxOffset` value by `1` for the count to be an expected statement `maxOffset - currentOffset`.\r\n\r\nAlso did add a method to `RepeatedListReaderImpl` but named it `isEmpty()` (instead of `isNull()`) as it appears to fit better. ', 'commenter': 'KazydubB'}]"
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -17,36 +17,161 @@
  */
 package org.apache.drill.exec.fn.impl;
 
-import org.apache.drill.test.BaseTestQuery;
 import org.apache.drill.categories.SqlFunctionTest;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+
 @Category(SqlFunctionTest.class)
-public class TestNewSimpleRepeatedFunctions extends BaseTestQuery {
+public class TestNewSimpleRepeatedFunctions extends ClusterTest {
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+  }
+
   @Test
   public void testRepeatedContainsForWildCards() throws Exception {
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
-
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(false)
-       .baselineValues(false)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, true, true, true, false)
+        .go();
+
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, false, false, true, false)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedMap() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1}, {""field1"": 2}]}
+      {""mapArray"": [{""field2"": ""val2""}, {""field1"": 2}, {""field1"": 2}]}
+      {""mapArray"": []}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val4""}, {""field1"": 2}, {""field1"": 2}, {""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}, {""field1"": 2}]}
+    */
+    String fileName = ""repeated_count_map.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""{\""field1\"": 1, \""field2\"": \""val1\""}, {\""field1\"": 2}"",
+          ""{\""field1\"": 1}, {\""field1\"": 2}"",
+          ""{\""field2\"": \""val2\""}, {\""field1\"": 2}, {\""field1\"": 2}"",
+          """",
+          ""{\""field1\"": 1, \""field2\"": \""val3\""}"",
+          ""{\""field1\"": 1, \""field2\"": \""val4\""}, {\""field1\"": 2}, {\""field1\"": 2}, {\""field2\"": \""val1\""}, {\""field1\"": 2}"",
+          ""{\""field1\"": 1, \""field2\"": \""val3\""}, {\""field1\"": 2}""
+      };
+      for (String value : arrayElements) {
+        String entry = String.format(""{\""mapArray\"": [%s]}\n"", value);
+        writer.write(entry);
+      }
+    }
+
+    String selectQuery = ""select repeated_count(mapArray) from dfs.`%s`"";
+    testBuilder()
+        .sqlQuery(selectQuery, fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 2, 3, 0, 1, 5, 2)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" where repeated_count(mapArray) > 2"", fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(3, 5)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 3, 0, 1, 5)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1 having repeated_count(mapArray) < 3"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 0, 1)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedList() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""id"": 1, ""array"": [[1, 2], [1, 3], [2, 3]]}
+      {""id"": 2, ""array"": []}
+      {""id"": 3, ""array"": [[2, 3], [1, 3, 4]]}
+      {""id"": 4, ""array"": [[1], [2], [3, 4], [5], [6]]}
+      {""id"": 5, ""array"": [[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]]}
+      {""id"": 6, ""array"": [[1, 2], [3], [4], [5]]}
+      {""id"": 7, ""array"": []}
+      {""id"": 8, ""array"": [[1], [2], [3]]}
+    */
+    String fileName = ""repeated_count_list.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""[1, 2], [1, 3], [2, 3]"",
+          """",
+          ""[2, 3], [1, 3, 4]"",
+          ""[1], [2], [3, 4], [5], [6]"",
+          ""[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]"",
+          ""[1, 2], [3], [4], [5]"",
+          """",
+          ""[1], [2], [3]""};
+      int elementId = 1;
+      for (String value : arrayElements) {
+        String entry = String.format(""{\""id\"": %d, \""array\"": [%s]}\n"", elementId++, value);
+        writer.write(entry);
+      }
+    }","[{'comment': 'Why do not use datasource instead of creating file in the test?', 'commenter': 'vdiravka'}, {'comment': 'Added file generation because thought it will be used only for one test. Created datasource file instead.', 'commenter': 'KazydubB'}]"
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -17,36 +17,161 @@
  */
 package org.apache.drill.exec.fn.impl;
 
-import org.apache.drill.test.BaseTestQuery;
 import org.apache.drill.categories.SqlFunctionTest;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+
 @Category(SqlFunctionTest.class)
-public class TestNewSimpleRepeatedFunctions extends BaseTestQuery {
+public class TestNewSimpleRepeatedFunctions extends ClusterTest {
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+  }
+
   @Test
   public void testRepeatedContainsForWildCards() throws Exception {
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
-
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(false)
-       .baselineValues(false)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, true, true, true, false)
+        .go();
+
+    testBuilder()","[{'comment': 'It is better to divide these big tests into smaller one. Usually one `testBuilder` per test case.', 'commenter': 'vdiravka'}, {'comment': 'Good suggestion - divided all other `@Test` methods to contain one `testBuilder()` only, however left these two together (though updated) since they test the same thing.', 'commenter': 'KazydubB'}]"
1641,exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/impl/SimpleRepeatedFunctions.java,"@@ -77,8 +79,6 @@ public void eval() {
     }
   }
 
-  /*
-  // TODO - need to confirm that these work   SMP: They do not","[{'comment': 'What about TODO?\r\n> // TODO - replace with a freemarker template and fill out the rest of the types\r\n>   // focused on getting functions defined for JSON types as this is the primary format\r\n>   // users are extracting repeated data out of currently\r\n\r\nMay it be used for other datatypes?', 'commenter': 'vdiravka'}, {'comment': 'Not sure what you mean. Do you mean to create a template instead as the TODO suggests?', 'commenter': 'KazydubB'}, {'comment': '> Not sure what you mean. Do you mean to create a template instead as the TODO suggests?\r\n\r\nYes. But I am not sure that it will save much more space for current `repeated_count` functions, since there are different implementations of `eval()` method. But if some additional data types can be used with this function, then it makes sense to add them and to consider rewriting this calss with FreeMarker template.\r\nSo can other data types be used with `repeated_count` function?', 'commenter': 'vdiravka'}, {'comment': 'Moved `repeated_count` functions with uniform implementation to a separate freemarker template file. Thanks for pointing the `TODO` out :+1: ', 'commenter': 'KazydubB'}]"
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -17,36 +17,161 @@
  */
 package org.apache.drill.exec.fn.impl;
 
-import org.apache.drill.test.BaseTestQuery;
 import org.apache.drill.categories.SqlFunctionTest;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+
 @Category(SqlFunctionTest.class)
-public class TestNewSimpleRepeatedFunctions extends BaseTestQuery {
+public class TestNewSimpleRepeatedFunctions extends ClusterTest {
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+  }
+
   @Test
   public void testRepeatedContainsForWildCards() throws Exception {
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
-
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(false)
-       .baselineValues(false)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, true, true, true, false)
+        .go();
+
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, false, false, true, false)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedMap() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1}, {""field1"": 2}]}
+      {""mapArray"": [{""field2"": ""val2""}, {""field1"": 2}, {""field1"": 2}]}
+      {""mapArray"": []}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val4""}, {""field1"": 2}, {""field1"": 2}, {""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}, {""field1"": 2}]}
+    */
+    String fileName = ""repeated_count_map.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""{\""field1\"": 1, \""field2\"": \""val1\""}, {\""field1\"": 2}"",
+          ""{\""field1\"": 1}, {\""field1\"": 2}"",
+          ""{\""field2\"": \""val2\""}, {\""field1\"": 2}, {\""field1\"": 2}"",
+          """",
+          ""{\""field1\"": 1, \""field2\"": \""val3\""}"",
+          ""{\""field1\"": 1, \""field2\"": \""val4\""}, {\""field1\"": 2}, {\""field1\"": 2}, {\""field2\"": \""val1\""}, {\""field1\"": 2}"",
+          ""{\""field1\"": 1, \""field2\"": \""val3\""}, {\""field1\"": 2}""
+      };
+      for (String value : arrayElements) {
+        String entry = String.format(""{\""mapArray\"": [%s]}\n"", value);
+        writer.write(entry);
+      }
+    }
+
+    String selectQuery = ""select repeated_count(mapArray) from dfs.`%s`"";
+    testBuilder()
+        .sqlQuery(selectQuery, fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 2, 3, 0, 1, 5, 2)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" where repeated_count(mapArray) > 2"", fileName)
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(3, 5)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 3, 0, 1, 5)
+        .go();
+
+    testBuilder()
+        .sqlQuery(selectQuery + "" group by 1 having repeated_count(mapArray) < 3"", fileName)
+        .unOrdered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(2, 0, 1)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedList() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""id"": 1, ""array"": [[1, 2], [1, 3], [2, 3]]}
+      {""id"": 2, ""array"": []}
+      {""id"": 3, ""array"": [[2, 3], [1, 3, 4]]}
+      {""id"": 4, ""array"": [[1], [2], [3, 4], [5], [6]]}
+      {""id"": 5, ""array"": [[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]]}
+      {""id"": 6, ""array"": [[1, 2], [3], [4], [5]]}
+      {""id"": 7, ""array"": []}
+      {""id"": 8, ""array"": [[1], [2], [3]]}
+    */
+    String fileName = ""repeated_count_list.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""[1, 2], [1, 3], [2, 3]"",","[{'comment': 'Use `TestBuilder.listOf()` to create a `JsonStringArrayList` from the given values.\r\n', 'commenter': 'vdiravka'}]"
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -17,36 +17,161 @@
  */
 package org.apache.drill.exec.fn.impl;
 
-import org.apache.drill.test.BaseTestQuery;
 import org.apache.drill.categories.SqlFunctionTest;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+
 @Category(SqlFunctionTest.class)
-public class TestNewSimpleRepeatedFunctions extends BaseTestQuery {
+public class TestNewSimpleRepeatedFunctions extends ClusterTest {
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+  }
+
   @Test
   public void testRepeatedContainsForWildCards() throws Exception {
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
-
-     testBuilder().
-       sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
-       .ordered()
-       .baselineColumns(""EXPR$0"")
-       .baselineValues(true)
-       .baselineValues(false)
-       .baselineValues(false)
-       .baselineValues(true)
-       .baselineValues(false)
-       .build().run();
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, true, true, true, false)
+        .go();
+
+    testBuilder()
+        .sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
+        .ordered()
+        .baselineColumns(""EXPR$0"")
+        .baselineValuesForSingleColumn(true, false, false, true, false)
+        .go();
+  }
+
+  @Test
+  public void testRepeatedCountRepeatedMap() throws Exception {
+    // Contents of the generated file:
+    /*
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1}, {""field1"": 2}]}
+      {""mapArray"": [{""field2"": ""val2""}, {""field1"": 2}, {""field1"": 2}]}
+      {""mapArray"": []}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val4""}, {""field1"": 2}, {""field1"": 2}, {""field2"": ""val1""}, {""field1"": 2}]}
+      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}, {""field1"": 2}]}
+    */
+    String fileName = ""repeated_count_map.json"";
+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
+        new File(dirTestWatcher.getRootDir(), fileName)))) {
+      String[] arrayElements = {
+          ""{\""field1\"": 1, \""field2\"": \""val1\""}, {\""field1\"": 2}"",","[{'comment': 'Use `TestBuilder.mapOf()` to create a `JsonStringHashMap` from the given values.', 'commenter': 'vdiravka'}]"
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -24,154 +24,72 @@
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileWriter;
+import java.nio.file.Paths;
 
 @Category(SqlFunctionTest.class)
 public class TestNewSimpleRepeatedFunctions extends ClusterTest {
 
+  private static final String SELECT_REPEATED_CONTAINS = ""select repeated_contains(topping, '%s*') from cp.`testRepeatedWrite.json`"";","[{'comment': 'It can be used as a local variable for `testRepeatedContainsForWildCards` test', 'commenter': 'vdiravka'}]"
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -24,154 +24,72 @@
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileWriter;
+import java.nio.file.Paths;
 
 @Category(SqlFunctionTest.class)
 public class TestNewSimpleRepeatedFunctions extends ClusterTest {
 
+  private static final String SELECT_REPEATED_CONTAINS = ""select repeated_contains(topping, '%s*') from cp.`testRepeatedWrite.json`"";
+  private static final String SELECT_REPEATED_COUNT_LIST = ""select repeated_count(array) from dfs.`functions/repeated/repeated_list.json`"";
+  private static final String SELECT_REPEATED_COUNT_MAP = ""select repeated_count(mapArray) from dfs.`functions/repeated/repeated_map.json`"";
+  private static final String COLUMN_NAME = ""EXPR$0"";
+
   @BeforeClass
   public static void setUp() throws Exception {
+    dirTestWatcher.copyResourceToRoot(Paths.get(""functions"", ""repeated""));
     startCluster(ClusterFixture.builder(dirTestWatcher));
   }
 
   @Test
   public void testRepeatedContainsForWildCards() throws Exception {
-    testBuilder()
-        .sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(true, true, true, true, false)
-        .go();
-
-    testBuilder()
-        .sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(true, false, false, true, false)
-        .go();
+    performTest(SELECT_REPEATED_CONTAINS, new String[] {""Choc""}, true, true, true, true, false);
+    performTest(SELECT_REPEATED_CONTAINS, new String[] {""Pow""}, true, false, false, true, false);
   }
 
   @Test
   public void testRepeatedCountRepeatedMap() throws Exception {
-    // Contents of the generated file:
-    /*
-      {""mapArray"": [{""field1"": 1, ""field2"": ""val1""}, {""field1"": 2}]}
-      {""mapArray"": [{""field1"": 1}, {""field1"": 2}]}
-      {""mapArray"": [{""field2"": ""val2""}, {""field1"": 2}, {""field1"": 2}]}
-      {""mapArray"": []}
-      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}]}
-      {""mapArray"": [{""field1"": 1, ""field2"": ""val4""}, {""field1"": 2}, {""field1"": 2}, {""field2"": ""val1""}, {""field1"": 2}]}
-      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}, {""field1"": 2}]}
-    */
-    String fileName = ""repeated_count_map.json"";
-    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
-        new File(dirTestWatcher.getRootDir(), fileName)))) {
-      String[] arrayElements = {
-          ""{\""field1\"": 1, \""field2\"": \""val1\""}, {\""field1\"": 2}"",
-          ""{\""field1\"": 1}, {\""field1\"": 2}"",
-          ""{\""field2\"": \""val2\""}, {\""field1\"": 2}, {\""field1\"": 2}"",
-          """",
-          ""{\""field1\"": 1, \""field2\"": \""val3\""}"",
-          ""{\""field1\"": 1, \""field2\"": \""val4\""}, {\""field1\"": 2}, {\""field1\"": 2}, {\""field2\"": \""val1\""}, {\""field1\"": 2}"",
-          ""{\""field1\"": 1, \""field2\"": \""val3\""}, {\""field1\"": 2}""
-      };
-      for (String value : arrayElements) {
-        String entry = String.format(""{\""mapArray\"": [%s]}\n"", value);
-        writer.write(entry);
-      }
-    }
-
-    String selectQuery = ""select repeated_count(mapArray) from dfs.`%s`"";
-    testBuilder()
-        .sqlQuery(selectQuery, fileName)
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(2, 2, 3, 0, 1, 5, 2)
-        .go();
-
-    testBuilder()
-        .sqlQuery(selectQuery + "" where repeated_count(mapArray) > 2"", fileName)
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(3, 5)
-        .go();
+    performTest(SELECT_REPEATED_COUNT_MAP, 2, 2, 3, 0, 1, 5, 2);
+  }
 
-    testBuilder()
-        .sqlQuery(selectQuery + "" group by 1"", fileName)
-        .unOrdered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(2, 3, 0, 1, 5)
-        .go();
+  @Test
+  public void testRepeatedCountRepeatedMapInWhere() throws Exception {
+    String query = SELECT_REPEATED_COUNT_MAP + "" where repeated_count(mapArray) > 2"";
+    performTest(query, 3, 5);
+  }
 
-    testBuilder()
-        .sqlQuery(selectQuery + "" group by 1 having repeated_count(mapArray) < 3"", fileName)
-        .unOrdered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(2, 0, 1)
-        .go();
+  @Test
+  public void testRepeatedCountRepeatedMapInHaving() throws Exception {
+    String query = SELECT_REPEATED_COUNT_MAP + "" group by 1 having repeated_count(mapArray) < 3"";
+    performTest(query, 2, 0, 1);
   }
 
   @Test
   public void testRepeatedCountRepeatedList() throws Exception {
-    // Contents of the generated file:
-    /*
-      {""id"": 1, ""array"": [[1, 2], [1, 3], [2, 3]]}
-      {""id"": 2, ""array"": []}
-      {""id"": 3, ""array"": [[2, 3], [1, 3, 4]]}
-      {""id"": 4, ""array"": [[1], [2], [3, 4], [5], [6]]}
-      {""id"": 5, ""array"": [[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]]}
-      {""id"": 6, ""array"": [[1, 2], [3], [4], [5]]}
-      {""id"": 7, ""array"": []}
-      {""id"": 8, ""array"": [[1], [2], [3]]}
-    */
-    String fileName = ""repeated_count_list.json"";
-    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
-        new File(dirTestWatcher.getRootDir(), fileName)))) {
-      String[] arrayElements = {
-          ""[1, 2], [1, 3], [2, 3]"",
-          """",
-          ""[2, 3], [1, 3, 4]"",
-          ""[1], [2], [3, 4], [5], [6]"",
-          ""[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]"",
-          ""[1, 2], [3], [4], [5]"",
-          """",
-          ""[1], [2], [3]""};
-      int elementId = 1;
-      for (String value : arrayElements) {
-        String entry = String.format(""{\""id\"": %d, \""array\"": [%s]}\n"", elementId++, value);
-        writer.write(entry);
-      }
-    }
+    performTest(SELECT_REPEATED_COUNT_LIST, 3, 0, 2, 5, 9, 4, 0, 3);
+  }
 
-    String selectQuery = ""select repeated_count(array) from dfs.`%s`"";
-    testBuilder()
-        .sqlQuery(selectQuery, fileName)
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(3, 0, 2, 5, 9, 4, 0, 3)
-        .go();
+  @Test
+  public void testRepeatedCountRepeatedListInWhere() throws Exception {
+    String query = SELECT_REPEATED_COUNT_LIST + "" where repeated_count(array) > 4"";
+    performTest(query, 5, 9);
+  }
 
-    testBuilder()
-        .sqlQuery(selectQuery + "" where repeated_count(array) > 4"", fileName)
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(5, 9)
-        .go();
+  @Test
+  public void testRepeatedCountRepeatedListInHaving() throws Exception {
+    String query = SELECT_REPEATED_COUNT_LIST + "" group by 1 having repeated_count(array) < 4"";","[{'comment': 'There is overloaded `sqlQuery(String query, Object... replacements)` method.', 'commenter': 'vdiravka'}, {'comment': 'Not sure why this should be used - I find the concatenation fine. If you have any objections, please feel free to point it out.\r\nJust to be sure, do you mean to have the base query with `%s` so the additional part should be formatted (well, passed to the method as the second parameter)?', 'commenter': 'KazydubB'}, {'comment': 'agree, it will be more complicated with `String.format`', 'commenter': 'vdiravka'}]"
1641,exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestNewSimpleRepeatedFunctions.java,"@@ -24,154 +24,72 @@
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileWriter;
+import java.nio.file.Paths;
 
 @Category(SqlFunctionTest.class)
 public class TestNewSimpleRepeatedFunctions extends ClusterTest {
 
+  private static final String SELECT_REPEATED_CONTAINS = ""select repeated_contains(topping, '%s*') from cp.`testRepeatedWrite.json`"";
+  private static final String SELECT_REPEATED_COUNT_LIST = ""select repeated_count(array) from dfs.`functions/repeated/repeated_list.json`"";
+  private static final String SELECT_REPEATED_COUNT_MAP = ""select repeated_count(mapArray) from dfs.`functions/repeated/repeated_map.json`"";
+  private static final String COLUMN_NAME = ""EXPR$0"";
+
   @BeforeClass
   public static void setUp() throws Exception {
+    dirTestWatcher.copyResourceToRoot(Paths.get(""functions"", ""repeated""));
     startCluster(ClusterFixture.builder(dirTestWatcher));
   }
 
   @Test
   public void testRepeatedContainsForWildCards() throws Exception {
-    testBuilder()
-        .sqlQuery(""select repeated_contains(topping, 'Choc*') from cp.`testRepeatedWrite.json`"")
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(true, true, true, true, false)
-        .go();
-
-    testBuilder()
-        .sqlQuery(""select repeated_contains(topping, 'Pow*') from cp.`testRepeatedWrite.json`"")
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(true, false, false, true, false)
-        .go();
+    performTest(SELECT_REPEATED_CONTAINS, new String[] {""Choc""}, true, true, true, true, false);
+    performTest(SELECT_REPEATED_CONTAINS, new String[] {""Pow""}, true, false, false, true, false);
   }
 
   @Test
   public void testRepeatedCountRepeatedMap() throws Exception {
-    // Contents of the generated file:
-    /*
-      {""mapArray"": [{""field1"": 1, ""field2"": ""val1""}, {""field1"": 2}]}
-      {""mapArray"": [{""field1"": 1}, {""field1"": 2}]}
-      {""mapArray"": [{""field2"": ""val2""}, {""field1"": 2}, {""field1"": 2}]}
-      {""mapArray"": []}
-      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}]}
-      {""mapArray"": [{""field1"": 1, ""field2"": ""val4""}, {""field1"": 2}, {""field1"": 2}, {""field2"": ""val1""}, {""field1"": 2}]}
-      {""mapArray"": [{""field1"": 1, ""field2"": ""val3""}, {""field1"": 2}]}
-    */
-    String fileName = ""repeated_count_map.json"";
-    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
-        new File(dirTestWatcher.getRootDir(), fileName)))) {
-      String[] arrayElements = {
-          ""{\""field1\"": 1, \""field2\"": \""val1\""}, {\""field1\"": 2}"",
-          ""{\""field1\"": 1}, {\""field1\"": 2}"",
-          ""{\""field2\"": \""val2\""}, {\""field1\"": 2}, {\""field1\"": 2}"",
-          """",
-          ""{\""field1\"": 1, \""field2\"": \""val3\""}"",
-          ""{\""field1\"": 1, \""field2\"": \""val4\""}, {\""field1\"": 2}, {\""field1\"": 2}, {\""field2\"": \""val1\""}, {\""field1\"": 2}"",
-          ""{\""field1\"": 1, \""field2\"": \""val3\""}, {\""field1\"": 2}""
-      };
-      for (String value : arrayElements) {
-        String entry = String.format(""{\""mapArray\"": [%s]}\n"", value);
-        writer.write(entry);
-      }
-    }
-
-    String selectQuery = ""select repeated_count(mapArray) from dfs.`%s`"";
-    testBuilder()
-        .sqlQuery(selectQuery, fileName)
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(2, 2, 3, 0, 1, 5, 2)
-        .go();
-
-    testBuilder()
-        .sqlQuery(selectQuery + "" where repeated_count(mapArray) > 2"", fileName)
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(3, 5)
-        .go();
+    performTest(SELECT_REPEATED_COUNT_MAP, 2, 2, 3, 0, 1, 5, 2);
+  }
 
-    testBuilder()
-        .sqlQuery(selectQuery + "" group by 1"", fileName)
-        .unOrdered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(2, 3, 0, 1, 5)
-        .go();
+  @Test
+  public void testRepeatedCountRepeatedMapInWhere() throws Exception {
+    String query = SELECT_REPEATED_COUNT_MAP + "" where repeated_count(mapArray) > 2"";
+    performTest(query, 3, 5);
+  }
 
-    testBuilder()
-        .sqlQuery(selectQuery + "" group by 1 having repeated_count(mapArray) < 3"", fileName)
-        .unOrdered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(2, 0, 1)
-        .go();
+  @Test
+  public void testRepeatedCountRepeatedMapInHaving() throws Exception {
+    String query = SELECT_REPEATED_COUNT_MAP + "" group by 1 having repeated_count(mapArray) < 3"";
+    performTest(query, 2, 0, 1);
   }
 
   @Test
   public void testRepeatedCountRepeatedList() throws Exception {
-    // Contents of the generated file:
-    /*
-      {""id"": 1, ""array"": [[1, 2], [1, 3], [2, 3]]}
-      {""id"": 2, ""array"": []}
-      {""id"": 3, ""array"": [[2, 3], [1, 3, 4]]}
-      {""id"": 4, ""array"": [[1], [2], [3, 4], [5], [6]]}
-      {""id"": 5, ""array"": [[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]]}
-      {""id"": 6, ""array"": [[1, 2], [3], [4], [5]]}
-      {""id"": 7, ""array"": []}
-      {""id"": 8, ""array"": [[1], [2], [3]]}
-    */
-    String fileName = ""repeated_count_list.json"";
-    try (BufferedWriter writer = new BufferedWriter(new FileWriter(
-        new File(dirTestWatcher.getRootDir(), fileName)))) {
-      String[] arrayElements = {
-          ""[1, 2], [1, 3], [2, 3]"",
-          """",
-          ""[2, 3], [1, 3, 4]"",
-          ""[1], [2], [3, 4], [5], [6]"",
-          ""[1, 2, 3], [4, 5], [6], [7], [8, 9], [2, 3], [2, 3], [2, 3], [2]"",
-          ""[1, 2], [3], [4], [5]"",
-          """",
-          ""[1], [2], [3]""};
-      int elementId = 1;
-      for (String value : arrayElements) {
-        String entry = String.format(""{\""id\"": %d, \""array\"": [%s]}\n"", elementId++, value);
-        writer.write(entry);
-      }
-    }
+    performTest(SELECT_REPEATED_COUNT_LIST, 3, 0, 2, 5, 9, 4, 0, 3);
+  }
 
-    String selectQuery = ""select repeated_count(array) from dfs.`%s`"";
-    testBuilder()
-        .sqlQuery(selectQuery, fileName)
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(3, 0, 2, 5, 9, 4, 0, 3)
-        .go();
+  @Test
+  public void testRepeatedCountRepeatedListInWhere() throws Exception {
+    String query = SELECT_REPEATED_COUNT_LIST + "" where repeated_count(array) > 4"";
+    performTest(query, 5, 9);
+  }
 
-    testBuilder()
-        .sqlQuery(selectQuery + "" where repeated_count(array) > 4"", fileName)
-        .ordered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(5, 9)
-        .go();
+  @Test
+  public void testRepeatedCountRepeatedListInHaving() throws Exception {
+    String query = SELECT_REPEATED_COUNT_LIST + "" group by 1 having repeated_count(array) < 4"";
+    performTest(query, 3, 0, 2);
+  }
 
-    testBuilder()
-        .sqlQuery(selectQuery + "" group by 1"", fileName)
-        .unOrdered()
-        .baselineColumns(""EXPR$0"")
-        .baselineValuesForSingleColumn(3, 0, 2, 5, 9, 4)
-        .go();
+  private void performTest(String query, Object... expectedValues) throws Exception {","[{'comment': 'I see why you added it, but tests are worse readable with this method. It is necessary to see the implementation of this method to understand the tests.\r\nI suggest you to use regular `testBuilder` here, but if you want to proceed with this, please add it to the `ClusterTest` class.', 'commenter': 'vdiravka'}, {'comment': 'Removed the method.', 'commenter': 'KazydubB'}]"
1641,exec/java-exec/src/main/java/org/apache/drill/exec/expr/EvaluationVisitor.java,"@@ -479,6 +479,14 @@ private HoldingContainer visitValueVectorReadExpression(ValueVectorReadExpressio
 
       if (!hasReadPath && !complex) {
         JBlock eval = new JBlock();
+
+        if (repeated) {","[{'comment': ""Was it the bug?\r\nAm I right, we just didn't reach this part of code with other functions with `Repeated` DataMode?"", 'commenter': 'vdiravka'}, {'comment': ""It was kind of a bug. Yes, this code wasn't reached by other functions (at least those I am aware of)."", 'commenter': 'KazydubB'}]"
1643,distribution/pom.xml,"@@ -441,26 +441,18 @@
                   <goal>jdeb</goal>
                 </goals>
                 <configuration>
-                  <deb>target/drill-${project.version}.deb</deb>
+                  <deb>target/apache-drill-${project.version}.deb</deb>","[{'comment': 'Could you please update Maven org.vafer:jdeb plugin to the latest 1.7 version?\r\nhttps://mvnrepository.com/artifact/org.vafer/jdeb', 'commenter': 'vdiravka'}, {'comment': 'Add the following configs in order to update plugin version:\r\n```\r\n<skipPOMs>false</skipPOMs>\r\n<deb>distribution/target/apache-drill-${project.version}/apache-drill-${project.version}.deb</deb>\r\n```\r\nsee more: https://github.com/tcurdt/jdeb/blob/master/docs/maven.md', 'commenter': 'vdiravka'}, {'comment': '@vdiravka Thank you very much. I made the changes and the patch is ready. I am planning to test on a clean slate system both the DEB/RPM packages and push the patch for review hopefully by tomorrow.', 'commenter': 'nareshgbhat'}]"
1648,exec/java-exec/src/main/java/org/apache/drill/exec/util/ValueVectorElementFormatter.java,"@@ -71,6 +71,9 @@ public String format(Object value, TypeProtos.MinorType minorType) {
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIME),
                         (v, p) -> v.format(getTimeFormatter(p)));
         }
+      case VARBINARY:
+        byte[] bytes = (byte[]) value;","[{'comment': ""This logic inside ```case VARBINARY:``` may cause  problems. The reason is that whole switch case is suspiciously implemented, check previous case statements : \r\n```java\r\n     case TIMESTAMP:\r\n        if (value instanceof LocalDateTime) {\r\n          return format((LocalDateTime) value,\r\n                        options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIMESTAMP),\r\n                        (v, p) -> v.format(getTimestampFormatter(p)));\r\n        }\r\n      case DATE:\r\n        if (value instanceof LocalDate) {\r\n          return format((LocalDate) value,\r\n                        options.getString(ExecConstants.WEB_DISPLAY_FORMAT_DATE),\r\n                        (v, p) -> v.format(getDateFormatter(p)));\r\n        }\r\n      case TIME:\r\n        if (value instanceof LocalTime) {\r\n          return format((LocalTime) value,\r\n                        options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIME),\r\n                        (v, p) -> v.format(getTimeFormatter(p)));\r\n        } \r\n```\r\nHere there is no ```break``` keyword after any ```case``` match, so if for example in ```switch (minorType)``` we got TIMESTAMP, but ```value instanceof LocalDateTime``` returned false, we will fall through all other ```case``` checks and code within each next ```case``` section will be called. Despite of this all previous cases won't fail (because each one contains instance of check), but yours will cast to ```byte[]``` immediately and may fail here. In my opinion it would be better to rewrite the switch case by adding ```break```s and also ensure that for ```VARBINARY```  casting to ```byte[]``` is always safe.    \r\n"", 'commenter': 'ihuzenko'}]"
1651,distribution/pom.xml,"@@ -433,34 +473,67 @@
           <plugin>
             <artifactId>jdeb</artifactId>
             <groupId>org.vafer</groupId>
-            <version>1.0</version>
+            <version>1.7</version>
             <executions>
               <execution>
                 <phase>package</phase>
                 <goals>
                   <goal>jdeb</goal>
                 </goals>
-                <configuration>
+		<configuration>
+		<skipPOMs>false</skipPOMs>
                   <deb>target/drill-${project.version}.deb</deb>","[{'comment': 'It will be better to keep all archives in one place. Since tarball archive is already in this directory, please use it for `rpm` and `deb` archives too.\r\n```suggestion\r\n                  <deb>distribution/target/apache-drill-${project.version}/apache-drill-${project.version}.deb</deb>\r\n```', 'commenter': 'vdiravka'}, {'comment': '@vdiravka Thank you very much for review.  I try to address all your comments.  Please review.', 'commenter': 'nareshgbhat'}]"
1651,distribution/pom.xml,"@@ -379,34 +379,74 @@
               <name>drill</name>","[{'comment': ""Currently the name of rpm is `drill-1.0-SNAPSHOT.noarch.rpm`\r\n1. \r\n```suggestion\r\n              <name>apache-drill</name>\r\n```\r\n2. Edit an above string:\r\nfrom `<version>1.0</version>` -> `<version>1.16.0-SNAPSHOT</version>`\r\n\r\n3. `<copyright>` tag isn't supported anymore for this plugin. Please use `<license>` instead of it. I think to put one line link `http://www.apache.org/licenses/LICENSE-2.0` as content instead of `2013 ASF` will be fine."", 'commenter': 'vdiravka'}, {'comment': 'OK sure I will make the rpm tobe named as apache-drill-1.16.0-SNAPSHOT.noarch.rpm\r\nHow about below code change\r\n[centos@centos drill]$ git diff\r\ndiff --git a/distribution/pom.xml b/distribution/pom.xml\r\nindex 5005155..e73bfd7 100644\r\n--- a/distribution/pom.xml\r\n+++ b/distribution/pom.xml\r\n@@ -196,9 +196,7 @@\r\n         <inherited>true</inherited>\r\n         <configuration>\r\n           <excludes>\r\n-            <exclude>**/conffiles</exclude>\r\n             <exclude>**/git.properties</exclude>\r\n-            <exclude>**/control</exclude>\r\n             <exclude>**/*.checkstyle</exclude>\r\n             <exclude>**/*.json</exclude>\r\n             <exclude>**/README.md</exclude>\r\n@@ -371,12 +369,12 @@\r\n               </execution>\r\n             </executions>\r\n             <configuration>\r\n-              <copyright>2013 ASF</copyright>\r\n+              <license>http://www.apache.org/licenses/LICENSE-2.0</license>\r\n               <group>Apache Software Foundation</group>\r\n               <prefix>/opt</prefix>\r\n               <release>SNAPSHOT</release>\r\n-              <version>1.0</version>\r\n-              <name>drill</name>\r\n+              <version>1.16.0</version>\r\n+              <name>apache-drill</name>\r\n               <mappings>\r\n                 <mapping>\r\n                   <directory>/opt/drill</directory>\r\n[centos@centos drill]$ \r\n', 'commenter': 'nareshgbhat'}, {'comment': ""@vdiravka That's really a very good review.  Thank you very much for suggestions.  Let me know if there are any more changes. Not sure keeping as SNAPSHOT makes sense after mentioning a release version.  I will also go through and double check any more changes are required."", 'commenter': 'nareshgbhat'}]"
1651,distribution/pom.xml,"@@ -371,42 +369,82 @@
               </execution>
             </executions>
             <configuration>
-              <copyright>2013 ASF</copyright>
+              <license>http://www.apache.org/licenses/LICENSE-2.0</license>
               <group>Apache Software Foundation</group>
               <prefix>/opt</prefix>
               <release>SNAPSHOT</release>
-              <version>1.0</version>
-              <name>drill</name>
+              <version>1.16.0</version>","[{'comment': '```suggestion\r\n              <version>${project.version}</version>\r\n```', 'commenter': 'vdiravka'}, {'comment': 'Regarding `<release>SNAPSHOT</release>`, looks like you can drop this line.', 'commenter': 'vdiravka'}]"
1651,distribution/pom.xml,"@@ -196,9 +196,7 @@
         <inherited>true</inherited>
         <configuration>
           <excludes>
-            <exclude>**/conffiles</exclude>","[{'comment': 'Also please drop these lines from `maven-license-plugin` in Drill root POM file:\r\n[`<exclude>**/control</exclude>`](https://github.com/apache/drill/blob/master/pom.xml#L674) and \r\n[`<conffiles>SCRIPT_STYLE</conffiles>`](https://github.com/apache/drill/blob/master/pom.xml#L701)\r\nand these [files itself](https://github.com/apache/drill/tree/master/distribution/src/deb/control).\r\n\r\n', 'commenter': 'vdiravka'}]"
1651,distribution/pom.xml,"@@ -371,42 +369,81 @@
               </execution>
             </executions>
             <configuration>
-              <copyright>2013 ASF</copyright>
+              <license>http://www.apache.org/licenses/LICENSE-2.0</license>","[{'comment': 'Looks like the following content for the license will be better, see [docs](https://github.com/mojohaus/rpm-maven-plugin/blob/9d66d4dbbbefb79a0fe01e0ef538668f8444cbcd/src/site/apt/ident-params.apt#L55):\r\n```suggestion\r\n              <license>${project.organization} ${project.licenses}</license>\r\n```', 'commenter': 'vdiravka'}, {'comment': 'I am not able to see 1.16.0 branch on https://github.com/apache/drill.git either ""git tag"" or ""git branch -a"" command will list the 1.16.0 branch. Can you please suggest ?', 'commenter': 'nareshgbhat'}, {'comment': '@vdiravka I have removed the `conffiles` and `control` files from drill root directory pom.xml file https://github.com/apache/drill/compare/c4feef490301f2b9e0f28a1b18dead7117cc3da8..28e617cc8b556e4adadb81379a31cdaa72a13d56 let me know if I am missing anything. ', 'commenter': 'nareshgbhat'}, {'comment': '@vdiravka I have also removed SNAPSHOT from distribution/pom.xml file hence how come it can generate `apache-drill-1.16.0-SNAPSHOT20190228134219.noarch.rpm` ?  Are you reviewing my latest patch ? You can see the difference https://github.com/apache/drill/compare/c4feef490301f2b9e0f28a1b18dead7117cc3da8..28e617cc8b556e4adadb81379a31cdaa72a13d56', 'commenter': 'nareshgbhat'}, {'comment': ""* There is no 1.16.0 branch, the last Drill release is 1.15.0. But current Drill [master](https://github.com/apache/drill) branch has [1.16.0-SNAPSHOT](https://github.com/apache/drill/blob/master/pom.xml#L33) version\r\n* No, you didn't remove these files. You removed only their mentions in Drill Maven plugins. This is your branch with this files: https://github.com/nareshgbhat/drill/tree/DRILL-7042/distribution/src/deb/control\r\n* You will have `rpm` with this name after rebase onto Drill master branch. And you are right `<release>SNAPSHOT</release>` is redundant.\r\n\r\nSo please:\r\n1. rebase your branch onto Drill master branch and force push your changes \r\n2. remove `conffiles` and `control` files \r\n3. update `<license>` content"", 'commenter': 'vdiravka'}, {'comment': '@vdiravka  DONE.', 'commenter': 'nareshgbhat'}, {'comment': ""But it isn't done. \r\n`<license>http://www.apache.org/licenses/LICENSE-2.0</license>` -> `\r\n<license>${project.organization} ${project.licenses}</license>` as I suggested in the first comment.\r\n\r\nAnd `conffiles` and `control` files are not deleted. They still present in your branch. This is the reason why TravisCI build is failed under your branch: https://travis-ci.org/apache/drill/builds/500336628?utm_source=github_status&utm_medium=notification\r\nthe checks for `maven-license-plugin` is removed, but files are not deleted.\r\nPlease fix above two issues and then PR can be merged. Or if you want I can fix it in the separate PR, but with your commit."", 'commenter': 'vdiravka'}]"
1663,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonEscapeAnyChar.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.complex.writer;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.drill.common.exceptions.UserRemoteException;
+import org.apache.drill.test.BaseTestQuery;
+import org.apache.drill.exec.ExecConstants;
+import org.junit.Test;
+
+import java.io.File;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertThat;
+
+public class TestJsonEscapeAnyChar extends BaseTestQuery {","[{'comment': 'Please try to use `ClusterTest` since `BaseTestQuery` is deprecated.', 'commenter': 'sohami'}, {'comment': ""Many tests I referred to in the complex writer package are still using BaseTestQuery. I'll try and see if I can change the tests to use ClusterTest. Please share if you have any tests I can refer."", 'commenter': 'Agirish'}, {'comment': 'Done. Please check.', 'commenter': 'Agirish'}]"
1663,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonEscapeAnyChar.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.complex.writer;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.drill.common.exceptions.UserRemoteException;
+import org.apache.drill.test.BaseTestQuery;
+import org.apache.drill.exec.ExecConstants;
+import org.junit.Test;
+
+import java.io.File;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertThat;
+
+public class TestJsonEscapeAnyChar extends BaseTestQuery {
+
+  @Test
+  public void testwithOptionEnabled() throws Exception {
+    String table = ""escape.json"";
+    File file = new File(dirTestWatcher.getRootDir(), table);
+    String json = ""{\""name\"": \""ABC\\S\""}"";
+    String query = String.format(""select * from dfs.`%s`"", table);","[{'comment': ""Line 35 to 38 can be moved to a testSetup method annotated with `@Before` tag so that you don't have to do same thing in both the tests."", 'commenter': 'sohami'}, {'comment': 'Done. Please check.', 'commenter': 'Agirish'}]"
1663,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonEscapeAnyChar.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.complex.writer;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.drill.common.exceptions.UserRemoteException;
+import org.apache.drill.test.BaseTestQuery;
+import org.apache.drill.exec.ExecConstants;
+import org.junit.Test;
+
+import java.io.File;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertThat;
+
+public class TestJsonEscapeAnyChar extends BaseTestQuery {
+
+  @Test
+  public void testwithOptionEnabled() throws Exception {
+    String table = ""escape.json"";
+    File file = new File(dirTestWatcher.getRootDir(), table);
+    String json = ""{\""name\"": \""ABC\\S\""}"";
+    String query = String.format(""select * from dfs.`%s`"", table);
+    try {
+      FileUtils.writeStringToFile(file, json);
+      test(""set `%s` = true"", ExecConstants.JSON_READER_ESCAPE_ANY_CHAR);
+      testBuilder()
+        .sqlQuery(query)
+        .unOrdered()
+        .baselineColumns(""name"")
+        .baselineValues(""ABCS"")
+        .build()
+        .run();
+    } finally {
+      test(""reset `%s`"", ExecConstants.JSON_READER_ESCAPE_ANY_CHAR);
+      FileUtils.deleteQuietly(file);
+    }
+  }
+
+  @Test(expected = UserRemoteException.class)
+  public void testwithOptionDisabled() throws Exception {
+    String table = ""escape.json"";
+    File file = new File(dirTestWatcher.getRootDir(), table);
+    String json = ""{\""name\"": \""ABC\\S\""}"";
+    String query = String.format(""select * from dfs.`%s`"", table);
+    try {
+      FileUtils.writeStringToFile(file, json);
+      test(query);
+    } catch (UserRemoteException e) {
+      assertThat(e.getMessage(), containsString(""DATA_READ ERROR: Error parsing JSON - Unrecognized character escape""));
+      throw e;
+    } finally {
+      FileUtils.deleteQuietly(file);","[{'comment': 'can be moved to `testCleanup` method annotated with `@After` tag', 'commenter': 'sohami'}, {'comment': 'Done. Please check.', 'commenter': 'Agirish'}]"
1663,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonEscapeAnyChar.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.complex.writer;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.drill.common.exceptions.UserRemoteException;
+import org.apache.drill.test.BaseTestQuery;
+import org.apache.drill.exec.ExecConstants;
+import org.junit.Test;
+
+import java.io.File;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertThat;
+
+public class TestJsonEscapeAnyChar extends BaseTestQuery {
+
+  @Test
+  public void testwithOptionEnabled() throws Exception {
+    String table = ""escape.json"";
+    File file = new File(dirTestWatcher.getRootDir(), table);
+    String json = ""{\""name\"": \""ABC\\S\""}"";
+    String query = String.format(""select * from dfs.`%s`"", table);
+    try {
+      FileUtils.writeStringToFile(file, json);
+      test(""set `%s` = true"", ExecConstants.JSON_READER_ESCAPE_ANY_CHAR);
+      testBuilder()
+        .sqlQuery(query)
+        .unOrdered()
+        .baselineColumns(""name"")
+        .baselineValues(""ABCS"")
+        .build()
+        .run();
+    } finally {
+      test(""reset `%s`"", ExecConstants.JSON_READER_ESCAPE_ANY_CHAR);
+      FileUtils.deleteQuietly(file);
+    }
+  }
+
+  @Test(expected = UserRemoteException.class)","[{'comment': 'not needed since you are catching `UserRemoteException` only', 'commenter': 'sohami'}, {'comment': 'Looks like it is needed. The test fails without it. I double checked. ', 'commenter': 'Agirish'}, {'comment': ""that's because you are throwing the exception which is caught in catch block. Remove line 72 then it should be fine."", 'commenter': 'sohami'}, {'comment': 'Done. Thanks!', 'commenter': 'Agirish'}]"
1663,exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java,"@@ -108,6 +109,7 @@ public Builder(DrillBuf managedBuf) {
       this.skipOuterList = false;
       this.allTextMode = false;
       this.enableNanInf = true;
+      this.enableEscapeAnyChar = false;","[{'comment': 'default value will be false so not required to explicit set it.', 'commenter': 'sohami'}, {'comment': 'Lines above this are setting this to false explicitly, so to keep it consistent I set it to false. But sure, I can change it. Do I also change others to remove the explicit false assignment?', 'commenter': 'Agirish'}, {'comment': ""I think i'll leave it as-is for consistency and clarity reasons - unless you think otherwise. If not, we'll need to change other options above as well. "", 'commenter': 'Agirish'}, {'comment': ""My suggestion would be to cleanup others as well but it's not mandatory."", 'commenter': 'sohami'}, {'comment': 'Done. I removed the explicit initialization of these variables in the constructor. ', 'commenter': 'Agirish'}, {'comment': 'Please check.', 'commenter': 'Agirish'}]"
1663,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonEscapeAnyChar.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.complex.writer;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.drill.common.exceptions.UserRemoteException;
+import org.apache.drill.test.BaseTestQuery;
+import org.apache.drill.exec.ExecConstants;
+import org.junit.Test;
+
+import java.io.File;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertThat;
+
+public class TestJsonEscapeAnyChar extends BaseTestQuery {
+
+  @Test
+  public void testwithOptionEnabled() throws Exception {
+    String table = ""escape.json"";
+    File file = new File(dirTestWatcher.getRootDir(), table);
+    String json = ""{\""name\"": \""ABC\\S\""}"";
+    String query = String.format(""select * from dfs.`%s`"", table);
+    try {
+      FileUtils.writeStringToFile(file, json);
+      test(""set `%s` = true"", ExecConstants.JSON_READER_ESCAPE_ANY_CHAR);
+      testBuilder()
+        .sqlQuery(query)
+        .unOrdered()
+        .baselineColumns(""name"")
+        .baselineValues(""ABCS"")","[{'comment': ""I would expect the output of this test should be `ABC\\S` since input string is `ABC\\\\S`. what will be the output for string `ABC\\S` ? Shouldn't this new option take care of `ABC\\S` kind of string values ?"", 'commenter': 'sohami'}, {'comment': ""True. I've escaped the backslash here because i'm constructing the JSON string here in Java. If it's just plain text JSON, the input would be 'ABC\\S'. So you can assume the test is actually for the JSON string 'ABC\\S'."", 'commenter': 'Agirish'}, {'comment': 'Makes sense. Thanks for explanation.', 'commenter': 'sohami'}]"
1663,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonEscapeAnyChar.java,"@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.complex.writer;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.drill.common.exceptions.UserRemoteException;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.File;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertThat;
+
+public class TestJsonEscapeAnyChar extends ClusterTest {
+
+  String table = ""escape.json"";
+  File file;
+  String json = ""{\""name\"": \""ABC\\S\""}"";
+  String query = String.format(""select * from dfs.`%s`"", table);","[{'comment': 'Please update as below.\r\n```\r\nprivate static final String TABLE = ""escape.json""\r\nprivate static final String JSON_DATA = ""{\\""name\\"": \\""ABC\\\\S\\""}"";\r\nprivate static final String QUERY = String.format(""select * from dfs.`%s`"", TABLE);\r\nprivate File testFile; \r\n```', 'commenter': 'sohami'}, {'comment': 'Done.', 'commenter': 'Agirish'}]"
1663,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonEscapeAnyChar.java,"@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.complex.writer;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.drill.common.exceptions.UserRemoteException;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.File;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertThat;
+
+public class TestJsonEscapeAnyChar extends ClusterTest {
+
+  String table = ""escape.json"";
+  File file;
+  String json = ""{\""name\"": \""ABC\\S\""}"";
+  String query = String.format(""select * from dfs.`%s`"", table);
+
+  @Before
+  public void setup() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+    file = new File(dirTestWatcher.getRootDir(), table);
+    FileUtils.writeStringToFile(file, json);
+  }
+
+  @Test
+  public void testwithOptionEnabled() throws Exception {
+
+    try {
+      enableJsonReaderEscapeAnyChar();
+      testBuilder()
+        .sqlQuery(query)
+        .unOrdered()
+        .baselineColumns(""name"")
+        .baselineValues(""ABCS"")
+        .build()
+        .run();
+    } finally {
+      resetJsonReaderEscapeAnyChar();
+    }
+  }
+
+  @Test(expected = UserRemoteException.class)
+  public void testwithOptionDisabled() throws Exception {
+    try {
+      queryBuilder().sql(query)
+        .run();
+    } catch (UserRemoteException e) {
+      assertThat(e.getMessage(), containsString(""DATA_READ ERROR: Error parsing JSON - Unrecognized character escape""));
+      throw e;
+    }
+  }
+
+  private static void enableJsonReaderEscapeAnyChar() {
+    client.alterSession(ExecConstants.JSON_READER_ESCAPE_ANY_CHAR, true);
+  }
+
+  private static void resetJsonReaderEscapeAnyChar() {
+    client.alterSession(ExecConstants.JSON_READER_ESCAPE_ANY_CHAR, false);
+  }","[{'comment': 'No need to have above methods as `static`. `private void` is fine.', 'commenter': 'sohami'}, {'comment': 'Done. ', 'commenter': 'Agirish'}]"
1666,exec/java-exec/src/main/codegen/data/Parser.tdd,"@@ -38,6 +38,7 @@
     ""IF"",
     ""JAR"",
     ""PROPERTIES""
+    ""COLUMNS"",","[{'comment': 'Looks like comma should be before `COLUMNS`.', 'commenter': 'arina-ielchiieva'}]"
1666,exec/java-exec/src/main/codegen/includes/parserImpls.ftl,"@@ -464,22 +464,30 @@ SqlNode SqlDropSchema(SqlParserPos pos) :
 
 /**
  * Parse refresh table metadata statement.
- * REFRESH TABLE METADATA tblname
+ * REFRESH TABLE METADATA [COLUMNS ((field1, field2,..) | NONE)] tblname
  */
 SqlNode SqlRefreshMetadata() :
 {
     SqlParserPos pos;
     SqlIdentifier tblName;
-    SqlNodeList fieldList;
+    SqlNodeList fieldList = null;
     SqlNode query;
+    boolean allColumns = true;
 }
 {
     <REFRESH> { pos = getPos(); }
     <TABLE>
     <METADATA>
+    [
+        <COLUMNS> { allColumns = false; }
+        (   fieldList = ParseRequiredFieldList(""Table"")
+            |
+            <NONE>","[{'comment': 'Do we need `NONE` here? I think if columns keyword is present list of columns is required.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Yes, since the columns keyword is optional, metadata will be collected for all the columns if the Columns keyword is not present. So the only way to express not to collect metadata for any column is to have NONE keyword which makes it more explicit.', 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/codegen/includes/parserImpls.ftl,"@@ -550,9 +558,9 @@ Pair<SqlNodeList, SqlNodeList> ParenthesizedCompoundIdentifierList() :
     SqlIdentifier id;
 }
 {
-    id = SimpleIdentifier() {list.add(id);}
+    id = CompoundIdentifier() {list.add(id);}","[{'comment': '1. Why this change is needed? \r\n2. Please updated java-doc to reflect new method behavior: `Parses a comma-separated list of simple identifiers`.', 'commenter': 'arina-ielchiieva'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/RefreshMetadataHandler.java,"@@ -130,5 +140,25 @@ public PhysicalPlan getPlan(SqlNode sqlNode) throws ForemanSetupException {
     }
   }
 
+  private Set<String> getColumnRootSegments(SqlNodeList columnList) {
+    Set<String> columnSet = new HashSet<>();
+    if (columnList != null) {
+      for (SqlNode column : columnList.getList()) {
+        // Add only the root segment. Collect metadata for all the columns under that root segment
+        columnSet.add(SchemaPath.parseFromString(column.toString()).getRootSegmentPath());
+      }
+    }
+    return columnSet;
+  }
+
+  /* Generates the column list specified in the Refresh statement */
+  private SqlNodeList getColumnList(final SqlRefreshMetadata sqlrefreshMetadata) {
+    SqlNodeList columnList = sqlrefreshMetadata.getFieldList();
+    if (columnList == null || columnList.size() <= 0) {","[{'comment': '`columnList.size() <= 0` -> `!columnsList.isEmpty()`', 'commenter': 'arina-ielchiieva'}, {'comment': ""I don't think isEmpty() method exists for SqlNodeList class."", 'commenter': 'dvjyothsna'}, {'comment': 'Looks like you can use the similar static method:\r\n`SqlNodeList.isEmptyList(columnList)`', 'commenter': 'vdiravka'}, {'comment': 'Changed to use isEmptyList()', 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/RefreshMetadataHandler.java,"@@ -130,5 +140,25 @@ public PhysicalPlan getPlan(SqlNode sqlNode) throws ForemanSetupException {
     }
   }
 
+  private Set<String> getColumnRootSegments(SqlNodeList columnList) {
+    Set<String> columnSet = new HashSet<>();
+    if (columnList != null) {
+      for (SqlNode column : columnList.getList()) {
+        // Add only the root segment. Collect metadata for all the columns under that root segment
+        columnSet.add(SchemaPath.parseFromString(column.toString()).getRootSegmentPath());
+      }
+    }
+    return columnSet;
+  }
+
+  /* Generates the column list specified in the Refresh statement */","[{'comment': 'Please add proper java doc rather than comment.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Added java doc.', 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/SqlRefreshMetadata.java,"@@ -43,15 +44,27 @@
   public static final SqlSpecialOperator OPERATOR = new SqlSpecialOperator(""REFRESH_TABLE_METADATA"", SqlKind.OTHER_DDL) {
     @Override
     public SqlCall createCall(SqlLiteral functionQualifier, SqlParserPos pos, SqlNode... operands) {
-      return new SqlRefreshMetadata(pos, (SqlIdentifier) operands[0]);
+      return new SqlRefreshMetadata(pos, (SqlIdentifier) operands[0], (SqlLiteral) operands[1], (SqlNodeList) operands[2]);
     }
   };
 
   private SqlIdentifier tblName;
+  private final SqlLiteral allColumns;
+  private final SqlNodeList fieldList;
 
-  public SqlRefreshMetadata(SqlParserPos pos, SqlIdentifier tblName){
+  public SqlNodeList getFieldList() {","[{'comment': 'Please move new getters after other getters, not before the constructor.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Fixed.', 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/codegen/includes/parserImpls.ftl,"@@ -464,22 +464,30 @@ SqlNode SqlDropSchema(SqlParserPos pos) :
 
 /**
  * Parse refresh table metadata statement.
- * REFRESH TABLE METADATA tblname
+ * REFRESH TABLE METADATA [COLUMNS ((field1, field2,..) | NONE)] tblname
  */
 SqlNode SqlRefreshMetadata() :
 {
     SqlParserPos pos;
     SqlIdentifier tblName;
-    SqlNodeList fieldList;
+    SqlNodeList fieldList = null;
     SqlNode query;
+    boolean allColumns = true;","[{'comment': ""I don't think we need allColumns state here, if fieldList is empty it means we refresh all columns, if not then part of them. Having `SqlNodeList fieldList` is enough."", 'commenter': 'arina-ielchiieva'}, {'comment': ""We have three scenarios here:\r\nRefresh metadata for all the columns -  allColumns - true, fieldList - []\r\nRefresh metadata for subset of columns - allColumns -false, fieldList - [c1,c2,..]\r\nRefresh metadata and don't collect column metadata - allColumns - false, fieldList - []\r\nSince we have three scenarios we can't represent all three by using just one variable."", 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/SqlRefreshMetadata.java,"@@ -43,15 +44,27 @@
   public static final SqlSpecialOperator OPERATOR = new SqlSpecialOperator(""REFRESH_TABLE_METADATA"", SqlKind.OTHER_DDL) {
     @Override
     public SqlCall createCall(SqlLiteral functionQualifier, SqlParserPos pos, SqlNode... operands) {
-      return new SqlRefreshMetadata(pos, (SqlIdentifier) operands[0]);
+      return new SqlRefreshMetadata(pos, (SqlIdentifier) operands[0], (SqlLiteral) operands[1], (SqlNodeList) operands[2]);
     }
   };
 
   private SqlIdentifier tblName;
+  private final SqlLiteral allColumns;
+  private final SqlNodeList fieldList;
 
-  public SqlRefreshMetadata(SqlParserPos pos, SqlIdentifier tblName){
+  public SqlNodeList getFieldList() {
+    return fieldList;
+  }
+
+  public SqlLiteral getAllColumns() {
+    return allColumns;","[{'comment': 'As mentioned above this state can be determined from fieldList, no need for second class variable.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Please see above comment.', 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/SqlRefreshMetadata.java,"@@ -63,6 +76,8 @@ public SqlOperator getOperator() {
   public List<SqlNode> getOperandList() {
     List<SqlNode> ops = Lists.newArrayList();
     ops.add(tblName);
+    ops.add(allColumns);
+    ops.add(fieldList);","[{'comment': 'You also need to amend `public void unparse(SqlWriter writer, int leftPrec, int rightPrec) {` based on existence of `fieldList`.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Fixed', 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -101,14 +102,15 @@ private Metadata(ParquetReaderConfig readerConfig) {
 
   /**
    * Create the parquet metadata file for the directory at the given path, and for any subdirectories.
-   *
    * @param fs file system
    * @param path path
    * @param readerConfig parquet reader configuration
+   * @param allColumns","[{'comment': 'Please add description to params to avoid warnings in the IDE.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Added description', 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -203,12 +205,14 @@ private static boolean ignoreReadingMetadata(MetadataContext metaContext, Path p
    *
    * @param path to the directory of the parquet table
    * @param fs file system
+   * @param allColumns","[{'comment': 'Same here.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Added description', 'commenter': 'dvjyothsna'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/RefreshMetadataHandler.java,"@@ -130,5 +140,29 @@ public PhysicalPlan getPlan(SqlNode sqlNode) throws ForemanSetupException {
     }
   }
 
+  private Set<String> getColumnRootSegments(SqlNodeList columnList) {
+    Set<String> columnSet = new HashSet<>();
+    if (columnList != null) {
+      for (SqlNode column : columnList.getList()) {
+        // Add only the root segment. Collect metadata for all the columns under that root segment
+        columnSet.add(SchemaPath.parseFromString(column.toString()).getRootSegmentPath());
+      }
+    }
+    return columnSet;
+  }
+
+  /**
+   * Generates the column list specified in the Refresh statement
+   * @param sqlrefreshMetadata sql parse node representing refresh statement
+   * @return columnlist list of columns specified in the refresh command","[{'comment': 'You could skip the variable name `columnlist` in the return. See elsewhere in this file.', 'commenter': 'gparai'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -413,7 +422,7 @@ private ColTypeInfo getColTypeInfo(MessageType schema, Type type, String[] path,
    * Get the metadata for a single file
    */
   private ParquetFileMetadata_v3 getParquetFileMetadata_v3(ParquetTableMetadata_v3 parquetTableMetadata,
-      final FileStatus file, final FileSystem fs) throws IOException, InterruptedException {
+                                                           final FileStatus file, final FileSystem fs, boolean allColumns, Set<String> columnSet) throws IOException, InterruptedException {","[{'comment': 'Correct indentation?', 'commenter': 'gparai'}]"
1666,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -462,25 +470,28 @@ private ParquetFileMetadata_v3 getParquetFileMetadata_v3(ParquetTableMetadata_v3
           parquetTableMetadata.columnTypeInfo = new ConcurrentHashMap<>();
         }
         parquetTableMetadata.columnTypeInfo.put(new ColumnTypeMetadata_v3.Key(columnTypeMetadata.name), columnTypeMetadata);
-
-        // Save the column schema info. We'll merge it into one list
-        Object minValue = null;
-        Object maxValue = null;
-        long numNulls = -1;
-        boolean statsAvailable = stats != null && !stats.isEmpty();
-        if (statsAvailable) {
-          if (stats.hasNonNullValue()) {
-            minValue = stats.genericGetMin();
-            maxValue = stats.genericGetMax();
-            if (containsCorruptDates == ParquetReaderUtility.DateCorruptionStatus.META_SHOWS_CORRUPTION && columnTypeMetadata.originalType == OriginalType.DATE) {
-              minValue = ParquetReaderUtility.autoCorrectCorruptedDate((Integer) minValue);
-              maxValue = ParquetReaderUtility.autoCorrectCorruptedDate((Integer) maxValue);
+          // Store column metadata only if allColumns is set to true or if the column belongs to the subset of columns specified in the refresh command
+          if (allColumns || columnSet == null || !allColumns && columnSet != null && columnSet.size() > 0 && columnSet.contains(columnSchemaName.getRootSegmentPath())) {","[{'comment': 'Correct indentation?', 'commenter': 'gparai'}]"
1672,exec/java-exec/src/main/java/org/apache/drill/exec/util/ValueVectorElementFormatter.java,"@@ -52,28 +52,51 @@ public ValueVectorElementFormatter(OptionManager options) {
    * @return the formatted value, null if failed
    */
   public String format(Object value, TypeProtos.MinorType minorType) {
+    String str = null;
     switch (minorType) {
       case TIMESTAMP:
         if (value instanceof LocalDateTime) {
-          return format((LocalDateTime) value,
+          str = format((LocalDateTime) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIMESTAMP),
                         (v, p) -> v.format(getTimestampFormatter(p)));
         }
+        else {
+          str = value.toString();
+        }
+        break;
       case DATE:
         if (value instanceof LocalDate) {
-          return format((LocalDate) value,
+          str = format((LocalDate) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_DATE),
                         (v, p) -> v.format(getDateFormatter(p)));
         }
+        else {
+          str = value.toString();
+        }
+        break;
       case TIME:
         if (value instanceof LocalTime) {
-          return format((LocalTime) value,
+          str = format((LocalTime) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIME),
                         (v, p) -> v.format(getTimeFormatter(p)));
         }
+        else {
+          str = value.toString();
+        }
+        break;
+      case VARBINARY:
+        if(value instanceof byte[]) {","[{'comment': '```suggestion\r\n        if (value instanceof byte[]) {\r\n```', 'commenter': 'vdiravka'}]"
1672,exec/java-exec/src/main/java/org/apache/drill/exec/util/ValueVectorElementFormatter.java,"@@ -52,28 +52,51 @@ public ValueVectorElementFormatter(OptionManager options) {
    * @return the formatted value, null if failed
    */
   public String format(Object value, TypeProtos.MinorType minorType) {
+    String str = null;
     switch (minorType) {
       case TIMESTAMP:
         if (value instanceof LocalDateTime) {
-          return format((LocalDateTime) value,
+          str = format((LocalDateTime) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIMESTAMP),
                         (v, p) -> v.format(getTimestampFormatter(p)));
         }
+        else {
+          str = value.toString();
+        }
+        break;
       case DATE:
         if (value instanceof LocalDate) {
-          return format((LocalDate) value,
+          str = format((LocalDate) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_DATE),
                         (v, p) -> v.format(getDateFormatter(p)));
         }
+        else {
+          str = value.toString();
+        }
+        break;
       case TIME:
         if (value instanceof LocalTime) {
-          return format((LocalTime) value,
+          str = format((LocalTime) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIME),
                         (v, p) -> v.format(getTimeFormatter(p)));
         }
+        else {
+          str = value.toString();
+        }
+        break;
+      case VARBINARY:
+        if(value instanceof byte[]) {
+          byte[] bytes = (byte[]) value;
+          str = org.apache.drill.common.util.DrillStringUtils.toBinaryString(bytes);
+        }
+        else {
+          str = value.toString();
+        }
+      break;
       default:
-        return value.toString();
+        str = value.toString();
     }
+    return str;","[{'comment': 'Looks like you can remove all `else` blocks and put here `return value.toString();`.\r\n_Note:_ `break` statements should be present (as is in your code), return statements should be preserved as earlier, default block can be omitted.', 'commenter': 'vdiravka'}]"
1672,exec/java-exec/src/main/java/org/apache/drill/exec/util/ValueVectorElementFormatter.java,"@@ -52,28 +52,42 @@ public ValueVectorElementFormatter(OptionManager options) {
    * @return the formatted value, null if failed
    */
   public String format(Object value, TypeProtos.MinorType minorType) {
+    String str = null;
     switch (minorType) {
       case TIMESTAMP:
         if (value instanceof LocalDateTime) {
-          return format((LocalDateTime) value,
+          str = format((LocalDateTime) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIMESTAMP),
                         (v, p) -> v.format(getTimestampFormatter(p)));
         }
+        break;
       case DATE:
         if (value instanceof LocalDate) {
-          return format((LocalDate) value,
+          str = format((LocalDate) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_DATE),
                         (v, p) -> v.format(getDateFormatter(p)));
         }
+        break;
       case TIME:
         if (value instanceof LocalTime) {
-          return format((LocalTime) value,
+          str = format((LocalTime) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIME),
                         (v, p) -> v.format(getTimeFormatter(p)));
         }
-      default:
-        return value.toString();
+        break;
+      case VARBINARY:
+        if (value instanceof byte[]) {
+          byte[] bytes = (byte[]) value;
+          str = org.apache.drill.common.util.DrillStringUtils.toBinaryString(bytes);
+        }
+        break;
+    }
+
+    if(str == null) {","[{'comment': '```suggestion\r\n    if (str == null) {\r\n```', 'commenter': 'vdiravka'}, {'comment': 'I agree. It is intended to work as follows: if the format is incorrect, then the exception is being logged and the formatter returns null. Returning values with default formatting can be confusing. And if we return an error message, that would be a column type violation.', 'commenter': 'agozhiy'}]"
1672,exec/java-exec/src/main/java/org/apache/drill/exec/util/ValueVectorElementFormatter.java,"@@ -52,28 +52,47 @@ public ValueVectorElementFormatter(OptionManager options) {
    * @return the formatted value, null if failed
    */
   public String format(Object value, TypeProtos.MinorType minorType) {
+    boolean handled = false;
+	String str = null;
     switch (minorType) {
       case TIMESTAMP:
         if (value instanceof LocalDateTime) {
-          return format((LocalDateTime) value,
+          handled = true;
+          str = format((LocalDateTime) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIMESTAMP),
                         (v, p) -> v.format(getTimestampFormatter(p)));
         }
+        break;
       case DATE:
         if (value instanceof LocalDate) {
-          return format((LocalDate) value,
+          handled = true;
+          str = format((LocalDate) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_DATE),
                         (v, p) -> v.format(getDateFormatter(p)));
         }
+        break;
       case TIME:
         if (value instanceof LocalTime) {
-          return format((LocalTime) value,
+          handled = true;
+          str = format((LocalTime) value,
                         options.getString(ExecConstants.WEB_DISPLAY_FORMAT_TIME),
                         (v, p) -> v.format(getTimeFormatter(p)));
         }
-      default:
-        return value.toString();
+        break;
+      case VARBINARY:
+        if (value instanceof byte[]) {
+          handled = true;
+          byte[] bytes = (byte[]) value;
+          str = org.apache.drill.common.util.DrillStringUtils.toBinaryString(bytes);
+        }
+        break;
+    }
+
+    if (!handled) {","[{'comment': 'It looks like current code execution is the same as in your PR. But logic from PR is more complex: additional flag `handled`, breaks in switch statements...\r\nI think we can leave current code from Drill master and add  your `case VARBINARY`.\r\nSeems it will be enough.', 'commenter': 'vdiravka'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PrelCostEstimates.java,"@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.cost;
+
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+/**
+ * Cost estimates per physical relation. These cost estimations are computed
+ * during physical planning by the optimizer. These are also used post physical
+ * planning to compute memory requirements in minor fragment generation phase.
+ *
+ */
+@JsonTypeName(""cost-estimates"")
+public class PrelCostEstimates {
+  // memory requirement for an operator.
+  private final double memoryCost;","[{'comment': 'Should we use `NodeResources` here instead of just `memoryCost` ? Since that will include all the resources which will be managed.', 'commenter': 'sohami'}, {'comment': ""I don't think using NodeResources makes sense here, generally NodeResources pertains to resources per Node. However, I do agree that some other class (like Resources) might make sense which can encapsulate memory, cpu etc but as we are currently concerned only with memory for now, I thought this can be further enhanced whenever we think it might make sense to pass on other important information."", 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/NodeResource.java,"@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.cost;
+
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import java.util.Map;
+
+/**
+ * This class abstracts the resources like cpu and memory used up by the operators.
+ * In future network resources can also be incorporated if required.
+ */
+public class NodeResource {
+  private long cpu;
+  private long memory;
+
+  public NodeResource(long cpu, long memory) {
+    this.cpu = cpu;
+    this.memory = memory;
+  }
+
+  public void add(NodeResource other) {
+    if (other == null) {
+      return;
+    }
+    this.cpu += other.cpu;
+    this.memory += other.memory;
+  }
+
+  public long getMemory() {
+    return memory;
+  }
+
+  // A utility function to merge the node resources from one drillbit map to other drillbit map.
+  public static Map<DrillbitEndpoint, NodeResource> merge(Map<DrillbitEndpoint, NodeResource> to,
+                                                          Map<DrillbitEndpoint, NodeResource> from) {
+    to.entrySet().stream().forEach((toEntry) -> toEntry.getValue().add(from.get(toEntry.getKey())));
+    return to;
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    sb.append(""CPU: "").append(cpu).append(""Memory: "").append(memory);
+    return sb.toString();
+  }
+
+  public static NodeResource create() {
+    return create(0,0);
+  }
+
+  public static NodeResource create(long cpu) {
+    return create(cpu,0);
+  }
+
+  public static NodeResource create(long cpu, long memory) {
+    return new NodeResource(cpu, memory);
+  }","[{'comment': 'Why not provide multiple constructors instead of static methods ? By the way we have to merge this with NodeResources provided in this [PR for DRILL-7046](https://github.com/apache/drill/pull/1652)', 'commenter': 'sohami'}, {'comment': 'Yes, this code needs to be merged with the PR for DRILL-7046. This can be merged and further enhanced later once both the checkins go through. ', 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/MakeFragmentsVisitor.java,"@@ -57,15 +55,15 @@ public Fragment visitOp(PhysicalOperator op, Fragment value)  throws ForemanSetu
     return value;
   }
 
-  private Fragment ensureBuilder(Fragment value) throws ForemanSetupException{
-    if (value != null) {
-      return value;
+  private Fragment ensureBuilder(Fragment currentFragment) throws ForemanSetupException{
+    if (currentFragment != null) {
+      return currentFragment;
     } else {
-      return getNextBuilder();
+      return getFragmentBuilder();
     }
   }
 
-  public Fragment getNextBuilder() {
+  public Fragment getFragmentBuilder() {","[{'comment': 'should we rename these to `ensureValidFragment` and `getNextFragment` ?', 'commenter': 'sohami'}, {'comment': 'Sure. done.', 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/MakeFragmentsVisitor.java,"@@ -26,29 +26,27 @@
  * Responsible for breaking a plan into its constituent Fragments.
  */
 public class MakeFragmentsVisitor extends AbstractPhysicalVisitor<Fragment, Fragment, ForemanSetupException> {
-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MakeFragmentsVisitor.class);
 
   public final static MakeFragmentsVisitor INSTANCE = new MakeFragmentsVisitor();
 
   private MakeFragmentsVisitor() {
   }
 
   @Override
-  public Fragment visitExchange(Exchange exchange, Fragment value) throws ForemanSetupException {
-//    logger.debug(""Visiting Exchange {}"", exchange);
-    if (value == null) {
-      throw new ForemanSetupException(""The simple fragmenter was called without a FragmentBuilder value.  This will only happen if the initial call to SimpleFragmenter is by a Exchange node.  This should never happen since an Exchange node should never be the root node of a plan."");
+  public Fragment visitExchange(Exchange exchange, Fragment currentFragment) throws ForemanSetupException {
+    if (currentFragment == null) {
+      throw new ForemanSetupException(""The simple fragmenter was called without a FragmentBuilder value. This will only happen if the initial call to SimpleFragmenter is by a"" +
+        "" Exchange node.  This should never happen since an Exchange node should never be the root node of a plan."");
     }
-    Fragment next = getNextBuilder();
-    value.addReceiveExchange(exchange, next);
-    next.addSendExchange(exchange, value);
-    exchange.getChild().accept(this, next);
-    return value;
+    Fragment childFragment = getFragmentBuilder();
+    currentFragment.addReceiveExchange(exchange, childFragment);
+    childFragment.addSendExchange(exchange, currentFragment);","[{'comment': 'how about renaming `currentFragment` to `receivingFragment` and `childFragment` to `sendingFragment` ?', 'commenter': 'sohami'}, {'comment': 'It makes sense. Done.', 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PhysicalPlanCreator.java,"@@ -54,7 +56,11 @@ public QueryContext getContext() {
 
   public PhysicalOperator addMetadata(Prel originalPrel, PhysicalOperator op){
     op.setOperatorId(opIdMap.get(originalPrel).getAsSingleInt());
-    op.setCost(originalPrel.estimateRowCount(originalPrel.getCluster().getMetadataQuery()));
+    PrelCostEstimates costEstimates = originalPrel.getCostEstimates(originalPrel.getCluster().getPlanner(), originalPrel.getCluster().getMetadataQuery());
+    if (!op.isBufferedOperator(context)) {
+      costEstimates = new PrelCostEstimates(context.getOptions().getLong(ExecConstants.OUTPUT_BATCH_SIZE), costEstimates.getOutputRowCount());
+    }
+    op.setCost(costEstimates);","[{'comment': 'I think this method should be the one creating the **PrelCostEstimates** after getting the DrillCostBase as below. Then it will act as a true factory for `PreCostEstimates` and `getCostEstimates` from Prel.java can be removed.\r\n\r\n```\r\n    final RelMetadataQuery mq = originalPrel.getCluster().getMetadataQuery();\r\n    final double estimatedRowCount = originalPrel.estimateRowCount(mq);\r\n    final DrillCostBase costBase = (DrillCostBase) originalPrel.computeSelfCost(originalPrel.getCluster().getPlanner(),\r\n      mq);\r\n    final PrelCostEstimates costEstimates;\r\n    if (!op.isBufferedOperator(context)) {\r\n      costEstimates = new PrelCostEstimates(context.getOptions().getLong(ExecConstants.OUTPUT_BATCH_SIZE), estimatedRowCount);\r\n    } else {\r\n      costEstimates = new PrelCostEstimates(costBase.getMemory(), estimatedRowCount);\r\n    }\r\n```', 'commenter': 'sohami'}, {'comment': 'Sure. It makes sense to put the all the code related at one place.', 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueryParallelizer.java,"@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.exec.proto.BitControl.QueryContextInformation;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.proto.UserBitShared.QueryId;
+import org.apache.drill.exec.rpc.user.UserSession;
+import org.apache.drill.exec.server.options.OptionList;
+import org.apache.drill.exec.work.QueryWorkUnit;
+
+import java.util.Collection;
+
+/**
+ * This class parallelizes the query plan. Once the optimizer finishes its job by producing a
+ * optimized plan, it is the job of this parallelizer to generate a parallel plan out of the
+ * optimized physical plan. It does so by using the optimizers estimates for row count etc.
+ * There are two kinds of parallelizers as explained below. Currently these both parallelizers
+ * differ only in memory assignment for the physical operators.
+ *
+ * a) Default Parallelizer: It optimistically assumes that the whole cluster is running only the
+ *    current query and based on hueristics assigns the optimal memory to the buffered operators.
+ *
+ * b) Queue Parallelizer: This parallelizer computes the memory that can be allocated at best based
+ *    on the current cluster state and also the configuraiton of the queue that it can run on.
+ */
+public interface QueryParallelizer extends ParallelizationParameters {
+
+  /**
+   * This is the only function exposed to the consumer of this parallelizer (currently Foreman) to parallelize
+   * the plan. The caller transforms the plan to a fragment tree and supplies the required information for
+   * the parallelizer to do its job.
+   * @param options List of all options that are set for the current session.
+   * @param foremanNode Endpoint information fo the foreman node.
+   * @param queryId Unique ID of the query.
+   * @param activeEndpoints Currently active endpoints on which the plan can run.
+   * @param rootFragment root of the fragment tree of the transformed physical plan
+   * @param session user session object.
+   * @param queryContextInfo query context.
+   * @return Executable query plan which contains all the information like minor frags and major frags.
+   * @throws ExecutionSetupException
+   */
+  QueryWorkUnit generateWorkUnits(OptionList options, DrillbitEndpoint foremanNode, QueryId queryId,","[{'comment': 'generateWorkUnit ?', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueryParallelizer.java,"@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.exec.proto.BitControl.QueryContextInformation;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.proto.UserBitShared.QueryId;
+import org.apache.drill.exec.rpc.user.UserSession;
+import org.apache.drill.exec.server.options.OptionList;
+import org.apache.drill.exec.work.QueryWorkUnit;
+
+import java.util.Collection;
+
+/**
+ * This class parallelizes the query plan. Once the optimizer finishes its job by producing a
+ * optimized plan, it is the job of this parallelizer to generate a parallel plan out of the
+ * optimized physical plan. It does so by using the optimizers estimates for row count etc.
+ * There are two kinds of parallelizers as explained below. Currently these both parallelizers","[{'comment': '... Currently **both these** ...', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueryParallelizer.java,"@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.exec.proto.BitControl.QueryContextInformation;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.proto.UserBitShared.QueryId;
+import org.apache.drill.exec.rpc.user.UserSession;
+import org.apache.drill.exec.server.options.OptionList;
+import org.apache.drill.exec.work.QueryWorkUnit;
+
+import java.util.Collection;
+
+/**
+ * This class parallelizes the query plan. Once the optimizer finishes its job by producing a
+ * optimized plan, it is the job of this parallelizer to generate a parallel plan out of the
+ * optimized physical plan. It does so by using the optimizers estimates for row count etc.
+ * There are two kinds of parallelizers as explained below. Currently these both parallelizers
+ * differ only in memory assignment for the physical operators.
+ *
+ * a) Default Parallelizer: It optimistically assumes that the whole cluster is running only the
+ *    current query and based on hueristics assigns the optimal memory to the buffered operators.","[{'comment': 'heuristics', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueryParallelizer.java,"@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.exec.proto.BitControl.QueryContextInformation;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.proto.UserBitShared.QueryId;
+import org.apache.drill.exec.rpc.user.UserSession;
+import org.apache.drill.exec.server.options.OptionList;
+import org.apache.drill.exec.work.QueryWorkUnit;
+
+import java.util.Collection;
+
+/**
+ * This class parallelizes the query plan. Once the optimizer finishes its job by producing a
+ * optimized plan, it is the job of this parallelizer to generate a parallel plan out of the
+ * optimized physical plan. It does so by using the optimizers estimates for row count etc.
+ * There are two kinds of parallelizers as explained below. Currently these both parallelizers
+ * differ only in memory assignment for the physical operators.
+ *
+ * a) Default Parallelizer: It optimistically assumes that the whole cluster is running only the
+ *    current query and based on hueristics assigns the optimal memory to the buffered operators.
+ *
+ * b) Queue Parallelizer: This parallelizer computes the memory that can be allocated at best based
+ *    on the current cluster state and also the configuraiton of the queue that it can run on.","[{'comment': 'In what ways will parallelizer consider the current cluster state ?', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueryParallelizer.java,"@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.exec.proto.BitControl.QueryContextInformation;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.proto.UserBitShared.QueryId;
+import org.apache.drill.exec.rpc.user.UserSession;
+import org.apache.drill.exec.server.options.OptionList;
+import org.apache.drill.exec.work.QueryWorkUnit;
+
+import java.util.Collection;
+
+/**
+ * This class parallelizes the query plan. Once the optimizer finishes its job by producing a
+ * optimized plan, it is the job of this parallelizer to generate a parallel plan out of the
+ * optimized physical plan. It does so by using the optimizers estimates for row count etc.
+ * There are two kinds of parallelizers as explained below. Currently these both parallelizers
+ * differ only in memory assignment for the physical operators.
+ *
+ * a) Default Parallelizer: It optimistically assumes that the whole cluster is running only the
+ *    current query and based on hueristics assigns the optimal memory to the buffered operators.
+ *
+ * b) Queue Parallelizer: This parallelizer computes the memory that can be allocated at best based
+ *    on the current cluster state and also the configuraiton of the queue that it can run on.
+ */
+public interface QueryParallelizer extends ParallelizationParameters {
+
+  /**
+   * This is the only function exposed to the consumer of this parallelizer (currently Foreman) to parallelize
+   * the plan. The caller transforms the plan to a fragment tree and supplies the required information for
+   * the parallelizer to do its job.
+   * @param options List of all options that are set for the current session.
+   * @param foremanNode Endpoint information fo the foreman node.","[{'comment': 'of the Foreman node', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractExchange.java,"@@ -132,4 +132,20 @@ public int getOperatorType() {
   public ParallelizationDependency getParallelizationDependency() {
     return ParallelizationDependency.RECEIVER_DEPENDS_ON_SENDER;
   }
+
+  // Memory requirement of the sender for the given receivers and senders in a major fragment.
+  // Currently set to zero but later once batch sizing for Exchanges is completed it will calling
+  // apropriate function.","[{'comment': 'it will **call appropriate** Same for function below.', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/DefaultQueryParallelizer.java,"@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.drill.exec.ops.QueryContext;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.util.MemoryAllocationUtilities;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.Set;
+import java.util.function.BiFunction;
+
+/**
+ * Non RM version of the parallelizer. The parallelization logic is fully inherited from SimpleParallelizer.
+ * The memory computation of the operators is based on the earlier logic to assign memory for the buffered
+ * operators.
+ */
+public class DefaultQueryParallelizer extends SimpleParallelizer {
+  private final boolean planHasMemory;
+  private final QueryContext queryContext;
+
+  public DefaultQueryParallelizer(boolean memoryAvailableInPlan, QueryContext queryContext) {
+    super(queryContext);
+    this.planHasMemory = memoryAvailableInPlan;
+    this.queryContext = queryContext;
+  }
+
+  public DefaultQueryParallelizer(boolean memoryPlanning, long parallelizationThreshold, int maxWidthPerNode,
+                                  int maxGlobalWidth, double affinityFactor) {
+    super(parallelizationThreshold, maxWidthPerNode, maxGlobalWidth, affinityFactor);
+    this.planHasMemory = memoryPlanning;
+    this.queryContext = null;
+  }
+
+  public void adjustMemory(PlanningSet planningSet, Set<Wrapper> roots,","[{'comment': 'Please put` @Override` to indicate the method belongs to parent class not only this implementation.', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/contrib/SplittingParallelizer.java,"@@ -81,12 +83,23 @@ public SplittingParallelizer(QueryContext context) {
       Collection<DrillbitEndpoint> activeEndpoints, PhysicalPlanReader reader, Fragment rootFragment,
       UserSession session, QueryContextInformation queryContextInfo) throws ExecutionSetupException {
 
-    final PlanningSet planningSet = getFragmentsHelper(activeEndpoints, rootFragment);
+    final PlanningSet planningSet = this.prepareFragmentTree(rootFragment);
+
+    Set<Wrapper> rootFragments = getRootFragments(planningSet);
+
+    collectStatsAndParallelizeFragments(planningSet, rootFragments, activeEndpoints);
+
+    adjustMemory(planningSet, rootFragments, activeEndpoints);
 
     return generateWorkUnits(
         options, foremanNode, queryId, reader, rootFragment, planningSet, session, queryContextInfo);
   }
 
+  @Override
+  protected BiFunction<DrillbitEndpoint, PhysicalOperator, Long> getMemory() {
+    return (endpoint, operator) -> operator.getMaxAllocation();
+  }","[{'comment': 'Can be removed as parent `(DefaultQueryParallelizer)` has same implementation', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/SimpleParallelizer.java,"@@ -108,27 +111,106 @@ public double getAffinityFactor() {
     return affinityFactor;
   }
 
+  public Set<Wrapper> getRootFragments(PlanningSet planningSet) {
+    //The following code gets the root fragment by removing all the dependent fragments on which root fragments depend upon.
+    //This is fine because the later parallelizer code traverses from these root fragments to their respective dependent
+    //fragments.
+    final Set<Wrapper> roots = Sets.newHashSet();
+    for(Wrapper w : planningSet) {
+      roots.add(w);
+    }
+
+    //roots will be left over with the fragments which are not depended upon by any other fragments.
+    for(Wrapper wrapper : planningSet) {
+      final List<Wrapper> fragmentDependencies = wrapper.getFragmentDependencies();
+      if (fragmentDependencies != null && fragmentDependencies.size() > 0) {
+        for(Wrapper dependency : fragmentDependencies) {
+          if (roots.contains(dependency)) {
+            roots.remove(dependency);
+          }
+        }
+      }
+    }
+
+    return roots;
+  }
+
+  public PlanningSet prepareFragmentTree(Fragment rootFragment) {
+    PlanningSet planningSet = new PlanningSet();
+
+    initFragmentWrappers(rootFragment, planningSet);
+
+    constructFragmentDependencyGraph(rootFragment, planningSet);
+
+    return planningSet;
+  }
+
+  /**
+   * Traverse all the major fragments and parallelize each major fragment based on
+   * collected stats. The children fragments are parallelized before a parent
+   * fragment.
+   * @param planningSet Set of all major fragments and their context.
+   * @param roots Root nodes of the plan.
+   * @param activeEndpoints currently active drillbit endpoints.
+   * @throws PhysicalOperatorSetupException
+   */
+  public void collectStatsAndParallelizeFragments(PlanningSet planningSet, Set<Wrapper> roots,
+                                                  Collection<DrillbitEndpoint> activeEndpoints) throws PhysicalOperatorSetupException {
+    for (Wrapper wrapper : roots) {
+      traverse(wrapper, CheckedConsumer.throwingConsumerWrapper((Wrapper fragmentWrapper) -> {
+        // If this fragment is already parallelized then no need do it again.
+        // This happens in the case of fragments which have MUX operators.
+        if (fragmentWrapper.isEndpointsAssignmentDone()) {
+          return;
+        }
+        fragmentWrapper.getNode().getRoot().accept(new StatsCollector(planningSet), fragmentWrapper);
+        fragmentWrapper.getStats()
+                       .getDistributionAffinity()
+                       .getFragmentParallelizer()
+                       .parallelizeFragment(fragmentWrapper, this, activeEndpoints);
+        //consolidate the cpu resources required by this major fragment per drillbit.
+        fragmentWrapper.computeCpuResources();
+      }));
+    }
+  }
+
+  public abstract void adjustMemory(PlanningSet planningSet, Set<Wrapper> roots,
+                                    Collection<DrillbitEndpoint> activeEndpoints) throws PhysicalOperatorSetupException;
+
   /**
-   * Generate a set of assigned fragments based on the provided fragment tree. Do not allow parallelization stages
-   * to go beyond the global max width.
+   * The starting function for the whole parallelization and memory computation logic.
+   * 1) Initially a fragment tree is prepared which contains a wrapper for each fragment.
+   *    The topology of this tree is same as that of the major fragment tree.
+   * 2) Traverse this fragment tree to collect stats for each major fragment and then
+   *    parallelize each fragment. At this stage minor fragments are not created but all
+   *    the required information to create minor fragment are computed.
+   * 3) Memory is computed for each operator and for the minor fragment.","[{'comment': 'How memory is calculated for the minor fragments when minor fragments are generated in step 4 ?', 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java,"@@ -0,0 +1,175 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.calcite.util.Pair;
+import org.apache.drill.common.util.function.CheckedConsumer;
+import org.apache.drill.exec.ops.QueryContext;
+import org.apache.drill.exec.physical.PhysicalOperatorSetupException;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.planner.cost.NodeResource;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.util.MemoryAllocationUtilities;
+
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Collection;
+import java.util.Set;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.function.BiFunction;
+import java.util.stream.Collectors;
+
+/**
+ * Paralellizer specialized for managing resources for a query based on Queues. This parallelizer
+ * does not deal with increase/decrease of the parallelization of a query plan based on the current
+ * cluster state. However, the memory assignment for each operator, minor fragment and major
+ * fragment is based on the cluster state and provided queue configuration.
+ */
+public class QueueQueryParallelizer extends SimpleParallelizer {
+  private final boolean enableMemoryPlanning;
+  private final QueryContext queryContext;
+  private final Map<DrillbitEndpoint, Map<PhysicalOperator, Long>> operators;
+
+  public QueueQueryParallelizer(boolean memoryPlanning, QueryContext queryContext) {
+    super(queryContext);
+    this.enableMemoryPlanning = memoryPlanning;
+    this.queryContext = queryContext;
+    this.operators = new HashMap<>();
+  }
+
+  // return the memory computed for a physical operator on a drillbitendpoint.
+  public BiFunction<DrillbitEndpoint, PhysicalOperator, Long> getMemory() {
+    return (endpoint, operator) -> {
+      if (enableMemoryPlanning) {
+        return operators.get(endpoint).get(operator);
+      }
+      else {
+        return operator.getMaxAllocation();
+      }
+    };
+  }
+
+  /**
+   * Function called by the SimpleParallelizer to adjust the memory post parallelization.
+   * The overall logic is to traverse the fragment tree and call the MemoryCalculator on
+   * each major fragment. Once the memory is computed, resource requirement are accumulated
+   * per drillbit.
+   *
+   * The total resource requirements are used to select a queue. If the selected queue's
+   * resource limit is more/less than the query's requirement than the memory will be re-adjusted.
+   *
+   * @param planningSet context of the fragments.
+   * @param roots root fragments.
+   * @param activeEndpoints currently active endpoints.
+   * @throws PhysicalOperatorSetupException
+   */
+  public void adjustMemory(PlanningSet planningSet, Set<Wrapper> roots,
+                           Collection<DrillbitEndpoint> activeEndpoints) throws PhysicalOperatorSetupException {
+
+    if (enableMemoryPlanning) {
+      List<PhysicalOperator> bufferedOpers = planningSet.getRootWrapper().getNode().getBufferedOperators();
+      MemoryAllocationUtilities.setupBufferedOpsMemoryAllocations(enableMemoryPlanning, bufferedOpers, queryContext);
+      return;
+    }
+    // total node resources for the query plan maintained per drillbit.","[{'comment': ""Shouldn't all the memory planning below happen if the flag `enableMemoryPlanning` is true ? Otherwise even for existing queue below planning will take place"", 'commenter': 'sohami'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/MemoryCalculator.java,"@@ -0,0 +1,160 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.calcite.util.Pair;
+import org.apache.drill.exec.ops.QueryContext;
+import org.apache.drill.exec.physical.base.Exchange;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.config.AbstractMuxExchange;
+import org.apache.drill.exec.planner.AbstractOpWrapperVisitor;
+import org.apache.drill.exec.planner.cost.NodeResource;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.HashMap;
+import java.util.function.Function;
+import java.util.stream.Collectors;
+
+/**
+ * A visitor to compute memory requirements for each operator in a minor fragment.
+ * This visitor will be called for each major fragment. It traverses the physical operators
+ * in that major fragment and computes the memory for each operator per each minor fragment.
+ * The minor fragment memory resources are further grouped into per Drillbit resource
+ * requirements.
+ */
+public class MemoryCalculator extends AbstractOpWrapperVisitor<Void, RuntimeException> {
+
+  private final PlanningSet planningSet;
+  // List of all the buffered operators and their memory requirement per drillbit.
+  private final Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> bufferedOperators;
+  private final QueryContext queryContext;
+
+  public MemoryCalculator(PlanningSet planningSet, QueryContext context) {
+    this.planningSet = planningSet;
+    this.bufferedOperators = new HashMap<>();
+    this.queryContext = context;
+  }
+
+  // Helper method to compute the minor fragment count per drillbit. This method returns
+  // a map with key as DrillbitEndpoint and value as the width (i.e #minorFragments)
+  // per Drillbit.
+  private Map<DrillbitEndpoint, Integer> getMinorFragCountPerDrillbit(Wrapper currFragment) {
+      return currFragment.getAssignedEndpoints().stream()
+                                                .collect(Collectors.groupingBy(Function.identity(),
+                                                                               Collectors.summingInt(x -> 1)));
+  }
+
+  // Helper method to merge the memory computations for each operator given memory per operator
+  // and the number of minor fragments per Drillbit.
+  private void merge(Wrapper currFrag,
+                     Map<DrillbitEndpoint, Integer> minorFragsPerDrillBit,
+                     Function<Entry<DrillbitEndpoint, Integer>, Long> getMemory) {
+
+    NodeResource.merge(currFrag.getResourceMap(),
+                       minorFragsPerDrillBit.entrySet()
+                                            .stream()
+                                            .collect(Collectors.toMap((x) -> x.getKey(),
+                                                                      (x) -> NodeResource.create(0,
+                                                                                                  getMemory.apply(x)))));
+  }
+
+  @Override
+  public Void visitSendingExchange(Exchange exchange, Wrapper fragment) throws RuntimeException {
+    Wrapper receivingFragment = planningSet.get(fragment.getNode().getSendingExchangePair().getNode());
+    merge(fragment,
+          getMinorFragCountPerDrillbit(fragment),
+          // get the memory requirements for the sender operator.
+          (x) -> exchange.getSenderMemory(receivingFragment.getWidth(), x.getValue()));
+    return visitOp(exchange, fragment);
+  }
+
+  @Override
+  public Void visitReceivingExchange(Exchange exchange, Wrapper fragment) throws RuntimeException {
+
+    final List<Fragment.ExchangeFragmentPair> receivingExchangePairs = fragment.getNode().getReceivingExchangePairs();
+    final Map<DrillbitEndpoint, Integer> sendingFragsPerDrillBit = new HashMap<>();
+
+    for(Fragment.ExchangeFragmentPair pair : receivingExchangePairs) {
+      if (pair.getExchange() == exchange) {
+        Wrapper sendingFragment = planningSet.get(pair.getNode());
+        Preconditions.checkArgument(sendingFragment.isEndpointsAssignmentDone());
+        for (DrillbitEndpoint endpoint : sendingFragment.getAssignedEndpoints()) {
+          sendingFragsPerDrillBit.putIfAbsent(endpoint, 0);
+          sendingFragsPerDrillBit.put(endpoint, sendingFragsPerDrillBit.get(endpoint)+1);
+        }
+      }
+    }
+    final int totalSendingFrags = sendingFragsPerDrillBit.entrySet().stream()
+                                                         .mapToInt((x) -> x.getValue()).reduce(0, (x, y) -> x+y);
+    merge(fragment,
+          getMinorFragCountPerDrillbit(fragment),
+          (x) -> exchange.getReceiverMemory(fragment.getWidth(),
+            // If the exchange is a MuxExchange then the sending fragments are from that particular drillbit otherwise
+            // sending fragments are from across the cluster.
+            exchange instanceof AbstractMuxExchange ? sendingFragsPerDrillBit.get(x.getKey()) : totalSendingFrags));
+    return null;
+  }
+
+  public List<Pair<PhysicalOperator, Long>> getBufferedOperators(DrillbitEndpoint endpoint) {
+    return this.bufferedOperators.getOrDefault(endpoint, new ArrayList<>());
+  }
+
+  @Override
+  public Void visitOp(PhysicalOperator op, Wrapper fragment) {
+    long memoryCost = (int)Math.ceil(op.getCost().getMemoryCost());
+    if (op.isBufferedOperator(queryContext)) {
+      // If the operator is a buffered operator then get the memory estimates of the optimizer.
+      // The memory estimates of the optimizer are for the whole operator spread across all the
+      // minor fragments. Divide this memory estimation by fragment width to get the memory
+      // requirement per minor fragment.
+      long memoryCostPerMinorFrag = (int)Math.ceil(memoryCost/fragment.getWidth());","[{'comment': 'Should we use `fragment.getAssignedEndpoints().size()` here instead of `fragment.getWidth()` ? Or they both are guaranteed to be same ?', 'commenter': 'sohami'}, {'comment': 'Currently, both should return same value. However, I do agree if someone changes underneath in future, then it makes sense to use fragment.getAssignedEndpoints().size(). Done.', 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java,"@@ -0,0 +1,175 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.calcite.util.Pair;
+import org.apache.drill.common.util.function.CheckedConsumer;
+import org.apache.drill.exec.ops.QueryContext;
+import org.apache.drill.exec.physical.PhysicalOperatorSetupException;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.planner.cost.NodeResource;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.util.MemoryAllocationUtilities;
+
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Collection;
+import java.util.Set;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.function.BiFunction;
+import java.util.stream.Collectors;
+
+/**
+ * Paralellizer specialized for managing resources for a query based on Queues. This parallelizer
+ * does not deal with increase/decrease of the parallelization of a query plan based on the current
+ * cluster state. However, the memory assignment for each operator, minor fragment and major
+ * fragment is based on the cluster state and provided queue configuration.
+ */
+public class QueueQueryParallelizer extends SimpleParallelizer {
+  private final boolean enableMemoryPlanning;
+  private final QueryContext queryContext;
+  private final Map<DrillbitEndpoint, Map<PhysicalOperator, Long>> operators;
+
+  public QueueQueryParallelizer(boolean memoryPlanning, QueryContext queryContext) {
+    super(queryContext);
+    this.enableMemoryPlanning = memoryPlanning;
+    this.queryContext = queryContext;
+    this.operators = new HashMap<>();
+  }
+
+  // return the memory computed for a physical operator on a drillbitendpoint.
+  public BiFunction<DrillbitEndpoint, PhysicalOperator, Long> getMemory() {
+    return (endpoint, operator) -> {
+      if (enableMemoryPlanning) {
+        return operators.get(endpoint).get(operator);
+      }
+      else {
+        return operator.getMaxAllocation();
+      }
+    };
+  }
+
+  /**
+   * Function called by the SimpleParallelizer to adjust the memory post parallelization.
+   * The overall logic is to traverse the fragment tree and call the MemoryCalculator on
+   * each major fragment. Once the memory is computed, resource requirement are accumulated
+   * per drillbit.
+   *
+   * The total resource requirements are used to select a queue. If the selected queue's
+   * resource limit is more/less than the query's requirement than the memory will be re-adjusted.
+   *
+   * @param planningSet context of the fragments.
+   * @param roots root fragments.
+   * @param activeEndpoints currently active endpoints.
+   * @throws PhysicalOperatorSetupException
+   */
+  public void adjustMemory(PlanningSet planningSet, Set<Wrapper> roots,
+                           Collection<DrillbitEndpoint> activeEndpoints) throws PhysicalOperatorSetupException {
+
+    if (enableMemoryPlanning) {
+      List<PhysicalOperator> bufferedOpers = planningSet.getRootWrapper().getNode().getBufferedOperators();
+      MemoryAllocationUtilities.setupBufferedOpsMemoryAllocations(enableMemoryPlanning, bufferedOpers, queryContext);
+      return;
+    }
+    // total node resources for the query plan maintained per drillbit.
+    final Map<DrillbitEndpoint, NodeResource> totalNodeResources =
+            activeEndpoints.stream().collect(Collectors.toMap(x ->x,
+                                                              x -> NodeResource.create()));
+
+    // list of the physical operators and their memory requirements per drillbit.
+    final Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> operators =
+            activeEndpoints.stream().collect(Collectors.toMap(x -> x,
+                                                              x -> new ArrayList<>()));
+
+    for (Wrapper wrapper : roots) {
+      traverse(wrapper, CheckedConsumer.throwingConsumerWrapper((Wrapper fragment) -> {
+        MemoryCalculator calculator = new MemoryCalculator(planningSet, queryContext);
+        fragment.getNode().getRoot().accept(calculator, fragment);
+        NodeResource.merge(totalNodeResources, fragment.getResourceMap());
+        operators.entrySet()
+                  .stream()
+                  .forEach((entry) -> entry.getValue()
+                                           .addAll(calculator.getBufferedOperators(entry.getKey())));
+      }));
+    }
+    //queryrm.selectQueue( pass the max node Resource) returns queue configuration.
+    Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> memoryAdjustedOperators = adjustMemoryForOperators(operators, totalNodeResources, 10);
+    memoryAdjustedOperators.entrySet().stream().forEach((x) -> {
+      Map<PhysicalOperator, Long> memoryPerOperator = x.getValue().stream()
+                                                                  .collect(Collectors.toMap(operatorLongPair -> operatorLongPair.left,
+                                                                                            operatorLongPair -> operatorLongPair.right));
+      this.operators.put(x.getKey(), memoryPerOperator);
+    });
+  }
+
+
+  /**
+   * Helper method to adjust the memory for the buffered operators.
+   * @param memoryPerOperator list of physical operators per drillbit
+   * @param nodeResourceMap resources per drillbit.
+   * @param nodeLimit permissible node limit.
+   * @return list of operators which contain adjusted memory limits.
+   */
+  private Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>>
+      adjustMemoryForOperators(Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> memoryPerOperator,
+                               Map<DrillbitEndpoint, NodeResource> nodeResourceMap, int nodeLimit) {
+    // Get the physical operators which are above the node memory limit.
+    Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> onlyMemoryAboveLimitOperators = new HashMap<>();
+    memoryPerOperator.entrySet().stream().forEach((entry) -> {
+      onlyMemoryAboveLimitOperators.putIfAbsent(entry.getKey(), new ArrayList<>());
+      if (nodeResourceMap.get(entry.getKey()).getMemory() > nodeLimit) {
+        onlyMemoryAboveLimitOperators.get(entry.getKey()).addAll(entry.getValue());
+      }
+    });
+
+
+    // Compute the total memory required by the physical operators on the drillbits which are above node limit.
+    // Then use the total memory to adjust the memory requirement based on the permissible node limit.
+    Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> memoryAdjustedDrillbits = new HashMap<>();
+    onlyMemoryAboveLimitOperators.entrySet().stream().forEach(
+      entry -> {
+        Long totalMemory = entry.getValue().stream().map(operatorMemory -> operatorMemory.getValue()).reduce(0L, (x,y) -> x+ y);","[{'comment': 'Can be changed to `entry.getValue().stream().mapToLong(Pair::getValue).sum();`', 'commenter': 'sohami'}, {'comment': 'I agree this is better to use than the one I used.', 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java,"@@ -0,0 +1,175 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.calcite.util.Pair;
+import org.apache.drill.common.util.function.CheckedConsumer;
+import org.apache.drill.exec.ops.QueryContext;
+import org.apache.drill.exec.physical.PhysicalOperatorSetupException;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.planner.cost.NodeResource;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.util.MemoryAllocationUtilities;
+
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Collection;
+import java.util.Set;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.function.BiFunction;
+import java.util.stream.Collectors;
+
+/**
+ * Paralellizer specialized for managing resources for a query based on Queues. This parallelizer
+ * does not deal with increase/decrease of the parallelization of a query plan based on the current
+ * cluster state. However, the memory assignment for each operator, minor fragment and major
+ * fragment is based on the cluster state and provided queue configuration.
+ */
+public class QueueQueryParallelizer extends SimpleParallelizer {
+  private final boolean enableMemoryPlanning;
+  private final QueryContext queryContext;
+  private final Map<DrillbitEndpoint, Map<PhysicalOperator, Long>> operators;
+
+  public QueueQueryParallelizer(boolean memoryPlanning, QueryContext queryContext) {
+    super(queryContext);
+    this.enableMemoryPlanning = memoryPlanning;
+    this.queryContext = queryContext;
+    this.operators = new HashMap<>();
+  }
+
+  // return the memory computed for a physical operator on a drillbitendpoint.
+  public BiFunction<DrillbitEndpoint, PhysicalOperator, Long> getMemory() {
+    return (endpoint, operator) -> {
+      if (enableMemoryPlanning) {
+        return operators.get(endpoint).get(operator);
+      }
+      else {
+        return operator.getMaxAllocation();
+      }
+    };
+  }
+
+  /**
+   * Function called by the SimpleParallelizer to adjust the memory post parallelization.
+   * The overall logic is to traverse the fragment tree and call the MemoryCalculator on
+   * each major fragment. Once the memory is computed, resource requirement are accumulated
+   * per drillbit.
+   *
+   * The total resource requirements are used to select a queue. If the selected queue's
+   * resource limit is more/less than the query's requirement than the memory will be re-adjusted.
+   *
+   * @param planningSet context of the fragments.
+   * @param roots root fragments.
+   * @param activeEndpoints currently active endpoints.
+   * @throws PhysicalOperatorSetupException
+   */
+  public void adjustMemory(PlanningSet planningSet, Set<Wrapper> roots,
+                           Collection<DrillbitEndpoint> activeEndpoints) throws PhysicalOperatorSetupException {
+
+    if (enableMemoryPlanning) {
+      List<PhysicalOperator> bufferedOpers = planningSet.getRootWrapper().getNode().getBufferedOperators();
+      MemoryAllocationUtilities.setupBufferedOpsMemoryAllocations(enableMemoryPlanning, bufferedOpers, queryContext);
+      return;
+    }
+    // total node resources for the query plan maintained per drillbit.
+    final Map<DrillbitEndpoint, NodeResource> totalNodeResources =
+            activeEndpoints.stream().collect(Collectors.toMap(x ->x,
+                                                              x -> NodeResource.create()));
+
+    // list of the physical operators and their memory requirements per drillbit.
+    final Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> operators =
+            activeEndpoints.stream().collect(Collectors.toMap(x -> x,
+                                                              x -> new ArrayList<>()));
+
+    for (Wrapper wrapper : roots) {
+      traverse(wrapper, CheckedConsumer.throwingConsumerWrapper((Wrapper fragment) -> {
+        MemoryCalculator calculator = new MemoryCalculator(planningSet, queryContext);
+        fragment.getNode().getRoot().accept(calculator, fragment);
+        NodeResource.merge(totalNodeResources, fragment.getResourceMap());
+        operators.entrySet()
+                  .stream()
+                  .forEach((entry) -> entry.getValue()
+                                           .addAll(calculator.getBufferedOperators(entry.getKey())));
+      }));
+    }
+    //queryrm.selectQueue( pass the max node Resource) returns queue configuration.
+    Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> memoryAdjustedOperators = adjustMemoryForOperators(operators, totalNodeResources, 10);
+    memoryAdjustedOperators.entrySet().stream().forEach((x) -> {
+      Map<PhysicalOperator, Long> memoryPerOperator = x.getValue().stream()
+                                                                  .collect(Collectors.toMap(operatorLongPair -> operatorLongPair.left,
+                                                                                            operatorLongPair -> operatorLongPair.right));","[{'comment': 'I think here each Physical Operator is only generated for physical plan and not for each minor fragment. Each wrapper object knows how many minor fragments needs to be spawned and on which DrillbitEndpoint. So there are chances that each Drillbit can be assigned same PhysicalOperator multiple time if there are multiple minor fragments running on it for that Wrapper or MajorFragment. So the `toMap` operation will fail unless we use different signature and provide a mergeFunction in case of duplicates', 'commenter': 'sohami'}, {'comment': 'Yes. Thank you for catching this. I was having it in my mind and that is the reason I used List. But I think I lost that thought and I missed it.', 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java,"@@ -0,0 +1,175 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.fragment;
+
+import org.apache.calcite.util.Pair;
+import org.apache.drill.common.util.function.CheckedConsumer;
+import org.apache.drill.exec.ops.QueryContext;
+import org.apache.drill.exec.physical.PhysicalOperatorSetupException;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.planner.cost.NodeResource;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.util.MemoryAllocationUtilities;
+
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Collection;
+import java.util.Set;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.function.BiFunction;
+import java.util.stream.Collectors;
+
+/**
+ * Paralellizer specialized for managing resources for a query based on Queues. This parallelizer
+ * does not deal with increase/decrease of the parallelization of a query plan based on the current
+ * cluster state. However, the memory assignment for each operator, minor fragment and major
+ * fragment is based on the cluster state and provided queue configuration.
+ */
+public class QueueQueryParallelizer extends SimpleParallelizer {
+  private final boolean enableMemoryPlanning;
+  private final QueryContext queryContext;
+  private final Map<DrillbitEndpoint, Map<PhysicalOperator, Long>> operators;
+
+  public QueueQueryParallelizer(boolean memoryPlanning, QueryContext queryContext) {
+    super(queryContext);
+    this.enableMemoryPlanning = memoryPlanning;
+    this.queryContext = queryContext;
+    this.operators = new HashMap<>();
+  }
+
+  // return the memory computed for a physical operator on a drillbitendpoint.
+  public BiFunction<DrillbitEndpoint, PhysicalOperator, Long> getMemory() {
+    return (endpoint, operator) -> {
+      if (enableMemoryPlanning) {
+        return operators.get(endpoint).get(operator);
+      }
+      else {
+        return operator.getMaxAllocation();
+      }
+    };
+  }
+
+  /**
+   * Function called by the SimpleParallelizer to adjust the memory post parallelization.
+   * The overall logic is to traverse the fragment tree and call the MemoryCalculator on
+   * each major fragment. Once the memory is computed, resource requirement are accumulated
+   * per drillbit.
+   *
+   * The total resource requirements are used to select a queue. If the selected queue's
+   * resource limit is more/less than the query's requirement than the memory will be re-adjusted.
+   *
+   * @param planningSet context of the fragments.
+   * @param roots root fragments.
+   * @param activeEndpoints currently active endpoints.
+   * @throws PhysicalOperatorSetupException
+   */
+  public void adjustMemory(PlanningSet planningSet, Set<Wrapper> roots,
+                           Collection<DrillbitEndpoint> activeEndpoints) throws PhysicalOperatorSetupException {
+
+    if (enableMemoryPlanning) {
+      List<PhysicalOperator> bufferedOpers = planningSet.getRootWrapper().getNode().getBufferedOperators();
+      MemoryAllocationUtilities.setupBufferedOpsMemoryAllocations(enableMemoryPlanning, bufferedOpers, queryContext);
+      return;
+    }
+    // total node resources for the query plan maintained per drillbit.
+    final Map<DrillbitEndpoint, NodeResource> totalNodeResources =
+            activeEndpoints.stream().collect(Collectors.toMap(x ->x,
+                                                              x -> NodeResource.create()));
+
+    // list of the physical operators and their memory requirements per drillbit.
+    final Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> operators =
+            activeEndpoints.stream().collect(Collectors.toMap(x -> x,
+                                                              x -> new ArrayList<>()));
+
+    for (Wrapper wrapper : roots) {
+      traverse(wrapper, CheckedConsumer.throwingConsumerWrapper((Wrapper fragment) -> {
+        MemoryCalculator calculator = new MemoryCalculator(planningSet, queryContext);
+        fragment.getNode().getRoot().accept(calculator, fragment);
+        NodeResource.merge(totalNodeResources, fragment.getResourceMap());
+        operators.entrySet()
+                  .stream()
+                  .forEach((entry) -> entry.getValue()
+                                           .addAll(calculator.getBufferedOperators(entry.getKey())));
+      }));
+    }
+    //queryrm.selectQueue( pass the max node Resource) returns queue configuration.
+    Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> memoryAdjustedOperators = adjustMemoryForOperators(operators, totalNodeResources, 10);
+    memoryAdjustedOperators.entrySet().stream().forEach((x) -> {
+      Map<PhysicalOperator, Long> memoryPerOperator = x.getValue().stream()
+                                                                  .collect(Collectors.toMap(operatorLongPair -> operatorLongPair.left,
+                                                                                            operatorLongPair -> operatorLongPair.right));
+      this.operators.put(x.getKey(), memoryPerOperator);
+    });
+  }
+
+
+  /**
+   * Helper method to adjust the memory for the buffered operators.
+   * @param memoryPerOperator list of physical operators per drillbit
+   * @param nodeResourceMap resources per drillbit.
+   * @param nodeLimit permissible node limit.
+   * @return list of operators which contain adjusted memory limits.
+   */
+  private Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>>
+      adjustMemoryForOperators(Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> memoryPerOperator,
+                               Map<DrillbitEndpoint, NodeResource> nodeResourceMap, int nodeLimit) {
+    // Get the physical operators which are above the node memory limit.
+    Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> onlyMemoryAboveLimitOperators = new HashMap<>();
+    memoryPerOperator.entrySet().stream().forEach((entry) -> {
+      onlyMemoryAboveLimitOperators.putIfAbsent(entry.getKey(), new ArrayList<>());
+      if (nodeResourceMap.get(entry.getKey()).getMemory() > nodeLimit) {
+        onlyMemoryAboveLimitOperators.get(entry.getKey()).addAll(entry.getValue());
+      }
+    });
+
+
+    // Compute the total memory required by the physical operators on the drillbits which are above node limit.
+    // Then use the total memory to adjust the memory requirement based on the permissible node limit.
+    Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> memoryAdjustedDrillbits = new HashMap<>();
+    onlyMemoryAboveLimitOperators.entrySet().stream().forEach(
+      entry -> {
+        Long totalMemory = entry.getValue().stream().map(operatorMemory -> operatorMemory.getValue()).reduce(0L, (x,y) -> x+ y);
+        List<Pair<PhysicalOperator, Long>> adjustedMemory = entry.getValue().stream().map(operatorMemory -> {
+          // formula to adjust the memory is (optimalMemory / totalMemory(this is for all the operators)) * permissible_node_limit.
+          return Pair.of(operatorMemory.getKey(), (long) Math.ceil(operatorMemory.getValue()/totalMemory * nodeLimit));
+        }).collect(Collectors.toList());
+        memoryAdjustedDrillbits.put(entry.getKey(), adjustedMemory);
+      }
+    );
+
+    // Get all the operations on drillbits which were adjusted for memory and merge them with operators which are not
+    // adjusted for memory.
+    Map<DrillbitEndpoint, List<Pair<PhysicalOperator, Long>>> allDrillbits = new HashMap<>();","[{'comment': ""Function name is `adjustMemoryForOperators` which means we are updating the memoryForOperators. Hence we just update the passed in parameter without returning anything that seems perfectly valid. Going with above it doesn't seem like we need all the below code. We can just update the `memoryPerOperator` with the adjusted drillbits and caller can just consume the passed in `memoryPerOperator`"", 'commenter': 'sohami'}, {'comment': 'Yes, I agree that function name is kind of misleading. I will change the name to reflect the same. It was not my intention to change the passed on parameter.  I think this function might further change during the course of Resource management changes. I will keep in mind about the comments.', 'commenter': 'HanumathRao'}]"
1677,exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/SimpleParallelizer.java,"@@ -108,27 +111,106 @@ public double getAffinityFactor() {
     return affinityFactor;
   }
 
+  public Set<Wrapper> getRootFragments(PlanningSet planningSet) {
+    //The following code gets the root fragment by removing all the dependent fragments on which root fragments depend upon.
+    //This is fine because the later parallelizer code traverses from these root fragments to their respective dependent
+    //fragments.
+    final Set<Wrapper> roots = Sets.newHashSet();
+    for(Wrapper w : planningSet) {
+      roots.add(w);
+    }
+
+    //roots will be left over with the fragments which are not depended upon by any other fragments.
+    for(Wrapper wrapper : planningSet) {
+      final List<Wrapper> fragmentDependencies = wrapper.getFragmentDependencies();
+      if (fragmentDependencies != null && fragmentDependencies.size() > 0) {
+        for(Wrapper dependency : fragmentDependencies) {
+          if (roots.contains(dependency)) {
+            roots.remove(dependency);
+          }
+        }
+      }
+    }
+
+    return roots;
+  }
+
+  public PlanningSet prepareFragmentTree(Fragment rootFragment) {
+    PlanningSet planningSet = new PlanningSet();
+
+    initFragmentWrappers(rootFragment, planningSet);
+
+    constructFragmentDependencyGraph(rootFragment, planningSet);
+
+    return planningSet;
+  }
+
+  /**
+   * Traverse all the major fragments and parallelize each major fragment based on
+   * collected stats. The children fragments are parallelized before a parent
+   * fragment.
+   * @param planningSet Set of all major fragments and their context.
+   * @param roots Root nodes of the plan.
+   * @param activeEndpoints currently active drillbit endpoints.
+   * @throws PhysicalOperatorSetupException
+   */
+  public void collectStatsAndParallelizeFragments(PlanningSet planningSet, Set<Wrapper> roots,
+                                                  Collection<DrillbitEndpoint> activeEndpoints) throws PhysicalOperatorSetupException {
+    for (Wrapper wrapper : roots) {
+      traverse(wrapper, CheckedConsumer.throwingConsumerWrapper((Wrapper fragmentWrapper) -> {
+        // If this fragment is already parallelized then no need do it again.
+        // This happens in the case of fragments which have MUX operators.
+        if (fragmentWrapper.isEndpointsAssignmentDone()) {
+          return;
+        }
+        fragmentWrapper.getNode().getRoot().accept(new StatsCollector(planningSet), fragmentWrapper);
+        fragmentWrapper.getStats()
+                       .getDistributionAffinity()
+                       .getFragmentParallelizer()
+                       .parallelizeFragment(fragmentWrapper, this, activeEndpoints);
+        //consolidate the cpu resources required by this major fragment per drillbit.
+        fragmentWrapper.computeCpuResources();
+      }));
+    }
+  }
+
+  public abstract void adjustMemory(PlanningSet planningSet, Set<Wrapper> roots,
+                                    Collection<DrillbitEndpoint> activeEndpoints) throws PhysicalOperatorSetupException;
+
   /**
-   * Generate a set of assigned fragments based on the provided fragment tree. Do not allow parallelization stages
-   * to go beyond the global max width.
+   * The starting function for the whole parallelization and memory computation logic.
+   * 1) Initially a fragment tree is prepared which contains a wrapper for each fragment.
+   *    The topology of this tree is same as that of the major fragment tree.
+   * 2) Traverse this fragment tree to collect stats for each major fragment and then
+   *    parallelize each fragment. At this stage minor fragments are not created but all
+   *    the required information to create minor fragment are computed.
+   * 3) Memory is computed for each operator and for the minor fragment.
+   * 4) Lastly all the above computed information is used to create the minor fragments
+   *    for each major fragment.
    *
-   * @param options         Option list
-   * @param foremanNode     The driving/foreman node for this query.  (this node)
-   * @param queryId         The queryId for this query.
-   * @param activeEndpoints The list of endpoints to consider for inclusion in planning this query.
-   * @param rootFragment    The root node of the PhysicalPlan that we will be parallelizing.
-   * @param session         UserSession of user who launched this query.
-   * @param queryContextInfo Info related to the context when query has started.
-   * @return The list of generated PlanFragment protobuf objects to be assigned out to the individual nodes.
+   * @param options List of options set by the user.
+   * @param foremanNode foreman node for this query plan.
+   * @param queryId  Query ID.
+   * @param activeEndpoints currently active endpoins on which this plan will run.
+   * @param rootFragment Root major fragment.
+   * @param session session context.
+   * @param queryContextInfo query context.
+   * @return
    * @throws ExecutionSetupException
    */
-  public QueryWorkUnit getFragments(OptionList options, DrillbitEndpoint foremanNode, QueryId queryId,
-      Collection<DrillbitEndpoint> activeEndpoints, Fragment rootFragment,
-      UserSession session, QueryContextInformation queryContextInfo) throws ExecutionSetupException {
+  @Override
+  public final QueryWorkUnit generateWorkUnits(OptionList options, DrillbitEndpoint foremanNode, QueryId queryId,
+                                               Collection<DrillbitEndpoint> activeEndpoints, Fragment rootFragment,
+                                               UserSession session, QueryContextInformation queryContextInfo) throws ExecutionSetupException {
+    PlanningSet planningSet = prepareFragmentTree(rootFragment);
+
+    Set<Wrapper> rootFragments = getRootFragments(planningSet);
+
+    collectStatsAndParallelizeFragments(planningSet, rootFragments, activeEndpoints);
 
-    final PlanningSet planningSet = getFragmentsHelper(activeEndpoints, rootFragment);
-    return generateWorkUnit(
-        options, foremanNode, queryId, rootFragment, planningSet, session, queryContextInfo);
+    adjustMemory(planningSet, rootFragments, activeEndpoints);
+
+    return generateWorkUnit(options, foremanNode, queryId, rootFragment, planningSet, session, queryContextInfo);","[{'comment': ""I am guessing that the code to actually set memory limit on each physical operator instance on each of the minor fragment will be done in subsequent PR's ? Right now it's being set using the old logic in `visitPhysicalPlan`"", 'commenter': 'sohami'}, {'comment': 'Yes, you are right.', 'commenter': 'HanumathRao'}]"
1679,distribution/src/deb/control/control,"@@ -0,0 +1,8 @@
+Package: drill
+Version: 1.0-SNAPSHOT","[{'comment': 'Please update Drill version in accoradance to the version from root POM file', 'commenter': 'vdiravka'}]"
1679,distribution/src/deb/control/control,"@@ -0,0 +1,8 @@
+Package: drill
+Version: 1.0-SNAPSHOT
+Section: misc
+Priority: optional
+Architecture: all
+Maintainer: name <>
+Description: Apache Drill
+Distribution: development","[{'comment': '```suggestion\r\nDistribution: development\r\n\r\n```', 'commenter': 'vdiravka'}, {'comment': 'Please add: \r\n```\r\nHomepage: http://drill.apache.org/\r\nDescription: Drill is an Apache open-source SQL query engine for Big Data exploration\r\n```', 'commenter': 'vdiravka'}]"
1681,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/auth/DrillSpnegoAuthenticator.java,"@@ -152,7 +153,7 @@ private Authentication authenticateSession(ServletRequest request, ServletRespon
         }
 
         response.setContentLength(0);
-        final HttpChannel channel = HttpChannel.getCurrentHttpChannel();
+        final HttpChannel channel = HttpConnection.getCurrentConnection().getHttpChannel();
         final Response base_response = channel.getResponse();
         final Request base_request = channel.getRequest();","[{'comment': 'Please update this as below:\r\n```\r\nfinal Request base_request = Request.getBaseRequest(req);\r\nfinal Response base_response = base_request.getResponse();\r\n```', 'commenter': 'sohami'}, {'comment': 'Thanks, it looks better.\r\nAlso I have dropped usage of `base_response` local variable.', 'commenter': 'vdiravka'}]"
1681,pom.xml,"@@ -85,6 +85,7 @@
     <reflections.version>0.9.10</reflections.version>
     <avro.version>1.8.2</avro.version>
     <metrics.version>4.0.2</metrics.version>
+    <jetty.version>9.4.15.v20190215</jetty.version>","[{'comment': ""I think this should still be defined in` java-exec pom.xml` file since it's only used in that package. Root pom should control the common dependencies across multiple packages."", 'commenter': 'sohami'}, {'comment': ""In current PR I have added jetty-dependencies to the `dependencyManagement` block only.\r\nIt was done in scope of DRILL-68.. I have added a [comment](https://issues.apache.org/jira/browse/DRILL-6540?focusedCommentId=16606306&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16606306) that it was necessary for the HBase unit tests. So it can't be excluded.\r\nAlso I checked `mvn dependency:tree -Dincludes=org.eclipse.jetty` without `dependencyManagement` block and found different versions of Jetty\r\n\r\n```\r\n[INFO] org.apache.drill.contrib:drill-format-mapr:jar:1.16.0-SNAPSHOT\r\n[INFO] +- com.mapr.fs:mapr-hbase:jar:6.1.0-mapr:compile\r\n[INFO] |  \\- org.apache.hbase:hbase-server:jar:2.1.1:compile\r\n[INFO] |     +- org.apache.hbase:hbase-http:jar:2.1.1:compile\r\n[INFO] |     |  +- org.eclipse.jetty:jetty-util:jar:9.3.19.v20170502:compile\r\n[INFO] |     |  \\- org.eclipse.jetty:jetty-util-ajax:jar:9.3.19.v20170502:compile\r\n[INFO] |     \\- org.eclipse.jetty:jetty-webapp:jar:9.3.19.v20170502:compile\r\n[INFO] |        \\- org.eclipse.jetty:jetty-xml:jar:9.3.19.v20170502:compile\r\n[INFO] \\- org.apache.drill.exec:drill-java-exec:jar:1.16.0-SNAPSHOT:compile\r\n[INFO]    +- org.eclipse.jetty:jetty-server:jar:9.4.15.v20190215:compile\r\n[INFO]    |  +- org.eclipse.jetty:jetty-http:jar:9.4.15.v20190215:compile\r\n[INFO]    |  \\- org.eclipse.jetty:jetty-io:jar:9.4.15.v20190215:compile\r\n[INFO]    +- org.eclipse.jetty:jetty-servlet:jar:9.4.15.v20190215:compile\r\n[INFO]    |  \\- org.eclipse.jetty:jetty-security:jar:9.4.15.v20190215:compile\r\n[INFO]    +- org.eclipse.jetty:jetty-servlets:jar:9.4.15.v20190215:compile\r\n[INFO]    \\- org.glassfish.jersey.containers:jersey-container-jetty-servlet:jar:2.8:compile\r\n[INFO]       \\- org.glassfish.jersey.containers:jersey-container-jetty-http:jar:2.8:compile\r\n[INFO]          \\- org.eclipse.jetty:jetty-continuation:jar:9.1.1.v20140108:compile\r\n[INFO] ------------------------------------------------------------------------\r\n```\r\nTherefore since Jetty dependencies are used not only in `java-exec` it is nesessary to handle their versions in Drill root POM file."", 'commenter': 'vdiravka'}, {'comment': ""Since during Drill's upgrade of Jetty version there was code change needed due to api changes. I am expecting even though if we enforce Jetty version to be `9.4.15` from root pom dependencyManagement block, it can cause issues for drill-format-mapr jar which is dependent upon mapr-hbase which in turn on jetty 9.3.19. There might be some api compatibility mismatch due to Jetty version upgrade."", 'commenter': 'sohami'}, {'comment': 'what is your thought on this ?', 'commenter': 'sohami'}, {'comment': 'I will investigate how hbase and mapr-db jetty dependencies affect Drill.', 'commenter': 'vdiravka'}, {'comment': 'It looks like HBase was earlier on Jetty 9.4 version, but there is an issue with `MiniDFSCluster` [HBASE-18943](https://issues.apache.org/jira/browse/HBASE-18943), [HBASE-19390](https://issues.apache.org/jira/browse/HBASE-19390), so they reverted to 9.3 Jetty version. It was the same reason why I have updated Jetty in the scope of [DRILL-6540: Upgrade to HADOOP-3.1 libraries](https://issues.apache.org/jira/browse/DRILL-6540).\r\nIt also appears that new Jersey version requires Jetty 9.4 version, see [comment](https://github.com/apache/hbase/blob/master/pom.xml#L1533) in HBase.\r\nPossibly we should consider the newest Jetty and Jersey versions only after [HBASE-19256](https://issues.apache.org/jira/browse/HBASE-19256) resolution.\r\n\r\nSo I will check my changes for Hadoop3.1 libs version in Drill with newest Jetty and Jersey. If there are some issues with them, I will put to Drill the same Jetty and Jersey versions as in HBase project. \r\n', 'commenter': 'vdiravka'}, {'comment': ""So with newest version of Jetty I am guessing you mean 9.3 since that is what is required by MiniDfsCluster test in Hadoop 3.1 as well ? It won't work with anything beyond that version unless HBASE-19256 is resolved."", 'commenter': 'sohami'}, {'comment': ""Finally jetty 9.3 is chosen for Drill.\r\nJetty dependencies are used in `java-exec pom.xml`, but I've left versions control in `dependencyManagement` block of root POM to avoid picking invalid jetty version by maven, in case when some libs will have other version.\r\nFor instance we can't exclude jetty from `hadoop-common` and `hbase` dependencies. But they have different jetty minor versions: \r\n[9.3.24.v20180605](https://github.com/apache/hadoop/blob/trunk/hadoop-project/pom.xml#L38)  for Hadoop\r\n[9.3.19.v20170502](https://github.com/apache/hbase/blob/rel/2.1.0/pom.xml#L1352) for HBase 2.1 and [9.3.25.v20180904](https://github.com/apache/hbase/blob/master/pom.xml#L1529) for master HBase version.\r\nI didn't find any API compatibility differences between these jetty minor versions (only 9.4 has it).\r\nPossibly in future we can consider shade Jetty version in Drill, [DRILL-7135](https://issues.apache.org/jira/browse/DRILL-7135). Not sure that is necessary for now.\r\n"", 'commenter': 'vdiravka'}]"
1681,pom.xml,"@@ -2553,7 +2548,13 @@
             <version>4.0.1</version>
             <scope>provided</scope>
           </dependency>
-          <!--/Javax Servlet dependecies-->
+
+          <dependency>
+            <groupId>javax.ws.rs</groupId>
+            <artifactId>javax.ws.rs-api</artifactId>","[{'comment': 'why we need this and other Javax Servlet dependencies ?', 'commenter': 'sohami'}, {'comment': 'This is versions control for `javax` transitive dependencies from `jersey` dependencies, since they are also widely used in Drill.\r\nI have updated them to the latest versions along with `jersey` dependencies.\r\nBut after reading some good practices, I rethought it. It is better to control only `jersey` dependencies. Since `javax.ws` can have an interfaces, which are not implemented in Jersey yet.\r\nSo I removed this `javax` versions control mechanism ', 'commenter': 'vdiravka'}]"
1687,exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestConvertFunctions.java,"@@ -536,68 +527,27 @@ public void testUTF8() throws Throwable {
     verifyPhysicalPlan(""convert_to('apache_drill', 'UTF8')"", new byte[] {'a', 'p', 'a', 'c', 'h', 'e', '_', 'd', 'r', 'i', 'l', 'l'});
   }
 
-  @Ignore // TODO(DRILL-2326) remove this when we get rid of the scalar replacement option test cases below
-  @Test
+  @Test // DRILL-2326
   public void testBigIntVarCharReturnTripConvertLogical() throws Exception {
-    final String logicalPlan = Resources.toString(
+    String logicalPlan = Resources.toString(
         Resources.getResource(CONVERSION_TEST_LOGICAL_PLAN), Charsets.UTF_8);
-    final List<QueryDataBatch> results =  testLogicalWithResults(logicalPlan);
-    int count = 0;
-    final RecordBatchLoader loader = new RecordBatchLoader(getAllocator());
-    for (QueryDataBatch result : results) {
-      count += result.getHeader().getRowCount();
-      loader.load(result.getHeader().getDef(), result.getData());
-      if (loader.getRecordCount() > 0) {
-        VectorUtil.logVectorAccessibleContent(loader);
-      }
-      loader.clear();
-      result.release();
-    }
-    assertTrue(count == 10);
-  }
 
+    List<String> compilers = Arrays.asList(ClassCompilerSelector.CompilerPolicy.JANINO.name(),
+      ClassCompilerSelector.CompilerPolicy.JDK.name());
 
-  @Test // TODO(DRILL-2326) temporary until we fix the scalar replacement bug for this case
-  public void testBigIntVarCharReturnTripConvertLogical_ScalarReplaceTRY() throws Exception {
-    final OptionValue srOption = QueryTestUtil.setupScalarReplacementOption(bits[0], ScalarReplacementOption.TRY);
-    try {
-      // this should work fine
-      testBigIntVarCharReturnTripConvertLogical();
-    } finally {
-      // restore the system option
-      QueryTestUtil.restoreScalarReplacementOption(bits[0], srOption.string_val);
-    }
-  }
-
-  @Test // TODO(DRILL-2326) temporary until we fix the scalar replacement bug for this case
-  @Ignore // Because this test sometimes fails, sometimes succeeds
-  public void testBigIntVarCharReturnTripConvertLogical_ScalarReplaceON() throws Exception {
-    final OptionValue srOption = QueryTestUtil.setupScalarReplacementOption(bits[0], ScalarReplacementOption.ON);
-    boolean caughtException = false;
     try {
-      // this used to fail (with a JUnit assertion) until we fix the SR bug
-      // Something in DRILL-5116 seemed to fix this problem, so the test now
-      // succeeds - sometimes.
-      testBigIntVarCharReturnTripConvertLogical();
-    } catch(RpcException e) {
-      caughtException = true;
-    } finally {
-      QueryTestUtil.restoreScalarReplacementOption(bits[0], srOption.string_val);
-    }
-
-    // Yes: sometimes this works, sometimes it does not...
-    assertTrue(!caughtException || caughtException);
-  }
+      setSessionOption(ExecConstants.SCALAR_REPLACEMENT_OPTION, ClassTransformer.ScalarReplacementOption.ON.name());
+      setSessionOption(ClassCompilerSelector.JAVA_COMPILER_DEBUG_OPTION, false);
+      for (String compilerName : compilers) {
+        setSessionOption(ClassCompilerSelector.JAVA_COMPILER_OPTION, compilerName);
 
-  @Test // TODO(DRILL-2326) temporary until we fix the scalar replacement bug for this case
-  public void testBigIntVarCharReturnTripConvertLogical_ScalarReplaceOFF() throws Exception {","[{'comment': 'Why these tests are no longer needed? \r\n`testBigIntVarCharReturnTripConvertLogical_ScalarReplaceOFF`\r\n`testBigIntVarCharReturnTripConvertLogical_ScalarReplaceON`\r\n`testBigIntVarCharReturnTripConvertLogical_ScalarReplaceTRY`', 'commenter': 'vdiravka'}, {'comment': 'All these tests submitted the same query and either checked error or validated the result depending on the value of `ScalarReplacementOption`. With this change, the query will pass for all option values, so these tests were removed and `testBigIntVarCharReturnTripConvertLogical()` was enabled.', 'commenter': 'vvysotskyi'}]"
1687,contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java,"@@ -114,33 +115,33 @@ public void testRand() throws Exception {
             .go();
   }
 
-  @Test //DRILL-4868
+  @Test //DRILL-4868 & DRILL-2326
   public void testEmbeddedHiveFunctionCall() throws Exception {
-    // TODO(DRILL-2326) temporary until we fix the scalar replacement bug for this case
-    final OptionValue srOption = QueryTestUtil.setupScalarReplacementOption(bits[0], ClassTransformer.ScalarReplacementOption.TRY);
+    String query =
+        ""SELECT convert_from(unhex(key2), 'INT_BE') as intkey \n"" +
+        ""FROM cp.`functions/conv/conv.json`"";
+
+    List<String> compilers = Arrays.asList(ClassCompilerSelector.CompilerPolicy.JANINO.name(),
+        ClassCompilerSelector.CompilerPolicy.JDK.name());
 
     try {
-      final String[] queries = {
-          ""SELECT convert_from(unhex(key2), 'INT_BE') as intkey \n"" +
-              ""FROM cp.`functions/conv/conv.json`"",
-      };
+      setSessionOption(ExecConstants.SCALAR_REPLACEMENT_OPTION, ClassTransformer.ScalarReplacementOption.ON.name());
+      setSessionOption(ClassCompilerSelector.JAVA_COMPILER_DEBUG_OPTION, false);","[{'comment': 'Looks like `ClassCompilerSelector.JAVA_COMPILER_DEBUG_OPTION` can be unchanged. So then for the debug level this test case can log the generated code (according the Javadoc for the `ClassCompilerSelector`)', 'commenter': 'vdiravka'}, {'comment': 'Agree, removed it.', 'commenter': 'vvysotskyi'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +64,86 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";
+
+  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR =
+      Comparator.comparing(PluginConfigWrapper::getName);
 
   @GET
-  @Path(""/storage.json"")
+  @Path(""/storage/{group}/plugins/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public List<PluginConfigWrapper> getStoragePluginsJSON() {
-
-    List<PluginConfigWrapper> list = Lists.newArrayList();
-    for (Map.Entry<String, StoragePluginConfig> entry : Lists.newArrayList(storage.getStore().getAll())) {
-      PluginConfigWrapper plugin = new PluginConfigWrapper(entry.getKey(), entry.getValue());
-      list.add(plugin);
+  public Response getPluginsConfigs(@PathParam(""group"") String pluginGroup, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equals(format) || HOCON_FILE_NAME.equals(format)) {","[{'comment': '`equalsIgnoreCase`?\r\nHere and in other places below.', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +64,86 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";
+
+  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR =
+      Comparator.comparing(PluginConfigWrapper::getName);
 
   @GET
-  @Path(""/storage.json"")
+  @Path(""/storage/{group}/plugins/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public List<PluginConfigWrapper> getStoragePluginsJSON() {
-
-    List<PluginConfigWrapper> list = Lists.newArrayList();
-    for (Map.Entry<String, StoragePluginConfig> entry : Lists.newArrayList(storage.getStore().getAll())) {
-      PluginConfigWrapper plugin = new PluginConfigWrapper(entry.getKey(), entry.getValue());
-      list.add(plugin);
+  public Response getPluginsConfigs(@PathParam(""group"") String pluginGroup, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equals(format) || HOCON_FILE_NAME.equals(format)) {
+      Response.ResponseBuilder response = Response.ok();
+      response.entity(getPluginsConfigs(pluginGroup).toArray());
+      response.header(""Content-Disposition"",
+          String.format(""attachment;filename=\""%s_storage_plugins.%s\"""", pluginGroup, format));
+      return response.build();
     }
-
-    Collections.sort(list, PLUGIN_COMPARATOR);
-
-    return list;
+    logger.error(""Unknown file type {} for {} Storage Plugin Configs"", format, pluginGroup);
+    return Response.status(Response.Status.NOT_FOUND).build();
   }
 
   @GET
   @Path(""/storage"")
   @Produces(MediaType.TEXT_HTML)
-  public Viewable getStoragePlugins() {
-    List<PluginConfigWrapper> list = getStoragePluginsJSON();
+  public Viewable getPlugins() {
+    List<PluginConfigWrapper> list = getPluginsConfigs(ALL_PLUGINS);
     return ViewableWithPermissions.create(authEnabled.get(), ""/rest/storage/list.ftl"", sc, list);
   }
 
+  private List<PluginConfigWrapper> getPluginsConfigs(String pluginsNumber) {
+    List<PluginConfigWrapper> list = Lists.newArrayList();","[{'comment': '```suggestion\r\n    List<PluginConfigWrapper> list = new ArrayList<>();\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +64,86 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";
+
+  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR =
+      Comparator.comparing(PluginConfigWrapper::getName);
 
   @GET
-  @Path(""/storage.json"")
+  @Path(""/storage/{group}/plugins/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public List<PluginConfigWrapper> getStoragePluginsJSON() {
-
-    List<PluginConfigWrapper> list = Lists.newArrayList();
-    for (Map.Entry<String, StoragePluginConfig> entry : Lists.newArrayList(storage.getStore().getAll())) {
-      PluginConfigWrapper plugin = new PluginConfigWrapper(entry.getKey(), entry.getValue());
-      list.add(plugin);
+  public Response getPluginsConfigs(@PathParam(""group"") String pluginGroup, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equals(format) || HOCON_FILE_NAME.equals(format)) {
+      Response.ResponseBuilder response = Response.ok();
+      response.entity(getPluginsConfigs(pluginGroup).toArray());
+      response.header(""Content-Disposition"",
+          String.format(""attachment;filename=\""%s_storage_plugins.%s\"""", pluginGroup, format));
+      return response.build();
     }
-
-    Collections.sort(list, PLUGIN_COMPARATOR);
-
-    return list;
+    logger.error(""Unknown file type {} for {} Storage Plugin Configs"", format, pluginGroup);
+    return Response.status(Response.Status.NOT_FOUND).build();
   }
 
   @GET
   @Path(""/storage"")
   @Produces(MediaType.TEXT_HTML)
-  public Viewable getStoragePlugins() {
-    List<PluginConfigWrapper> list = getStoragePluginsJSON();
+  public Viewable getPlugins() {
+    List<PluginConfigWrapper> list = getPluginsConfigs(ALL_PLUGINS);
     return ViewableWithPermissions.create(authEnabled.get(), ""/rest/storage/list.ftl"", sc, list);
   }
 
+  private List<PluginConfigWrapper> getPluginsConfigs(String pluginsNumber) {
+    List<PluginConfigWrapper> list = Lists.newArrayList();
+    Predicate<Map.Entry<String, StoragePluginConfig>> predicate = entry -> false;
+    if (ALL_PLUGINS.equals(pluginsNumber)) {
+      predicate = entry -> true;
+    } else if (ENABLED_PLUGINS.equals(pluginsNumber)) {
+      predicate = entry -> entry.getValue().isEnabled();
+    } else if (DISABLED_PLUGINS.equals(pluginsNumber)) {
+      predicate = entry -> !entry.getValue().isEnabled();
+    }
+    Lists.newArrayList(storage.getStore().getAll()).stream()","[{'comment': '```suggestion\r\n    new ArrayList<>()storage.getStore().getAll()).stream()\r\n```', 'commenter': 'arina-ielchiieva'}, {'comment': 'This List was used to obtain Stream from Iterator.\r\nReplaced List with Iterables:\r\n```\r\nIterable<Map.Entry<String, StoragePluginConfig>> iterable = () -> storage.getStore().getAll();\r\n    StreamSupport.stream(iterable.spliterator(), false)...\r\n```', 'commenter': 'vdiravka'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +64,86 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";
+
+  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR =
+      Comparator.comparing(PluginConfigWrapper::getName);
 
   @GET
-  @Path(""/storage.json"")
+  @Path(""/storage/{group}/plugins/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public List<PluginConfigWrapper> getStoragePluginsJSON() {
-
-    List<PluginConfigWrapper> list = Lists.newArrayList();
-    for (Map.Entry<String, StoragePluginConfig> entry : Lists.newArrayList(storage.getStore().getAll())) {
-      PluginConfigWrapper plugin = new PluginConfigWrapper(entry.getKey(), entry.getValue());
-      list.add(plugin);
+  public Response getPluginsConfigs(@PathParam(""group"") String pluginGroup, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equals(format) || HOCON_FILE_NAME.equals(format)) {
+      Response.ResponseBuilder response = Response.ok();
+      response.entity(getPluginsConfigs(pluginGroup).toArray());
+      response.header(""Content-Disposition"",
+          String.format(""attachment;filename=\""%s_storage_plugins.%s\"""", pluginGroup, format));
+      return response.build();
     }
-
-    Collections.sort(list, PLUGIN_COMPARATOR);
-
-    return list;
+    logger.error(""Unknown file type {} for {} Storage Plugin Configs"", format, pluginGroup);
+    return Response.status(Response.Status.NOT_FOUND).build();
   }
 
   @GET
   @Path(""/storage"")
   @Produces(MediaType.TEXT_HTML)
-  public Viewable getStoragePlugins() {
-    List<PluginConfigWrapper> list = getStoragePluginsJSON();
+  public Viewable getPlugins() {
+    List<PluginConfigWrapper> list = getPluginsConfigs(ALL_PLUGINS);
     return ViewableWithPermissions.create(authEnabled.get(), ""/rest/storage/list.ftl"", sc, list);
   }
 
+  private List<PluginConfigWrapper> getPluginsConfigs(String pluginsNumber) {","[{'comment': '```suggestion\r\n  private List<PluginConfigWrapper> getPluginsConfigs(String pluginGroup) {\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +64,86 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";
+
+  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR =
+      Comparator.comparing(PluginConfigWrapper::getName);
 
   @GET
-  @Path(""/storage.json"")
+  @Path(""/storage/{group}/plugins/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public List<PluginConfigWrapper> getStoragePluginsJSON() {
-
-    List<PluginConfigWrapper> list = Lists.newArrayList();
-    for (Map.Entry<String, StoragePluginConfig> entry : Lists.newArrayList(storage.getStore().getAll())) {
-      PluginConfigWrapper plugin = new PluginConfigWrapper(entry.getKey(), entry.getValue());
-      list.add(plugin);
+  public Response getPluginsConfigs(@PathParam(""group"") String pluginGroup, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equals(format) || HOCON_FILE_NAME.equals(format)) {
+      Response.ResponseBuilder response = Response.ok();
+      response.entity(getPluginsConfigs(pluginGroup).toArray());
+      response.header(""Content-Disposition"",
+          String.format(""attachment;filename=\""%s_storage_plugins.%s\"""", pluginGroup, format));
+      return response.build();
     }
-
-    Collections.sort(list, PLUGIN_COMPARATOR);
-
-    return list;
+    logger.error(""Unknown file type {} for {} Storage Plugin Configs"", format, pluginGroup);
+    return Response.status(Response.Status.NOT_FOUND).build();
   }
 
   @GET
   @Path(""/storage"")
   @Produces(MediaType.TEXT_HTML)
-  public Viewable getStoragePlugins() {
-    List<PluginConfigWrapper> list = getStoragePluginsJSON();
+  public Viewable getPlugins() {
+    List<PluginConfigWrapper> list = getPluginsConfigs(ALL_PLUGINS);
     return ViewableWithPermissions.create(authEnabled.get(), ""/rest/storage/list.ftl"", sc, list);
   }
 
+  private List<PluginConfigWrapper> getPluginsConfigs(String pluginsNumber) {
+    List<PluginConfigWrapper> list = Lists.newArrayList();
+    Predicate<Map.Entry<String, StoragePluginConfig>> predicate = entry -> false;
+    if (ALL_PLUGINS.equals(pluginsNumber)) {
+      predicate = entry -> true;
+    } else if (ENABLED_PLUGINS.equals(pluginsNumber)) {
+      predicate = entry -> entry.getValue().isEnabled();
+    } else if (DISABLED_PLUGINS.equals(pluginsNumber)) {
+      predicate = entry -> !entry.getValue().isEnabled();
+    }
+    Lists.newArrayList(storage.getStore().getAll()).stream()
+        .filter(predicate)
+        .map(entry -> new PluginConfigWrapper(entry.getKey(), entry.getValue()))
+        .forEach(list::add);
+    list.sort(PLUGIN_COMPARATOR);
+    return list;
+  }
+
   @SuppressWarnings(""resource"")
   @GET
   @Path(""/storage/{name}.json"")
   @Produces(MediaType.APPLICATION_JSON)
-  public PluginConfigWrapper getStoragePluginJSON(@PathParam(""name"") String name) {
+  public PluginConfigWrapper getPluginConfigs(@PathParam(""name"") String name) {","[{'comment': '```suggestion\r\n  public PluginConfigWrapper getPluginConfig(@PathParam(""name"") String name) {\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +64,86 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";
+
+  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR =
+      Comparator.comparing(PluginConfigWrapper::getName);
 
   @GET
-  @Path(""/storage.json"")
+  @Path(""/storage/{group}/plugins/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public List<PluginConfigWrapper> getStoragePluginsJSON() {
-
-    List<PluginConfigWrapper> list = Lists.newArrayList();
-    for (Map.Entry<String, StoragePluginConfig> entry : Lists.newArrayList(storage.getStore().getAll())) {
-      PluginConfigWrapper plugin = new PluginConfigWrapper(entry.getKey(), entry.getValue());
-      list.add(plugin);
+  public Response getPluginsConfigs(@PathParam(""group"") String pluginGroup, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equals(format) || HOCON_FILE_NAME.equals(format)) {
+      Response.ResponseBuilder response = Response.ok();
+      response.entity(getPluginsConfigs(pluginGroup).toArray());
+      response.header(""Content-Disposition"",
+          String.format(""attachment;filename=\""%s_storage_plugins.%s\"""", pluginGroup, format));
+      return response.build();
     }
-
-    Collections.sort(list, PLUGIN_COMPARATOR);
-
-    return list;
+    logger.error(""Unknown file type {} for {} Storage Plugin Configs"", format, pluginGroup);
+    return Response.status(Response.Status.NOT_FOUND).build();
   }
 
   @GET
   @Path(""/storage"")
   @Produces(MediaType.TEXT_HTML)
-  public Viewable getStoragePlugins() {
-    List<PluginConfigWrapper> list = getStoragePluginsJSON();
+  public Viewable getPlugins() {
+    List<PluginConfigWrapper> list = getPluginsConfigs(ALL_PLUGINS);
     return ViewableWithPermissions.create(authEnabled.get(), ""/rest/storage/list.ftl"", sc, list);
   }
 
+  private List<PluginConfigWrapper> getPluginsConfigs(String pluginsNumber) {
+    List<PluginConfigWrapper> list = Lists.newArrayList();
+    Predicate<Map.Entry<String, StoragePluginConfig>> predicate = entry -> false;
+    if (ALL_PLUGINS.equals(pluginsNumber)) {","[{'comment': 'equalsIgnoreCase?', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -174,32 +200,34 @@ public JsonResult createOrUpdatePluginJSON(PluginConfigWrapper plugin) {
       return message(""success"");
     } catch (ExecutionSetupException e) {
       logger.error(""Unable to create/ update plugin: "" + plugin.getName(), e);
-      return message(""Error while creating/ updating storage : "" + (e.getCause() != null ? e.getCause().getMessage() : e.getMessage()));
+      return message(""Error while creating / updating storage : "" + (e.getCause() != null ?
+          e.getCause().getMessage() : e.getMessage()));
     }
   }
 
   @POST
-  @Path(""/storage/{name}"")
-  @Consumes(""application/x-www-form-urlencoded"")
+  @Path(""/storage/create_new"")","[{'comment': '```suggestion\r\n  @Path(""/storage/create"")\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -58,38 +89,208 @@
                 ${plugin.getName()}
               </td>
               <td style=""border:none;"">
-                <a class=""btn btn-primary"" href=""/storage/${plugin.getName()}"">Update</a>
-                <a class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">Enable</a>
+                <button type=""button"" class=""btn btn-primary"" onclick=""location.href='/storage/${plugin.getName()}'"">
+                  Update
+                </button>
+                <button type=""button"" class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">
+                  Enable
+                </button>
+                <button type=""button"" class=""btn btn-primary"" name=""${plugin.getName()}"" data-toggle=""modal""
+                        data-target=""#pluginsModal"">
+                  Export
+                </button>
               </td>
             </tr>
           </#if>
         </#list>
       </tbody>
     </table>
   </div>
-  <div class=""page-header"">
+
+
+  <#-- Modal window for exporting plugin config (including group plugins modal) -->
+  <div class=""modal fade"" id=""pluginsModal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""exportPlugin"" aria-hidden=""true"">
+    <div class=""modal-dialog modal-sm"" role=""document"">
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""exportPlugin"">Export Plugin config</h4>
+        </div>
+        <div class=""modal-body"">
+          <div id=""format"" style=""display: inline-block; position: relative;"">
+            <label for=""format"">Format</label>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""format"" id=""json"" value=""json"" checked=""checked"">
+                JSON
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""format"" id=""hocon"" value=""conf"">
+                HOCON
+              </label>
+            </div>
+          </div>
+
+          <div id=""plugin-set"" class="""" style=""display: inline-block; position: relative; float: right;"">
+            <label for=""format"">Plugin group</label>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""all"" value=""all"" checked=""checked"">","[{'comment': '```suggestion\r\n                <input type=""radio"" name=""group"" id=""all"" value=""all"" checked=""checked"">\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -58,38 +89,208 @@
                 ${plugin.getName()}
               </td>
               <td style=""border:none;"">
-                <a class=""btn btn-primary"" href=""/storage/${plugin.getName()}"">Update</a>
-                <a class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">Enable</a>
+                <button type=""button"" class=""btn btn-primary"" onclick=""location.href='/storage/${plugin.getName()}'"">
+                  Update
+                </button>
+                <button type=""button"" class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">
+                  Enable
+                </button>
+                <button type=""button"" class=""btn btn-primary"" name=""${plugin.getName()}"" data-toggle=""modal""
+                        data-target=""#pluginsModal"">
+                  Export
+                </button>
               </td>
             </tr>
           </#if>
         </#list>
       </tbody>
     </table>
   </div>
-  <div class=""page-header"">
+
+
+  <#-- Modal window for exporting plugin config (including group plugins modal) -->
+  <div class=""modal fade"" id=""pluginsModal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""exportPlugin"" aria-hidden=""true"">
+    <div class=""modal-dialog modal-sm"" role=""document"">
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""exportPlugin"">Export Plugin config</h4>
+        </div>
+        <div class=""modal-body"">
+          <div id=""format"" style=""display: inline-block; position: relative;"">
+            <label for=""format"">Format</label>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""format"" id=""json"" value=""json"" checked=""checked"">
+                JSON
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""format"" id=""hocon"" value=""conf"">
+                HOCON
+              </label>
+            </div>
+          </div>
+
+          <div id=""plugin-set"" class="""" style=""display: inline-block; position: relative; float: right;"">
+            <label for=""format"">Plugin group</label>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""all"" value=""all"" checked=""checked"">
+                ALL
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""enabled"" value=""enabled"">
+                ENABLED
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""disabled"" value=""disabled"">
+                DISABLED
+              </label>
+            </div>
+          </div>
+        </div>
+
+        <div class=""modal-footer"">
+          <button type=""button"" class=""btn btn-default"" data-dismiss=""modal"">Close</button>
+          <button type=""button"" id=""export"" class=""btn btn-primary"">Export</button>
+        </div>
+      </div>
+    </div>
   </div>
-  <div>
-    <h4>New Storage Plugin</h4>
-    <form class=""form-inline"" id=""newStorage"" role=""form"" action=""/"" method=""GET"">
-      <div class=""form-group"">
-        <input type=""text"" class=""form-control"" id=""storageName"" placeholder=""Storage Name"">
+  <#-- Modal window for exporting plugin config (including group plugins modal) -->
+
+  <#-- Modal window for creating plugin -->
+  <div class=""modal fade"" id=""new-plugin-modal"" role=""dialog"" aria-labelledby=""configuration"">
+    <div class=""modal-dialog"" role=""document"">
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""configuration"">New Storage Plugin</h4>
+        </div>
+        <div class=""modal-body"">
+
+          <form id=""createForm"" role=""form"" action=""/storage/create_new"" method=""POST"">","[{'comment': '```suggestion\r\n          <form id=""createForm"" role=""form"" action=""/storage/create"" method=""POST"">\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -58,38 +89,208 @@
                 ${plugin.getName()}
               </td>
               <td style=""border:none;"">
-                <a class=""btn btn-primary"" href=""/storage/${plugin.getName()}"">Update</a>
-                <a class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">Enable</a>
+                <button type=""button"" class=""btn btn-primary"" onclick=""location.href='/storage/${plugin.getName()}'"">
+                  Update
+                </button>
+                <button type=""button"" class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">
+                  Enable
+                </button>
+                <button type=""button"" class=""btn btn-primary"" name=""${plugin.getName()}"" data-toggle=""modal""
+                        data-target=""#pluginsModal"">
+                  Export
+                </button>
               </td>
             </tr>
           </#if>
         </#list>
       </tbody>
     </table>
   </div>
-  <div class=""page-header"">
+
+
+  <#-- Modal window for exporting plugin config (including group plugins modal) -->
+  <div class=""modal fade"" id=""pluginsModal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""exportPlugin"" aria-hidden=""true"">
+    <div class=""modal-dialog modal-sm"" role=""document"">
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""exportPlugin"">Export Plugin config</h4>
+        </div>
+        <div class=""modal-body"">
+          <div id=""format"" style=""display: inline-block; position: relative;"">
+            <label for=""format"">Format</label>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""format"" id=""json"" value=""json"" checked=""checked"">
+                JSON
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""format"" id=""hocon"" value=""conf"">
+                HOCON
+              </label>
+            </div>
+          </div>
+
+          <div id=""plugin-set"" class="""" style=""display: inline-block; position: relative; float: right;"">
+            <label for=""format"">Plugin group</label>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""all"" value=""all"" checked=""checked"">
+                ALL
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""enabled"" value=""enabled"">
+                ENABLED
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""disabled"" value=""disabled"">
+                DISABLED
+              </label>
+            </div>
+          </div>
+        </div>
+
+        <div class=""modal-footer"">
+          <button type=""button"" class=""btn btn-default"" data-dismiss=""modal"">Close</button>
+          <button type=""button"" id=""export"" class=""btn btn-primary"">Export</button>
+        </div>
+      </div>
+    </div>
   </div>
-  <div>
-    <h4>New Storage Plugin</h4>
-    <form class=""form-inline"" id=""newStorage"" role=""form"" action=""/"" method=""GET"">
-      <div class=""form-group"">
-        <input type=""text"" class=""form-control"" id=""storageName"" placeholder=""Storage Name"">
+  <#-- Modal window for exporting plugin config (including group plugins modal) -->
+
+  <#-- Modal window for creating plugin -->
+  <div class=""modal fade"" id=""new-plugin-modal"" role=""dialog"" aria-labelledby=""configuration"">
+    <div class=""modal-dialog"" role=""document"">
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""configuration"">New Storage Plugin</h4>
+        </div>
+        <div class=""modal-body"">
+
+          <form id=""createForm"" role=""form"" action=""/storage/create_new"" method=""POST"">
+            <input type=""text"" class=""form-control"" name=""name"" placeholder=""Storage Name"">
+            <h3>Configuration</h3>
+            <div class=""form-group"">
+              <div id=""editor"" class=""form-control""></div>
+                <textarea class=""form-control"" id=""config"" name=""config"" data-editor=""json"" style=""display: none;"">
+                </textarea>
+            </div>
+            <div style=""text-align: right; margin: 10px"">
+              <button type=""button"" class=""btn btn-default"" data-dismiss=""modal"">Close</button>
+              <button type=""submit"" class=""btn btn-primary"" onclick=""doCreate()"">Create</button>
+            </div>
+          </form>
+
+          <div id=""message"" class=""hidden alert alert-info"">
+          </div>
+        </div>
       </div>
-      <button type=""submit"" class=""btn btn-default"" onclick=""doSubmit()"">Create</button>
-    </form>
+    </div>
   </div>
+  <#-- Modal window for creating plugin -->
+
   <script>
-    function doSubmit() {
-      var name = document.getElementById(""storageName"");
-      var form = document.getElementById(""newStorage"");
-      form.action = ""/storage/"" + name.value;
-      form.submit();
-    };
     function doEnable(name, flag) {
-      $.get(""/storage/"" + name + ""/enable/"" + flag, function(data) {
+      $.get(""/storage/"" + name + ""/enable/"" + flag, function() {
         location.reload();
       });
-    };
+    }
+
+    function doCreate() {
+      $(""#createForm"").ajaxForm(function(data) {
+        // alert(""ajax works"");
+        const messageEl = $(""#message"");
+        if (data.result === ""success"") {
+          messageEl.removeClass(""hidden"")
+                   .removeClass(""alert-danger"")
+                   .addClass(""alert-info"")
+                   .text(data.result).alert();
+          setTimeout(function() { location.reload(); }, 800);
+      } else {
+        messageEl.addClass(""hidden"");
+        // Wait a fraction of a second before showing the message again. This
+        // makes it clear if a second attempt gives the same error as
+        // the first that a ""new"" message came back from the server
+        setTimeout(function() {
+            messageEl.removeClass(""hidden"")
+                     .removeClass(""alert-info"")
+                     .addClass(""alert-danger"")
+                     .text(""Please retry: "" + data.result).alert();
+        }, 200);
+        }
+      });
+    }
+
+    // Formatting create plugin textarea
+    $('#new-plugin-modal').on('show.bs.modal', function() {
+        const editor = ace.edit(""editor"");
+        const textarea = $('textarea[name=""config""]');
+
+        editor.setAutoScrollEditorIntoView(true);
+        editor.setOption(""maxLines"", 25);
+        editor.setOption(""minLines"", 10);
+        editor.renderer.setShowGutter(true);
+        editor.renderer.setOption('showLineNumbers', true);
+        editor.renderer.setOption('showPrintMargin', false);
+        editor.getSession().setMode(""ace/mode/json"");
+        editor.setTheme(""ace/theme/eclipse"");
+
+        // copy back to textarea on form submit...
+        editor.getSession().on('change', function(){
+            textarea.val(editor.getSession().getValue());
+        });
+    });
+
+    // Modal windows management
+    let exportInstance; // global variable
+    $('#pluginsModal').on('show.bs.modal', function(event) {
+        console.log(""alarm"");
+      const button = $(event.relatedTarget); // Button that triggered the modal
+      const modal = $(this);
+      exportInstance = button.attr(""name"");
+
+      const optionalBlock = modal.find('#plugin-set');
+      if (exportInstance === ""all"") {
+        optionalBlock.removeClass('hide');
+        modal.find('.modal-title').text('Export all Plugin configs');
+      } else {
+        modal.find('#plugin-set').addClass('hide');
+        modal.find('.modal-title').text('Export '+ exportInstance.toUpperCase() +' Plugin config');
+      }
+
+      modal.find('#export').click(function() {
+        let format;
+        if (modal.find('#json').is("":checked"")) {
+          format = 'json';
+        }
+        if (modal.find('#hocon').is("":checked"")) {
+          format = 'conf';
+        }
+        let url;
+        if (exportInstance === ""all"") {
+          let pluginsNumber = """";","[{'comment': '```suggestion\r\n          let pluginGroup = """";\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -58,38 +89,208 @@
                 ${plugin.getName()}
               </td>
               <td style=""border:none;"">
-                <a class=""btn btn-primary"" href=""/storage/${plugin.getName()}"">Update</a>
-                <a class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">Enable</a>
+                <button type=""button"" class=""btn btn-primary"" onclick=""location.href='/storage/${plugin.getName()}'"">
+                  Update
+                </button>
+                <button type=""button"" class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">
+                  Enable
+                </button>
+                <button type=""button"" class=""btn btn-primary"" name=""${plugin.getName()}"" data-toggle=""modal""
+                        data-target=""#pluginsModal"">
+                  Export
+                </button>
               </td>
             </tr>
           </#if>
         </#list>
       </tbody>
     </table>
   </div>
-  <div class=""page-header"">
+
+
+  <#-- Modal window for exporting plugin config (including group plugins modal) -->
+  <div class=""modal fade"" id=""pluginsModal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""exportPlugin"" aria-hidden=""true"">
+    <div class=""modal-dialog modal-sm"" role=""document"">
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""exportPlugin"">Export Plugin config</h4>","[{'comment': '```suggestion\r\n          <h4 class=""modal-title"" id=""exportPlugin"">Plugin config</h4>\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/resources/rest/storage/update.ftl,"@@ -107,15 +140,34 @@
           }, 200);
         }
       });
-    };
+    }
     function deleteFunction() {
-      var temp = confirm(""Are you sure?"");
-      if (temp == true) {
-        $.get(""/storage/${model.getName()}/delete"", function(data) {
+      if (confirm(""Are you sure?"")) {
+        $.get(""/storage/${model.getName()}/delete"", function() {
           window.location.href = ""/storage"";
         });
       }
-    };
+    }
+
+    // Modal window management
+    $('#pluginsModal').on('show.bs.modal', function (event) {
+      const button = $(event.relatedTarget) // Button that triggered the modal
+      let exportInstance = button.attr(""name"");
+      const modal = $(this);
+      modal.find('.modal-title').text('Export '+ exportInstance.toUpperCase() +' Plugin configs');","[{'comment': ""```suggestion\r\n      modal.find('.modal-title').text(exportInstance.toUpperCase() +' Plugin config');\r\n```"", 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/resources/rest/storage/update.ftl,"@@ -38,26 +38,59 @@
       </textarea>
     </div>
     <a class=""btn btn-default"" href=""/storage"">Back</a>
-    <button class=""btn btn-default"" type=""submit"" onclick=""doUpdate();"">
-      <#if model.exists()>Update<#else>Create</#if>
-    </button>
-    <#if model.exists()>
-      <#if model.enabled()>
-        <a id=""enabled"" class=""btn btn-default"">Disable</a>
-      <#else>
-        <a id=""enabled"" class=""btn btn-primary"">Enable</a>
-      </#if>
-      <a class=""btn btn-default"" href=""/storage/${model.getName()}/export"""">Export</a>
-      <a id=""del"" class=""btn btn-danger"" onclick=""deleteFunction()"">Delete</a>
+    <button class=""btn btn-default"" type=""submit"" onclick=""doUpdate();"">Update</button>
+    <#if model.enabled()>
+      <a id=""enabled"" class=""btn btn-default"">Disable</a>
+    <#else>
+      <a id=""enabled"" class=""btn btn-primary"">Enable</a>
     </#if>
+    <button type=""button"" class=""btn btn-default export"" name=""${model.getName()}"" data-toggle=""modal""
+            data-target=""#pluginsModal"">
+      Export
+    </button>
+    <a id=""del"" class=""btn btn-danger"" onclick=""deleteFunction()"">Delete</a>
   </form>
   <br>
   <div id=""message"" class=""hidden alert alert-info"">
   </div>
-  <script>
-    var editor = ace.edit(""editor"");
-    var textarea = $('textarea[name=""config""]');
 
+  <#-- Modal window-->
+  <div class=""modal fade"" id=""pluginsModal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""exportPlugin"" aria-hidden=""true"">
+    <div class=""modal-dialog modal-sm"" role=""document"">
+      <!-- Modal content-->
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""exportPlugin"">Export Plugin config</h4>
+        </div>
+        <div class=""modal-body"">
+          <div id=""format"" style=""display: inline-block; position: center;"">
+            <label for=""format"">File type</label>","[{'comment': '```suggestion\r\n            <label for=""format"">Format</label>\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/resources/rest/storage/update.ftl,"@@ -38,26 +38,59 @@
       </textarea>
     </div>
     <a class=""btn btn-default"" href=""/storage"">Back</a>
-    <button class=""btn btn-default"" type=""submit"" onclick=""doUpdate();"">
-      <#if model.exists()>Update<#else>Create</#if>
-    </button>
-    <#if model.exists()>
-      <#if model.enabled()>
-        <a id=""enabled"" class=""btn btn-default"">Disable</a>
-      <#else>
-        <a id=""enabled"" class=""btn btn-primary"">Enable</a>
-      </#if>
-      <a class=""btn btn-default"" href=""/storage/${model.getName()}/export"""">Export</a>
-      <a id=""del"" class=""btn btn-danger"" onclick=""deleteFunction()"">Delete</a>
+    <button class=""btn btn-default"" type=""submit"" onclick=""doUpdate();"">Update</button>
+    <#if model.enabled()>
+      <a id=""enabled"" class=""btn btn-default"">Disable</a>
+    <#else>
+      <a id=""enabled"" class=""btn btn-primary"">Enable</a>
     </#if>
+    <button type=""button"" class=""btn btn-default export"" name=""${model.getName()}"" data-toggle=""modal""
+            data-target=""#pluginsModal"">
+      Export
+    </button>
+    <a id=""del"" class=""btn btn-danger"" onclick=""deleteFunction()"">Delete</a>
   </form>
   <br>
   <div id=""message"" class=""hidden alert alert-info"">
   </div>
-  <script>
-    var editor = ace.edit(""editor"");
-    var textarea = $('textarea[name=""config""]');
 
+  <#-- Modal window-->
+  <div class=""modal fade"" id=""pluginsModal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""exportPlugin"" aria-hidden=""true"">
+    <div class=""modal-dialog modal-sm"" role=""document"">
+      <!-- Modal content-->
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""exportPlugin"">Export Plugin config</h4>","[{'comment': '```suggestion\r\n          <h4 class=""modal-title"" id=""exportPlugin"">Plugin config</h4>\r\n```', 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -58,38 +89,208 @@
                 ${plugin.getName()}
               </td>
               <td style=""border:none;"">
-                <a class=""btn btn-primary"" href=""/storage/${plugin.getName()}"">Update</a>
-                <a class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">Enable</a>
+                <button type=""button"" class=""btn btn-primary"" onclick=""location.href='/storage/${plugin.getName()}'"">
+                  Update
+                </button>
+                <button type=""button"" class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', true)"">
+                  Enable
+                </button>
+                <button type=""button"" class=""btn btn-primary"" name=""${plugin.getName()}"" data-toggle=""modal""
+                        data-target=""#pluginsModal"">
+                  Export
+                </button>
               </td>
             </tr>
           </#if>
         </#list>
       </tbody>
     </table>
   </div>
-  <div class=""page-header"">
+
+
+  <#-- Modal window for exporting plugin config (including group plugins modal) -->
+  <div class=""modal fade"" id=""pluginsModal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""exportPlugin"" aria-hidden=""true"">
+    <div class=""modal-dialog modal-sm"" role=""document"">
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""exportPlugin"">Export Plugin config</h4>
+        </div>
+        <div class=""modal-body"">
+          <div id=""format"" style=""display: inline-block; position: relative;"">
+            <label for=""format"">Format</label>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""format"" id=""json"" value=""json"" checked=""checked"">
+                JSON
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""format"" id=""hocon"" value=""conf"">
+                HOCON
+              </label>
+            </div>
+          </div>
+
+          <div id=""plugin-set"" class="""" style=""display: inline-block; position: relative; float: right;"">
+            <label for=""format"">Plugin group</label>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""all"" value=""all"" checked=""checked"">
+                ALL
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""enabled"" value=""enabled"">
+                ENABLED
+              </label>
+            </div>
+            <div class=""radio"">
+              <label>
+                <input type=""radio"" name=""number"" id=""disabled"" value=""disabled"">
+                DISABLED
+              </label>
+            </div>
+          </div>
+        </div>
+
+        <div class=""modal-footer"">
+          <button type=""button"" class=""btn btn-default"" data-dismiss=""modal"">Close</button>
+          <button type=""button"" id=""export"" class=""btn btn-primary"">Export</button>
+        </div>
+      </div>
+    </div>
   </div>
-  <div>
-    <h4>New Storage Plugin</h4>
-    <form class=""form-inline"" id=""newStorage"" role=""form"" action=""/"" method=""GET"">
-      <div class=""form-group"">
-        <input type=""text"" class=""form-control"" id=""storageName"" placeholder=""Storage Name"">
+  <#-- Modal window for exporting plugin config (including group plugins modal) -->
+
+  <#-- Modal window for creating plugin -->
+  <div class=""modal fade"" id=""new-plugin-modal"" role=""dialog"" aria-labelledby=""configuration"">
+    <div class=""modal-dialog"" role=""document"">
+      <div class=""modal-content"">
+        <div class=""modal-header"">
+          <button type=""button"" class=""close"" data-dismiss=""modal"" aria-hidden=""true"">&times;</button>
+          <h4 class=""modal-title"" id=""configuration"">New Storage Plugin</h4>
+        </div>
+        <div class=""modal-body"">
+
+          <form id=""createForm"" role=""form"" action=""/storage/create_new"" method=""POST"">
+            <input type=""text"" class=""form-control"" name=""name"" placeholder=""Storage Name"">
+            <h3>Configuration</h3>
+            <div class=""form-group"">
+              <div id=""editor"" class=""form-control""></div>
+                <textarea class=""form-control"" id=""config"" name=""config"" data-editor=""json"" style=""display: none;"">
+                </textarea>
+            </div>
+            <div style=""text-align: right; margin: 10px"">
+              <button type=""button"" class=""btn btn-default"" data-dismiss=""modal"">Close</button>
+              <button type=""submit"" class=""btn btn-primary"" onclick=""doCreate()"">Create</button>
+            </div>
+          </form>
+
+          <div id=""message"" class=""hidden alert alert-info"">
+          </div>
+        </div>
       </div>
-      <button type=""submit"" class=""btn btn-default"" onclick=""doSubmit()"">Create</button>
-    </form>
+    </div>
   </div>
+  <#-- Modal window for creating plugin -->
+
   <script>
-    function doSubmit() {
-      var name = document.getElementById(""storageName"");
-      var form = document.getElementById(""newStorage"");
-      form.action = ""/storage/"" + name.value;
-      form.submit();
-    };
     function doEnable(name, flag) {
-      $.get(""/storage/"" + name + ""/enable/"" + flag, function(data) {
+      $.get(""/storage/"" + name + ""/enable/"" + flag, function() {
         location.reload();
       });
-    };
+    }
+
+    function doCreate() {
+      $(""#createForm"").ajaxForm(function(data) {
+        // alert(""ajax works"");
+        const messageEl = $(""#message"");
+        if (data.result === ""success"") {
+          messageEl.removeClass(""hidden"")
+                   .removeClass(""alert-danger"")
+                   .addClass(""alert-info"")
+                   .text(data.result).alert();
+          setTimeout(function() { location.reload(); }, 800);
+      } else {
+        messageEl.addClass(""hidden"");
+        // Wait a fraction of a second before showing the message again. This
+        // makes it clear if a second attempt gives the same error as
+        // the first that a ""new"" message came back from the server
+        setTimeout(function() {
+            messageEl.removeClass(""hidden"")
+                     .removeClass(""alert-info"")
+                     .addClass(""alert-danger"")
+                     .text(""Please retry: "" + data.result).alert();
+        }, 200);
+        }
+      });
+    }
+
+    // Formatting create plugin textarea
+    $('#new-plugin-modal').on('show.bs.modal', function() {
+        const editor = ace.edit(""editor"");
+        const textarea = $('textarea[name=""config""]');
+
+        editor.setAutoScrollEditorIntoView(true);
+        editor.setOption(""maxLines"", 25);
+        editor.setOption(""minLines"", 10);
+        editor.renderer.setShowGutter(true);
+        editor.renderer.setOption('showLineNumbers', true);
+        editor.renderer.setOption('showPrintMargin', false);
+        editor.getSession().setMode(""ace/mode/json"");
+        editor.setTheme(""ace/theme/eclipse"");
+
+        // copy back to textarea on form submit...
+        editor.getSession().on('change', function(){
+            textarea.val(editor.getSession().getValue());
+        });
+    });
+
+    // Modal windows management
+    let exportInstance; // global variable
+    $('#pluginsModal').on('show.bs.modal', function(event) {
+        console.log(""alarm"");
+      const button = $(event.relatedTarget); // Button that triggered the modal
+      const modal = $(this);
+      exportInstance = button.attr(""name"");
+
+      const optionalBlock = modal.find('#plugin-set');
+      if (exportInstance === ""all"") {
+        optionalBlock.removeClass('hide');
+        modal.find('.modal-title').text('Export all Plugin configs');","[{'comment': ""```suggestion\r\n        modal.find('.modal-title').text('Export all Plugins configs');\r\n```"", 'commenter': 'arina-ielchiieva'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -174,32 +202,34 @@ public JsonResult createOrUpdatePluginJSON(PluginConfigWrapper plugin) {
       return message(""success"");
     } catch (ExecutionSetupException e) {
       logger.error(""Unable to create/ update plugin: "" + plugin.getName(), e);
-      return message(""Error while creating/ updating storage : "" + (e.getCause() != null ? e.getCause().getMessage() : e.getMessage()));
+      return message(""Error while creating / updating storage : "" + (e.getCause() != null ?","[{'comment': '```suggestion\r\n      return message(""Error while creating / updating storage : %s"", e.getCause() == null ?  e.getMessage() : e.getCause().getMessage();\r\n```', 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +65,87 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";","[{'comment': '```suggestion\r\n  public static final String JSON_FORMAT = ""json"";\r\n```', 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +65,87 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";","[{'comment': '```suggestion\r\n  public static final String HOCON_FORMAT = ""conf"";\r\n```', 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +65,87 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";","[{'comment': 'Consider changing visibility of this and previous constants to private.  ', 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +65,87 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";
+
+  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR =
+      Comparator.comparing(PluginConfigWrapper::getName);
 
   @GET
-  @Path(""/storage.json"")
+  @Path(""/storage/{group}/plugins/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public List<PluginConfigWrapper> getStoragePluginsJSON() {
-
-    List<PluginConfigWrapper> list = Lists.newArrayList();
-    for (Map.Entry<String, StoragePluginConfig> entry : Lists.newArrayList(storage.getStore().getAll())) {
-      PluginConfigWrapper plugin = new PluginConfigWrapper(entry.getKey(), entry.getValue());
-      list.add(plugin);
+  public Response getPluginsConfigs(@PathParam(""group"") String pluginGroup, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equalsIgnoreCase(format) || HOCON_FILE_NAME.equalsIgnoreCase(format)) {
+      Response.ResponseBuilder response = Response.ok();
+      response.entity(getPluginsConfigs(pluginGroup).toArray());
+      response.header(""Content-Disposition"",
+          String.format(""attachment;filename=\""%s_storage_plugins.%s\"""", pluginGroup, format));
+      return response.build();","[{'comment': 'Please use method chaining on builder instance, like: \r\n```java\r\n  return Response.ok().entity(...).header(...).build();\r\n```', 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -63,67 +65,87 @@
   @Inject ObjectMapper mapper;
   @Inject SecurityContext sc;
 
-  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR = new Comparator<PluginConfigWrapper>() {
-    @Override
-    public int compare(PluginConfigWrapper o1, PluginConfigWrapper o2) {
-      return o1.getName().compareTo(o2.getName());
-    }
-  };
+  public static final String JSON_FILE_NAME = ""json"";
+  public static final String HOCON_FILE_NAME = ""conf"";
+  public static final String ALL_PLUGINS = ""all"";
+  public static final String ENABLED_PLUGINS = ""enabled"";
+  public static final String DISABLED_PLUGINS = ""disabled"";
+
+  private static final Comparator<PluginConfigWrapper> PLUGIN_COMPARATOR =
+      Comparator.comparing(PluginConfigWrapper::getName);
 
   @GET
-  @Path(""/storage.json"")
+  @Path(""/storage/{group}/plugins/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public List<PluginConfigWrapper> getStoragePluginsJSON() {
-
-    List<PluginConfigWrapper> list = Lists.newArrayList();
-    for (Map.Entry<String, StoragePluginConfig> entry : Lists.newArrayList(storage.getStore().getAll())) {
-      PluginConfigWrapper plugin = new PluginConfigWrapper(entry.getKey(), entry.getValue());
-      list.add(plugin);
+  public Response getPluginsConfigs(@PathParam(""group"") String pluginGroup, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equalsIgnoreCase(format) || HOCON_FILE_NAME.equalsIgnoreCase(format)) {
+      Response.ResponseBuilder response = Response.ok();
+      response.entity(getPluginsConfigs(pluginGroup).toArray());
+      response.header(""Content-Disposition"",
+          String.format(""attachment;filename=\""%s_storage_plugins.%s\"""", pluginGroup, format));
+      return response.build();
     }
-
-    Collections.sort(list, PLUGIN_COMPARATOR);
-
-    return list;
+    logger.error(""Unknown file type {} for {} Storage Plugin Configs"", format, pluginGroup);
+    return Response.status(Response.Status.NOT_FOUND).build();
   }
 
   @GET
   @Path(""/storage"")
   @Produces(MediaType.TEXT_HTML)
-  public Viewable getStoragePlugins() {
-    List<PluginConfigWrapper> list = getStoragePluginsJSON();
+  public Viewable getPlugins() {
+    List<PluginConfigWrapper> list = getPluginsConfigs(ALL_PLUGINS);
     return ViewableWithPermissions.create(authEnabled.get(), ""/rest/storage/list.ftl"", sc, list);
   }
 
+  private List<PluginConfigWrapper> getPluginsConfigs(String pluginGroup) {","[{'comment': 'Could you please try to rename and split the method (PS: the example is untested but expected to be working :) :\r\n\r\n```java\r\n  private List<PluginConfigWrapper> getConfigurationsFor(String pluginGroup) {\r\n    return StreamSupport.stream(Spliterators.spliteratorUnknownSize(storage.getStore().getAll(), Spliterator.ORDERED), /*parallel*/ false)\r\n        .filter(byPluginGroup(pluginGroup))\r\n        .map(pluginEntry -> new PluginConfigWrapper(pluginEntry.getKey(), pluginEntry.getValue()))\r\n        .sorted(PLUGIN_COMPARATOR)\r\n        .collect(Collectors.toList());\r\n  }\r\n\r\n  private Predicate<Map.Entry<String, StoragePluginConfig>> byPluginGroup(String pluginGroup) {\r\n    switch (pluginGroup == null ? """" : pluginGroup.toLowerCase()) {\r\n      case ALL_PLUGINS:\r\n        return entry -> true;\r\n      case ENABLED_PLUGINS:\r\n        return entry -> entry.getValue().isEnabled();\r\n      case DISABLED_PLUGINS:\r\n        return entry -> !entry.getValue().isEnabled();\r\n      default:\r\n        return entry -> false;\r\n    }\r\n  }\r\n```', 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -137,31 +159,37 @@ public JsonResult enablePlugin(@PathParam(""name"") String name, @PathParam(""val"")
   }
 
   @GET
-  @Path(""/storage/{name}/export"")
+  @Path(""/storage/{name}/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public Response exportPlugin(@PathParam(""name"") String name) {
-    Response.ResponseBuilder response = Response.ok(getStoragePluginJSON(name));
-    response.header(""Content-Disposition"", String.format(""attachment;filename=\""%s.json\"""", name));
-    return response.build();
+  public Response exportPlugin(@PathParam(""name"") String name, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equalsIgnoreCase(format) || HOCON_FILE_NAME.equalsIgnoreCase(format)) {
+      PluginConfigWrapper storagePluginConfigs = getPluginConfig(name);
+      Response.ResponseBuilder response = Response.ok(storagePluginConfigs);
+      response.header(""Content-Disposition"", String.format(""attachment;filename=\""%s.%s\"""", name, format));
+      return response.build();","[{'comment': ""Please use Builder's method chaining. See comment above... "", 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -137,31 +159,37 @@ public JsonResult enablePlugin(@PathParam(""name"") String name, @PathParam(""val"")
   }
 
   @GET
-  @Path(""/storage/{name}/export"")
+  @Path(""/storage/{name}/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public Response exportPlugin(@PathParam(""name"") String name) {
-    Response.ResponseBuilder response = Response.ok(getStoragePluginJSON(name));
-    response.header(""Content-Disposition"", String.format(""attachment;filename=\""%s.json\"""", name));
-    return response.build();
+  public Response exportPlugin(@PathParam(""name"") String name, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equalsIgnoreCase(format) || HOCON_FILE_NAME.equalsIgnoreCase(format)) {
+      PluginConfigWrapper storagePluginConfigs = getPluginConfig(name);
+      Response.ResponseBuilder response = Response.ok(storagePluginConfigs);
+      response.header(""Content-Disposition"", String.format(""attachment;filename=\""%s.%s\"""", name, format));
+      return response.build();
+    }
+    logger.error(""Unknown file type {} for Storage Plugin Config: {}"", format, name);
+    return Response.status(Response.Status.NOT_FOUND).build();
   }
 
   @DELETE
-  @Path(""/storage/{name}.json"")
+  @Path(""/storage/{name}.{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public JsonResult deletePluginJSON(@PathParam(""name"") String name) {
-    PluginConfigWrapper plugin = getStoragePluginJSON(name);
-    if (plugin.deleteFromStorage(storage)) {
-      return message(""success"");
-    } else {
-      return message(""error (unable to delete storage)"");
+  public JsonResult deletePlugin(@PathParam(""name"") String name, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equalsIgnoreCase(format) || HOCON_FILE_NAME.equalsIgnoreCase(format)) {","[{'comment': 'Here and in other places format check may be extracted, after that code will look like: \r\n```java\r\n    return isSupported(format) && getPluginConfig(name).deleteFromStorage(storage)\r\n        ? message(""Success"")\r\n        : message(""Error (unable to delete %s.%s storage plugin)"", name, format);\r\n```', 'commenter': 'ihuzenko'}, {'comment': ""Note that similar simplification of nested ```if```'s possible in other methods too. Please revisit other methods in the class. "", 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -137,31 +159,37 @@ public JsonResult enablePlugin(@PathParam(""name"") String name, @PathParam(""val"")
   }
 
   @GET
-  @Path(""/storage/{name}/export"")
+  @Path(""/storage/{name}/export/{format}"")
   @Produces(MediaType.APPLICATION_JSON)
-  public Response exportPlugin(@PathParam(""name"") String name) {
-    Response.ResponseBuilder response = Response.ok(getStoragePluginJSON(name));
-    response.header(""Content-Disposition"", String.format(""attachment;filename=\""%s.json\"""", name));
-    return response.build();
+  public Response exportPlugin(@PathParam(""name"") String name, @PathParam(""format"") String format) {
+    if (JSON_FILE_NAME.equalsIgnoreCase(format) || HOCON_FILE_NAME.equalsIgnoreCase(format)) {
+      PluginConfigWrapper storagePluginConfigs = getPluginConfig(name);
+      Response.ResponseBuilder response = Response.ok(storagePluginConfigs);
+      response.header(""Content-Disposition"", String.format(""attachment;filename=\""%s.%s\"""", name, format));","[{'comment': 'Please use ```javax.ws.rs.core.HttpHeaders``` for header name constants here and in other places.', 'commenter': 'ihuzenko'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -17,79 +17,280 @@
     limitations under the License.
 
 -->
+
 <#include ""*/generic.ftl"">
 <#macro page_head>
+  <script src=""/static/js/jquery.form.js""></script>
+
+  <!-- Ace Libraries for Syntax Formatting -->
+  <script src=""/static/js/ace-code-editor/ace.js"" type=""text/javascript"" charset=""utf-8""></script>
+  <script src=""/static/js/ace-code-editor/theme-eclipse.js"" type=""text/javascript"" charset=""utf-8""></script>
 </#macro>
 
 <#macro page_body>
   <div class=""page-header"">
   </div>
-  <h4>Enabled Storage Plugins</h4>
-  <div class=""table-responsive"">
+
+  <h4 class=""col-xs-6"">Plugin Management</h4>
+  <table style=""margin: 10px"" class=""table"">
+    <tbody>
+    <tr>
+      <td style=""border:none;"">
+        <button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#new-plugin-modal"">
+          Create
+        </button>
+        <button type=""button"" class=""btn btn-primary"" name=""all"" data-toggle=""modal"" data-target=""#pluginsModal"">
+          Export all
+        </button>
+      </td>
+    </tr>
+    </tbody>
+  </table>
+
+  <div class=""page-header"" style=""margin: 5px;""></div>
+
+  <div class=""table-responsive col-sm-12 col-md-6 col-lg-5 col-xl-5"">
+    <h4>Enabled Storage Plugins</h4>
     <table class=""table"">","[{'comment': '```suggestion\r\n    <table class=""table table-hover"">\r\n```\r\nSince the disabled plugins are numerous and there is no table border, hovering will highlight the rows.', 'commenter': 'kkhatua'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -17,79 +17,280 @@
     limitations under the License.
 
 -->
+
 <#include ""*/generic.ftl"">
 <#macro page_head>
+  <script src=""/static/js/jquery.form.js""></script>
+
+  <!-- Ace Libraries for Syntax Formatting -->
+  <script src=""/static/js/ace-code-editor/ace.js"" type=""text/javascript"" charset=""utf-8""></script>
+  <script src=""/static/js/ace-code-editor/theme-eclipse.js"" type=""text/javascript"" charset=""utf-8""></script>
 </#macro>
 
 <#macro page_body>
   <div class=""page-header"">
   </div>
-  <h4>Enabled Storage Plugins</h4>
-  <div class=""table-responsive"">
+
+  <h4 class=""col-xs-6"">Plugin Management</h4>
+  <table style=""margin: 10px"" class=""table"">
+    <tbody>
+    <tr>
+      <td style=""border:none;"">
+        <button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#new-plugin-modal"">
+          Create
+        </button>
+        <button type=""button"" class=""btn btn-primary"" name=""all"" data-toggle=""modal"" data-target=""#pluginsModal"">
+          Export all
+        </button>
+      </td>
+    </tr>
+    </tbody>
+  </table>
+
+  <div class=""page-header"" style=""margin: 5px;""></div>
+
+  <div class=""table-responsive col-sm-12 col-md-6 col-lg-5 col-xl-5"">
+    <h4>Enabled Storage Plugins</h4>
     <table class=""table"">
       <tbody>
         <#list model as plugin>
           <#if plugin.enabled() == true>
             <tr>
-              <td style=""border:none; width:200px;"">
+              <td style=""border:none; max-width: 200px; overflow: hidden; text-overflow: ellipsis;"">
                 ${plugin.getName()}
               </td>
               <td style=""border:none;"">
-                <a class=""btn btn-primary"" href=""/storage/${plugin.getName()}"">Update</a>
-                <a class=""btn btn-default"" onclick=""doEnable('${plugin.getName()}', false)"">Disable</a>
-                <a class=""btn btn-default"" href=""/storage/${plugin.getName()}/export"""">Export</a>
+                <button type=""button"" class=""btn btn-primary"" onclick=""location.href='/storage/${plugin.getName()}'"">
+                  Update
+                </button>
+                <button type=""button"" class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', false)"">
+                  Disable
+                </button>
+                <button type=""button"" class=""btn btn-primary"" name=""${plugin.getName()}"" data-toggle=""modal""
+                        data-target=""#pluginsModal"">
+                  Export
+                </button>
               </td>
             </tr>
           </#if>
         </#list>
       </tbody>
     </table>
   </div>
-  <div class=""page-header"">
-  </div>
-  <h4>Disabled Storage Plugins</h4>
-  <div class=""table-responsive"">
+
+  <div class=""table-responsive col-sm-12 col-md-6 col-lg-7 col-xl-7"">
+    <h4>Disabled Storage Plugins</h4>
     <table class=""table"">","[{'comment': '```suggestion\r\n    <table class=""table table-hover"">\r\n```', 'commenter': 'kkhatua'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -17,79 +17,280 @@
     limitations under the License.
 
 -->
+
 <#include ""*/generic.ftl"">
 <#macro page_head>
+  <script src=""/static/js/jquery.form.js""></script>
+
+  <!-- Ace Libraries for Syntax Formatting -->
+  <script src=""/static/js/ace-code-editor/ace.js"" type=""text/javascript"" charset=""utf-8""></script>
+  <script src=""/static/js/ace-code-editor/theme-eclipse.js"" type=""text/javascript"" charset=""utf-8""></script>
 </#macro>
 
 <#macro page_body>
   <div class=""page-header"">
   </div>
-  <h4>Enabled Storage Plugins</h4>
-  <div class=""table-responsive"">
+
+  <h4 class=""col-xs-6"">Plugin Management</h4>
+  <table style=""margin: 10px"" class=""table"">
+    <tbody>
+    <tr>
+      <td style=""border:none;"">
+        <button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#new-plugin-modal"">
+          Create
+        </button>
+        <button type=""button"" class=""btn btn-primary"" name=""all"" data-toggle=""modal"" data-target=""#pluginsModal"">
+          Export all
+        </button>
+      </td>
+    </tr>
+    </tbody>
+  </table>
+
+  <div class=""page-header"" style=""margin: 5px;""></div>
+
+  <div class=""table-responsive col-sm-12 col-md-6 col-lg-5 col-xl-5"">
+    <h4>Enabled Storage Plugins</h4>
     <table class=""table"">
       <tbody>
         <#list model as plugin>
           <#if plugin.enabled() == true>
             <tr>
-              <td style=""border:none; width:200px;"">
+              <td style=""border:none; max-width: 200px; overflow: hidden; text-overflow: ellipsis;"">
                 ${plugin.getName()}
               </td>
               <td style=""border:none;"">
-                <a class=""btn btn-primary"" href=""/storage/${plugin.getName()}"">Update</a>
-                <a class=""btn btn-default"" onclick=""doEnable('${plugin.getName()}', false)"">Disable</a>
-                <a class=""btn btn-default"" href=""/storage/${plugin.getName()}/export"""">Export</a>
+                <button type=""button"" class=""btn btn-primary"" onclick=""location.href='/storage/${plugin.getName()}'"">
+                  Update
+                </button>
+                <button type=""button"" class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', false)"">","[{'comment': '```suggestion\r\n                <button type=""button"" class=""btn btn-warning"" onclick=""doEnable(\'${plugin.getName()}\', false)"">\r\n```\r\nChanging colors make it easier to avoid disabling plugins accidentally.', 'commenter': 'kkhatua'}]"
1692,exec/java-exec/src/main/resources/rest/storage/list.ftl,"@@ -17,79 +17,280 @@
     limitations under the License.
 
 -->
+
 <#include ""*/generic.ftl"">
 <#macro page_head>
+  <script src=""/static/js/jquery.form.js""></script>
+
+  <!-- Ace Libraries for Syntax Formatting -->
+  <script src=""/static/js/ace-code-editor/ace.js"" type=""text/javascript"" charset=""utf-8""></script>
+  <script src=""/static/js/ace-code-editor/theme-eclipse.js"" type=""text/javascript"" charset=""utf-8""></script>
 </#macro>
 
 <#macro page_body>
   <div class=""page-header"">
   </div>
-  <h4>Enabled Storage Plugins</h4>
-  <div class=""table-responsive"">
+
+  <h4 class=""col-xs-6"">Plugin Management</h4>
+  <table style=""margin: 10px"" class=""table"">
+    <tbody>
+    <tr>
+      <td style=""border:none;"">
+        <button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#new-plugin-modal"">
+          Create
+        </button>
+        <button type=""button"" class=""btn btn-primary"" name=""all"" data-toggle=""modal"" data-target=""#pluginsModal"">
+          Export all
+        </button>
+      </td>
+    </tr>
+    </tbody>
+  </table>
+
+  <div class=""page-header"" style=""margin: 5px;""></div>
+
+  <div class=""table-responsive col-sm-12 col-md-6 col-lg-5 col-xl-5"">
+    <h4>Enabled Storage Plugins</h4>
     <table class=""table"">
       <tbody>
         <#list model as plugin>
           <#if plugin.enabled() == true>
             <tr>
-              <td style=""border:none; width:200px;"">
+              <td style=""border:none; max-width: 200px; overflow: hidden; text-overflow: ellipsis;"">
                 ${plugin.getName()}
               </td>
               <td style=""border:none;"">
-                <a class=""btn btn-primary"" href=""/storage/${plugin.getName()}"">Update</a>
-                <a class=""btn btn-default"" onclick=""doEnable('${plugin.getName()}', false)"">Disable</a>
-                <a class=""btn btn-default"" href=""/storage/${plugin.getName()}/export"""">Export</a>
+                <button type=""button"" class=""btn btn-primary"" onclick=""location.href='/storage/${plugin.getName()}'"">
+                  Update
+                </button>
+                <button type=""button"" class=""btn btn-primary"" onclick=""doEnable('${plugin.getName()}', false)"">
+                  Disable
+                </button>
+                <button type=""button"" class=""btn btn-primary"" name=""${plugin.getName()}"" data-toggle=""modal""","[{'comment': '```suggestion\r\n                <button type=""button"" class=""btn"" name=""${plugin.getName()}"" data-toggle=""modal""\r\n```', 'commenter': 'kkhatua'}]"
1716,exec/java-exec/src/main/java/org/apache/drill/exec/util/record/RecordBatchStats.java,"@@ -271,6 +271,19 @@ public static String printAllocatorStats(BufferAllocator allocator) {
     return msg.toString();
   }
 
+  /**
+   * Prints the configured batch size
+   *
+   * @param batchStatsContext batch stats context object
+   * @param batchSize contains the configured batch size
+   */
+  public static void printConfiguredBatchSize(RecordBatchStatsContext batchStatsContext,
+    int batchSize) {
+","[{'comment': '_logRecordBatchStats(...)_ does nothing if _batchStatsContext.isEnableBatchSzLogging()_ is false. The overhead of creating _message_ and the call overhead of _logRecordBatchStats(...)_ is wasted if that is the case. Please add a check and avoid the overhead.', 'commenter': 'karthik-man'}, {'comment': 'Done.', 'commenter': 'rhou1'}]"
1716,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/batchsizing/RecordBatchSizerManager.java,"@@ -336,8 +336,7 @@ private long normalizeMemorySizePerBatch() {
     }
 
     if (batchStatsContext.isEnableBatchSzLogging()) {
-      final String message = String.format(""The Parquet reader batch memory has been set to [%d] byte(s)"", normalizedMemorySize);
-      RecordBatchStats.logRecordBatchStats(message, batchStatsContext);
+      RecordBatchStats.printConfiguredBatchSize(batchStatsContext, (int)normalizedMemorySize);","[{'comment': '```suggestion\r\n      RecordBatchStats.printConfiguredBatchSize(batchStatsContext, (int) normalizedMemorySize);\r\n```', 'commenter': 'arina-ielchiieva'}, {'comment': 'Done', 'commenter': 'rhou1'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/RefreshMetadataHandler.java,"@@ -161,7 +161,7 @@ public PhysicalPlan getPlan(SqlNode sqlNode) throws ForemanSetupException {
    */
   private SqlNodeList getColumnList(final SqlRefreshMetadata sqlrefreshMetadata) {
     SqlNodeList columnList = sqlrefreshMetadata.getFieldList();
-    if (columnList == null || !SqlNodeList.isEmptyList(columnList)) {","[{'comment': '`SqlNodeList.isEmptyList(node)` checks `node instanceof SqlNodeList`, therefore no need to check `columnList == null` explicitly. \r\n', 'commenter': 'vdiravka'}, {'comment': 'Removed the extra check.', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetFormatPlugin.java,"@@ -301,8 +301,25 @@ private Path getMetadataPath(FileStatus dir) {
       return new Path(dir.getPath(), Metadata.METADATA_FILENAME);
     }
 
+    /**
+     * Check if the metadata version 4 files exist
+     * @param dir the path of the directory
+     * @param fs
+     * @return true if both file metadata and summary cache file exist
+     * @throws IOException in case of problems during accessing files
+     */
+    private boolean currentMetadataFileExists(FileStatus dir, FileSystem fs) throws IOException {
+      for (String metaFileName : Metadata.CURRENT_METADATA_FILENAMES) {","[{'comment': ""It's a pity that functional style can't be used here, since `FileSystem.exists()` throws checked exception.\r\n```\r\nreturn Arrays.stream(Metadata.OLD_METADATA_FILENAMES)\r\n              .allMatch(metaFileName -> fs.exists(new Path(dir.getPath(), metaFileName)));\r\n```\r\n\r\nPossibly we will introduce `FunctionWithException` in Drill in future.\r\n\r\nAll are fine here now."", 'commenter': 'vdiravka'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java,"@@ -387,6 +389,14 @@ private static boolean allowBinaryMetadata(String drillVersion, ParquetReaderCon
           names.add(Arrays.asList(columnTypeMetadata.name));
         }
       }
+    } else if (parquetTableMetadata instanceof ParquetTableMetadata_v4) {","[{'comment': 'This logic is almost the same as for `ParquetTableMetadata_v3`.\r\nCan you change usage of `columnTypeInfo` field to getter everywhere?', 'commenter': 'vdiravka'}, {'comment': 'Changed to get columnTypeInfo from MetadataBase', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata_V4.java,"@@ -0,0 +1,648 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.metadata;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.KeyDeserializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
+import org.apache.drill.common.expression.SchemaPath;
+
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ColumnMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ParquetFileMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ParquetTableMetadataBase;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.RowGroupMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataVersion.Constants.V4;
+import org.apache.hadoop.fs.Path;
+import org.apache.parquet.io.api.Binary;
+import org.apache.parquet.schema.OriginalType;
+import org.apache.parquet.schema.PrimitiveType;
+
+public class Metadata_V4 {
+
+  public static class ParquetTableMetadata_v4 extends ParquetTableMetadataBase {
+
+    MetadataSummary metadataSummary = new MetadataSummary();
+    FileMetadata fileMetadata = new FileMetadata();
+
+    public ParquetTableMetadata_v4(MetadataSummary metadataSummary) {
+      this.metadataSummary = metadataSummary;
+    }
+
+    public ParquetTableMetadata_v4(MetadataSummary metadataSummary, FileMetadata fileMetadata) {
+      this.metadataSummary = metadataSummary;
+      this.fileMetadata = fileMetadata;
+    }
+
+    public ParquetTableMetadata_v4(String metadataVersion, ParquetTableMetadataBase parquetTableMetadata,
+                                   List<ParquetFileMetadata_v4> files, List<Path> directories, String drillVersion, long totalRowCount, boolean allColumnsInteresting) {
+      this.metadataSummary.metadataVersion = metadataVersion;
+      this.fileMetadata.files = files;
+      this.metadataSummary.directories = directories;
+      this.metadataSummary.columnTypeInfo = ((ParquetTableMetadata_v4) parquetTableMetadata).metadataSummary.columnTypeInfo;
+      this.metadataSummary.drillVersion = drillVersion;
+      this.metadataSummary.totalRowCount = totalRowCount;
+      this.metadataSummary.allColumnsInteresting = allColumnsInteresting;
+    }
+
+    public ColumnTypeMetadata_v4 getColumnTypeInfo(String[] name) {
+      return metadataSummary.getColumnTypeInfo(name);
+    }
+
+    @JsonIgnore
+    @Override
+    public List<Path> getDirectories() {
+      return metadataSummary.getDirectories();
+    }
+
+    @Override
+    public List<? extends ParquetFileMetadata> getFiles() {
+      return fileMetadata.getFiles();
+    }
+
+    @Override
+    public String getMetadataVersion() {
+      return metadataSummary.getMetadataVersion();
+    }
+
+    /**
+     * If directories list and file metadata list contain relative paths, update it to absolute ones
+     *
+     * @param baseDir base parent directory
+     */
+    public void updateRelativePaths(String baseDir) {
+      // update directories paths to absolute ones
+      this.metadataSummary.directories = MetadataPathUtils.convertToAbsolutePaths(metadataSummary.directories, baseDir);
+
+      // update files paths to absolute ones
+      this.fileMetadata.files = MetadataPathUtils.convertToFilesWithAbsolutePathsForV4(fileMetadata.files, baseDir);
+    }
+
+    @Override
+    public void assignFiles(List<? extends ParquetFileMetadata> newFiles) {
+      this.fileMetadata.assignFiles(newFiles);
+    }
+
+    @Override
+    public boolean hasColumnMetadata() {
+      return true;
+    }
+
+    @Override
+    public PrimitiveType.PrimitiveTypeName getPrimitiveType(String[] columnName) {
+      return getColumnTypeInfo(columnName).primitiveType;
+    }
+
+    @Override
+    public OriginalType getOriginalType(String[] columnName) {
+      return getColumnTypeInfo(columnName).originalType;
+    }
+
+    @Override
+    public Integer getRepetitionLevel(String[] columnName) {
+      return getColumnTypeInfo(columnName).repetitionLevel;
+    }
+
+    @Override
+    public Integer getDefinitionLevel(String[] columnName) {
+      return getColumnTypeInfo(columnName).definitionLevel;
+    }
+
+    @Override
+    public Integer getScale(String[] columnName) {
+      return getColumnTypeInfo(columnName).scale;
+    }
+
+    @Override
+    public Integer getPrecision(String[] columnName) {
+      return getColumnTypeInfo(columnName).precision;
+    }
+
+    @Override
+    public boolean isRowGroupPrunable() {
+      return true;
+    }
+
+    @Override
+    public ParquetTableMetadataBase clone() {
+      return new ParquetTableMetadata_v4(metadataSummary, fileMetadata);
+    }
+
+    @Override
+    public String getDrillVersion() {
+      return metadataSummary.drillVersion;
+    }
+
+
+    public MetadataSummary getSummary() {
+      return metadataSummary;
+    }
+
+
+    public long getTotalRowCount() {
+      return metadataSummary.getTotalRowCount();
+    }
+
+
+    public long getTotalNullCount(String[] columnName) {
+      return getColumnTypeInfo(columnName).totalNullCount;
+    }
+
+
+    public boolean isAllColumnsInteresting() {
+      return metadataSummary.isAllColumnsInteresting();
+    }
+
+    public ConcurrentHashMap<ColumnTypeMetadata_v4.Key, ColumnTypeMetadata_v4> getColumnTypeInfoMap() {
+      return metadataSummary.columnTypeInfo;
+    }
+
+    public void setTotalRowCount(long totalRowCount) {
+      metadataSummary.setTotalRowCount(totalRowCount);
+    }
+
+    public void setAllColumnsInteresting(boolean allColumnsInteresting) {
+      metadataSummary.allColumnsInteresting = allColumnsInteresting;
+    }
+
+  }
+
+  /**
+   * Struct which contains the metadata for a single parquet file
+   */
+  public static class ParquetFileMetadata_v4 extends ParquetFileMetadata {
+    @JsonProperty
+    public Path path;
+    @JsonProperty
+    public Long length;
+    @JsonProperty
+    public List<RowGroupMetadata_v4> rowGroups;
+
+    public ParquetFileMetadata_v4() {
+
+    }
+
+    public ParquetFileMetadata_v4(Path path, Long length, List<RowGroupMetadata_v4> rowGroups) {
+      this.path = path;
+      this.length = length;
+      this.rowGroups = rowGroups;
+    }
+
+    @Override
+    public String toString() {
+      return String.format(""path: %s rowGroups: %s"", path, rowGroups);
+    }
+
+    @JsonIgnore
+    @Override
+    public Path getPath() {
+      return path;
+    }
+
+    @JsonIgnore
+    @Override
+    public Long getLength() {
+      return length;
+    }
+
+    @JsonIgnore
+    @Override
+    public List<? extends RowGroupMetadata> getRowGroups() {
+      return rowGroups;
+    }
+  }
+
+
+  /**
+   * A struct that contains the metadata for a parquet row group
+   */
+  public static class RowGroupMetadata_v4 extends RowGroupMetadata {
+    @JsonProperty
+    public Long start;
+    @JsonProperty
+    public Long length;
+    @JsonProperty
+    public Long rowCount;
+    @JsonProperty
+    public Map<String, Float> hostAffinity;
+    @JsonProperty
+    public List<ColumnMetadata_v4> columns;
+
+    public RowGroupMetadata_v4() {
+    }
+
+    public RowGroupMetadata_v4(Long start, Long length, Long rowCount, Map<String, Float> hostAffinity,
+                               List<ColumnMetadata_v4> columns) {
+      this.start = start;
+      this.length = length;
+      this.rowCount = rowCount;
+      this.hostAffinity = hostAffinity;
+      this.columns = columns;
+    }
+
+    @Override
+    public Long getStart() {
+      return start;
+    }
+
+    @Override
+    public Long getLength() {
+      return length;
+    }
+
+    @Override
+    public Long getRowCount() {
+      return rowCount;
+    }
+
+    @Override
+    public Map<String, Float> getHostAffinity() {
+      return hostAffinity;
+    }
+
+    @Override
+    public List<? extends ColumnMetadata> getColumns() {
+      return columns;
+    }
+  }
+
+
+  public static class ColumnTypeMetadata_v4 {
+    @JsonProperty
+    public String[] name;
+    @JsonProperty
+    public PrimitiveType.PrimitiveTypeName primitiveType;
+    @JsonProperty
+    public OriginalType originalType;
+    @JsonProperty
+    public int precision;
+    @JsonProperty
+    public int scale;
+    @JsonProperty
+    public int repetitionLevel;
+    @JsonProperty
+    public int definitionLevel;
+    @JsonProperty
+    public long totalNullCount = 0;
+    @JsonProperty
+    public boolean isInteresting = false;
+
+    // Key to find by name only
+    @JsonIgnore
+    private Key key;
+
+    public ColumnTypeMetadata_v4() {
+    }
+
+    public ColumnTypeMetadata_v4(String[] name, PrimitiveType.PrimitiveTypeName primitiveType, OriginalType originalType, int precision, int scale, int repetitionLevel, int definitionLevel, long totalNullCount, boolean isInteresting) {
+      this.name = name;
+      this.primitiveType = primitiveType;
+      this.originalType = originalType;
+      this.precision = precision;
+      this.scale = scale;
+      this.repetitionLevel = repetitionLevel;
+      this.definitionLevel = definitionLevel;
+      this.key = new Key(name);
+      this.totalNullCount = totalNullCount;
+      this.isInteresting = isInteresting;
+    }
+
+    @JsonIgnore
+    private Key key() {
+      return this.key;
+    }
+
+    public static class Key {
+      private SchemaPath name;
+      private int hashCode = 0;
+
+      public Key(String[] name) {
+        this.name = SchemaPath.getCompoundPath(name);
+      }
+
+      public Key(SchemaPath name) {
+        this.name = new SchemaPath(name);
+      }
+
+      @Override
+      public int hashCode() {
+        if (hashCode == 0) {
+          hashCode = name.hashCode();
+        }
+        return hashCode;
+      }
+
+      @Override
+      public boolean equals(Object obj) {
+        if (obj == null) {
+          return false;
+        }
+        if (getClass() != obj.getClass()) {
+          return false;
+        }
+        final Key other = (Key) obj;
+        return this.name.equals(other.name);
+      }
+
+      @Override
+      public String toString() {
+        return name.toString();
+      }
+
+      public static class DeSerializer extends KeyDeserializer {
+
+        public DeSerializer() {
+        }
+
+        @Override
+        public Object deserializeKey(String key, com.fasterxml.jackson.databind.DeserializationContext ctxt) {
+          // key string should contain '`' char if the field was serialized as SchemaPath object
+          if (key.contains(""`"")) {
+            return new Key(SchemaPath.parseFromString(key));
+          }
+          return new Key(key.split(""\\.""));
+        }
+      }
+    }
+  }
+
+
+  /**
+   * A struct that contains the metadata for a column in a parquet file
+   */
+  public static class ColumnMetadata_v4 extends ColumnMetadata {","[{'comment': '`ColumnMetadata_v4` is the same as `ColumnMetadata_v3` along with its serializer. I wonder if it is possible to use only one version of ColumnMetadata to avoid duplicating of code.', 'commenter': 'vdiravka'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata_V4.java,"@@ -0,0 +1,648 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.metadata;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.KeyDeserializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
+import org.apache.drill.common.expression.SchemaPath;
+
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ColumnMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ParquetFileMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ParquetTableMetadataBase;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.RowGroupMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataVersion.Constants.V4;
+import org.apache.hadoop.fs.Path;
+import org.apache.parquet.io.api.Binary;
+import org.apache.parquet.schema.OriginalType;
+import org.apache.parquet.schema.PrimitiveType;
+
+public class Metadata_V4 {
+
+  public static class ParquetTableMetadata_v4 extends ParquetTableMetadataBase {
+
+    MetadataSummary metadataSummary = new MetadataSummary();
+    FileMetadata fileMetadata = new FileMetadata();
+
+    public ParquetTableMetadata_v4(MetadataSummary metadataSummary) {
+      this.metadataSummary = metadataSummary;
+    }
+
+    public ParquetTableMetadata_v4(MetadataSummary metadataSummary, FileMetadata fileMetadata) {
+      this.metadataSummary = metadataSummary;
+      this.fileMetadata = fileMetadata;
+    }
+
+    public ParquetTableMetadata_v4(String metadataVersion, ParquetTableMetadataBase parquetTableMetadata,
+                                   List<ParquetFileMetadata_v4> files, List<Path> directories, String drillVersion, long totalRowCount, boolean allColumnsInteresting) {
+      this.metadataSummary.metadataVersion = metadataVersion;
+      this.fileMetadata.files = files;
+      this.metadataSummary.directories = directories;
+      this.metadataSummary.columnTypeInfo = ((ParquetTableMetadata_v4) parquetTableMetadata).metadataSummary.columnTypeInfo;
+      this.metadataSummary.drillVersion = drillVersion;
+      this.metadataSummary.totalRowCount = totalRowCount;
+      this.metadataSummary.allColumnsInteresting = allColumnsInteresting;
+    }
+
+    public ColumnTypeMetadata_v4 getColumnTypeInfo(String[] name) {
+      return metadataSummary.getColumnTypeInfo(name);
+    }
+
+    @JsonIgnore
+    @Override
+    public List<Path> getDirectories() {
+      return metadataSummary.getDirectories();
+    }
+
+    @Override
+    public List<? extends ParquetFileMetadata> getFiles() {
+      return fileMetadata.getFiles();
+    }
+
+    @Override
+    public String getMetadataVersion() {
+      return metadataSummary.getMetadataVersion();
+    }
+
+    /**
+     * If directories list and file metadata list contain relative paths, update it to absolute ones
+     *
+     * @param baseDir base parent directory
+     */
+    public void updateRelativePaths(String baseDir) {
+      // update directories paths to absolute ones
+      this.metadataSummary.directories = MetadataPathUtils.convertToAbsolutePaths(metadataSummary.directories, baseDir);
+
+      // update files paths to absolute ones
+      this.fileMetadata.files = MetadataPathUtils.convertToFilesWithAbsolutePathsForV4(fileMetadata.files, baseDir);
+    }
+
+    @Override
+    public void assignFiles(List<? extends ParquetFileMetadata> newFiles) {
+      this.fileMetadata.assignFiles(newFiles);
+    }
+
+    @Override
+    public boolean hasColumnMetadata() {
+      return true;
+    }
+
+    @Override
+    public PrimitiveType.PrimitiveTypeName getPrimitiveType(String[] columnName) {
+      return getColumnTypeInfo(columnName).primitiveType;
+    }
+
+    @Override
+    public OriginalType getOriginalType(String[] columnName) {
+      return getColumnTypeInfo(columnName).originalType;
+    }
+
+    @Override
+    public Integer getRepetitionLevel(String[] columnName) {
+      return getColumnTypeInfo(columnName).repetitionLevel;
+    }
+
+    @Override
+    public Integer getDefinitionLevel(String[] columnName) {
+      return getColumnTypeInfo(columnName).definitionLevel;
+    }
+
+    @Override
+    public Integer getScale(String[] columnName) {
+      return getColumnTypeInfo(columnName).scale;
+    }
+
+    @Override
+    public Integer getPrecision(String[] columnName) {
+      return getColumnTypeInfo(columnName).precision;
+    }
+
+    @Override
+    public boolean isRowGroupPrunable() {
+      return true;
+    }
+
+    @Override
+    public ParquetTableMetadataBase clone() {
+      return new ParquetTableMetadata_v4(metadataSummary, fileMetadata);
+    }
+
+    @Override
+    public String getDrillVersion() {
+      return metadataSummary.drillVersion;
+    }
+
+
+    public MetadataSummary getSummary() {
+      return metadataSummary;
+    }
+
+
+    public long getTotalRowCount() {
+      return metadataSummary.getTotalRowCount();
+    }
+
+
+    public long getTotalNullCount(String[] columnName) {
+      return getColumnTypeInfo(columnName).totalNullCount;
+    }
+
+
+    public boolean isAllColumnsInteresting() {
+      return metadataSummary.isAllColumnsInteresting();
+    }
+
+    public ConcurrentHashMap<ColumnTypeMetadata_v4.Key, ColumnTypeMetadata_v4> getColumnTypeInfoMap() {
+      return metadataSummary.columnTypeInfo;
+    }
+
+    public void setTotalRowCount(long totalRowCount) {
+      metadataSummary.setTotalRowCount(totalRowCount);
+    }
+
+    public void setAllColumnsInteresting(boolean allColumnsInteresting) {
+      metadataSummary.allColumnsInteresting = allColumnsInteresting;
+    }
+
+  }
+
+  /**
+   * Struct which contains the metadata for a single parquet file
+   */
+  public static class ParquetFileMetadata_v4 extends ParquetFileMetadata {
+    @JsonProperty
+    public Path path;
+    @JsonProperty
+    public Long length;
+    @JsonProperty
+    public List<RowGroupMetadata_v4> rowGroups;
+
+    public ParquetFileMetadata_v4() {
+
+    }
+
+    public ParquetFileMetadata_v4(Path path, Long length, List<RowGroupMetadata_v4> rowGroups) {
+      this.path = path;
+      this.length = length;
+      this.rowGroups = rowGroups;
+    }
+
+    @Override
+    public String toString() {
+      return String.format(""path: %s rowGroups: %s"", path, rowGroups);
+    }
+
+    @JsonIgnore
+    @Override
+    public Path getPath() {
+      return path;
+    }
+
+    @JsonIgnore
+    @Override
+    public Long getLength() {
+      return length;
+    }
+
+    @JsonIgnore
+    @Override
+    public List<? extends RowGroupMetadata> getRowGroups() {
+      return rowGroups;
+    }
+  }
+
+
+  /**
+   * A struct that contains the metadata for a parquet row group
+   */
+  public static class RowGroupMetadata_v4 extends RowGroupMetadata {
+    @JsonProperty
+    public Long start;
+    @JsonProperty
+    public Long length;
+    @JsonProperty
+    public Long rowCount;
+    @JsonProperty
+    public Map<String, Float> hostAffinity;
+    @JsonProperty
+    public List<ColumnMetadata_v4> columns;
+
+    public RowGroupMetadata_v4() {
+    }
+
+    public RowGroupMetadata_v4(Long start, Long length, Long rowCount, Map<String, Float> hostAffinity,
+                               List<ColumnMetadata_v4> columns) {
+      this.start = start;
+      this.length = length;
+      this.rowCount = rowCount;
+      this.hostAffinity = hostAffinity;
+      this.columns = columns;
+    }
+
+    @Override
+    public Long getStart() {
+      return start;
+    }
+
+    @Override
+    public Long getLength() {
+      return length;
+    }
+
+    @Override
+    public Long getRowCount() {
+      return rowCount;
+    }
+
+    @Override
+    public Map<String, Float> getHostAffinity() {
+      return hostAffinity;
+    }
+
+    @Override
+    public List<? extends ColumnMetadata> getColumns() {
+      return columns;
+    }
+  }
+
+
+  public static class ColumnTypeMetadata_v4 {
+    @JsonProperty
+    public String[] name;
+    @JsonProperty
+    public PrimitiveType.PrimitiveTypeName primitiveType;
+    @JsonProperty
+    public OriginalType originalType;
+    @JsonProperty
+    public int precision;
+    @JsonProperty
+    public int scale;
+    @JsonProperty
+    public int repetitionLevel;
+    @JsonProperty
+    public int definitionLevel;
+    @JsonProperty
+    public long totalNullCount = 0;
+    @JsonProperty
+    public boolean isInteresting = false;
+
+    // Key to find by name only
+    @JsonIgnore
+    private Key key;
+
+    public ColumnTypeMetadata_v4() {
+    }
+
+    public ColumnTypeMetadata_v4(String[] name, PrimitiveType.PrimitiveTypeName primitiveType, OriginalType originalType, int precision, int scale, int repetitionLevel, int definitionLevel, long totalNullCount, boolean isInteresting) {
+      this.name = name;
+      this.primitiveType = primitiveType;
+      this.originalType = originalType;
+      this.precision = precision;
+      this.scale = scale;
+      this.repetitionLevel = repetitionLevel;
+      this.definitionLevel = definitionLevel;
+      this.key = new Key(name);
+      this.totalNullCount = totalNullCount;
+      this.isInteresting = isInteresting;
+    }
+
+    @JsonIgnore
+    private Key key() {
+      return this.key;
+    }
+
+    public static class Key {
+      private SchemaPath name;
+      private int hashCode = 0;
+
+      public Key(String[] name) {
+        this.name = SchemaPath.getCompoundPath(name);
+      }
+
+      public Key(SchemaPath name) {
+        this.name = new SchemaPath(name);
+      }
+
+      @Override
+      public int hashCode() {
+        if (hashCode == 0) {
+          hashCode = name.hashCode();
+        }
+        return hashCode;
+      }
+
+      @Override
+      public boolean equals(Object obj) {
+        if (obj == null) {
+          return false;
+        }
+        if (getClass() != obj.getClass()) {
+          return false;
+        }
+        final Key other = (Key) obj;
+        return this.name.equals(other.name);
+      }
+
+      @Override
+      public String toString() {
+        return name.toString();
+      }
+
+      public static class DeSerializer extends KeyDeserializer {
+
+        public DeSerializer() {
+        }
+
+        @Override
+        public Object deserializeKey(String key, com.fasterxml.jackson.databind.DeserializationContext ctxt) {
+          // key string should contain '`' char if the field was serialized as SchemaPath object
+          if (key.contains(""`"")) {
+            return new Key(SchemaPath.parseFromString(key));
+          }
+          return new Key(key.split(""\\.""));
+        }
+      }
+    }
+  }
+
+
+  /**
+   * A struct that contains the metadata for a column in a parquet file
+   */
+  public static class ColumnMetadata_v4 extends ColumnMetadata {
+    // Use a string array for name instead of Schema Path to make serialization easier
+    @JsonProperty
+    public String[] name;
+    @JsonProperty
+    public Long nulls;
+
+    public Object minValue;
+    public Object maxValue;
+
+    @JsonIgnore
+    private PrimitiveType.PrimitiveTypeName primitiveType;
+
+    public ColumnMetadata_v4() {
+    }
+
+    public ColumnMetadata_v4(String[] name, PrimitiveType.PrimitiveTypeName primitiveType, Object minValue, Object maxValue, Long nulls) {
+      this.name = name;
+      this.minValue = minValue;
+      this.maxValue = maxValue;
+      this.nulls = nulls;
+      this.primitiveType = primitiveType;
+    }
+
+    @JsonProperty(value = ""minValue"")
+    public void setMin(Object minValue) {
+      this.minValue = minValue;
+    }
+
+    @JsonProperty(value = ""maxValue"")
+    public void setMax(Object maxValue) {
+      this.maxValue = maxValue;
+    }
+
+    @Override
+    public String[] getName() {
+      return name;
+    }
+
+    @Override
+    public Long getNulls() {
+      return nulls;
+    }
+
+    /**
+     * Checks that the column chunk has a single value.
+     * Returns {@code true} if {@code minValue} and {@code maxValue} are the same but not null
+     * and nulls count is 0 or equal to the rows count.
+     * <p>
+     * Returns {@code true} if {@code minValue} and {@code maxValue} are null and the number of null values
+     * in the column chunk is equal to the rows count.
+     * <p>
+     * Comparison of nulls and rows count is needed for the cases:
+     * <ul>
+     * <li>column with primitive type has single value and null values</li>
+     *
+     * <li>column <b>with primitive type</b> has only null values, min/max couldn't be null,
+     * but column has single value</li>
+     * </ul>
+     *
+     * @param rowCount rows count in column chunk
+     * @return true if column has single value
+     */
+    @Override
+    public boolean hasSingleValue(long rowCount) {
+      if (isNumNullsSet()) {
+        if (minValue != null) {
+          // Objects.deepEquals() is used here, since min and max may be byte arrays
+          return (nulls == 0 || nulls == rowCount) && Objects.deepEquals(minValue, maxValue);
+        } else {
+          return nulls == rowCount && maxValue == null;
+        }
+      }
+      return false;
+    }
+
+    @Override
+    public Object getMinValue() {
+      return minValue;
+    }
+
+    @Override
+    public Object getMaxValue() {
+      return maxValue;
+    }
+
+    @Override
+    public PrimitiveType.PrimitiveTypeName getPrimitiveType() {
+      return null;
+    }
+
+    @Override
+    public OriginalType getOriginalType() {
+      return null;
+    }
+
+    // We use a custom serializer and write only non null values.
+    public static class Serializer extends JsonSerializer<ColumnMetadata_v4> {
+      @Override
+      public void serialize(ColumnMetadata_v4 value, JsonGenerator jgen, SerializerProvider provider) throws IOException {
+        jgen.writeStartObject();
+        jgen.writeArrayFieldStart(""name"");
+        for (String n : value.name) {
+          jgen.writeString(n);
+        }
+        jgen.writeEndArray();
+        if (value.minValue != null) {
+          Object val;
+          if (value.primitiveType == PrimitiveType.PrimitiveTypeName.BINARY
+                  || value.primitiveType == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {
+            val = ((Binary) value.minValue).getBytes();
+          } else {
+            val = value.minValue;
+          }
+          jgen.writeObjectField(""minValue"", val);
+        }
+        if (value.maxValue != null) {
+          Object val;
+          if (value.primitiveType == PrimitiveType.PrimitiveTypeName.BINARY
+                  || value.primitiveType == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {
+            val = ((Binary) value.maxValue).getBytes();
+          } else {
+            val = value.maxValue;
+          }
+          jgen.writeObjectField(""maxValue"", val);
+        }
+
+        if (value.nulls != null) {
+          jgen.writeObjectField(""nulls"", value.nulls);
+        }
+        jgen.writeEndObject();
+      }
+    }
+  }
+
+  @JsonTypeName(V4)
+  public static class MetadataSummary {
+
+    @JsonProperty(value = ""metadata_version"")
+    private String metadataVersion;
+    /*
+     ColumnTypeInfo is schema information from all the files and row groups, merged into
+     one. To get this info, we pass the ParquetTableMetadata object all the way dow to the
+     RowGroup and the column type is built there as it is read from the footer.
+     */
+    @JsonProperty
+    public ConcurrentHashMap<ColumnTypeMetadata_v4.Key, ColumnTypeMetadata_v4> columnTypeInfo;
+    @JsonProperty
+    List<Path> directories;
+    @JsonProperty
+    String drillVersion;
+    @JsonProperty
+    long totalRowCount = 0;
+    @JsonProperty
+    boolean allColumnsInteresting = false;
+
+    public MetadataSummary() {
+
+    }
+
+    public MetadataSummary(String metadataVersion, String drillVersion) {
+      this.metadataVersion = metadataVersion;
+      this.drillVersion = drillVersion;
+    }
+
+    public MetadataSummary(String metadataVersion, String drillVersion, List<Path> directories) {
+      this.metadataVersion = metadataVersion;
+      this.drillVersion = drillVersion;
+      this.directories = directories;
+    }
+
+    public ColumnTypeMetadata_v4 getColumnTypeInfo(String[] name) {
+      return columnTypeInfo.get(new ColumnTypeMetadata_v4.Key(name));
+    }
+
+    public List<Path> getDirectories() {
+      return directories;
+    }
+
+    @JsonIgnore
+    public String getMetadataVersion() {
+      return metadataVersion;
+    }
+
+    public boolean isAllColumnsInteresting() {
+      return allColumnsInteresting;
+    }
+
+    public void setAllColumnsInteresting(boolean allColumnsInteresting) {
+      this.allColumnsInteresting = allColumnsInteresting;
+    }
+
+    public void setTotalRowCount(Long totalRowCount) {
+      this.totalRowCount = totalRowCount;
+    }
+
+    public Long getTotalRowCount() {
+      return this.totalRowCount;
+    }
+  }
+
+  //  @JsonTypeName (V4)","[{'comment': 'Please add proper Javadoc', 'commenter': 'vdiravka'}, {'comment': 'Added JavaDoc', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata_V4.java,"@@ -0,0 +1,648 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.metadata;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.KeyDeserializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
+import org.apache.drill.common.expression.SchemaPath;
+
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ColumnMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ParquetFileMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ParquetTableMetadataBase;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.RowGroupMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataVersion.Constants.V4;
+import org.apache.hadoop.fs.Path;
+import org.apache.parquet.io.api.Binary;
+import org.apache.parquet.schema.OriginalType;
+import org.apache.parquet.schema.PrimitiveType;
+
+public class Metadata_V4 {
+
+  public static class ParquetTableMetadata_v4 extends ParquetTableMetadataBase {
+
+    MetadataSummary metadataSummary = new MetadataSummary();
+    FileMetadata fileMetadata = new FileMetadata();
+
+    public ParquetTableMetadata_v4(MetadataSummary metadataSummary) {
+      this.metadataSummary = metadataSummary;
+    }
+
+    public ParquetTableMetadata_v4(MetadataSummary metadataSummary, FileMetadata fileMetadata) {
+      this.metadataSummary = metadataSummary;
+      this.fileMetadata = fileMetadata;
+    }
+
+    public ParquetTableMetadata_v4(String metadataVersion, ParquetTableMetadataBase parquetTableMetadata,
+                                   List<ParquetFileMetadata_v4> files, List<Path> directories, String drillVersion, long totalRowCount, boolean allColumnsInteresting) {
+      this.metadataSummary.metadataVersion = metadataVersion;
+      this.fileMetadata.files = files;
+      this.metadataSummary.directories = directories;
+      this.metadataSummary.columnTypeInfo = ((ParquetTableMetadata_v4) parquetTableMetadata).metadataSummary.columnTypeInfo;
+      this.metadataSummary.drillVersion = drillVersion;
+      this.metadataSummary.totalRowCount = totalRowCount;
+      this.metadataSummary.allColumnsInteresting = allColumnsInteresting;
+    }
+
+    public ColumnTypeMetadata_v4 getColumnTypeInfo(String[] name) {
+      return metadataSummary.getColumnTypeInfo(name);
+    }
+
+    @JsonIgnore
+    @Override
+    public List<Path> getDirectories() {
+      return metadataSummary.getDirectories();
+    }
+
+    @Override
+    public List<? extends ParquetFileMetadata> getFiles() {
+      return fileMetadata.getFiles();
+    }
+
+    @Override
+    public String getMetadataVersion() {
+      return metadataSummary.getMetadataVersion();
+    }
+
+    /**
+     * If directories list and file metadata list contain relative paths, update it to absolute ones
+     *
+     * @param baseDir base parent directory
+     */
+    public void updateRelativePaths(String baseDir) {
+      // update directories paths to absolute ones
+      this.metadataSummary.directories = MetadataPathUtils.convertToAbsolutePaths(metadataSummary.directories, baseDir);
+
+      // update files paths to absolute ones
+      this.fileMetadata.files = MetadataPathUtils.convertToFilesWithAbsolutePathsForV4(fileMetadata.files, baseDir);
+    }
+
+    @Override
+    public void assignFiles(List<? extends ParquetFileMetadata> newFiles) {
+      this.fileMetadata.assignFiles(newFiles);
+    }
+
+    @Override
+    public boolean hasColumnMetadata() {
+      return true;
+    }
+
+    @Override
+    public PrimitiveType.PrimitiveTypeName getPrimitiveType(String[] columnName) {
+      return getColumnTypeInfo(columnName).primitiveType;
+    }
+
+    @Override
+    public OriginalType getOriginalType(String[] columnName) {
+      return getColumnTypeInfo(columnName).originalType;
+    }
+
+    @Override
+    public Integer getRepetitionLevel(String[] columnName) {
+      return getColumnTypeInfo(columnName).repetitionLevel;
+    }
+
+    @Override
+    public Integer getDefinitionLevel(String[] columnName) {
+      return getColumnTypeInfo(columnName).definitionLevel;
+    }
+
+    @Override
+    public Integer getScale(String[] columnName) {
+      return getColumnTypeInfo(columnName).scale;
+    }
+
+    @Override
+    public Integer getPrecision(String[] columnName) {
+      return getColumnTypeInfo(columnName).precision;
+    }
+
+    @Override
+    public boolean isRowGroupPrunable() {
+      return true;
+    }
+
+    @Override
+    public ParquetTableMetadataBase clone() {
+      return new ParquetTableMetadata_v4(metadataSummary, fileMetadata);
+    }
+
+    @Override
+    public String getDrillVersion() {
+      return metadataSummary.drillVersion;
+    }
+
+
+    public MetadataSummary getSummary() {
+      return metadataSummary;
+    }
+
+
+    public long getTotalRowCount() {
+      return metadataSummary.getTotalRowCount();
+    }
+
+
+    public long getTotalNullCount(String[] columnName) {
+      return getColumnTypeInfo(columnName).totalNullCount;
+    }
+
+
+    public boolean isAllColumnsInteresting() {
+      return metadataSummary.isAllColumnsInteresting();
+    }
+
+    public ConcurrentHashMap<ColumnTypeMetadata_v4.Key, ColumnTypeMetadata_v4> getColumnTypeInfoMap() {
+      return metadataSummary.columnTypeInfo;
+    }
+
+    public void setTotalRowCount(long totalRowCount) {
+      metadataSummary.setTotalRowCount(totalRowCount);
+    }
+
+    public void setAllColumnsInteresting(boolean allColumnsInteresting) {
+      metadataSummary.allColumnsInteresting = allColumnsInteresting;
+    }
+
+  }
+
+  /**
+   * Struct which contains the metadata for a single parquet file
+   */
+  public static class ParquetFileMetadata_v4 extends ParquetFileMetadata {
+    @JsonProperty
+    public Path path;
+    @JsonProperty
+    public Long length;
+    @JsonProperty
+    public List<RowGroupMetadata_v4> rowGroups;
+
+    public ParquetFileMetadata_v4() {
+
+    }
+
+    public ParquetFileMetadata_v4(Path path, Long length, List<RowGroupMetadata_v4> rowGroups) {
+      this.path = path;
+      this.length = length;
+      this.rowGroups = rowGroups;
+    }
+
+    @Override
+    public String toString() {
+      return String.format(""path: %s rowGroups: %s"", path, rowGroups);
+    }
+
+    @JsonIgnore
+    @Override
+    public Path getPath() {
+      return path;
+    }
+
+    @JsonIgnore
+    @Override
+    public Long getLength() {
+      return length;
+    }
+
+    @JsonIgnore
+    @Override
+    public List<? extends RowGroupMetadata> getRowGroups() {
+      return rowGroups;
+    }
+  }
+
+
+  /**
+   * A struct that contains the metadata for a parquet row group
+   */
+  public static class RowGroupMetadata_v4 extends RowGroupMetadata {
+    @JsonProperty
+    public Long start;
+    @JsonProperty
+    public Long length;
+    @JsonProperty
+    public Long rowCount;
+    @JsonProperty
+    public Map<String, Float> hostAffinity;
+    @JsonProperty
+    public List<ColumnMetadata_v4> columns;
+
+    public RowGroupMetadata_v4() {
+    }
+
+    public RowGroupMetadata_v4(Long start, Long length, Long rowCount, Map<String, Float> hostAffinity,
+                               List<ColumnMetadata_v4> columns) {
+      this.start = start;
+      this.length = length;
+      this.rowCount = rowCount;
+      this.hostAffinity = hostAffinity;
+      this.columns = columns;
+    }
+
+    @Override
+    public Long getStart() {
+      return start;
+    }
+
+    @Override
+    public Long getLength() {
+      return length;
+    }
+
+    @Override
+    public Long getRowCount() {
+      return rowCount;
+    }
+
+    @Override
+    public Map<String, Float> getHostAffinity() {
+      return hostAffinity;
+    }
+
+    @Override
+    public List<? extends ColumnMetadata> getColumns() {
+      return columns;
+    }
+  }
+
+
+  public static class ColumnTypeMetadata_v4 {
+    @JsonProperty
+    public String[] name;
+    @JsonProperty
+    public PrimitiveType.PrimitiveTypeName primitiveType;
+    @JsonProperty
+    public OriginalType originalType;
+    @JsonProperty
+    public int precision;
+    @JsonProperty
+    public int scale;
+    @JsonProperty
+    public int repetitionLevel;
+    @JsonProperty
+    public int definitionLevel;
+    @JsonProperty
+    public long totalNullCount = 0;
+    @JsonProperty
+    public boolean isInteresting = false;
+
+    // Key to find by name only
+    @JsonIgnore
+    private Key key;
+
+    public ColumnTypeMetadata_v4() {
+    }
+
+    public ColumnTypeMetadata_v4(String[] name, PrimitiveType.PrimitiveTypeName primitiveType, OriginalType originalType, int precision, int scale, int repetitionLevel, int definitionLevel, long totalNullCount, boolean isInteresting) {
+      this.name = name;
+      this.primitiveType = primitiveType;
+      this.originalType = originalType;
+      this.precision = precision;
+      this.scale = scale;
+      this.repetitionLevel = repetitionLevel;
+      this.definitionLevel = definitionLevel;
+      this.key = new Key(name);
+      this.totalNullCount = totalNullCount;
+      this.isInteresting = isInteresting;
+    }
+
+    @JsonIgnore
+    private Key key() {
+      return this.key;
+    }
+
+    public static class Key {
+      private SchemaPath name;
+      private int hashCode = 0;
+
+      public Key(String[] name) {
+        this.name = SchemaPath.getCompoundPath(name);
+      }
+
+      public Key(SchemaPath name) {
+        this.name = new SchemaPath(name);
+      }
+
+      @Override
+      public int hashCode() {
+        if (hashCode == 0) {
+          hashCode = name.hashCode();
+        }
+        return hashCode;
+      }
+
+      @Override
+      public boolean equals(Object obj) {
+        if (obj == null) {
+          return false;
+        }
+        if (getClass() != obj.getClass()) {
+          return false;
+        }
+        final Key other = (Key) obj;
+        return this.name.equals(other.name);
+      }
+
+      @Override
+      public String toString() {
+        return name.toString();
+      }
+
+      public static class DeSerializer extends KeyDeserializer {
+
+        public DeSerializer() {
+        }
+
+        @Override
+        public Object deserializeKey(String key, com.fasterxml.jackson.databind.DeserializationContext ctxt) {
+          // key string should contain '`' char if the field was serialized as SchemaPath object
+          if (key.contains(""`"")) {
+            return new Key(SchemaPath.parseFromString(key));
+          }
+          return new Key(key.split(""\\.""));
+        }
+      }
+    }
+  }
+
+
+  /**
+   * A struct that contains the metadata for a column in a parquet file
+   */
+  public static class ColumnMetadata_v4 extends ColumnMetadata {
+    // Use a string array for name instead of Schema Path to make serialization easier
+    @JsonProperty
+    public String[] name;
+    @JsonProperty
+    public Long nulls;
+
+    public Object minValue;
+    public Object maxValue;
+
+    @JsonIgnore
+    private PrimitiveType.PrimitiveTypeName primitiveType;
+
+    public ColumnMetadata_v4() {
+    }
+
+    public ColumnMetadata_v4(String[] name, PrimitiveType.PrimitiveTypeName primitiveType, Object minValue, Object maxValue, Long nulls) {
+      this.name = name;
+      this.minValue = minValue;
+      this.maxValue = maxValue;
+      this.nulls = nulls;
+      this.primitiveType = primitiveType;
+    }
+
+    @JsonProperty(value = ""minValue"")
+    public void setMin(Object minValue) {
+      this.minValue = minValue;
+    }
+
+    @JsonProperty(value = ""maxValue"")
+    public void setMax(Object maxValue) {
+      this.maxValue = maxValue;
+    }
+
+    @Override
+    public String[] getName() {
+      return name;
+    }
+
+    @Override
+    public Long getNulls() {
+      return nulls;
+    }
+
+    /**
+     * Checks that the column chunk has a single value.
+     * Returns {@code true} if {@code minValue} and {@code maxValue} are the same but not null
+     * and nulls count is 0 or equal to the rows count.
+     * <p>
+     * Returns {@code true} if {@code minValue} and {@code maxValue} are null and the number of null values
+     * in the column chunk is equal to the rows count.
+     * <p>
+     * Comparison of nulls and rows count is needed for the cases:
+     * <ul>
+     * <li>column with primitive type has single value and null values</li>
+     *
+     * <li>column <b>with primitive type</b> has only null values, min/max couldn't be null,
+     * but column has single value</li>
+     * </ul>
+     *
+     * @param rowCount rows count in column chunk
+     * @return true if column has single value
+     */
+    @Override
+    public boolean hasSingleValue(long rowCount) {
+      if (isNumNullsSet()) {
+        if (minValue != null) {
+          // Objects.deepEquals() is used here, since min and max may be byte arrays
+          return (nulls == 0 || nulls == rowCount) && Objects.deepEquals(minValue, maxValue);
+        } else {
+          return nulls == rowCount && maxValue == null;
+        }
+      }
+      return false;
+    }
+
+    @Override
+    public Object getMinValue() {
+      return minValue;
+    }
+
+    @Override
+    public Object getMaxValue() {
+      return maxValue;
+    }
+
+    @Override
+    public PrimitiveType.PrimitiveTypeName getPrimitiveType() {
+      return null;
+    }
+
+    @Override
+    public OriginalType getOriginalType() {
+      return null;
+    }
+
+    // We use a custom serializer and write only non null values.
+    public static class Serializer extends JsonSerializer<ColumnMetadata_v4> {
+      @Override
+      public void serialize(ColumnMetadata_v4 value, JsonGenerator jgen, SerializerProvider provider) throws IOException {
+        jgen.writeStartObject();
+        jgen.writeArrayFieldStart(""name"");
+        for (String n : value.name) {
+          jgen.writeString(n);
+        }
+        jgen.writeEndArray();
+        if (value.minValue != null) {
+          Object val;
+          if (value.primitiveType == PrimitiveType.PrimitiveTypeName.BINARY
+                  || value.primitiveType == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {
+            val = ((Binary) value.minValue).getBytes();
+          } else {
+            val = value.minValue;
+          }
+          jgen.writeObjectField(""minValue"", val);
+        }
+        if (value.maxValue != null) {
+          Object val;
+          if (value.primitiveType == PrimitiveType.PrimitiveTypeName.BINARY
+                  || value.primitiveType == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {
+            val = ((Binary) value.maxValue).getBytes();
+          } else {
+            val = value.maxValue;
+          }
+          jgen.writeObjectField(""maxValue"", val);
+        }
+
+        if (value.nulls != null) {
+          jgen.writeObjectField(""nulls"", value.nulls);
+        }
+        jgen.writeEndObject();
+      }
+    }
+  }
+
+  @JsonTypeName(V4)
+  public static class MetadataSummary {","[{'comment': 'Please post the examples of content for new metadata cache files to the Jira description', 'commenter': 'vdiravka'}]"
1723,exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java,"@@ -465,4 +469,24 @@ private static String getPrefixJoinOrderFromPlan(String plan, String joinKeyWord
 
     return builder.toString();
   }
+
+  /**
+   * Create a temp metadata directory to query the metadata summary cache file
+   * @param table table name or table path
+   */
+  public static void createMetadataDir(String table) throws IOException {
+    final String tmpDir;
+    try {
+      tmpDir = dirTestWatcher.getRootDir().getCanonicalPath();
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+    File metadataDir = dirTestWatcher.makeRootSubDir(Paths.get(tmpDir+""/""+table+""/metadataDir""));
+    File metaFile, newFile;
+    metaFile = table.startsWith(tmpDir) ? FileUtils.getFile(table, Metadata.METADATA_SUMMARY_FILENAME)
+            : FileUtils.getFile(tmpDir, table, Metadata.METADATA_SUMMARY_FILENAME);
+    newFile = new File(tmpDir+""/""+table+""/summary_meta.json"");","[{'comment': '```suggestion\r\n    newFile = new File(tmpDir + ""/"" + table + ""/summary_meta.json"");\r\n```', 'commenter': 'vdiravka'}, {'comment': 'FileSeparator is different on different OS', 'commenter': 'vdiravka'}, {'comment': 'Changed', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java,"@@ -17,6 +17,7 @@
  */
 package org.apache.drill.exec.store.parquet;
 
+import static java.lang.Thread.sleep;","[{'comment': 'No need to import static `sleep` method. There are a lot of similar usages in Drill code and class usage is better for visibility.', 'commenter': 'vdiravka'}, {'comment': 'Changed to use Thread.sleep()', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -91,9 +95,14 @@
 public class Metadata {
   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Metadata.class);
 
-  public static final String[] OLD_METADATA_FILENAMES = {"".drill.parquet_metadata.v2""};
+  public static final String[] OLD_METADATA_FILENAMES = {"".drill.parquet_metadata"", "".drill.parquet_metadata.v2""};
   public static final String METADATA_FILENAME = "".drill.parquet_metadata"";
   public static final String METADATA_DIRECTORIES_FILENAME = "".drill.parquet_metadata_directories"";
+  public static final String FILE_METADATA_FILENAME = "".drill.parquet_file_metadata.v4"";
+  public static final String METADATA_SUMMARY_FILENAME = "".drill.parquet_summary_metadata.v4"";
+  public static final String[] CURRENT_METADATA_FILENAMES = {METADATA_SUMMARY_FILENAME, FILE_METADATA_FILENAME};","[{'comment': 'What about `METADATA_DIRECTORIES_FILENAME`? Is it current metadata fileName?', 'commenter': 'vdiravka'}, {'comment': 'METADATA_DIRECTORIES_FILENAME is same across all the versions. CURRENT_METADATA_FILENAMES contains the newer version cache files. In this case summary and file metadata cache file names', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -91,9 +95,14 @@
 public class Metadata {
   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Metadata.class);
 
-  public static final String[] OLD_METADATA_FILENAMES = {"".drill.parquet_metadata.v2""};
+  public static final String[] OLD_METADATA_FILENAMES = {"".drill.parquet_metadata"", "".drill.parquet_metadata.v2""};
   public static final String METADATA_FILENAME = "".drill.parquet_metadata"";
   public static final String METADATA_DIRECTORIES_FILENAME = "".drill.parquet_metadata_directories"";
+  public static final String FILE_METADATA_FILENAME = "".drill.parquet_file_metadata.v4"";","[{'comment': 'Looks like `METADATA_FILENAME` should be some sort of `OLD_METADATA_FILENAME`\r\nbut `FILE_METADATA_FILENAME` -> `METADATA_FILENAME`', 'commenter': 'vdiravka'}, {'comment': 'Changed.', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -633,43 +713,120 @@ private void readBlockMeta(Path path, boolean dirsOnly, MetadataContext metaCont
         parquetTableMetadataDirs.updateRelativePaths(metadataParentDirPath);
         if (!alreadyCheckedModification && tableModified(parquetTableMetadataDirs.getDirectories(), path, metadataParentDir, metaContext, fs)) {
           parquetTableMetadataDirs =
-              (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null)).getRight();
+              (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null, true)).getRight();
           newMetadata = true;
         }
       } else {
-        parquetTableMetadata = mapper.readValue(is, ParquetTableMetadataBase.class);
+        if (isFileMetadata) {
+          parquetTableMetadata.assignFiles((mapper.readValue(is, FileMetadata.class)).getFiles());
+          if (new MetadataVersion(parquetTableMetadata.getMetadataVersion()).compareTo(new MetadataVersion(4, 0)) >= 0) {
+            ((ParquetTableMetadata_v4) parquetTableMetadata).updateRelativePaths(metadataParentDirPath);
+          }
+
+          if (!alreadyCheckedModification && tableModified(parquetTableMetadata.getDirectories(), path, metadataParentDir, metaContext, fs)) {
+            parquetTableMetadata =
+                    (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null, true)).getLeft();
+            newMetadata = true;
+          }
+        } else if (isSummaryFile) {
+          MetadataSummary metadataSummary = mapper.readValue(is, Metadata_V4.MetadataSummary.class);
+          ParquetTableMetadata_v4 parquetTableMetadata_v4 = new ParquetTableMetadata_v4(metadataSummary);
+          parquetTableMetadata = (ParquetTableMetadataBase) parquetTableMetadata_v4;
+        } else {
+          parquetTableMetadata = mapper.readValue(is, ParquetTableMetadataBase.class);
+          if (new MetadataVersion(parquetTableMetadata.getMetadataVersion()).compareTo(new MetadataVersion(3, 0)) >= 0) {
+            ((Metadata_V3.ParquetTableMetadata_v3) parquetTableMetadata).updateRelativePaths(metadataParentDirPath);
+          }
+          if (!alreadyCheckedModification && tableModified((parquetTableMetadata.getDirectories()), path, metadataParentDir, metaContext, fs)) {
+            parquetTableMetadata =
+                    (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null, true)).getLeft();
+            newMetadata = true;
+          }
+        }
         if (timer != null) {
           logger.debug(""Took {} ms to read metadata from cache file"", timer.elapsed(TimeUnit.MILLISECONDS));
           timer.stop();
         }
-        if (new MetadataVersion(parquetTableMetadata.getMetadataVersion()).compareTo(new MetadataVersion(3, 0)) >= 0) {
-          ((ParquetTableMetadata_v3) parquetTableMetadata).updateRelativePaths(metadataParentDirPath);
-        }
-          if (!alreadyCheckedModification && tableModified(parquetTableMetadata.getDirectories(), path, metadataParentDir, metaContext, fs)) {
-          // TODO change with current columns in existing metadata (auto refresh feature)
-          parquetTableMetadata =
-              (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null)).getLeft();
-          newMetadata = true;
+        if (!isSummaryFile) {
+          // DRILL-5009: Remove the RowGroup if it is empty
+          List<? extends ParquetFileMetadata> files = parquetTableMetadata.getFiles();
+          if (files != null) {
+            for (ParquetFileMetadata file : files) {
+              List<? extends RowGroupMetadata> rowGroups = file.getRowGroups();
+              rowGroups.removeIf(r -> r.getRowCount() == 0);
+            }
+          }
         }
-
-        // DRILL-5009: Remove the RowGroup if it is empty
-        List<? extends ParquetFileMetadata> files = parquetTableMetadata.getFiles();
-        for (ParquetFileMetadata file : files) {
-          List<? extends RowGroupMetadata> rowGroups = file.getRowGroups();
-          rowGroups.removeIf(r -> r.getRowCount() == 0);
+        if (newMetadata) {
+          // if new metadata files were created, invalidate the existing metadata context
+          metaContext.clear();
         }
-
-      }
-      if (newMetadata) {
-        // if new metadata files were created, invalidate the existing metadata context
-        metaContext.clear();
       }
     } catch (IOException e) {
       logger.error(""Failed to read '{}' metadata file"", path, e);
       metaContext.setMetadataCacheCorrupted(true);
     }
   }
 
+  private Set<String> getInterestingColumns(FileSystem fs, Path metadataParentDir, boolean autoRefreshTriggered) {
+    Metadata_V4.MetadataSummary metadataSummary = getSummary(fs, metadataParentDir, autoRefreshTriggered, null);
+    if (metadataSummary == null) {
+      return null;
+    } else {
+      Set<String> interestingColumns = new HashSet<String>();
+      for (ColumnTypeMetadata_v4 columnTypeMetadata_v4: metadataSummary.columnTypeInfo.values()) {
+        if (columnTypeMetadata_v4.isInteresting) {
+          interestingColumns.add(String.join("""", columnTypeMetadata_v4.name));
+        }
+      }
+      return interestingColumns;
+    }
+  }
+
+  private boolean getallColumnsInteresting(FileSystem fs, Path metadataParentDir, boolean autoRefreshTriggered) {
+    Metadata_V4.MetadataSummary metadataSummary = getSummary(fs, metadataParentDir, autoRefreshTriggered, null);
+    if (metadataSummary == null) {
+      return true;
+    }
+    return metadataSummary.isAllColumnsInteresting();
+  }
+
+  public static Metadata_V4.MetadataSummary getSummary(FileSystem fs, Path metadataParentDir, boolean autoRefreshTriggered, ParquetReaderConfig readerConfig) {
+    Path summaryFile = new Path(metadataParentDir, METADATA_SUMMARY_FILENAME);
+    Path metadataDirFile = new Path(metadataParentDir, METADATA_DIRECTORIES_FILENAME);
+    MetadataContext metaContext = new MetadataContext();
+    try {
+      if (!fs.exists(summaryFile)) {
+        return null;
+      } else {
+        // If the autorefresh is not triggered, check if the cache file is stale and trigger auto-refresh
+        if (!autoRefreshTriggered) {
+          Metadata metadata = new Metadata(readerConfig);
+          ParquetTableMetadataDirs metadataDirs  = readMetadataDirs(fs, metadataDirFile, metaContext, readerConfig);
+          if (metadata.tableModified(metadataDirs.getDirectories(), summaryFile, metadataParentDir, metaContext, fs) && true) {
+            ParquetTableMetadata_v4 parquetTableMetadata = (metadata.createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(summaryFile.getParent()), fs, true, null, true)).getLeft();
+            return parquetTableMetadata.getSummary();
+          }
+        }
+        // Read the existing metadataSummary cache file to get the metadataSummary
+        ObjectMapper mapper = new ObjectMapper();
+        final SimpleModule serialModule = new SimpleModule();
+        serialModule.addDeserializer(SchemaPath.class, new SchemaPath.De());
+        serialModule.addKeyDeserializer(ColumnTypeMetadata_v4.Key.class, new ColumnTypeMetadata_v4.Key.DeSerializer());
+        AfterburnerModule module = new AfterburnerModule();
+        module.setUseOptimizedBeanDeserializer(true);
+        mapper.registerModule(serialModule);
+        mapper.registerModule(module);
+        mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
+        InputStream is = fs.open(summaryFile);
+        Metadata_V4.MetadataSummary metadataSummary = mapper.readValue(is, Metadata_V4.MetadataSummary.class);
+        return metadataSummary;
+        }
+    } catch (IOException e) {
+      return null;","[{'comment': 'How the case with null is handled?\r\nIt is necessary to print the info from `e` to logs possibly', 'commenter': 'vdiravka'}, {'comment': 'Added logging.', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -633,43 +713,120 @@ private void readBlockMeta(Path path, boolean dirsOnly, MetadataContext metaCont
         parquetTableMetadataDirs.updateRelativePaths(metadataParentDirPath);
         if (!alreadyCheckedModification && tableModified(parquetTableMetadataDirs.getDirectories(), path, metadataParentDir, metaContext, fs)) {
           parquetTableMetadataDirs =
-              (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null)).getRight();
+              (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null, true)).getRight();
           newMetadata = true;
         }
       } else {
-        parquetTableMetadata = mapper.readValue(is, ParquetTableMetadataBase.class);
+        if (isFileMetadata) {
+          parquetTableMetadata.assignFiles((mapper.readValue(is, FileMetadata.class)).getFiles());
+          if (new MetadataVersion(parquetTableMetadata.getMetadataVersion()).compareTo(new MetadataVersion(4, 0)) >= 0) {
+            ((ParquetTableMetadata_v4) parquetTableMetadata).updateRelativePaths(metadataParentDirPath);
+          }
+
+          if (!alreadyCheckedModification && tableModified(parquetTableMetadata.getDirectories(), path, metadataParentDir, metaContext, fs)) {
+            parquetTableMetadata =
+                    (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null, true)).getLeft();
+            newMetadata = true;
+          }
+        } else if (isSummaryFile) {
+          MetadataSummary metadataSummary = mapper.readValue(is, Metadata_V4.MetadataSummary.class);
+          ParquetTableMetadata_v4 parquetTableMetadata_v4 = new ParquetTableMetadata_v4(metadataSummary);
+          parquetTableMetadata = (ParquetTableMetadataBase) parquetTableMetadata_v4;
+        } else {
+          parquetTableMetadata = mapper.readValue(is, ParquetTableMetadataBase.class);
+          if (new MetadataVersion(parquetTableMetadata.getMetadataVersion()).compareTo(new MetadataVersion(3, 0)) >= 0) {
+            ((Metadata_V3.ParquetTableMetadata_v3) parquetTableMetadata).updateRelativePaths(metadataParentDirPath);
+          }
+          if (!alreadyCheckedModification && tableModified((parquetTableMetadata.getDirectories()), path, metadataParentDir, metaContext, fs)) {
+            parquetTableMetadata =
+                    (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null, true)).getLeft();
+            newMetadata = true;
+          }
+        }
         if (timer != null) {
           logger.debug(""Took {} ms to read metadata from cache file"", timer.elapsed(TimeUnit.MILLISECONDS));
           timer.stop();
         }
-        if (new MetadataVersion(parquetTableMetadata.getMetadataVersion()).compareTo(new MetadataVersion(3, 0)) >= 0) {
-          ((ParquetTableMetadata_v3) parquetTableMetadata).updateRelativePaths(metadataParentDirPath);
-        }
-          if (!alreadyCheckedModification && tableModified(parquetTableMetadata.getDirectories(), path, metadataParentDir, metaContext, fs)) {
-          // TODO change with current columns in existing metadata (auto refresh feature)
-          parquetTableMetadata =
-              (createMetaFilesRecursivelyAsProcessUser(Path.getPathWithoutSchemeAndAuthority(path.getParent()), fs, true, null)).getLeft();
-          newMetadata = true;
+        if (!isSummaryFile) {
+          // DRILL-5009: Remove the RowGroup if it is empty
+          List<? extends ParquetFileMetadata> files = parquetTableMetadata.getFiles();
+          if (files != null) {
+            for (ParquetFileMetadata file : files) {
+              List<? extends RowGroupMetadata> rowGroups = file.getRowGroups();
+              rowGroups.removeIf(r -> r.getRowCount() == 0);
+            }
+          }
         }
-
-        // DRILL-5009: Remove the RowGroup if it is empty
-        List<? extends ParquetFileMetadata> files = parquetTableMetadata.getFiles();
-        for (ParquetFileMetadata file : files) {
-          List<? extends RowGroupMetadata> rowGroups = file.getRowGroups();
-          rowGroups.removeIf(r -> r.getRowCount() == 0);
+        if (newMetadata) {
+          // if new metadata files were created, invalidate the existing metadata context
+          metaContext.clear();
         }
-
-      }
-      if (newMetadata) {
-        // if new metadata files were created, invalidate the existing metadata context
-        metaContext.clear();
       }
     } catch (IOException e) {
       logger.error(""Failed to read '{}' metadata file"", path, e);
       metaContext.setMetadataCacheCorrupted(true);
     }
   }
 
+  private Set<String> getInterestingColumns(FileSystem fs, Path metadataParentDir, boolean autoRefreshTriggered) {
+    Metadata_V4.MetadataSummary metadataSummary = getSummary(fs, metadataParentDir, autoRefreshTriggered, null);
+    if (metadataSummary == null) {
+      return null;
+    } else {
+      Set<String> interestingColumns = new HashSet<String>();
+      for (ColumnTypeMetadata_v4 columnTypeMetadata_v4: metadataSummary.columnTypeInfo.values()) {
+        if (columnTypeMetadata_v4.isInteresting) {
+          interestingColumns.add(String.join("""", columnTypeMetadata_v4.name));
+        }
+      }
+      return interestingColumns;
+    }
+  }
+
+  private boolean getallColumnsInteresting(FileSystem fs, Path metadataParentDir, boolean autoRefreshTriggered) {
+    Metadata_V4.MetadataSummary metadataSummary = getSummary(fs, metadataParentDir, autoRefreshTriggered, null);
+    if (metadataSummary == null) {
+      return true;
+    }
+    return metadataSummary.isAllColumnsInteresting();
+  }
+
+  public static Metadata_V4.MetadataSummary getSummary(FileSystem fs, Path metadataParentDir, boolean autoRefreshTriggered, ParquetReaderConfig readerConfig) {
+    Path summaryFile = new Path(metadataParentDir, METADATA_SUMMARY_FILENAME);
+    Path metadataDirFile = new Path(metadataParentDir, METADATA_DIRECTORIES_FILENAME);
+    MetadataContext metaContext = new MetadataContext();
+    try {
+      if (!fs.exists(summaryFile)) {
+        return null;","[{'comment': 'logger', 'commenter': 'vdiravka'}, {'comment': 'Added logging.', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java,"@@ -712,5 +869,4 @@ private boolean logAndStopTimer(boolean isModified, String directoryName,
     return isModified;
   }
 
-}
-
+}","[{'comment': '```suggestion\r\n}\r\n\r\n```', 'commenter': 'vdiravka'}, {'comment': 'Changed', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/MetadataPathUtils.java,"@@ -60,6 +61,22 @@
    * @param baseDir base parent directory
    * @return list of files with absolute paths
    */
+  public static List<ParquetFileMetadata_v4> convertToFilesWithAbsolutePathsForV4(","[{'comment': 'Please modify existing method `convertToFilesWithAbsolutePaths` for using with `ParquetFileMetadata_v4` and `ParquetFileMetadata_v3` to avoid duplicating the code.', 'commenter': 'vdiravka'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata_V1.java,"@@ -90,6 +90,16 @@ public Integer getDefinitionLevel(String[] columnName) {
       return null;
     }
 
+    @Override","[{'comment': '`@JsonIgnore` ?', 'commenter': 'vdiravka'}]"
1723,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata_V4.java,"@@ -0,0 +1,648 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.metadata;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.KeyDeserializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
+import org.apache.drill.common.expression.SchemaPath;
+
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ColumnMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ParquetFileMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.ParquetTableMetadataBase;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataBase.RowGroupMetadata;
+import static org.apache.drill.exec.store.parquet.metadata.MetadataVersion.Constants.V4;
+import org.apache.hadoop.fs.Path;
+import org.apache.parquet.io.api.Binary;
+import org.apache.parquet.schema.OriginalType;
+import org.apache.parquet.schema.PrimitiveType;
+
+public class Metadata_V4 {","[{'comment': 'Please check all methods whether they are serializable or not.', 'commenter': 'vdiravka'}]"
1723,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java,"@@ -956,4 +957,302 @@ public void testRefreshNone() throws Exception {
     int actualRowCount = testSql(query);
     assertEquals(expectedRowCount, actualRowCount);
   }
+
+  @Test
+  public void testTotalRowCount() throws Exception {
+    String tableName = ""nation_ctas_rowcount"";
+    test(""use dfs"");
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+    long rowCount = testSql(""select * from `nation_ctas_rowcount`"");
+    test(""refresh table metadata %s"", tableName);
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(rowCount)
+            .go();
+  }
+
+  @Test
+  public void testTotalRowCountPerFile() throws Exception {
+    String tableName = ""nation_ctas_rowcount1"";
+    test(""use dfs"");
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+    long rowCount = testSql(""select * from `nation_ctas_rowcount1/t1`"");
+    test(""refresh table metadata %s"", tableName);
+    tableName = tableName + ""/t1"";
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(rowCount)
+            .go();
+  }
+
+
+  @Test
+  public void testTotalRowCountAddDirectory() throws Exception {
+    String tableName = ""nation_ctas_rowcount2"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    test(""refresh table metadata %s"", tableName);
+    sleep(1000);
+    test(""create table `%s/t5` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    String query = String.format(""select count(*) as count from `%s`"", tableName);
+    String rowCountQuery = String.format(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName);
+
+    testBuilder()
+            .sqlQuery(query)
+            .unOrdered()
+            .baselineColumns(""count"")
+            .baselineValues(125L)
+            .go();
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(125L)
+            .go();
+  }
+
+
+  @Test
+  public void testTotalRowCountAddSubDir() throws Exception {
+    String tableName = ""nation_ctas_rowcount3"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    test(""refresh table metadata %s"", tableName);
+    sleep(1000);
+    tableName = tableName + ""/t1"";
+    test(""create table `%s/t5` as select * from cp.`tpch/nation.parquet`"", tableName);
+    String query = String.format(""select count(*) as count from `%s`"", tableName);
+    String rowCountQuery = String.format(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    testBuilder()
+            .sqlQuery(query)
+            .unOrdered()
+            .baselineColumns(""count"")
+            .baselineValues(50L)
+            .go();
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(50L)
+            .go();
+  }
+
+  @Test
+  public void testTotalRowCountAddSubDir1() throws Exception {
+    String tableName = ""nation_ctas_rowcount4"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    test(""refresh table metadata %s"", tableName);
+    sleep(1000);
+    tableName = tableName + ""/t1"";
+
+    test(""create table `%s/t5` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    String query = String.format(""select count(*) as count from `%s`"", tableName);
+    String rowCountQuery = String.format(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    testBuilder()
+            .sqlQuery(query)
+            .unOrdered()
+            .baselineColumns(""count"")
+            .baselineValues(50L)
+            .go();
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(50L)
+            .go();
+  }
+
+  @Ignore
+  @Test
+  public void testAutoRefreshRowCount() throws Exception {
+    String tableName = ""orders_ctas_refresh5"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""refresh table metadata COLUMNS (o_orderdate) %s"", tableName);
+    sleep(1000);
+
+    tableName = tableName + ""/t1"";
+    String query = String.format(""select count(*) as count from `%s`"", tableName);
+    String rowCountQuery = String.format(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    dirTestWatcher.copyResourceToRoot(Paths.get(""multilevel/parquet/1994/Q1/orders_94_q1.parquet""), Paths.get(""orders_ctas_refresh5/t1/q1.parquet""));
+
+    testBuilder()
+            .sqlQuery(query)
+            .unOrdered()
+            .baselineColumns(""count"")
+            .baselineValues(100L)
+            .go();
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(100L)
+            .go();
+  }
+
+  @Test
+  public void testRefreshWithInterestingColumn() throws Exception {
+    String tableName = ""orders_ctas_refresh"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/orders.parquet`"", tableName);
+
+    test(""refresh table metadata COLUMNS (o_orderdate) %s"", tableName);
+    sleep(1000);
+
+    String rowCountQuery = String.format(""select t.allColumnsInteresting as allColumnsInteresting from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""allColumnsInteresting"")
+            .baselineValues(false)
+            .go();
+  }
+
+  @Test
+  public void testDefaultRefresh() throws Exception {
+    String tableName = ""orders_ctas_refresh1"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/orders.parquet`"", tableName);
+
+    test(""refresh table metadata %s"", tableName);
+    sleep(1000);
+
+    String rowCountQuery = String.format(""select t.allColumnsInteresting as allColumnsInteresting from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""allColumnsInteresting"")
+            .baselineValues(true)
+            .go();
+  }
+
+  @Test
+  public void testAutoRefreshWithInterestingColumn() throws Exception {
+    String tableName = ""orders_ctas_refresh2"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""refresh table metadata COLUMNS (o_orderdate) %s"", tableName);
+    sleep(1000);
+    test(""create table `%s/t5` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""Select count(*) from `%s`"", tableName);
+    tableName = tableName + ""/t5"";
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    String rowCountQuery = String.format(""select t.allColumnsInteresting as allColumnsInteresting from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""allColumnsInteresting"")
+            .baselineValues(false)
+            .go();
+  }
+
+
+  @Test
+  public void testAutoRefreshWithInterestingColumnFile() throws Exception {
+    String tableName = ""orders_ctas_refresh3"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""refresh table metadata COLUMNS (o_orderdate) %s"", tableName);
+    sleep(1000);
+    test(""create table `%s/t5` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""Select count(*) from `%s`"", tableName);
+    tableName = tableName + ""/t5"";
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    dirTestWatcher.copyResourceToRoot(Paths.get(""multilevel/parquet/1994/Q1/orders_94_q1.parquet""), Paths.get(""orders_ctas_refresh3/t5/q1.parquet""));
+    String rowCountQuery = String.format(""select t.allColumnsInteresting as allColumnsInteresting from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""allColumnsInteresting"")
+            .baselineValues(false)
+            .go();
+  }
+
+
+  @Test
+  public void testRefreshWithIsNull() throws Exception {
+    String tableName = ""orders_ctas_refresh_not_null"";
+    test(""use dfs"");
+    test(""create table `%s/t4` as select * from cp.`tpch/orders.parquet`"", tableName);
+    test(""refresh table metadata COLUMNS (o_orderdate) %s"", tableName);
+    String query = String.format(""Select count(*) as cnt from `%s` where o_orderpriority is not null"", tableName);
+
+    checkForMetadataFile(tableName);
+
+    testBuilder()
+            .sqlQuery(query)","[{'comment': ""4 spaces indent is used in Drill code for builder's chain. "", 'commenter': 'vdiravka'}, {'comment': 'Changed.', 'commenter': 'dvjyothsna'}]"
1723,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java,"@@ -956,4 +957,302 @@ public void testRefreshNone() throws Exception {
     int actualRowCount = testSql(query);
     assertEquals(expectedRowCount, actualRowCount);
   }
+
+  @Test
+  public void testTotalRowCount() throws Exception {
+    String tableName = ""nation_ctas_rowcount"";
+    test(""use dfs"");
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+    long rowCount = testSql(""select * from `nation_ctas_rowcount`"");
+    test(""refresh table metadata %s"", tableName);
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(rowCount)
+            .go();
+  }
+
+  @Test
+  public void testTotalRowCountPerFile() throws Exception {
+    String tableName = ""nation_ctas_rowcount1"";
+    test(""use dfs"");
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+    long rowCount = testSql(""select * from `nation_ctas_rowcount1/t1`"");
+    test(""refresh table metadata %s"", tableName);
+    tableName = tableName + ""/t1"";
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(rowCount)
+            .go();
+  }
+
+
+  @Test
+  public void testTotalRowCountAddDirectory() throws Exception {
+    String tableName = ""nation_ctas_rowcount2"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    test(""refresh table metadata %s"", tableName);
+    sleep(1000);
+    test(""create table `%s/t5` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    String query = String.format(""select count(*) as count from `%s`"", tableName);
+    String rowCountQuery = String.format(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName);
+
+    testBuilder()
+            .sqlQuery(query)
+            .unOrdered()
+            .baselineColumns(""count"")
+            .baselineValues(125L)
+            .go();
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(125L)
+            .go();
+  }
+
+
+  @Test
+  public void testTotalRowCountAddSubDir() throws Exception {
+    String tableName = ""nation_ctas_rowcount3"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    test(""refresh table metadata %s"", tableName);
+    sleep(1000);
+    tableName = tableName + ""/t1"";
+    test(""create table `%s/t5` as select * from cp.`tpch/nation.parquet`"", tableName);
+    String query = String.format(""select count(*) as count from `%s`"", tableName);
+    String rowCountQuery = String.format(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    testBuilder()
+            .sqlQuery(query)
+            .unOrdered()
+            .baselineColumns(""count"")
+            .baselineValues(50L)
+            .go();
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(50L)
+            .go();
+  }
+
+  @Test
+  public void testTotalRowCountAddSubDir1() throws Exception {
+    String tableName = ""nation_ctas_rowcount4"";
+    test(""use dfs"");
+
+    test(""create table `%s/t1` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t2` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t3` as select * from cp.`tpch/nation.parquet`"", tableName);
+    test(""create table `%s/t4` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    test(""refresh table metadata %s"", tableName);
+    sleep(1000);
+    tableName = tableName + ""/t1"";
+
+    test(""create table `%s/t5` as select * from cp.`tpch/nation.parquet`"", tableName);
+
+    String query = String.format(""select count(*) as count from `%s`"", tableName);
+    String rowCountQuery = String.format(""select t.totalRowCount as rowCount from `%s/metadataDir/summary_meta.json` as t"", tableName);
+    testBuilder()
+            .sqlQuery(query)
+            .unOrdered()
+            .baselineColumns(""count"")
+            .baselineValues(50L)
+            .go();
+    checkForMetadataFile(tableName);
+    createMetadataDir(tableName);
+    testBuilder()
+            .sqlQuery(rowCountQuery)
+            .unOrdered()
+            .baselineColumns(""rowCount"")
+            .baselineValues(50L)
+            .go();
+  }
+
+  @Ignore","[{'comment': 'Why it is disabled?', 'commenter': 'vdiravka'}]"
1731,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/FilterEvaluatorUtils.java,"@@ -56,7 +56,8 @@ private FilterEvaluatorUtils() {
 
   public static RowsMatch evalFilter(LogicalExpression expr, MetadataBase.ParquetTableMetadataBase footer,
                                      int rowGroupIndex, OptionManager options, FragmentContext fragmentContext) {
-    List<SchemaPath> schemaPathsInExpr = new ArrayList<>(expr.accept(new FieldReferenceFinder(), null));
+    List<SchemaPath> schemaPathsInExpr = new ArrayList<>(expr.<Set<SchemaPath>,","[{'comment': '@cgivre, as discussed in mail thread, could you please add a comment which explains why this change is required with the link to JDK issue which causes a compilation error.\r\n\r\nAlso, as discussed in mail thread, please add suppression to avoid warnings in IDE.', 'commenter': 'vvysotskyi'}, {'comment': ""I'm not sure which JDK issue references this specific problem, but I put a link to the one from the email chain. "", 'commenter': 'cgivre'}]"
1731,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/FilterEvaluatorUtils.java,"@@ -54,9 +54,12 @@
   private FilterEvaluatorUtils() {
   }
 
+  @SuppressWarnings(""unchecked"")","[{'comment': 'Please change it to `RedundantTypeArguments`, since current suppression does not help.\r\n```suggestion\r\n  @SuppressWarnings(""RedundantTypeArguments"")\r\n```', 'commenter': 'vvysotskyi'}]"
1731,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/FilterEvaluatorUtils.java,"@@ -54,9 +54,12 @@
   private FilterEvaluatorUtils() {
   }
 
+  @SuppressWarnings(""unchecked"")
   public static RowsMatch evalFilter(LogicalExpression expr, MetadataBase.ParquetTableMetadataBase footer,
                                      int rowGroupIndex, OptionManager options, FragmentContext fragmentContext) {
-    List<SchemaPath> schemaPathsInExpr = new ArrayList<>(expr.accept(new FieldReferenceFinder(), null));
+    //This change is necessary to prevent build errors on JDK 1.8.0_65.  See (https://bugs.openjdk.java.net/browse/JDK-8066974)","[{'comment': 'Please rephrase it to:\r\n```suggestion\r\n    // Specifies type arguments explicitly to avoid compilation error caused by JDK-8066974\r\n```', 'commenter': 'vvysotskyi'}]"
1731,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/FilterEvaluatorUtils.java,"@@ -54,9 +54,12 @@
   private FilterEvaluatorUtils() {
   }
 
+  @SuppressWarnings(""unchecked"")
   public static RowsMatch evalFilter(LogicalExpression expr, MetadataBase.ParquetTableMetadataBase footer,
                                      int rowGroupIndex, OptionManager options, FragmentContext fragmentContext) {
-    List<SchemaPath> schemaPathsInExpr = new ArrayList<>(expr.accept(new FieldReferenceFinder(), null));
+    //This change is necessary to prevent build errors on JDK 1.8.0_65.  See (https://bugs.openjdk.java.net/browse/JDK-8066974)
+    List<SchemaPath> schemaPathsInExpr = new ArrayList<>(expr.<Set<SchemaPath>,
+            Void, RuntimeException>accept(new FieldReferenceFinder(), null));","[{'comment': 'Please change these two lines to specify all type arguments in the same line, for example like there:\r\n```\r\nList<SchemaPath> schemaPathsInExpr = new ArrayList<>(\r\n        expr.<Set<SchemaPath>, Void, RuntimeException>accept(new FieldReferenceFinder(), null));\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Done and commits squashed.', 'commenter': 'cgivre'}]"
1731,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/FilterEvaluatorUtils.java,"@@ -54,9 +54,12 @@
   private FilterEvaluatorUtils() {
   }
 
+  @SuppressWarnings(""RedundantTypeArguments"")
   public static RowsMatch evalFilter(LogicalExpression expr, MetadataBase.ParquetTableMetadataBase footer,
                                      int rowGroupIndex, OptionManager options, FragmentContext fragmentContext) {
-    List<SchemaPath> schemaPathsInExpr = new ArrayList<>(expr.accept(new FieldReferenceFinder(), null));
+    // Specifies type arguments explicitly to avoid compilation error caused by JDK-8066974
+    List<SchemaPath> schemaPathsInExpr = new ArrayList<>(
+            expr.<Set<SchemaPath>, Void, RuntimeException>accept(new FieldReferenceFinder(),null));","[{'comment': 'Please add space back:\r\n```suggestion\r\n            expr.<Set<SchemaPath>, Void, RuntimeException>accept(new FieldReferenceFinder(), null));\r\n```', 'commenter': 'vvysotskyi'}]"
1778,contrib/format-hdf5/README.md,"@@ -0,0 +1,128 @@
+# Drill HDF5 Format Plugin","[{'comment': ""Thanks much for this wonderful writeup. Really helps to understand what's happening."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5Attribute.java,"@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.hdf5;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+
+public class HDF5Attribute {
+  private TypeProtos.MinorType dataType;
+  private String key;
+  private Object value;","[{'comment': 'All `final` since set in constructor.', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5Attribute.java,"@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.hdf5;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+
+public class HDF5Attribute {
+  private TypeProtos.MinorType dataType;
+  private String key;
+  private Object value;
+
+  public HDF5Attribute(TypeProtos.MinorType type, String key, Object value){
+    this.dataType = type;
+    this.key = key;
+    this.value = value;
+  }
+
+  public MinorType getDataType(){ return dataType; }
+  public void setDataType( MinorType type ){ this.dataType = type;}","[{'comment': 'Would be cleaner if were item is immutable: no setter.', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5Attribute.java,"@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.hdf5;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+
+public class HDF5Attribute {
+  private TypeProtos.MinorType dataType;
+  private String key;
+  private Object value;
+
+  public HDF5Attribute(TypeProtos.MinorType type, String key, Object value){
+    this.dataType = type;
+    this.key = key;
+    this.value = value;
+  }
+
+  public MinorType getDataType(){ return dataType; }
+  public void setDataType( MinorType type ){ this.dataType = type;}
+
+  public String getKey() { return key; }
+  public void setKey(String key){ this.key = key;}
+
+  public Object getValue() { return value;}
+  public void setValue(Object v) { this.value = v;}
+
+  @Override
+  public String toString() {
+    return getKey() + "":"" + getValue().toString() + "" type:"" + getDataType().toString();","[{'comment': 'Nit: the modern way to do this is:\r\n\r\n```\r\n  return String.format(""%s: %s type: %s"", getKey(), getValue(), getDataType());\r\n```', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;","[{'comment': '`final` since set in constructor', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;","[{'comment': '`final` since set in constructor', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;","[{'comment': 'All `final` since they should be immutable. Is `schema` used?', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();","[{'comment': '`loader` is set twice. `build()` is not meant to be called twice.', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);","[{'comment': '`this.` not really needed', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);","[{'comment': ""close `in` since we're failing. May have to check if `in` is null before closing."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();","[{'comment': ""This is less than ideal as multiple queries might produce the same file, and the files are written in whatever the working directory happens to be. Look at the spilling code to see how we create temp files elsewhere. See `SpillSet` for the details.\r\n\r\nYou'll probably want to add a config variable to point to the temp file system. Can't use the existing `SPILL_DIRS` because that can point to an HDFS/MFS file system. You need a guaranteed local location. Might be able to use `drill.tmp-dir`."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);","[{'comment': ""Make the member variable be this one, the `File` rather than the string. You'll need the `File` later to delete the file."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);","[{'comment': 'Is this guaranteed to delete the file if the copy fails? Do we need a try/catch block to ensure we delete the file?', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {","[{'comment': 'Nit: `this.` is not needed.', 'commenter': 'paul-rogers'}, {'comment': 'We make the two-part `if` check for every row. Seems inefficient. should this be done once?', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);","[{'comment': ""I don't follow. We are in a loop for every row. But, we return immediately on the first row..."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());","[{'comment': 'This is inefficient. Better to add the known columns up front, and save their `ScalarWriter`s so we can avoid the per-column hash-map lookups.', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );","[{'comment': 'Here, `inFile` seems to be invariant. Seem inefficient to do the replace on every row. Do once outside the loop?', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {","[{'comment': ""This loops over the metadata and creates a new row. I suppose, most of the time, the number of metadata records will be small relative to the maximum batch size. But, if not, then you'll hit an error.\r\n\r\nThe preferred way to do this is to keep track of which record we're reading, then load records until either we run out, or we fill the batch."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+","[{'comment': 'Nit: extra blank line', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;","[{'comment': 'Nit: variables start with lower case, so `this.hdf5Reader`. Also, no need for `this.`.', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data","[{'comment': 'Nit: here and elsewhere, space after `//`', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data
+    if (dimensions.length <= 1) {
+      TypeProtos.MinorType currentDataType = HDF5Utils.getDataType(dsInfo);
+      // Case for null or unknown data types:
+      if (currentDataType == null) {
+        System.out.println(dsInfo.getTypeInformation().tryGetJavaType());","[{'comment': ""Can't do this in a production server; must write to log file instead."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data
+    if (dimensions.length <= 1) {
+      TypeProtos.MinorType currentDataType = HDF5Utils.getDataType(dsInfo);
+      // Case for null or unknown data types:
+      if (currentDataType == null) {
+        System.out.println(dsInfo.getTypeInformation().tryGetJavaType());
+      }
+
+      switch (currentDataType) {
+        case GENERIC_OBJECT:
+          ArrayList enumData = new ArrayList(Arrays.asList(HDF5reader.readEnumArray(datapath).toStringArray()));","[{'comment': 'Do we need the `ArrayList`, or can we just use the string array? Also, if we do want an `ArrayList`, please parameterize it: `ArrayList<String>`, say.', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data
+    if (dimensions.length <= 1) {
+      TypeProtos.MinorType currentDataType = HDF5Utils.getDataType(dsInfo);
+      // Case for null or unknown data types:
+      if (currentDataType == null) {
+        System.out.println(dsInfo.getTypeInformation().tryGetJavaType());
+      }
+
+      switch (currentDataType) {
+        case GENERIC_OBJECT:
+          ArrayList enumData = new ArrayList(Arrays.asList(HDF5reader.readEnumArray(datapath).toStringArray()));
+          break;
+        case VARCHAR:
+          try {
+            if (readerConfig.defaultPath != null) {
+              String[] tempStringList = HDF5reader.readStringArray(datapath);
+              for (String value : tempStringList) {
+                rowWriter.start();","[{'comment': ""Another case where we are not checking if the batch is full.\r\n\r\nThe logic here is pretty hard to follow. Seems we may be creating a number of hetrogeneous records for various bits of the file. Since Drill is a SQL engine, and requires a single tabular schema, I wonder if this is the correct approach?\r\n\r\nThere is no good way, at present, to tell the reader what you want to read. Would be nice to be able to say:\r\n\r\n```\r\nSELECT * FROM hdf5File WHERE itemType = 'fileNames'\r\n```\r\n\r\nTo extract only file names."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data
+    if (dimensions.length <= 1) {
+      TypeProtos.MinorType currentDataType = HDF5Utils.getDataType(dsInfo);
+      // Case for null or unknown data types:
+      if (currentDataType == null) {
+        System.out.println(dsInfo.getTypeInformation().tryGetJavaType());
+      }
+
+      switch (currentDataType) {
+        case GENERIC_OBJECT:
+          ArrayList enumData = new ArrayList(Arrays.asList(HDF5reader.readEnumArray(datapath).toStringArray()));
+          break;
+        case VARCHAR:
+          try {
+            if (readerConfig.defaultPath != null) {
+              String[] tempStringList = HDF5reader.readStringArray(datapath);
+              for (String value : tempStringList) {
+                rowWriter.start();
+                writeStringColumn(rowWriter, fieldName, value);
+                rowWriter.save();
+              }
+            } else {
+              String[] data = HDF5reader.readStringArray(datapath);
+              writeStringListColumn(rowWriter, fieldName, data);
+            }
+          } catch (Exception e) {
+            logger.warn(""Unknown HDF5 data type: "" + datapath);
+          }
+          break;
+        case TIMESTAMP:
+          long ts = HDF5reader.readTimeStamp(datapath);
+          writeTimestampColumn(rowWriter, fieldName, ts);
+          break;
+        case INT:
+          if (!dsInfo.getTypeInformation().isSigned()) {
+            if (dsInfo.getTypeInformation().getElementSize() > 4) {
+              if (readerConfig.defaultPath != null) {
+                long[] tempLongList = HDF5reader.readLongArray(datapath);
+                for (long i : tempLongList) {
+                  rowWriter.start();
+                  writeLongColumn(rowWriter, fieldName, i);
+                  rowWriter.save();
+                }
+              } else {
+                long[] longList = HDF5reader.uint64().readArray(datapath);
+                if(!isMetadataQuery) {","[{'comment': 'Nit: space after `if`. Here and several other places.', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data
+    if (dimensions.length <= 1) {
+      TypeProtos.MinorType currentDataType = HDF5Utils.getDataType(dsInfo);
+      // Case for null or unknown data types:
+      if (currentDataType == null) {
+        System.out.println(dsInfo.getTypeInformation().tryGetJavaType());
+      }
+
+      switch (currentDataType) {
+        case GENERIC_OBJECT:
+          ArrayList enumData = new ArrayList(Arrays.asList(HDF5reader.readEnumArray(datapath).toStringArray()));
+          break;
+        case VARCHAR:
+          try {
+            if (readerConfig.defaultPath != null) {
+              String[] tempStringList = HDF5reader.readStringArray(datapath);
+              for (String value : tempStringList) {
+                rowWriter.start();
+                writeStringColumn(rowWriter, fieldName, value);
+                rowWriter.save();
+              }
+            } else {
+              String[] data = HDF5reader.readStringArray(datapath);
+              writeStringListColumn(rowWriter, fieldName, data);
+            }
+          } catch (Exception e) {
+            logger.warn(""Unknown HDF5 data type: "" + datapath);
+          }
+          break;
+        case TIMESTAMP:
+          long ts = HDF5reader.readTimeStamp(datapath);
+          writeTimestampColumn(rowWriter, fieldName, ts);
+          break;
+        case INT:
+          if (!dsInfo.getTypeInformation().isSigned()) {
+            if (dsInfo.getTypeInformation().getElementSize() > 4) {
+              if (readerConfig.defaultPath != null) {
+                long[] tempLongList = HDF5reader.readLongArray(datapath);
+                for (long i : tempLongList) {
+                  rowWriter.start();
+                  writeLongColumn(rowWriter, fieldName, i);
+                  rowWriter.save();
+                }
+              } else {
+                long[] longList = HDF5reader.uint64().readArray(datapath);
+                if(!isMetadataQuery) {
+                  rowWriter.start();
+                }
+                writeLongListColumn(rowWriter, fieldName, longList);
+                if(!isMetadataQuery) {
+                  rowWriter.save();","[{'comment': ""This seems pretty error prone. We save the row, but don't start a new one. Is there a cleaner way to work through the rows?\r\n\r\nOne idea is to create a series of ad-hoc iterators that will step though the various items. Then you can say:\r\n\r\n```\r\n  open() {\r\n    MyIterator iter = makeIterator();\r\n    ..\r\n\r\n  read() {\r\n    while (iter.hasNext() && ! rowWriter.isFull()) {\r\n      rowWriter.start();\r\n      iter.readNex(rowWriter);\r\n      rowWriter.save();\r\n```\r\n\r\nBasically, encapsulate all these loops in iterators to make the logic a bit cleaner."", 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data
+    if (dimensions.length <= 1) {
+      TypeProtos.MinorType currentDataType = HDF5Utils.getDataType(dsInfo);
+      // Case for null or unknown data types:
+      if (currentDataType == null) {
+        System.out.println(dsInfo.getTypeInformation().tryGetJavaType());
+      }
+
+      switch (currentDataType) {
+        case GENERIC_OBJECT:
+          ArrayList enumData = new ArrayList(Arrays.asList(HDF5reader.readEnumArray(datapath).toStringArray()));
+          break;
+        case VARCHAR:
+          try {
+            if (readerConfig.defaultPath != null) {
+              String[] tempStringList = HDF5reader.readStringArray(datapath);
+              for (String value : tempStringList) {
+                rowWriter.start();
+                writeStringColumn(rowWriter, fieldName, value);
+                rowWriter.save();
+              }
+            } else {
+              String[] data = HDF5reader.readStringArray(datapath);
+              writeStringListColumn(rowWriter, fieldName, data);
+            }
+          } catch (Exception e) {
+            logger.warn(""Unknown HDF5 data type: "" + datapath);
+          }
+          break;
+        case TIMESTAMP:
+          long ts = HDF5reader.readTimeStamp(datapath);
+          writeTimestampColumn(rowWriter, fieldName, ts);
+          break;
+        case INT:
+          if (!dsInfo.getTypeInformation().isSigned()) {
+            if (dsInfo.getTypeInformation().getElementSize() > 4) {
+              if (readerConfig.defaultPath != null) {
+                long[] tempLongList = HDF5reader.readLongArray(datapath);
+                for (long i : tempLongList) {
+                  rowWriter.start();
+                  writeLongColumn(rowWriter, fieldName, i);
+                  rowWriter.save();
+                }
+              } else {
+                long[] longList = HDF5reader.uint64().readArray(datapath);
+                if(!isMetadataQuery) {
+                  rowWriter.start();
+                }
+                writeLongListColumn(rowWriter, fieldName, longList);
+                if(!isMetadataQuery) {
+                  rowWriter.save();
+                }
+              }
+            }
+          } else {
+            if (readerConfig.defaultPath != null) {
+              int[] tempIntList = HDF5reader.readIntArray(datapath);
+              for (int i : tempIntList) {","[{'comment': 'I think this creates a bunch of boxing/unboxing. Better is plain old index loop:\r\n\r\n```\r\nfor (int i = 0; i < tempIntlist.length; i++) {\r\n  int n = tempIntList[i];\r\n```', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data
+    if (dimensions.length <= 1) {
+      TypeProtos.MinorType currentDataType = HDF5Utils.getDataType(dsInfo);
+      // Case for null or unknown data types:
+      if (currentDataType == null) {
+        System.out.println(dsInfo.getTypeInformation().tryGetJavaType());
+      }
+
+      switch (currentDataType) {
+        case GENERIC_OBJECT:
+          ArrayList enumData = new ArrayList(Arrays.asList(HDF5reader.readEnumArray(datapath).toStringArray()));
+          break;
+        case VARCHAR:
+          try {
+            if (readerConfig.defaultPath != null) {
+              String[] tempStringList = HDF5reader.readStringArray(datapath);
+              for (String value : tempStringList) {
+                rowWriter.start();
+                writeStringColumn(rowWriter, fieldName, value);
+                rowWriter.save();
+              }
+            } else {
+              String[] data = HDF5reader.readStringArray(datapath);
+              writeStringListColumn(rowWriter, fieldName, data);
+            }
+          } catch (Exception e) {
+            logger.warn(""Unknown HDF5 data type: "" + datapath);
+          }
+          break;
+        case TIMESTAMP:
+          long ts = HDF5reader.readTimeStamp(datapath);
+          writeTimestampColumn(rowWriter, fieldName, ts);
+          break;
+        case INT:
+          if (!dsInfo.getTypeInformation().isSigned()) {
+            if (dsInfo.getTypeInformation().getElementSize() > 4) {
+              if (readerConfig.defaultPath != null) {
+                long[] tempLongList = HDF5reader.readLongArray(datapath);
+                for (long i : tempLongList) {
+                  rowWriter.start();
+                  writeLongColumn(rowWriter, fieldName, i);
+                  rowWriter.save();
+                }
+              } else {
+                long[] longList = HDF5reader.uint64().readArray(datapath);
+                if(!isMetadataQuery) {
+                  rowWriter.start();
+                }
+                writeLongListColumn(rowWriter, fieldName, longList);
+                if(!isMetadataQuery) {
+                  rowWriter.save();
+                }
+              }
+            }
+          } else {
+            if (readerConfig.defaultPath != null) {
+              int[] tempIntList = HDF5reader.readIntArray(datapath);
+              for (int i : tempIntList) {
+                rowWriter.start();
+                writeIntColumn(rowWriter, fieldName, i);
+                rowWriter.save();
+              }
+
+            } else {
+              int[] intList = HDF5reader.readIntArray(datapath);
+              if(!isMetadataQuery) {
+                rowWriter.start();
+              }
+              writeIntListColumn(rowWriter, fieldName, intList);
+              if(!isMetadataQuery) {
+                rowWriter.save();
+              }
+            }
+          }
+          break;
+       case FLOAT4:
+          if (readerConfig.defaultPath != null) {
+            float[] tempFloatList = HDF5reader.readFloatArray(datapath);
+            for (float i : tempFloatList) {
+              rowWriter.start();
+              writeFloat4Column(rowWriter, fieldName, i);
+              rowWriter.save();
+            }
+          } else {
+            float[] tempFloatList = HDF5reader.readFloatArray(datapath);
+            if(!isMetadataQuery) {
+              rowWriter.start();
+            }
+            writeFloat4ListColumn(rowWriter, fieldName, tempFloatList);
+            if(!isMetadataQuery) {
+              rowWriter.save();
+            }
+          }
+          break;
+        case FLOAT8:  // TODO Add auto-flatten here...
+          if (readerConfig.defaultPath != null) {
+            double[] tempDoubleList = HDF5reader.readDoubleArray(datapath);
+            for (double i : tempDoubleList) {
+              rowWriter.start();
+              writeFloat8Column(rowWriter, fieldName, i);
+              rowWriter.save();
+            }
+          } else {
+            double[] tempFloatList = HDF5reader.readDoubleArray(datapath);
+            if(!isMetadataQuery) {
+              rowWriter.start();
+            }
+            writeFloat8ListColumn(rowWriter, fieldName, tempFloatList);
+            if(!isMetadataQuery) {
+              rowWriter.save();
+            }
+          }
+          break;
+        case BIGINT:
+          if (!dsInfo.getTypeInformation().isSigned()) {
+            logger.warn(""Drill does not support unsigned 64bit integers."");
+            break;
+          }
+          if (readerConfig.defaultPath != null) {
+            long[] tempLongList = HDF5reader.readLongArray(datapath);
+            for (long i : tempLongList) {
+              rowWriter.start();
+              writeLongColumn(rowWriter, fieldName, i);
+              rowWriter.save();
+            }
+
+          } else {
+            long[] tempLongList = HDF5reader.readLongArray(datapath);
+            if(!isMetadataQuery) {
+              rowWriter.start();
+            }
+            writeLongListColumn(rowWriter, fieldName, tempLongList);
+            if(!isMetadataQuery) {
+              rowWriter.save();
+            }
+          }
+          break;
+        case MAP:
+          //try { // TODO Auto flatten compound data type
+            getAndMapCompoundData(datapath, new ArrayList<>(), this.HDF5reader, rowWriter);
+          //} catch (Exception e) {
+          //  throw UserException
+           //   .dataWriteError()
+             // .addContext(""Error writing Compound Field: "")
+             // .addContext(e.getMessage())
+             // .build(logger);
+          //}
+          break;
+        default:
+          //Case for data types that cannot be read
+          logger.warn(dsInfo.getTypeInformation().tryGetJavaType() + "" not implemented....yet."");
+          break;
+      }
+    } else if (dimensions.length == 2) {","[{'comment': 'This function is really far too large to understand.', 'commenter': 'paul-rogers'}]"
1778,contrib/format-hdf5/src/main/java/org/apache/drill/exec/store/hdf5/HDF5BatchReader.java,"@@ -0,0 +1,887 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.hdf5;
+
+import ch.systemsx.cisd.hdf5.HDF5CompoundMemberInformation;
+import ch.systemsx.cisd.hdf5.HDF5DataClass;
+import ch.systemsx.cisd.hdf5.HDF5DataSetInformation;
+import ch.systemsx.cisd.hdf5.HDF5FactoryProvider;
+import ch.systemsx.cisd.hdf5.HDF5LinkInformation;
+import ch.systemsx.cisd.hdf5.IHDF5Factory;
+import ch.systemsx.cisd.hdf5.IHDF5Reader;
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.BitSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+public class HDF5BatchReader implements ManagedReader<FileSchemaNegotiator> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HDF5BatchReader.class);
+  private FileSplit split;
+  private HDF5FormatConfig formatConfig;
+  private ResultSetLoader loader;
+  private String tempFileName;
+  private IHDF5Reader HDF5reader;
+  private File infile;
+  private BufferedReader reader;
+  protected HDF5ReaderConfig readerConfig;
+  private boolean finish;
+
+
+  public static class HDF5ReaderConfig {
+    protected final HDF5FormatPlugin plugin;
+    protected TupleMetadata schema;
+    protected String defaultPath;
+    protected HDF5FormatConfig formatConfig;
+
+    public HDF5ReaderConfig(HDF5FormatPlugin plugin, HDF5FormatConfig formatConfig) {
+      this.plugin = plugin;
+      this.formatConfig = formatConfig;
+      this.defaultPath = formatConfig.getDefaultPath();
+    }
+  }
+
+
+  public HDF5BatchReader(HDF5ReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+    this.formatConfig = readerConfig.formatConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    loader = negotiator.build();
+    openFile(negotiator);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    InputStream in;
+    try {
+      in = negotiator.fileSystem().open(split.getPath());
+      IHDF5Factory factory = HDF5FactoryProvider.get();
+      this.infile = convertInputStreamToFile(in);
+      this.HDF5reader = factory.openForReading(infile);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath())
+        .build(logger);
+    }
+    reader = new BufferedReader(new InputStreamReader(in));
+  }
+
+  /**
+   * This function converts the Drill inputstream into a File object for the HDF5 library.  This function
+   * exists due to a known limitation in the HDF5 library which cannot parse HDF5 directly from an input stream.  A future
+   * release of the library will support this.
+   *
+   * @param stream
+   * @return
+   * @throws IOException
+   */
+  private File convertInputStreamToFile(InputStream stream) throws IOException {
+    this.tempFileName = ""./~"" + split.getPath().getName();
+    File targetFile = new File(tempFileName);
+    java.nio.file.Files.copy(stream, targetFile.toPath(), StandardCopyOption.REPLACE_EXISTING);
+    IOUtils.closeQuietly(stream);
+    return targetFile;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (this.formatConfig.getDefaultPath() == null || this.formatConfig.getDefaultPath().isEmpty()) {
+        return projectFileMetadata(rowWriter);
+      } else {
+        return projectDataset(rowWriter, readerConfig.defaultPath, false);
+      }
+
+    }
+    return false;
+  }
+
+  private boolean projectFileMetadata(RowSetLoader rowWriter) {
+    List<HDF5DrillMetadata> metadata = getFileMetadata(this.HDF5reader.object().getGroupMemberInformation(""/"", true), new ArrayList<>());
+
+    for (HDF5DrillMetadata record : metadata) {
+      rowWriter.start();
+      writeStringColumn(rowWriter, ""path"", record.getPath());
+      writeStringColumn(rowWriter, ""data_type"", record.getDataType());
+      writeStringColumn(rowWriter, ""file_name"",this.infile.getName().replace(""~"", """") );
+
+      //Write attributes if present
+      if (record.getAttributes().size() > 0) {
+        writeAttributes(rowWriter, record);
+      }
+      if (record.getDataType().equals(""DATASET"")) {
+        if (readerConfig.defaultPath != null) {
+          projectDataset(rowWriter, readerConfig.defaultPath, true);
+        } else {
+          projectDataset(rowWriter, record.getPath(), true);
+        }
+      }
+      rowWriter.save();
+    }
+    return false;
+  }
+
+  /**
+   * This helper function returns the name of a HDF5 record from a data path
+   * @param path Path to HDF5 data
+   * @return String name of data
+   */
+  private String getNameFromPath(String path) {
+    String pattern = ""/*.*/(.+?)$"";
+    Pattern r = Pattern.compile(pattern);
+    // Now create matcher object.
+    Matcher m = r.matcher(path);
+    if (m.find()) {
+      return m.group(1);
+    } else {
+      return """";
+    }
+  }
+
+  private List<HDF5DrillMetadata> getFileMetadata(List<HDF5LinkInformation> members, List<HDF5DrillMetadata> metadata) {
+    for (HDF5LinkInformation info : members) {
+      HDF5DrillMetadata metadataRow = new HDF5DrillMetadata();
+
+      metadataRow.setPath(info.getPath());
+      metadataRow.setDataType(info.getType().toString());
+
+      switch (info.getType()) {
+        case DATASET:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          HDF5DataSetInformation dsInfo = HDF5reader.object().getDataSetInformation(info.getPath());
+          metadata.add(metadataRow);
+          break;
+        case SOFT_LINK:
+          // Soft links cannot have attributes
+          metadata.add(metadataRow);
+          break;
+        case GROUP:
+          metadataRow.setAttributes(getAttributes(HDF5reader, info.getPath()));
+          metadata.add(metadataRow);
+          metadata = getFileMetadata(HDF5reader.object().getGroupMemberInformation(info.getPath(), true), metadata);
+          break;
+        default:
+          break;
+      }
+    }
+    return metadata;
+  }
+
+  private HashMap getAttributes(IHDF5Reader reader, String path) {
+    HashMap<String, HDF5Attribute> attributes = new HashMap<>();
+    long attrCount = reader.object().getObjectInformation(path).getNumberOfAttributes();
+    if (attrCount > 0) {
+      List<String> attrNames = reader.object().getAllAttributeNames(path);
+      for (String name : attrNames) {
+        try {
+          HDF5Attribute attribute = HDF5Utils.getAttribute(path, name, reader);
+          attributes.put(attribute.getKey(), attribute);
+        } catch (Exception e) {
+          logger.info(""Couldn't add attribute: "" + path + "" "" + name);
+        }
+      }
+    }
+    return attributes;
+  }
+
+
+  private boolean projectDataset(RowSetLoader rowWriter, String datapath, boolean isMetadataQuery) {
+    int resultCount = 0;
+
+    String fieldName = getNameFromPath(datapath);
+    IHDF5Reader reader = this.HDF5reader;
+    HDF5DataSetInformation dsInfo = reader.object().getDataSetInformation(datapath);
+    HDF5DataClass dataType = dsInfo.getTypeInformation().getRawDataClass();
+    long[] dimensions = dsInfo.getDimensions();
+    //Case for single dimensional data
+    if (dimensions.length <= 1) {
+      TypeProtos.MinorType currentDataType = HDF5Utils.getDataType(dsInfo);
+      // Case for null or unknown data types:
+      if (currentDataType == null) {
+        System.out.println(dsInfo.getTypeInformation().tryGetJavaType());
+      }
+
+      switch (currentDataType) {
+        case GENERIC_OBJECT:
+          ArrayList enumData = new ArrayList(Arrays.asList(HDF5reader.readEnumArray(datapath).toStringArray()));
+          break;
+        case VARCHAR:
+          try {
+            if (readerConfig.defaultPath != null) {
+              String[] tempStringList = HDF5reader.readStringArray(datapath);
+              for (String value : tempStringList) {
+                rowWriter.start();
+                writeStringColumn(rowWriter, fieldName, value);
+                rowWriter.save();
+              }
+            } else {
+              String[] data = HDF5reader.readStringArray(datapath);
+              writeStringListColumn(rowWriter, fieldName, data);
+            }
+          } catch (Exception e) {
+            logger.warn(""Unknown HDF5 data type: "" + datapath);
+          }
+          break;
+        case TIMESTAMP:
+          long ts = HDF5reader.readTimeStamp(datapath);
+          writeTimestampColumn(rowWriter, fieldName, ts);
+          break;
+        case INT:
+          if (!dsInfo.getTypeInformation().isSigned()) {
+            if (dsInfo.getTypeInformation().getElementSize() > 4) {
+              if (readerConfig.defaultPath != null) {
+                long[] tempLongList = HDF5reader.readLongArray(datapath);
+                for (long i : tempLongList) {
+                  rowWriter.start();
+                  writeLongColumn(rowWriter, fieldName, i);
+                  rowWriter.save();
+                }
+              } else {
+                long[] longList = HDF5reader.uint64().readArray(datapath);
+                if(!isMetadataQuery) {
+                  rowWriter.start();
+                }
+                writeLongListColumn(rowWriter, fieldName, longList);
+                if(!isMetadataQuery) {
+                  rowWriter.save();
+                }
+              }
+            }
+          } else {
+            if (readerConfig.defaultPath != null) {
+              int[] tempIntList = HDF5reader.readIntArray(datapath);
+              for (int i : tempIntList) {
+                rowWriter.start();
+                writeIntColumn(rowWriter, fieldName, i);
+                rowWriter.save();
+              }
+
+            } else {
+              int[] intList = HDF5reader.readIntArray(datapath);
+              if(!isMetadataQuery) {
+                rowWriter.start();
+              }
+              writeIntListColumn(rowWriter, fieldName, intList);
+              if(!isMetadataQuery) {
+                rowWriter.save();
+              }
+            }
+          }
+          break;
+       case FLOAT4:
+          if (readerConfig.defaultPath != null) {
+            float[] tempFloatList = HDF5reader.readFloatArray(datapath);
+            for (float i : tempFloatList) {
+              rowWriter.start();
+              writeFloat4Column(rowWriter, fieldName, i);
+              rowWriter.save();
+            }
+          } else {
+            float[] tempFloatList = HDF5reader.readFloatArray(datapath);
+            if(!isMetadataQuery) {
+              rowWriter.start();
+            }
+            writeFloat4ListColumn(rowWriter, fieldName, tempFloatList);
+            if(!isMetadataQuery) {
+              rowWriter.save();
+            }
+          }
+          break;
+        case FLOAT8:  // TODO Add auto-flatten here...
+          if (readerConfig.defaultPath != null) {
+            double[] tempDoubleList = HDF5reader.readDoubleArray(datapath);
+            for (double i : tempDoubleList) {
+              rowWriter.start();
+              writeFloat8Column(rowWriter, fieldName, i);
+              rowWriter.save();
+            }
+          } else {
+            double[] tempFloatList = HDF5reader.readDoubleArray(datapath);
+            if(!isMetadataQuery) {
+              rowWriter.start();
+            }
+            writeFloat8ListColumn(rowWriter, fieldName, tempFloatList);
+            if(!isMetadataQuery) {
+              rowWriter.save();
+            }
+          }
+          break;
+        case BIGINT:
+          if (!dsInfo.getTypeInformation().isSigned()) {
+            logger.warn(""Drill does not support unsigned 64bit integers."");
+            break;
+          }
+          if (readerConfig.defaultPath != null) {
+            long[] tempLongList = HDF5reader.readLongArray(datapath);
+            for (long i : tempLongList) {
+              rowWriter.start();
+              writeLongColumn(rowWriter, fieldName, i);
+              rowWriter.save();
+            }
+
+          } else {
+            long[] tempLongList = HDF5reader.readLongArray(datapath);
+            if(!isMetadataQuery) {
+              rowWriter.start();
+            }
+            writeLongListColumn(rowWriter, fieldName, tempLongList);
+            if(!isMetadataQuery) {
+              rowWriter.save();
+            }
+          }
+          break;
+        case MAP:
+          //try { // TODO Auto flatten compound data type
+            getAndMapCompoundData(datapath, new ArrayList<>(), this.HDF5reader, rowWriter);
+          //} catch (Exception e) {
+          //  throw UserException
+           //   .dataWriteError()
+             // .addContext(""Error writing Compound Field: "")
+             // .addContext(e.getMessage())
+             // .build(logger);
+          //}
+          break;
+        default:
+          //Case for data types that cannot be read
+          logger.warn(dsInfo.getTypeInformation().tryGetJavaType() + "" not implemented....yet."");
+          break;
+      }
+    } else if (dimensions.length == 2) {
+      long cols = dimensions[1];
+      long rows = dimensions[0];
+      switch (HDF5Utils.getDataType(dsInfo)) {
+        case INT:
+          int[][] colData = HDF5reader.readIntMatrix(datapath);
+          mapIntMatrixField(fieldName, colData, (int) cols, (int) rows, rowWriter);
+          break;
+        /*case FLOAT4:
+          float[][] floatData = HDF5reader.readFloatMatrix(datapath);
+          resultCount = mapFloatMatrixField(fieldName, floatData, (int) cols, (int) rows, map);
+          break;
+        case FLOAT8:
+          double[][] doubleData = HDF5reader.readDoubleMatrix(datapath);
+          resultCount = mapDoubleMatrixField(fieldName, doubleData, (int) cols, (int) rows, map);
+          break;
+        case BIGINT:
+          long[][] longData = HDF5reader.readLongMatrix(datapath);
+          resultCount = mapLongMatrixField(fieldName, longData, (int) cols, (int) rows, map);
+          break;*/
+        default:
+          logger.info(HDF5Utils.getDataType(dsInfo) + "" not implemented."");
+          break;
+      }
+    }/* else {
+      // Case for data sets with dimensions > 2
+      long cols = dimensions[1];
+      long rows = dimensions[0];
+      switch (HDF5Utils.getDataType(dsInfo)) {
+        case INT:
+          int[][] colData = HDF5reader.int32().readMDArray(datapath).toMatrix();
+          resultCount = mapIntMatrixField(fieldName, colData, (int) cols, (int) rows, map);
+          break;
+        case FLOAT4:
+          float[][] floatData = HDF5reader.float32().readMDArray(datapath).toMatrix();
+          resultCount = mapFloatMatrixField(fieldName, floatData, (int) cols, (int) rows, map);
+          break;
+        case FLOAT8:
+          double[][] doubleData = HDF5reader.float64().readMDArray(datapath).toMatrix();
+          resultCount = mapDoubleMatrixField(fieldName, doubleData, (int) cols, (int) rows, map);
+          break;
+        case BIGINT:
+          long[][] longData = HDF5reader.int64().readMDArray(datapath).toMatrix();
+          resultCount = mapLongMatrixField(fieldName, longData, (int) cols, (int) rows, map);
+          break;
+        default:
+          logger.info(HDF5Utils.getDataType(dsInfo) + "" not implemented."");
+          break;
+      }*/
+      //}
+    //}
+
+    return false;
+  }
+
+  private void writeBooleanColumn(TupleWriter rowWriter, String name, int value) {
+    boolean bool_value = true;
+    if(value == 0) {
+      bool_value = false;
+    }
+    writeBooleanColumn(rowWriter, name, bool_value);","[{'comment': '```\r\nwriteBooleanColumn(rowWriter, name, value != 0);\r\n```', 'commenter': 'paul-rogers'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -587,6 +622,49 @@
       if (e.target.form) 
         <#if model.isOnlyImpersonationEnabled()>doSubmitQueryWithUserName()<#else>doSubmitQueryWithAutoLimit()</#if>;
     });
+
+    // Convert scientific to Decimal [Ref: https://gist.github.com/jiggzson/b5f489af9ad931e3d186]
+    function scientificToDecimal(num) {","[{'comment': ""Won't this be more convenient to do in Java and just pass expected string representation to UI?"", 'commenter': 'arina-ielchiieva'}, {'comment': 'Ideally, yes.\r\n\r\nBut the way we construct the `Operators Overview` table is by only injecting values (which then get converted to human readable formats based on data types). That makes it difficult for me to inject these `<div>` elements during construction.\r\n\r\nAlso, with Javascript, I reduce the processing and construction overhead on the WebServer and have the client browsers do this formatting.\r\n\r\nIn theory, the entire profile can be constructed by Javascript from the JSON profile, but basic summaries are easier to implement (and faster to calculate) in Java.', 'commenter': 'kkhatua'}, {'comment': 'Why do you need this at all? Javascript parseInt() function which you use can parse scientific format without a problem.', 'commenter': 'agozhiy'}, {'comment': ""I don't believe that works. The parseInt function will parse until the 'E' symbol. "", 'commenter': 'kkhatua'}, {'comment': 'From the Chrome web console:\r\n`> parseInt(1.6E7)`\r\n`< 16000000`\r\nSo it works it seems.\r\nUpdate: as you use Number(), this is not needed all the more.', 'commenter': 'agozhiy'}, {'comment': 'Yep. Verified it. Thanks for pointing it out.', 'commenter': 'kkhatua'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -587,6 +622,49 @@
       if (e.target.form) 
         <#if model.isOnlyImpersonationEnabled()>doSubmitQueryWithUserName()<#else>doSubmitQueryWithAutoLimit()</#if>;
     });
+
+    // Convert scientific to Decimal [Ref: https://gist.github.com/jiggzson/b5f489af9ad931e3d186]
+    function scientificToDecimal(num) {
+      //if the number is in scientific notation remove it
+      if(/\d+\.?\d*e[\+\-]*\d+/i.test(num)) {
+        var zero = '0',
+            parts = String(num).toLowerCase().split('e'), //split into coeff and exponent
+            e = parts.pop(),//store the exponential part
+            l = Math.abs(e), //get the number of zeros
+            sign = e/l,
+            coeff_array = parts[0].split('.');
+        if(sign === -1) {
+            coeff_array[0] = Math.abs(coeff_array[0]);
+            num = '-'+zero + '.' + new Array(l).join(zero) + coeff_array.join('');
+        }
+        else {
+            var dec = coeff_array[1];
+            if(dec) l = l - dec.length;
+            num = coeff_array.join('') + new Array(l+1).join(zero);
+        }
+      }
+      return num;
+    }
+
+    // Extract estimated rowcount map
+    var opRowCountMap = {};
+    // Get OpId-Rowocunt Map
+    function buildRowCountMap() {
+      var phyText = $('#query-physical').find('pre').text();
+      var opLines = phyText.split(""\n"");
+      for (var l in opLines) {
+        var line = opLines[l];
+        if (line.trim().length > 0) {
+          var opId = line.match(/\d+-\d+/g)[0];
+          var opRowCount = line.match(/rowcount = \S+/g)[0].split(' ')[2].replace(',','').trim();
+          if (opRowCount.includes(""E"")) {
+            opRowCountMap[opId] = parseInt(scientificToDecimal(opRowCount)).toLocaleString('en');","[{'comment': ""What if rowCount exceeds the int limit? It's type was made double on purpose."", 'commenter': 'agozhiy'}, {'comment': ""Good point. I'll check for larger values and apply the parseLong() method instead."", 'commenter': 'kkhatua'}, {'comment': 'Tested with ##E18  using the Number() function.', 'commenter': 'kkhatua'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -587,6 +622,49 @@
       if (e.target.form) 
         <#if model.isOnlyImpersonationEnabled()>doSubmitQueryWithUserName()<#else>doSubmitQueryWithAutoLimit()</#if>;
     });
+
+    // Convert scientific to Decimal [Ref: https://gist.github.com/jiggzson/b5f489af9ad931e3d186]
+    function scientificToDecimal(num) {
+      //if the number is in scientific notation remove it
+      if(/\d+\.?\d*e[\+\-]*\d+/i.test(num)) {
+        var zero = '0',
+            parts = String(num).toLowerCase().split('e'), //split into coeff and exponent
+            e = parts.pop(),//store the exponential part
+            l = Math.abs(e), //get the number of zeros
+            sign = e/l,
+            coeff_array = parts[0].split('.');
+        if(sign === -1) {
+            coeff_array[0] = Math.abs(coeff_array[0]);
+            num = '-'+zero + '.' + new Array(l).join(zero) + coeff_array.join('');
+        }
+        else {
+            var dec = coeff_array[1];
+            if(dec) l = l - dec.length;
+            num = coeff_array.join('') + new Array(l+1).join(zero);
+        }
+      }
+      return num;
+    }
+
+    // Extract estimated rowcount map
+    var opRowCountMap = {};
+    // Get OpId-Rowocunt Map
+    function buildRowCountMap() {
+      var phyText = $('#query-physical').find('pre').text();
+      var opLines = phyText.split(""\n"");
+      for (var l in opLines) {
+        var line = opLines[l];
+        if (line.trim().length > 0) {
+          var opId = line.match(/\d+-\d+/g)[0];
+          var opRowCount = line.match(/rowcount = \S+/g)[0].split(' ')[2].replace(',','').trim();","[{'comment': '```suggestion\r\n          var opRowCount = line.match(/rowcount = (\\d+.\\d+)/)[1];\r\n```', 'commenter': 'agozhiy'}, {'comment': ""I can't use this because the `rowcount` might be in the scientific format, resulting in a partial extraction."", 'commenter': 'kkhatua'}, {'comment': 'Right. Still, it can be simplified. This should work with both formats:\r\n`line.match(/rowcount = ([^,]+)/)[1]`', 'commenter': 'agozhiy'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -587,6 +622,49 @@
       if (e.target.form) 
         <#if model.isOnlyImpersonationEnabled()>doSubmitQueryWithUserName()<#else>doSubmitQueryWithAutoLimit()</#if>;
     });
+
+    // Convert scientific to Decimal [Ref: https://gist.github.com/jiggzson/b5f489af9ad931e3d186]
+    function scientificToDecimal(num) {
+      //if the number is in scientific notation remove it
+      if(/\d+\.?\d*e[\+\-]*\d+/i.test(num)) {
+        var zero = '0',
+            parts = String(num).toLowerCase().split('e'), //split into coeff and exponent
+            e = parts.pop(),//store the exponential part
+            l = Math.abs(e), //get the number of zeros
+            sign = e/l,
+            coeff_array = parts[0].split('.');
+        if(sign === -1) {
+            coeff_array[0] = Math.abs(coeff_array[0]);
+            num = '-'+zero + '.' + new Array(l).join(zero) + coeff_array.join('');
+        }
+        else {
+            var dec = coeff_array[1];
+            if(dec) l = l - dec.length;
+            num = coeff_array.join('') + new Array(l+1).join(zero);
+        }
+      }
+      return num;
+    }
+
+    // Extract estimated rowcount map
+    var opRowCountMap = {};
+    // Get OpId-Rowocunt Map
+    function buildRowCountMap() {
+      var phyText = $('#query-physical').find('pre').text();
+      var opLines = phyText.split(""\n"");
+      for (var l in opLines) {","[{'comment': '```suggestion\r\n      opLines.forEach(line => {\r\n```', 'commenter': 'agozhiy'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -84,6 +84,29 @@
         document.getElementById(warningElemId).style.display=""none"";
     }
 
+    //Injects Estimated Rows
+    function injectEstimatedRows() {
+      Object.keys(opRowCountMap).forEach(key => {
+        var tgtElem = $(""td.estRowsAnchor[key='""+key+""']""); ","[{'comment': '```suggestion\r\n        var tgtElem = $(""td.estRowsAnchor[key=\'"" + key + ""\']"");\r\n```', 'commenter': 'agozhiy'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -84,6 +84,29 @@
         document.getElementById(warningElemId).style.display=""none"";
     }
 
+    //Injects Estimated Rows
+    function injectEstimatedRows() {
+      Object.keys(opRowCountMap).forEach(key => {
+        var tgtElem = $(""td.estRowsAnchor[key='""+key+""']""); 
+        var status = tgtElem.append( ""<div class='estRows' title='Estimated'>(""+opRowCountMap[key]+"")</div>"" );","[{'comment': '```suggestion\r\n        var status = tgtElem.append(""<div class=\'estRows\' title=\'Estimated\'>("" + opRowCountMap[key] + "")</div>"");\r\n```', 'commenter': 'agozhiy'}, {'comment': 'Spaces are still present around the expression inside parentheses. This is inconsistent with the overall code style. ', 'commenter': 'agozhiy'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -84,6 +84,29 @@
         document.getElementById(warningElemId).style.display=""none"";
     }
 
+    //Injects Estimated Rows
+    function injectEstimatedRows() {
+      Object.keys(opRowCountMap).forEach(key => {
+        var tgtElem = $(""td.estRowsAnchor[key='""+key+""']""); 
+        var status = tgtElem.append( ""<div class='estRows' title='Estimated'>(""+opRowCountMap[key]+"")</div>"" );
+      });
+    }
+
+    //Toggle Estimates' visibility
+    function toggleEstimates(tgtColumn) {
+      var colClass = '.est'+tgtColumn","[{'comment': ""```suggestion\r\n      var colClass = '.est' + tgtColumn\r\n```"", 'commenter': 'agozhiy'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -84,6 +84,29 @@
         document.getElementById(warningElemId).style.display=""none"";
     }
 
+    //Injects Estimated Rows
+    function injectEstimatedRows() {
+      Object.keys(opRowCountMap).forEach(key => {
+        var tgtElem = $(""td.estRowsAnchor[key='""+key+""']""); 
+        var status = tgtElem.append( ""<div class='estRows' title='Estimated'>(""+opRowCountMap[key]+"")</div>"" );
+      });
+    }
+
+    //Toggle Estimates' visibility
+    function toggleEstimates(tgtColumn) {
+      var colClass = '.est'+tgtColumn
+      var estimates = $(colClass);
+      if (estimates.filter("":visible"").length > 0) {
+        $(colClass).each(function(){","[{'comment': '```suggestion\r\n        $(colClass).each(function () {\r\n```', 'commenter': 'agozhiy'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -84,6 +84,29 @@
         document.getElementById(warningElemId).style.display=""none"";
     }
 
+    //Injects Estimated Rows
+    function injectEstimatedRows() {
+      Object.keys(opRowCountMap).forEach(key => {
+        var tgtElem = $(""td.estRowsAnchor[key='""+key+""']""); 
+        var status = tgtElem.append( ""<div class='estRows' title='Estimated'>(""+opRowCountMap[key]+"")</div>"" );
+      });
+    }
+
+    //Toggle Estimates' visibility
+    function toggleEstimates(tgtColumn) {
+      var colClass = '.est'+tgtColumn
+      var estimates = $(colClass);
+      if (estimates.filter("":visible"").length > 0) {
+        $(colClass).each(function(){
+          $(this).attr(""style"", ""display:none"");
+        });
+      } else {
+        $(colClass).each(function(){","[{'comment': '```suggestion\r\n        $(colClass).each(function () {\r\n```', 'commenter': 'agozhiy'}]"
1779,exec/java-exec/src/main/resources/rest/profile/profile.ftl,"@@ -587,6 +622,25 @@
       if (e.target.form) 
         <#if model.isOnlyImpersonationEnabled()>doSubmitQueryWithUserName()<#else>doSubmitQueryWithAutoLimit()</#if>;
     });
+
+    // Extract estimated rowcount map
+    var opRowCountMap = {};
+    // Get OpId-Rowocunt Map
+    function buildRowCountMap() {
+      var phyText = $('#query-physical').find('pre').text();
+      var opLines = phyText.split(""\n"");
+      opLines.forEach(line => {
+        if (line.trim().length > 0) {
+          var opId = line.match(/\d+-\d+/g)[0];
+          var opRowCount = line.match(/rowcount = ([^,]+)/)[1];
+          if (opRowCount.includes(""E"")) {","[{'comment': 'This ""if"" is redundant now.', 'commenter': 'agozhiy'}, {'comment': 'Sorry... missed this in the email notification. Done now.', 'commenter': 'kkhatua'}]"
1821,common/src/main/java/org/apache/drill/common/types/Types.java,"@@ -549,7 +549,7 @@ public static MajorType withPrecision(final MinorType type, final DataMode mode,
     return MajorType.newBuilder().setMinorType(type).setMode(mode).setPrecision(precision).build();
   }
 
-  public static MajorType withScaleAndPrecision(final MinorType type, final DataMode mode, final int scale, final int precision) {
+  public static MajorType withScaleAndPrecision(MinorType type, DataMode mode, int precision, int scale) {","[{'comment': '```suggestion\r\n  public static MajorType withPrecisionAndScale(MinorType type, DataMode mode, int precision, int scale) {\r\n```', 'commenter': 'ihuzenko'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/RowSetReader.java,"@@ -46,8 +46,7 @@
    * The index of the underlying row which may be indexed by an
    * SV2 or SV4.
    *
-   * @return
+   * @return index of the underlying row
    */
-
   int offset();
 }","[{'comment': 'New line.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Done.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/RowSetPrinter.java,"@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.drill.test.rowSet;
+package org.apache.drill.exec.physical.rowSet;","[{'comment': ""Make sure Printer returns strings instead of sout\r\nMaybe rename it to other name since now it won't print."", 'commenter': 'arina-ielchiieva'}, {'comment': 'Please rename method: `void setPosn(int index);`', 'commenter': 'arina-ielchiieva'}, {'comment': '1. Remove `protected SchemaChangeCallBack callBack = new SchemaChangeCallBack();`\r\n2. Better error message: `public long size() {\r\n    throw new UnsupportedOperationException(""getSize"");\r\n  }`', 'commenter': 'arina-ielchiieva'}, {'comment': 'Replace usage of `Sets.<Integer>newHashSet()`', 'commenter': 'arina-ielchiieva'}, {'comment': '1. Remove public: `public interface SingleRowSet extends RowSet {`\r\n2. Please try to fix all java doc errors.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}, {'comment': 'Thanks, replaced with `Collections.emptySet()`.', 'commenter': 'vvysotskyi'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}, {'comment': 'Thanks, replaced `PrintStream` usage with `StringBuilder`, renamed class to `RowSetStringBuilder` and renamed its methods.', 'commenter': 'vvysotskyi'}, {'comment': 'Thanks, renamed to `setPosition()`.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/AbstractSingleRowSet.java,"@@ -15,15 +15,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.drill.test.rowSet;
+package org.apache.drill.exec.physical.rowSet;","[{'comment': 'Constructors can be protected.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Done.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/DirectRowSet.java,"@@ -33,9 +32,10 @@
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.vector.accessor.convert.ColumnConversionFactory;
-import org.apache.drill.test.rowSet.RowSet.ExtendableRowSet;
-import org.apache.drill.test.rowSet.RowSetWriterImpl.WriterIndexImpl;
+import org.apache.drill.exec.physical.rowSet.RowSet.ExtendableRowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetWriterImpl.WriterIndexImpl;
 
+import java.util.HashSet;","[{'comment': '`public RowSetWriter writer() {\r\n    return writer(10);\r\n  }`\r\n\r\nCreate a constant instead with Java-doc.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/DirectRowSet.java,"@@ -132,7 +131,7 @@ public RowSetReader reader() {
 
   @Override
   public SingleRowSet toIndirect() {
-    return new IndirectRowSet(this, Sets.<Integer>newHashSet());
+    return new IndirectRowSet(this, new HashSet<>());","[{'comment': 'Should be immutable?', 'commenter': 'arina-ielchiieva'}, {'comment': 'Agree, replaced with `Collections.emptySet()`.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/test/java/org/apache/drill/test/rowSet/RowSetBuilder.java,"@@ -20,12 +20,15 @@
 import java.util.Set;
 
 import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.rowSet.DirectRowSet;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetWriter;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.metadata.MetadataUtils;
 import org.apache.drill.exec.record.metadata.TupleMetadata;
 import org.apache.drill.exec.vector.accessor.convert.ColumnConversionFactory;
 import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
-import org.apache.drill.test.rowSet.RowSet.SingleRowSet;
+import org.apache.drill.exec.physical.rowSet.RowSet.SingleRowSet;","[{'comment': '1. Capacity here can use constant we introduce for `10`.\r\n2. Also can be considered to be moved.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/RowSetStringBuilder.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.rowSet;
+
+import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+
+/**
+ * Helper class to obtain string representation of RowSet.
+ */
+public class RowSetStringBuilder {
+  private RowSet rowSet;
+
+  public RowSetStringBuilder(RowSet rowSet) {
+    this.rowSet = rowSet;
+  }
+
+  public static void appendTupleSchema(StringBuilder stringBuilder, TupleMetadata schema) {
+    for (int i = 0; i < schema.size(); i++) {
+      if (i > 0) {
+        stringBuilder.append("", "");
+      }
+      ColumnMetadata colSchema = schema.metadata(i);
+      stringBuilder.append(colSchema.name());
+      if (colSchema.isMap()) {
+        stringBuilder.append(""{"");
+        appendTupleSchema(stringBuilder, colSchema.mapSchema());
+        stringBuilder.append(""}"");
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder result = new StringBuilder();
+    appendTo(result);
+    return result.toString();
+  }
+
+  public void appendTo(StringBuilder stringBuilder) {
+    SelectionVectorMode selectionMode = rowSet.indirectionType();
+    RowSetReader reader = rowSet.reader();
+    int colCount = reader.tupleSchema().size();
+    appendSchema(stringBuilder, selectionMode, reader);
+    while (reader.next()) {
+      appendHeader(stringBuilder, reader, selectionMode);
+      for (int i = 0; i < colCount; i++) {
+        if (i > 0) {
+          stringBuilder.append("", "");
+        }
+        stringBuilder.append(reader.column(i).getAsString());
+      }
+      stringBuilder.append(""\n"");
+    }
+  }
+
+  private void appendSchema(StringBuilder stringBuilder, SelectionVectorMode selectionMode, RowSetReader reader) {
+    stringBuilder.append(""#"");
+    switch (selectionMode) {
+      case FOUR_BYTE:
+        stringBuilder.append("" (batch #, row #)"");
+        break;
+      case TWO_BYTE:
+        stringBuilder.append("" (row #)"");
+        break;
+      default:
+        break;
+    }
+    stringBuilder.append("": "");
+    TupleMetadata schema = reader.tupleSchema();
+    appendTupleSchema(stringBuilder, schema);
+    stringBuilder.append(""\n"");
+  }
+
+  private void appendHeader(StringBuilder stringBuilder, RowSetReader reader, SelectionVectorMode selectionMode) {
+    stringBuilder.append(reader.logicalIndex());
+    switch (selectionMode) {
+      case FOUR_BYTE:
+        stringBuilder.append("" ("");
+        stringBuilder.append(reader.hyperVectorIndex());
+        stringBuilder.append("", "");
+        stringBuilder.append(reader.offset());
+        stringBuilder.append("")"");
+        break;
+      case TWO_BYTE:
+        stringBuilder.append("" ("");
+        stringBuilder.append(reader.offset());
+        stringBuilder.append("")"");
+        break;
+      default:","[{'comment': 'Do we need default here?', 'commenter': 'arina-ielchiieva'}, {'comment': 'No, it is not required.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/RowSetStringBuilder.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.rowSet;
+
+import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+
+/**
+ * Helper class to obtain string representation of RowSet.
+ */
+public class RowSetStringBuilder {
+  private RowSet rowSet;","[{'comment': 'final?', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/RowSetStringBuilder.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.rowSet;
+
+import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+
+/**
+ * Helper class to obtain string representation of RowSet.
+ */
+public class RowSetStringBuilder {
+  private RowSet rowSet;
+
+  public RowSetStringBuilder(RowSet rowSet) {
+    this.rowSet = rowSet;
+  }
+
+  public static void appendTupleSchema(StringBuilder stringBuilder, TupleMetadata schema) {","[{'comment': 'Why this is public and static?', 'commenter': 'arina-ielchiieva'}, {'comment': 'Made private and non-static.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/RowSetStringBuilder.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.rowSet;
+
+import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+
+/**
+ * Helper class to obtain string representation of RowSet.
+ */
+public class RowSetStringBuilder {
+  private RowSet rowSet;
+
+  public RowSetStringBuilder(RowSet rowSet) {
+    this.rowSet = rowSet;
+  }
+
+  public static void appendTupleSchema(StringBuilder stringBuilder, TupleMetadata schema) {
+    for (int i = 0; i < schema.size(); i++) {
+      if (i > 0) {
+        stringBuilder.append("", "");
+      }
+      ColumnMetadata colSchema = schema.metadata(i);
+      stringBuilder.append(colSchema.name());
+      if (colSchema.isMap()) {
+        stringBuilder.append(""{"");
+        appendTupleSchema(stringBuilder, colSchema.mapSchema());
+        stringBuilder.append(""}"");
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder result = new StringBuilder();
+    appendTo(result);
+    return result.toString();
+  }
+
+  public void appendTo(StringBuilder stringBuilder) {","[{'comment': 'Same here?', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}]"
1843,exec/java-exec/src/main/java/org/apache/drill/exec/physical/rowSet/RowSetStringBuilder.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.rowSet;
+
+import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+
+/**
+ * Helper class to obtain string representation of RowSet.","[{'comment': 'It would be nice to add an example of the output.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Agree, added an example.', 'commenter': 'vvysotskyi'}]"
1854,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -259,15 +265,116 @@ public LogicalExpression visitCall(RexCall call) {
             return SchemaPath.getSimplePath(rootSegName + literal.getValue2().toString());
           }
 
-          final RexLiteral literal = (RexLiteral) call.getOperands().get(1);
-          switch(literal.getTypeName()){
-          case DECIMAL:
-          case INTEGER:
-            return left.getChild(((BigDecimal)literal.getValue()).intValue());
-          case CHAR:
-            return left.getChild(literal.getValue2().toString());
-          default:
-            // fall through
+          final RexLiteral literal;
+          RexNode operand = call.getOperands().get(1);
+          if (operand instanceof RexLiteral) {
+            literal = (RexLiteral) operand;
+          } else {
+            if (isMap && operand.getKind() == SqlKind.CAST) {","[{'comment': 'The nested ```if-else``` could be combined with outer one. \r\n```java\r\nif (operand instanceof RexLiteral) {\r\n   // simple cast \r\n} else if (isMap && operand.getKind() == SqlKind.CAST){\r\n    // sql cast\r\n} else {\r\n   // assertion error\r\n}\r\n```', 'commenter': 'ihuzenko'}]"
1854,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -251,6 +254,9 @@ public LogicalExpression visitCall(RexCall call) {
         if (call.getOperator() == SqlStdOperatorTable.ITEM) {","[{'comment': ""I believe that logic inside the ```if (call.getOperator() == SqlStdOperatorTable.ITEM) {``` may be extracted to separate method, although there is some ```// fall through``` at the end of if it's actually visible that at the end of ```visitCall(RexCall call)``` it'll be finished by assertion error. Also the newly added ```switch-case``` expressions may be extracted to separate methods too. "", 'commenter': 'ihuzenko'}]"
1854,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -345,21 +452,21 @@ private LogicalExpression getDrillCastFunctionFromOptiq(RexCall call){
       LogicalExpression arg = call.getOperands().get(0).accept(this);
       MajorType castType;
 
-      switch(call.getType().getSqlTypeName().getName()){
-      case ""VARCHAR"":
-      case ""CHAR"":
+      switch(call.getType().getSqlTypeName()){","[{'comment': 'please fix alignment for the switch case. ', 'commenter': 'ihuzenko'}]"
1854,exec/vector/src/main/codegen/templates/BaseReader.java,"@@ -83,6 +83,20 @@
      */
     int find(int key);
 
+    /**
+     * Obtain the index for given key in current row used to find a corresponding value with.
+     * Used in generated code when retrieving value from Dict using {@link org.apache.drill.common.expression.PathSegment}
+     * with provided {@link org.apache.drill.common.expression.PathSegment#getOriginalValue()}
+     * in cases when {@link org.apache.drill.exec.vector.complex.DictVector#getValueType()} is complex.
+     *
+     * <p>The {@code key} is assumed to be of actual type, is not converted and used as is.
+     *
+     * @param key key value
+     * @return index for the given key
+     * @see org.apache.drill.exec.vector.complex.DictVector
+     */
+    int find(Object key);","[{'comment': 'Are there further reasons to keep ```int find(String key);``` and ```void read(String key, ValueHolder holder);```  methods in this interface after adding methods with ```Object key``` ? ', 'commenter': 'ihuzenko'}, {'comment': 'Yes, other methods are still used when there is no notion of column types during validation phase (when querying `DynamicDrillTable`, for example) based on `PathSegment` info (`int` for `ArraySegment` and `String` for `NamedSegment`).', 'commenter': 'KazydubB'}]"
1854,logical/src/main/java/org/apache/drill/common/expression/PathSegment.java,"@@ -17,13 +17,29 @@
  */
 package org.apache.drill.common.expression;
 
+import org.apache.drill.common.types.TypeProtos;
+
 public abstract class PathSegment {
 
+  /**
+   * Holds original value associated with the path segment.
+   * Used when reading data from DICT.
+   */
+  protected final Object originalValue;
+
+  /**
+   * Indicates the type of original value.
+   * @see #originalValue
+   */
+  protected final TypeProtos.MajorType valueType;","[{'comment': '```suggestion\r\n  protected final TypeProtos.MajorType originalValueType;\r\n```', 'commenter': 'ihuzenko'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;","[{'comment': 'No need for = 0', 'commenter': 'paul-rogers'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);","[{'comment': 'Nit: statics usually go at the top of a class', 'commenter': 'paul-rogers'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;","[{'comment': 'If this config holds only a single item, then it can be omitted. Just pass teh plugin into the batch reader instead of this class.', 'commenter': 'paul-rogers'}, {'comment': ""I'd agree.  However, the next step for this plugin is that I'm going to add packet dissectors.  Since there will likely be a performance hit, I'm going to also add a configuration option to disable them and then the reader config won't be useless.  So I'd like to leave it for now, even though at the moment, it serves no purpose. "", 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;
+    public PcapReaderConfig(PcapFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PcapBatchReader(PcapReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    openFile(negotiator);
+    SchemaBuilder builder = new SchemaBuilder();
+    this.pcapSchema = new Schema();
+    TupleMetadata schema = pcapSchema.buildSchema(builder);
+    negotiator.setTableSchema(schema, false);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!parseNextPacket(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      fsStream.close();
+    } catch (IOException e) {
+      throw UserException.
+        dataReadError()
+        .addContext(""Error closing InputStream: "" + e.getMessage())
+        .build(logger);
+    }
+    fsStream = null;
+    this.buffer = null;
+    this.decoder = null;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    String filePath = null;
+    try {
+      filePath = split.getPath().toString();
+      this.fsStream = negotiator.fileSystem().open(new Path(filePath));
+      this.decoder = new PacketDecoder(fsStream);
+      this.buffer = new byte[BUFFER_SIZE + decoder.getMaxLength()];
+      this.validBytes = fsStream.read(buffer);","[{'comment': 'Nit: The this. prefix is not needed.', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;
+    public PcapReaderConfig(PcapFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PcapBatchReader(PcapReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    openFile(negotiator);
+    SchemaBuilder builder = new SchemaBuilder();
+    this.pcapSchema = new Schema();
+    TupleMetadata schema = pcapSchema.buildSchema(builder);
+    negotiator.setTableSchema(schema, false);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!parseNextPacket(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      fsStream.close();
+    } catch (IOException e) {
+      throw UserException.
+        dataReadError()
+        .addContext(""Error closing InputStream: "" + e.getMessage())
+        .build(logger);
+    }
+    fsStream = null;
+    this.buffer = null;
+    this.decoder = null;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    String filePath = null;
+    try {
+      filePath = split.getPath().toString();
+      this.fsStream = negotiator.fileSystem().open(new Path(filePath));
+      this.decoder = new PacketDecoder(fsStream);
+      this.buffer = new byte[BUFFER_SIZE + decoder.getMaxLength()];
+      this.validBytes = fsStream.read(buffer);
+    } catch (IOException io) {
+      throw UserException.dataReadError(io).addContext(""File name:"", filePath).build(logger);
+    }
+  }
+
+  private boolean parseNextPacket(RowSetLoader rowWriter){
+    Packet packet = new Packet();
+
+    if(offset >= validBytes){","[{'comment': 'Nit: space before {', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;
+    public PcapReaderConfig(PcapFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PcapBatchReader(PcapReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    openFile(negotiator);
+    SchemaBuilder builder = new SchemaBuilder();
+    this.pcapSchema = new Schema();
+    TupleMetadata schema = pcapSchema.buildSchema(builder);
+    negotiator.setTableSchema(schema, false);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!parseNextPacket(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      fsStream.close();
+    } catch (IOException e) {
+      throw UserException.
+        dataReadError()
+        .addContext(""Error closing InputStream: "" + e.getMessage())
+        .build(logger);
+    }
+    fsStream = null;
+    this.buffer = null;
+    this.decoder = null;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    String filePath = null;
+    try {
+      filePath = split.getPath().toString();
+      this.fsStream = negotiator.fileSystem().open(new Path(filePath));
+      this.decoder = new PacketDecoder(fsStream);
+      this.buffer = new byte[BUFFER_SIZE + decoder.getMaxLength()];
+      this.validBytes = fsStream.read(buffer);
+    } catch (IOException io) {
+      throw UserException.dataReadError(io).addContext(""File name:"", filePath).build(logger);
+    }
+  }
+
+  private boolean parseNextPacket(RowSetLoader rowWriter){
+    Packet packet = new Packet();
+
+    if(offset >= validBytes){
+      return false;
+    }
+    if (validBytes - offset < decoder.getMaxLength()) {
+      getNextPacket(rowWriter);
+    }
+
+    if(packet == null) {","[{'comment': 'Nit: space after if', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;
+    public PcapReaderConfig(PcapFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PcapBatchReader(PcapReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    openFile(negotiator);
+    SchemaBuilder builder = new SchemaBuilder();
+    this.pcapSchema = new Schema();
+    TupleMetadata schema = pcapSchema.buildSchema(builder);
+    negotiator.setTableSchema(schema, false);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!parseNextPacket(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      fsStream.close();
+    } catch (IOException e) {
+      throw UserException.
+        dataReadError()
+        .addContext(""Error closing InputStream: "" + e.getMessage())
+        .build(logger);
+    }
+    fsStream = null;
+    this.buffer = null;
+    this.decoder = null;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    String filePath = null;
+    try {
+      filePath = split.getPath().toString();
+      this.fsStream = negotiator.fileSystem().open(new Path(filePath));
+      this.decoder = new PacketDecoder(fsStream);
+      this.buffer = new byte[BUFFER_SIZE + decoder.getMaxLength()];
+      this.validBytes = fsStream.read(buffer);
+    } catch (IOException io) {
+      throw UserException.dataReadError(io).addContext(""File name:"", filePath).build(logger);
+    }
+  }
+
+  private boolean parseNextPacket(RowSetLoader rowWriter){
+    Packet packet = new Packet();
+
+    if(offset >= validBytes){
+      return false;
+    }
+    if (validBytes - offset < decoder.getMaxLength()) {
+      getNextPacket(rowWriter);
+    }
+
+    if(packet == null) {
+      return false;
+    }
+    int old = offset;
+    offset = decoder.decodePacket(buffer, offset, packet, decoder.getMaxLength(), validBytes);
+    if (offset > validBytes) {
+      logger.error(""Invalid packet at offset {}"", old);
+    }
+    addDataToTable(packet, decoder.getNetwork(), rowWriter);
+
+    return true;
+  }
+
+  private boolean getNextPacket(RowSetLoader rowWriter) {
+    Packet packet = new Packet();
+    try {
+      if (validBytes == buffer.length) {
+        // shift data and read more. This is the common case.
+        System.arraycopy(buffer, offset, buffer, 0, validBytes - offset);
+        validBytes = validBytes - offset;
+        offset = 0;
+
+        int n = fsStream.read(buffer, validBytes, buffer.length - validBytes);
+        if (n > 0) {
+          validBytes += n;
+        }
+        logger.info(""read {} bytes, at {} offset"", n, validBytes);
+      } else {
+        // near the end of the file, we will just top up the buffer without shifting
+        int n = fsStream.read(buffer, offset, buffer.length - offset);
+        if (n > 0) {
+          validBytes = validBytes + n;
+          logger.info(""Topped up buffer with {} bytes to yield {}\n"", n, validBytes);
+        }
+      }
+    } catch (Exception e) {","[{'comment': 'We want to silently ignore any error and pretend it is EOF? Or, the exception is how this thing reports EOF? If so, a comment might be helpful.', 'commenter': 'paul-rogers'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;
+    public PcapReaderConfig(PcapFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PcapBatchReader(PcapReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    openFile(negotiator);
+    SchemaBuilder builder = new SchemaBuilder();
+    this.pcapSchema = new Schema();
+    TupleMetadata schema = pcapSchema.buildSchema(builder);
+    negotiator.setTableSchema(schema, false);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!parseNextPacket(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      fsStream.close();
+    } catch (IOException e) {
+      throw UserException.
+        dataReadError()
+        .addContext(""Error closing InputStream: "" + e.getMessage())
+        .build(logger);
+    }
+    fsStream = null;
+    this.buffer = null;
+    this.decoder = null;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    String filePath = null;
+    try {
+      filePath = split.getPath().toString();
+      this.fsStream = negotiator.fileSystem().open(new Path(filePath));
+      this.decoder = new PacketDecoder(fsStream);
+      this.buffer = new byte[BUFFER_SIZE + decoder.getMaxLength()];
+      this.validBytes = fsStream.read(buffer);
+    } catch (IOException io) {
+      throw UserException.dataReadError(io).addContext(""File name:"", filePath).build(logger);
+    }
+  }
+
+  private boolean parseNextPacket(RowSetLoader rowWriter){
+    Packet packet = new Packet();
+
+    if(offset >= validBytes){
+      return false;
+    }
+    if (validBytes - offset < decoder.getMaxLength()) {
+      getNextPacket(rowWriter);
+    }
+
+    if(packet == null) {
+      return false;
+    }
+    int old = offset;
+    offset = decoder.decodePacket(buffer, offset, packet, decoder.getMaxLength(), validBytes);
+    if (offset > validBytes) {
+      logger.error(""Invalid packet at offset {}"", old);
+    }
+    addDataToTable(packet, decoder.getNetwork(), rowWriter);
+
+    return true;
+  }
+
+  private boolean getNextPacket(RowSetLoader rowWriter) {
+    Packet packet = new Packet();
+    try {
+      if (validBytes == buffer.length) {
+        // shift data and read more. This is the common case.
+        System.arraycopy(buffer, offset, buffer, 0, validBytes - offset);
+        validBytes = validBytes - offset;
+        offset = 0;
+
+        int n = fsStream.read(buffer, validBytes, buffer.length - validBytes);
+        if (n > 0) {
+          validBytes += n;
+        }
+        logger.info(""read {} bytes, at {} offset"", n, validBytes);
+      } else {
+        // near the end of the file, we will just top up the buffer without shifting
+        int n = fsStream.read(buffer, offset, buffer.length - offset);
+        if (n > 0) {
+          validBytes = validBytes + n;
+          logger.info(""Topped up buffer with {} bytes to yield {}\n"", n, validBytes);
+        }
+      }
+    } catch (Exception e) {
+      return false;
+    }
+    return true;
+  }
+
+  private boolean addDataToTable(Packet packet, int networkType,  RowSetLoader rowWriter) {
+    rowWriter.start();
+    writeStringColumn(rowWriter, ""type"", packet.getPacketType());","[{'comment': 'While this works, we could do better. The code earlier created a schema. Ensure that schema contains all possible columns and their data types. We can do that because we are hard-coding the columns here.\r\n\r\nAs created, we are doing a hash lookup for every column. Since the set of columns are fixed, we can get them all up front and use them directly:\r\n\r\n```\r\n  // In open:\r\n  ScalarWriter typeWriter = rowWriter.scalar(""type"");\r\n  // In addDataToTable\r\n  typeWriter.setString( packet.getPacketType() );\r\n```\r\nOf course, the variable has to be a field, not a local variable...\r\n\r\nThis works because you declared all columns in the schema, so the result set loader creates column writers for each. If it turns out that a column is unprojected, then the column writer is a ""dummy"", it will simply discard the value.', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;
+    public PcapReaderConfig(PcapFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PcapBatchReader(PcapReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    openFile(negotiator);
+    SchemaBuilder builder = new SchemaBuilder();
+    this.pcapSchema = new Schema();
+    TupleMetadata schema = pcapSchema.buildSchema(builder);
+    negotiator.setTableSchema(schema, false);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!parseNextPacket(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      fsStream.close();
+    } catch (IOException e) {
+      throw UserException.
+        dataReadError()
+        .addContext(""Error closing InputStream: "" + e.getMessage())
+        .build(logger);
+    }
+    fsStream = null;
+    this.buffer = null;
+    this.decoder = null;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    String filePath = null;
+    try {
+      filePath = split.getPath().toString();
+      this.fsStream = negotiator.fileSystem().open(new Path(filePath));
+      this.decoder = new PacketDecoder(fsStream);
+      this.buffer = new byte[BUFFER_SIZE + decoder.getMaxLength()];
+      this.validBytes = fsStream.read(buffer);
+    } catch (IOException io) {
+      throw UserException.dataReadError(io).addContext(""File name:"", filePath).build(logger);
+    }
+  }
+
+  private boolean parseNextPacket(RowSetLoader rowWriter){
+    Packet packet = new Packet();
+
+    if(offset >= validBytes){
+      return false;
+    }
+    if (validBytes - offset < decoder.getMaxLength()) {
+      getNextPacket(rowWriter);
+    }
+
+    if(packet == null) {
+      return false;
+    }
+    int old = offset;
+    offset = decoder.decodePacket(buffer, offset, packet, decoder.getMaxLength(), validBytes);
+    if (offset > validBytes) {
+      logger.error(""Invalid packet at offset {}"", old);
+    }
+    addDataToTable(packet, decoder.getNetwork(), rowWriter);
+
+    return true;
+  }
+
+  private boolean getNextPacket(RowSetLoader rowWriter) {
+    Packet packet = new Packet();
+    try {
+      if (validBytes == buffer.length) {
+        // shift data and read more. This is the common case.
+        System.arraycopy(buffer, offset, buffer, 0, validBytes - offset);
+        validBytes = validBytes - offset;
+        offset = 0;
+
+        int n = fsStream.read(buffer, validBytes, buffer.length - validBytes);
+        if (n > 0) {
+          validBytes += n;
+        }
+        logger.info(""read {} bytes, at {} offset"", n, validBytes);
+      } else {
+        // near the end of the file, we will just top up the buffer without shifting
+        int n = fsStream.read(buffer, offset, buffer.length - offset);
+        if (n > 0) {
+          validBytes = validBytes + n;
+          logger.info(""Topped up buffer with {} bytes to yield {}\n"", n, validBytes);
+        }
+      }
+    } catch (Exception e) {
+      return false;
+    }
+    return true;
+  }
+
+  private boolean addDataToTable(Packet packet, int networkType,  RowSetLoader rowWriter) {
+    rowWriter.start();
+    writeStringColumn(rowWriter, ""type"", packet.getPacketType());
+    writeTimestampColumn(rowWriter, ""timestamp"", new Instant(packet.getTimestamp()));
+    writeLongValue(rowWriter, ""timestamp_micro"", packet.getTimestampMicro());
+    writeIntColumn(rowWriter, ""network"", networkType);
+    writeStringColumn(rowWriter, ""src_mac_address"", packet.getEthernetSource());
+    writeStringColumn(rowWriter, ""dst_mac_address"", packet.getEthernetDestination());
+
+    if (packet.getDst_ip() != null) {
+      writeStringColumn(rowWriter, ""dst_ip"", packet.getDst_ip().getHostAddress());
+    } else {
+      writeStringColumn(rowWriter, ""dst_ip"", null);
+    }
+    if (packet.getSrc_ip() != null) {","[{'comment': 'If you go with the above, no need for the if-statement. the `setString()` method accepts a `null` value which it interprets as a request for a SQL `NULL` value. That is, the right thing will happen automagically.', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;
+    public PcapReaderConfig(PcapFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PcapBatchReader(PcapReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    openFile(negotiator);
+    SchemaBuilder builder = new SchemaBuilder();
+    this.pcapSchema = new Schema();
+    TupleMetadata schema = pcapSchema.buildSchema(builder);
+    negotiator.setTableSchema(schema, false);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!parseNextPacket(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      fsStream.close();
+    } catch (IOException e) {
+      throw UserException.
+        dataReadError()
+        .addContext(""Error closing InputStream: "" + e.getMessage())
+        .build(logger);
+    }
+    fsStream = null;
+    this.buffer = null;
+    this.decoder = null;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    String filePath = null;
+    try {
+      filePath = split.getPath().toString();
+      this.fsStream = negotiator.fileSystem().open(new Path(filePath));
+      this.decoder = new PacketDecoder(fsStream);
+      this.buffer = new byte[BUFFER_SIZE + decoder.getMaxLength()];
+      this.validBytes = fsStream.read(buffer);
+    } catch (IOException io) {
+      throw UserException.dataReadError(io).addContext(""File name:"", filePath).build(logger);
+    }
+  }
+
+  private boolean parseNextPacket(RowSetLoader rowWriter){
+    Packet packet = new Packet();
+
+    if(offset >= validBytes){
+      return false;
+    }
+    if (validBytes - offset < decoder.getMaxLength()) {
+      getNextPacket(rowWriter);
+    }
+
+    if(packet == null) {
+      return false;
+    }
+    int old = offset;
+    offset = decoder.decodePacket(buffer, offset, packet, decoder.getMaxLength(), validBytes);
+    if (offset > validBytes) {
+      logger.error(""Invalid packet at offset {}"", old);
+    }
+    addDataToTable(packet, decoder.getNetwork(), rowWriter);
+
+    return true;
+  }
+
+  private boolean getNextPacket(RowSetLoader rowWriter) {
+    Packet packet = new Packet();
+    try {
+      if (validBytes == buffer.length) {
+        // shift data and read more. This is the common case.
+        System.arraycopy(buffer, offset, buffer, 0, validBytes - offset);
+        validBytes = validBytes - offset;
+        offset = 0;
+
+        int n = fsStream.read(buffer, validBytes, buffer.length - validBytes);
+        if (n > 0) {
+          validBytes += n;
+        }
+        logger.info(""read {} bytes, at {} offset"", n, validBytes);
+      } else {
+        // near the end of the file, we will just top up the buffer without shifting
+        int n = fsStream.read(buffer, offset, buffer.length - offset);
+        if (n > 0) {
+          validBytes = validBytes + n;
+          logger.info(""Topped up buffer with {} bytes to yield {}\n"", n, validBytes);
+        }
+      }
+    } catch (Exception e) {
+      return false;
+    }
+    return true;
+  }
+
+  private boolean addDataToTable(Packet packet, int networkType,  RowSetLoader rowWriter) {
+    rowWriter.start();
+    writeStringColumn(rowWriter, ""type"", packet.getPacketType());
+    writeTimestampColumn(rowWriter, ""timestamp"", new Instant(packet.getTimestamp()));
+    writeLongValue(rowWriter, ""timestamp_micro"", packet.getTimestampMicro());
+    writeIntColumn(rowWriter, ""network"", networkType);
+    writeStringColumn(rowWriter, ""src_mac_address"", packet.getEthernetSource());
+    writeStringColumn(rowWriter, ""dst_mac_address"", packet.getEthernetDestination());
+
+    if (packet.getDst_ip() != null) {
+      writeStringColumn(rowWriter, ""dst_ip"", packet.getDst_ip().getHostAddress());
+    } else {
+      writeStringColumn(rowWriter, ""dst_ip"", null);
+    }
+    if (packet.getSrc_ip() != null) {
+      writeStringColumn(rowWriter, ""src_ip"", packet.getSrc_ip().getHostAddress());
+    } else {
+      writeStringColumn(rowWriter, ""src_ip"", null);
+    }
+    writeIntColumn(rowWriter, ""src_port"", packet.getSrc_port());
+    writeIntColumn(rowWriter, ""dst_port"", packet.getDst_port());
+    writeIntColumn(rowWriter, ""packet_length"", packet.getPacketLength());
+
+    // TCP Only Packet Data
+    if (packet.isTcpPacket()) {
+      writeLongValue(rowWriter, ""tcp_session"", packet.getSessionHash());
+      writeIntColumn(rowWriter, ""tcp_sequence"", packet.getSequenceNumber());
+      writeBooleanColumn(rowWriter, ""tcp_ack"", packet.getAckNumber());
+      writeIntColumn(rowWriter, ""tcp_flags"", packet.getFlags());
+      writeStringColumn(rowWriter, ""tcp_parsed_flags"", packet.getParsedFlags());
+      writeBooleanColumn(rowWriter, ""tcp_flags_ns"", (packet.getFlags() & 0x100) != 0);","[{'comment': ""Here you could even be more clever. Since the only difference in many of these lines is the column name and the mask, you can create an array of mappings:\r\n\r\n```\r\nclass FieldMapping {\r\n  ScalarWriter writer;\r\n  int mask;\r\n\r\n  void write(int flags) {\r\n    writer.setBoolean((flags & mask) != 0);\r\n  }\r\n}\r\n```\r\n\r\nIn open, you'd create a list of these mappings, then would spin though them in this function."", 'commenter': 'paul-rogers'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java,"@@ -0,0 +1,295 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcap;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.pcap.decoder.Packet;
+import org.apache.drill.exec.store.pcap.decoder.PacketDecoder;
+import org.apache.drill.exec.store.pcap.schema.Schema;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import org.apache.hadoop.fs.Path;
+
+import static org.apache.drill.exec.store.pcap.PcapFormatUtils.parseBytesToASCII;
+
+public class PcapBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private FileSplit split;
+
+  private PcapReaderConfig readerConfig;
+
+  private PacketDecoder decoder;
+
+  private ResultSetLoader loader;
+
+  private FSDataInputStream fsStream;
+
+  private Schema pcapSchema;
+
+  private int validBytes;
+
+  private byte[] buffer;
+
+  private int offset = 0;
+
+  static final int BUFFER_SIZE = 500_000;
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapBatchReader.class);
+
+  public static class PcapReaderConfig {
+
+    protected final PcapFormatPlugin plugin;
+    public PcapReaderConfig(PcapFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PcapBatchReader(PcapReaderConfig readerConfig) {
+    this.readerConfig = readerConfig;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    split = negotiator.split();
+    openFile(negotiator);
+    SchemaBuilder builder = new SchemaBuilder();
+    this.pcapSchema = new Schema();
+    TupleMetadata schema = pcapSchema.buildSchema(builder);
+    negotiator.setTableSchema(schema, false);
+    this.loader = negotiator.build();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!parseNextPacket(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      fsStream.close();
+    } catch (IOException e) {
+      throw UserException.
+        dataReadError()
+        .addContext(""Error closing InputStream: "" + e.getMessage())
+        .build(logger);
+    }
+    fsStream = null;
+    this.buffer = null;
+    this.decoder = null;
+  }
+
+  private void openFile(FileSchemaNegotiator negotiator) {
+    String filePath = null;
+    try {
+      filePath = split.getPath().toString();
+      this.fsStream = negotiator.fileSystem().open(new Path(filePath));
+      this.decoder = new PacketDecoder(fsStream);
+      this.buffer = new byte[BUFFER_SIZE + decoder.getMaxLength()];
+      this.validBytes = fsStream.read(buffer);
+    } catch (IOException io) {
+      throw UserException.dataReadError(io).addContext(""File name:"", filePath).build(logger);
+    }
+  }
+
+  private boolean parseNextPacket(RowSetLoader rowWriter){
+    Packet packet = new Packet();
+
+    if(offset >= validBytes){
+      return false;
+    }
+    if (validBytes - offset < decoder.getMaxLength()) {
+      getNextPacket(rowWriter);
+    }
+
+    if(packet == null) {
+      return false;
+    }
+    int old = offset;
+    offset = decoder.decodePacket(buffer, offset, packet, decoder.getMaxLength(), validBytes);
+    if (offset > validBytes) {
+      logger.error(""Invalid packet at offset {}"", old);
+    }
+    addDataToTable(packet, decoder.getNetwork(), rowWriter);
+
+    return true;
+  }
+
+  private boolean getNextPacket(RowSetLoader rowWriter) {
+    Packet packet = new Packet();
+    try {
+      if (validBytes == buffer.length) {
+        // shift data and read more. This is the common case.
+        System.arraycopy(buffer, offset, buffer, 0, validBytes - offset);
+        validBytes = validBytes - offset;
+        offset = 0;
+
+        int n = fsStream.read(buffer, validBytes, buffer.length - validBytes);
+        if (n > 0) {
+          validBytes += n;
+        }
+        logger.info(""read {} bytes, at {} offset"", n, validBytes);
+      } else {
+        // near the end of the file, we will just top up the buffer without shifting
+        int n = fsStream.read(buffer, offset, buffer.length - offset);
+        if (n > 0) {
+          validBytes = validBytes + n;
+          logger.info(""Topped up buffer with {} bytes to yield {}\n"", n, validBytes);
+        }
+      }
+    } catch (Exception e) {
+      return false;
+    }
+    return true;
+  }
+
+  private boolean addDataToTable(Packet packet, int networkType,  RowSetLoader rowWriter) {
+    rowWriter.start();
+    writeStringColumn(rowWriter, ""type"", packet.getPacketType());
+    writeTimestampColumn(rowWriter, ""timestamp"", new Instant(packet.getTimestamp()));
+    writeLongValue(rowWriter, ""timestamp_micro"", packet.getTimestampMicro());
+    writeIntColumn(rowWriter, ""network"", networkType);
+    writeStringColumn(rowWriter, ""src_mac_address"", packet.getEthernetSource());
+    writeStringColumn(rowWriter, ""dst_mac_address"", packet.getEthernetDestination());
+
+    if (packet.getDst_ip() != null) {
+      writeStringColumn(rowWriter, ""dst_ip"", packet.getDst_ip().getHostAddress());
+    } else {
+      writeStringColumn(rowWriter, ""dst_ip"", null);
+    }
+    if (packet.getSrc_ip() != null) {
+      writeStringColumn(rowWriter, ""src_ip"", packet.getSrc_ip().getHostAddress());
+    } else {
+      writeStringColumn(rowWriter, ""src_ip"", null);
+    }
+    writeIntColumn(rowWriter, ""src_port"", packet.getSrc_port());
+    writeIntColumn(rowWriter, ""dst_port"", packet.getDst_port());
+    writeIntColumn(rowWriter, ""packet_length"", packet.getPacketLength());
+
+    // TCP Only Packet Data
+    if (packet.isTcpPacket()) {
+      writeLongValue(rowWriter, ""tcp_session"", packet.getSessionHash());
+      writeIntColumn(rowWriter, ""tcp_sequence"", packet.getSequenceNumber());
+      writeBooleanColumn(rowWriter, ""tcp_ack"", packet.getAckNumber());
+      writeIntColumn(rowWriter, ""tcp_flags"", packet.getFlags());
+      writeStringColumn(rowWriter, ""tcp_parsed_flags"", packet.getParsedFlags());
+      writeBooleanColumn(rowWriter, ""tcp_flags_ns"", (packet.getFlags() & 0x100) != 0);
+      writeBooleanColumn(rowWriter, ""tcp_flags_cwr"", (packet.getFlags() & 0x80) != 0);
+      writeBooleanColumn(rowWriter, ""tcp_flags_ece"", (packet.getFlags() & 0x40) != 0);
+      writeBooleanColumn(rowWriter, ""tcp_flags_ece_ecn_capable"", (packet.getFlags() & 0x42) == 0x42);
+      writeBooleanColumn(rowWriter, ""tcp_flags_ece_congestion_experienced"", (packet.getFlags() & 0x42) == 0x40);
+      writeBooleanColumn(rowWriter, ""tcp_flags_urg"", (packet.getFlags() & 0x20) != 0);
+      writeBooleanColumn(rowWriter, ""tcp_flags_ack"", (packet.getFlags() & 0x10) != 0);
+      writeBooleanColumn(rowWriter, ""tcp_flags_psh"", (packet.getFlags() & 0x8) != 0);
+      writeBooleanColumn(rowWriter, ""tcp_flags_rst"", (packet.getFlags() & 0x4) != 0);
+      writeBooleanColumn(rowWriter, ""tcp_flags_syn"", (packet.getFlags() & 0x2) != 0);
+      writeBooleanColumn(rowWriter, ""tcp_flags_fin"", (packet.getFlags() & 0x1) != 0);
+    }
+    if (packet.getData() != null) {
+      writeStringColumn(rowWriter, ""data"", parseBytesToASCII(packet.getData()));
+    } else {
+      writeStringColumn(rowWriter, ""data"", ""[]"");
+    }
+    writeBooleanColumn(rowWriter, ""is_corrupt"", packet.isCorrupt());
+    rowWriter.save();
+
+    // TODO Parse Data Packet Here
+    return true;
+  }
+
+  private void writeTimestampColumn(TupleWriter rowWriter, String name, Instant ts) {","[{'comment': 'See comments above. This level of dynamic column handling is not needed and these methods can be removed.', 'commenter': 'paul-rogers'}, {'comment': 'I removed all these methods.', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java,"@@ -17,12 +17,27 @@
  */
 package org.apache.drill.exec.store.pcap;
 
+import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonTypeName;
 import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+
+import java.util.List;
 
 @JsonTypeName(""pcap"")
 public class PcapFormatConfig implements FormatPluginConfig {
 
+  private static final List<String> DEFAULT_EXTS = ImmutableList.of(""pcap"");","[{'comment': 'Best if we define `""pcap""` to be a constant and use that constant here and above.', 'commenter': 'paul-rogers'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java,"@@ -17,12 +17,27 @@
  */
 package org.apache.drill.exec.store.pcap;
 
+import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonTypeName;
 import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+
+import java.util.List;
 
 @JsonTypeName(""pcap"")
 public class PcapFormatConfig implements FormatPluginConfig {
 
+  private static final List<String> DEFAULT_EXTS = ImmutableList.of(""pcap"");
+  public List<String> extensions;
+
+  @JsonInclude(JsonInclude.Include.NON_DEFAULT)
+  public List<String> getExtensions() {
+    if (extensions == null) {
+      return DEFAULT_EXTS;
+    }
+    return extensions;","[{'comment': 'Nit: `return extensions == null ? DEFAULT_EXTS : extensions`', 'commenter': 'paul-rogers'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java,"@@ -17,112 +17,71 @@
  */
 package org.apache.drill.exec.store.pcap;
 
-import org.apache.drill.exec.planner.common.DrillStatsTable;
-import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileReaderFactory;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileScanBuilder;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.server.options.OptionManager;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
 import org.apache.drill.common.exceptions.ExecutionSetupException;
-import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.common.logical.StoragePluginConfig;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.planner.logical.DrillTable;
 import org.apache.drill.exec.proto.UserBitShared;
 import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.RecordReader;
-import org.apache.drill.exec.store.RecordWriter;
-import org.apache.drill.exec.store.SchemaConfig;
-import org.apache.drill.exec.store.dfs.BasicFormatMatcher;
-import org.apache.drill.exec.store.dfs.DrillFileSystem;
-import org.apache.drill.exec.store.dfs.FileSelection;
-import org.apache.drill.exec.store.dfs.FileSystemPlugin;
-import org.apache.drill.exec.store.dfs.FormatMatcher;
-import org.apache.drill.exec.store.dfs.FormatSelection;
-import org.apache.drill.exec.store.dfs.MagicString;
 import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;
-import org.apache.drill.exec.store.dfs.easy.EasyWriter;
-import org.apache.drill.exec.store.dfs.easy.FileWork;
 import org.apache.hadoop.conf.Configuration;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.regex.Pattern;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
+import org.apache.drill.exec.store.pcap.PcapBatchReader.PcapReaderConfig;
 
 public class PcapFormatPlugin extends EasyFormatPlugin<PcapFormatConfig> {
 
-  private final PcapFormatMatcher matcher;
-
-  public PcapFormatPlugin(String name, DrillbitContext context, Configuration fsConf,
-                          StoragePluginConfig storagePluginConfig) {
-    this(name, context, fsConf, storagePluginConfig, new PcapFormatConfig());
-  }
-
-  public PcapFormatPlugin(String name, DrillbitContext context, Configuration fsConf, StoragePluginConfig config, PcapFormatConfig formatPluginConfig) {
-    super(name, context, fsConf, config, formatPluginConfig, true, false, true, false, Lists.newArrayList(""pcap""), ""pcap"");
-    this.matcher = new PcapFormatMatcher(this);
-  }
-
-  @Override
-  public boolean supportsPushDown() {
-    return true;
-  }
-
-  @Override
-  public RecordReader getRecordReader(FragmentContext context, DrillFileSystem dfs, FileWork fileWork, List<SchemaPath> columns, String userName) throws ExecutionSetupException {
-    return new PcapRecordReader(fileWork.getPath(), dfs, columns);
-  }
-
-  @Override
-  public RecordWriter getRecordWriter(FragmentContext context, EasyWriter writer) throws IOException {
-    throw new UnsupportedOperationException(""unimplemented"");
-  }
+  private static class PcapReaderFactory extends FileReaderFactory {
+    private final PcapReaderConfig readerConfig;
 
-  @Override
-  public int getReaderOperatorType() {
-    return UserBitShared.CoreOperatorType.PCAP_SUB_SCAN_VALUE;
-  }
+    public PcapReaderFactory(PcapReaderConfig config) {
+      readerConfig = config;
+    }
 
-  @Override
-  public int getWriterOperatorType() {
-    throw new UnsupportedOperationException();
+    @Override
+    public ManagedReader<? extends FileSchemaNegotiator> newReader() {
+      return new PcapBatchReader(readerConfig);
+    }
   }
-
-  @Override
-  public FormatMatcher getMatcher() {
-    return this.matcher;
+  public PcapFormatPlugin(String name, DrillbitContext context,
+                           Configuration fsConf, StoragePluginConfig storageConfig,
+                           PcapFormatConfig formatConfig) {
+    super(name, easyConfig(fsConf, formatConfig), context, storageConfig, formatConfig);
   }
 
-  @Override
-  public boolean supportsStatistics() {
-    return false;
+  private static EasyFormatConfig easyConfig(Configuration fsConf, PcapFormatConfig pluginConfig) {
+    EasyFormatConfig config = new EasyFormatConfig();
+    config.readable = true;
+    config.writable = false;
+    config.blockSplittable = true;
+    config.compressible = true;
+    config.supportsProjectPushdown = true;
+    config.extensions = pluginConfig.getExtensions();
+    config.fsConf = fsConf;
+    config.defaultName = ""pcap"";","[{'comment': 'Use the constant suggested above.', 'commenter': 'paul-rogers'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java,"@@ -17,112 +17,71 @@
  */
 package org.apache.drill.exec.store.pcap;
 
-import org.apache.drill.exec.planner.common.DrillStatsTable;
-import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileReaderFactory;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileScanBuilder;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.server.options.OptionManager;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
 import org.apache.drill.common.exceptions.ExecutionSetupException;
-import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.common.logical.StoragePluginConfig;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.planner.logical.DrillTable;
 import org.apache.drill.exec.proto.UserBitShared;
 import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.RecordReader;
-import org.apache.drill.exec.store.RecordWriter;
-import org.apache.drill.exec.store.SchemaConfig;
-import org.apache.drill.exec.store.dfs.BasicFormatMatcher;
-import org.apache.drill.exec.store.dfs.DrillFileSystem;
-import org.apache.drill.exec.store.dfs.FileSelection;
-import org.apache.drill.exec.store.dfs.FileSystemPlugin;
-import org.apache.drill.exec.store.dfs.FormatMatcher;
-import org.apache.drill.exec.store.dfs.FormatSelection;
-import org.apache.drill.exec.store.dfs.MagicString;
 import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;
-import org.apache.drill.exec.store.dfs.easy.EasyWriter;
-import org.apache.drill.exec.store.dfs.easy.FileWork;
 import org.apache.hadoop.conf.Configuration;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.regex.Pattern;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
+import org.apache.drill.exec.store.pcap.PcapBatchReader.PcapReaderConfig;
 
 public class PcapFormatPlugin extends EasyFormatPlugin<PcapFormatConfig> {
 
-  private final PcapFormatMatcher matcher;
-
-  public PcapFormatPlugin(String name, DrillbitContext context, Configuration fsConf,
-                          StoragePluginConfig storagePluginConfig) {
-    this(name, context, fsConf, storagePluginConfig, new PcapFormatConfig());
-  }
-
-  public PcapFormatPlugin(String name, DrillbitContext context, Configuration fsConf, StoragePluginConfig config, PcapFormatConfig formatPluginConfig) {
-    super(name, context, fsConf, config, formatPluginConfig, true, false, true, false, Lists.newArrayList(""pcap""), ""pcap"");
-    this.matcher = new PcapFormatMatcher(this);
-  }
-
-  @Override
-  public boolean supportsPushDown() {
-    return true;
-  }
-
-  @Override
-  public RecordReader getRecordReader(FragmentContext context, DrillFileSystem dfs, FileWork fileWork, List<SchemaPath> columns, String userName) throws ExecutionSetupException {
-    return new PcapRecordReader(fileWork.getPath(), dfs, columns);
-  }
-
-  @Override
-  public RecordWriter getRecordWriter(FragmentContext context, EasyWriter writer) throws IOException {
-    throw new UnsupportedOperationException(""unimplemented"");
-  }
+  private static class PcapReaderFactory extends FileReaderFactory {
+    private final PcapReaderConfig readerConfig;
 
-  @Override
-  public int getReaderOperatorType() {
-    return UserBitShared.CoreOperatorType.PCAP_SUB_SCAN_VALUE;
-  }
+    public PcapReaderFactory(PcapReaderConfig config) {
+      readerConfig = config;
+    }
 
-  @Override
-  public int getWriterOperatorType() {
-    throw new UnsupportedOperationException();
+    @Override
+    public ManagedReader<? extends FileSchemaNegotiator> newReader() {
+      return new PcapBatchReader(readerConfig);
+    }
   }
-
-  @Override
-  public FormatMatcher getMatcher() {
-    return this.matcher;
+  public PcapFormatPlugin(String name, DrillbitContext context,
+                           Configuration fsConf, StoragePluginConfig storageConfig,
+                           PcapFormatConfig formatConfig) {
+    super(name, easyConfig(fsConf, formatConfig), context, storageConfig, formatConfig);
   }
 
-  @Override
-  public boolean supportsStatistics() {
-    return false;
+  private static EasyFormatConfig easyConfig(Configuration fsConf, PcapFormatConfig pluginConfig) {
+    EasyFormatConfig config = new EasyFormatConfig();
+    config.readable = true;
+    config.writable = false;
+    config.blockSplittable = true;
+    config.compressible = true;
+    config.supportsProjectPushdown = true;
+    config.extensions = pluginConfig.getExtensions();
+    config.fsConf = fsConf;
+    config.defaultName = ""pcap"";
+    config.readerOperatorType = UserBitShared.CoreOperatorType.PCAP_SUB_SCAN_VALUE;
+    config.useEnhancedScan = true;
+    return config;
   }
 
   @Override
-  public DrillStatsTable.TableStatistics readStatistics(FileSystem fs, Path statsTablePath) throws IOException {
-    return null;
+  public ManagedReader<? extends FileSchemaNegotiator> newBatchReader(
+    EasySubScan scan, OptionManager options) throws ExecutionSetupException {
+    return new PcapBatchReader(new PcapReaderConfig(this));","[{'comment': 'Nit: indent', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/schema/Schema.java,"@@ -78,4 +81,23 @@ public ColumnDto getColumnByIndex(int i) {
   public int getNumberOfColumns() {
     return columns.size();
   }
+
+  public TupleMetadata buildSchema(SchemaBuilder builder) {
+    for(ColumnDto column : columns) {
+      if(column.getColumnType() == PcapTypes.BOOLEAN) {","[{'comment': 'Nit: space after if', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/schema/Schema.java,"@@ -78,4 +81,23 @@ public ColumnDto getColumnByIndex(int i) {
   public int getNumberOfColumns() {
     return columns.size();
   }
+
+  public TupleMetadata buildSchema(SchemaBuilder builder) {
+    for(ColumnDto column : columns) {
+      if(column.getColumnType() == PcapTypes.BOOLEAN) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.BIT);
+      } else if(column.getColumnType() == PcapTypes.INTEGER) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.INT);
+      } else if(column.getColumnType() == PcapTypes.STRING) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.VARCHAR);
+      } else if(column.getColumnType() == PcapTypes.TIMESTAMP) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.TIMESTAMP);
+      } else if(column.getColumnType() == PcapTypes.LONG) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.BIGINT);","[{'comment': 'Nit: would be simpler to do something like the following:\r\n\r\n```\r\n  for (ColumnTdo column : columns) {\r\n    builder.addNullable(column.getColumnName(), convertType(column));\r\n  }\r\n...\r\nMinorType convertType(ColumnDto column) {\r\n  switch (column.getColumnType()) {\r\n    case PcapTypes.BOOLEAN:\r\n      return MinorType.BIT;\r\n   ...\r\n```\r\n\r\nThe above requires that we handle the case of a type that is not in the switch: maybe throw an exception or some such.\r\n\r\nAlso, if the types are integers and configuous (which they are of PcapTypes is an enum), then you can create a mapping table:\r\n\r\n```\r\n  MinorType typeMap[] = new int[PcapTypes.getSize()];\r\n  typeMap[PcapTypes.BOOLEAN.getOrdinal()] = MinorType.BIT;\r\n  ...\r\n```\r\n\r\nThen:\r\n\r\n```\r\n  for (ColumnTdo column : columns) {\r\n    builder.addNullable(column.getColumnName(), typeMap[column.getColumType().getOrdinal());\r\n```\r\n\r\nThe mapping table is the fastest solution. But, this is not super critical in a once-per-file activity.', 'commenter': 'paul-rogers'}, {'comment': 'I feel the need for speed... Fixed and went for the fastest solution. ', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/schema/Schema.java,"@@ -78,4 +81,23 @@ public ColumnDto getColumnByIndex(int i) {
   public int getNumberOfColumns() {
     return columns.size();
   }
+
+  public TupleMetadata buildSchema(SchemaBuilder builder) {
+    for(ColumnDto column : columns) {
+      if(column.getColumnType() == PcapTypes.BOOLEAN) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.BIT);
+      } else if(column.getColumnType() == PcapTypes.INTEGER) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.INT);
+      } else if(column.getColumnType() == PcapTypes.STRING) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.VARCHAR);
+      } else if(column.getColumnType() == PcapTypes.TIMESTAMP) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.TIMESTAMP);
+      } else if(column.getColumnType() == PcapTypes.LONG) {
+        builder.addNullable(column.getColumnName(), TypeProtos.MinorType.BIGINT);
+      }
+    }
+
+    TupleMetadata schema = builder.buildSchema();
+    return schema;","[{'comment': 'Nit: `return builder.buildSchema();`', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1862,exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapRecordReader.java,"@@ -45,8 +45,16 @@ public void testStarQuery() throws Exception {
   @Test
   public void testCorruptPCAPQuery() throws Exception {
     runSQLVerifyCount(""select * from dfs.`store/pcap/testv1.pcap`"", 7000);
-    runSQLVerifyCount(""select * from dfs.`store/pcap/testv1.pcap` WHERE is_corrupt=false"", 6408);
-    runSQLVerifyCount(""select * from dfs.`store/pcap/testv1.pcap` WHERE is_corrupt=true"", 592);
+  }
+
+  @Test
+  public void testTrueCorruptPCAPQuery() throws Exception {
+    runSQLVerifyCount(""select * from dfs.`store/pcap/testv1.pcap` WHERE is_corrupt=true"", 16);
+  }
+
+  @Test
+  public void testNotCorruptPCAPQuery() throws Exception {
+    runSQLVerifyCount(""select * from dfs.`store/pcap/testv1.pcap` WHERE is_corrupt=false"", 6984);","[{'comment': 'Looks like the original test were pretty light. It leaves to the user to test per-column setup and type conversions.\r\n\r\nWould recommend adding tests that:\r\n\r\n1) Verify the schema: names, types\r\n2) Verifies the data (using the usual mechanism to read a few rows and validate the results.)\r\n', 'commenter': 'paul-rogers'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/reader/NullReader.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.accessor.reader;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.record.metadata.VariantMetadata;
+import org.apache.drill.exec.vector.accessor.ArrayReader;
+import org.apache.drill.exec.vector.accessor.ObjectReader;
+import org.apache.drill.exec.vector.accessor.ObjectType;
+import org.apache.drill.exec.vector.accessor.ScalarReader;
+import org.apache.drill.exec.vector.accessor.TupleReader;
+import org.apache.drill.exec.vector.accessor.ValueType;
+import org.apache.drill.exec.vector.accessor.VariantReader;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+
+import java.math.BigDecimal;
+
+/**
+ * Dummy reader which returns {@code null} for scalar types and itself for complex types.
+ */
+public class NullReader implements ScalarReader, ArrayReader, TupleReader, VariantReader, ObjectReader {
+
+  private static final NullReader INSTANCE = new NullReader();
+
+  public static NullReader instance() {
+    return INSTANCE;
+  }
+
+  @Override
+  public int size() {
+    notSupported();","[{'comment': 'Please pass message into `notSupported()` method to show that exactly is not supported and why.', 'commenter': 'arina-ielchiieva'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {","[{'comment': 'Would be great to add a few comments to explain the DICT structure. Is it of the form DICT<KEY, VALUE>, where the key is of one type, the value of another? Or, can keys and values be of different types?\r\n\r\nSince a MAP is a collection of (key, value) pairs in which the keys are declared, the keys are strings, and the values can each be of a distinct type, it is not likely that DICT does the same.\r\n\r\nI will assume the DICT<KEY_TYPE, VALUE_TYPE> format in my comments here.', 'commenter': 'paul-rogers'}, {'comment': ""The `key` can be of any `REQUIRED` primitive type and `value` can be of any primitive or complex types. Dict's key is not limited to be a string.\r\nYes, your assumption is correct."", 'commenter': 'KazydubB'}, {'comment': 'Follow-up question. For the value, is a Dict defined as, say, `DICT<STRING, INT>` so that all values are of the same type? If so, that is the easy case.\r\n\r\nOr, can I define a Dict as `DICT<STRING, UNION<INT, STRING, DOUBLE>>`, meaning that key values can be any one of the three types? Note that, in Drill, we use the (barely-working) Union vector to handle this situation.\r\n\r\nOr, is the Dict trying, itself, to be a Union for the value type?\r\n\r\nJust read the (very nice) comments in `DictVector`, so it looks like, if I want my Dict to hold three types, I make it a `DICT<STRING, UNION>`. Correct?', 'commenter': 'paul-rogers'}, {'comment': ""Yes, `Dict` is defined as `DICT<STRING, INT>` to represent mapping (like in Java's Map<String, Integer>) from `String` to `int`. \r\n`Dict` does not try to be a `Union` for the value type. If one wants the `Dict` to have `Union` value, one must define it as `DICT<STRING, UNION<...>>` (the `key` type is going to be `STRING` and the `value` is going to be a union of some types; like union with all problems it has in Drill :) )"", 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {
+
+  private final SchemaContainer parent;
+  private final TupleBuilder tupleBuilder = new TupleBuilder();
+  private final String memberName;
+  private final TypeProtos.DataMode mode;
+
+  public DictBuilder(String memberName, TypeProtos.DataMode mode) {
+    this(null, memberName, mode);
+  }
+
+  public DictBuilder(SchemaContainer parent, String memberName, TypeProtos.DataMode mode) {
+    this.parent = parent;
+    this.memberName = memberName;
+    this.mode = mode;
+  }
+
+  @Override
+  public void addColumn(ColumnMetadata column) {","[{'comment': 'If we agree DICT is DICT<KEY_TYPE, VALUE_TYPE> then we might want the building of a DICT to be something like:\r\n\r\n```\r\n    TupleMetadata = new SchemaBuilder()\r\n      .addDict(""myDict"")\r\n        .key(MinorType.VARCHAR)\r\n        .valueArray(MinorType.DOUBLE)\r\n        .resumeSchema()\r\n      ...\r\n```\r\n\r\nThat is, we need to add one key. Can keys only be scalars? Only non-nullable? If so, then the single `key()` method is all that is needed. This returns the same builder.\r\n\r\nCan the value be of any type? Then we need the full set of type builders: scalar, array, map and so on.\r\n\r\nThe current form does not make this clear: what does it mean to add a column? This isn\'t a map...\r\n', 'commenter': 'paul-rogers'}, {'comment': 'Yes, naming of the method can be confusing in this case (the method comes from `SchemaContainer`). Added validation to make sure only `key` and `value` fields may be added if they were not previously added.', 'commenter': 'KazydubB'}, {'comment': ""As a side comment, I will take the blame for the mess we are creating in the schema builder. When I first wrote it, I was mostly concerned with the top-level row and with maps. Then came repeated lists, unions and now Dict.\r\n\r\nIs there a cleaner way to handle the nesting? Something clever with parameterized types to avoid all the 'resumeFoo' methods? Feel free to be creative here."", 'commenter': 'paul-rogers'}, {'comment': ""I suppose, this will do for now (at least I see it OK). Changed the validation to check if the added field is `value` as `key` can't be complex and the method does not make sense for the case."", 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {
+
+  private final SchemaContainer parent;
+  private final TupleBuilder tupleBuilder = new TupleBuilder();
+  private final String memberName;
+  private final TypeProtos.DataMode mode;
+
+  public DictBuilder(String memberName, TypeProtos.DataMode mode) {
+    this(null, memberName, mode);
+  }
+
+  public DictBuilder(SchemaContainer parent, String memberName, TypeProtos.DataMode mode) {
+    this.parent = parent;
+    this.memberName = memberName;
+    this.mode = mode;
+  }
+
+  @Override
+  public void addColumn(ColumnMetadata column) {
+    assert DictVector.fieldNames.contains(column.name());
+    tupleBuilder.addColumn(column);
+  }
+
+  public DictBuilder addKey(TypeProtos.MajorType type) {","[{'comment': 'Good, but we allow only one key, do we not? If so, then rather than `addKey()` (which suggests I can add multiple), a simple `key()` would be simpler. Also, we should check if the key type was already set and not allow setting it a second time.', 'commenter': 'paul-rogers'}, {'comment': 'Changed the name to the `key()` and added validations, thanks!', 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {
+
+  private final SchemaContainer parent;
+  private final TupleBuilder tupleBuilder = new TupleBuilder();
+  private final String memberName;
+  private final TypeProtos.DataMode mode;
+
+  public DictBuilder(String memberName, TypeProtos.DataMode mode) {
+    this(null, memberName, mode);
+  }
+
+  public DictBuilder(SchemaContainer parent, String memberName, TypeProtos.DataMode mode) {
+    this.parent = parent;
+    this.memberName = memberName;
+    this.mode = mode;
+  }
+
+  @Override
+  public void addColumn(ColumnMetadata column) {
+    assert DictVector.fieldNames.contains(column.name());
+    tupleBuilder.addColumn(column);
+  }
+
+  public DictBuilder addKey(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_KEY_NAME, type));
+  }
+
+  public DictBuilder addValue(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_VALUE_NAME, type));
+  }
+
+  public DictBuilder add(MaterializedField col) {","[{'comment': 'Probably no longer need this form. It is in the code to ease conversion from old code, but does not actually seem to have been used.', 'commenter': 'paul-rogers'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {
+
+  private final SchemaContainer parent;
+  private final TupleBuilder tupleBuilder = new TupleBuilder();
+  private final String memberName;
+  private final TypeProtos.DataMode mode;
+
+  public DictBuilder(String memberName, TypeProtos.DataMode mode) {
+    this(null, memberName, mode);
+  }
+
+  public DictBuilder(SchemaContainer parent, String memberName, TypeProtos.DataMode mode) {
+    this.parent = parent;
+    this.memberName = memberName;
+    this.mode = mode;
+  }
+
+  @Override
+  public void addColumn(ColumnMetadata column) {
+    assert DictVector.fieldNames.contains(column.name());
+    tupleBuilder.addColumn(column);
+  }
+
+  public DictBuilder addKey(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_KEY_NAME, type));
+  }
+
+  public DictBuilder addValue(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_VALUE_NAME, type));
+  }
+
+  public DictBuilder add(MaterializedField col) {
+    assert DictVector.fieldNames.contains(col.getName());
+    tupleBuilder.add(col);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, TypeProtos.DataMode mode) {","[{'comment': ""Does it make sense for a key to be repeated or nullable? Isn't non-nullable the only valid key mode?"", 'commenter': 'paul-rogers'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {
+
+  private final SchemaContainer parent;
+  private final TupleBuilder tupleBuilder = new TupleBuilder();
+  private final String memberName;
+  private final TypeProtos.DataMode mode;
+
+  public DictBuilder(String memberName, TypeProtos.DataMode mode) {
+    this(null, memberName, mode);
+  }
+
+  public DictBuilder(SchemaContainer parent, String memberName, TypeProtos.DataMode mode) {
+    this.parent = parent;
+    this.memberName = memberName;
+    this.mode = mode;
+  }
+
+  @Override
+  public void addColumn(ColumnMetadata column) {
+    assert DictVector.fieldNames.contains(column.name());
+    tupleBuilder.addColumn(column);
+  }
+
+  public DictBuilder addKey(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_KEY_NAME, type));
+  }
+
+  public DictBuilder addValue(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_VALUE_NAME, type));
+  }
+
+  public DictBuilder add(MaterializedField col) {
+    assert DictVector.fieldNames.contains(col.getName());
+    tupleBuilder.add(col);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, TypeProtos.DataMode mode) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type, mode);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, TypeProtos.DataMode mode) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type, mode);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, int width) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type, width);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, int width) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type, width);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, int precision, int scale) {","[{'comment': 'This form is used for `DECIMAL`. Does it make sense to allow a `DECIMAL` for a key? Maybe here, and in the width case above, just allow a `MajorType`.\r\n\r\nMaybe, in general, the `SchemaBuilder` should have relied on the `Types` methods to build up such types rather than having methods for each...', 'commenter': 'paul-rogers'}, {'comment': 'Yes, `key` can be a `DECIMAL`. Added `key(MajorType)` method to allow creation for the case (the methods validates allowed `key` types).', 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {
+
+  private final SchemaContainer parent;
+  private final TupleBuilder tupleBuilder = new TupleBuilder();
+  private final String memberName;
+  private final TypeProtos.DataMode mode;
+
+  public DictBuilder(String memberName, TypeProtos.DataMode mode) {
+    this(null, memberName, mode);
+  }
+
+  public DictBuilder(SchemaContainer parent, String memberName, TypeProtos.DataMode mode) {
+    this.parent = parent;
+    this.memberName = memberName;
+    this.mode = mode;
+  }
+
+  @Override
+  public void addColumn(ColumnMetadata column) {
+    assert DictVector.fieldNames.contains(column.name());
+    tupleBuilder.addColumn(column);
+  }
+
+  public DictBuilder addKey(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_KEY_NAME, type));
+  }
+
+  public DictBuilder addValue(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_VALUE_NAME, type));
+  }
+
+  public DictBuilder add(MaterializedField col) {
+    assert DictVector.fieldNames.contains(col.getName());
+    tupleBuilder.add(col);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, TypeProtos.DataMode mode) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type, mode);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, TypeProtos.DataMode mode) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type, mode);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, int width) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type, width);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, int width) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type, width);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(DictVector.FIELD_KEY_NAME, type, TypeProtos.DataMode.REQUIRED, precision, scale);
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(DictVector.FIELD_VALUE_NAME, type, TypeProtos.DataMode.REQUIRED, precision, scale);
+  }
+
+  public DictBuilder addNullable(String name, TypeProtos.MinorType type) {","[{'comment': 'Yeah, all these forms make sense when only working with the top-level row, but become tedious when repeated or all these specialized builders...', 'commenter': 'paul-rogers'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {
+
+  private final SchemaContainer parent;
+  private final TupleBuilder tupleBuilder = new TupleBuilder();
+  private final String memberName;
+  private final TypeProtos.DataMode mode;
+
+  public DictBuilder(String memberName, TypeProtos.DataMode mode) {
+    this(null, memberName, mode);
+  }
+
+  public DictBuilder(SchemaContainer parent, String memberName, TypeProtos.DataMode mode) {
+    this.parent = parent;
+    this.memberName = memberName;
+    this.mode = mode;
+  }
+
+  @Override
+  public void addColumn(ColumnMetadata column) {
+    assert DictVector.fieldNames.contains(column.name());
+    tupleBuilder.addColumn(column);
+  }
+
+  public DictBuilder addKey(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_KEY_NAME, type));
+  }
+
+  public DictBuilder addValue(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_VALUE_NAME, type));
+  }
+
+  public DictBuilder add(MaterializedField col) {
+    assert DictVector.fieldNames.contains(col.getName());
+    tupleBuilder.add(col);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, TypeProtos.DataMode mode) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type, mode);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, TypeProtos.DataMode mode) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type, mode);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, int width) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type, width);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, int width) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type, width);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(DictVector.FIELD_KEY_NAME, type, TypeProtos.DataMode.REQUIRED, precision, scale);
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(DictVector.FIELD_VALUE_NAME, type, TypeProtos.DataMode.REQUIRED, precision, scale);
+  }
+
+  public DictBuilder addNullable(String name, TypeProtos.MinorType type) {
+    tupleBuilder.addNullable(name,  type);
+    return this;
+  }
+
+  public DictBuilder addNullable(String name, TypeProtos.MinorType type, int width) {
+    tupleBuilder.addNullable(name, type, width);
+    return this;
+  }
+
+  public DictBuilder addNullable(String name, TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(name, type, TypeProtos.DataMode.OPTIONAL, precision, scale);
+  }
+
+  public DictBuilder addArrayValue(TypeProtos.MinorType type) {
+    tupleBuilder.addArray(DictVector.FIELD_VALUE_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addArrayValue(TypeProtos.MinorType type, int dims) {
+    tupleBuilder.addArray(DictVector.FIELD_VALUE_NAME, type, dims);
+    return this;
+  }
+
+  public DictBuilder addArrayValue(TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(DictVector.FIELD_VALUE_NAME, type, TypeProtos.DataMode.REPEATED, precision, scale);
+  }
+
+  public DictBuilder addDecimal(String name, TypeProtos.MinorType type,
+                               TypeProtos.DataMode mode, int precision, int scale) {
+    tupleBuilder.addDecimal(name, type, mode, precision, scale);
+    return this;
+  }
+
+  /**
+   * Add a map column as dict's value. The returned schema builder is for the nested
+   * map. Building that map, using {@link DictBuilder#resumeSchema()},
+   * will return the original schema builder.
+   *
+   * @return a builder for the map
+   */
+  public MapBuilder addMapValue() {
+    return tupleBuilder.addMap(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public MapBuilder addMapArrayValue() {
+    return tupleBuilder.addMapArray(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public DictBuilder addDictValue() {
+    return tupleBuilder.addDict(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public UnionBuilder addUnionValue() {
+    return tupleBuilder.addUnion(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public UnionBuilder addListValue() {","[{'comment': 'Again, if we can only have one value type, maybe use `listValue()`, etc. instead of `addListValue()`.', 'commenter': 'paul-rogers'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/DictBuilder.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.metadata;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.vector.complex.DictVector;
+
+public class DictBuilder implements SchemaContainer {
+
+  private final SchemaContainer parent;
+  private final TupleBuilder tupleBuilder = new TupleBuilder();
+  private final String memberName;
+  private final TypeProtos.DataMode mode;
+
+  public DictBuilder(String memberName, TypeProtos.DataMode mode) {
+    this(null, memberName, mode);
+  }
+
+  public DictBuilder(SchemaContainer parent, String memberName, TypeProtos.DataMode mode) {
+    this.parent = parent;
+    this.memberName = memberName;
+    this.mode = mode;
+  }
+
+  @Override
+  public void addColumn(ColumnMetadata column) {
+    assert DictVector.fieldNames.contains(column.name());
+    tupleBuilder.addColumn(column);
+  }
+
+  public DictBuilder addKey(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_KEY_NAME, type));
+  }
+
+  public DictBuilder addValue(TypeProtos.MajorType type) {
+    return add(MaterializedField.create(DictVector.FIELD_VALUE_NAME, type));
+  }
+
+  public DictBuilder add(MaterializedField col) {
+    assert DictVector.fieldNames.contains(col.getName());
+    tupleBuilder.add(col);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, TypeProtos.DataMode mode) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type, mode);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, TypeProtos.DataMode mode) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type, mode);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, int width) {
+    tupleBuilder.add(DictVector.FIELD_KEY_NAME, type, width);
+    return this;
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, int width) {
+    tupleBuilder.add(DictVector.FIELD_VALUE_NAME, type, width);
+    return this;
+  }
+
+  public DictBuilder addKey(TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(DictVector.FIELD_KEY_NAME, type, TypeProtos.DataMode.REQUIRED, precision, scale);
+  }
+
+  public DictBuilder addValue(TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(DictVector.FIELD_VALUE_NAME, type, TypeProtos.DataMode.REQUIRED, precision, scale);
+  }
+
+  public DictBuilder addNullable(String name, TypeProtos.MinorType type) {
+    tupleBuilder.addNullable(name,  type);
+    return this;
+  }
+
+  public DictBuilder addNullable(String name, TypeProtos.MinorType type, int width) {
+    tupleBuilder.addNullable(name, type, width);
+    return this;
+  }
+
+  public DictBuilder addNullable(String name, TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(name, type, TypeProtos.DataMode.OPTIONAL, precision, scale);
+  }
+
+  public DictBuilder addArrayValue(TypeProtos.MinorType type) {
+    tupleBuilder.addArray(DictVector.FIELD_VALUE_NAME, type);
+    return this;
+  }
+
+  public DictBuilder addArrayValue(TypeProtos.MinorType type, int dims) {
+    tupleBuilder.addArray(DictVector.FIELD_VALUE_NAME, type, dims);
+    return this;
+  }
+
+  public DictBuilder addArrayValue(TypeProtos.MinorType type, int precision, int scale) {
+    return addDecimal(DictVector.FIELD_VALUE_NAME, type, TypeProtos.DataMode.REPEATED, precision, scale);
+  }
+
+  public DictBuilder addDecimal(String name, TypeProtos.MinorType type,
+                               TypeProtos.DataMode mode, int precision, int scale) {
+    tupleBuilder.addDecimal(name, type, mode, precision, scale);
+    return this;
+  }
+
+  /**
+   * Add a map column as dict's value. The returned schema builder is for the nested
+   * map. Building that map, using {@link DictBuilder#resumeSchema()},
+   * will return the original schema builder.
+   *
+   * @return a builder for the map
+   */
+  public MapBuilder addMapValue() {
+    return tupleBuilder.addMap(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public MapBuilder addMapArrayValue() {
+    return tupleBuilder.addMapArray(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public DictBuilder addDictValue() {
+    return tupleBuilder.addDict(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public UnionBuilder addUnionValue() {
+    return tupleBuilder.addUnion(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public UnionBuilder addListValue() {
+    return tupleBuilder.addList(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public RepeatedListBuilder addRepeatedListValue() {
+    return tupleBuilder.addRepeatedList(this, DictVector.FIELD_VALUE_NAME);
+  }
+
+  public DictColumnMetadata buildColumn() {
+    return new DictColumnMetadata(memberName, mode, tupleBuilder.schema());","[{'comment': ""Sorry, I'm confused. A `DICT` is essentially a tuple? I thought it was an array of (key, value) pairs? Essentially (key: ARRAY<KEY_TYPE>, value: ARRAY<VALUE_TYPE>)?\r\n\r\nLet's clarify this before I comment more as I'll need to understand that to make useful comments elsewhere.\r\n\r\nIt may be that this class was patterned after the Tuple classes. But, if so, then those classes may have been poor examples because a MAP solves a different problem than a DICT (I think...)"", 'commenter': 'paul-rogers'}, {'comment': 'Indeed, the column metadata was patterned after `TUPLE` classes. This may seem weird, but I see it perfectly fit: `dict` is a ""tuple"" with two fields (`key`, which has type restrictions, and `value`). This ""tuple"" is then handled by a special writer, `DictWriter`, and a special reader `DictReader`.', 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/DictWriter.java,"@@ -0,0 +1,28 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.accessor;
+
+/**
+ * Dict writer is represented as an array of Maps (Array with Tuple entry).","[{'comment': 'Not sure I follow. A Dict is an array of key/value pairs, correct? It is a Map turned sideways: the key/value pairs are rows in an array, not columns in a map (as in the Map) type. So, a Dict is not ""an array of Maps"" is it?\r\n\r\nIf we want a Dict to allow heterogeneous value members, then we\'d have an array of key/Union pairs, not key/Map pairs. Or, am I missing something?', 'commenter': 'paul-rogers'}, {'comment': 'I wanted to say the dict writer is an array of dict `entries` (you were talking about below), just used the `map` somewhat in another context, like some type of tuple.', 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/DictWriter.java,"@@ -0,0 +1,28 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.accessor;
+
+/**
+ * Dict writer is represented as an array of Maps (Array with Tuple entry).
+ */
+public interface DictWriter extends ArrayWriter {
+  ValueType keyType();
+  ObjectType valueType();
+  ScalarWriter keyWriter();
+  ObjectWriter valueWriter();","[{'comment': 'The above is getting close, but it does introduce an ambiguity. The array writer already has the following members:\r\n\r\n```\r\n ObjectType entryType();\r\n ObjectWriter entry();\r\n```\r\n\r\nAlong with some other helpers. In the `DictWriter` case, it is not clear that I could use this object a `ArrayWriter`. Which of above does the entry correspond to?\r\n\r\nMight it be better to have `DictWriter` inherit directly from `ColumnWriter` so you can define clean semantics?\r\n\r\nIf it is worth having common functionality (only `save()` would be common), we could define a `SequenceWriter` which is the base of `ArrayWriter` and `DictWriter`.\r\n\r\nOr, you could change `ArrayWriter` to add a new element writer; a `DictEntry` which would provide the methods above. That is:\r\n\r\n```\r\n  ArrayWriter aw = reader.get(\'myDict\').array();\r\n  DictWriter dw = aw.dict();\r\n  for (int i = 0; i < kvCount; i++) {\r\n    dw.keyWriter().setString(""key + "" i);\r\n    dw.valueWriter().union().setInt(i);\r\n     aw.save();\r\n  }\r\n```\r\n\r\nThat is, a Dict is an array. The entries in the array are key/value pairs. The key is a scalar, the value is of some type (above I suggested a Union -- with all the problems that Unions bring.)', 'commenter': 'paul-rogers'}, {'comment': ""My initial intention was to make it something like you proposed in your second option, though was done so implicitly. Made it the way you described with the exception that `DictWriter` actually encapsulates '`DictEntry`' (no need to perform `DictWriter dw = aw.dict()` rather use `DictWriter` itself to write the pairs; `aw` in this case is also `DictWriter`)."", 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/reader/NullReader.java,"@@ -0,0 +1,369 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.vector.accessor.reader;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.record.metadata.VariantMetadata;
+import org.apache.drill.exec.vector.accessor.ArrayReader;
+import org.apache.drill.exec.vector.accessor.DictReader;
+import org.apache.drill.exec.vector.accessor.ObjectReader;
+import org.apache.drill.exec.vector.accessor.ObjectType;
+import org.apache.drill.exec.vector.accessor.ScalarReader;
+import org.apache.drill.exec.vector.accessor.TupleReader;
+import org.apache.drill.exec.vector.accessor.ValueType;
+import org.apache.drill.exec.vector.accessor.VariantReader;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+
+import java.math.BigDecimal;
+
+/**
+ * Dummy reader which returns {@code null} for scalar types and itself for complex types.
+ */
+public class NullReader implements ScalarReader, ArrayReader, TupleReader, VariantReader, ObjectReader, DictReader {","[{'comment': ""Good idea. But, is this too general? Does it follow the actual metadata structure?\r\n\r\nThe writer has a `NullScalarWriter` and a few other components that try to allow only structurally-legal operations. I suppose, however, that is less important when reading: being lenient won't break anything."", 'commenter': 'paul-rogers'}, {'comment': 'It does not follow the actual metadata structure, I suppose general case will do.', 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/MetadataUtils.java,"@@ -205,4 +205,10 @@ private static ColumnMetadata newDecimal(String name, MinorType type, DataMode m
     return new PrimitiveColumnMetadata(field);
   }
 
+  private static boolean isScalar(MinorType type) {
+    return type != MinorType.MAP
+        && type != MinorType.UNION
+        && type != MinorType.LIST
+        && type != MinorType.DICT;
+  }","[{'comment': 'Maybe change this to `isComplex`, and define `isScalar()` as `! isComplex()`.', 'commenter': 'paul-rogers'}, {'comment': ""Yeah, much negation... Changed the body of the method - didn't want to introduce the new `isComplex()`."", 'commenter': 'KazydubB'}]"
1870,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/MapBuilder.java,"@@ -144,6 +144,14 @@ public RepeatedListBuilder addRepeatedList(String name) {
     return tupleBuilder.addRepeatedList(this, name);
   }
 
+  public DictBuilder addDict(String name) {
+    return tupleBuilder.addDict(this, name);
+  }","[{'comment': ""Maybe `addDict(String name, MinorType keyType)`. Works because the key must be a scalar and is required. Then the `DictBuilder` simply builds the value type:\r\n\r\n```\r\nscalarValue(MinorType type);\r\nMapBuilder mapValue();\r\nMapBuilder mapArrayValue();\r\nUnionBuilder unionValue();\r\nFoo resumeFoo();\r\n```\r\nCan the value be nullable? (The comment in `DictVector` doesn't say yes or no.) If nullable is allowed, then only the scalar can be nullable, so maybe add:\r\n\r\n```\r\nnullableValue(MinorType type)\r\n```"", 'commenter': 'paul-rogers'}, {'comment': 'Good point. Yes, `value` can be nullable (added explicit comment to `DictVector`).', 'commenter': 'KazydubB'}]"
1870,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/model/single/BaseReaderBuilder.java,"@@ -77,22 +81,48 @@ protected AbstractObjectReader buildVectorReader(ValueVector vector, VectorDescr
     final MajorType type = va.type();
 
     switch(type.getMinorType()) {
-    case MAP:
-      return buildMap((AbstractMapVector) vector, va, type.getMode(), descrip);
-    case UNION:
-      return buildUnion((UnionVector) vector, va, descrip);
-    case LIST:
-      return buildList(vector, va, descrip);
-    case LATE:
-
-      // Occurs for a list with no type: a list of nulls.
-
-      return AbstractScalarReader.nullReader(descrip.metadata);
-    default:
-      return buildScalarReader(va, descrip.metadata);
+      case DICT:
+        return buildDict(vector, va, descrip);
+      case MAP:
+        return buildMap((AbstractMapVector) vector, va, type.getMode(), descrip);
+      case UNION:
+        return buildUnion((UnionVector) vector, va, descrip);
+      case LIST:
+        return buildList(vector, va, descrip);
+      case LATE:
+
+        // Occurs for a list with no type: a list of nulls.
+
+        return AbstractScalarReader.nullReader(descrip.metadata);
+      default:
+        return buildScalarReader(va, descrip.metadata);
     }
   }
 
+  private AbstractObjectReader buildDict(ValueVector vector, VectorAccessor va, VectorDescrip descrip) {
+
+    boolean isArray = descrip.metadata.isArray();
+
+    DictVector dictVector;
+    VectorAccessor dictAccessor;
+    if (isArray) {","[{'comment': 'It *should* be possible to let the column builder recursively build the readers for each of the two component fields, handling the details of arrays, lists. repeated lists and so on. That this code works is probably due to insufficient tests: do we have tests where the value is, say, a union, or a list (of unions), etc.?\r\n\r\nThis is a very tricky bit of the code, took me forever to work out how to properly build nested structures.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/README.md,"@@ -0,0 +1,24 @@
+# Drill Apache Druid Plugin
+
+Drill druid storage plugin allows you to perform SQL queries against Druid datasource(s).","[{'comment': 'Perhaps mention if filter push-down is supported, and if so, the kinds of filters supported.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidCompareFunctionProcessor.java,"@@ -0,0 +1,256 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import org.apache.drill.common.expression.CastExpression;
+import org.apache.drill.common.expression.ConvertExpression;
+import org.apache.drill.common.expression.FunctionCall;
+import org.apache.drill.common.expression.LogicalExpression;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.expression.visitors.AbstractExprVisitor;
+import org.apache.drill.common.expression.ValueExpressions.BooleanExpression;
+import org.apache.drill.common.expression.ValueExpressions.DateExpression;
+import org.apache.drill.common.expression.ValueExpressions.DoubleExpression;
+import org.apache.drill.common.expression.ValueExpressions.FloatExpression;
+import org.apache.drill.common.expression.ValueExpressions.IntExpression;
+import org.apache.drill.common.expression.ValueExpressions.LongExpression;
+import org.apache.drill.common.expression.ValueExpressions.QuotedString;
+import org.apache.drill.common.expression.ValueExpressions.TimeExpression;
+
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableSet;
+
+public class DruidCompareFunctionProcessor extends
+        AbstractExprVisitor<Boolean, LogicalExpression, RuntimeException> {
+
+    private Object value;
+    private boolean success;
+    private boolean isEqualityFn;
+    private SchemaPath path;
+    private String functionName;
+
+    public static boolean isCompareFunction(String functionName) {
+        return COMPARE_FUNCTIONS_TRANSPOSE_MAP.keySet().contains(functionName);
+    }
+
+    public static DruidCompareFunctionProcessor process(FunctionCall call) {
+        String functionName = call.getName();
+        LogicalExpression nameArg = call.args.get(0);
+        LogicalExpression valueArg = call.args.size() == 2 ? call.args.get(1)
+                : null;
+        DruidCompareFunctionProcessor evaluator = new DruidCompareFunctionProcessor(
+                functionName);
+
+        if (valueArg != null) { // binary function
+            if (VALUE_EXPRESSION_CLASSES.contains(nameArg.getClass())) {
+                LogicalExpression swapArg = valueArg;
+                valueArg = nameArg;
+                nameArg = swapArg;
+                evaluator.functionName = COMPARE_FUNCTIONS_TRANSPOSE_MAP
+                        .get(functionName);
+            }
+            evaluator.success = nameArg.accept(evaluator, valueArg);
+        } else if (call.args.get(0) instanceof SchemaPath) {
+            evaluator.success = true;
+            evaluator.path = (SchemaPath) nameArg;
+        }
+
+        return evaluator;
+    }
+
+    public DruidCompareFunctionProcessor(String functionName) {
+        this.success = false;
+        this.functionName = functionName;
+        this.isEqualityFn = COMPARE_FUNCTIONS_TRANSPOSE_MAP
+                .containsKey(functionName)
+                && COMPARE_FUNCTIONS_TRANSPOSE_MAP.get(functionName).equals(
+                functionName);
+    }
+
+    public Object getValue() {
+        return value;
+    }
+
+    public boolean isSuccess() {
+        return success;
+    }
+
+    public SchemaPath getPath() {
+        return path;
+    }
+
+    public String getFunctionName() {
+        return functionName;
+    }
+
+    @Override
+    public Boolean visitCastExpression(CastExpression e,
+                                       LogicalExpression valueArg) throws RuntimeException {
+        if (e.getInput() instanceof CastExpression
+                || e.getInput() instanceof SchemaPath) {
+            return e.getInput().accept(this, valueArg);
+        }
+        return false;
+    }
+
+    @Override
+    public Boolean visitConvertExpression(ConvertExpression e,
+                                          LogicalExpression valueArg) throws RuntimeException {
+        if (e.getConvertFunction() == ConvertExpression.CONVERT_FROM
+                && e.getInput() instanceof SchemaPath) {
+            String encodingType = e.getEncodingType();
+            switch (encodingType) {
+                case ""INT_BE"":
+                case ""INT"":
+                case ""UINT_BE"":
+                case ""UINT"":
+                case ""UINT4_BE"":
+                case ""UINT4"":
+                    if (valueArg instanceof IntExpression
+                            && (isEqualityFn || encodingType.startsWith(""U""))) {
+                        this.value = ((IntExpression) valueArg).getInt();
+                    }
+                    break;
+                case ""BIGINT_BE"":
+                case ""BIGINT"":
+                case ""UINT8_BE"":
+                case ""UINT8"":
+                    if (valueArg instanceof LongExpression
+                            && (isEqualityFn || encodingType.startsWith(""U""))) {
+                        this.value = ((LongExpression) valueArg).getLong();
+                    }
+                    break;
+                case ""FLOAT"":
+                    if (valueArg instanceof FloatExpression && isEqualityFn) {
+                        this.value = ((FloatExpression) valueArg).getFloat();
+                    }
+                    break;
+                case ""DOUBLE"":
+                    if (valueArg instanceof DoubleExpression && isEqualityFn) {
+                        this.value = ((DoubleExpression) valueArg).getDouble();
+                    }
+                    break;
+                case ""TIME_EPOCH"":
+                case ""TIME_EPOCH_BE"":
+                    if (valueArg instanceof TimeExpression) {
+                        this.value = ((TimeExpression) valueArg).getTime();
+                    }
+                    break;
+                case ""DATE_EPOCH"":
+                case ""DATE_EPOCH_BE"":
+                    if (valueArg instanceof DateExpression) {
+                        this.value = ((DateExpression) valueArg).getDate();
+                    }
+                    break;
+                case ""BOOLEAN_BYTE"":
+                    if (valueArg instanceof BooleanExpression) {
+                        this.value = ((BooleanExpression) valueArg).getBoolean();
+                    }
+                    break;
+                case ""UTF8"":
+                    // let visitSchemaPath() handle this.
+                    return e.getInput().accept(this, valueArg);
+            }
+
+            if (value != null) {
+                this.path = (SchemaPath) e.getInput();
+                return true;
+            }
+        }
+        return false;
+    }
+
+    @Override
+    public Boolean visitUnknown(LogicalExpression e, LogicalExpression valueArg)
+            throws RuntimeException {
+        return false;
+    }
+
+    @Override
+    public Boolean visitSchemaPath(SchemaPath path, LogicalExpression valueArg)
+            throws RuntimeException {
+        if (valueArg instanceof QuotedString) {
+            this.value = ((QuotedString) valueArg).value;
+            this.path = path;
+            return true;
+        }
+
+        if (valueArg instanceof IntExpression) {
+            this.value = ((IntExpression) valueArg).getInt();
+            this.path = path;
+            return true;
+        }
+
+        if (valueArg instanceof LongExpression) {
+            this.value = ((LongExpression) valueArg).getLong();
+            this.path = path;
+            return true;
+        }
+
+        if (valueArg instanceof FloatExpression) {
+            this.value = ((FloatExpression) valueArg).getFloat();
+            this.path = path;
+            return true;
+        }
+
+        if (valueArg instanceof DoubleExpression) {
+            this.value = ((DoubleExpression) valueArg).getDouble();
+            this.path = path;
+            return true;
+        }
+
+        if (valueArg instanceof BooleanExpression) {
+            this.value = ((BooleanExpression) valueArg).getBoolean();
+            this.path = path;
+            return true;
+        }
+
+        return false;
+    }
+
+    private static final ImmutableSet<Class<? extends LogicalExpression>> VALUE_EXPRESSION_CLASSES;
+    static {
+        ImmutableSet.Builder<Class<? extends LogicalExpression>> builder = ImmutableSet.builder();
+        VALUE_EXPRESSION_CLASSES = builder.add(BooleanExpression.class)
+                .add(DateExpression.class).add(DoubleExpression.class)
+                .add(FloatExpression.class).add(IntExpression.class)
+                .add(LongExpression.class).add(QuotedString.class)
+                .add(TimeExpression.class).build();
+    }
+
+    private static final ImmutableMap<String, String> COMPARE_FUNCTIONS_TRANSPOSE_MAP;
+    static {
+        ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();
+        COMPARE_FUNCTIONS_TRANSPOSE_MAP = builder
+                // unary functions
+                .put(""isnotnull"", ""isnotnull"")
+                .put(""isNotNull"", ""isNotNull"")
+                .put(""is not null"", ""is not null"")
+                .put(""isnull"", ""isnull"")
+                .put(""isNull"", ""isNull"")
+                .put(""is null"", ""is null"")
+                // binary functions
+                .put(""equal"", ""equal"").put(""not_equal"", ""not_equal"")
+                .put(""greater_than_or_equal_to"", ""less_than_or_equal_to"")
+                .put(""greater_than"", ""less_than"")
+                .put(""less_than_or_equal_to"", ""greater_than_or_equal_to"")
+                .put(""less_than"", ""greater_than"")
+                .put(""like"", ""like"")
+                .put(""LIKE"", ""like"").build();","[{'comment': 'The like/like conversion is symmetrical, no need to add the ""LIKE"" form.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidGroupScan.java,"@@ -0,0 +1,247 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.physical.EndpointAffinity;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.proto.CoordinationProtos;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+
+import org.apache.drill.exec.store.schedule.AffinityCreator;
+import org.apache.drill.exec.store.schedule.AssignmentCreator;
+import org.apache.drill.exec.store.schedule.CompleteWork;
+import org.apache.drill.exec.store.schedule.EndpointByteMap;
+import org.apache.drill.exec.store.schedule.EndpointByteMapImpl;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.drill.shaded.guava.com.google.common.collect.ListMultimap;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+
+@JsonTypeName(""druid-scan"")
+public class DruidGroupScan extends AbstractGroupScan {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidGroupScan.class);
+    private static final long DEFAULT_TABLET_SIZE = 1000;
+
+    private DruidStoragePluginConfig storagePluginConfig;
+    private List<SchemaPath> columns;
+    private DruidScanSpec scanSpec;
+    private DruidStoragePlugin storagePlugin;
+    private boolean filterPushedDown = false;
+    private List<DruidWork> druidWorkList = new ArrayList<>();
+    private ListMultimap<Integer,DruidWork> assignments;
+    private List<EndpointAffinity> affinities;
+    private String objectName;
+
+    @JsonCreator
+    public DruidGroupScan(@JsonProperty(""scanSpec"") DruidScanSpec scanSpec,","[{'comment': 'This form is used to read a logical plan back into Drill. It seems that some of the fields above are not serialized, meaning that they will be lost when writing a logical plan. In general, unless a field really is transient (such as a cache), or is used only in the physical plan (the affinities), all other fields should be Jackson-serialized.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidGroupScan.java,"@@ -0,0 +1,247 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.physical.EndpointAffinity;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.proto.CoordinationProtos;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+
+import org.apache.drill.exec.store.schedule.AffinityCreator;
+import org.apache.drill.exec.store.schedule.AssignmentCreator;
+import org.apache.drill.exec.store.schedule.CompleteWork;
+import org.apache.drill.exec.store.schedule.EndpointByteMap;
+import org.apache.drill.exec.store.schedule.EndpointByteMapImpl;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.drill.shaded.guava.com.google.common.collect.ListMultimap;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+
+@JsonTypeName(""druid-scan"")
+public class DruidGroupScan extends AbstractGroupScan {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidGroupScan.class);
+    private static final long DEFAULT_TABLET_SIZE = 1000;
+
+    private DruidStoragePluginConfig storagePluginConfig;","[{'comment': 'All fields should be `final` to show that they should not be changed (except for transient caches). This makes clear that to make a change, we must make a copy. Calcite will hold multiple copies and compare the costs of the several choices.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidGroupScan.java,"@@ -0,0 +1,247 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.physical.EndpointAffinity;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.proto.CoordinationProtos;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+
+import org.apache.drill.exec.store.schedule.AffinityCreator;
+import org.apache.drill.exec.store.schedule.AssignmentCreator;
+import org.apache.drill.exec.store.schedule.CompleteWork;
+import org.apache.drill.exec.store.schedule.EndpointByteMap;
+import org.apache.drill.exec.store.schedule.EndpointByteMapImpl;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.drill.shaded.guava.com.google.common.collect.ListMultimap;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+
+@JsonTypeName(""druid-scan"")
+public class DruidGroupScan extends AbstractGroupScan {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidGroupScan.class);
+    private static final long DEFAULT_TABLET_SIZE = 1000;
+
+    private DruidStoragePluginConfig storagePluginConfig;
+    private List<SchemaPath> columns;
+    private DruidScanSpec scanSpec;
+    private DruidStoragePlugin storagePlugin;
+    private boolean filterPushedDown = false;
+    private List<DruidWork> druidWorkList = new ArrayList<>();
+    private ListMultimap<Integer,DruidWork> assignments;
+    private List<EndpointAffinity> affinities;
+    private String objectName;
+
+    @JsonCreator
+    public DruidGroupScan(@JsonProperty(""scanSpec"") DruidScanSpec scanSpec,
+                         @JsonProperty(""storagePluginConfig"") DruidStoragePluginConfig storagePluginConfig,
+                         @JsonProperty(""columns"") List<SchemaPath> columns,
+                         @JacksonInject StoragePluginRegistry pluginRegistry)
+            throws IOException, ExecutionSetupException {
+        this((DruidStoragePlugin) pluginRegistry.getPlugin(storagePluginConfig), scanSpec, columns);
+        int columnSize = (columns == null) ? 0 : columns.size();
+    }
+
+    public DruidGroupScan(DruidStoragePlugin storagePlugin, DruidScanSpec scanSpec,
+                         List<SchemaPath> columns) {
+        super(""someuser"");
+        objectName = UUID.randomUUID().toString();
+        this.storagePlugin = storagePlugin;
+        this.storagePluginConfig = storagePlugin.getConfig();
+        this.scanSpec = scanSpec;
+        this.columns = columns == null || columns.size() == 0? ALL_COLUMNS : columns;
+        init();
+    }
+
+    /**
+     * Private constructor, used for cloning.
+     * @param that The DruidGroupScan to clone
+     */
+    private DruidGroupScan(DruidGroupScan that) {
+        super(that);
+        this.columns = that.columns;
+        this.scanSpec = that.scanSpec;
+        this.storagePlugin = that.storagePlugin;
+        this.storagePluginConfig = that.storagePluginConfig;
+        this.filterPushedDown = that.filterPushedDown;
+        this.druidWorkList = that.druidWorkList;
+        this.assignments = that.assignments;
+        this.objectName = that.objectName;
+    }
+
+    @Override
+    public GroupScan clone(List<SchemaPath> columns) {
+        DruidGroupScan newScan = new DruidGroupScan(this);
+        newScan.columns = columns;
+        return newScan;
+    }
+
+    @Override
+    public List<EndpointAffinity> getOperatorAffinity() {
+        if (affinities == null) {
+            affinities = AffinityCreator.getAffinityMap(druidWorkList);
+        }
+        return affinities;
+    }
+
+    @Override
+    public boolean canPushdownProjects(List<SchemaPath> columns) {
+        return true;
+    }
+
+    @JsonIgnore
+    public boolean isFilterPushedDown() {
+        return filterPushedDown;
+    }
+
+    @JsonIgnore
+    public void setFilterPushedDown(boolean filterPushedDown) {
+        this.filterPushedDown = filterPushedDown;
+    }
+
+    private void init() {
+        logger.debug(""Adding Druid Work for Table - "" + getTableName() + "" Filter - "" + getScanSpec().getFilters());
+
+        DruidWork druidWork =
+                new DruidWork(
+                        new DruidSubScan.DruidSubScanSpec(
+                                getTableName(),
+                                getScanSpec().getFilters()
+                        )
+                );
+        druidWorkList.add(druidWork);
+    }
+
+    private static class DruidWork implements CompleteWork {
+
+        private EndpointByteMapImpl byteMap = new EndpointByteMapImpl();
+
+        private DruidSubScan.DruidSubScanSpec druidSubScanSpec;
+
+        public DruidWork(DruidSubScan.DruidSubScanSpec druidSubScanSpec) {
+            this.druidSubScanSpec = druidSubScanSpec;
+        }
+
+        public DruidSubScan.DruidSubScanSpec getDruidSubScanSpec() {
+            return druidSubScanSpec;
+        }
+
+        @Override
+        public long getTotalBytes() {
+            return DEFAULT_TABLET_SIZE;
+        }
+
+        @Override
+        public EndpointByteMap getByteMap() {
+            return byteMap;
+        }
+
+        @Override
+        public int compareTo(CompleteWork o) {
+            return 0;
+        }
+    }
+
+    //TODO - MAY GET MORE PRECISE COUNT FROM DRUID ITSELF.
+    public ScanStats getScanStats() {","[{'comment': 'Good that we have a large record count. Multiply that number by some guessed average row width (say 100 or 200) to get a byte count.\r\n\r\nThen, be sure to adjust the estimate as push-downs are applied. Pushing down columns should reduce the average row width (lowering cost) so that Calcite prefers it. Filter push-down should lower row count (again, so Calcite prefers it.)\r\n\r\n(Sorry this is so complex; we should have a simpler API.)', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidGroupScan.java,"@@ -0,0 +1,247 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.physical.EndpointAffinity;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.proto.CoordinationProtos;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+
+import org.apache.drill.exec.store.schedule.AffinityCreator;
+import org.apache.drill.exec.store.schedule.AssignmentCreator;
+import org.apache.drill.exec.store.schedule.CompleteWork;
+import org.apache.drill.exec.store.schedule.EndpointByteMap;
+import org.apache.drill.exec.store.schedule.EndpointByteMapImpl;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.drill.shaded.guava.com.google.common.collect.ListMultimap;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+
+@JsonTypeName(""druid-scan"")
+public class DruidGroupScan extends AbstractGroupScan {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidGroupScan.class);
+    private static final long DEFAULT_TABLET_SIZE = 1000;
+
+    private DruidStoragePluginConfig storagePluginConfig;
+    private List<SchemaPath> columns;
+    private DruidScanSpec scanSpec;
+    private DruidStoragePlugin storagePlugin;
+    private boolean filterPushedDown = false;
+    private List<DruidWork> druidWorkList = new ArrayList<>();
+    private ListMultimap<Integer,DruidWork> assignments;
+    private List<EndpointAffinity> affinities;
+    private String objectName;
+
+    @JsonCreator
+    public DruidGroupScan(@JsonProperty(""scanSpec"") DruidScanSpec scanSpec,
+                         @JsonProperty(""storagePluginConfig"") DruidStoragePluginConfig storagePluginConfig,
+                         @JsonProperty(""columns"") List<SchemaPath> columns,
+                         @JacksonInject StoragePluginRegistry pluginRegistry)
+            throws IOException, ExecutionSetupException {
+        this((DruidStoragePlugin) pluginRegistry.getPlugin(storagePluginConfig), scanSpec, columns);
+        int columnSize = (columns == null) ? 0 : columns.size();
+    }
+
+    public DruidGroupScan(DruidStoragePlugin storagePlugin, DruidScanSpec scanSpec,
+                         List<SchemaPath> columns) {
+        super(""someuser"");
+        objectName = UUID.randomUUID().toString();
+        this.storagePlugin = storagePlugin;
+        this.storagePluginConfig = storagePlugin.getConfig();
+        this.scanSpec = scanSpec;
+        this.columns = columns == null || columns.size() == 0? ALL_COLUMNS : columns;
+        init();
+    }
+
+    /**
+     * Private constructor, used for cloning.
+     * @param that The DruidGroupScan to clone
+     */
+    private DruidGroupScan(DruidGroupScan that) {
+        super(that);
+        this.columns = that.columns;
+        this.scanSpec = that.scanSpec;
+        this.storagePlugin = that.storagePlugin;
+        this.storagePluginConfig = that.storagePluginConfig;
+        this.filterPushedDown = that.filterPushedDown;
+        this.druidWorkList = that.druidWorkList;
+        this.assignments = that.assignments;
+        this.objectName = that.objectName;
+    }
+
+    @Override
+    public GroupScan clone(List<SchemaPath> columns) {
+        DruidGroupScan newScan = new DruidGroupScan(this);
+        newScan.columns = columns;
+        return newScan;
+    }
+
+    @Override
+    public List<EndpointAffinity> getOperatorAffinity() {
+        if (affinities == null) {
+            affinities = AffinityCreator.getAffinityMap(druidWorkList);
+        }
+        return affinities;
+    }
+
+    @Override
+    public boolean canPushdownProjects(List<SchemaPath> columns) {
+        return true;
+    }
+
+    @JsonIgnore
+    public boolean isFilterPushedDown() {
+        return filterPushedDown;
+    }
+
+    @JsonIgnore
+    public void setFilterPushedDown(boolean filterPushedDown) {
+        this.filterPushedDown = filterPushedDown;
+    }
+
+    private void init() {
+        logger.debug(""Adding Druid Work for Table - "" + getTableName() + "" Filter - "" + getScanSpec().getFilters());
+
+        DruidWork druidWork =
+                new DruidWork(
+                        new DruidSubScan.DruidSubScanSpec(
+                                getTableName(),
+                                getScanSpec().getFilters()
+                        )
+                );
+        druidWorkList.add(druidWork);
+    }
+
+    private static class DruidWork implements CompleteWork {
+
+        private EndpointByteMapImpl byteMap = new EndpointByteMapImpl();
+
+        private DruidSubScan.DruidSubScanSpec druidSubScanSpec;
+
+        public DruidWork(DruidSubScan.DruidSubScanSpec druidSubScanSpec) {
+            this.druidSubScanSpec = druidSubScanSpec;
+        }
+
+        public DruidSubScan.DruidSubScanSpec getDruidSubScanSpec() {
+            return druidSubScanSpec;
+        }
+
+        @Override
+        public long getTotalBytes() {
+            return DEFAULT_TABLET_SIZE;
+        }
+
+        @Override
+        public EndpointByteMap getByteMap() {
+            return byteMap;
+        }
+
+        @Override
+        public int compareTo(CompleteWork o) {
+            return 0;
+        }
+    }
+
+    //TODO - MAY GET MORE PRECISE COUNT FROM DRUID ITSELF.
+    public ScanStats getScanStats() {
+        long recordCount = 100000;
+        return new ScanStats(ScanStats.GroupScanProperty.NO_EXACT_ROW_COUNT, recordCount, 1, recordCount);
+    }
+
+    @Override
+    public void applyAssignments(List<CoordinationProtos.DrillbitEndpoint> endpoints) {
+        assignments = AssignmentCreator.getMappings(endpoints, druidWorkList);
+    }
+
+    @Override
+    public DruidSubScan getSpecificScan(int minorFragmentId) throws ExecutionSetupException {
+
+        List<DruidWork> workList = assignments.get(minorFragmentId);
+
+        List<DruidSubScan.DruidSubScanSpec> scanSpecList = Lists.newArrayList();
+        for (DruidWork druidWork : workList) {
+            scanSpecList
+                    .add(
+                            new DruidSubScan.DruidSubScanSpec(
+                                    druidWork.getDruidSubScanSpec().getDataSourceName(),
+                                    druidWork.getDruidSubScanSpec().getFilter()
+                            )
+                    );
+        }
+
+        return new DruidSubScan(storagePlugin, storagePluginConfig, scanSpecList, this.columns);
+    }
+
+    @JsonIgnore
+    public String getTableName() {
+        return getScanSpec().getDataSourceName();
+    }
+
+    @Override
+    public int getMaxParallelizationWidth() {
+        return druidWorkList.size();
+    }
+
+    @Override
+    public String getDigest() {
+        return toString();","[{'comment': 'This item will show up in the `DESCRIBE PLAN FOR` output and should produce a string of format:\r\n\r\n```\r\n<class name> [x=foo, y=bar, ...]\r\n```', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidRecordReader.java,"@@ -0,0 +1,173 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.ops.FragmentContext;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.OutputMutator;
+import org.apache.drill.exec.store.AbstractRecordReader;
+import org.apache.drill.exec.store.druid.druid.DruidSelectResponse;
+import org.apache.drill.exec.store.druid.druid.PagingIdentifier;
+import org.apache.drill.exec.store.druid.druid.PagingSpec;
+import org.apache.drill.exec.store.druid.druid.SelectQuery;
+import org.apache.drill.exec.vector.complex.fn.JsonReader;
+import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import java.util.Set;
+
+public class DruidRecordReader extends AbstractRecordReader {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidRecordReader.class);
+    private DruidStoragePlugin plugin;
+    private final DruidSubScan.DruidSubScanSpec scanSpec;
+    private List<String> dimensions;
+    private String filters;
+    private ArrayList<PagingIdentifier> pagingIdentifiers = new ArrayList<>();
+
+    private JsonReader jsonReader;
+    private VectorContainerWriter writer;
+
+    private OutputMutator output;
+    private OperatorContext context;
+    private final FragmentContext fragmentContext;
+
+    private ObjectMapper objectMapper = new ObjectMapper();
+
+    public DruidRecordReader(DruidSubScan.DruidSubScanSpec subScanSpec, List<SchemaPath> projectedColumns,
+                             FragmentContext context, DruidStoragePlugin plugin) {
+        dimensions = new ArrayList<String>();
+        setColumns(projectedColumns);
+        this.plugin = plugin;
+        scanSpec = subScanSpec;
+        fragmentContext = context;
+        this.filters = subScanSpec.getFilter();
+    }
+
+    @Override
+    protected Collection<SchemaPath> transformColumns(Collection<SchemaPath> projectedColumns) {
+        Set<SchemaPath> transformed = Sets.newLinkedHashSet();
+        if (isStarQuery()) {
+            transformed.add(SchemaPath.STAR_COLUMN);
+        } else {
+            for (SchemaPath column : projectedColumns) {
+                String fieldName = column.getRootSegment().getPath();
+                transformed.add(column);
+                this.dimensions.add(fieldName);
+            }
+        }
+        return transformed;
+    }
+
+    @Override
+    public void setup(OperatorContext context, OutputMutator output) throws ExecutionSetupException {
+        this.context = context;
+        this.output = output;
+        this.writer = new VectorContainerWriter(output);
+
+        //Lists.newArrayList(getColumns()), true, false, false
+        this.jsonReader =
+                new JsonReader.Builder(fragmentContext.getManagedBuffer())
+                        .schemaPathColumns(ImmutableList.copyOf(getColumns()))
+                        .skipOuterList(true)
+                        .build();
+        logger.debug("" Initialized JsonRecordReader. "");
+    }
+
+    @Override
+    public int next() {","[{'comment': ""Ideally, we'd return Druid responses in Drill batches up to some maximum record count, say 1000. This means that a single Druid query might be split into multiple Drill batches, if the number of returned records is large.\r\n\r\nHowever, it seems here each call to `next()` executes  query and returns all results. Maybe there was some partitioning to split each query (the `DruidWork`) so that each returns a fixed number of records?\r\n\r\nIf so, the second call to `next()` should return 0 so that Drill knows the scan is done."", 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidRecordReader.java,"@@ -0,0 +1,173 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.ops.FragmentContext;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.OutputMutator;
+import org.apache.drill.exec.store.AbstractRecordReader;
+import org.apache.drill.exec.store.druid.druid.DruidSelectResponse;
+import org.apache.drill.exec.store.druid.druid.PagingIdentifier;
+import org.apache.drill.exec.store.druid.druid.PagingSpec;
+import org.apache.drill.exec.store.druid.druid.SelectQuery;
+import org.apache.drill.exec.vector.complex.fn.JsonReader;
+import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import java.util.Set;
+
+public class DruidRecordReader extends AbstractRecordReader {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidRecordReader.class);
+    private DruidStoragePlugin plugin;
+    private final DruidSubScan.DruidSubScanSpec scanSpec;
+    private List<String> dimensions;
+    private String filters;
+    private ArrayList<PagingIdentifier> pagingIdentifiers = new ArrayList<>();
+
+    private JsonReader jsonReader;
+    private VectorContainerWriter writer;
+
+    private OutputMutator output;
+    private OperatorContext context;
+    private final FragmentContext fragmentContext;
+
+    private ObjectMapper objectMapper = new ObjectMapper();
+
+    public DruidRecordReader(DruidSubScan.DruidSubScanSpec subScanSpec, List<SchemaPath> projectedColumns,
+                             FragmentContext context, DruidStoragePlugin plugin) {
+        dimensions = new ArrayList<String>();
+        setColumns(projectedColumns);
+        this.plugin = plugin;
+        scanSpec = subScanSpec;
+        fragmentContext = context;
+        this.filters = subScanSpec.getFilter();
+    }
+
+    @Override
+    protected Collection<SchemaPath> transformColumns(Collection<SchemaPath> projectedColumns) {
+        Set<SchemaPath> transformed = Sets.newLinkedHashSet();
+        if (isStarQuery()) {
+            transformed.add(SchemaPath.STAR_COLUMN);
+        } else {
+            for (SchemaPath column : projectedColumns) {
+                String fieldName = column.getRootSegment().getPath();
+                transformed.add(column);
+                this.dimensions.add(fieldName);
+            }
+        }
+        return transformed;
+    }
+
+    @Override
+    public void setup(OperatorContext context, OutputMutator output) throws ExecutionSetupException {
+        this.context = context;
+        this.output = output;
+        this.writer = new VectorContainerWriter(output);
+
+        //Lists.newArrayList(getColumns()), true, false, false
+        this.jsonReader =
+                new JsonReader.Builder(fragmentContext.getManagedBuffer())
+                        .schemaPathColumns(ImmutableList.copyOf(getColumns()))
+                        .skipOuterList(true)
+                        .build();
+        logger.debug("" Initialized JsonRecordReader. "");
+    }
+
+    @Override
+    public int next() {
+
+        writer.allocate();
+        writer.reset();
+        SelectQuery selectQuery = new SelectQuery(scanSpec.dataSourceName);
+        selectQuery.setDimensions(this.dimensions);
+        selectQuery.setFilter(this.filters);
+
+        ObjectNode paging = objectMapper.createObjectNode();
+        if (this.pagingIdentifiers != null && !this.pagingIdentifiers.isEmpty()) {
+            for (PagingIdentifier pagingIdentifier : this.pagingIdentifiers) {
+                paging.put(pagingIdentifier.getSegmentName(), pagingIdentifier.getSegmentOffset());
+            }
+        }
+
+        PagingSpec pagingSpec = new PagingSpec(paging);
+        selectQuery.setPagingSpec(pagingSpec);
+
+        DruidQueryClient druidQueryClient = plugin.getDruidQueryClient();
+
+        try {
+            String query = selectQuery.toJson();
+            logger.debug(""Executing DRUID query - "" + query);
+            DruidSelectResponse druidSelectResponse = druidQueryClient.ExecuteQuery(query);
+            ArrayList<PagingIdentifier> newPagingIdentifiers = druidSelectResponse.getPagingIdentifiers();
+
+            ArrayList<String> newPagingIdentifierNames = new ArrayList<>();
+            for (PagingIdentifier pagingIdentifier : newPagingIdentifiers) {
+                newPagingIdentifierNames.add(pagingIdentifier.getSegmentName());
+            }
+
+            for (PagingIdentifier pagingIdentifier : this.pagingIdentifiers) {
+                if (!newPagingIdentifierNames.contains(pagingIdentifier.getSegmentName())) {
+                    newPagingIdentifiers.add(
+                            new PagingIdentifier(pagingIdentifier.getSegmentName(),
+                                    pagingIdentifier.getSegmentOffset() + 1)
+                    );
+                }
+            }
+
+            //update the paging identifiers
+            this.pagingIdentifiers = newPagingIdentifiers;
+
+            int docCount = 0;
+            for (ObjectNode eventNode : druidSelectResponse.getEvents()) {
+                writer.setPosition(docCount);
+                jsonReader.setSource(eventNode);
+                try {
+                    jsonReader.write(writer);
+                } catch (IOException e) {
+                    String msg = ""Failure while reading document. - Parser was at record: "" + eventNode.toString();
+                    logger.error(msg, e);
+                    throw new DrillRuntimeException(msg, e);
+                }
+                docCount++;
+            }
+
+            writer.setValueCount(docCount);
+            return docCount;
+        } catch (IOException e) {
+            String msg = ""Failure while reading documents"";
+            logger.error(msg, e);
+            throw new DrillRuntimeException(msg, e);
+        }
+    }
+
+    @Override
+    public void close() throws Exception {","[{'comment': 'Would be good to set pointers to null for any large buffered result sets. Because of the way Drill works, all readers are retained or the full duration of a scan, so if they hold large Java heap objects, there may be excessive memory pressure. (Yet another thing we should fix, and is fixed in the new ""EVF"" version.)', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidScanSpecBuilder.java,"@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.store.druid.common.DruidBoundFilter;
+import org.apache.drill.exec.store.druid.common.DruidIntervalFilter;
+import org.apache.drill.exec.store.druid.common.DruidNotFilter;
+import org.apache.drill.exec.store.druid.common.DruidRegexFilter;
+import org.apache.drill.exec.store.druid.common.DruidSearchFilter;
+import org.apache.drill.exec.store.druid.common.DruidSelectorFilter;
+import org.apache.drill.exec.store.druid.druid.SelectQuery;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+class DruidScanSpecBuilder {
+
+    private static final Logger logger = LoggerFactory.getLogger(DruidScanSpecBuilder.class);
+
+    private static final String REGEX_KEYWORD_HINT = ""$regex$_"";
+
+    DruidScanSpec build(String dataSourceName,
+                        String functionName,
+                        SchemaPath field,
+                        Object fieldValue) throws IOException {
+        // extract the field name
+
+        String fieldName = field.getAsNamePart().getName(); //.getAsUnescapedPath();
+        String filter;
+
+        logger.debug(""createDruidScanSpec called. FunctionName - ""
+                + functionName + "", field - "" + fieldName + "", fieldValue - "" + fieldValue);
+
+        switch (functionName) {
+            case ""equal"":
+            {
+                if (fieldName.equalsIgnoreCase(SelectQuery.IntervalDimensionName)) {
+                    DruidIntervalFilter druidIntervalFilter = new DruidIntervalFilter((String)fieldValue);
+                    filter = druidIntervalFilter.toJson();
+                    break;
+                } else {
+                    DruidSelectorFilter druidSelectorFilter = new DruidSelectorFilter(fieldName, (String) fieldValue);
+                    filter = druidSelectorFilter.toJson();
+                    break;
+                }
+            }
+            case ""not_equal"":
+            {
+                DruidSelectorFilter druidSelectorFilter = new DruidSelectorFilter(fieldName, String.valueOf(fieldValue));
+                String selectorFilter = druidSelectorFilter.toJson();
+                DruidNotFilter druidNotFilter = new DruidNotFilter(selectorFilter);
+                filter = druidNotFilter.toJson();
+                break;
+            }
+            case ""greater_than_or_equal_to"":
+            {
+                DruidBoundFilter druidBoundFilter = new DruidBoundFilter(fieldName, String.valueOf(fieldValue), null);
+                filter = druidBoundFilter.toJson();
+                break;
+            }
+            case ""greater_than"":
+            {
+                DruidBoundFilter druidBoundFilter = new DruidBoundFilter(fieldName, String.valueOf(fieldValue), null);
+                druidBoundFilter.setLowerStrict(true);
+                filter = druidBoundFilter.toJson();
+                break;
+            }
+            case ""less_than_or_equal_to"":
+            {
+                DruidBoundFilter druidBoundFilter = new DruidBoundFilter(fieldName, null, String.valueOf(fieldValue));
+                filter = druidBoundFilter.toJson();
+                break;
+            }
+            case ""less_than"":
+            {
+                DruidBoundFilter druidBoundFilter = new DruidBoundFilter(fieldName, null, String.valueOf(fieldValue));
+                druidBoundFilter.setUpperStrict(true);
+                filter = druidBoundFilter.toJson();
+                break;
+            }
+            case ""isnull"":","[{'comment': 'As it turns out, only one of these is actually used. We have lots of copy/paste that has preserved the fact that someone did not know the correct form. An upcoming PR will declare these names as constants in the related physical operator nodes to avoid this ongoing confusion.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidScanner.java,"@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+
+import java.util.List;
+
+public class DruidScanner {","[{'comment': 'In general, a few words of explanation would help us poor reviewers understand the purpose of Druid-specific classes. Very hard to reverse-engineer the purpose otherwise. :-)', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidStoragePlugin.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.calcite.schema.SchemaPlus;
+import org.apache.drill.common.JSONOptions;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.store.AbstractStoragePlugin;
+import org.apache.drill.exec.store.SchemaConfig;
+import org.apache.drill.exec.store.druid.schema.DruidSchemaFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+public class DruidStoragePlugin extends AbstractStoragePlugin {
+
+    static final Logger logger = LoggerFactory.getLogger(DruidStoragePlugin.class);
+
+    private final DrillbitContext context;
+    private final DruidStoragePluginConfig pluginConfig;
+    private final DruidAdminClient druidAdminClient;
+    private final DruidQueryClient druidQueryClient;
+    private final DruidSchemaFactory schemaFactory;
+
+    public DruidStoragePlugin(DruidStoragePluginConfig pluginConfig, DrillbitContext context, String name) {
+        super(context, name);
+        this.pluginConfig = pluginConfig;
+        this.context = context;
+        this.druidAdminClient = new DruidAdminClient(pluginConfig.GetCoordinatorURI());
+        this.druidQueryClient = new DruidQueryClient(pluginConfig.GetBrokerURI());
+        this.schemaFactory = new DruidSchemaFactory(this, name);
+    }
+
+    @Override
+    public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection) throws IOException {
+        DruidScanSpec scanSpec = selection.getListWith(new ObjectMapper(), new TypeReference<DruidScanSpec>() {});
+        return new DruidGroupScan(this, scanSpec, null);
+    }
+
+    @Override
+    public void registerSchemas(SchemaConfig schemaConfig, SchemaPlus parent) throws IOException {
+        schemaFactory.registerSchemas(schemaConfig, parent);
+    }
+
+/*    @Override
+    public Set<StoragePluginOptimizerRule> getPhysicalOptimizerRules(OptimizerRulesContext optimizerRulesContext) {
+        return ImmutableSet.of(DruidPushDownFilterForScan.INSTANCE);
+    }*/","[{'comment': ""Isn't this needed to apply the filter rules defined in this PR?"", 'commenter': 'paul-rogers'}, {'comment': 'Please remove the commented out code.', 'commenter': 'cgivre'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidStoragePlugin.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.calcite.schema.SchemaPlus;
+import org.apache.drill.common.JSONOptions;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.store.AbstractStoragePlugin;
+import org.apache.drill.exec.store.SchemaConfig;
+import org.apache.drill.exec.store.druid.schema.DruidSchemaFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+public class DruidStoragePlugin extends AbstractStoragePlugin {
+
+    static final Logger logger = LoggerFactory.getLogger(DruidStoragePlugin.class);
+
+    private final DrillbitContext context;
+    private final DruidStoragePluginConfig pluginConfig;
+    private final DruidAdminClient druidAdminClient;
+    private final DruidQueryClient druidQueryClient;
+    private final DruidSchemaFactory schemaFactory;
+
+    public DruidStoragePlugin(DruidStoragePluginConfig pluginConfig, DrillbitContext context, String name) {
+        super(context, name);
+        this.pluginConfig = pluginConfig;
+        this.context = context;
+        this.druidAdminClient = new DruidAdminClient(pluginConfig.GetCoordinatorURI());
+        this.druidQueryClient = new DruidQueryClient(pluginConfig.GetBrokerURI());
+        this.schemaFactory = new DruidSchemaFactory(this, name);
+    }
+
+    @Override
+    public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection) throws IOException {
+        DruidScanSpec scanSpec = selection.getListWith(new ObjectMapper(), new TypeReference<DruidScanSpec>() {});
+        return new DruidGroupScan(this, scanSpec, null);
+    }
+
+    @Override
+    public void registerSchemas(SchemaConfig schemaConfig, SchemaPlus parent) throws IOException {
+        schemaFactory.registerSchemas(schemaConfig, parent);
+    }
+
+/*    @Override
+    public Set<StoragePluginOptimizerRule> getPhysicalOptimizerRules(OptimizerRulesContext optimizerRulesContext) {
+        return ImmutableSet.of(DruidPushDownFilterForScan.INSTANCE);
+    }*/
+
+    @Override
+    public boolean supportsRead() {
+        return true;
+    }
+
+    @Override
+    public boolean supportsWrite() {
+        return false;","[{'comment': 'Nit: Drill prefers two-space indentation.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidStoragePluginConfig.java,"@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+
+@JsonTypeName(DruidStoragePluginConfig.NAME)
+public class DruidStoragePluginConfig extends StoragePluginConfigBase {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidStoragePluginConfig.class);
+
+    public static final String NAME = ""druid"";
+
+    @JsonProperty
+    private final String brokerAddress;
+
+    @JsonProperty
+    private final String coordinatorAddress;
+
+    @JsonCreator
+    public DruidStoragePluginConfig(
+            @JsonProperty(""brokerAddress"") String brokerAddress,
+            @JsonProperty(""coordinatorAddress"") String coordinatorAddress) {
+
+        this.brokerAddress = brokerAddress;
+        this.coordinatorAddress = coordinatorAddress;
+        logger.info(""Broker Address - {}, Coordinator Address - {}"", brokerAddress, coordinatorAddress);
+        //TODO Make this configurable.
+    }
+
+    @Override
+    public boolean equals(Object that) {
+        if (this == that) {
+            return true;
+        } else if (that == null || getClass() != that.getClass()) {
+            return false;
+        }
+        DruidStoragePluginConfig thatConfig = (DruidStoragePluginConfig) that;
+        return
+                (this.brokerAddress.equals(thatConfig.brokerAddress)
+                        && this.coordinatorAddress.equals(thatConfig.coordinatorAddress));
+    }
+
+    @Override
+    public int hashCode() {
+        int brokerAddressHashCode = this.brokerAddress != null ? this.brokerAddress.hashCode() : 0;
+        int coordinatorAddressHashCode = this.coordinatorAddress != null ? this.coordinatorAddress.hashCode() : 0;
+        return brokerAddressHashCode ^ coordinatorAddressHashCode;
+    }
+
+    public String GetCoordinatorURI() {","[{'comment': 'Drill prefers Java style: `getCoordinatorURI`. Jackson will use that as the accesor for this property.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidStoragePluginConfig.java,"@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+
+@JsonTypeName(DruidStoragePluginConfig.NAME)
+public class DruidStoragePluginConfig extends StoragePluginConfigBase {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidStoragePluginConfig.class);
+
+    public static final String NAME = ""druid"";
+
+    @JsonProperty","[{'comment': 'Not needed if we rename the ""Get"" accessors to ""get"".', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidSubScan.java,"@@ -0,0 +1,145 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.physical.base.AbstractBase;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+
+import static java.util.Collections.emptyIterator;
+
+/**
+ * A Class containing information to read a single druid data source.
+ */
+@JsonTypeName(""druid-datasource-scan"")
+public class DruidSubScan extends AbstractBase implements SubScan {
+
+    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidSubScan.class);
+
+    @JsonProperty
+    public final DruidStoragePluginConfig druidStoragePluginConfig;
+
+    @JsonIgnore
+    private final DruidStoragePlugin druidStoragePlugin;
+
+    private final List<DruidSubScanSpec> dataSourceScanSpecList;
+    private final List<SchemaPath> columns;
+
+    @JsonCreator
+    public DruidSubScan(@JacksonInject StoragePluginRegistry registry,
+                       @JsonProperty(""druidStoragePluginConfig"") StoragePluginConfig druidStoragePluginConfig,","[{'comment': 'This is a ""pubic"" JSON property name, so a simpler name might be better: `config` maybe. And, `scanSpec` for the one below.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidSubScan.java,"@@ -0,0 +1,145 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.physical.base.AbstractBase;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+
+import static java.util.Collections.emptyIterator;
+
+/**
+ * A Class containing information to read a single druid data source.
+ */
+@JsonTypeName(""druid-datasource-scan"")
+public class DruidSubScan extends AbstractBase implements SubScan {
+
+    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidSubScan.class);
+
+    @JsonProperty
+    public final DruidStoragePluginConfig druidStoragePluginConfig;
+
+    @JsonIgnore
+    private final DruidStoragePlugin druidStoragePlugin;
+
+    private final List<DruidSubScanSpec> dataSourceScanSpecList;
+    private final List<SchemaPath> columns;
+
+    @JsonCreator
+    public DruidSubScan(@JacksonInject StoragePluginRegistry registry,
+                       @JsonProperty(""druidStoragePluginConfig"") StoragePluginConfig druidStoragePluginConfig,
+                       @JsonProperty(""datasourceScanSpecList"") LinkedList<DruidSubScanSpec> datasourceScanSpecList,
+                       @JsonProperty(""columns"") List<SchemaPath> columns) throws ExecutionSetupException {
+        super(""someuser"");
+        druidStoragePlugin = (DruidStoragePlugin) registry.getPlugin(druidStoragePluginConfig);
+        this.dataSourceScanSpecList = datasourceScanSpecList;
+        this.druidStoragePluginConfig = (DruidStoragePluginConfig) druidStoragePluginConfig;
+        this.columns = columns;
+    }
+
+    public DruidSubScan(DruidStoragePlugin plugin, DruidStoragePluginConfig config,
+                       List<DruidSubScanSpec> dataSourceInfoList, List<SchemaPath> columns) {
+        super(""someuser"");
+        druidStoragePlugin = plugin;
+        druidStoragePluginConfig = config;
+        this.dataSourceScanSpecList = dataSourceInfoList;
+        this.columns = columns;
+    }
+
+    @Override
+    public <T, X, E extends Throwable> T accept(PhysicalVisitor<T, X, E> physicalVisitor, X value) throws E {
+        return physicalVisitor.visitSubScan(this, value);
+    }
+
+    public List<DruidSubScanSpec> getDataSourceScanSpecList() {
+        return dataSourceScanSpecList;
+    }
+
+    @JsonIgnore
+    public DruidStoragePluginConfig getStorageConfig() {
+        return druidStoragePluginConfig;
+    }
+
+    public List<SchemaPath> getColumns() {
+        return columns;
+    }
+
+    @Override","[{'comment': '`@JsonIgnore` to prevent Jackson from serializing since this looks like a Java Bean property.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidSubScan.java,"@@ -0,0 +1,145 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.physical.base.AbstractBase;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+
+import static java.util.Collections.emptyIterator;
+
+/**
+ * A Class containing information to read a single druid data source.
+ */
+@JsonTypeName(""druid-datasource-scan"")
+public class DruidSubScan extends AbstractBase implements SubScan {
+
+    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidSubScan.class);
+
+    @JsonProperty
+    public final DruidStoragePluginConfig druidStoragePluginConfig;
+
+    @JsonIgnore
+    private final DruidStoragePlugin druidStoragePlugin;
+
+    private final List<DruidSubScanSpec> dataSourceScanSpecList;
+    private final List<SchemaPath> columns;
+
+    @JsonCreator
+    public DruidSubScan(@JacksonInject StoragePluginRegistry registry,
+                       @JsonProperty(""druidStoragePluginConfig"") StoragePluginConfig druidStoragePluginConfig,
+                       @JsonProperty(""datasourceScanSpecList"") LinkedList<DruidSubScanSpec> datasourceScanSpecList,
+                       @JsonProperty(""columns"") List<SchemaPath> columns) throws ExecutionSetupException {
+        super(""someuser"");
+        druidStoragePlugin = (DruidStoragePlugin) registry.getPlugin(druidStoragePluginConfig);
+        this.dataSourceScanSpecList = datasourceScanSpecList;
+        this.druidStoragePluginConfig = (DruidStoragePluginConfig) druidStoragePluginConfig;
+        this.columns = columns;
+    }
+
+    public DruidSubScan(DruidStoragePlugin plugin, DruidStoragePluginConfig config,
+                       List<DruidSubScanSpec> dataSourceInfoList, List<SchemaPath> columns) {
+        super(""someuser"");
+        druidStoragePlugin = plugin;
+        druidStoragePluginConfig = config;
+        this.dataSourceScanSpecList = dataSourceInfoList;
+        this.columns = columns;
+    }
+
+    @Override
+    public <T, X, E extends Throwable> T accept(PhysicalVisitor<T, X, E> physicalVisitor, X value) throws E {
+        return physicalVisitor.visitSubScan(this, value);
+    }
+
+    public List<DruidSubScanSpec> getDataSourceScanSpecList() {
+        return dataSourceScanSpecList;
+    }
+
+    @JsonIgnore
+    public DruidStoragePluginConfig getStorageConfig() {
+        return druidStoragePluginConfig;
+    }
+
+    public List<SchemaPath> getColumns() {
+        return columns;
+    }
+
+    @Override
+    public boolean isExecutable() {
+        return false;
+    }
+
+    @JsonIgnore
+    public DruidStoragePlugin getStorageEngine(){
+        return druidStoragePlugin;
+    }
+
+    @Override
+    public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) throws ExecutionSetupException {
+        Preconditions.checkArgument(children.isEmpty());
+        return new DruidSubScan(druidStoragePlugin, druidStoragePluginConfig, dataSourceScanSpecList, columns);
+    }
+
+    @Override","[{'comment': '`@JsonIgnore`', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidSubScan.java,"@@ -0,0 +1,145 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.physical.base.AbstractBase;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+
+import static java.util.Collections.emptyIterator;
+
+/**
+ * A Class containing information to read a single druid data source.
+ */
+@JsonTypeName(""druid-datasource-scan"")
+public class DruidSubScan extends AbstractBase implements SubScan {
+
+    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidSubScan.class);
+
+    @JsonProperty
+    public final DruidStoragePluginConfig druidStoragePluginConfig;
+
+    @JsonIgnore
+    private final DruidStoragePlugin druidStoragePlugin;
+
+    private final List<DruidSubScanSpec> dataSourceScanSpecList;
+    private final List<SchemaPath> columns;
+
+    @JsonCreator
+    public DruidSubScan(@JacksonInject StoragePluginRegistry registry,
+                       @JsonProperty(""druidStoragePluginConfig"") StoragePluginConfig druidStoragePluginConfig,
+                       @JsonProperty(""datasourceScanSpecList"") LinkedList<DruidSubScanSpec> datasourceScanSpecList,
+                       @JsonProperty(""columns"") List<SchemaPath> columns) throws ExecutionSetupException {
+        super(""someuser"");
+        druidStoragePlugin = (DruidStoragePlugin) registry.getPlugin(druidStoragePluginConfig);
+        this.dataSourceScanSpecList = datasourceScanSpecList;
+        this.druidStoragePluginConfig = (DruidStoragePluginConfig) druidStoragePluginConfig;
+        this.columns = columns;
+    }
+
+    public DruidSubScan(DruidStoragePlugin plugin, DruidStoragePluginConfig config,
+                       List<DruidSubScanSpec> dataSourceInfoList, List<SchemaPath> columns) {
+        super(""someuser"");
+        druidStoragePlugin = plugin;
+        druidStoragePluginConfig = config;
+        this.dataSourceScanSpecList = dataSourceInfoList;
+        this.columns = columns;
+    }
+
+    @Override
+    public <T, X, E extends Throwable> T accept(PhysicalVisitor<T, X, E> physicalVisitor, X value) throws E {
+        return physicalVisitor.visitSubScan(this, value);
+    }
+
+    public List<DruidSubScanSpec> getDataSourceScanSpecList() {
+        return dataSourceScanSpecList;
+    }
+
+    @JsonIgnore
+    public DruidStoragePluginConfig getStorageConfig() {
+        return druidStoragePluginConfig;
+    }
+
+    public List<SchemaPath> getColumns() {
+        return columns;
+    }
+
+    @Override
+    public boolean isExecutable() {
+        return false;
+    }
+
+    @JsonIgnore
+    public DruidStoragePlugin getStorageEngine(){
+        return druidStoragePlugin;
+    }
+
+    @Override
+    public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) throws ExecutionSetupException {
+        Preconditions.checkArgument(children.isEmpty());
+        return new DruidSubScan(druidStoragePlugin, druidStoragePluginConfig, dataSourceScanSpecList, columns);
+    }
+
+    @Override
+    public int getOperatorType() {
+        return 1009;","[{'comment': ""Best to add this to the normal table so that the name appears in query profiles. Yes, it is not a good design to have one global list, but that's how it is until we fix it."", 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidSubScan.java,"@@ -0,0 +1,145 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.physical.base.AbstractBase;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+
+import static java.util.Collections.emptyIterator;
+
+/**
+ * A Class containing information to read a single druid data source.
+ */
+@JsonTypeName(""druid-datasource-scan"")
+public class DruidSubScan extends AbstractBase implements SubScan {
+
+    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidSubScan.class);
+
+    @JsonProperty
+    public final DruidStoragePluginConfig druidStoragePluginConfig;
+
+    @JsonIgnore
+    private final DruidStoragePlugin druidStoragePlugin;
+
+    private final List<DruidSubScanSpec> dataSourceScanSpecList;
+    private final List<SchemaPath> columns;
+
+    @JsonCreator
+    public DruidSubScan(@JacksonInject StoragePluginRegistry registry,
+                       @JsonProperty(""druidStoragePluginConfig"") StoragePluginConfig druidStoragePluginConfig,
+                       @JsonProperty(""datasourceScanSpecList"") LinkedList<DruidSubScanSpec> datasourceScanSpecList,
+                       @JsonProperty(""columns"") List<SchemaPath> columns) throws ExecutionSetupException {
+        super(""someuser"");
+        druidStoragePlugin = (DruidStoragePlugin) registry.getPlugin(druidStoragePluginConfig);
+        this.dataSourceScanSpecList = datasourceScanSpecList;
+        this.druidStoragePluginConfig = (DruidStoragePluginConfig) druidStoragePluginConfig;
+        this.columns = columns;
+    }
+
+    public DruidSubScan(DruidStoragePlugin plugin, DruidStoragePluginConfig config,
+                       List<DruidSubScanSpec> dataSourceInfoList, List<SchemaPath> columns) {
+        super(""someuser"");
+        druidStoragePlugin = plugin;
+        druidStoragePluginConfig = config;
+        this.dataSourceScanSpecList = dataSourceInfoList;
+        this.columns = columns;
+    }
+
+    @Override
+    public <T, X, E extends Throwable> T accept(PhysicalVisitor<T, X, E> physicalVisitor, X value) throws E {
+        return physicalVisitor.visitSubScan(this, value);
+    }
+
+    public List<DruidSubScanSpec> getDataSourceScanSpecList() {
+        return dataSourceScanSpecList;
+    }
+
+    @JsonIgnore
+    public DruidStoragePluginConfig getStorageConfig() {
+        return druidStoragePluginConfig;
+    }
+
+    public List<SchemaPath> getColumns() {
+        return columns;
+    }
+
+    @Override
+    public boolean isExecutable() {
+        return false;
+    }
+
+    @JsonIgnore
+    public DruidStoragePlugin getStorageEngine(){
+        return druidStoragePlugin;
+    }
+
+    @Override
+    public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) throws ExecutionSetupException {
+        Preconditions.checkArgument(children.isEmpty());
+        return new DruidSubScan(druidStoragePlugin, druidStoragePluginConfig, dataSourceScanSpecList, columns);
+    }
+
+    @Override
+    public int getOperatorType() {
+        return 1009;
+    }
+
+    @Override
+    public Iterator<PhysicalOperator> iterator() {
+        return emptyIterator();
+    }
+
+    public static class DruidSubScanSpec {
+
+        protected String dataSourceName;
+        protected String filter;","[{'comment': '`final`, then remove the default constructor.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/common/DruidBoundFilter.java,"@@ -0,0 +1,123 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid.common;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.commons.lang3.StringUtils;
+
+import java.util.HashMap;
+
+public class DruidBoundFilter {
+
+    private static final ObjectMapper objectMapper = new ObjectMapper();
+    private String type = DruidCompareOp.TYPE_BOUND.getCompareOp();
+    private String dimension;
+    private String lower;
+    private String upper;
+    private Boolean lowerStrict =  false;
+    private Boolean upperStrict = false;
+
+    @JsonCreator","[{'comment': 'Need only one of these constructors. The `lowerStrict` can be `boolean`. Then, in the constructor below:\r\n\r\n```\r\nthis.lowerStrict = (lowerStrict == null) ? false : lowerStrict;\r\n```', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/common/DruidBoundFilter.java,"@@ -0,0 +1,123 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid.common;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.commons.lang3.StringUtils;
+
+import java.util.HashMap;
+
+public class DruidBoundFilter {
+
+    private static final ObjectMapper objectMapper = new ObjectMapper();
+    private String type = DruidCompareOp.TYPE_BOUND.getCompareOp();
+    private String dimension;
+    private String lower;
+    private String upper;
+    private Boolean lowerStrict =  false;
+    private Boolean upperStrict = false;
+
+    @JsonCreator
+    public DruidBoundFilter(@JsonProperty(""dimension"") String dimension,
+                               @JsonProperty(""lower"") String lower,
+                            @JsonProperty(""upper"") String upper) {
+        this.dimension = dimension;
+        this.lower = lower;
+        this.upper= upper;
+    }
+
+    @JsonCreator
+    public DruidBoundFilter(@JsonProperty(""dimension"") String dimension,
+                            @JsonProperty(""lower"") String lower,
+                            @JsonProperty(""upper"") String upper,
+                            @JsonProperty(""lowerStrict"") Boolean lowerStrict,
+                            @JsonProperty(""upperStrict"") Boolean upperStrict) {
+        this.dimension = dimension;
+        this.lower = lower;
+        this.upper= upper;
+        this.lowerStrict = lowerStrict;
+        this.upperStrict = upperStrict;
+    }
+
+    public String getType() {
+        return type;
+    }
+
+    public void setType(String type) {
+        this.type = type;
+    }
+
+    public String getDimension() {
+        return dimension;
+    }
+
+    public void setDimension(String dimension) {
+        this.dimension = dimension;
+    }
+
+    public String getLower() {
+        return lower;
+    }
+
+    public void setLower(String lower) {","[{'comment': 'These probably should be `final`, with values pass into the constructor. For stuff used with Calcite, having immutable objects makes the flow easier to reason about.', 'commenter': 'paul-rogers'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidGroupScan.java,"@@ -0,0 +1,247 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.physical.EndpointAffinity;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.proto.CoordinationProtos;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+
+import org.apache.drill.exec.store.schedule.AffinityCreator;
+import org.apache.drill.exec.store.schedule.AssignmentCreator;
+import org.apache.drill.exec.store.schedule.CompleteWork;
+import org.apache.drill.exec.store.schedule.EndpointByteMap;
+import org.apache.drill.exec.store.schedule.EndpointByteMapImpl;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.drill.shaded.guava.com.google.common.collect.ListMultimap;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+
+@JsonTypeName(""druid-scan"")
+public class DruidGroupScan extends AbstractGroupScan {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidGroupScan.class);","[{'comment': 'Please import the `org.slf4j` class directly.', 'commenter': 'cgivre'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidQueryClient.java,"@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.node.ArrayNode;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+import org.apache.commons.collections.CollectionUtils;
+import org.apache.drill.exec.store.druid.druid.DruidSelectResponse;
+import org.apache.drill.exec.store.druid.druid.PagingIdentifier;
+import org.apache.http.HttpEntity;
+import org.apache.http.HttpResponse;
+import org.apache.http.client.HttpClient;
+import org.apache.http.client.methods.HttpPost;
+import org.apache.http.entity.ByteArrayEntity;
+import org.apache.http.impl.client.DefaultHttpClient;
+import org.apache.http.util.EntityUtils;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.Map;
+
+import static org.apache.http.protocol.HTTP.CONTENT_TYPE;
+import static javax.ws.rs.core.MediaType.APPLICATION_JSON;
+
+public class DruidQueryClient {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidQueryClient.class);","[{'comment': 'Same here.', 'commenter': 'cgivre'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidRecordReader.java,"@@ -0,0 +1,173 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.ops.FragmentContext;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.OutputMutator;
+import org.apache.drill.exec.store.AbstractRecordReader;
+import org.apache.drill.exec.store.druid.druid.DruidSelectResponse;
+import org.apache.drill.exec.store.druid.druid.PagingIdentifier;
+import org.apache.drill.exec.store.druid.druid.PagingSpec;
+import org.apache.drill.exec.store.druid.druid.SelectQuery;
+import org.apache.drill.exec.vector.complex.fn.JsonReader;
+import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import java.util.Set;
+
+public class DruidRecordReader extends AbstractRecordReader {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidRecordReader.class);","[{'comment': 'And here...', 'commenter': 'cgivre'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidRecordReader.java,"@@ -0,0 +1,173 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.ops.FragmentContext;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.OutputMutator;
+import org.apache.drill.exec.store.AbstractRecordReader;
+import org.apache.drill.exec.store.druid.druid.DruidSelectResponse;
+import org.apache.drill.exec.store.druid.druid.PagingIdentifier;
+import org.apache.drill.exec.store.druid.druid.PagingSpec;
+import org.apache.drill.exec.store.druid.druid.SelectQuery;
+import org.apache.drill.exec.vector.complex.fn.JsonReader;
+import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import java.util.Set;
+
+public class DruidRecordReader extends AbstractRecordReader {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidRecordReader.class);
+    private DruidStoragePlugin plugin;
+    private final DruidSubScan.DruidSubScanSpec scanSpec;
+    private List<String> dimensions;
+    private String filters;
+    private ArrayList<PagingIdentifier> pagingIdentifiers = new ArrayList<>();
+
+    private JsonReader jsonReader;
+    private VectorContainerWriter writer;
+
+    private OutputMutator output;
+    private OperatorContext context;
+    private final FragmentContext fragmentContext;
+
+    private ObjectMapper objectMapper = new ObjectMapper();
+
+    public DruidRecordReader(DruidSubScan.DruidSubScanSpec subScanSpec, List<SchemaPath> projectedColumns,
+                             FragmentContext context, DruidStoragePlugin plugin) {
+        dimensions = new ArrayList<String>();
+        setColumns(projectedColumns);
+        this.plugin = plugin;
+        scanSpec = subScanSpec;
+        fragmentContext = context;
+        this.filters = subScanSpec.getFilter();
+    }
+
+    @Override
+    protected Collection<SchemaPath> transformColumns(Collection<SchemaPath> projectedColumns) {
+        Set<SchemaPath> transformed = Sets.newLinkedHashSet();
+        if (isStarQuery()) {
+            transformed.add(SchemaPath.STAR_COLUMN);
+        } else {
+            for (SchemaPath column : projectedColumns) {
+                String fieldName = column.getRootSegment().getPath();
+                transformed.add(column);
+                this.dimensions.add(fieldName);
+            }
+        }
+        return transformed;
+    }
+
+    @Override
+    public void setup(OperatorContext context, OutputMutator output) throws ExecutionSetupException {
+        this.context = context;
+        this.output = output;
+        this.writer = new VectorContainerWriter(output);
+
+        //Lists.newArrayList(getColumns()), true, false, false
+        this.jsonReader =
+                new JsonReader.Builder(fragmentContext.getManagedBuffer())
+                        .schemaPathColumns(ImmutableList.copyOf(getColumns()))
+                        .skipOuterList(true)
+                        .build();
+        logger.debug("" Initialized JsonRecordReader. "");
+    }
+
+    @Override
+    public int next() {
+
+        writer.allocate();
+        writer.reset();
+        SelectQuery selectQuery = new SelectQuery(scanSpec.dataSourceName);
+        selectQuery.setDimensions(this.dimensions);
+        selectQuery.setFilter(this.filters);
+
+        ObjectNode paging = objectMapper.createObjectNode();
+        if (this.pagingIdentifiers != null && !this.pagingIdentifiers.isEmpty()) {
+            for (PagingIdentifier pagingIdentifier : this.pagingIdentifiers) {
+                paging.put(pagingIdentifier.getSegmentName(), pagingIdentifier.getSegmentOffset());
+            }
+        }
+
+        PagingSpec pagingSpec = new PagingSpec(paging);
+        selectQuery.setPagingSpec(pagingSpec);
+
+        DruidQueryClient druidQueryClient = plugin.getDruidQueryClient();
+
+        try {
+            String query = selectQuery.toJson();
+            logger.debug(""Executing DRUID query - "" + query);
+            DruidSelectResponse druidSelectResponse = druidQueryClient.ExecuteQuery(query);
+            ArrayList<PagingIdentifier> newPagingIdentifiers = druidSelectResponse.getPagingIdentifiers();
+
+            ArrayList<String> newPagingIdentifierNames = new ArrayList<>();
+            for (PagingIdentifier pagingIdentifier : newPagingIdentifiers) {
+                newPagingIdentifierNames.add(pagingIdentifier.getSegmentName());
+            }
+
+            for (PagingIdentifier pagingIdentifier : this.pagingIdentifiers) {
+                if (!newPagingIdentifierNames.contains(pagingIdentifier.getSegmentName())) {
+                    newPagingIdentifiers.add(
+                            new PagingIdentifier(pagingIdentifier.getSegmentName(),
+                                    pagingIdentifier.getSegmentOffset() + 1)
+                    );
+                }
+            }
+
+            //update the paging identifiers
+            this.pagingIdentifiers = newPagingIdentifiers;
+
+            int docCount = 0;
+            for (ObjectNode eventNode : druidSelectResponse.getEvents()) {
+                writer.setPosition(docCount);
+                jsonReader.setSource(eventNode);
+                try {
+                    jsonReader.write(writer);
+                } catch (IOException e) {
+                    String msg = ""Failure while reading document. - Parser was at record: "" + eventNode.toString();
+                    logger.error(msg, e);
+                    throw new DrillRuntimeException(msg, e);
+                }
+                docCount++;
+            }
+
+            writer.setValueCount(docCount);
+            return docCount;
+        } catch (IOException e) {
+            String msg = ""Failure while reading documents"";
+            logger.error(msg, e);
+            throw new DrillRuntimeException(msg, e);
+        }
+    }
+
+    @Override
+    public void close() throws Exception {
+","[{'comment': 'You should close out any readers here.', 'commenter': 'cgivre'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidScanBatchCreator.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.ops.ExecutorFragmentContext;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.impl.BatchCreator;
+import org.apache.drill.exec.physical.impl.ScanBatch;
+import org.apache.drill.exec.record.CloseableRecordBatch;
+import org.apache.drill.exec.record.RecordBatch;
+import org.apache.drill.exec.store.RecordReader;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+
+import java.util.List;
+
+public class DruidScanBatchCreator implements BatchCreator<DruidSubScan> {
+
+    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidScanBatchCreator.class);","[{'comment': 'Please make private and import the `org.slf4j` class as well.  ', 'commenter': 'cgivre'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidStoragePluginConfig.java,"@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+
+@JsonTypeName(DruidStoragePluginConfig.NAME)
+public class DruidStoragePluginConfig extends StoragePluginConfigBase {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidStoragePluginConfig.class);","[{'comment': 'You know the routine about the logger ;-)', 'commenter': 'cgivre'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidStoragePluginConfig.java,"@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+
+@JsonTypeName(DruidStoragePluginConfig.NAME)
+public class DruidStoragePluginConfig extends StoragePluginConfigBase {
+
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidStoragePluginConfig.class);
+
+    public static final String NAME = ""druid"";
+
+    @JsonProperty
+    private final String brokerAddress;
+
+    @JsonProperty
+    private final String coordinatorAddress;
+
+    @JsonCreator
+    public DruidStoragePluginConfig(
+            @JsonProperty(""brokerAddress"") String brokerAddress,
+            @JsonProperty(""coordinatorAddress"") String coordinatorAddress) {
+
+        this.brokerAddress = brokerAddress;
+        this.coordinatorAddress = coordinatorAddress;
+        logger.info(""Broker Address - {}, Coordinator Address - {}"", brokerAddress, coordinatorAddress);
+        //TODO Make this configurable.
+    }
+
+    @Override
+    public boolean equals(Object that) {
+        if (this == that) {
+            return true;
+        } else if (that == null || getClass() != that.getClass()) {
+            return false;
+        }
+        DruidStoragePluginConfig thatConfig = (DruidStoragePluginConfig) that;
+        return
+                (this.brokerAddress.equals(thatConfig.brokerAddress)
+                        && this.coordinatorAddress.equals(thatConfig.coordinatorAddress));
+    }
+
+    @Override
+    public int hashCode() {
+        int brokerAddressHashCode = this.brokerAddress != null ? this.brokerAddress.hashCode() : 0;
+        int coordinatorAddressHashCode = this.coordinatorAddress != null ? this.coordinatorAddress.hashCode() : 0;
+        return brokerAddressHashCode ^ coordinatorAddressHashCode;","[{'comment': 'There are easier ways of doing this. \r\nTake a look here: \r\nhttps://github.com/apache/drill/blob/6c9257b6c73e038d86c9d7b65fb7ac798d1dac4d/contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelFormatConfig.java#L86-#L107', 'commenter': 'cgivre'}]"
1888,contrib/storage-druid/src/main/java/org/apache/drill/exec/store/druid/DruidAdminClient.java,"@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.druid;
+
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.http.HttpResponse;
+import org.apache.http.client.HttpClient;
+import org.apache.http.client.methods.HttpGet;
+import org.apache.http.impl.client.DefaultHttpClient;
+import org.apache.http.util.EntityUtils;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import static javax.ws.rs.core.HttpHeaders.CONTENT_TYPE;
+import static javax.ws.rs.core.MediaType.APPLICATION_JSON;
+
+public class DruidAdminClient {
+
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DruidAdminClient.class);","[{'comment': 'Please fix `logger` import.  ', 'commenter': 'cgivre'}]"
1892,contrib/storage-http/README.md,"@@ -0,0 +1,31 @@
+
+# Generic API Storage Plugin
+This plugin is intended to enable you to query APIs over HTTP/REST.  At this point, the API reader will only accept JSON as input however in the future, it may be possible to","[{'comment': '""..accept JSON as input. However, in the future..."" Run on sentence.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/README.md,"@@ -0,0 +1,31 @@
+
+# Generic API Storage Plugin
+This plugin is intended to enable you to query APIs over HTTP/REST.  At this point, the API reader will only accept JSON as input however in the future, it may be possible to
+ add additional format readers to allow for APIs which return XML, CSV or other formats.  ","[{'comment': 'I presume the response has to look like what the JSON reader expects? As JSON objects with the fields delimited by object fields?\r\n\r\nUnfortunately, the JSON which the JSON reader expects is not valid since files usually contain a list of objects, which is not valid JSON. Valid JSON would be an array of objects, which the JSON reader can handle. That is, the following is a typical JSON file:\r\n\r\n```\r\n{a: 10, b: ""foo""}\r\n{a: 20, b: ""bar""}\r\n```\r\n\r\nBut the following is a typical JSON message response:\r\n\r\n```\r\n[ {a: 10, b: ""foo""},\r\n{a: 20, b: ""bar""} ]\r\n```\r\n\r\nOr:\r\n\r\n```\r\n{ status: ""OK"",\r\n  records: [ {a: 10, b: ""foo""},\r\n{a: 20, b: ""bar""} ]\r\n}\r\n```\r\n\r\nHow does this plugin handle those? Even worse, does it handle common forms like:\r\n\r\n```\r\n{ status: ""OK"",\r\n  description: ""Request accepted"",\r\n  records: [ {a: 10, b: ""foo""},\r\n{a: 20, b: ""bar""} ]\r\n}\r\n```\r\n\r\nOr:\r\n```\r\n{ status: ""OK"",\r\n  records: [\r\n    {values: [ {key: ""a"", value: 10}, {key: ""b"", value: ""foo""} ],\r\n              [ {key: ""a"", value: 20}, {key: ""b"", value: ""bar""} ] ]\r\n} }\r\n```\r\n\r\nThat is, records are expressed as a series of key/value pairs so that the column schema is not fixed. In some cases, I\'ve seen such messages include a ""schema"" section to explain the dynamic schema.\r\n\r\n', 'commenter': 'paul-rogers'}, {'comment': ""I updated the plugin to use Drill's JSONRecordReader. Having the home-grown version just didn't seem like a good idea, especially with the work that @paul-rogers is doing to update the reader to use EVF. "", 'commenter': 'cgivre'}]"
1892,contrib/storage-http/README.md,"@@ -0,0 +1,31 @@
+
+# Generic API Storage Plugin
+This plugin is intended to enable you to query APIs over HTTP/REST.  At this point, the API reader will only accept JSON as input however in the future, it may be possible to
+ add additional format readers to allow for APIs which return XML, CSV or other formats.  
+ 
+Note:  This plugin should **NOT** be used for interacting with tools which have REST APIs such as Splunk or Solr.  It will not be performant for those use cases.  
+
+## Configuration
+To configure the plugin, create a new storage plugin, and add the following configuration options:
+
+```
+{
+  ""type"": ""http"",
+  ""connection"": ""https://api.sunrise-sunset.org/"",
+  ""resultKey"": ""results"",
+  ""enabled"": true","[{'comment': 'Many APIs require some form of security key. Sometimes set as a cookie, sometimes embedded in the REST request.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpEqualFunctionProcessor.java,"@@ -0,0 +1,120 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import org.apache.drill.common.expression.FunctionCall;
+import org.apache.drill.common.expression.LogicalExpression;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.expression.ValueExpressions.BooleanExpression;
+import org.apache.drill.common.expression.ValueExpressions.DoubleExpression;
+import org.apache.drill.common.expression.ValueExpressions.FloatExpression;
+import org.apache.drill.common.expression.ValueExpressions.IntExpression;
+import org.apache.drill.common.expression.ValueExpressions.LongExpression;
+import org.apache.drill.common.expression.ValueExpressions.QuotedString;
+import org.apache.drill.common.expression.visitors.AbstractExprVisitor;
+
+public class HttpEqualFunctionProcessor extends AbstractExprVisitor<Boolean, LogicalExpression, RuntimeException> {","[{'comment': ""Maybe a comment to explain what this is trying to do? I'm having a hard time guessing."", 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpFilterBuilder.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.List;
+
+import org.apache.drill.common.expression.BooleanOperator;
+import org.apache.drill.common.expression.FunctionCall;
+import org.apache.drill.common.expression.LogicalExpression;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.expression.visitors.AbstractExprVisitor;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpFilterBuilder extends
+  AbstractExprVisitor<HttpScanSpec, Void, RuntimeException> {
+  static final Logger logger = LoggerFactory.getLogger(HttpFilterBuilder.class);
+  private final HttpGroupScan groupScan;
+  private final LogicalExpression le;
+  private boolean allExpressionsConverted = true;
+
+  public boolean isAllExpressionsConverted() {
+    return allExpressionsConverted;
+  }
+
+  public HttpFilterBuilder(HttpGroupScan groupScan, LogicalExpression conditionExp) {
+    this.groupScan = groupScan;
+    this.le = conditionExp;
+    logger.debug(""HttpFilterBuilder created"");
+  }
+
+  public HttpScanSpec parseTree() {
+    HttpScanSpec parsedSpec = le.accept(this, null);
+    if (parsedSpec != null) {
+      parsedSpec = mergeScanSpecs(this.groupScan.getScanSpec(), parsedSpec);
+    }
+    return parsedSpec;
+  }
+
+  private HttpScanSpec mergeScanSpecs(HttpScanSpec leftScanSpec, HttpScanSpec rightScanSpec) {
+    leftScanSpec.merge(rightScanSpec);
+    return leftScanSpec;
+  }
+
+  @Override
+  public HttpScanSpec visitUnknown(LogicalExpression e, Void value)
+    throws RuntimeException {
+    allExpressionsConverted = false;
+    return null;
+  }
+
+  // only process `boolean and` expression
+  // `a and b and c` will call this, and argument size = 3
+  @Override
+  public HttpScanSpec visitBooleanOperator(BooleanOperator op, Void value) {","[{'comment': 'Drill supports both AND and OR. Looks like this code checks for AND, but happily handles OR clauses.\r\n\r\nFWIW: I\'m trying to create a generic form of this stuff since as I discovered the hard way, it is very hard to get this stuff right and nearly impossible to test generically. There will be a PR with a generic form and a ""test mule"" data source that shows handling of AND and OR. It also includes a generic processor to translate from Drill structure to a very simple RelOp structure. Your choice as to whether to forge ahead with the current approach, or wait for the new stuff.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpFilterBuilder.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.List;
+
+import org.apache.drill.common.expression.BooleanOperator;
+import org.apache.drill.common.expression.FunctionCall;
+import org.apache.drill.common.expression.LogicalExpression;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.expression.visitors.AbstractExprVisitor;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpFilterBuilder extends
+  AbstractExprVisitor<HttpScanSpec, Void, RuntimeException> {
+  static final Logger logger = LoggerFactory.getLogger(HttpFilterBuilder.class);
+  private final HttpGroupScan groupScan;
+  private final LogicalExpression le;
+  private boolean allExpressionsConverted = true;
+
+  public boolean isAllExpressionsConverted() {
+    return allExpressionsConverted;
+  }
+
+  public HttpFilterBuilder(HttpGroupScan groupScan, LogicalExpression conditionExp) {
+    this.groupScan = groupScan;
+    this.le = conditionExp;
+    logger.debug(""HttpFilterBuilder created"");
+  }
+
+  public HttpScanSpec parseTree() {
+    HttpScanSpec parsedSpec = le.accept(this, null);
+    if (parsedSpec != null) {
+      parsedSpec = mergeScanSpecs(this.groupScan.getScanSpec(), parsedSpec);
+    }
+    return parsedSpec;
+  }
+
+  private HttpScanSpec mergeScanSpecs(HttpScanSpec leftScanSpec, HttpScanSpec rightScanSpec) {
+    leftScanSpec.merge(rightScanSpec);
+    return leftScanSpec;
+  }
+
+  @Override
+  public HttpScanSpec visitUnknown(LogicalExpression e, Void value)
+    throws RuntimeException {
+    allExpressionsConverted = false;
+    return null;
+  }
+
+  // only process `boolean and` expression
+  // `a and b and c` will call this, and argument size = 3
+  @Override
+  public HttpScanSpec visitBooleanOperator(BooleanOperator op, Void value) {
+    List<LogicalExpression> args = op.args;
+    HttpScanSpec nodeScanSpec = null;
+    String functionName = op.getName();
+    if (!functionName.equals(""booleanAnd"")) {
+      allExpressionsConverted = false;
+      return nodeScanSpec;
+    }
+    logger.debug(""boolean 'and' operator {}"", args.size());
+    for (int i = 0; i < args.size(); ++i) {
+      if (nodeScanSpec == null) {
+        nodeScanSpec = args.get(i).accept(this, null);
+      } else {
+        HttpScanSpec scanSpec = args.get(i).accept(this, null);
+        if (scanSpec != null) {
+          nodeScanSpec = mergeScanSpecs(nodeScanSpec, scanSpec);
+        } else {
+          allExpressionsConverted = false;
+        }
+      }
+    }
+    return nodeScanSpec;
+  }
+
+  // only process expression like `$key=value`
+  @Override
+  public HttpScanSpec visitFunctionCall(FunctionCall call, Void value) throws RuntimeException {
+    HttpScanSpec nodeScanSpec = null;
+    String functionName = call.getName();
+    logger.debug(""visit function call {}"", functionName);
+    if (!HttpEqualFunctionProcessor.match(functionName)) {
+      allExpressionsConverted = false;
+      return nodeScanSpec;
+    }
+    HttpEqualFunctionProcessor processor = HttpEqualFunctionProcessor.process(call);
+    if (!processor.isSuccess()) {
+      allExpressionsConverted = false;","[{'comment': ""Actually, it is not enough to know if all nodes are converted. Consider the following:\r\n\r\n```\r\n... WHERE x='foo' AND myUDF(y) = 'bar' AND isMumble = true\r\n```\r\n\r\nWe can, perhaps, convert the following to:\r\n\r\n```\r\nHTTP: ...?foo=bar&isMumble=true\r\nSQL: WHERE myUDF(y) = 'bar'\r\n```\r\n\r\nThat is, some predicates are pushed, others are left for Drill. (The new framework mentioned above handles all this.)"", 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpGroupScan.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.List;
+import org.apache.drill.common.expression.SchemaPath;
+
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.physical.base.ScanStats.GroupScanProperty;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpGroupScan extends AbstractGroupScan {
+  private static final Logger logger = LoggerFactory.getLogger(HttpGroupScan.class);
+
+  private final List<SchemaPath> columns;
+  private final HttpScanSpec httpScanSpec;
+  private final HttpStoragePluginConfig httpStoragePluginConfig;
+  private boolean filterPushedDown = true;
+
+  public HttpGroupScan (","[{'comment': 'Group scans are wildly complicate (too much so.) One of the odd things is that they are serialized to JSON. (As part of the ""logical plan."") This means that this class must be Jackson-serializable. Here, your plugin takes the config (which is serializable). If you find you need the plugin itself, then there is a hokey-pokey you must do. See [this wiki](https://github.com/paul-rogers/drill/wiki/Create-a-Storage-Plugin) for more info.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpGroupScan.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.List;
+import org.apache.drill.common.expression.SchemaPath;
+
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.physical.base.ScanStats.GroupScanProperty;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpGroupScan extends AbstractGroupScan {
+  private static final Logger logger = LoggerFactory.getLogger(HttpGroupScan.class);
+
+  private final List<SchemaPath> columns;
+  private final HttpScanSpec httpScanSpec;
+  private final HttpStoragePluginConfig httpStoragePluginConfig;
+  private boolean filterPushedDown = true;
+
+  public HttpGroupScan (
+    HttpStoragePluginConfig config,
+    HttpScanSpec scanSpec,
+    List<SchemaPath> columns
+  ) {
+    super(""no-user"");
+    this.httpStoragePluginConfig = config;
+    this.httpScanSpec = scanSpec;
+    this.columns = columns == null || columns.size() == 0 ? ALL_COLUMNS : columns;
+  }
+
+  public HttpGroupScan(HttpGroupScan that) {
+    super(that);
+    httpStoragePluginConfig = that.getStorageConfig();
+    httpScanSpec = that.getScanSpec();
+    columns = that.getColumns();
+  }
+
+  @Override
+  public void applyAssignments(List<DrillbitEndpoint> endpoints) {
+    logger.debug(""HttpGroupScan applyAssignments"");
+  }
+
+  @Override
+  public int getMaxParallelizationWidth() {
+    return 1;
+  }
+
+
+  @Override
+  public boolean canPushdownProjects(List<SchemaPath> columns) {
+    return true;
+  }
+
+  @Override
+  public SubScan getSpecificScan(int minorFragmentId) {
+    logger.debug(""HttpGroupScan getSpecificScan"");
+    return new HttpSubScan(httpStoragePluginConfig, httpScanSpec, columns);","[{'comment': ""This will be called once because `getMaxParallelizationWidth()` returns 1. Should this plugin allow sharding requests? If I'm requesting a time range, say, should the plugin try to split the time ranges into multiple small requests that Drill can spread across Drillbits? That is a VERY complex topic that I'm still trying to sort out..."", 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpGroupScan.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.List;
+import org.apache.drill.common.expression.SchemaPath;
+
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.physical.base.ScanStats.GroupScanProperty;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpGroupScan extends AbstractGroupScan {
+  private static final Logger logger = LoggerFactory.getLogger(HttpGroupScan.class);
+
+  private final List<SchemaPath> columns;
+  private final HttpScanSpec httpScanSpec;
+  private final HttpStoragePluginConfig httpStoragePluginConfig;
+  private boolean filterPushedDown = true;
+
+  public HttpGroupScan (
+    HttpStoragePluginConfig config,
+    HttpScanSpec scanSpec,
+    List<SchemaPath> columns
+  ) {
+    super(""no-user"");
+    this.httpStoragePluginConfig = config;
+    this.httpScanSpec = scanSpec;
+    this.columns = columns == null || columns.size() == 0 ? ALL_COLUMNS : columns;
+  }
+
+  public HttpGroupScan(HttpGroupScan that) {
+    super(that);
+    httpStoragePluginConfig = that.getStorageConfig();
+    httpScanSpec = that.getScanSpec();
+    columns = that.getColumns();
+  }
+
+  @Override
+  public void applyAssignments(List<DrillbitEndpoint> endpoints) {
+    logger.debug(""HttpGroupScan applyAssignments"");
+  }
+
+  @Override
+  public int getMaxParallelizationWidth() {
+    return 1;
+  }
+
+
+  @Override
+  public boolean canPushdownProjects(List<SchemaPath> columns) {
+    return true;
+  }
+
+  @Override
+  public SubScan getSpecificScan(int minorFragmentId) {
+    logger.debug(""HttpGroupScan getSpecificScan"");
+    return new HttpSubScan(httpStoragePluginConfig, httpScanSpec, columns);
+  }
+
+  @Override
+  public GroupScan clone(List<SchemaPath> columns) {
+    logger.debug(""HttpGroupScan clone {}"", columns);
+    return new HttpGroupScan(this);
+  }
+
+  @Override
+  public String getDigest() {
+    return toString();
+  }
+
+  @Override
+  public List<SchemaPath> getColumns() {
+    return columns;
+  }
+
+  @Override
+  public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {
+    Preconditions.checkArgument(children.isEmpty());
+    return new HttpGroupScan(this);
+  }
+
+  @Override
+  public ScanStats getScanStats() {","[{'comment': 'These stats are more important than they look. Seems that you are doing filter push down (though I see no members on this class to hold those filters.) When you do, you must ensure that the version of your group scan with filters has a lower cost than the one without, else Calcite won\'t actually push down the filters. (That is a VERY difficult bug to find!)\r\n\r\nAlso, the esimated record count here is 1. This means Drill is free to ""broadcast"" the rows to all Drillbits if this table appears in a join. But, if the API could return 1M rows, that will turn out to be a very bad choice (Drill should have done a partitioned hash join instead.) So, choose a row count that is the maximum of what you\'d expect. No harm in being high, there is harm in being low.\r\n\r\nThen, make two adjustments:\r\n\r\n1. Once columns are pushed down, reduce the estimated row width (from 200 to 100, say). That will help Calcite realize that pushing projection is a good choice.\r\n2. Once filters are pushed down, reduce the estimated row count (by half, say.) That again will tell Calcite to prefer the version with filter push down.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpGroupScan.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.List;
+import org.apache.drill.common.expression.SchemaPath;
+
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.physical.base.ScanStats.GroupScanProperty;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpGroupScan extends AbstractGroupScan {
+  private static final Logger logger = LoggerFactory.getLogger(HttpGroupScan.class);
+
+  private final List<SchemaPath> columns;
+  private final HttpScanSpec httpScanSpec;
+  private final HttpStoragePluginConfig httpStoragePluginConfig;
+  private boolean filterPushedDown = true;
+
+  public HttpGroupScan (
+    HttpStoragePluginConfig config,
+    HttpScanSpec scanSpec,
+    List<SchemaPath> columns
+  ) {
+    super(""no-user"");
+    this.httpStoragePluginConfig = config;
+    this.httpScanSpec = scanSpec;
+    this.columns = columns == null || columns.size() == 0 ? ALL_COLUMNS : columns;
+  }
+
+  public HttpGroupScan(HttpGroupScan that) {
+    super(that);
+    httpStoragePluginConfig = that.getStorageConfig();
+    httpScanSpec = that.getScanSpec();
+    columns = that.getColumns();
+  }
+
+  @Override
+  public void applyAssignments(List<DrillbitEndpoint> endpoints) {
+    logger.debug(""HttpGroupScan applyAssignments"");
+  }
+
+  @Override
+  public int getMaxParallelizationWidth() {
+    return 1;
+  }
+
+
+  @Override
+  public boolean canPushdownProjects(List<SchemaPath> columns) {
+    return true;
+  }
+
+  @Override
+  public SubScan getSpecificScan(int minorFragmentId) {
+    logger.debug(""HttpGroupScan getSpecificScan"");
+    return new HttpSubScan(httpStoragePluginConfig, httpScanSpec, columns);
+  }
+
+  @Override
+  public GroupScan clone(List<SchemaPath> columns) {
+    logger.debug(""HttpGroupScan clone {}"", columns);
+    return new HttpGroupScan(this);
+  }
+
+  @Override
+  public String getDigest() {
+    return toString();
+  }
+
+  @Override
+  public List<SchemaPath> getColumns() {
+    return columns;
+  }
+
+  @Override
+  public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {
+    Preconditions.checkArgument(children.isEmpty());
+    return new HttpGroupScan(this);
+  }
+
+  @Override
+  public ScanStats getScanStats() {
+    int estRowCount = 1;
+    int estDataSize = estRowCount * 200;
+    int estCpuCost = 1;
+    return new ScanStats(GroupScanProperty.NO_EXACT_ROW_COUNT,estRowCount, estCpuCost, estDataSize);
+  }
+
+
+  public boolean isFilterPushedDown() {
+    return filterPushedDown;
+  }
+
+  public void setFilterPushedDown(boolean filterPushedDown) {
+    this.filterPushedDown = filterPushedDown;","[{'comment': 'How are you tracking the filter so you can pass it to the SubScan which will pass it to the reader?', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpRecordReader.java,"@@ -0,0 +1,149 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.ops.FragmentContext;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.OutputMutator;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.ExecConstants;
+
+import org.apache.drill.exec.store.AbstractRecordReader;
+import org.apache.drill.exec.store.http.util.JsonConverter;
+import org.apache.drill.exec.store.http.util.SimpleHttp;
+import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;
+import org.apache.drill.exec.vector.complex.fn.JsonReader;
+
+import com.fasterxml.jackson.databind.JsonNode;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpRecordReader extends AbstractRecordReader {
+  static final Logger logger = LoggerFactory.getLogger(HttpRecordReader.class);
+
+  private VectorContainerWriter writer;
+  private JsonReader jsonReader;
+  private FragmentContext fragmentContext;
+  private HttpSubScan subScan;
+  private Iterator<JsonNode> jsonIt;
+  private JsonNode root;
+  private ResultSetLoader loader;
+  private final boolean enableAllTextMode;
+  private final boolean enableNanInf;
+  private final boolean readNumbersAsDouble;
+  private final HttpStoragePluginConfig config;
+
+  public HttpRecordReader(FragmentContext context, List<SchemaPath> projectedColumns, HttpStoragePluginConfig config, HttpSubScan subScan) {
+    this.config = config;
+    this.subScan = subScan;
+    fragmentContext = context;
+    setColumns(projectedColumns);
+
+    enableAllTextMode = fragmentContext.getOptions().getOption(ExecConstants.JSON_ALL_TEXT_MODE).bool_val;
+    enableNanInf = fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_NAN_INF_NUMBERS).bool_val;
+    readNumbersAsDouble = fragmentContext.getOptions().getOption(ExecConstants.JSON_READ_NUMBERS_AS_DOUBLE).bool_val;
+  }
+
+  @Override
+  public void setup(OperatorContext context,
+                    OutputMutator output)
+    throws ExecutionSetupException {
+    this.writer = new VectorContainerWriter(output);
+    this.jsonReader = new JsonReader.Builder(fragmentContext.getManagedBuffer())
+      .schemaPathColumns(Lists.newArrayList(getColumns()))
+      .allTextMode(enableAllTextMode)
+      .readNumbersAsDouble(readNumbersAsDouble)
+      .enableNanInf(enableNanInf)
+      .build();
+    String q = subScan.getURL();
+    if (q.startsWith(""file://"")) {
+      loadFile();
+    } else {
+      loadHttp();
+    }
+  }
+
+  private void loadHttp() {
+    String url = subScan.getFullURL();
+    SimpleHttp http = new SimpleHttp();
+    String content = http.get(url);","[{'comment': 'This seems a bit too simplistic.\r\n\r\n* No login/tokens/cookies passed to validate session.\r\n* No error reporting (just throws a generic error)?\r\n* If result is large, loading xxMB into a string is excessive, especially since we later read it as a stream.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpRecordReader.java,"@@ -0,0 +1,149 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.ops.FragmentContext;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.OutputMutator;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.ExecConstants;
+
+import org.apache.drill.exec.store.AbstractRecordReader;
+import org.apache.drill.exec.store.http.util.JsonConverter;
+import org.apache.drill.exec.store.http.util.SimpleHttp;
+import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;
+import org.apache.drill.exec.vector.complex.fn.JsonReader;
+
+import com.fasterxml.jackson.databind.JsonNode;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpRecordReader extends AbstractRecordReader {
+  static final Logger logger = LoggerFactory.getLogger(HttpRecordReader.class);
+
+  private VectorContainerWriter writer;
+  private JsonReader jsonReader;
+  private FragmentContext fragmentContext;
+  private HttpSubScan subScan;
+  private Iterator<JsonNode> jsonIt;
+  private JsonNode root;
+  private ResultSetLoader loader;
+  private final boolean enableAllTextMode;
+  private final boolean enableNanInf;
+  private final boolean readNumbersAsDouble;
+  private final HttpStoragePluginConfig config;
+
+  public HttpRecordReader(FragmentContext context, List<SchemaPath> projectedColumns, HttpStoragePluginConfig config, HttpSubScan subScan) {
+    this.config = config;
+    this.subScan = subScan;
+    fragmentContext = context;
+    setColumns(projectedColumns);
+
+    enableAllTextMode = fragmentContext.getOptions().getOption(ExecConstants.JSON_ALL_TEXT_MODE).bool_val;
+    enableNanInf = fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_NAN_INF_NUMBERS).bool_val;
+    readNumbersAsDouble = fragmentContext.getOptions().getOption(ExecConstants.JSON_READ_NUMBERS_AS_DOUBLE).bool_val;
+  }
+
+  @Override
+  public void setup(OperatorContext context,
+                    OutputMutator output)
+    throws ExecutionSetupException {
+    this.writer = new VectorContainerWriter(output);
+    this.jsonReader = new JsonReader.Builder(fragmentContext.getManagedBuffer())
+      .schemaPathColumns(Lists.newArrayList(getColumns()))
+      .allTextMode(enableAllTextMode)
+      .readNumbersAsDouble(readNumbersAsDouble)
+      .enableNanInf(enableNanInf)
+      .build();
+    String q = subScan.getURL();
+    if (q.startsWith(""file://"")) {
+      loadFile();
+    } else {
+      loadHttp();
+    }
+  }
+
+  private void loadHttp() {
+    String url = subScan.getFullURL();
+    SimpleHttp http = new SimpleHttp();
+    String content = http.get(url);
+    logger.info(""http '{}' response {} bytes"", url, content.length());
+    parseResult(content);
+  }
+
+  private void loadFile() {
+    logger.debug(""load local file {}"", subScan.getTableSpec().getURI());
+    String file = subScan.getTableSpec().getURI().substring(""file://"".length() - 1);
+    String content = JsonConverter.stringFromFile(file);","[{'comment': 'Same issues as above.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpScanSpec.java,"@@ -0,0 +1,79 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.shaded.guava.com.google.common.base.Joiner;
+
+@JsonTypeName(""http-scan-spec"")
+public class HttpScanSpec {
+  private String uri;
+  private Map<String, Object> args = new HashMap<String, Object>();
+  private String pluginName = HttpStoragePluginConfig.NAME;
+
+  @JsonCreator
+  public HttpScanSpec(@JsonProperty(""uri"") String uri) {
+    this.uri = uri;
+  }
+
+  public HttpScanSpec(String uri, String key, Object val) {
+    this.uri = uri;
+    this.args.put(key, val);
+  }
+
+  @JsonIgnore
+  public String getURI() {
+    return uri;
+  }
+
+  @JsonIgnore
+  public String getURL() {","[{'comment': 'Is there no existing code that does this in one of the Apache commons libraries? Here are [some suggestions](https://stackoverflow.com/questions/2809877/how-to-convert-map-to-url-query-string).', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpScanSpec.java,"@@ -0,0 +1,79 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.shaded.guava.com.google.common.base.Joiner;
+
+@JsonTypeName(""http-scan-spec"")
+public class HttpScanSpec {
+  private String uri;
+  private Map<String, Object> args = new HashMap<String, Object>();
+  private String pluginName = HttpStoragePluginConfig.NAME;","[{'comment': 'All `final`. It is important that these objects are immutable: copy to change, don\'t change existing instances. Calcite will copy/modify, then compare the ""before"" and ""after"" versions.', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpGroupScan.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.util.List;
+import org.apache.drill.common.expression.SchemaPath;
+
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.physical.base.ScanStats.GroupScanProperty;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpGroupScan extends AbstractGroupScan {
+  private static final Logger logger = LoggerFactory.getLogger(HttpGroupScan.class);
+
+  private final List<SchemaPath> columns;
+  private final HttpScanSpec httpScanSpec;
+  private final HttpStoragePluginConfig httpStoragePluginConfig;
+  private boolean filterPushedDown = true;","[{'comment': 'This only works if it is copied on each copy. Better to check if filters exist in yuor `httpScanSpec` object.', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpSchemaFactory.java,"@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.calcite.schema.SchemaPlus;
+import org.apache.calcite.schema.Table;
+import org.apache.drill.exec.planner.logical.DynamicDrillTable;
+import org.apache.drill.exec.store.AbstractSchema;
+import org.apache.drill.exec.store.AbstractSchemaFactory;
+import org.apache.drill.exec.store.SchemaConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+public class HttpSchemaFactory extends AbstractSchemaFactory {
+  private static final Logger logger = LoggerFactory.getLogger(HttpSchemaFactory.class);
+
+  private final HttpStoragePlugin plugin;
+
+  public static final String MY_TABLE = ""result_table"";
+
+  public HttpSchemaFactory(HttpStoragePlugin plugin, String schemaName) {
+    super(schemaName);
+    this.plugin = plugin;
+  }
+
+  @Override
+  public void registerSchemas(SchemaConfig schemaConfig, SchemaPlus parent) throws IOException {
+    logger.debug(""registerSchema {}"", getName());
+    HttpSchema schema = new HttpSchema(getName());
+    parent.add(schema.getName(), schema);
+  }
+
+  class HttpSchema extends AbstractSchema {
+
+    private final Map<String, DynamicDrillTable> activeTables = new HashMap<>();
+
+    public HttpSchema(String name) {
+      super(Collections.emptyList(), name);
+    }
+
+    @Override
+    public Table getTable(String tableName) { // table name can be any of string
+      DynamicDrillTable table = activeTables.get(tableName);
+      if (table != null) {
+        return table;
+      }
+
+      logger.debug(""HttpSchema.getTable {}"", tableName);
+      HttpScanSpec spec = new HttpScanSpec(tableName);","[{'comment': 'What does the table name mean here? Is this the REST endpoint?\r\n\r\n```\r\n<base url>/<table name>?<params>\r\n```\r\n?', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpStoragePlugin.java,"@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import org.apache.calcite.schema.SchemaPlus;
+import org.apache.drill.common.JSONOptions;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.store.AbstractStoragePlugin;
+import org.apache.drill.exec.store.SchemaConfig;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.core.type.TypeReference;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+public class HttpStoragePlugin extends AbstractStoragePlugin {
+  static final Logger logger = LoggerFactory.getLogger(HttpStoragePlugin.class);
+
+  private DrillbitContext context;","[{'comment': '`final`', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpStoragePluginConfig.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.shaded.guava.com.google.common.base.MoreObjects;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+import java.util.Arrays;
+
+@JsonTypeName(HttpStoragePluginConfig.NAME)
+public class HttpStoragePluginConfig extends StoragePluginConfigBase {
+  private static final Logger logger = LoggerFactory.getLogger(HttpStoragePluginConfig.class);
+
+  public static final String NAME = ""http"";
+
+  public String connection;
+
+  public String resultKey;","[{'comment': 'Docuemtation would be super helpful. I assume `connection` is the URL up to the resource? `http[s]://foo.bar.com`. Should it have the trailing slash or not? That is, should it be `http[s]://foo.bar.com/`?\r\n\r\nAnd, is the `resultKey` the resource? `<connection>/<resultKey>`?', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpStoragePluginConfig.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.shaded.guava.com.google.common.base.MoreObjects;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+import java.util.Arrays;
+
+@JsonTypeName(HttpStoragePluginConfig.NAME)
+public class HttpStoragePluginConfig extends StoragePluginConfigBase {
+  private static final Logger logger = LoggerFactory.getLogger(HttpStoragePluginConfig.class);
+
+  public static final String NAME = ""http"";
+
+  public String connection;
+
+  public String resultKey;
+
+  @JsonCreator
+  public HttpStoragePluginConfig(@JsonProperty(""connection"") String connection,
+                                 @JsonProperty(""resultKey"") String resultKey) {
+    logger.debug(""initialize HttpStoragePluginConfig {}"", connection);
+    this.connection = connection;
+    this.resultKey = resultKey;
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    if (this == that) {
+      return true;
+    } else if (that == null || getClass() != that.getClass()) {
+      return false;
+    }
+    HttpStoragePluginConfig t = (HttpStoragePluginConfig) that;
+    return this.connection.equals(t.connection) &&","[{'comment': 'Will crash if the strings are null. Consider `Objects.equals()`.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpStoragePluginConfig.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.shaded.guava.com.google.common.base.MoreObjects;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+import java.util.Arrays;
+
+@JsonTypeName(HttpStoragePluginConfig.NAME)
+public class HttpStoragePluginConfig extends StoragePluginConfigBase {
+  private static final Logger logger = LoggerFactory.getLogger(HttpStoragePluginConfig.class);
+
+  public static final String NAME = ""http"";
+
+  public String connection;
+
+  public String resultKey;
+
+  @JsonCreator
+  public HttpStoragePluginConfig(@JsonProperty(""connection"") String connection,
+                                 @JsonProperty(""resultKey"") String resultKey) {
+    logger.debug(""initialize HttpStoragePluginConfig {}"", connection);
+    this.connection = connection;
+    this.resultKey = resultKey;
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    if (this == that) {
+      return true;
+    } else if (that == null || getClass() != that.getClass()) {
+      return false;
+    }
+    HttpStoragePluginConfig t = (HttpStoragePluginConfig) that;
+    return this.connection.equals(t.connection) &&
+      this.resultKey.equals(t.resultKey);
+  }
+
+  @Override
+  public int hashCode() {
+    return Arrays.hashCode(new Object[]{connection, resultKey});","[{'comment': 'Consider `Objects.hashCode(a, b, c)`.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpStoragePluginConfig.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.shaded.guava.com.google.common.base.MoreObjects;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+import java.util.Arrays;
+
+@JsonTypeName(HttpStoragePluginConfig.NAME)
+public class HttpStoragePluginConfig extends StoragePluginConfigBase {
+  private static final Logger logger = LoggerFactory.getLogger(HttpStoragePluginConfig.class);
+
+  public static final String NAME = ""http"";
+
+  public String connection;
+
+  public String resultKey;
+
+  @JsonCreator
+  public HttpStoragePluginConfig(@JsonProperty(""connection"") String connection,
+                                 @JsonProperty(""resultKey"") String resultKey) {
+    logger.debug(""initialize HttpStoragePluginConfig {}"", connection);
+    this.connection = connection;
+    this.resultKey = resultKey;
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    if (this == that) {
+      return true;
+    } else if (that == null || getClass() != that.getClass()) {
+      return false;
+    }
+    HttpStoragePluginConfig t = (HttpStoragePluginConfig) that;
+    return this.connection.equals(t.connection) &&
+      this.resultKey.equals(t.resultKey);
+  }
+
+  @Override
+  public int hashCode() {
+    return Arrays.hashCode(new Object[]{connection, resultKey});
+  }
+
+  @Override
+  public String toString() {","[{'comment': 'Turns out this has to be in Drill logical plan format:\r\n\r\n```\r\nFoo [bar=x, mumble=y]\r\n```\r\n\r\nThere is a builder for this in the upcoming Storage plugin helper PR.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpStoragePluginConfig.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import org.apache.drill.shaded.guava.com.google.common.base.MoreObjects;
+import org.apache.drill.common.logical.StoragePluginConfigBase;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+import java.util.Arrays;
+
+@JsonTypeName(HttpStoragePluginConfig.NAME)
+public class HttpStoragePluginConfig extends StoragePluginConfigBase {
+  private static final Logger logger = LoggerFactory.getLogger(HttpStoragePluginConfig.class);
+
+  public static final String NAME = ""http"";
+
+  public String connection;
+
+  public String resultKey;
+
+  @JsonCreator
+  public HttpStoragePluginConfig(@JsonProperty(""connection"") String connection,
+                                 @JsonProperty(""resultKey"") String resultKey) {
+    logger.debug(""initialize HttpStoragePluginConfig {}"", connection);
+    this.connection = connection;
+    this.resultKey = resultKey;
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    if (this == that) {
+      return true;
+    } else if (that == null || getClass() != that.getClass()) {
+      return false;
+    }
+    HttpStoragePluginConfig t = (HttpStoragePluginConfig) that;
+    return this.connection.equals(t.connection) &&
+      this.resultKey.equals(t.resultKey);
+  }
+
+  @Override
+  public int hashCode() {
+    return Arrays.hashCode(new Object[]{connection, resultKey});
+  }
+
+  @Override
+  public String toString() {
+    return MoreObjects.toStringHelper(this)
+      .add(""connection"", connection)
+      .add(""resultKey"", resultKey)
+      .toString();
+  }
+
+  @JsonIgnore","[{'comment': 'Why ignore? Should be `@JsonProperty(""connection"")` Actually, Jackson will figure it out because the name is in JavaBean format.', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpSubScan.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+
+import java.util.Iterator;
+import java.util.List;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.physical.base.AbstractBase;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.UserBitShared.CoreOperatorType;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableSet;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@JsonTypeName(""http-sub-scan"")
+public class HttpSubScan extends AbstractBase implements SubScan {
+  private static final Logger logger = LoggerFactory.getLogger(HttpSubScan.class);
+
+  private final HttpScanSpec tableSpec;
+  private final HttpStoragePluginConfig config;
+  private final List<SchemaPath> columns;
+
+  @JsonCreator
+  public HttpSubScan(
+    @JsonProperty(""config"") HttpStoragePluginConfig config,
+    @JsonProperty(""tableSpec"") HttpScanSpec tableSpec,
+    @JsonProperty(""columns"") List<SchemaPath> columns) {
+    super(""user-if-needed"");
+    this.config = config;
+    this.tableSpec = tableSpec;
+    this.columns = columns;
+  }
+
+  public HttpScanSpec getTableSpec() {
+    return tableSpec;
+  }
+
+  public String getURL() {
+    return tableSpec.getURL();
+  }
+
+  public String getFullURL() {","[{'comment': '`@JsonIgnore` because this is in Java Bean format (`getFoo()`).', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpSubScan.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http;
+
+
+import java.util.Iterator;
+import java.util.List;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.physical.base.AbstractBase;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.UserBitShared.CoreOperatorType;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableSet;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@JsonTypeName(""http-sub-scan"")
+public class HttpSubScan extends AbstractBase implements SubScan {
+  private static final Logger logger = LoggerFactory.getLogger(HttpSubScan.class);
+
+  private final HttpScanSpec tableSpec;
+  private final HttpStoragePluginConfig config;
+  private final List<SchemaPath> columns;
+
+  @JsonCreator
+  public HttpSubScan(
+    @JsonProperty(""config"") HttpStoragePluginConfig config,
+    @JsonProperty(""tableSpec"") HttpScanSpec tableSpec,
+    @JsonProperty(""columns"") List<SchemaPath> columns) {
+    super(""user-if-needed"");
+    this.config = config;
+    this.tableSpec = tableSpec;
+    this.columns = columns;
+  }
+
+  public HttpScanSpec getTableSpec() {
+    return tableSpec;
+  }
+
+  public String getURL() {
+    return tableSpec.getURL();
+  }
+
+  public String getFullURL() {
+    return config.getConnection() + getURL();
+  }
+
+  public List<SchemaPath> getColumns() {
+    return columns;
+  }
+
+  public HttpStoragePluginConfig getConfig() {
+    return config;
+  }
+
+ @Override
+  public <T, X, E extends Throwable> T accept(
+   PhysicalVisitor<T, X, E> physicalVisitor, X value) throws E {
+    return physicalVisitor.visitSubScan(this, value);
+  }
+
+  @Override
+  public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children)
+    throws ExecutionSetupException {
+    return new HttpSubScan(config, tableSpec, columns);
+  }
+
+  @Override
+  @JsonIgnore
+  public int getOperatorType() {
+    return CoreOperatorType.HTTP_SUB_SCAN_VALUE;
+  }
+
+  @Override
+  public Iterator<PhysicalOperator> iterator() {
+    return ImmutableSet.<PhysicalOperator>of().iterator();
+  }
+
+  @Override
+  public String toString() {
+    return ""["" + this.getClass().getSimpleName() +","[{'comment': 'Should be in Drill ""logical plan"" format: `Foo [bar=mumble]`', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/main/resources/bootstrap-storage-plugins.json,"@@ -0,0 +1,10 @@
+{
+  ""storage"":{
+    ""http"" : {
+      ""type"":""http"",
+      ""connection"": ""https://api.sunrise-sunset.org/json"",","[{'comment': 'Do we want to put a dependency on an external system in this source code?', 'commenter': 'paul-rogers'}]"
1892,contrib/storage-http/src/test/java/org/apache/drill/exec/store/http/TestHttpPlugin.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryBuilder;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.junit.BeforeClass;
+import org.junit.Ignore;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import static org.junit.Assert.assertEquals;
+
+@Ignore(""requires remote http server"")","[{'comment': 'There is a way to run a simple HTTP server in your test. (We run the Drill HTTP server, for example.) I\'d recommend adding that, then using it to return a variety of outputs:\r\n\r\n* Error code\r\n* Empty response\r\n* Normal size response (that can be verified in test)\r\n* Odd format (that requires, say, all text mode)\r\n* Very large output (which will blow up the current load-to-string implementation)\r\n* Query parameters\r\n* Complex filters (including OR clauses which can be converted to multiple GETs)\r\n\r\nFor example, here is [the second hit](https://www.logicbig.com/tutorials/core-java-tutorial/http-server/http-server-basic.html) on a Java search for ""java simple in-memory HTTP server"".', 'commenter': 'paul-rogers'}]"
1894,exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/DrillFuncHolder.java,"@@ -304,19 +305,23 @@ protected void declareVarArgArray(JCodeModel model, JBlock jBlock, HoldingContai
   private JType getParamClass(JCodeModel model, ValueReference parameter, JType defaultType) {
     if (parameter.isFieldReader()) {
       return model._ref(FieldReader.class);
-    } else if (parameter.getType().getMinorType() == MinorType.MAP) {
-      if (parameter.getType().getMode() == TypeProtos.DataMode.REPEATED) {
-        return model._ref(RepeatedMapHolder.class);
-      } else {
-        return model._ref(MapHolder.class);
-      }
-    } else if (parameter.getType().getMinorType() == MinorType.LIST) {
-      if (parameter.getType().getMode() == TypeProtos.DataMode.REPEATED) {
-        return model._ref(RepeatedListHolder.class);
-      } else {
-        return model._ref(ListHolder.class);
-      }
     }
+
+    boolean repeated = parameter.getType().getMode() == TypeProtos.DataMode.REPEATED;
+    Class<?> holderClass = null;
+    MinorType type = parameter.getType().getMinorType();
+    if (type == MinorType.MAP) {
+      holderClass = repeated ? RepeatedMapHolder.class : MapHolder.class;
+    } else if (type == MinorType.LIST) {
+      holderClass = repeated ? RepeatedListHolder.class : ListHolder.class;
+    } else if (type == MinorType.DICT) {
+      holderClass = repeated ? RepeatedDictHolder.class : DictHolder.class;
+    }
+
+    if (holderClass != null) {
+      return model._ref(holderClass);","[{'comment': 'Could you please utilize ```org.apache.drill.exec.expr.TypeHelper``` for holder selection ? ', 'commenter': 'ihuzenko'}]"
1895,exec/jdbc-all/pom.xml,"@@ -528,7 +531,7 @@
                   This is likely due to you adding new dependencies to a java-exec and not updating the excludes in this module. This is important as it minimizes the size of the dependency of Drill application users.
 
                   </message>
-                  <maxsize>41000000</maxsize>
+                  <maxsize>42600000</maxsize>","[{'comment': 'Could you please round it to megabytes? Some time ago this check failed because the limit was so close to the actual size, that adding new classes was causing exceeding the limit.', 'commenter': 'vvysotskyi'}]"
1895,exec/java-exec/pom.xml,"@@ -437,6 +457,43 @@
         </exclusion>
       </exclusions>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-hdfs</artifactId>
+      <scope>test</scope>
+      <exclusions>
+        <exclusion>
+          <groupId>io.netty</groupId>
+          <artifactId>netty</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>io.netty</groupId>
+          <artifactId>netty-all</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>commons-codec</groupId>
+          <artifactId>commons-codec</artifactId>
+        </exclusion>
+        <!---->","[{'comment': 'Please remove this empty comment', 'commenter': 'vvysotskyi'}]"
1895,exec/java-exec/pom.xml,"@@ -437,6 +457,43 @@
         </exclusion>
       </exclusions>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-hdfs</artifactId>
+      <scope>test</scope>
+      <exclusions>
+        <exclusion>
+          <groupId>io.netty</groupId>
+          <artifactId>netty</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>io.netty</groupId>
+          <artifactId>netty-all</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>commons-codec</groupId>
+          <artifactId>commons-codec</artifactId>
+        </exclusion>
+        <!---->
+        <exclusion>
+          <groupId>com.sun.jersey</groupId>
+          <artifactId>jersey-core</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>com.sun.jersey</groupId>
+          <artifactId>jersey-server</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>com.sun.jersey</groupId>
+          <artifactId>jersey-json</artifactId>
+        </exclusion>
+        <!---->","[{'comment': 'And this one.', 'commenter': 'vvysotskyi'}]"
1895,exec/java-exec/src/main/java/org/apache/commons/logging/impl/Log4JLogger.java,"@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.commons.logging.impl;
+
+public class Log4JLogger {","[{'comment': 'Please add JavaDoc with information on why this class was introduced and when it may be removed again.', 'commenter': 'vvysotskyi'}, {'comment': 'https://issues.apache.org/jira/browse/HADOOP-15749', 'commenter': 'vdiravka'}, {'comment': 'Done.', 'commenter': 'agozhiy'}]"
1895,exec/java-exec/src/main/java/org/apache/drill/exec/store/LocalSyncableFileSystem.java,"@@ -141,7 +141,7 @@ public LocalSyncableOutputStream(Path path) throws FileNotFoundException {
       output = new BufferedOutputStream(fos, 64*1024);
     }
 
-    @Override
+    // TODO: remove it after upgrade MapR profile onto hadoop.version 3.1
     public void sync() throws IOException {
       output.flush();
       fos.getFD().sync();","[{'comment': 'Please replace this method implementation with the `hsync()` call.', 'commenter': 'vvysotskyi'}, {'comment': '`hsync` and `hflush` can be unified too', 'commenter': 'vdiravka'}]"
1895,exec/java-exec/src/main/java/org/apache/drill/exec/store/LocalSyncableFileSystem.java,"@@ -65,7 +65,7 @@ public FSDataInputStream open(Path path, int i) throws IOException {
 
   @Override
   public FSDataOutputStream create(Path path, FsPermission fsPermission, boolean b, int i, short i2, long l, Progressable progressable) throws IOException {
-    return new FSDataOutputStream(new LocalSyncableOutputStream(path));
+    return new FSDataOutputStream(new LocalSyncableOutputStream(path), new Statistics(path.toUri().getScheme()));","[{'comment': ""Wouldn't it be better to obtain the `Statistics` instance using its static method `Statistics.getStatistics(String scheme, Class<? extends FileSystem> cls)` to use cached instance instead of creating the new one?"", 'commenter': 'vvysotskyi'}]"
1895,exec/java-exec/src/test/java/org/apache/drill/exec/physical/unit/TestOutputBatchSize.java,"@@ -328,7 +327,7 @@ public void testProjectVariableWidthImpl(boolean transfer, int columnCount, Stri
         expr[i * 2] = ""lower("" + baselineColumns[i] + "")"";
         expr[i * 2 + 1] = baselineColumns[i];
       }
-      baselineValues[i] = (transfer ? testString : Strings.lowerCase(testString));
+      baselineValues[i] = (transfer ? testString : testString.toLowerCase());","[{'comment': '`Strings.lowerCase()` returned null if string was null, but this code will throw NPE...', 'commenter': 'vvysotskyi'}]"
1895,exec/jdbc-all/src/test/java/org/apache/drill/jdbc/DrillbitClassLoader.java,"@@ -26,16 +26,16 @@
 
 public class DrillbitClassLoader extends URLClassLoader {
 
-  public DrillbitClassLoader() {
+  DrillbitClassLoader() {
     super(URLS);
   }
 
   private static final URL[] URLS;
 
   static {
-    ArrayList<URL> urlList = new ArrayList<URL>();
+    ArrayList<URL> urlList = new ArrayList<>();","[{'comment': '```suggestion\r\n    List<URL> urlList = new ArrayList<>();\r\n```', 'commenter': 'vvysotskyi'}]"
1895,exec/jdbc-all/src/test/java/org/apache/drill/jdbc/DrillbitClassLoader.java,"@@ -61,21 +58,21 @@ public DrillbitClassLoader() {
    *
    * Taken from Apache Harmony","[{'comment': 'If we update this method, perhaps this comment should be also updated. But I have a question, why not to use `String.split` instead of this method?', 'commenter': 'vvysotskyi'}, {'comment': 'Agree, this method can be replaced', 'commenter': 'vdiravka'}]"
1895,pom.xml,"@@ -2046,6 +2124,76 @@
               </exclusion>
             </exclusions>
           </dependency>
+          <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-hdfs</artifactId>
+            <version>${hadoop.version}</version>
+            <scope>test</scope>
+            <exclusions>
+              <exclusion>
+                <groupId>commons-logging</groupId>
+                <artifactId>commons-logging</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>org.mortbay.jetty</groupId>
+                <artifactId>servlet-api</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>javax.servlet</groupId>
+                <artifactId>servlet-api</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>io.netty</groupId>
+                <artifactId>netty-all</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>io.netty</groupId>
+                <artifactId>netty</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>log4j</groupId>
+                <artifactId>log4j</artifactId>
+              </exclusion>
+            </exclusions>
+          </dependency>
+          <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-hdfs</artifactId>
+            <version>${hadoop.version}</version>
+            <scope>test</scope>
+            <classifier>tests</classifier>
+            <exclusions>
+              <exclusion>
+                <groupId>commons-logging</groupId>
+                <artifactId>commons-logging</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>org.mortbay.jetty</groupId>
+                <artifactId>servlet-api</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>javax.servlet</groupId>
+                <artifactId>servlet-api</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>log4j</groupId>
+                <artifactId>log4j</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>com.sun.jersey</groupId>
+                <artifactId>jersey-core</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>io.netty</groupId>
+                <artifactId>netty-all</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>io.netty</groupId>
+                <artifactId>netty</artifactId>
+              </exclusion>
+            </exclusions>
+          </dependency>
+          <!-- Hadoop Test Dependencies -->","[{'comment': 'Please move this comment to the correct place. Dependencies below are used not only for tests.', 'commenter': 'vvysotskyi'}, {'comment': '```suggestion\r\n          <!-- Hadoop Test Dependencies -->\r\n          \r\n```', 'commenter': 'vdiravka'}, {'comment': 'Done.', 'commenter': 'agozhiy'}, {'comment': 'Done.', 'commenter': 'agozhiy'}]"
1895,exec/java-exec/src/test/java/org/apache/drill/exec/physical/unit/TestOutputBatchSize.java,"@@ -385,7 +384,7 @@ public void testProjectVariableWidthMixed() throws Exception {
       expr[i * 2] = ""lower("" + baselineColumns[i] + "")"";
       expr[i * 2 + 1] = baselineColumns[i];
 
-      baselineValues[i] = Strings.lowerCase(testString);
+      baselineValues[i] = testString.toLowerCase();","[{'comment': '@agozhiy please revert this too', 'commenter': 'vdiravka'}]"
1895,exec/java-exec/src/test/java/org/apache/drill/exec/work/batch/FileTest.java,"@@ -43,7 +43,7 @@ public static void main(String[] args) throws IOException {
     FSDataOutputStream out = fs.create(path);
     byte[] s = ""hello world"".getBytes();
     out.write(s);
-    out.sync();","[{'comment': 'minor:\r\n`sync` -> `hflush`\r\nhttps://github.com/mapr/hadoop-common/blob/release-2.7.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Syncable.java#L31', 'commenter': 'vdiravka'}, {'comment': 'Done.', 'commenter': 'agozhiy'}]"
1895,docs/dev/HadoopWinutils.md,"@@ -0,0 +1,11 @@
+## Hadoop Winutils
+
+Hadoop Winutils native libraries are required to run Drill on Windows. The last version presented in maven repository is 2.7.1 and doesn't updated anymore.","[{'comment': ""presented in maven repository -> present in Maven repository\r\ndoesn't updated anymore -> is not updated"", 'commenter': 'arina-ielchiieva'}, {'comment': 'Fixed.', 'commenter': 'agozhiy'}]"
1895,docs/dev/HadoopWinutils.md,"@@ -0,0 +1,11 @@
+## Hadoop Winutils
+
+Hadoop Winutils native libraries are required to run Drill on Windows. The last version presented in maven repository is 2.7.1 and doesn't updated anymore.
+So instead of heaving a dependency Drill now contains required files as resources at distribution/src/main/resources.","[{'comment': ""That's why Winutils version matching Hadoop version used in Drill is located in distribution/src/main/resources."", 'commenter': 'arina-ielchiieva'}, {'comment': 'Fixed, thanks.', 'commenter': 'agozhiy'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/protocol/VectorContainerAccessor.java,"@@ -22,41 +22,83 @@
 
 import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.RecordBatch;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
 
 public class VectorContainerAccessor implements BatchAccessor {
 
-  public static class ContainerAndSv2Accessor extends VectorContainerAccessor {
+  public static class ExtendedContainerAccessor extends VectorContainerAccessor {","[{'comment': ""Please extract the class to separate file and provide javadoc. Also I know it's hard but it would be cool to provide more meaningful name for the class. Maybe something like, ```ExternalSelectionVectorsContainerAccessor```. Also if batch can contain only either sv2 or sv4 maybe it would be better to have 2 separate classes. "", 'commenter': 'ihuzenko'}, {'comment': ""Extracted and renamed class. The original code had two subclasses. I used this as a batch holder for the row set copier. However, I found that, in practice, an operator has no control over the form of its input container uses. So, this version of the class allows accepting any form: no SV, an SV2 or SV4.\r\n\r\nYes, it would be better to have a fixed class, but we'll need a bit more work before that is possible."", 'commenter': 'paul-rogers'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/protocol/VectorContainerAccessor.java,"@@ -22,41 +22,83 @@
 
 import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.RecordBatch;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
 
 public class VectorContainerAccessor implements BatchAccessor {
 
-  public static class ContainerAndSv2Accessor extends VectorContainerAccessor {
+  public static class ExtendedContainerAccessor extends VectorContainerAccessor {
 
     private SelectionVector2 sv2;
+    private SelectionVector4 sv4;
+
+    public void setBatch(RecordBatch batch) {","[{'comment': 'This method seems to be unused, can we remove it ? ', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/protocol/VectorContainerAccessor.java,"@@ -22,41 +22,83 @@
 
 import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.RecordBatch;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
 
 public class VectorContainerAccessor implements BatchAccessor {
 
-  public static class ContainerAndSv2Accessor extends VectorContainerAccessor {
+  public static class ExtendedContainerAccessor extends VectorContainerAccessor {
 
     private SelectionVector2 sv2;
+    private SelectionVector4 sv4;
+
+    public void setBatch(RecordBatch batch) {
+      addBatch(batch.getContainer());
+      switch (container.getSchema().getSelectionVectorMode()) {
+      case TWO_BYTE:
+         setSelectionVector(batch.getSelectionVector2());
+         break;
+      case FOUR_BYTE:
+         setSelectionVector(batch.getSelectionVector4());
+         break;
+       default:
+         break;","[{'comment': ""please remove redundant default case if we can't get rid of the method completely."", 'commenter': 'ihuzenko'}]"
1899,common/src/main/java/org/apache/drill/common/types/Types.java,"@@ -806,16 +806,20 @@ public static boolean isSortable(MinorType type) {
     return typeBuilder;
   }
 
+  public static boolean isSameType(MajorType type1, MajorType type2) {
+    return type1.getMinorType() == type2.getMinorType() &&
+           type1.getMode() == type2.getMode() &&
+           type1.getScale() == type2.getScale() &&
+           type1.getPrecision() == type2.getPrecision();
+  }
+
   public static boolean isEquivalent(MajorType type1, MajorType type2) {
 
     // Requires full type equality, including fields such as precision and scale.
     // But, unset fields are equivalent to 0. Can't use the protobuf-provided
     // isEquals() which treats set and unset fields as different.
 
-    if (type1.getMinorType() != type2.getMinorType() ||
-        type1.getMode() != type2.getMode() ||
-        type1.getScale() != type2.getScale() ||
-        type1.getPrecision() != type2.getPrecision()) {
+    if (! isSameType(type1, type2)) {","[{'comment': '```suggestion\r\n    if (!isSameType(type1, type2)) {\r\n```', 'commenter': 'ihuzenko'}]"
1899,common/src/main/java/org/apache/drill/common/types/Types.java,"@@ -806,16 +806,20 @@ public static boolean isSortable(MinorType type) {
     return typeBuilder;
   }
 
+  public static boolean isSameType(MajorType type1, MajorType type2) {","[{'comment': '```suggestion\r\n  private static boolean isSameType(MajorType type1, MajorType type2) {\r\n```', 'commenter': 'ihuzenko'}, {'comment': 'This was pulled out to use elsewhere: it checks if the ""core"" type is the same, ignoring the child details.', 'commenter': 'paul-rogers'}]"
1899,common/src/main/java/org/apache/drill/common/types/Types.java,"@@ -806,16 +806,20 @@ public static boolean isSortable(MinorType type) {
     return typeBuilder;
   }
 
+  public static boolean isSameType(MajorType type1, MajorType type2) {
+    return type1.getMinorType() == type2.getMinorType() &&
+           type1.getMode() == type2.getMode() &&
+           type1.getScale() == type2.getScale() &&
+           type1.getPrecision() == type2.getPrecision();
+  }
+
   public static boolean isEquivalent(MajorType type1, MajorType type2) {
 
     // Requires full type equality, including fields such as precision and scale.
     // But, unset fields are equivalent to 0. Can't use the protobuf-provided
     // isEquals() which treats set and unset fields as different.","[{'comment': 'This comment could be converted to javadoc for newly created ```isSameType(MajorType type1, MajorType type2)``` method. ', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/protocol/VectorContainerAccessor.java,"@@ -22,41 +22,83 @@
 
 import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.RecordBatch;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
 
 public class VectorContainerAccessor implements BatchAccessor {
 
-  public static class ContainerAndSv2Accessor extends VectorContainerAccessor {
+  public static class ExtendedContainerAccessor extends VectorContainerAccessor {
 
     private SelectionVector2 sv2;
+    private SelectionVector4 sv4;
+
+    public void setBatch(RecordBatch batch) {
+      addBatch(batch.getContainer());
+      switch (container.getSchema().getSelectionVectorMode()) {
+      case TWO_BYTE:
+         setSelectionVector(batch.getSelectionVector2());
+         break;
+      case FOUR_BYTE:
+         setSelectionVector(batch.getSelectionVector4());
+         break;
+       default:
+         break;
+      }
+    }
 
     public void setSelectionVector(SelectionVector2 sv2) {
+      Preconditions.checkState(sv4 == null);
       this.sv2 = sv2;
     }
 
+    public void setSelectionVector(SelectionVector4 sv4) {
+      Preconditions.checkState(sv2 == null);
+      this.sv4 = sv4;
+    }
+
     @Override
     public SelectionVector2 selectionVector2() {
       return sv2;
     }
-  }
-
-  public static class ContainerAndSv4Accessor extends VectorContainerAccessor {
-
-    private SelectionVector4 sv4;
 
     @Override
     public SelectionVector4 selectionVector4() {
       return sv4;
     }
+
+    @Override
+    public int rowCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else if (sv4 != null) {
+        return sv4.getCount();
+      } else {
+        return super.rowCount();
+      }
+    }
+
+    @Override
+    public void release() {
+      super.release();
+      if (sv2 != null) {
+        sv2.clear();
+        sv2 = null;
+      }
+      if (sv4 != null) {
+        sv4.clear();
+        sv4 = null;
+      }
+    }
   }
 
-  private VectorContainer container;
-  private SchemaTracker schemaTracker = new SchemaTracker();
+  protected VectorContainer container;","[{'comment': 'I think package-private access is enough for extracted child class. ', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();","[{'comment': '```suggestion\r\n  void startOutputBatch();\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();
+
+  /**
+   * Start the next input batch. The input batch must be held
+   * by the VectorAccessor passed into the constructor.
+   */
+  void startInput();","[{'comment': '```suggestion\r\n  void startInputBatch();\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();
+
+  /**
+   * Start the next input batch. The input batch must be held
+   * by the VectorAccessor passed into the constructor.
+   */
+  void startInput();
+
+  /**
+   * If copying rows one by one, copy the next row from the
+   * input.
+   *
+   * @return true if more rows remain on the input, false
+   * if all rows are exhausted
+   */
+  boolean copyNext();","[{'comment': '```suggestion\r\n  boolean copyNextRow();\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();
+
+  /**
+   * Start the next input batch. The input batch must be held
+   * by the VectorAccessor passed into the constructor.
+   */
+  void startInput();
+
+  /**
+   * If copying rows one by one, copy the next row from the
+   * input.
+   *
+   * @return true if more rows remain on the input, false
+   * if all rows are exhausted
+   */
+  boolean copyNext();
+
+  /**
+   * Copy a row at the given position. For those cases in
+   * which random copying is needed, but a selection vector
+   * is not available. Note that this version is slow because
+   * of the need to reset indexes for every row. Better to
+   * use a selection vector, then copy sequentially.
+   *
+   * @param posn the input row position. If a selection vector
+   * is attached, then this is the selection vector position
+   */
+  void copyRecord(int posn);","[{'comment': '```suggestion\r\n  void copyRow(int inputRowIndex);\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();
+
+  /**
+   * Start the next input batch. The input batch must be held
+   * by the VectorAccessor passed into the constructor.
+   */
+  void startInput();
+
+  /**
+   * If copying rows one by one, copy the next row from the
+   * input.
+   *
+   * @return true if more rows remain on the input, false
+   * if all rows are exhausted
+   */
+  boolean copyNext();
+
+  /**
+   * Copy a row at the given position. For those cases in
+   * which random copying is needed, but a selection vector
+   * is not available. Note that this version is slow because
+   * of the need to reset indexes for every row. Better to
+   * use a selection vector, then copy sequentially.
+   *
+   * @param posn the input row position. If a selection vector
+   * is attached, then this is the selection vector position
+   */
+  void copyRecord(int posn);
+
+  /**
+   * Copy all (remaining) input rows to the output.
+   * If insufficient space exists in the output, does a partial
+   * copy, and {@link #isCopyPending()} will return true.
+   */
+  void copyAll();","[{'comment': '```suggestion\r\n  void copyRows();\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();
+
+  /**
+   * Start the next input batch. The input batch must be held
+   * by the VectorAccessor passed into the constructor.
+   */
+  void startInput();
+
+  /**
+   * If copying rows one by one, copy the next row from the
+   * input.
+   *
+   * @return true if more rows remain on the input, false
+   * if all rows are exhausted
+   */
+  boolean copyNext();
+
+  /**
+   * Copy a row at the given position. For those cases in
+   * which random copying is needed, but a selection vector
+   * is not available. Note that this version is slow because
+   * of the need to reset indexes for every row. Better to
+   * use a selection vector, then copy sequentially.
+   *
+   * @param posn the input row position. If a selection vector
+   * is attached, then this is the selection vector position
+   */
+  void copyRecord(int posn);
+
+  /**
+   * Copy all (remaining) input rows to the output.
+   * If insufficient space exists in the output, does a partial
+   * copy, and {@link #isCopyPending()} will return true.
+   */
+  void copyAll();
+
+  /**
+   * Release the input. Must be called (explicitly, or via
+   * {@link #copyInput()} before loading another input batch.","[{'comment': ""method referred by ```@link``` doesn't exists. "", 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();
+
+  /**
+   * Start the next input batch. The input batch must be held
+   * by the VectorAccessor passed into the constructor.
+   */
+  void startInput();
+
+  /**
+   * If copying rows one by one, copy the next row from the
+   * input.
+   *
+   * @return true if more rows remain on the input, false
+   * if all rows are exhausted
+   */
+  boolean copyNext();
+
+  /**
+   * Copy a row at the given position. For those cases in
+   * which random copying is needed, but a selection vector
+   * is not available. Note that this version is slow because
+   * of the need to reset indexes for every row. Better to
+   * use a selection vector, then copy sequentially.
+   *
+   * @param posn the input row position. If a selection vector
+   * is attached, then this is the selection vector position
+   */
+  void copyRecord(int posn);
+
+  /**
+   * Copy all (remaining) input rows to the output.
+   * If insufficient space exists in the output, does a partial
+   * copy, and {@link #isCopyPending()} will return true.
+   */
+  void copyAll();
+
+  /**
+   * Release the input. Must be called (explicitly, or via
+   * {@link #copyInput()} before loading another input batch.
+   */
+  void freeInput();","[{'comment': '```suggestion\r\n  void releaseInputBatch();\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();
+
+  /**
+   * Start the next input batch. The input batch must be held
+   * by the VectorAccessor passed into the constructor.
+   */
+  void startInput();
+
+  /**
+   * If copying rows one by one, copy the next row from the
+   * input.
+   *
+   * @return true if more rows remain on the input, false
+   * if all rows are exhausted
+   */
+  boolean copyNext();
+
+  /**
+   * Copy a row at the given position. For those cases in
+   * which random copying is needed, but a selection vector
+   * is not available. Note that this version is slow because
+   * of the need to reset indexes for every row. Better to
+   * use a selection vector, then copy sequentially.
+   *
+   * @param posn the input row position. If a selection vector
+   * is attached, then this is the selection vector position
+   */
+  void copyRecord(int posn);
+
+  /**
+   * Copy all (remaining) input rows to the output.
+   * If insufficient space exists in the output, does a partial
+   * copy, and {@link #isCopyPending()} will return true.
+   */
+  void copyAll();
+
+  /**
+   * Release the input. Must be called (explicitly, or via
+   * {@link #copyInput()} before loading another input batch.
+   */
+  void freeInput();
+
+  /**
+   * Reports if the output batch has rows. Useful after the end
+   * of input to determine if a partial output batch exists to
+   * send downstream.
+   * @return true if the output batch has one or more rows
+   */
+  boolean hasRows();","[{'comment': '```suggestion\r\n  boolean hasCopiedRows();\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/ResultSetCopier.java,"@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet;
+
+import org.apache.drill.exec.physical.impl.aggregate.BatchIterator;
+import org.apache.drill.exec.record.VectorContainer;
+
+/**
+ * Copies rows from an input batch to an output batch. The input
+ * batch is assumed to have a selection vector, or the caller
+ * will pick the rows to copy.
+ * <p>
+ * Works to create full output batches to minimize per-batch
+ * overhead and to eliminate unnecessary empty batches if no
+ * rows are copied.
+ * <p>
+ * The output batches are assumed to have the same schema as
+ * input batches. (No projection occurs.) The output schema will
+ * change each time the input schema changes. (For an SV4, then
+ * the upstream operator must have ensured all batches covered
+ * by the SV4 have the same schema.)
+ * <p>
+ * This implementation works with a single stream of batches which,
+ * following Drill's rules, must consist of the same set of vectors on
+ * each non-schema-change batch.
+ *
+ * <h4>Protocol</h4>
+ * Overall lifecycle:
+ * <ol>
+ * <li>Create an instance of the
+ *     {@link org.apache.drill.exec.physical.resultSet.impl.ResultSetCopierImpl
+ *      ResultSetCopierImpl} class, passing the input batch
+ *      accessor to the constructor.</li>
+ * <li>Loop to process each output batch as shown below. That is, continually
+ *     process calls to the {@link BatchIterator#next()} method.</li>
+ * <li>Call {@link #close()}.</li>
+ * </ol>
+ * <p>
+ *
+ * To build each output batch:
+ *
+ * <pre><code>
+ * public IterOutcome next() {
+ *   copier.startBatch();
+ *   while (! copier.isFull() {
+ *     copier.freeInput();
+ *     IterOutcome innerResult = inner.next();
+ *     if (innerResult == DONE) { break; }
+ *     copier.startInput();
+ *     copier.copyAll();
+ *   }
+ *   if (copier.hasRows()) {
+ *     outputContainer = copier.harvest();
+ *     return outputContainer.isSchemaChanged() ? OK_NEW_SCHEMA ? OK;
+ *   } else { return DONE; }
+ * }
+ * </code></pre>
+ * <p>
+ * The above assumes that the upstream operator can be polled
+ * multiple times in the DONE state. The extra polling is
+ * needed to handle any in-flight copies when the input
+ * exhausts its batches.
+ * <p>
+ * The above also shows that the copier handles and reports
+ * schema changes by setting the schema change flag in the
+ * output container. Real code must handle multiple calls to
+ * next() in the DONE state, and work around lack of such support
+ * in its input (perhaps by tracking a state.)
+ * <p>
+ * An input batch is processed by copying the rows. Copying can be done
+ * row-by row, via a row range, or by copying the entire input batch as
+ * shown in the example.
+ * Copying the entire batch make sense when the input batch carries as
+ * selection vector that identifies which rows to copy, in which
+ * order.
+ * <p>
+ * Because we wish to fill the output batch, we may be able to copy
+ * part of a batch, the whole batch, or multiple batches to the output.
+ */
+
+public interface ResultSetCopier {
+
+  /**
+   * Start the next output batch.
+   */
+  void startBatch();
+
+  /**
+   * Start the next input batch. The input batch must be held
+   * by the VectorAccessor passed into the constructor.
+   */
+  void startInput();
+
+  /**
+   * If copying rows one by one, copy the next row from the
+   * input.
+   *
+   * @return true if more rows remain on the input, false
+   * if all rows are exhausted
+   */
+  boolean copyNext();
+
+  /**
+   * Copy a row at the given position. For those cases in
+   * which random copying is needed, but a selection vector
+   * is not available. Note that this version is slow because
+   * of the need to reset indexes for every row. Better to
+   * use a selection vector, then copy sequentially.
+   *
+   * @param posn the input row position. If a selection vector
+   * is attached, then this is the selection vector position
+   */
+  void copyRecord(int posn);
+
+  /**
+   * Copy all (remaining) input rows to the output.
+   * If insufficient space exists in the output, does a partial
+   * copy, and {@link #isCopyPending()} will return true.
+   */
+  void copyAll();
+
+  /**
+   * Release the input. Must be called (explicitly, or via
+   * {@link #copyInput()} before loading another input batch.
+   */
+  void freeInput();
+
+  /**
+   * Reports if the output batch has rows. Useful after the end
+   * of input to determine if a partial output batch exists to
+   * send downstream.
+   * @return true if the output batch has one or more rows
+   */
+  boolean hasRows();
+
+  /**
+   * Reports if the output batch is full and must be sent
+   * downstream. The output batch can be full in the middle
+   * of a copy, in which case {@lik #isCopyPending()} will
+   * also return true.
+   * <p>
+   * This function also returns true if a schema change
+   * occurred on the latest input row, in which case the
+   * partially-completed batch of the old schema must be
+   * flushed downstream.
+   *
+   * @return true if the output is full and must be harvested
+   * and sent downstream
+   */
+  boolean isFull();","[{'comment': '```suggestion\r\n  boolean isFullOutputBatch();\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/ResultSetCopierImpl.java,"@@ -0,0 +1,321 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet.impl;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.impl.protocol.BatchAccessor;
+import org.apache.drill.exec.physical.resultSet.ResultSetCopier;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.ResultSetReader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetReader;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnReader;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+public class ResultSetCopierImpl implements ResultSetCopier {
+
+  private enum State {
+    START,
+    NO_SCHEMA,
+    BETWEEN_BATCHES,
+    BATCH_ACTIVE,
+    NEW_SCHEMA,
+    SCHEMA_PENDING,
+    CLOSED
+  }
+
+  private interface BlockCopy {
+    void copy();
+    boolean hasMore();
+  }
+
+  private class CopyAll implements BlockCopy {
+
+    @Override
+    public void copy() {
+      while (!rowWriter.isFull() && rowReader.next()) {
+        project();
+      }
+    }
+
+    @Override
+    public boolean hasMore() {
+      return rowReader.hasNext();
+    }
+  }
+
+  private static class CopyPair {
+    protected final ColumnWriter writer;
+    protected final ColumnReader reader;
+
+    protected CopyPair(ColumnWriter writer, ColumnReader reader) {
+      this.writer = writer;
+      this.reader = reader;
+    }
+  }
+
+  // Input state
+
+  private int currentSchemaVersion = -1;
+  private final ResultSetReader resultSetReader;
+  protected RowSetReader rowReader;
+
+  // Output state
+
+  private final BufferAllocator allocator;
+  private final OptionBuilder writerOptions;
+  private ResultSetLoader resultSetWriter;
+  private RowSetLoader rowWriter;
+
+  // Copy state
+
+  private State state;
+  private CopyPair[] projection;
+  private BlockCopy activeCopy;
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch) {
+    this(allocator, inputBatch, new OptionBuilder());
+  }
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch,
+      OptionBuilder outputOptions) {
+    this.allocator = allocator;
+    resultSetReader = new ResultSetReaderImpl(inputBatch);","[{'comment': '```suggestion\r\n    this.resultSetReader = new ResultSetReaderImpl(inputBatch);\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/ResultSetCopierImpl.java,"@@ -0,0 +1,321 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet.impl;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.impl.protocol.BatchAccessor;
+import org.apache.drill.exec.physical.resultSet.ResultSetCopier;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.ResultSetReader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetReader;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnReader;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+public class ResultSetCopierImpl implements ResultSetCopier {
+
+  private enum State {
+    START,
+    NO_SCHEMA,
+    BETWEEN_BATCHES,
+    BATCH_ACTIVE,
+    NEW_SCHEMA,
+    SCHEMA_PENDING,
+    CLOSED
+  }
+
+  private interface BlockCopy {
+    void copy();
+    boolean hasMore();
+  }
+
+  private class CopyAll implements BlockCopy {
+
+    @Override
+    public void copy() {
+      while (!rowWriter.isFull() && rowReader.next()) {
+        project();
+      }
+    }
+
+    @Override
+    public boolean hasMore() {
+      return rowReader.hasNext();
+    }
+  }
+
+  private static class CopyPair {
+    protected final ColumnWriter writer;
+    protected final ColumnReader reader;
+
+    protected CopyPair(ColumnWriter writer, ColumnReader reader) {
+      this.writer = writer;
+      this.reader = reader;
+    }
+  }
+
+  // Input state
+
+  private int currentSchemaVersion = -1;
+  private final ResultSetReader resultSetReader;
+  protected RowSetReader rowReader;
+
+  // Output state
+
+  private final BufferAllocator allocator;
+  private final OptionBuilder writerOptions;
+  private ResultSetLoader resultSetWriter;
+  private RowSetLoader rowWriter;
+
+  // Copy state
+
+  private State state;
+  private CopyPair[] projection;
+  private BlockCopy activeCopy;
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch) {
+    this(allocator, inputBatch, new OptionBuilder());
+  }
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch,
+      OptionBuilder outputOptions) {
+    this.allocator = allocator;
+    resultSetReader = new ResultSetReaderImpl(inputBatch);
+    writerOptions = outputOptions;","[{'comment': '```suggestion\r\n    this.writerOptions = outputOptions.setVectorCache(new ResultVectorCacheImpl(allocator));\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/ResultSetCopierImpl.java,"@@ -0,0 +1,321 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet.impl;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.impl.protocol.BatchAccessor;
+import org.apache.drill.exec.physical.resultSet.ResultSetCopier;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.ResultSetReader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetReader;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnReader;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+public class ResultSetCopierImpl implements ResultSetCopier {
+
+  private enum State {
+    START,
+    NO_SCHEMA,
+    BETWEEN_BATCHES,
+    BATCH_ACTIVE,
+    NEW_SCHEMA,
+    SCHEMA_PENDING,
+    CLOSED
+  }
+
+  private interface BlockCopy {
+    void copy();
+    boolean hasMore();
+  }
+
+  private class CopyAll implements BlockCopy {
+
+    @Override
+    public void copy() {
+      while (!rowWriter.isFull() && rowReader.next()) {
+        project();
+      }
+    }
+
+    @Override
+    public boolean hasMore() {
+      return rowReader.hasNext();
+    }
+  }
+
+  private static class CopyPair {
+    protected final ColumnWriter writer;
+    protected final ColumnReader reader;
+
+    protected CopyPair(ColumnWriter writer, ColumnReader reader) {
+      this.writer = writer;
+      this.reader = reader;
+    }
+  }
+
+  // Input state
+
+  private int currentSchemaVersion = -1;
+  private final ResultSetReader resultSetReader;
+  protected RowSetReader rowReader;
+
+  // Output state
+
+  private final BufferAllocator allocator;
+  private final OptionBuilder writerOptions;
+  private ResultSetLoader resultSetWriter;
+  private RowSetLoader rowWriter;
+
+  // Copy state
+
+  private State state;
+  private CopyPair[] projection;
+  private BlockCopy activeCopy;
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch) {
+    this(allocator, inputBatch, new OptionBuilder());
+  }
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch,
+      OptionBuilder outputOptions) {
+    this.allocator = allocator;
+    resultSetReader = new ResultSetReaderImpl(inputBatch);
+    writerOptions = outputOptions;
+    writerOptions.setVectorCache(new ResultVectorCacheImpl(allocator));","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/ResultSetCopierImpl.java,"@@ -0,0 +1,321 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet.impl;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.impl.protocol.BatchAccessor;
+import org.apache.drill.exec.physical.resultSet.ResultSetCopier;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.ResultSetReader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetReader;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnReader;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+public class ResultSetCopierImpl implements ResultSetCopier {
+
+  private enum State {
+    START,
+    NO_SCHEMA,
+    BETWEEN_BATCHES,
+    BATCH_ACTIVE,
+    NEW_SCHEMA,
+    SCHEMA_PENDING,
+    CLOSED
+  }
+
+  private interface BlockCopy {
+    void copy();
+    boolean hasMore();
+  }
+
+  private class CopyAll implements BlockCopy {
+
+    @Override
+    public void copy() {
+      while (!rowWriter.isFull() && rowReader.next()) {
+        project();
+      }
+    }
+
+    @Override
+    public boolean hasMore() {
+      return rowReader.hasNext();
+    }
+  }
+
+  private static class CopyPair {
+    protected final ColumnWriter writer;
+    protected final ColumnReader reader;
+
+    protected CopyPair(ColumnWriter writer, ColumnReader reader) {
+      this.writer = writer;
+      this.reader = reader;
+    }
+  }
+
+  // Input state
+
+  private int currentSchemaVersion = -1;
+  private final ResultSetReader resultSetReader;
+  protected RowSetReader rowReader;
+
+  // Output state
+
+  private final BufferAllocator allocator;
+  private final OptionBuilder writerOptions;
+  private ResultSetLoader resultSetWriter;
+  private RowSetLoader rowWriter;
+
+  // Copy state
+
+  private State state;
+  private CopyPair[] projection;
+  private BlockCopy activeCopy;
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch) {
+    this(allocator, inputBatch, new OptionBuilder());
+  }
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch,
+      OptionBuilder outputOptions) {
+    this.allocator = allocator;
+    resultSetReader = new ResultSetReaderImpl(inputBatch);
+    writerOptions = outputOptions;
+    writerOptions.setVectorCache(new ResultVectorCacheImpl(allocator));
+    state = State.START;","[{'comment': '```suggestion\r\n    this.state = State.START;\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/ResultSetCopierImpl.java,"@@ -0,0 +1,321 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet.impl;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.impl.protocol.BatchAccessor;
+import org.apache.drill.exec.physical.resultSet.ResultSetCopier;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.ResultSetReader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetReader;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnReader;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+public class ResultSetCopierImpl implements ResultSetCopier {
+
+  private enum State {
+    START,
+    NO_SCHEMA,
+    BETWEEN_BATCHES,
+    BATCH_ACTIVE,
+    NEW_SCHEMA,
+    SCHEMA_PENDING,
+    CLOSED
+  }
+
+  private interface BlockCopy {","[{'comment': 'do we really need the interface with only one implementation ? ', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/ResultSetCopierImpl.java,"@@ -0,0 +1,321 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet.impl;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.impl.protocol.BatchAccessor;
+import org.apache.drill.exec.physical.resultSet.ResultSetCopier;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.ResultSetReader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetReader;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnReader;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+public class ResultSetCopierImpl implements ResultSetCopier {
+
+  private enum State {
+    START,
+    NO_SCHEMA,
+    BETWEEN_BATCHES,
+    BATCH_ACTIVE,
+    NEW_SCHEMA,
+    SCHEMA_PENDING,
+    CLOSED
+  }
+
+  private interface BlockCopy {
+    void copy();
+    boolean hasMore();
+  }
+
+  private class CopyAll implements BlockCopy {
+
+    @Override
+    public void copy() {
+      while (!rowWriter.isFull() && rowReader.next()) {
+        project();
+      }
+    }
+
+    @Override
+    public boolean hasMore() {
+      return rowReader.hasNext();
+    }
+  }
+
+  private static class CopyPair {
+    protected final ColumnWriter writer;
+    protected final ColumnReader reader;
+
+    protected CopyPair(ColumnWriter writer, ColumnReader reader) {
+      this.writer = writer;
+      this.reader = reader;
+    }
+  }
+
+  // Input state
+
+  private int currentSchemaVersion = -1;
+  private final ResultSetReader resultSetReader;
+  protected RowSetReader rowReader;
+
+  // Output state
+
+  private final BufferAllocator allocator;
+  private final OptionBuilder writerOptions;
+  private ResultSetLoader resultSetWriter;
+  private RowSetLoader rowWriter;
+
+  // Copy state
+
+  private State state;
+  private CopyPair[] projection;
+  private BlockCopy activeCopy;
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch) {
+    this(allocator, inputBatch, new OptionBuilder());
+  }
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch,
+      OptionBuilder outputOptions) {
+    this.allocator = allocator;
+    resultSetReader = new ResultSetReaderImpl(inputBatch);
+    writerOptions = outputOptions;
+    writerOptions.setVectorCache(new ResultVectorCacheImpl(allocator));
+    state = State.START;
+  }
+
+  @Override
+  public void startBatch() {
+    if (state == State.START) {
+
+      // No schema yet. Defer real batch start until we see an input
+      // batch.
+
+      state = State.NO_SCHEMA;
+      return;
+    }
+    Preconditions.checkState(state == State.BETWEEN_BATCHES || state == State.SCHEMA_PENDING);
+    if (state == State.SCHEMA_PENDING) {
+
+      // We have a pending new schema. Create new writers to match.
+
+      createMapping();
+    }
+    resultSetWriter.startBatch();
+    state = State.BATCH_ACTIVE;
+    if (isCopyPending()) {
+
+      // Resume copying if a copy is active.
+
+      copyBlock();
+    }
+  }
+
+  @Override
+  public void startInput() {
+    Preconditions.checkState(state == State.NO_SCHEMA || state == State.NEW_SCHEMA ||
+                             state == State.BATCH_ACTIVE,
+        ""Can only start input while in an output batch"");
+    Preconditions.checkState(!isCopyPending(),
+        ""Finish the pending copy before changing input"");
+
+    bindInput();
+
+    if (state == State.BATCH_ACTIVE) {
+
+      // If no schema change, we are ready to copy.
+
+      if (currentSchemaVersion == resultSetReader.inputBatch().schemaVersion()) {
+        return;
+      }
+
+      // The schema has changed. Handle it now or later.
+
+      if (hasRows()) {
+
+        // Output batch has rows. Can't switch and bind inputs
+        // until current batch is sent downstream.
+
+        state = State.NEW_SCHEMA;
+        return;
+      }
+    }
+
+    // The schema changed: first schema, or a change while a bath
+    // is active, but is empty.
+
+    if (state == State.NO_SCHEMA) {
+      state = State.BATCH_ACTIVE;
+    } else {
+
+      // Discard the unused empty batch
+
+      harvest().zeroVectors();
+    }
+    createMapping();
+    resultSetWriter.startBatch();
+
+    // Stay in the current state.
+  }
+
+  protected void bindInput() {
+    resultSetReader.start();
+    rowReader = resultSetReader.reader();
+  }
+
+  @Override
+  public void freeInput() {
+    Preconditions.checkState(state != State.CLOSED);
+    resultSetReader.release();
+  }
+
+  private void createMapping() {","[{'comment': '```suggestion\r\n  private void createProjection() {\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/ResultSetCopierImpl.java,"@@ -0,0 +1,321 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet.impl;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.impl.protocol.BatchAccessor;
+import org.apache.drill.exec.physical.resultSet.ResultSetCopier;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.ResultSetReader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetReader;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnReader;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+public class ResultSetCopierImpl implements ResultSetCopier {
+
+  private enum State {
+    START,
+    NO_SCHEMA,
+    BETWEEN_BATCHES,
+    BATCH_ACTIVE,
+    NEW_SCHEMA,
+    SCHEMA_PENDING,
+    CLOSED
+  }
+
+  private interface BlockCopy {
+    void copy();
+    boolean hasMore();
+  }
+
+  private class CopyAll implements BlockCopy {
+
+    @Override
+    public void copy() {
+      while (!rowWriter.isFull() && rowReader.next()) {
+        project();
+      }
+    }
+
+    @Override
+    public boolean hasMore() {
+      return rowReader.hasNext();
+    }
+  }
+
+  private static class CopyPair {
+    protected final ColumnWriter writer;
+    protected final ColumnReader reader;
+
+    protected CopyPair(ColumnWriter writer, ColumnReader reader) {
+      this.writer = writer;
+      this.reader = reader;
+    }
+  }
+
+  // Input state
+
+  private int currentSchemaVersion = -1;
+  private final ResultSetReader resultSetReader;
+  protected RowSetReader rowReader;
+
+  // Output state
+
+  private final BufferAllocator allocator;
+  private final OptionBuilder writerOptions;
+  private ResultSetLoader resultSetWriter;
+  private RowSetLoader rowWriter;
+
+  // Copy state
+
+  private State state;
+  private CopyPair[] projection;
+  private BlockCopy activeCopy;
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch) {
+    this(allocator, inputBatch, new OptionBuilder());
+  }
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch,
+      OptionBuilder outputOptions) {
+    this.allocator = allocator;
+    resultSetReader = new ResultSetReaderImpl(inputBatch);
+    writerOptions = outputOptions;
+    writerOptions.setVectorCache(new ResultVectorCacheImpl(allocator));
+    state = State.START;
+  }
+
+  @Override
+  public void startBatch() {
+    if (state == State.START) {
+
+      // No schema yet. Defer real batch start until we see an input
+      // batch.
+
+      state = State.NO_SCHEMA;
+      return;
+    }
+    Preconditions.checkState(state == State.BETWEEN_BATCHES || state == State.SCHEMA_PENDING);
+    if (state == State.SCHEMA_PENDING) {
+
+      // We have a pending new schema. Create new writers to match.
+
+      createMapping();
+    }
+    resultSetWriter.startBatch();
+    state = State.BATCH_ACTIVE;
+    if (isCopyPending()) {
+
+      // Resume copying if a copy is active.
+
+      copyBlock();
+    }
+  }
+
+  @Override
+  public void startInput() {
+    Preconditions.checkState(state == State.NO_SCHEMA || state == State.NEW_SCHEMA ||
+                             state == State.BATCH_ACTIVE,
+        ""Can only start input while in an output batch"");
+    Preconditions.checkState(!isCopyPending(),
+        ""Finish the pending copy before changing input"");
+
+    bindInput();
+
+    if (state == State.BATCH_ACTIVE) {
+
+      // If no schema change, we are ready to copy.
+
+      if (currentSchemaVersion == resultSetReader.inputBatch().schemaVersion()) {
+        return;
+      }
+
+      // The schema has changed. Handle it now or later.
+
+      if (hasRows()) {
+
+        // Output batch has rows. Can't switch and bind inputs
+        // until current batch is sent downstream.
+
+        state = State.NEW_SCHEMA;
+        return;
+      }
+    }
+
+    // The schema changed: first schema, or a change while a bath
+    // is active, but is empty.
+
+    if (state == State.NO_SCHEMA) {
+      state = State.BATCH_ACTIVE;
+    } else {
+
+      // Discard the unused empty batch
+
+      harvest().zeroVectors();
+    }
+    createMapping();
+    resultSetWriter.startBatch();
+
+    // Stay in the current state.
+  }
+
+  protected void bindInput() {
+    resultSetReader.start();
+    rowReader = resultSetReader.reader();
+  }
+
+  @Override
+  public void freeInput() {
+    Preconditions.checkState(state != State.CLOSED);
+    resultSetReader.release();
+  }
+
+  private void createMapping() {
+    if (resultSetWriter != null) {
+
+      // Need to build a new writer. Close this one. Doing so
+      // will tear down the whole show. But, the vector cache will
+      // ensure that the new writer reuses any matching vectors from
+      // the prior batch to provide vector persistence as Drill expects.
+
+      resultSetWriter.close();
+    }
+    TupleMetadata schema = MetadataUtils.fromFields(resultSetReader.inputBatch().schema());
+    writerOptions.setSchema(schema);
+    resultSetWriter = new ResultSetLoaderImpl(allocator, writerOptions.build());
+    rowWriter = resultSetWriter.writer();
+    currentSchemaVersion = resultSetReader.inputBatch().schemaVersion();
+
+    int colCount = schema.size();
+    projection = new CopyPair[colCount];
+    for (int i = 0; i < colCount; i++) {
+      projection[i] = new CopyPair(
+          rowWriter.column(i).writer(),
+          rowReader.column(i).reader());
+    }
+  }
+
+  @Override
+  public boolean hasRows() {
+    switch (state) {
+    case BATCH_ACTIVE:
+    case NEW_SCHEMA:
+      return resultSetWriter.hasRows();
+    default:
+      return false;
+    }
+  }
+
+  @Override
+  public boolean isFull() {
+    switch (state) {
+    case BATCH_ACTIVE:
+      return rowWriter.isFull();
+    case NEW_SCHEMA:
+      return true;
+    default:
+      return false;
+    }
+  }
+
+  protected void verifyWritable() {
+    Preconditions.checkState(state != State.NEW_SCHEMA,
+        ""Must harvest current batch to flush for new schema."");
+    Preconditions.checkState(state == State.BATCH_ACTIVE,
+        ""Start an output batch before copying"");
+    Preconditions.checkState(!isCopyPending(),
+        ""Resume the in-flight copy before copying another"");
+    Preconditions.checkState(!rowWriter.isFull(),
+        ""Output batch is full; harvest before adding more"");
+  }
+
+  @Override
+  public boolean copyNext() {
+    verifyWritable();
+    if (!rowReader.next()) {
+      return false;
+    }
+    project();
+    return true;
+  }
+
+  @Override
+  public void copyRecord(int posn) {
+    verifyWritable();
+    rowReader.setPosition(posn);
+    project();
+  }
+
+  private final void project() {
+    rowWriter.start();
+    for (int i = 0; i < projection.length; i++) {
+      CopyPair pair = projection[i];
+      pair.writer.copy(pair.reader);","[{'comment': 'Please move copying into new method of ```CopyPair``` and use for-each here: \r\n```java\r\n    for (CopyPair pair : projection) {\r\n      pair.copy();\r\n    }\r\n``` ', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/ResultSetCopierImpl.java,"@@ -0,0 +1,321 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.resultSet.impl;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.physical.impl.protocol.BatchAccessor;
+import org.apache.drill.exec.physical.resultSet.ResultSetCopier;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.ResultSetReader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.physical.rowSet.RowSetReader;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnReader;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+public class ResultSetCopierImpl implements ResultSetCopier {
+
+  private enum State {
+    START,
+    NO_SCHEMA,
+    BETWEEN_BATCHES,
+    BATCH_ACTIVE,
+    NEW_SCHEMA,
+    SCHEMA_PENDING,
+    CLOSED
+  }
+
+  private interface BlockCopy {
+    void copy();
+    boolean hasMore();
+  }
+
+  private class CopyAll implements BlockCopy {
+
+    @Override
+    public void copy() {
+      while (!rowWriter.isFull() && rowReader.next()) {
+        project();
+      }
+    }
+
+    @Override
+    public boolean hasMore() {
+      return rowReader.hasNext();
+    }
+  }
+
+  private static class CopyPair {
+    protected final ColumnWriter writer;
+    protected final ColumnReader reader;
+
+    protected CopyPair(ColumnWriter writer, ColumnReader reader) {
+      this.writer = writer;
+      this.reader = reader;
+    }
+  }
+
+  // Input state
+
+  private int currentSchemaVersion = -1;
+  private final ResultSetReader resultSetReader;
+  protected RowSetReader rowReader;
+
+  // Output state
+
+  private final BufferAllocator allocator;
+  private final OptionBuilder writerOptions;
+  private ResultSetLoader resultSetWriter;
+  private RowSetLoader rowWriter;
+
+  // Copy state
+
+  private State state;
+  private CopyPair[] projection;
+  private BlockCopy activeCopy;
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch) {
+    this(allocator, inputBatch, new OptionBuilder());
+  }
+
+  public ResultSetCopierImpl(BufferAllocator allocator, BatchAccessor inputBatch,
+      OptionBuilder outputOptions) {
+    this.allocator = allocator;
+    resultSetReader = new ResultSetReaderImpl(inputBatch);
+    writerOptions = outputOptions;
+    writerOptions.setVectorCache(new ResultVectorCacheImpl(allocator));
+    state = State.START;
+  }
+
+  @Override
+  public void startBatch() {
+    if (state == State.START) {
+
+      // No schema yet. Defer real batch start until we see an input
+      // batch.
+
+      state = State.NO_SCHEMA;
+      return;
+    }
+    Preconditions.checkState(state == State.BETWEEN_BATCHES || state == State.SCHEMA_PENDING);
+    if (state == State.SCHEMA_PENDING) {
+
+      // We have a pending new schema. Create new writers to match.
+
+      createMapping();
+    }
+    resultSetWriter.startBatch();
+    state = State.BATCH_ACTIVE;
+    if (isCopyPending()) {
+
+      // Resume copying if a copy is active.
+
+      copyBlock();
+    }
+  }
+
+  @Override
+  public void startInput() {
+    Preconditions.checkState(state == State.NO_SCHEMA || state == State.NEW_SCHEMA ||
+                             state == State.BATCH_ACTIVE,
+        ""Can only start input while in an output batch"");
+    Preconditions.checkState(!isCopyPending(),
+        ""Finish the pending copy before changing input"");
+
+    bindInput();
+
+    if (state == State.BATCH_ACTIVE) {
+
+      // If no schema change, we are ready to copy.
+
+      if (currentSchemaVersion == resultSetReader.inputBatch().schemaVersion()) {
+        return;
+      }
+
+      // The schema has changed. Handle it now or later.
+
+      if (hasRows()) {
+
+        // Output batch has rows. Can't switch and bind inputs
+        // until current batch is sent downstream.
+
+        state = State.NEW_SCHEMA;
+        return;
+      }
+    }
+
+    // The schema changed: first schema, or a change while a bath
+    // is active, but is empty.
+
+    if (state == State.NO_SCHEMA) {
+      state = State.BATCH_ACTIVE;
+    } else {
+
+      // Discard the unused empty batch
+
+      harvest().zeroVectors();
+    }
+    createMapping();
+    resultSetWriter.startBatch();
+
+    // Stay in the current state.
+  }
+
+  protected void bindInput() {
+    resultSetReader.start();
+    rowReader = resultSetReader.reader();
+  }
+
+  @Override
+  public void freeInput() {
+    Preconditions.checkState(state != State.CLOSED);
+    resultSetReader.release();
+  }
+
+  private void createMapping() {
+    if (resultSetWriter != null) {
+
+      // Need to build a new writer. Close this one. Doing so
+      // will tear down the whole show. But, the vector cache will
+      // ensure that the new writer reuses any matching vectors from
+      // the prior batch to provide vector persistence as Drill expects.
+
+      resultSetWriter.close();
+    }
+    TupleMetadata schema = MetadataUtils.fromFields(resultSetReader.inputBatch().schema());
+    writerOptions.setSchema(schema);
+    resultSetWriter = new ResultSetLoaderImpl(allocator, writerOptions.build());
+    rowWriter = resultSetWriter.writer();
+    currentSchemaVersion = resultSetReader.inputBatch().schemaVersion();
+
+    int colCount = schema.size();
+    projection = new CopyPair[colCount];
+    for (int i = 0; i < colCount; i++) {
+      projection[i] = new CopyPair(
+          rowWriter.column(i).writer(),
+          rowReader.column(i).reader());
+    }
+  }
+
+  @Override
+  public boolean hasRows() {
+    switch (state) {
+    case BATCH_ACTIVE:
+    case NEW_SCHEMA:
+      return resultSetWriter.hasRows();
+    default:
+      return false;
+    }
+  }
+
+  @Override
+  public boolean isFull() {
+    switch (state) {
+    case BATCH_ACTIVE:
+      return rowWriter.isFull();
+    case NEW_SCHEMA:
+      return true;
+    default:
+      return false;
+    }
+  }
+
+  protected void verifyWritable() {
+    Preconditions.checkState(state != State.NEW_SCHEMA,
+        ""Must harvest current batch to flush for new schema."");
+    Preconditions.checkState(state == State.BATCH_ACTIVE,
+        ""Start an output batch before copying"");
+    Preconditions.checkState(!isCopyPending(),
+        ""Resume the in-flight copy before copying another"");
+    Preconditions.checkState(!rowWriter.isFull(),
+        ""Output batch is full; harvest before adding more"");
+  }
+
+  @Override
+  public boolean copyNext() {
+    verifyWritable();
+    if (!rowReader.next()) {
+      return false;
+    }
+    project();
+    return true;
+  }
+
+  @Override
+  public void copyRecord(int posn) {
+    verifyWritable();
+    rowReader.setPosition(posn);
+    project();
+  }
+
+  private final void project() {","[{'comment': '```suggestion\r\n  private void copyInputRow() {\r\n```', 'commenter': 'ihuzenko'}]"
1899,exec/java-exec/src/main/java/org/apache/drill/exec/record/selection/SelectionVector2Builder.java,"@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.record.selection;
+
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.record.VectorContainer;
+
+public class SelectionVector2Builder {","[{'comment': 'Seems like the class is only intended for test purposes. If so, please move to test sources. ', 'commenter': 'ihuzenko'}]"
1899,exec/vector/src/main/codegen/templates/ColumnAccessors.java,"@@ -592,6 +592,32 @@ public final void setDefaultValue(final Object value) {
       }
     </#if>
     }
+
+    @Override
+    public final void copy(ColumnReader from) {
+      ${drillType}ColumnReader source = (${drillType}ColumnReader) from;
+      final DrillBuf sourceBuf = source.buffer();
+      <#-- First cut, copy materialized value.
+    <#if varWidth>
+      byte[] bytes = source.getBytes();
+      setBytes(bytes, bytes.length);
+    <#else>
+      set${label}(source.get${label}());
+    </#if> -->","[{'comment': 'please remove commented lines', 'commenter': 'ihuzenko'}]"
1899,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/convert/AbstractWriteConverter.java,"@@ -143,4 +144,9 @@ public void setTime(LocalTime value) {
   public void setTimestamp(Instant value) {
     baseWriter.setTimestamp(value);
   }
+
+  @Override
+  public void copy(ColumnReader from) {
+    throw new UnsupportedOperationException(""Cannot copy values through a type converter"");","[{'comment': ""If type converters shouldn't support copy then maybe makes sense to mark the method as final here. "", 'commenter': 'ihuzenko'}]"
1899,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/writer/UnionWriterImpl.java,"@@ -332,6 +334,14 @@ public int writeIndex() {
     return index.vectorIndex();
   }
 
+  @Override
+  public void copy(ColumnReader from) {
+    if (! from.isNull()) {","[{'comment': '```suggestion\r\n    if (!from.isNull()) {\r\n```', 'commenter': 'ihuzenko'}]"
1910,common/src/test/java/org/apache/drill/common/util/function/TestCheckedFunction.java,"@@ -17,14 +17,15 @@
  */
 package org.apache.drill.common.util.function;
 
+import org.apache.drill.test.DrillTest;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 
 import java.util.HashMap;
 import java.util.Map;
 
-public class TestCheckedFunction {
+public class TestCheckedFunction extends DrillTest {","[{'comment': 'Why not `BaseTest`?', 'commenter': 'vvysotskyi'}]"
1910,exec/java-exec/src/test/java/org/apache/drill/BaseTestInheritance.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill;
+
+import org.apache.drill.categories.UnlikelyTest;
+import org.apache.drill.test.BaseTest;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.reflections.Reflections;
+import org.reflections.scanners.SubTypesScanner;
+
+import java.lang.reflect.Method;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import static org.junit.Assert.assertTrue;
+
+public class BaseTestInheritance extends BaseTest {
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void verifyInheritance() {
+    // Get all BaseTest inheritors
+    Reflections reflections = new Reflections(""org.apache.drill"", new SubTypesScanner(false));
+    Set<String> baseTestInheritors = reflections.getSubTypesOf(BaseTest.class).stream()
+        .map(Class::getName)
+        .collect(Collectors.toSet());
+    // Get all tests
+    Set<String> testClasses = reflections.getSubTypesOf(Object.class).stream()
+        .filter(c -> !c.isInterface())
+        .filter(c -> c.getSimpleName().toLowerCase().contains(""test""))
+        .filter(c -> {
+          for (Method m : c.getDeclaredMethods()) {","[{'comment': 'Please use find any here.', 'commenter': 'vvysotskyi'}]"
1910,exec/java-exec/src/test/java/org/apache/drill/BaseTestInheritance.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill;
+
+import org.apache.drill.categories.UnlikelyTest;
+import org.apache.drill.test.BaseTest;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.reflections.Reflections;
+import org.reflections.scanners.SubTypesScanner;
+
+import java.lang.reflect.Method;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import static org.junit.Assert.assertTrue;
+
+public class BaseTestInheritance extends BaseTest {
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void verifyInheritance() {
+    // Get all BaseTest inheritors
+    Reflections reflections = new Reflections(""org.apache.drill"", new SubTypesScanner(false));
+    Set<String> baseTestInheritors = reflections.getSubTypesOf(BaseTest.class).stream()
+        .map(Class::getName)
+        .collect(Collectors.toSet());
+    // Get all tests
+    Set<String> testClasses = reflections.getSubTypesOf(Object.class).stream()
+        .filter(c -> !c.isInterface())
+        .filter(c -> c.getSimpleName().toLowerCase().contains(""test""))
+        .filter(c -> {
+          for (Method m : c.getDeclaredMethods()) {
+            if (m.getAnnotation(Test.class) != null) {
+              return true;
+            }
+          }
+          return false;
+        })
+        .map(Class::getName)
+        .collect(Collectors.toSet());
+
+    testClasses.removeAll(baseTestInheritors);","[{'comment': 'We may avoid this by adding one more filter into the stream above.', 'commenter': 'vvysotskyi'}]"
1910,exec/java-exec/src/test/java/org/apache/drill/BaseTestInheritance.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill;
+
+import org.apache.drill.categories.UnlikelyTest;
+import org.apache.drill.test.BaseTest;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.reflections.Reflections;
+import org.reflections.scanners.SubTypesScanner;
+
+import java.lang.reflect.Method;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import static org.junit.Assert.assertTrue;
+
+public class BaseTestInheritance extends BaseTest {
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void verifyInheritance() {
+    // Get all BaseTest inheritors
+    Reflections reflections = new Reflections(""org.apache.drill"", new SubTypesScanner(false));
+    Set<String> baseTestInheritors = reflections.getSubTypesOf(BaseTest.class).stream()
+        .map(Class::getName)
+        .collect(Collectors.toSet());
+    // Get all tests
+    Set<String> testClasses = reflections.getSubTypesOf(Object.class).stream()
+        .filter(c -> !c.isInterface())
+        .filter(c -> c.getSimpleName().toLowerCase().contains(""test""))
+        .filter(c -> {
+          for (Method m : c.getDeclaredMethods()) {
+            if (m.getAnnotation(Test.class) != null) {
+              return true;
+            }
+          }
+          return false;
+        })
+        .map(Class::getName)
+        .collect(Collectors.toSet());
+
+    testClasses.removeAll(baseTestInheritors);
+    String illegitimateTests = String.join(""\n"", testClasses);
+    assertTrue(String.format(""The following test classes are not inherited from BaseTest:\n%s"", illegitimateTests),","[{'comment': ""We may use assert equals and specify empty collection, so it wouldn't be required to append the list into the error string."", 'commenter': 'vvysotskyi'}]"
1910,common/src/main/java/org/apache/drill/common/util/GuavaPatcher.java,"@@ -24,154 +24,157 @@
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
-import javassist.CannotCompileException;
 import javassist.ClassPool;
 import javassist.CtClass;
 import javassist.CtConstructor;
 import javassist.CtMethod;
 import javassist.CtNewMethod;
-import javassist.NotFoundException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 public class GuavaPatcher {
   private static final Logger logger = LoggerFactory.getLogger(GuavaPatcher.class);
 
-  private static boolean patched;
+  private static boolean patchingAttempted;","[{'comment': 'Maven runs tests concurrently in the same JVM. Does this flag need synchronization?', 'commenter': 'paul-rogers'}, {'comment': 'It is used in synchronized method so it is synchronized.', 'commenter': 'agozhiy'}, {'comment': 'Also, as was mentioned in another comment, patching is done for production code, so it should be synchronized.', 'commenter': 'vvysotskyi'}]"
1910,common/src/main/java/org/apache/drill/common/util/GuavaPatcher.java,"@@ -24,154 +24,157 @@
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
-import javassist.CannotCompileException;
 import javassist.ClassPool;
 import javassist.CtClass;
 import javassist.CtConstructor;
 import javassist.CtMethod;
 import javassist.CtNewMethod;
-import javassist.NotFoundException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 public class GuavaPatcher {
   private static final Logger logger = LoggerFactory.getLogger(GuavaPatcher.class);
 
-  private static boolean patched;
+  private static boolean patchingAttempted;
 
   public static synchronized void patch() {
-    if (!patched) {
-      try {
-        patchStopwatch();
-        patchCloseables();
-        patchPreconditions();
-        patched = true;
-      } catch (Throwable e) {
-        logger.warn(""Unable to patch Guava classes."", e);
-      }
+    if (!patchingAttempted) {
+      patchingAttempted = true;","[{'comment': ""This version no longer has a try/catch block. Is the idea that, if patching fails, the test should fail? If this particular test fails, we've marked patching as attempted and we won't try again. This means that future tests that needed the patching will mysteriously fail.\r\n\r\nShould we, instead, simply do a `System.exit(1)` if patching fails to shut down the tests? Or, print a warning to stderr that subsequent tests may fail?"", 'commenter': 'paul-rogers'}, {'comment': 'The try/catch block was moved to the component methods. It is simpler this way to pinpoint possible errors.', 'commenter': 'agozhiy'}, {'comment': 'For the case when patching is used when starting Drill, it would be better to avoid killing a drillbit if it will fail. In this case, part of the functionality would still be available.\r\nRegarding unit tests use case, there was introduced a test that checks that patching is produced for all test classes.', 'commenter': 'vvysotskyi'}]"
1910,common/src/main/java/org/apache/drill/common/util/GuavaPatcher.java,"@@ -24,154 +24,157 @@
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
-import javassist.CannotCompileException;
 import javassist.ClassPool;
 import javassist.CtClass;
 import javassist.CtConstructor;
 import javassist.CtMethod;
 import javassist.CtNewMethod;
-import javassist.NotFoundException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 public class GuavaPatcher {
   private static final Logger logger = LoggerFactory.getLogger(GuavaPatcher.class);
 
-  private static boolean patched;
+  private static boolean patchingAttempted;
 
   public static synchronized void patch() {
-    if (!patched) {
-      try {
-        patchStopwatch();
-        patchCloseables();
-        patchPreconditions();
-        patched = true;
-      } catch (Throwable e) {
-        logger.warn(""Unable to patch Guava classes."", e);
-      }
+    if (!patchingAttempted) {
+      patchingAttempted = true;
+      patchStopwatch();
+      patchCloseables();
+      patchPreconditions();
     }
   }
 
   /**
    * Makes Guava stopwatch look like the old version for compatibility with hbase-server (for test purposes).
    */
-  private static void patchStopwatch() throws Exception {
-
-    ClassPool cp = ClassPool.getDefault();
-    CtClass cc = cp.get(""com.google.common.base.Stopwatch"");
-
-    // Expose the constructor for Stopwatch for old libraries who use the pattern new Stopwatch().start().
-    for (CtConstructor c : cc.getConstructors()) {
-      if (!Modifier.isStatic(c.getModifiers())) {
-        c.setModifiers(Modifier.PUBLIC);
+  private static void patchStopwatch() {
+    try {
+      ClassPool cp = ClassPool.getDefault();
+      CtClass cc = cp.get(""com.google.common.base.Stopwatch"");
+
+      // Expose the constructor for Stopwatch for old libraries who use the pattern new Stopwatch().start().
+      for (CtConstructor c : cc.getConstructors()) {
+        if (!Modifier.isStatic(c.getModifiers())) {
+          c.setModifiers(Modifier.PUBLIC);
+        }
       }
-    }
 
-    // Add back the Stopwatch.elapsedMillis() method for old consumers.
-    CtMethod newMethod = CtNewMethod.make(
-        ""public long elapsedMillis() { return elapsed(java.util.concurrent.TimeUnit.MILLISECONDS); }"", cc);
-    cc.addMethod(newMethod);
+      // Add back the Stopwatch.elapsedMillis() method for old consumers.
+      CtMethod newMethod = CtNewMethod.make(
+          ""public long elapsedMillis() { return elapsed(java.util.concurrent.TimeUnit.MILLISECONDS); }"", cc);
+      cc.addMethod(newMethod);
 
-    // Load the modified class instead of the original.
-    cc.toClass();
+      // Load the modified class instead of the original.
+      cc.toClass();
 
-    logger.info(""Google's Stopwatch patched for old HBase Guava version."");
+      logger.info(""Google's Stopwatch patched for old HBase Guava version."");
+    } catch (Exception e) {
+      logger.warn(""Unable to patch Guava classes."", e);","[{'comment': 'We are warning on errors, but then proceeding. The test that caused this patch will still run, but presumably will fail. Is this the desired behaviour?', 'commenter': 'paul-rogers'}, {'comment': ""I didn't change the behavior. \r\nWhat do you suggest? Tests are executed by JUnit, so in case of an exception they will fail just as well, but with a less clear message (because BaseTest would fail to initialize). "", 'commenter': 'agozhiy'}]"
1910,common/src/test/java/org/apache/drill/test/BaseTest.java,"@@ -15,24 +15,25 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.drill.exec.expr;
+package org.apache.drill.test;
 
-import org.apache.drill.BaseTestQuery;
-import org.apache.drill.common.util.TestTools;
-import org.junit.Test;
+import org.apache.drill.common.util.GuavaPatcher;
+import org.apache.drill.common.util.ProtobufPatcher;
 
-public class TestPrune extends BaseTestQuery {
+public class BaseTest {
 
-  String MULTILEVEL = TestTools.getWorkingPath() + ""/../java-exec/src/test/resources/multilevel"";
-
-  @Test
-  public void pruneCompound() throws Exception {
-    test(String.format(""select * from dfs.`%s/csv` where x is null and dir1 in ('Q1', 'Q2')"", MULTILEVEL));
-  }
-
-  @Test
-  public void pruneSimple() throws Exception {
-    test(String.format(""select * from dfs.`%s/csv` where dir1 in ('Q1', 'Q2')"", MULTILEVEL));
+  static {
+    /*
+     * HBase and MapR-DB clients use older version of protobuf,
+     * and override some methods that became final in recent versions.
+     * This code removes these final modifiers.","[{'comment': ""Shouldn't these clients be addressed directly? We're doing patching for tests, but what about production code? Possible solutions:\r\n\r\n* Load these clients into their own class loader so their dependencies do not conflict.\r\n* Shade them.\r\n* Upgrade to newer versions that use more recent Guava libraries.\r\n* Do the patching within Drill: perhaps have a plugin-specific startup event that can trigger patching where needed.\r\n* Lobby the creators of the libraries to fix the problems.\r\n\r\nMy concern is that changing all our tests to work around bugs in external libraries is an unstable solution: we'll find ourselves doing more of this each time some new external conflict arises."", 'commenter': 'paul-rogers'}, {'comment': ""We're doing patching both in tests and in production (see Drillbit class). \r\nThis is actually quite extensive topic. We already have shaded Guava that is used by Drill. The problem is: some of libraries we're using have transitive dependencies to different Guava versions. In total we have 3 or 4 versions we should make friends with. So shading is not the way. I guess we can do some playing with classloaders, but it'll be tricky and additionally we'll have bloated jars.\r\nAs for your last paragraph, this would definitely be great if every library had the same dependencies versions or if Guava was fully backward compatible... Sadly, there is nothing we can do on this account."", 'commenter': 'agozhiy'}, {'comment': ""Good ideas, but I have some questions:\r\n1. I'm not sure how to tell maven to keep a specific version to use it in the future but do not use it for other libraries.\r\n2. When I was shading Guava for Drill usage, I considered this approach, but we have a lot of libraries with their specific versions, for example, hbase, hadoop-common, hadoop-client, hadoop-hdfs. It would be excessive to shade them all.\r\n3. Unfortunately, even the latest versions of most libraries use obsolete guava versions (I have checked hbase).\r\n"", 'commenter': 'vvysotskyi'}]"
1910,common/src/test/java/org/apache/drill/test/BaseTest.java,"@@ -15,24 +15,25 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.drill.exec.expr;
+package org.apache.drill.test;
 
-import org.apache.drill.BaseTestQuery;
-import org.apache.drill.common.util.TestTools;
-import org.junit.Test;
+import org.apache.drill.common.util.GuavaPatcher;
+import org.apache.drill.common.util.ProtobufPatcher;
 
-public class TestPrune extends BaseTestQuery {
+public class BaseTest {","[{'comment': 'The term `BaseTest` is too generic: soon this will contain all manner of common code. Better solutions are:\r\n\r\n* MapRTestBase and HBaseTestBase to patch the code as needed for just those clients.\r\n* Patchers as a service added to tests where needed, such as is done with the ""dirTestWatcher""\r\n* The patch-on-load approach suggested below.\r\n\r\nThe approach here is really far too much of a hack to maintain.\r\n\r\nAlso, this patching means that code running in tests operates in a different environment than the same code running in production. That will become the cause of very mysterious bugs.\r\n\r\nCan we rethink our approach?', 'commenter': 'paul-rogers'}, {'comment': ""The original problem was caused by tests that didn't require patching, they just loaded a Guava class needed to be patched for someone else, an there is no way we can patch a class that was already loaded. Considering this we should apply patching as early as possible."", 'commenter': 'agozhiy'}, {'comment': 'Patching is done for production code also, so with patching, we make testing env closer to the production one. Also, pathing is required for a newer hadoop libraries version which is used in exec module, so comment in `BaseTest` perhaps should be updated a little bit.', 'commenter': 'vvysotskyi'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches","[{'comment': '```suggestion\r\n * <li>Input mode {@link InputBatch}: Used to buffer in-memory batches\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set","[{'comment': '```suggestion\r\n * <li>Spill mode {@link SpilledRun}: Holds a ""memento"" to a set\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 ","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (sv2 != null) {
+        sv2.clear();
+      }
+      super.close();
+    }
   }
 
   /**
-   * Updates the schema for this batch group. The current as well as any deserialized batches will be coerced to this schema
-   * @param schema
+   * Holds a set of spilled batches, represented by a file on disk.
+   * Handles reads from, and writes to the spill file. The data structure
+   * is:
+   * <ul>
+   * <li>A pointer to a file that contains serialized batches.</li>
+   * <li>When writing, each batch is appended to the output file.</li>
+   * <li>When reading, iterates over each spilled batch, and for each
+   * of those, each spilled record.</li>
+   * </ul>
+   * <p>
+   * Starts out with no current batch. Defines the current batch to be the
+   * (shell: schema without data) of the last batch spilled to disk.
+   * <p>
+   * When reading, has destructive read-once behavior: closing the
+   * batch (after reading) deletes the underlying spill file.
+   * <p>
+   * This single class does three tasks: load data, hold data and
+   * read data. This should be split into three separate classes. But,
+   * the original (combined) structure is retained for expedience at
+   * present.
    */
-  public void setSchema(BatchSchema schema) {
-    currentContainer = SchemaUtil.coerceContainer(currentContainer, schema, context);
-    this.schema = schema;
-  }
 ","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {","[{'comment': 'I really appreciate that you split the responsibilities of ```BatchGroup``` class and created ```InputBatch``` & ```SpilledRun```.  Could you please extract them into separate source files? ', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);","[{'comment': 'For other methods in the ```InputBatch```, the  _**sv2**_ field is checked for `null` before access. Is it always safe to skip the check here? ', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (sv2 != null) {
+        sv2.clear();
+      }
+      super.close();
+    }
   }
 
   /**
-   * Updates the schema for this batch group. The current as well as any deserialized batches will be coerced to this schema
-   * @param schema
+   * Holds a set of spilled batches, represented by a file on disk.
+   * Handles reads from, and writes to the spill file. The data structure
+   * is:
+   * <ul>
+   * <li>A pointer to a file that contains serialized batches.</li>
+   * <li>When writing, each batch is appended to the output file.</li>
+   * <li>When reading, iterates over each spilled batch, and for each
+   * of those, each spilled record.</li>
+   * </ul>
+   * <p>
+   * Starts out with no current batch. Defines the current batch to be the
+   * (shell: schema without data) of the last batch spilled to disk.
+   * <p>
+   * When reading, has destructive read-once behavior: closing the
+   * batch (after reading) deletes the underlying spill file.
+   * <p>
+   * This single class does three tasks: load data, hold data and
+   * read data. This should be split into three separate classes. But,
+   * the original (combined) structure is retained for expedience at
+   * present.
    */
-  public void setSchema(BatchSchema schema) {
-    currentContainer = SchemaUtil.coerceContainer(currentContainer, schema, context);
-    this.schema = schema;
-  }
 
-  public void addBatch(VectorContainer newContainer) throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (outputStream == null) {
-      outputStream = fs.create(path);
+  public static class SpilledRun extends BatchGroup {","[{'comment': 'Please extract to separate source file. ', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (sv2 != null) {
+        sv2.clear();
+      }
+      super.close();
+    }
   }
 
   /**
-   * Updates the schema for this batch group. The current as well as any deserialized batches will be coerced to this schema
-   * @param schema
+   * Holds a set of spilled batches, represented by a file on disk.
+   * Handles reads from, and writes to the spill file. The data structure
+   * is:
+   * <ul>
+   * <li>A pointer to a file that contains serialized batches.</li>
+   * <li>When writing, each batch is appended to the output file.</li>
+   * <li>When reading, iterates over each spilled batch, and for each
+   * of those, each spilled record.</li>
+   * </ul>
+   * <p>
+   * Starts out with no current batch. Defines the current batch to be the
+   * (shell: schema without data) of the last batch spilled to disk.
+   * <p>
+   * When reading, has destructive read-once behavior: closing the
+   * batch (after reading) deletes the underlying spill file.
+   * <p>
+   * This single class does three tasks: load data, hold data and
+   * read data. This should be split into three separate classes. But,
+   * the original (combined) structure is retained for expedience at
+   * present.
    */
-  public void setSchema(BatchSchema schema) {
-    currentContainer = SchemaUtil.coerceContainer(currentContainer, schema, context);
-    this.schema = schema;
-  }
 
-  public void addBatch(VectorContainer newContainer) throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (outputStream == null) {
-      outputStream = fs.create(path);
+  public static class SpilledRun extends BatchGroup {
+    private InputStream inputStream;
+    private String path;
+    private SpillSet spillSet;
+    private BufferAllocator allocator;
+    private int spilledBatches;
+    private long batchSize;","[{'comment': ""Could you please provide a comment or rename `batchSize`? It's hard to guess units of measurement here. "", 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (sv2 != null) {
+        sv2.clear();
+      }
+      super.close();
+    }
   }
 
   /**
-   * Updates the schema for this batch group. The current as well as any deserialized batches will be coerced to this schema
-   * @param schema
+   * Holds a set of spilled batches, represented by a file on disk.
+   * Handles reads from, and writes to the spill file. The data structure
+   * is:
+   * <ul>
+   * <li>A pointer to a file that contains serialized batches.</li>
+   * <li>When writing, each batch is appended to the output file.</li>
+   * <li>When reading, iterates over each spilled batch, and for each
+   * of those, each spilled record.</li>
+   * </ul>
+   * <p>
+   * Starts out with no current batch. Defines the current batch to be the
+   * (shell: schema without data) of the last batch spilled to disk.
+   * <p>
+   * When reading, has destructive read-once behavior: closing the
+   * batch (after reading) deletes the underlying spill file.
+   * <p>
+   * This single class does three tasks: load data, hold data and
+   * read data. This should be split into three separate classes. But,
+   * the original (combined) structure is retained for expedience at
+   * present.
    */
-  public void setSchema(BatchSchema schema) {
-    currentContainer = SchemaUtil.coerceContainer(currentContainer, schema, context);
-    this.schema = schema;
-  }
 
-  public void addBatch(VectorContainer newContainer) throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (outputStream == null) {
-      outputStream = fs.create(path);
+  public static class SpilledRun extends BatchGroup {
+    private InputStream inputStream;
+    private String path;
+    private SpillSet spillSet;
+    private BufferAllocator allocator;
+    private int spilledBatches;
+    private long batchSize;
+    private Writer writer;
+    private VectorSerializer.Reader reader;
+
+    public SpilledRun(SpillSet spillSet, String path, BufferAllocator allocator) throws IOException {
+      super(null, allocator);
+      this.spillSet = spillSet;
+      this.path = path;
+      this.allocator = allocator;
+      writer = spillSet.writer(path);
     }
-    int recordCount = newContainer.getRecordCount();
-    WritableBatch batch = WritableBatch.getBatchNoHVWrap(recordCount, newContainer, false);
-    VectorAccessibleSerializable outputBatch = new VectorAccessibleSerializable(batch, allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    outputBatch.writeToStream(outputStream);
-    newContainer.zeroVectors();
-    logger.debug(""Took {} us to spill {} records"", watch.elapsed(TimeUnit.MICROSECONDS), recordCount);
-    spilledBatches++;
-  }
-
-  private VectorContainer getBatch() throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (inputStream == null) {
-      inputStream = fs.open(path);
+
+    public void addBatch(VectorContainer newContainer) throws IOException {","[{'comment': '```suggestion\r\n    public void spillBatch(VectorContainer newContainer) throws IOException {\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (sv2 != null) {
+        sv2.clear();
+      }
+      super.close();
+    }
   }
 
   /**
-   * Updates the schema for this batch group. The current as well as any deserialized batches will be coerced to this schema
-   * @param schema
+   * Holds a set of spilled batches, represented by a file on disk.
+   * Handles reads from, and writes to the spill file. The data structure
+   * is:
+   * <ul>
+   * <li>A pointer to a file that contains serialized batches.</li>
+   * <li>When writing, each batch is appended to the output file.</li>
+   * <li>When reading, iterates over each spilled batch, and for each
+   * of those, each spilled record.</li>
+   * </ul>
+   * <p>
+   * Starts out with no current batch. Defines the current batch to be the
+   * (shell: schema without data) of the last batch spilled to disk.
+   * <p>
+   * When reading, has destructive read-once behavior: closing the
+   * batch (after reading) deletes the underlying spill file.
+   * <p>
+   * This single class does three tasks: load data, hold data and
+   * read data. This should be split into three separate classes. But,
+   * the original (combined) structure is retained for expedience at
+   * present.
    */
-  public void setSchema(BatchSchema schema) {
-    currentContainer = SchemaUtil.coerceContainer(currentContainer, schema, context);
-    this.schema = schema;
-  }
 
-  public void addBatch(VectorContainer newContainer) throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (outputStream == null) {
-      outputStream = fs.create(path);
+  public static class SpilledRun extends BatchGroup {
+    private InputStream inputStream;
+    private String path;
+    private SpillSet spillSet;
+    private BufferAllocator allocator;
+    private int spilledBatches;
+    private long batchSize;
+    private Writer writer;
+    private VectorSerializer.Reader reader;
+
+    public SpilledRun(SpillSet spillSet, String path, BufferAllocator allocator) throws IOException {
+      super(null, allocator);
+      this.spillSet = spillSet;
+      this.path = path;
+      this.allocator = allocator;
+      writer = spillSet.writer(path);
     }
-    int recordCount = newContainer.getRecordCount();
-    WritableBatch batch = WritableBatch.getBatchNoHVWrap(recordCount, newContainer, false);
-    VectorAccessibleSerializable outputBatch = new VectorAccessibleSerializable(batch, allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    outputBatch.writeToStream(outputStream);
-    newContainer.zeroVectors();
-    logger.debug(""Took {} us to spill {} records"", watch.elapsed(TimeUnit.MICROSECONDS), recordCount);
-    spilledBatches++;
-  }
-
-  private VectorContainer getBatch() throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (inputStream == null) {
-      inputStream = fs.open(path);
+
+    public void addBatch(VectorContainer newContainer) throws IOException {
+      writer.write(newContainer);
+      newContainer.zeroVectors();
+      logger.trace(""Wrote {} records in {} us"", newContainer.getRecordCount(), writer.time(TimeUnit.MICROSECONDS));
+      spilledBatches++;
+
+      // Hold onto the husk of the last added container so that we have a
+      // current container when starting to read rows back later.
+
+      currentContainer = newContainer;
+      currentContainer.setRecordCount(0);
     }
-    VectorAccessibleSerializable vas = new VectorAccessibleSerializable(allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    vas.readFromStream(inputStream);
-    VectorContainer c =  vas.get();
-    if (schema != null) {
-      c = SchemaUtil.coerceContainer(c, schema, context);
+
+    public void setBatchSize(long batchSize) {
+      this.batchSize = batchSize;
     }
-    logger.trace(""Took {} us to read {} records"", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());
-    spilledBatches--;
-    currentContainer.zeroVectors();
-    Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();
-    for (VectorWrapper<?> w : currentContainer) {
-      TransferPair pair = wrapperIterator.next().getValueVector().makeTransferPair(w.getValueVector());
-      pair.transfer();
+
+    public long getBatchSize() { return batchSize; }
+    public String getPath() { return path; }
+
+    @Override
+    public int getNextIndex() {
+      if (pointer == getRecordCount()) {
+        if (spilledBatches == 0) {
+          return -1;
+        }
+        try {
+          currentContainer.zeroVectors();
+          getBatch();
+        } catch (IOException e) {
+          // Release any partially-loaded data.
+          currentContainer.clear();
+          throw UserException.dataReadError(e)
+              .message(""Failure while reading spilled data"")
+              .build(logger);
+        }
+
+        // The pointer indicates the NEXT index, not the one we
+        // return here. At this point, we just started reading a
+        // new batch and have returned index 0. So, the next index
+        // is 1.
+
+        pointer = 1;
+        return 0;
+      }
+      return super.getNextIndex();
     }
-    currentContainer.setRecordCount(c.getRecordCount());
-    c.zeroVectors();
-    return c;
-  }
 
-  public int getNextIndex() {
-    int val;
-    if (pointer == getRecordCount()) {
-      if (spilledBatches == 0) {
-        return -1;
+    private VectorContainer getBatch() throws IOException {
+      if (inputStream == null) {
+        inputStream = spillSet.openForInput(path);
+        reader = VectorSerializer.reader(allocator, inputStream);
+      }
+      Stopwatch watch = Stopwatch.createStarted();
+      long start = allocator.getAllocatedMemory();
+      VectorContainer c =  reader.read();
+      long end = allocator.getAllocatedMemory();
+      logger.trace(""Read {} records in {} us; size = {}, memory = {}"",
+                   c.getRecordCount(),
+                   watch.elapsed(TimeUnit.MICROSECONDS),
+                   (end - start), end);
+      if (schema != null) {
+        c = SchemaUtil.coerceContainer(c, schema, allocator);
+      }
+      spilledBatches--;
+      currentContainer.zeroVectors();
+      Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();
+      for (VectorWrapper<?> w : currentContainer) {
+        TransferPair pair = wrapperIterator.next().getValueVector().makeTransferPair(w.getValueVector());
+        pair.transfer();
+      }
+      currentContainer.setRecordCount(c.getRecordCount());
+      c.zeroVectors();
+      return c;
+    }
+
+    /**
+     * Close resources owned by this batch group. Each can fail; report
+     * only the first error. This is cluttered because this class tries
+     * to do multiple tasks. TODO: Split into multiple classes.
+     */
+
+    @Override
+    public void close() throws IOException {
+      IOException ex = null;
+      try {
+        super.close();
+      } catch (IOException e) {
+        ex = e;
       }
       try {
-        currentContainer.zeroVectors();
-        getBatch();
+        closeWriter();
       } catch (IOException e) {
-        throw new RuntimeException(e);
+        ex = ex == null ? e : ex;
       }
-      pointer = 1;
-      return 0;
+      try {
+        closeInputStream();
+      } catch (IOException e) {
+        ex = ex == null ? e : ex;
+      }
+      try {
+        spillSet.delete(path);
+      } catch (IOException e) {
+        ex = ex == null ? e : ex;
+      }
+      if (ex != null) {
+        throw ex;
+      }
+    }
+
+    private void closeInputStream() throws IOException {
+      if (inputStream == null) {
+        return;
+      }
+      long readLength = spillSet.getPosition(inputStream);
+      spillSet.tallyReadBytes(readLength);
+      inputStream.close();
+      inputStream = null;
+      reader = null;
+      logger.trace(""Summary: Read {} bytes from {}"", readLength, path);
     }
-    if (sv2 == null) {
-      val = pointer;
-      pointer++;
-      assert val < currentContainer.getRecordCount();
-    } else {
-      val = pointer;
-      pointer++;
-      assert val < currentContainer.getRecordCount();
-      val = sv2.getIndex(val);
+
+    public void closeWriter() throws IOException {
+      if (writer != null) {
+        spillSet.close(writer);
+        logger.trace(""Summary: Wrote {} bytes in {} us to {}"", writer.getBytesWritten(), writer.time(TimeUnit.MICROSECONDS), path);
+        writer = null;
+      }
     }
+  }
+
+  protected VectorContainer currentContainer;
+  protected int pointer = 0;","[{'comment': 'Could you please provide a more meaningful name for this field? For example, `nextRecordPointer`. ', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -221,4 +363,19 @@ public SelectionVector4 getSelectionVector4() {
     throw new UnsupportedOperationException();
   }
 
+  public static void closeAll(Collection<? extends BatchGroup> groups) {","[{'comment': 'Please remove the method and use one from ```AutoCloseables```. Note the util methods in `AutoCloseables` might be extended to not throw checked exception but accept function or consumer which will convert to runtime.', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (sv2 != null) {
+        sv2.clear();
+      }
+      super.close();
+    }
   }
 
   /**
-   * Updates the schema for this batch group. The current as well as any deserialized batches will be coerced to this schema
-   * @param schema
+   * Holds a set of spilled batches, represented by a file on disk.
+   * Handles reads from, and writes to the spill file. The data structure
+   * is:
+   * <ul>
+   * <li>A pointer to a file that contains serialized batches.</li>
+   * <li>When writing, each batch is appended to the output file.</li>
+   * <li>When reading, iterates over each spilled batch, and for each
+   * of those, each spilled record.</li>
+   * </ul>
+   * <p>
+   * Starts out with no current batch. Defines the current batch to be the
+   * (shell: schema without data) of the last batch spilled to disk.
+   * <p>
+   * When reading, has destructive read-once behavior: closing the
+   * batch (after reading) deletes the underlying spill file.
+   * <p>
+   * This single class does three tasks: load data, hold data and
+   * read data. This should be split into three separate classes. But,
+   * the original (combined) structure is retained for expedience at
+   * present.
    */
-  public void setSchema(BatchSchema schema) {
-    currentContainer = SchemaUtil.coerceContainer(currentContainer, schema, context);
-    this.schema = schema;
-  }
 
-  public void addBatch(VectorContainer newContainer) throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (outputStream == null) {
-      outputStream = fs.create(path);
+  public static class SpilledRun extends BatchGroup {
+    private InputStream inputStream;
+    private String path;
+    private SpillSet spillSet;
+    private BufferAllocator allocator;
+    private int spilledBatches;
+    private long batchSize;
+    private Writer writer;
+    private VectorSerializer.Reader reader;
+
+    public SpilledRun(SpillSet spillSet, String path, BufferAllocator allocator) throws IOException {
+      super(null, allocator);
+      this.spillSet = spillSet;
+      this.path = path;
+      this.allocator = allocator;
+      writer = spillSet.writer(path);
     }
-    int recordCount = newContainer.getRecordCount();
-    WritableBatch batch = WritableBatch.getBatchNoHVWrap(recordCount, newContainer, false);
-    VectorAccessibleSerializable outputBatch = new VectorAccessibleSerializable(batch, allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    outputBatch.writeToStream(outputStream);
-    newContainer.zeroVectors();
-    logger.debug(""Took {} us to spill {} records"", watch.elapsed(TimeUnit.MICROSECONDS), recordCount);
-    spilledBatches++;
-  }
-
-  private VectorContainer getBatch() throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (inputStream == null) {
-      inputStream = fs.open(path);
+
+    public void addBatch(VectorContainer newContainer) throws IOException {
+      writer.write(newContainer);
+      newContainer.zeroVectors();
+      logger.trace(""Wrote {} records in {} us"", newContainer.getRecordCount(), writer.time(TimeUnit.MICROSECONDS));
+      spilledBatches++;
+
+      // Hold onto the husk of the last added container so that we have a
+      // current container when starting to read rows back later.
+
+      currentContainer = newContainer;
+      currentContainer.setRecordCount(0);
     }
-    VectorAccessibleSerializable vas = new VectorAccessibleSerializable(allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    vas.readFromStream(inputStream);
-    VectorContainer c =  vas.get();
-    if (schema != null) {
-      c = SchemaUtil.coerceContainer(c, schema, context);
+
+    public void setBatchSize(long batchSize) {
+      this.batchSize = batchSize;
     }
-    logger.trace(""Took {} us to read {} records"", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());
-    spilledBatches--;
-    currentContainer.zeroVectors();
-    Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();
-    for (VectorWrapper<?> w : currentContainer) {
-      TransferPair pair = wrapperIterator.next().getValueVector().makeTransferPair(w.getValueVector());
-      pair.transfer();
+
+    public long getBatchSize() { return batchSize; }
+    public String getPath() { return path; }
+
+    @Override
+    public int getNextIndex() {
+      if (pointer == getRecordCount()) {
+        if (spilledBatches == 0) {
+          return -1;
+        }
+        try {
+          currentContainer.zeroVectors();
+          getBatch();
+        } catch (IOException e) {
+          // Release any partially-loaded data.
+          currentContainer.clear();
+          throw UserException.dataReadError(e)
+              .message(""Failure while reading spilled data"")
+              .build(logger);
+        }
+
+        // The pointer indicates the NEXT index, not the one we
+        // return here. At this point, we just started reading a
+        // new batch and have returned index 0. So, the next index
+        // is 1.
+
+        pointer = 1;
+        return 0;
+      }
+      return super.getNextIndex();
     }
-    currentContainer.setRecordCount(c.getRecordCount());
-    c.zeroVectors();
-    return c;
-  }
 
-  public int getNextIndex() {
-    int val;
-    if (pointer == getRecordCount()) {
-      if (spilledBatches == 0) {
-        return -1;
+    private VectorContainer getBatch() throws IOException {
+      if (inputStream == null) {
+        inputStream = spillSet.openForInput(path);
+        reader = VectorSerializer.reader(allocator, inputStream);
+      }
+      Stopwatch watch = Stopwatch.createStarted();
+      long start = allocator.getAllocatedMemory();
+      VectorContainer c =  reader.read();
+      long end = allocator.getAllocatedMemory();
+      logger.trace(""Read {} records in {} us; size = {}, memory = {}"",
+                   c.getRecordCount(),
+                   watch.elapsed(TimeUnit.MICROSECONDS),
+                   (end - start), end);
+      if (schema != null) {
+        c = SchemaUtil.coerceContainer(c, schema, allocator);
+      }
+      spilledBatches--;
+      currentContainer.zeroVectors();
+      Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();
+      for (VectorWrapper<?> w : currentContainer) {
+        TransferPair pair = wrapperIterator.next().getValueVector().makeTransferPair(w.getValueVector());
+        pair.transfer();
+      }
+      currentContainer.setRecordCount(c.getRecordCount());
+      c.zeroVectors();
+      return c;
+    }
+
+    /**
+     * Close resources owned by this batch group. Each can fail; report
+     * only the first error. This is cluttered because this class tries
+     * to do multiple tasks. TODO: Split into multiple classes.
+     */
+","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (sv2 != null) {
+        sv2.clear();
+      }
+      super.close();
+    }
   }
 
   /**
-   * Updates the schema for this batch group. The current as well as any deserialized batches will be coerced to this schema
-   * @param schema
+   * Holds a set of spilled batches, represented by a file on disk.
+   * Handles reads from, and writes to the spill file. The data structure
+   * is:
+   * <ul>
+   * <li>A pointer to a file that contains serialized batches.</li>
+   * <li>When writing, each batch is appended to the output file.</li>
+   * <li>When reading, iterates over each spilled batch, and for each
+   * of those, each spilled record.</li>
+   * </ul>
+   * <p>
+   * Starts out with no current batch. Defines the current batch to be the
+   * (shell: schema without data) of the last batch spilled to disk.
+   * <p>
+   * When reading, has destructive read-once behavior: closing the
+   * batch (after reading) deletes the underlying spill file.
+   * <p>
+   * This single class does three tasks: load data, hold data and
+   * read data. This should be split into three separate classes. But,
+   * the original (combined) structure is retained for expedience at
+   * present.
    */
-  public void setSchema(BatchSchema schema) {
-    currentContainer = SchemaUtil.coerceContainer(currentContainer, schema, context);
-    this.schema = schema;
-  }
 
-  public void addBatch(VectorContainer newContainer) throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (outputStream == null) {
-      outputStream = fs.create(path);
+  public static class SpilledRun extends BatchGroup {
+    private InputStream inputStream;
+    private String path;
+    private SpillSet spillSet;
+    private BufferAllocator allocator;
+    private int spilledBatches;
+    private long batchSize;
+    private Writer writer;
+    private VectorSerializer.Reader reader;
+
+    public SpilledRun(SpillSet spillSet, String path, BufferAllocator allocator) throws IOException {
+      super(null, allocator);
+      this.spillSet = spillSet;
+      this.path = path;
+      this.allocator = allocator;
+      writer = spillSet.writer(path);
     }
-    int recordCount = newContainer.getRecordCount();
-    WritableBatch batch = WritableBatch.getBatchNoHVWrap(recordCount, newContainer, false);
-    VectorAccessibleSerializable outputBatch = new VectorAccessibleSerializable(batch, allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    outputBatch.writeToStream(outputStream);
-    newContainer.zeroVectors();
-    logger.debug(""Took {} us to spill {} records"", watch.elapsed(TimeUnit.MICROSECONDS), recordCount);
-    spilledBatches++;
-  }
-
-  private VectorContainer getBatch() throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (inputStream == null) {
-      inputStream = fs.open(path);
+
+    public void addBatch(VectorContainer newContainer) throws IOException {
+      writer.write(newContainer);
+      newContainer.zeroVectors();
+      logger.trace(""Wrote {} records in {} us"", newContainer.getRecordCount(), writer.time(TimeUnit.MICROSECONDS));
+      spilledBatches++;
+
+      // Hold onto the husk of the last added container so that we have a
+      // current container when starting to read rows back later.
+
+      currentContainer = newContainer;
+      currentContainer.setRecordCount(0);
     }
-    VectorAccessibleSerializable vas = new VectorAccessibleSerializable(allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    vas.readFromStream(inputStream);
-    VectorContainer c =  vas.get();
-    if (schema != null) {
-      c = SchemaUtil.coerceContainer(c, schema, context);
+
+    public void setBatchSize(long batchSize) {
+      this.batchSize = batchSize;
     }
-    logger.trace(""Took {} us to read {} records"", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());
-    spilledBatches--;
-    currentContainer.zeroVectors();
-    Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();
-    for (VectorWrapper<?> w : currentContainer) {
-      TransferPair pair = wrapperIterator.next().getValueVector().makeTransferPair(w.getValueVector());
-      pair.transfer();
+
+    public long getBatchSize() { return batchSize; }
+    public String getPath() { return path; }
+
+    @Override
+    public int getNextIndex() {
+      if (pointer == getRecordCount()) {
+        if (spilledBatches == 0) {
+          return -1;
+        }
+        try {
+          currentContainer.zeroVectors();
+          getBatch();
+        } catch (IOException e) {
+          // Release any partially-loaded data.
+          currentContainer.clear();
+          throw UserException.dataReadError(e)
+              .message(""Failure while reading spilled data"")
+              .build(logger);
+        }
+
+        // The pointer indicates the NEXT index, not the one we
+        // return here. At this point, we just started reading a
+        // new batch and have returned index 0. So, the next index
+        // is 1.
+
+        pointer = 1;
+        return 0;
+      }
+      return super.getNextIndex();
     }
-    currentContainer.setRecordCount(c.getRecordCount());
-    c.zeroVectors();
-    return c;
-  }
 
-  public int getNextIndex() {
-    int val;
-    if (pointer == getRecordCount()) {
-      if (spilledBatches == 0) {
-        return -1;
+    private VectorContainer getBatch() throws IOException {
+      if (inputStream == null) {
+        inputStream = spillSet.openForInput(path);
+        reader = VectorSerializer.reader(allocator, inputStream);
+      }
+      Stopwatch watch = Stopwatch.createStarted();
+      long start = allocator.getAllocatedMemory();
+      VectorContainer c =  reader.read();
+      long end = allocator.getAllocatedMemory();
+      logger.trace(""Read {} records in {} us; size = {}, memory = {}"",
+                   c.getRecordCount(),
+                   watch.elapsed(TimeUnit.MICROSECONDS),
+                   (end - start), end);
+      if (schema != null) {
+        c = SchemaUtil.coerceContainer(c, schema, allocator);
+      }
+      spilledBatches--;
+      currentContainer.zeroVectors();
+      Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();
+      for (VectorWrapper<?> w : currentContainer) {
+        TransferPair pair = wrapperIterator.next().getValueVector().makeTransferPair(w.getValueVector());
+        pair.transfer();
+      }
+      currentContainer.setRecordCount(c.getRecordCount());
+      c.zeroVectors();
+      return c;
+    }
+
+    /**
+     * Close resources owned by this batch group. Each can fail; report
+     * only the first error. This is cluttered because this class tries
+     * to do multiple tasks. TODO: Split into multiple classes.
+     */
+
+    @Override
+    public void close() throws IOException {","[{'comment': 'Please use ```AutoCloseables``` instead of long try-catches: \r\n```java\r\n      try {\r\n        AutoCloseables.close(super::close, this::closeWriter,\r\n            this::closeInputStream, () -> spillSet.delete(path));\r\n      } catch (Exception e) {\r\n        throw (e instanceof IOException) ? (IOException) e : new IOException(e);\r\n      }\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java,"@@ -18,140 +18,300 @@
 package org.apache.drill.exec.physical.impl.xsort;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.drill.common.exceptions.UserException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.cache.VectorAccessibleSerializable;
+import org.apache.drill.exec.cache.VectorSerializer;
+import org.apache.drill.exec.cache.VectorSerializer.Writer;
 import org.apache.drill.exec.memory.BufferAllocator;
-import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.SchemaUtil;
 import org.apache.drill.exec.record.TransferPair;
 import org.apache.drill.exec.record.TypedFieldId;
 import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
 
-public class BatchGroup implements VectorAccessible, AutoCloseable {
+/**
+ * Represents a group of batches spilled to disk.
+ * <p>
+ * The batches are defined by a schema which can change over time. When the schema changes,
+ * all existing and new batches are coerced into the new schema. Provides a
+ * uniform way to iterate over records for one or more batches whether
+ * the batches are in memory or on disk.
+ * <p>
+ * The <code>BatchGroup</code> operates in two modes as given by the two
+ * subclasses:
+ * <ul>
+ * <li>Input mode (@link InputBatchGroup): Used to buffer in-memory batches
+ * prior to spilling.</li>
+ * <li>Spill mode (@link SpilledBatchGroup): Holds a ""memento"" to a set
+ * of batches written to disk. Acts as both a reader and writer for
+ * those batches.</li>
+ */
+
+public abstract class BatchGroup implements VectorAccessible, AutoCloseable {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BatchGroup.class);
 
-  private VectorContainer currentContainer;
-  private SelectionVector2 sv2;
-  private int pointer = 0;
-  private FSDataInputStream inputStream;
-  private FSDataOutputStream outputStream;
-  private Path path;
-  private FileSystem fs;
-  private BufferAllocator allocator;
-  private int spilledBatches = 0;
-  private OperatorContext context;
-  private BatchSchema schema;
-
-  public BatchGroup(VectorContainer container, SelectionVector2 sv2, OperatorContext context) {
-    this.sv2 = sv2;
-    this.currentContainer = container;
-    this.context = context;
-  }
+  /**
+   * The input batch group gathers batches buffered in memory before
+   * spilling. The structure of the data is:
+   * <ul>
+   * <li>Contains a single batch received from the upstream (input)
+   * operator.</li>
+   * <li>Associated selection vector that provides a sorted
+   * indirection to the values in the batch.</li>
+   * </ul>
+   */
 
-  public BatchGroup(VectorContainer container, FileSystem fs, String path, OperatorContext context) {
-    currentContainer = container;
-    this.fs = fs;
-    this.path = new Path(path);
-    this.allocator = context.getAllocator();
-    this.context = context;
-  }
+  public static class InputBatch extends BatchGroup {
+    private final SelectionVector2 sv2;
+    private final long dataSize;
+
+    public InputBatch(VectorContainer container, SelectionVector2 sv2, BufferAllocator allocator, long dataSize) {
+      super(container, allocator);
+      this.sv2 = sv2;
+      this.dataSize = dataSize;
+    }
 
-  public SelectionVector2 getSv2() {
-    return sv2;
+    public SelectionVector2 getSv2() { return sv2; }
+
+    public long getDataSize() { return dataSize; }
+
+    @Override
+    public int getRecordCount() {
+      if (sv2 != null) {
+        return sv2.getCount();
+      } else {
+        return super.getRecordCount();
+      }
+    }
+
+    @Override
+    public int getNextIndex() {
+      int val = super.getNextIndex();
+      if (val == -1) {
+        return val;
+      }
+      return sv2.getIndex(val);
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (sv2 != null) {
+        sv2.clear();
+      }
+      super.close();
+    }
   }
 
   /**
-   * Updates the schema for this batch group. The current as well as any deserialized batches will be coerced to this schema
-   * @param schema
+   * Holds a set of spilled batches, represented by a file on disk.
+   * Handles reads from, and writes to the spill file. The data structure
+   * is:
+   * <ul>
+   * <li>A pointer to a file that contains serialized batches.</li>
+   * <li>When writing, each batch is appended to the output file.</li>
+   * <li>When reading, iterates over each spilled batch, and for each
+   * of those, each spilled record.</li>
+   * </ul>
+   * <p>
+   * Starts out with no current batch. Defines the current batch to be the
+   * (shell: schema without data) of the last batch spilled to disk.
+   * <p>
+   * When reading, has destructive read-once behavior: closing the
+   * batch (after reading) deletes the underlying spill file.
+   * <p>
+   * This single class does three tasks: load data, hold data and
+   * read data. This should be split into three separate classes. But,
+   * the original (combined) structure is retained for expedience at
+   * present.
    */
-  public void setSchema(BatchSchema schema) {
-    currentContainer = SchemaUtil.coerceContainer(currentContainer, schema, context);
-    this.schema = schema;
-  }
 
-  public void addBatch(VectorContainer newContainer) throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (outputStream == null) {
-      outputStream = fs.create(path);
+  public static class SpilledRun extends BatchGroup {
+    private InputStream inputStream;
+    private String path;
+    private SpillSet spillSet;
+    private BufferAllocator allocator;
+    private int spilledBatches;
+    private long batchSize;
+    private Writer writer;
+    private VectorSerializer.Reader reader;
+
+    public SpilledRun(SpillSet spillSet, String path, BufferAllocator allocator) throws IOException {
+      super(null, allocator);
+      this.spillSet = spillSet;
+      this.path = path;
+      this.allocator = allocator;
+      writer = spillSet.writer(path);
     }
-    int recordCount = newContainer.getRecordCount();
-    WritableBatch batch = WritableBatch.getBatchNoHVWrap(recordCount, newContainer, false);
-    VectorAccessibleSerializable outputBatch = new VectorAccessibleSerializable(batch, allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    outputBatch.writeToStream(outputStream);
-    newContainer.zeroVectors();
-    logger.debug(""Took {} us to spill {} records"", watch.elapsed(TimeUnit.MICROSECONDS), recordCount);
-    spilledBatches++;
-  }
-
-  private VectorContainer getBatch() throws IOException {
-    assert fs != null;
-    assert path != null;
-    if (inputStream == null) {
-      inputStream = fs.open(path);
+
+    public void addBatch(VectorContainer newContainer) throws IOException {
+      writer.write(newContainer);
+      newContainer.zeroVectors();
+      logger.trace(""Wrote {} records in {} us"", newContainer.getRecordCount(), writer.time(TimeUnit.MICROSECONDS));
+      spilledBatches++;
+
+      // Hold onto the husk of the last added container so that we have a
+      // current container when starting to read rows back later.
+
+      currentContainer = newContainer;
+      currentContainer.setRecordCount(0);
     }
-    VectorAccessibleSerializable vas = new VectorAccessibleSerializable(allocator);
-    Stopwatch watch = Stopwatch.createStarted();
-    vas.readFromStream(inputStream);
-    VectorContainer c =  vas.get();
-    if (schema != null) {
-      c = SchemaUtil.coerceContainer(c, schema, context);
+
+    public void setBatchSize(long batchSize) {
+      this.batchSize = batchSize;
     }
-    logger.trace(""Took {} us to read {} records"", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());
-    spilledBatches--;
-    currentContainer.zeroVectors();
-    Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();
-    for (VectorWrapper<?> w : currentContainer) {
-      TransferPair pair = wrapperIterator.next().getValueVector().makeTransferPair(w.getValueVector());
-      pair.transfer();
+
+    public long getBatchSize() { return batchSize; }
+    public String getPath() { return path; }
+
+    @Override
+    public int getNextIndex() {
+      if (pointer == getRecordCount()) {
+        if (spilledBatches == 0) {
+          return -1;
+        }
+        try {
+          currentContainer.zeroVectors();
+          getBatch();
+        } catch (IOException e) {
+          // Release any partially-loaded data.
+          currentContainer.clear();
+          throw UserException.dataReadError(e)
+              .message(""Failure while reading spilled data"")
+              .build(logger);
+        }","[{'comment': 'I think it would be better to push try-catch inside the ```getBatch()``` method and then rename it to ```private void loadBatch() {```. ', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -17,250 +17,255 @@
  */
 package org.apache.drill.exec.physical.impl.xsort;
 
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
-import org.apache.calcite.rel.RelFieldCollation.Direction;
-import org.apache.drill.common.AutoCloseables;
-import org.apache.drill.common.config.DrillConfig;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.EMIT;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.NONE;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.OK;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.OK_NEW_SCHEMA;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.STOP;
+
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.expression.ErrorCollector;
-import org.apache.drill.common.expression.ErrorCollectorImpl;
-import org.apache.drill.common.expression.LogicalExpression;
-import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.common.logical.data.Order.Ordering;
-import org.apache.drill.exec.ExecConstants;
-import org.apache.drill.exec.compile.sig.GeneratorMapping;
-import org.apache.drill.exec.compile.sig.MappingSet;
-import org.apache.drill.exec.exception.ClassTransformationException;
-import org.apache.drill.exec.exception.OutOfMemoryException;
-import org.apache.drill.exec.exception.SchemaChangeException;
-import org.apache.drill.exec.expr.ClassGenerator;
-import org.apache.drill.exec.expr.ClassGenerator.HoldingContainer;
-import org.apache.drill.exec.expr.CodeGenerator;
-import org.apache.drill.exec.expr.ExpressionTreeMaterializer;
-import org.apache.drill.exec.expr.TypeHelper;
-import org.apache.drill.exec.expr.fn.FunctionGenerationHelper;
-import org.apache.drill.exec.memory.BufferAllocator;
 import org.apache.drill.exec.ops.FragmentContext;
 import org.apache.drill.exec.ops.MetricDef;
 import org.apache.drill.exec.physical.config.ExternalSort;
-import org.apache.drill.exec.physical.impl.sort.RecordBatchData;
-import org.apache.drill.exec.physical.impl.sort.SortRecordBatchBuilder;
-import org.apache.drill.exec.proto.ExecProtos.FragmentHandle;
-import org.apache.drill.exec.proto.helper.QueryIdHelper;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
+import org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator;
+import org.apache.drill.exec.physical.impl.xsort.SortImpl.SortResults;
 import org.apache.drill.exec.record.AbstractRecordBatch;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
-import org.apache.drill.exec.record.MaterializedField;
 import org.apache.drill.exec.record.RecordBatch;
 import org.apache.drill.exec.record.SchemaUtil;
-import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
 import org.apache.drill.exec.testing.ControlsInjector;
 import org.apache.drill.exec.testing.ControlsInjectorFactory;
-import org.apache.drill.exec.vector.CopyUtil;
 import org.apache.drill.exec.vector.ValueVector;
 import org.apache.drill.exec.vector.complex.AbstractContainerVector;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.apache.drill.shaded.guava.com.google.common.base.Joiner;
-import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
-import org.apache.drill.shaded.guava.com.google.common.collect.Iterators;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
-import com.sun.codemodel.JConditional;
-import com.sun.codemodel.JExpr;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * External sort batch: a sort batch which can spill to disk in
+ * order to operate within a defined memory footprint.
+ * <p>
+ * <h4>Basic Operation</h4>
+ * The operator has three key phases:
+ * <p>
+ * <ul>
+ * <li>The load phase in which batches are read from upstream.</li>
+ * <li>The merge phase in which spilled batches are combined to
+ * reduce the number of files below the configured limit. (Best
+ * practice is to configure the system to avoid this phase.)
+ * <li>The delivery phase in which batches are combined to produce
+ * the final output.</li>
+ * </ul>
+ * During the load phase:
+ * <p>
+ * <ul>
+ * <li>The incoming (upstream) operator provides a series of batches.</li>
+ * <li>This operator sorts each batch, and accumulates them in an in-memory
+ * buffer.</li>
+ * <li>If the in-memory buffer becomes too large, this operator selects
+ * a subset of the buffered batches to spill.</li>
+ * <li>Each spill set is merged to create a new, sorted collection of
+ * batches, and each is spilled to disk.</li>
+ * <li>To allow the use of multiple disk storage, each spill group is written
+ * round-robin to a set of spill directories.</li>
+ * </ul>
+ * <p>
+ * Data is spilled to disk as a ""run"". A run consists of one or more (typically
+ * many) batches, each of which is itself a sorted run of records.
+ * <p>
+ * During the sort/merge phase:
+ * <p>
+ * <ul>
+ * <li>When the input operator is complete, this operator merges the accumulated
+ * batches (which may be all in memory or partially on disk), and returns
+ * them to the output (downstream) operator in chunks of no more than
+ * 64K records.</li>
+ * <li>The final merge must combine a collection of in-memory and spilled
+ * batches. Several limits apply to the maximum ""width"" of this merge. For
+ * example, each open spill run consumes a file handle, and we may wish
+ * to limit the number of file handles. Further, memory must hold one batch
+ * from each run, so we may need to reduce the number of runs so that the
+ * remaining runs can fit into memory. A consolidation phase combines
+ * in-memory and spilled batches prior to the final merge to control final
+ * merge width.</li>
+ * <li>A special case occurs if no batches were spilled. In this case, the input
+ * batches are sorted in memory without merging.</li>
+ * </ul>
+ * <p>
+ * Many complex details are involved in doing the above; the details are explained
+ * in the methods of this class.
+ * <p>
+ * <h4>Configuration Options</h4>
+ * <dl>
+ * <dt>drill.exec.sort.external.spill.fs</dt>
+ * <dd>The file system (file://, hdfs://, etc.) of the spill directory.</dd>
+ * <dt>drill.exec.sort.external.spill.directories</dt>
+ * <dd>The comma delimited list of directories, on the above file
+ * system, to which to spill files in round-robin fashion. The query will
+ * fail if any one of the directories becomes full.</dt>
+ * <dt>drill.exec.sort.external.spill.file_size</dt>
+ * <dd>Target size for first-generation spill files Set this to large
+ * enough to get nice long writes, but not so large that spill directories
+ * are overwhelmed.</dd>
+ * <dt>drill.exec.sort.external.mem_limit</dt>
+ * <dd>Maximum memory to use for the in-memory buffer. (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.batch_limit</dt>
+ * <dd>Maximum number of batches to hold in memory. (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.spill.max_count</dt>
+ * <dd>Maximum number of batches to add to ""first generation"" files.
+ * Defaults to 0 (no limit). (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.spill.min_count</dt>
+ * <dd>Minimum number of batches to add to ""first generation"" files.
+ * Defaults to 0 (no limit). (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.merge_limit</dt>
+ * <dd>Sets the maximum number of runs to be merged in a single pass (limits
+ * the number of open files.)</dd>
+ * </dl>
+ * <p>
+ * The memory limit observed by this operator is the lesser of:
+ * <ul>
+ * <li>The maximum allocation allowed the allocator assigned to this batch
+ * as set by the Foreman, or</li>
+ * <li>The maximum limit configured in the mem_limit parameter above. (Primarily for
+ * testing.</li>
+ * </ul>
+ * <h4>Output</h4>
+ * It is helpful to note that the sort operator will produce one of two kinds of
+ * output batches.
+ * <ul>
+ * <li>A large output with sv4 if data is sorted in memory. The sv4 addresses
+ * the entire in-memory sort set. A selection vector remover will copy results
+ * into new batches of a size determined by that operator.</li>
+ * <li>A series of batches, without a selection vector, if the sort spills to
+ * disk. In this case, the downstream operator will still be a selection vector
+ * remover, but there is nothing for that operator to remove.
+ * </ul>
+ * Note that, even in the in-memory sort case, this operator could do the copying
+ * to eliminate the extra selection vector remover. That is left as an exercise
+ * for another time.
+ * <h4>Logging</h4>
+ * Logging in this operator serves two purposes:
+ * <li>
+ * <ul>
+ * <li>Normal diagnostic information.</li>
+ * <li>Capturing the essence of the operator functionality for analysis in unit
+ * tests.</li>
+ * </ul>
+ * Test logging is designed to capture key events and timings. Take care
+ * when changing or removing log messages as you may need to adjust unit tests
+ * accordingly.
+ */
 
 public class ExternalSortBatch extends AbstractRecordBatch<ExternalSort> {
-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ExternalSortBatch.class);
-  private static final ControlsInjector injector = ControlsInjectorFactory.getInjector(ExternalSortBatch.class);
-
-  private static final GeneratorMapping COPIER_MAPPING = new GeneratorMapping(""doSetup"", ""doCopy"", null, null);
-  private final MappingSet MAIN_MAPPING = new MappingSet( (String) null, null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet LEFT_MAPPING = new MappingSet(""leftIndex"", null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet RIGHT_MAPPING = new MappingSet(""rightIndex"", null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet COPIER_MAPPING_SET = new MappingSet(COPIER_MAPPING, COPIER_MAPPING);
-
-  private final int SPILL_BATCH_GROUP_SIZE;
-  private final int SPILL_THRESHOLD;
-  private final Iterator<String> dirs;
+  static final Logger logger = LoggerFactory.getLogger(ExternalSortBatch.class);
+
+  // For backward compatibility, masquerade as the original
+  // external sort. Else, some tests don't pass.
+","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -17,250 +17,255 @@
  */
 package org.apache.drill.exec.physical.impl.xsort;
 
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
-import org.apache.calcite.rel.RelFieldCollation.Direction;
-import org.apache.drill.common.AutoCloseables;
-import org.apache.drill.common.config.DrillConfig;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.EMIT;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.NONE;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.OK;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.OK_NEW_SCHEMA;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.STOP;
+
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.expression.ErrorCollector;
-import org.apache.drill.common.expression.ErrorCollectorImpl;
-import org.apache.drill.common.expression.LogicalExpression;
-import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.common.logical.data.Order.Ordering;
-import org.apache.drill.exec.ExecConstants;
-import org.apache.drill.exec.compile.sig.GeneratorMapping;
-import org.apache.drill.exec.compile.sig.MappingSet;
-import org.apache.drill.exec.exception.ClassTransformationException;
-import org.apache.drill.exec.exception.OutOfMemoryException;
-import org.apache.drill.exec.exception.SchemaChangeException;
-import org.apache.drill.exec.expr.ClassGenerator;
-import org.apache.drill.exec.expr.ClassGenerator.HoldingContainer;
-import org.apache.drill.exec.expr.CodeGenerator;
-import org.apache.drill.exec.expr.ExpressionTreeMaterializer;
-import org.apache.drill.exec.expr.TypeHelper;
-import org.apache.drill.exec.expr.fn.FunctionGenerationHelper;
-import org.apache.drill.exec.memory.BufferAllocator;
 import org.apache.drill.exec.ops.FragmentContext;
 import org.apache.drill.exec.ops.MetricDef;
 import org.apache.drill.exec.physical.config.ExternalSort;
-import org.apache.drill.exec.physical.impl.sort.RecordBatchData;
-import org.apache.drill.exec.physical.impl.sort.SortRecordBatchBuilder;
-import org.apache.drill.exec.proto.ExecProtos.FragmentHandle;
-import org.apache.drill.exec.proto.helper.QueryIdHelper;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
+import org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator;
+import org.apache.drill.exec.physical.impl.xsort.SortImpl.SortResults;
 import org.apache.drill.exec.record.AbstractRecordBatch;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
-import org.apache.drill.exec.record.MaterializedField;
 import org.apache.drill.exec.record.RecordBatch;
 import org.apache.drill.exec.record.SchemaUtil;
-import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
 import org.apache.drill.exec.testing.ControlsInjector;
 import org.apache.drill.exec.testing.ControlsInjectorFactory;
-import org.apache.drill.exec.vector.CopyUtil;
 import org.apache.drill.exec.vector.ValueVector;
 import org.apache.drill.exec.vector.complex.AbstractContainerVector;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.apache.drill.shaded.guava.com.google.common.base.Joiner;
-import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
-import org.apache.drill.shaded.guava.com.google.common.collect.Iterators;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
-import com.sun.codemodel.JConditional;
-import com.sun.codemodel.JExpr;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * External sort batch: a sort batch which can spill to disk in
+ * order to operate within a defined memory footprint.
+ * <p>
+ * <h4>Basic Operation</h4>
+ * The operator has three key phases:
+ * <p>
+ * <ul>
+ * <li>The load phase in which batches are read from upstream.</li>
+ * <li>The merge phase in which spilled batches are combined to
+ * reduce the number of files below the configured limit. (Best
+ * practice is to configure the system to avoid this phase.)
+ * <li>The delivery phase in which batches are combined to produce
+ * the final output.</li>
+ * </ul>
+ * During the load phase:
+ * <p>
+ * <ul>
+ * <li>The incoming (upstream) operator provides a series of batches.</li>
+ * <li>This operator sorts each batch, and accumulates them in an in-memory
+ * buffer.</li>
+ * <li>If the in-memory buffer becomes too large, this operator selects
+ * a subset of the buffered batches to spill.</li>
+ * <li>Each spill set is merged to create a new, sorted collection of
+ * batches, and each is spilled to disk.</li>
+ * <li>To allow the use of multiple disk storage, each spill group is written
+ * round-robin to a set of spill directories.</li>
+ * </ul>
+ * <p>
+ * Data is spilled to disk as a ""run"". A run consists of one or more (typically
+ * many) batches, each of which is itself a sorted run of records.
+ * <p>
+ * During the sort/merge phase:
+ * <p>
+ * <ul>
+ * <li>When the input operator is complete, this operator merges the accumulated
+ * batches (which may be all in memory or partially on disk), and returns
+ * them to the output (downstream) operator in chunks of no more than
+ * 64K records.</li>
+ * <li>The final merge must combine a collection of in-memory and spilled
+ * batches. Several limits apply to the maximum ""width"" of this merge. For
+ * example, each open spill run consumes a file handle, and we may wish
+ * to limit the number of file handles. Further, memory must hold one batch
+ * from each run, so we may need to reduce the number of runs so that the
+ * remaining runs can fit into memory. A consolidation phase combines
+ * in-memory and spilled batches prior to the final merge to control final
+ * merge width.</li>
+ * <li>A special case occurs if no batches were spilled. In this case, the input
+ * batches are sorted in memory without merging.</li>
+ * </ul>
+ * <p>
+ * Many complex details are involved in doing the above; the details are explained
+ * in the methods of this class.
+ * <p>
+ * <h4>Configuration Options</h4>
+ * <dl>
+ * <dt>drill.exec.sort.external.spill.fs</dt>
+ * <dd>The file system (file://, hdfs://, etc.) of the spill directory.</dd>
+ * <dt>drill.exec.sort.external.spill.directories</dt>
+ * <dd>The comma delimited list of directories, on the above file
+ * system, to which to spill files in round-robin fashion. The query will
+ * fail if any one of the directories becomes full.</dt>
+ * <dt>drill.exec.sort.external.spill.file_size</dt>
+ * <dd>Target size for first-generation spill files Set this to large
+ * enough to get nice long writes, but not so large that spill directories
+ * are overwhelmed.</dd>
+ * <dt>drill.exec.sort.external.mem_limit</dt>
+ * <dd>Maximum memory to use for the in-memory buffer. (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.batch_limit</dt>
+ * <dd>Maximum number of batches to hold in memory. (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.spill.max_count</dt>
+ * <dd>Maximum number of batches to add to ""first generation"" files.
+ * Defaults to 0 (no limit). (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.spill.min_count</dt>
+ * <dd>Minimum number of batches to add to ""first generation"" files.
+ * Defaults to 0 (no limit). (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.merge_limit</dt>
+ * <dd>Sets the maximum number of runs to be merged in a single pass (limits
+ * the number of open files.)</dd>
+ * </dl>
+ * <p>
+ * The memory limit observed by this operator is the lesser of:
+ * <ul>
+ * <li>The maximum allocation allowed the allocator assigned to this batch
+ * as set by the Foreman, or</li>
+ * <li>The maximum limit configured in the mem_limit parameter above. (Primarily for
+ * testing.</li>
+ * </ul>
+ * <h4>Output</h4>
+ * It is helpful to note that the sort operator will produce one of two kinds of
+ * output batches.
+ * <ul>
+ * <li>A large output with sv4 if data is sorted in memory. The sv4 addresses
+ * the entire in-memory sort set. A selection vector remover will copy results
+ * into new batches of a size determined by that operator.</li>
+ * <li>A series of batches, without a selection vector, if the sort spills to
+ * disk. In this case, the downstream operator will still be a selection vector
+ * remover, but there is nothing for that operator to remove.
+ * </ul>
+ * Note that, even in the in-memory sort case, this operator could do the copying
+ * to eliminate the extra selection vector remover. That is left as an exercise
+ * for another time.
+ * <h4>Logging</h4>
+ * Logging in this operator serves two purposes:
+ * <li>
+ * <ul>
+ * <li>Normal diagnostic information.</li>
+ * <li>Capturing the essence of the operator functionality for analysis in unit
+ * tests.</li>
+ * </ul>
+ * Test logging is designed to capture key events and timings. Take care
+ * when changing or removing log messages as you may need to adjust unit tests
+ * accordingly.
+ */
 
 public class ExternalSortBatch extends AbstractRecordBatch<ExternalSort> {
-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ExternalSortBatch.class);
-  private static final ControlsInjector injector = ControlsInjectorFactory.getInjector(ExternalSortBatch.class);
-
-  private static final GeneratorMapping COPIER_MAPPING = new GeneratorMapping(""doSetup"", ""doCopy"", null, null);
-  private final MappingSet MAIN_MAPPING = new MappingSet( (String) null, null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet LEFT_MAPPING = new MappingSet(""leftIndex"", null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet RIGHT_MAPPING = new MappingSet(""rightIndex"", null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet COPIER_MAPPING_SET = new MappingSet(COPIER_MAPPING, COPIER_MAPPING);
-
-  private final int SPILL_BATCH_GROUP_SIZE;
-  private final int SPILL_THRESHOLD;
-  private final Iterator<String> dirs;
+  static final Logger logger = LoggerFactory.getLogger(ExternalSortBatch.class);
+
+  // For backward compatibility, masquerade as the original
+  // external sort. Else, some tests don't pass.
+
+  protected static final ControlsInjector injector =
+      ControlsInjectorFactory.getInjector(org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.class);
+
+  public static final String INTERRUPTION_AFTER_SORT = ""after-sort"";
+  public static final String INTERRUPTION_AFTER_SETUP = ""after-setup"";
+  public static final String INTERRUPTION_WHILE_SPILLING = ""spilling"";
+  public static final String INTERRUPTION_WHILE_MERGING = ""merging"";","[{'comment': 'please move `public` above `protected`. ', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -17,250 +17,255 @@
  */
 package org.apache.drill.exec.physical.impl.xsort;
 
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
-import org.apache.calcite.rel.RelFieldCollation.Direction;
-import org.apache.drill.common.AutoCloseables;
-import org.apache.drill.common.config.DrillConfig;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.EMIT;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.NONE;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.OK;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.OK_NEW_SCHEMA;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.STOP;
+
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.expression.ErrorCollector;
-import org.apache.drill.common.expression.ErrorCollectorImpl;
-import org.apache.drill.common.expression.LogicalExpression;
-import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.common.logical.data.Order.Ordering;
-import org.apache.drill.exec.ExecConstants;
-import org.apache.drill.exec.compile.sig.GeneratorMapping;
-import org.apache.drill.exec.compile.sig.MappingSet;
-import org.apache.drill.exec.exception.ClassTransformationException;
-import org.apache.drill.exec.exception.OutOfMemoryException;
-import org.apache.drill.exec.exception.SchemaChangeException;
-import org.apache.drill.exec.expr.ClassGenerator;
-import org.apache.drill.exec.expr.ClassGenerator.HoldingContainer;
-import org.apache.drill.exec.expr.CodeGenerator;
-import org.apache.drill.exec.expr.ExpressionTreeMaterializer;
-import org.apache.drill.exec.expr.TypeHelper;
-import org.apache.drill.exec.expr.fn.FunctionGenerationHelper;
-import org.apache.drill.exec.memory.BufferAllocator;
 import org.apache.drill.exec.ops.FragmentContext;
 import org.apache.drill.exec.ops.MetricDef;
 import org.apache.drill.exec.physical.config.ExternalSort;
-import org.apache.drill.exec.physical.impl.sort.RecordBatchData;
-import org.apache.drill.exec.physical.impl.sort.SortRecordBatchBuilder;
-import org.apache.drill.exec.proto.ExecProtos.FragmentHandle;
-import org.apache.drill.exec.proto.helper.QueryIdHelper;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
+import org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator;
+import org.apache.drill.exec.physical.impl.xsort.SortImpl.SortResults;
 import org.apache.drill.exec.record.AbstractRecordBatch;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
-import org.apache.drill.exec.record.MaterializedField;
 import org.apache.drill.exec.record.RecordBatch;
 import org.apache.drill.exec.record.SchemaUtil;
-import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
 import org.apache.drill.exec.testing.ControlsInjector;
 import org.apache.drill.exec.testing.ControlsInjectorFactory;
-import org.apache.drill.exec.vector.CopyUtil;
 import org.apache.drill.exec.vector.ValueVector;
 import org.apache.drill.exec.vector.complex.AbstractContainerVector;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.apache.drill.shaded.guava.com.google.common.base.Joiner;
-import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
-import org.apache.drill.shaded.guava.com.google.common.collect.Iterators;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
-import com.sun.codemodel.JConditional;
-import com.sun.codemodel.JExpr;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * External sort batch: a sort batch which can spill to disk in
+ * order to operate within a defined memory footprint.
+ * <p>
+ * <h4>Basic Operation</h4>
+ * The operator has three key phases:
+ * <p>
+ * <ul>
+ * <li>The load phase in which batches are read from upstream.</li>
+ * <li>The merge phase in which spilled batches are combined to
+ * reduce the number of files below the configured limit. (Best
+ * practice is to configure the system to avoid this phase.)
+ * <li>The delivery phase in which batches are combined to produce
+ * the final output.</li>
+ * </ul>
+ * During the load phase:
+ * <p>
+ * <ul>
+ * <li>The incoming (upstream) operator provides a series of batches.</li>
+ * <li>This operator sorts each batch, and accumulates them in an in-memory
+ * buffer.</li>
+ * <li>If the in-memory buffer becomes too large, this operator selects
+ * a subset of the buffered batches to spill.</li>
+ * <li>Each spill set is merged to create a new, sorted collection of
+ * batches, and each is spilled to disk.</li>
+ * <li>To allow the use of multiple disk storage, each spill group is written
+ * round-robin to a set of spill directories.</li>
+ * </ul>
+ * <p>
+ * Data is spilled to disk as a ""run"". A run consists of one or more (typically
+ * many) batches, each of which is itself a sorted run of records.
+ * <p>
+ * During the sort/merge phase:
+ * <p>
+ * <ul>
+ * <li>When the input operator is complete, this operator merges the accumulated
+ * batches (which may be all in memory or partially on disk), and returns
+ * them to the output (downstream) operator in chunks of no more than
+ * 64K records.</li>
+ * <li>The final merge must combine a collection of in-memory and spilled
+ * batches. Several limits apply to the maximum ""width"" of this merge. For
+ * example, each open spill run consumes a file handle, and we may wish
+ * to limit the number of file handles. Further, memory must hold one batch
+ * from each run, so we may need to reduce the number of runs so that the
+ * remaining runs can fit into memory. A consolidation phase combines
+ * in-memory and spilled batches prior to the final merge to control final
+ * merge width.</li>
+ * <li>A special case occurs if no batches were spilled. In this case, the input
+ * batches are sorted in memory without merging.</li>
+ * </ul>
+ * <p>
+ * Many complex details are involved in doing the above; the details are explained
+ * in the methods of this class.
+ * <p>
+ * <h4>Configuration Options</h4>
+ * <dl>
+ * <dt>drill.exec.sort.external.spill.fs</dt>
+ * <dd>The file system (file://, hdfs://, etc.) of the spill directory.</dd>
+ * <dt>drill.exec.sort.external.spill.directories</dt>
+ * <dd>The comma delimited list of directories, on the above file
+ * system, to which to spill files in round-robin fashion. The query will
+ * fail if any one of the directories becomes full.</dt>
+ * <dt>drill.exec.sort.external.spill.file_size</dt>
+ * <dd>Target size for first-generation spill files Set this to large
+ * enough to get nice long writes, but not so large that spill directories
+ * are overwhelmed.</dd>
+ * <dt>drill.exec.sort.external.mem_limit</dt>
+ * <dd>Maximum memory to use for the in-memory buffer. (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.batch_limit</dt>
+ * <dd>Maximum number of batches to hold in memory. (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.spill.max_count</dt>
+ * <dd>Maximum number of batches to add to ""first generation"" files.
+ * Defaults to 0 (no limit). (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.spill.min_count</dt>
+ * <dd>Minimum number of batches to add to ""first generation"" files.
+ * Defaults to 0 (no limit). (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.merge_limit</dt>
+ * <dd>Sets the maximum number of runs to be merged in a single pass (limits
+ * the number of open files.)</dd>
+ * </dl>
+ * <p>
+ * The memory limit observed by this operator is the lesser of:
+ * <ul>
+ * <li>The maximum allocation allowed the allocator assigned to this batch
+ * as set by the Foreman, or</li>
+ * <li>The maximum limit configured in the mem_limit parameter above. (Primarily for
+ * testing.</li>
+ * </ul>
+ * <h4>Output</h4>
+ * It is helpful to note that the sort operator will produce one of two kinds of
+ * output batches.
+ * <ul>
+ * <li>A large output with sv4 if data is sorted in memory. The sv4 addresses
+ * the entire in-memory sort set. A selection vector remover will copy results
+ * into new batches of a size determined by that operator.</li>
+ * <li>A series of batches, without a selection vector, if the sort spills to
+ * disk. In this case, the downstream operator will still be a selection vector
+ * remover, but there is nothing for that operator to remove.
+ * </ul>
+ * Note that, even in the in-memory sort case, this operator could do the copying
+ * to eliminate the extra selection vector remover. That is left as an exercise
+ * for another time.
+ * <h4>Logging</h4>
+ * Logging in this operator serves two purposes:
+ * <li>
+ * <ul>
+ * <li>Normal diagnostic information.</li>
+ * <li>Capturing the essence of the operator functionality for analysis in unit
+ * tests.</li>
+ * </ul>
+ * Test logging is designed to capture key events and timings. Take care
+ * when changing or removing log messages as you may need to adjust unit tests
+ * accordingly.
+ */
 
 public class ExternalSortBatch extends AbstractRecordBatch<ExternalSort> {
-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ExternalSortBatch.class);
-  private static final ControlsInjector injector = ControlsInjectorFactory.getInjector(ExternalSortBatch.class);
-
-  private static final GeneratorMapping COPIER_MAPPING = new GeneratorMapping(""doSetup"", ""doCopy"", null, null);
-  private final MappingSet MAIN_MAPPING = new MappingSet( (String) null, null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet LEFT_MAPPING = new MappingSet(""leftIndex"", null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet RIGHT_MAPPING = new MappingSet(""rightIndex"", null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet COPIER_MAPPING_SET = new MappingSet(COPIER_MAPPING, COPIER_MAPPING);
-
-  private final int SPILL_BATCH_GROUP_SIZE;
-  private final int SPILL_THRESHOLD;
-  private final Iterator<String> dirs;
+  static final Logger logger = LoggerFactory.getLogger(ExternalSortBatch.class);
+
+  // For backward compatibility, masquerade as the original
+  // external sort. Else, some tests don't pass.
+
+  protected static final ControlsInjector injector =
+      ControlsInjectorFactory.getInjector(org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.class);
+
+  public static final String INTERRUPTION_AFTER_SORT = ""after-sort"";
+  public static final String INTERRUPTION_AFTER_SETUP = ""after-setup"";
+  public static final String INTERRUPTION_WHILE_SPILLING = ""spilling"";
+  public static final String INTERRUPTION_WHILE_MERGING = ""merging"";
+  private boolean retainInMemoryBatchesOnNone;
+
   private final RecordBatch incoming;
-  private final BufferAllocator oAllocator;
-  private final BufferAllocator copierAllocator;
 
-  private BatchSchema schema;
-  private SingleBatchSorter sorter;
-  private SortRecordBatchBuilder builder;
-  private MSorter mSorter;
   /**
-   * A single PriorityQueueCopier instance is used for 2 purposes:
-   * 1. Merge sorted batches before spilling
-   * 2. Merge sorted batches when all incoming data fits in memory
+   * Schema of batches that this operator produces.
    */
-  private PriorityQueueCopier copier;
-  private LinkedList<BatchGroup> batchGroups = Lists.newLinkedList();
-  private LinkedList<BatchGroup> spilledBatchGroups = Lists.newLinkedList();
-  private SelectionVector4 sv4;
-  private FileSystem fs;
-  private int spillCount = 0;
-  private int batchesSinceLastSpill = 0;
-  private boolean first = true;
-  private int targetRecordCount;
-  private final String fileName;
-  private Set<Path> currSpillDirs = Sets.newTreeSet();
-  private int firstSpillBatchCount = 0;
-  private int peakNumBatches = -1;
+
+  private BatchSchema schema;
 
   /**
-   * The copier uses the COPIER_BATCH_MEM_LIMIT to estimate the target
-   * number of records to return in each batch.
+   * Iterates over the final, sorted results.
    */
-  private static final int COPIER_BATCH_MEM_LIMIT = 256 * 1024;
 
-  public static final String INTERRUPTION_AFTER_SORT = ""after-sort"";
-  public static final String INTERRUPTION_AFTER_SETUP = ""after-setup"";
-  public static final String INTERRUPTION_WHILE_SPILLING = ""spilling"";
+  private SortResults resultsIterator;
+  private enum SortState { START, LOAD, DELIVER, DONE }","[{'comment': 'please move down and rewrite to align with existing ```Metric```, so the code will be written in the same style.  ', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -17,250 +17,255 @@
  */
 package org.apache.drill.exec.physical.impl.xsort;
 
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
-import org.apache.calcite.rel.RelFieldCollation.Direction;
-import org.apache.drill.common.AutoCloseables;
-import org.apache.drill.common.config.DrillConfig;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.EMIT;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.NONE;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.OK;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.OK_NEW_SCHEMA;
+import static org.apache.drill.exec.record.RecordBatch.IterOutcome.STOP;
+
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.expression.ErrorCollector;
-import org.apache.drill.common.expression.ErrorCollectorImpl;
-import org.apache.drill.common.expression.LogicalExpression;
-import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.common.logical.data.Order.Ordering;
-import org.apache.drill.exec.ExecConstants;
-import org.apache.drill.exec.compile.sig.GeneratorMapping;
-import org.apache.drill.exec.compile.sig.MappingSet;
-import org.apache.drill.exec.exception.ClassTransformationException;
-import org.apache.drill.exec.exception.OutOfMemoryException;
-import org.apache.drill.exec.exception.SchemaChangeException;
-import org.apache.drill.exec.expr.ClassGenerator;
-import org.apache.drill.exec.expr.ClassGenerator.HoldingContainer;
-import org.apache.drill.exec.expr.CodeGenerator;
-import org.apache.drill.exec.expr.ExpressionTreeMaterializer;
-import org.apache.drill.exec.expr.TypeHelper;
-import org.apache.drill.exec.expr.fn.FunctionGenerationHelper;
-import org.apache.drill.exec.memory.BufferAllocator;
 import org.apache.drill.exec.ops.FragmentContext;
 import org.apache.drill.exec.ops.MetricDef;
 import org.apache.drill.exec.physical.config.ExternalSort;
-import org.apache.drill.exec.physical.impl.sort.RecordBatchData;
-import org.apache.drill.exec.physical.impl.sort.SortRecordBatchBuilder;
-import org.apache.drill.exec.proto.ExecProtos.FragmentHandle;
-import org.apache.drill.exec.proto.helper.QueryIdHelper;
+import org.apache.drill.exec.physical.impl.spill.SpillSet;
+import org.apache.drill.exec.physical.impl.validate.IteratorValidatorBatchIterator;
+import org.apache.drill.exec.physical.impl.xsort.SortImpl.SortResults;
 import org.apache.drill.exec.record.AbstractRecordBatch;
 import org.apache.drill.exec.record.BatchSchema;
 import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
-import org.apache.drill.exec.record.MaterializedField;
 import org.apache.drill.exec.record.RecordBatch;
 import org.apache.drill.exec.record.SchemaUtil;
-import org.apache.drill.exec.record.VectorAccessible;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.record.WritableBatch;
 import org.apache.drill.exec.record.selection.SelectionVector2;
 import org.apache.drill.exec.record.selection.SelectionVector4;
 import org.apache.drill.exec.testing.ControlsInjector;
 import org.apache.drill.exec.testing.ControlsInjectorFactory;
-import org.apache.drill.exec.vector.CopyUtil;
 import org.apache.drill.exec.vector.ValueVector;
 import org.apache.drill.exec.vector.complex.AbstractContainerVector;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.apache.drill.shaded.guava.com.google.common.base.Joiner;
-import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
-import org.apache.drill.shaded.guava.com.google.common.collect.Iterators;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
-import com.sun.codemodel.JConditional;
-import com.sun.codemodel.JExpr;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * External sort batch: a sort batch which can spill to disk in
+ * order to operate within a defined memory footprint.
+ * <p>
+ * <h4>Basic Operation</h4>
+ * The operator has three key phases:
+ * <p>
+ * <ul>
+ * <li>The load phase in which batches are read from upstream.</li>
+ * <li>The merge phase in which spilled batches are combined to
+ * reduce the number of files below the configured limit. (Best
+ * practice is to configure the system to avoid this phase.)
+ * <li>The delivery phase in which batches are combined to produce
+ * the final output.</li>
+ * </ul>
+ * During the load phase:
+ * <p>
+ * <ul>
+ * <li>The incoming (upstream) operator provides a series of batches.</li>
+ * <li>This operator sorts each batch, and accumulates them in an in-memory
+ * buffer.</li>
+ * <li>If the in-memory buffer becomes too large, this operator selects
+ * a subset of the buffered batches to spill.</li>
+ * <li>Each spill set is merged to create a new, sorted collection of
+ * batches, and each is spilled to disk.</li>
+ * <li>To allow the use of multiple disk storage, each spill group is written
+ * round-robin to a set of spill directories.</li>
+ * </ul>
+ * <p>
+ * Data is spilled to disk as a ""run"". A run consists of one or more (typically
+ * many) batches, each of which is itself a sorted run of records.
+ * <p>
+ * During the sort/merge phase:
+ * <p>
+ * <ul>
+ * <li>When the input operator is complete, this operator merges the accumulated
+ * batches (which may be all in memory or partially on disk), and returns
+ * them to the output (downstream) operator in chunks of no more than
+ * 64K records.</li>
+ * <li>The final merge must combine a collection of in-memory and spilled
+ * batches. Several limits apply to the maximum ""width"" of this merge. For
+ * example, each open spill run consumes a file handle, and we may wish
+ * to limit the number of file handles. Further, memory must hold one batch
+ * from each run, so we may need to reduce the number of runs so that the
+ * remaining runs can fit into memory. A consolidation phase combines
+ * in-memory and spilled batches prior to the final merge to control final
+ * merge width.</li>
+ * <li>A special case occurs if no batches were spilled. In this case, the input
+ * batches are sorted in memory without merging.</li>
+ * </ul>
+ * <p>
+ * Many complex details are involved in doing the above; the details are explained
+ * in the methods of this class.
+ * <p>
+ * <h4>Configuration Options</h4>
+ * <dl>
+ * <dt>drill.exec.sort.external.spill.fs</dt>
+ * <dd>The file system (file://, hdfs://, etc.) of the spill directory.</dd>
+ * <dt>drill.exec.sort.external.spill.directories</dt>
+ * <dd>The comma delimited list of directories, on the above file
+ * system, to which to spill files in round-robin fashion. The query will
+ * fail if any one of the directories becomes full.</dt>
+ * <dt>drill.exec.sort.external.spill.file_size</dt>
+ * <dd>Target size for first-generation spill files Set this to large
+ * enough to get nice long writes, but not so large that spill directories
+ * are overwhelmed.</dd>
+ * <dt>drill.exec.sort.external.mem_limit</dt>
+ * <dd>Maximum memory to use for the in-memory buffer. (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.batch_limit</dt>
+ * <dd>Maximum number of batches to hold in memory. (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.spill.max_count</dt>
+ * <dd>Maximum number of batches to add to ""first generation"" files.
+ * Defaults to 0 (no limit). (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.spill.min_count</dt>
+ * <dd>Minimum number of batches to add to ""first generation"" files.
+ * Defaults to 0 (no limit). (Primarily for testing.)</dd>
+ * <dt>drill.exec.sort.external.merge_limit</dt>
+ * <dd>Sets the maximum number of runs to be merged in a single pass (limits
+ * the number of open files.)</dd>
+ * </dl>
+ * <p>
+ * The memory limit observed by this operator is the lesser of:
+ * <ul>
+ * <li>The maximum allocation allowed the allocator assigned to this batch
+ * as set by the Foreman, or</li>
+ * <li>The maximum limit configured in the mem_limit parameter above. (Primarily for
+ * testing.</li>
+ * </ul>
+ * <h4>Output</h4>
+ * It is helpful to note that the sort operator will produce one of two kinds of
+ * output batches.
+ * <ul>
+ * <li>A large output with sv4 if data is sorted in memory. The sv4 addresses
+ * the entire in-memory sort set. A selection vector remover will copy results
+ * into new batches of a size determined by that operator.</li>
+ * <li>A series of batches, without a selection vector, if the sort spills to
+ * disk. In this case, the downstream operator will still be a selection vector
+ * remover, but there is nothing for that operator to remove.
+ * </ul>
+ * Note that, even in the in-memory sort case, this operator could do the copying
+ * to eliminate the extra selection vector remover. That is left as an exercise
+ * for another time.
+ * <h4>Logging</h4>
+ * Logging in this operator serves two purposes:
+ * <li>
+ * <ul>
+ * <li>Normal diagnostic information.</li>
+ * <li>Capturing the essence of the operator functionality for analysis in unit
+ * tests.</li>
+ * </ul>
+ * Test logging is designed to capture key events and timings. Take care
+ * when changing or removing log messages as you may need to adjust unit tests
+ * accordingly.
+ */
 
 public class ExternalSortBatch extends AbstractRecordBatch<ExternalSort> {
-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ExternalSortBatch.class);
-  private static final ControlsInjector injector = ControlsInjectorFactory.getInjector(ExternalSortBatch.class);
-
-  private static final GeneratorMapping COPIER_MAPPING = new GeneratorMapping(""doSetup"", ""doCopy"", null, null);
-  private final MappingSet MAIN_MAPPING = new MappingSet( (String) null, null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet LEFT_MAPPING = new MappingSet(""leftIndex"", null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet RIGHT_MAPPING = new MappingSet(""rightIndex"", null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);
-  private final MappingSet COPIER_MAPPING_SET = new MappingSet(COPIER_MAPPING, COPIER_MAPPING);
-
-  private final int SPILL_BATCH_GROUP_SIZE;
-  private final int SPILL_THRESHOLD;
-  private final Iterator<String> dirs;
+  static final Logger logger = LoggerFactory.getLogger(ExternalSortBatch.class);
+
+  // For backward compatibility, masquerade as the original
+  // external sort. Else, some tests don't pass.
+
+  protected static final ControlsInjector injector =
+      ControlsInjectorFactory.getInjector(org.apache.drill.exec.physical.impl.xsort.ExternalSortBatch.class);
+
+  public static final String INTERRUPTION_AFTER_SORT = ""after-sort"";
+  public static final String INTERRUPTION_AFTER_SETUP = ""after-setup"";
+  public static final String INTERRUPTION_WHILE_SPILLING = ""spilling"";
+  public static final String INTERRUPTION_WHILE_MERGING = ""merging"";
+  private boolean retainInMemoryBatchesOnNone;
+
   private final RecordBatch incoming;
-  private final BufferAllocator oAllocator;
-  private final BufferAllocator copierAllocator;
 
-  private BatchSchema schema;
-  private SingleBatchSorter sorter;
-  private SortRecordBatchBuilder builder;
-  private MSorter mSorter;
   /**
-   * A single PriorityQueueCopier instance is used for 2 purposes:
-   * 1. Merge sorted batches before spilling
-   * 2. Merge sorted batches when all incoming data fits in memory
+   * Schema of batches that this operator produces.
    */
-  private PriorityQueueCopier copier;
-  private LinkedList<BatchGroup> batchGroups = Lists.newLinkedList();
-  private LinkedList<BatchGroup> spilledBatchGroups = Lists.newLinkedList();
-  private SelectionVector4 sv4;
-  private FileSystem fs;
-  private int spillCount = 0;
-  private int batchesSinceLastSpill = 0;
-  private boolean first = true;
-  private int targetRecordCount;
-  private final String fileName;
-  private Set<Path> currSpillDirs = Sets.newTreeSet();
-  private int firstSpillBatchCount = 0;
-  private int peakNumBatches = -1;
+
+  private BatchSchema schema;
 
   /**
-   * The copier uses the COPIER_BATCH_MEM_LIMIT to estimate the target
-   * number of records to return in each batch.
+   * Iterates over the final, sorted results.
    */
-  private static final int COPIER_BATCH_MEM_LIMIT = 256 * 1024;
 
-  public static final String INTERRUPTION_AFTER_SORT = ""after-sort"";
-  public static final String INTERRUPTION_AFTER_SETUP = ""after-setup"";
-  public static final String INTERRUPTION_WHILE_SPILLING = ""spilling"";
+  private SortResults resultsIterator;
+  private enum SortState { START, LOAD, DELIVER, DONE }
+  private SortState sortState = SortState.START;
+
+  private final SortConfig sortConfig;
+
+  private SortImpl sortImpl;
+
+  private IterOutcome lastKnownOutcome;
+
+  private boolean firstBatchOfSchema;
+
+  private final VectorContainer outputWrapperContainer;
 
-  // Be careful here! This enum is used in TWO places! First, it is used
-  // in this code to build up metrics. Easy enough. But, it is also used
-  // in OperatorMetricRegistry to define the metrics for the
-  // operator ID defined in CoreOperatorType. As a result, the values
-  // defined here are shared between this legacy version AND the new
-  // managed version. (Though the new, managed version has its own
-  // copy of this enum.) The two enums MUST be identical.
+  private final SelectionVector4 outputSV4;
 
   public enum Metric implements MetricDef {
     SPILL_COUNT,            // number of times operator spilled to disk
-    RETIRED1,               // Was: peak value for totalSizeInMemory
+    NOT_USED,               // Was: peak value for totalSizeInMemory
                             // But operator already provides this value
     PEAK_BATCHES_IN_MEMORY, // maximum number of batches kept in memory
-    MERGE_COUNT,            // Used only by the managed version.
-    MIN_BUFFER,             // Used only by the managed version.
-    INPUT_BATCHES;          // Used only by the managed version.
+    MERGE_COUNT,            // Number of second+ generation merges
+    MIN_BUFFER,             // Minimum memory level observed in operation.
+    SPILL_MB;               // Number of MB of data spilled to disk. This
+                            // amount is first written, then later re-read.
+                            // So, disk I/O is twice this amount.
 
     @Override
     public int metricId() {
       return ordinal();
     }
   }
 
-  public ExternalSortBatch(ExternalSort popConfig, FragmentContext context, RecordBatch incoming) throws OutOfMemoryException {
+  public ExternalSortBatch(ExternalSort popConfig, FragmentContext context, RecordBatch incoming) {
     super(popConfig, context, true);
     this.incoming = incoming;
-    DrillConfig config = context.getConfig();
-    Configuration conf = new Configuration();
-    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, config.getString(ExecConstants.EXTERNAL_SORT_SPILL_FILESYSTEM));
-    try {
-      this.fs = FileSystem.get(conf);
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-    SPILL_BATCH_GROUP_SIZE = config.getInt(ExecConstants.EXTERNAL_SORT_SPILL_GROUP_SIZE);
-    SPILL_THRESHOLD = config.getInt(ExecConstants.EXTERNAL_SORT_SPILL_THRESHOLD);
-    dirs = Iterators.cycle(config.getStringList(ExecConstants.EXTERNAL_SORT_SPILL_DIRS));
-    oAllocator = oContext.getAllocator();
-    copierAllocator = oAllocator.newChildAllocator(oAllocator.getName() + "":copier"",
-        PriorityQueueCopier.INITIAL_ALLOCATION, PriorityQueueCopier.MAX_ALLOCATION);
-    FragmentHandle handle = context.getHandle();
-    fileName = String.format(""%s_majorfragment%s_minorfragment%s_operator%s"", QueryIdHelper.getQueryId(handle.getQueryId()),
-        handle.getMajorFragmentId(), handle.getMinorFragmentId(), popConfig.getOperatorId());
+    outputWrapperContainer = new VectorContainer(context.getAllocator());
+    outputSV4 = new SelectionVector4(context.getAllocator(), 0);
+    sortConfig = new SortConfig(context.getConfig(), context.getOptions());
+    oContext.setInjector(injector);
+    sortImpl = createNewSortImpl();
+
+    // The upstream operator checks on record count before we have
+    // results. Create an empty result set temporarily to handle
+    // these calls.
+
+    resultsIterator = new SortImpl.EmptyResults(outputWrapperContainer);
   }
 
   @Override
   public int getRecordCount() {
-    if (sv4 != null) {
-      return sv4.getCount();
-    }
-    return container.getRecordCount();
+    return resultsIterator.getRecordCount();
   }
 
   @Override
   public SelectionVector4 getSelectionVector4() {
-    return sv4;
-  }
-
-  private void closeBatchGroups(Collection<BatchGroup> groups) {
-    for (BatchGroup group: groups) {
-      try {
-        group.close();
-      } catch (Exception e) {
-        // collect all failure and make sure to cleanup all remaining batches
-        // Originally we would have thrown a RuntimeException that would propagate to FragmentExecutor.closeOutResources()
-        // where it would have been passed to context.fail()
-        // passing the exception directly to context.fail(e) will let the cleanup process continue instead of stopping
-        // right away, this will also make sure we collect any additional exception we may get while cleaning up
-        context.getExecutorState().fail(e);
-      }
-    }
+    // Return outputSV4 instead of resultsIterator sv4. For resultsIterator which has null SV4 outputSV4 will be empty.
+    // But Sort with EMIT outcome will ideally fail in those cases while preparing output container as it's not
+    // supported currently, like for spilling scenarios","[{'comment': 'maybe convert to javadoc? ', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 ","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 
-        builder.build(container);
-        sv4 = builder.getSv4();
-        mSorter = createNewMSorter();
-        mSorter.setup(context, oAllocator, getSelectionVector4(), this.container);
+    IterOutcome result = OK;
+    for (;;) {
+      result = loadBatch();
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes setup
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SETUP);
-        mSorter.sort(this.container);
+      // NONE/EMIT means all batches have been read at this record boundary
+      if (result == NONE || result == EMIT) {
+        break; }
 
-        // sort may have prematurely exited due to should continue returning false.
-        if (!context.getExecutorState().shouldContinue()) {
-          return IterOutcome.STOP;
-        }
+      // if result is STOP that means something went wrong.
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes sorting
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SORT);
-        sv4 = mSorter.getSV4();
+      if (result == STOP) {
+        return result; }
+    }","[{'comment': '```java\r\n    IterOutcome result;\r\n    loop: for (;;) {\r\n      switch (result = loadBatch()) {\r\n        case NONE:\r\n        case EMIT:\r\n          break loop; // all batches have been read at this record boundary\r\n        case STOP:\r\n          return STOP; // something went wrong.\r\n      }\r\n    }\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 
-        builder.build(container);
-        sv4 = builder.getSv4();
-        mSorter = createNewMSorter();
-        mSorter.setup(context, oAllocator, getSelectionVector4(), this.container);
+    IterOutcome result = OK;
+    for (;;) {
+      result = loadBatch();
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes setup
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SETUP);
-        mSorter.sort(this.container);
+      // NONE/EMIT means all batches have been read at this record boundary
+      if (result == NONE || result == EMIT) {
+        break; }
 
-        // sort may have prematurely exited due to should continue returning false.
-        if (!context.getExecutorState().shouldContinue()) {
-          return IterOutcome.STOP;
-        }
+      // if result is STOP that means something went wrong.
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes sorting
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SORT);
-        sv4 = mSorter.getSV4();
+      if (result == STOP) {
+        return result; }
+    }
 
-        container.buildSchema(SelectionVectorMode.FOUR_BYTE);
-      } else { // some batches were spilled
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        }
-        batchGroups.addAll(spilledBatchGroups);
-        spilledBatchGroups = null; // no need to cleanup spilledBatchGroups, all it's batches are in batchGroups now
-
-        logger.warn(""Starting to merge. {} batch groups. Current allocated memory: {}"", batchGroups.size(), oAllocator.getAllocatedMemory());
-        VectorContainer hyperBatch = constructHyperBatch(batchGroups);
-        createCopier(hyperBatch, batchGroups, container, false);
-
-        int estimatedRecordSize = 0;
-        for (VectorWrapper<?> w : batchGroups.get(0)) {
-          try {
-            estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-          } catch (UnsupportedOperationException e) {
-            estimatedRecordSize += 50;
-          }
-        }
-        targetRecordCount = Math.min(MAX_BATCH_ROW_COUNT, Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize));
-        int count = copier.next(targetRecordCount);
-        container.buildSchema(SelectionVectorMode.NONE);
-        container.setRecordCount(count);
+    // Anything to actually sort?
+    resultsIterator = sortImpl.startMerge();
+    if (! resultsIterator.next()) {
+      // If there is no records to sort and we got NONE then just return NONE
+      if (result == NONE) {
+        sortState = SortState.DONE;
+        return NONE;
       }
-
-      return IterOutcome.OK_NEW_SCHEMA;
-
-    } catch (SchemaChangeException ex) {
-      kill(false);
-      context.getExecutorState().fail(UserException.unsupportedError(ex)
-        .message(""Sort doesn't currently support sorts with changing schemas"").build(logger));
-      return IterOutcome.STOP;
-    } catch(ClassTransformationException | IOException ex) {
-      kill(false);
-      context.getExecutorState().fail(ex);
-      return IterOutcome.STOP;
-    } catch (UnsupportedOperationException e) {
-      throw new RuntimeException(e);
     }
-  }
-
-  private boolean hasMemoryForInMemorySort(int currentRecordCount) {
-    long currentlyAvailable =  popConfig.getMaxAllocation() - oAllocator.getAllocatedMemory();
 
-    long neededForInMemorySort = SortRecordBatchBuilder.memoryNeeded(currentRecordCount) +
-        MSortTemplate.memoryNeeded(currentRecordCount);
-
-    return currentlyAvailable > neededForInMemorySort;
-  }
+    // sort may have prematurely exited due to shouldContinue() returning false.
 
-  public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws SchemaChangeException {
-    logger.debug(""Copier allocator current allocation {}"", copierAllocator.getAllocatedMemory());
-    logger.debug(""mergeAndSpill: starting total size in memory = {}"", oAllocator.getAllocatedMemory());
-    VectorContainer outputContainer = new VectorContainer();
-    List<BatchGroup> batchGroupList = Lists.newArrayList();
-    int batchCount = batchGroups.size();
-    for (int i = 0; i < batchCount / 2; i++) {
-      if (batchGroups.size() == 0) {
-        break;
-      }
-      BatchGroup batch = batchGroups.pollLast();
-      assert batch != null : ""Encountered a null batch during merge and spill operation"";
-      batchGroupList.add(batch);
+    if (!context.getExecutorState().shouldContinue()) {
+      sortState = SortState.DONE;
+      return STOP;
     }","[{'comment': 'May be replaced by: \r\n```java\r\n    resultsIterator = sortImpl.startMerge();\r\n    if (!resultsIterator.next() && result == NONE) {\r\n      sortState = SortState.DONE;\r\n      return NONE; // no records to sort\r\n    } else if (!context.getExecutorState().shouldContinue()) {\r\n      sortState = SortState.DONE;\r\n      return STOP; // aborted\r\n    }\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 
-        builder.build(container);
-        sv4 = builder.getSv4();
-        mSorter = createNewMSorter();
-        mSorter.setup(context, oAllocator, getSelectionVector4(), this.container);
+    IterOutcome result = OK;
+    for (;;) {
+      result = loadBatch();
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes setup
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SETUP);
-        mSorter.sort(this.container);
+      // NONE/EMIT means all batches have been read at this record boundary
+      if (result == NONE || result == EMIT) {
+        break; }
 
-        // sort may have prematurely exited due to should continue returning false.
-        if (!context.getExecutorState().shouldContinue()) {
-          return IterOutcome.STOP;
-        }
+      // if result is STOP that means something went wrong.
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes sorting
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SORT);
-        sv4 = mSorter.getSV4();
+      if (result == STOP) {
+        return result; }
+    }
 
-        container.buildSchema(SelectionVectorMode.FOUR_BYTE);
-      } else { // some batches were spilled
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        }
-        batchGroups.addAll(spilledBatchGroups);
-        spilledBatchGroups = null; // no need to cleanup spilledBatchGroups, all it's batches are in batchGroups now
-
-        logger.warn(""Starting to merge. {} batch groups. Current allocated memory: {}"", batchGroups.size(), oAllocator.getAllocatedMemory());
-        VectorContainer hyperBatch = constructHyperBatch(batchGroups);
-        createCopier(hyperBatch, batchGroups, container, false);
-
-        int estimatedRecordSize = 0;
-        for (VectorWrapper<?> w : batchGroups.get(0)) {
-          try {
-            estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-          } catch (UnsupportedOperationException e) {
-            estimatedRecordSize += 50;
-          }
-        }
-        targetRecordCount = Math.min(MAX_BATCH_ROW_COUNT, Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize));
-        int count = copier.next(targetRecordCount);
-        container.buildSchema(SelectionVectorMode.NONE);
-        container.setRecordCount(count);
+    // Anything to actually sort?
+    resultsIterator = sortImpl.startMerge();
+    if (! resultsIterator.next()) {
+      // If there is no records to sort and we got NONE then just return NONE
+      if (result == NONE) {
+        sortState = SortState.DONE;
+        return NONE;
       }
-
-      return IterOutcome.OK_NEW_SCHEMA;
-
-    } catch (SchemaChangeException ex) {
-      kill(false);
-      context.getExecutorState().fail(UserException.unsupportedError(ex)
-        .message(""Sort doesn't currently support sorts with changing schemas"").build(logger));
-      return IterOutcome.STOP;
-    } catch(ClassTransformationException | IOException ex) {
-      kill(false);
-      context.getExecutorState().fail(ex);
-      return IterOutcome.STOP;
-    } catch (UnsupportedOperationException e) {
-      throw new RuntimeException(e);
     }
-  }
-
-  private boolean hasMemoryForInMemorySort(int currentRecordCount) {
-    long currentlyAvailable =  popConfig.getMaxAllocation() - oAllocator.getAllocatedMemory();
 
-    long neededForInMemorySort = SortRecordBatchBuilder.memoryNeeded(currentRecordCount) +
-        MSortTemplate.memoryNeeded(currentRecordCount);
-
-    return currentlyAvailable > neededForInMemorySort;
-  }
+    // sort may have prematurely exited due to shouldContinue() returning false.
 
-  public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws SchemaChangeException {
-    logger.debug(""Copier allocator current allocation {}"", copierAllocator.getAllocatedMemory());
-    logger.debug(""mergeAndSpill: starting total size in memory = {}"", oAllocator.getAllocatedMemory());
-    VectorContainer outputContainer = new VectorContainer();
-    List<BatchGroup> batchGroupList = Lists.newArrayList();
-    int batchCount = batchGroups.size();
-    for (int i = 0; i < batchCount / 2; i++) {
-      if (batchGroups.size() == 0) {
-        break;
-      }
-      BatchGroup batch = batchGroups.pollLast();
-      assert batch != null : ""Encountered a null batch during merge and spill operation"";
-      batchGroupList.add(batch);
+    if (!context.getExecutorState().shouldContinue()) {
+      sortState = SortState.DONE;
+      return STOP;
     }
 
-    if (batchGroupList.size() == 0) {
-      return null;
-    }
-    int estimatedRecordSize = 0;
-    for (VectorWrapper<?> w : batchGroupList.get(0)) {
-      try {
-        estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-      } catch (UnsupportedOperationException e) {
-        estimatedRecordSize += 50;
-      }
-    }
-    int targetRecordCount = Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize);
-    VectorContainer hyperBatch = constructHyperBatch(batchGroupList);
-    createCopier(hyperBatch, batchGroupList, outputContainer, true);
+    // If we are here that means there is some data to be returned downstream.
+    // We have to prepare output container
+    prepareOutputContainer(resultsIterator);
+    return getFinalOutcome();
+  }
 
-    int count = copier.next(targetRecordCount);
-    assert count > 0;
+  /**
+   * Load and process a single batch, handling schema changes. In general, the
+   * external sort accepts only one schema.
+   *
+   * @return return code depending on the amount of data read from upstream
+   */
 ","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 ","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 
-        builder.build(container);
-        sv4 = builder.getSv4();
-        mSorter = createNewMSorter();
-        mSorter.setup(context, oAllocator, getSelectionVector4(), this.container);
+    IterOutcome result = OK;
+    for (;;) {
+      result = loadBatch();
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes setup
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SETUP);
-        mSorter.sort(this.container);
+      // NONE/EMIT means all batches have been read at this record boundary
+      if (result == NONE || result == EMIT) {
+        break; }
 
-        // sort may have prematurely exited due to should continue returning false.
-        if (!context.getExecutorState().shouldContinue()) {
-          return IterOutcome.STOP;
-        }
+      // if result is STOP that means something went wrong.
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes sorting
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SORT);
-        sv4 = mSorter.getSV4();
+      if (result == STOP) {
+        return result; }
+    }
 
-        container.buildSchema(SelectionVectorMode.FOUR_BYTE);
-      } else { // some batches were spilled
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        }
-        batchGroups.addAll(spilledBatchGroups);
-        spilledBatchGroups = null; // no need to cleanup spilledBatchGroups, all it's batches are in batchGroups now
-
-        logger.warn(""Starting to merge. {} batch groups. Current allocated memory: {}"", batchGroups.size(), oAllocator.getAllocatedMemory());
-        VectorContainer hyperBatch = constructHyperBatch(batchGroups);
-        createCopier(hyperBatch, batchGroups, container, false);
-
-        int estimatedRecordSize = 0;
-        for (VectorWrapper<?> w : batchGroups.get(0)) {
-          try {
-            estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-          } catch (UnsupportedOperationException e) {
-            estimatedRecordSize += 50;
-          }
-        }
-        targetRecordCount = Math.min(MAX_BATCH_ROW_COUNT, Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize));
-        int count = copier.next(targetRecordCount);
-        container.buildSchema(SelectionVectorMode.NONE);
-        container.setRecordCount(count);
+    // Anything to actually sort?
+    resultsIterator = sortImpl.startMerge();
+    if (! resultsIterator.next()) {
+      // If there is no records to sort and we got NONE then just return NONE
+      if (result == NONE) {
+        sortState = SortState.DONE;
+        return NONE;
       }
-
-      return IterOutcome.OK_NEW_SCHEMA;
-
-    } catch (SchemaChangeException ex) {
-      kill(false);
-      context.getExecutorState().fail(UserException.unsupportedError(ex)
-        .message(""Sort doesn't currently support sorts with changing schemas"").build(logger));
-      return IterOutcome.STOP;
-    } catch(ClassTransformationException | IOException ex) {
-      kill(false);
-      context.getExecutorState().fail(ex);
-      return IterOutcome.STOP;
-    } catch (UnsupportedOperationException e) {
-      throw new RuntimeException(e);
     }
-  }
-
-  private boolean hasMemoryForInMemorySort(int currentRecordCount) {
-    long currentlyAvailable =  popConfig.getMaxAllocation() - oAllocator.getAllocatedMemory();
 
-    long neededForInMemorySort = SortRecordBatchBuilder.memoryNeeded(currentRecordCount) +
-        MSortTemplate.memoryNeeded(currentRecordCount);
-
-    return currentlyAvailable > neededForInMemorySort;
-  }
+    // sort may have prematurely exited due to shouldContinue() returning false.
 
-  public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws SchemaChangeException {
-    logger.debug(""Copier allocator current allocation {}"", copierAllocator.getAllocatedMemory());
-    logger.debug(""mergeAndSpill: starting total size in memory = {}"", oAllocator.getAllocatedMemory());
-    VectorContainer outputContainer = new VectorContainer();
-    List<BatchGroup> batchGroupList = Lists.newArrayList();
-    int batchCount = batchGroups.size();
-    for (int i = 0; i < batchCount / 2; i++) {
-      if (batchGroups.size() == 0) {
-        break;
-      }
-      BatchGroup batch = batchGroups.pollLast();
-      assert batch != null : ""Encountered a null batch during merge and spill operation"";
-      batchGroupList.add(batch);
+    if (!context.getExecutorState().shouldContinue()) {
+      sortState = SortState.DONE;
+      return STOP;
     }
 
-    if (batchGroupList.size() == 0) {
-      return null;
-    }
-    int estimatedRecordSize = 0;
-    for (VectorWrapper<?> w : batchGroupList.get(0)) {
-      try {
-        estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-      } catch (UnsupportedOperationException e) {
-        estimatedRecordSize += 50;
-      }
-    }
-    int targetRecordCount = Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize);
-    VectorContainer hyperBatch = constructHyperBatch(batchGroupList);
-    createCopier(hyperBatch, batchGroupList, outputContainer, true);
+    // If we are here that means there is some data to be returned downstream.
+    // We have to prepare output container
+    prepareOutputContainer(resultsIterator);
+    return getFinalOutcome();
+  }
 
-    int count = copier.next(targetRecordCount);
-    assert count > 0;
+  /**
+   * Load and process a single batch, handling schema changes. In general, the
+   * external sort accepts only one schema.
+   *
+   * @return return code depending on the amount of data read from upstream
+   */
 
-    logger.debug(""mergeAndSpill: estimated record size = {}, target record count = {}"", estimatedRecordSize, targetRecordCount);
+  private IterOutcome loadBatch() {
 
-    // 1 output container is kept in memory, so we want to hold on to it and transferClone
-    // allows keeping ownership
-    VectorContainer c1 = VectorContainer.getTransferClone(outputContainer, oContext);
-    c1.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-    c1.setRecordCount(count);
+    // If this is the very first batch, then AbstractRecordBatch
+    // already loaded it for us in buildSchema().
 
-    String spillDir = dirs.next();
-    Path currSpillPath = new Path(Joiner.on(""/"").join(spillDir, fileName));
-    currSpillDirs.add(currSpillPath);
-    String outputFile = Joiner.on(""/"").join(currSpillPath, spillCount++);
-    try {
-        fs.deleteOnExit(currSpillPath);
-    } catch (IOException e) {
-        // since this is meant to be used in a batches's spilling, we don't propagate the exception
-        logger.warn(""Unable to mark spill directory "" + currSpillPath + "" for deleting on exit"", e);
+    if (sortState == SortState.START) {
+      sortState = SortState.LOAD;
+      lastKnownOutcome = OK_NEW_SCHEMA;
+    } else {
+      lastKnownOutcome = next(incoming);
     }","[{'comment': '```java\r\n    if (sortState == SortState.START) {\r\n      // first batch is preloaded by AbstractRecordBatch.buildSchema()\r\n      sortState = SortState.LOAD;\r\n      lastKnownOutcome = OK_NEW_SCHEMA;\r\n    } else {\r\n      lastKnownOutcome = next(incoming);\r\n    }\r\n\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 
-        builder.build(container);
-        sv4 = builder.getSv4();
-        mSorter = createNewMSorter();
-        mSorter.setup(context, oAllocator, getSelectionVector4(), this.container);
+    IterOutcome result = OK;
+    for (;;) {
+      result = loadBatch();
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes setup
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SETUP);
-        mSorter.sort(this.container);
+      // NONE/EMIT means all batches have been read at this record boundary
+      if (result == NONE || result == EMIT) {
+        break; }
 
-        // sort may have prematurely exited due to should continue returning false.
-        if (!context.getExecutorState().shouldContinue()) {
-          return IterOutcome.STOP;
-        }
+      // if result is STOP that means something went wrong.
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes sorting
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SORT);
-        sv4 = mSorter.getSV4();
+      if (result == STOP) {
+        return result; }
+    }
 
-        container.buildSchema(SelectionVectorMode.FOUR_BYTE);
-      } else { // some batches were spilled
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        }
-        batchGroups.addAll(spilledBatchGroups);
-        spilledBatchGroups = null; // no need to cleanup spilledBatchGroups, all it's batches are in batchGroups now
-
-        logger.warn(""Starting to merge. {} batch groups. Current allocated memory: {}"", batchGroups.size(), oAllocator.getAllocatedMemory());
-        VectorContainer hyperBatch = constructHyperBatch(batchGroups);
-        createCopier(hyperBatch, batchGroups, container, false);
-
-        int estimatedRecordSize = 0;
-        for (VectorWrapper<?> w : batchGroups.get(0)) {
-          try {
-            estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-          } catch (UnsupportedOperationException e) {
-            estimatedRecordSize += 50;
-          }
-        }
-        targetRecordCount = Math.min(MAX_BATCH_ROW_COUNT, Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize));
-        int count = copier.next(targetRecordCount);
-        container.buildSchema(SelectionVectorMode.NONE);
-        container.setRecordCount(count);
+    // Anything to actually sort?
+    resultsIterator = sortImpl.startMerge();
+    if (! resultsIterator.next()) {
+      // If there is no records to sort and we got NONE then just return NONE
+      if (result == NONE) {
+        sortState = SortState.DONE;
+        return NONE;
       }
-
-      return IterOutcome.OK_NEW_SCHEMA;
-
-    } catch (SchemaChangeException ex) {
-      kill(false);
-      context.getExecutorState().fail(UserException.unsupportedError(ex)
-        .message(""Sort doesn't currently support sorts with changing schemas"").build(logger));
-      return IterOutcome.STOP;
-    } catch(ClassTransformationException | IOException ex) {
-      kill(false);
-      context.getExecutorState().fail(ex);
-      return IterOutcome.STOP;
-    } catch (UnsupportedOperationException e) {
-      throw new RuntimeException(e);
     }
-  }
-
-  private boolean hasMemoryForInMemorySort(int currentRecordCount) {
-    long currentlyAvailable =  popConfig.getMaxAllocation() - oAllocator.getAllocatedMemory();
 
-    long neededForInMemorySort = SortRecordBatchBuilder.memoryNeeded(currentRecordCount) +
-        MSortTemplate.memoryNeeded(currentRecordCount);
-
-    return currentlyAvailable > neededForInMemorySort;
-  }
+    // sort may have prematurely exited due to shouldContinue() returning false.
 
-  public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws SchemaChangeException {
-    logger.debug(""Copier allocator current allocation {}"", copierAllocator.getAllocatedMemory());
-    logger.debug(""mergeAndSpill: starting total size in memory = {}"", oAllocator.getAllocatedMemory());
-    VectorContainer outputContainer = new VectorContainer();
-    List<BatchGroup> batchGroupList = Lists.newArrayList();
-    int batchCount = batchGroups.size();
-    for (int i = 0; i < batchCount / 2; i++) {
-      if (batchGroups.size() == 0) {
-        break;
-      }
-      BatchGroup batch = batchGroups.pollLast();
-      assert batch != null : ""Encountered a null batch during merge and spill operation"";
-      batchGroupList.add(batch);
+    if (!context.getExecutorState().shouldContinue()) {
+      sortState = SortState.DONE;
+      return STOP;
     }
 
-    if (batchGroupList.size() == 0) {
-      return null;
-    }
-    int estimatedRecordSize = 0;
-    for (VectorWrapper<?> w : batchGroupList.get(0)) {
-      try {
-        estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-      } catch (UnsupportedOperationException e) {
-        estimatedRecordSize += 50;
-      }
-    }
-    int targetRecordCount = Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize);
-    VectorContainer hyperBatch = constructHyperBatch(batchGroupList);
-    createCopier(hyperBatch, batchGroupList, outputContainer, true);
+    // If we are here that means there is some data to be returned downstream.
+    // We have to prepare output container
+    prepareOutputContainer(resultsIterator);
+    return getFinalOutcome();
+  }
 
-    int count = copier.next(targetRecordCount);
-    assert count > 0;
+  /**
+   * Load and process a single batch, handling schema changes. In general, the
+   * external sort accepts only one schema.
+   *
+   * @return return code depending on the amount of data read from upstream
+   */
 
-    logger.debug(""mergeAndSpill: estimated record size = {}, target record count = {}"", estimatedRecordSize, targetRecordCount);
+  private IterOutcome loadBatch() {
 
-    // 1 output container is kept in memory, so we want to hold on to it and transferClone
-    // allows keeping ownership
-    VectorContainer c1 = VectorContainer.getTransferClone(outputContainer, oContext);
-    c1.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-    c1.setRecordCount(count);
+    // If this is the very first batch, then AbstractRecordBatch
+    // already loaded it for us in buildSchema().
 
-    String spillDir = dirs.next();
-    Path currSpillPath = new Path(Joiner.on(""/"").join(spillDir, fileName));
-    currSpillDirs.add(currSpillPath);
-    String outputFile = Joiner.on(""/"").join(currSpillPath, spillCount++);
-    try {
-        fs.deleteOnExit(currSpillPath);
-    } catch (IOException e) {
-        // since this is meant to be used in a batches's spilling, we don't propagate the exception
-        logger.warn(""Unable to mark spill directory "" + currSpillPath + "" for deleting on exit"", e);
+    if (sortState == SortState.START) {
+      sortState = SortState.LOAD;
+      lastKnownOutcome = OK_NEW_SCHEMA;
+    } else {
+      lastKnownOutcome = next(incoming);
     }
-    stats.setLongStat(Metric.SPILL_COUNT, spillCount);
-    BatchGroup newGroup = new BatchGroup(c1, fs, outputFile, oContext);
-    try (AutoCloseable a = AutoCloseables.all(batchGroupList)) {
-      logger.info(""Merging and spilling to {}"", outputFile);
-      while ((count = copier.next(targetRecordCount)) > 0) {
-        outputContainer.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-        outputContainer.setRecordCount(count);
-        // note that addBatch also clears the outputContainer
-        newGroup.addBatch(outputContainer);
+    switch (lastKnownOutcome) {","[{'comment': 'Please adjust indents and remove empty lines for the `switch-case`: \r\n```java\r\n    switch (lastKnownOutcome) {\r\n      case NONE:\r\n      case STOP:\r\n        return lastKnownOutcome;\r\n      case OK_NEW_SCHEMA:\r\n        firstBatchOfSchema = true;\r\n        setupSchema();\r\n        // Fall through\r\n      case OK:\r\n      case EMIT:\r\n        // Add the batch to the in-memory generation, spilling if needed.\r\n        sortImpl.addBatch(incoming);\r\n        break;\r\n      case OUT_OF_MEMORY:\r\n        // Note: it is highly doubtful that this code actually works. It\r\n        // requires that the upstream batches got to a safe place to run\r\n        // out of memory and that no work was in-flight and thus abandoned.\r\n        // Consider removing this case once resource management is in place.\r\n        logger.error(""received OUT_OF_MEMORY, trying to spill"");\r\n        if (!sortImpl.forceSpill()) {\r\n          throw UserException.memoryError(""Received OUT_OF_MEMORY, but not enough batches to spill"")\r\n              .build(logger);\r\n        }\r\n        break;\r\n      default:\r\n        throw new IllegalStateException(""Unexpected iter outcome: "" + lastKnownOutcome);\r\n    }\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 
-        builder.build(container);
-        sv4 = builder.getSv4();
-        mSorter = createNewMSorter();
-        mSorter.setup(context, oAllocator, getSelectionVector4(), this.container);
+    IterOutcome result = OK;
+    for (;;) {
+      result = loadBatch();
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes setup
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SETUP);
-        mSorter.sort(this.container);
+      // NONE/EMIT means all batches have been read at this record boundary
+      if (result == NONE || result == EMIT) {
+        break; }
 
-        // sort may have prematurely exited due to should continue returning false.
-        if (!context.getExecutorState().shouldContinue()) {
-          return IterOutcome.STOP;
-        }
+      // if result is STOP that means something went wrong.
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes sorting
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SORT);
-        sv4 = mSorter.getSV4();
+      if (result == STOP) {
+        return result; }
+    }
 
-        container.buildSchema(SelectionVectorMode.FOUR_BYTE);
-      } else { // some batches were spilled
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        }
-        batchGroups.addAll(spilledBatchGroups);
-        spilledBatchGroups = null; // no need to cleanup spilledBatchGroups, all it's batches are in batchGroups now
-
-        logger.warn(""Starting to merge. {} batch groups. Current allocated memory: {}"", batchGroups.size(), oAllocator.getAllocatedMemory());
-        VectorContainer hyperBatch = constructHyperBatch(batchGroups);
-        createCopier(hyperBatch, batchGroups, container, false);
-
-        int estimatedRecordSize = 0;
-        for (VectorWrapper<?> w : batchGroups.get(0)) {
-          try {
-            estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-          } catch (UnsupportedOperationException e) {
-            estimatedRecordSize += 50;
-          }
-        }
-        targetRecordCount = Math.min(MAX_BATCH_ROW_COUNT, Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize));
-        int count = copier.next(targetRecordCount);
-        container.buildSchema(SelectionVectorMode.NONE);
-        container.setRecordCount(count);
+    // Anything to actually sort?
+    resultsIterator = sortImpl.startMerge();
+    if (! resultsIterator.next()) {
+      // If there is no records to sort and we got NONE then just return NONE
+      if (result == NONE) {
+        sortState = SortState.DONE;
+        return NONE;
       }
-
-      return IterOutcome.OK_NEW_SCHEMA;
-
-    } catch (SchemaChangeException ex) {
-      kill(false);
-      context.getExecutorState().fail(UserException.unsupportedError(ex)
-        .message(""Sort doesn't currently support sorts with changing schemas"").build(logger));
-      return IterOutcome.STOP;
-    } catch(ClassTransformationException | IOException ex) {
-      kill(false);
-      context.getExecutorState().fail(ex);
-      return IterOutcome.STOP;
-    } catch (UnsupportedOperationException e) {
-      throw new RuntimeException(e);
     }
-  }
-
-  private boolean hasMemoryForInMemorySort(int currentRecordCount) {
-    long currentlyAvailable =  popConfig.getMaxAllocation() - oAllocator.getAllocatedMemory();
 
-    long neededForInMemorySort = SortRecordBatchBuilder.memoryNeeded(currentRecordCount) +
-        MSortTemplate.memoryNeeded(currentRecordCount);
-
-    return currentlyAvailable > neededForInMemorySort;
-  }
+    // sort may have prematurely exited due to shouldContinue() returning false.
 
-  public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws SchemaChangeException {
-    logger.debug(""Copier allocator current allocation {}"", copierAllocator.getAllocatedMemory());
-    logger.debug(""mergeAndSpill: starting total size in memory = {}"", oAllocator.getAllocatedMemory());
-    VectorContainer outputContainer = new VectorContainer();
-    List<BatchGroup> batchGroupList = Lists.newArrayList();
-    int batchCount = batchGroups.size();
-    for (int i = 0; i < batchCount / 2; i++) {
-      if (batchGroups.size() == 0) {
-        break;
-      }
-      BatchGroup batch = batchGroups.pollLast();
-      assert batch != null : ""Encountered a null batch during merge and spill operation"";
-      batchGroupList.add(batch);
+    if (!context.getExecutorState().shouldContinue()) {
+      sortState = SortState.DONE;
+      return STOP;
     }
 
-    if (batchGroupList.size() == 0) {
-      return null;
-    }
-    int estimatedRecordSize = 0;
-    for (VectorWrapper<?> w : batchGroupList.get(0)) {
-      try {
-        estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-      } catch (UnsupportedOperationException e) {
-        estimatedRecordSize += 50;
-      }
-    }
-    int targetRecordCount = Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize);
-    VectorContainer hyperBatch = constructHyperBatch(batchGroupList);
-    createCopier(hyperBatch, batchGroupList, outputContainer, true);
+    // If we are here that means there is some data to be returned downstream.
+    // We have to prepare output container
+    prepareOutputContainer(resultsIterator);
+    return getFinalOutcome();
+  }
 
-    int count = copier.next(targetRecordCount);
-    assert count > 0;
+  /**
+   * Load and process a single batch, handling schema changes. In general, the
+   * external sort accepts only one schema.
+   *
+   * @return return code depending on the amount of data read from upstream
+   */
 
-    logger.debug(""mergeAndSpill: estimated record size = {}, target record count = {}"", estimatedRecordSize, targetRecordCount);
+  private IterOutcome loadBatch() {
 
-    // 1 output container is kept in memory, so we want to hold on to it and transferClone
-    // allows keeping ownership
-    VectorContainer c1 = VectorContainer.getTransferClone(outputContainer, oContext);
-    c1.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-    c1.setRecordCount(count);
+    // If this is the very first batch, then AbstractRecordBatch
+    // already loaded it for us in buildSchema().
 
-    String spillDir = dirs.next();
-    Path currSpillPath = new Path(Joiner.on(""/"").join(spillDir, fileName));
-    currSpillDirs.add(currSpillPath);
-    String outputFile = Joiner.on(""/"").join(currSpillPath, spillCount++);
-    try {
-        fs.deleteOnExit(currSpillPath);
-    } catch (IOException e) {
-        // since this is meant to be used in a batches's spilling, we don't propagate the exception
-        logger.warn(""Unable to mark spill directory "" + currSpillPath + "" for deleting on exit"", e);
+    if (sortState == SortState.START) {
+      sortState = SortState.LOAD;
+      lastKnownOutcome = OK_NEW_SCHEMA;
+    } else {
+      lastKnownOutcome = next(incoming);
     }
-    stats.setLongStat(Metric.SPILL_COUNT, spillCount);
-    BatchGroup newGroup = new BatchGroup(c1, fs, outputFile, oContext);
-    try (AutoCloseable a = AutoCloseables.all(batchGroupList)) {
-      logger.info(""Merging and spilling to {}"", outputFile);
-      while ((count = copier.next(targetRecordCount)) > 0) {
-        outputContainer.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-        outputContainer.setRecordCount(count);
-        // note that addBatch also clears the outputContainer
-        newGroup.addBatch(outputContainer);
+    switch (lastKnownOutcome) {
+    case NONE:
+    case STOP:
+      return lastKnownOutcome;
+    case OK_NEW_SCHEMA:
+      firstBatchOfSchema = true;
+      setupSchema();
+      // Fall through
+
+    case OK:
+    case EMIT:
+
+      // Add the batch to the in-memory generation, spilling if
+      // needed.
+
+      sortImpl.addBatch(incoming);
+      break;
+    case OUT_OF_MEMORY:
+
+      // Note: it is highly doubtful that this code actually works. It
+      // requires that the upstream batches got to a safe place to run
+      // out of memory and that no work was in-flight and thus abandoned.
+      // Consider removing this case once resource management is in place.
+
+      logger.error(""received OUT_OF_MEMORY, trying to spill"");
+      if (! sortImpl.forceSpill()) {
+        throw UserException.memoryError(""Received OUT_OF_MEMORY, but not enough batches to spill"")
+          .build(logger);
       }
-      injector.injectChecked(context.getExecutionControls(), INTERRUPTION_WHILE_SPILLING, IOException.class);
-      newGroup.closeOutputStream();
-    } catch (Throwable e) {
-      // we only need to cleanup newGroup if spill failed
-      try {
-        AutoCloseables.close(e, newGroup);
-      } catch (Throwable t) { /* close() may hit the same IO issue; just ignore */ }
-      throw UserException.resourceError(e)
-        .message(""External Sort encountered an error while spilling to disk"")
-              .addContext(e.getMessage() /* more detail */)
-        .build(logger);
-    } finally {
-      hyperBatch.clear();
+      break;
+    default:
+      throw new IllegalStateException(""Unexpected iter outcome: "" + lastKnownOutcome);
     }
-    logger.debug(""mergeAndSpill: final total size in memory = {}"", oAllocator.getAllocatedMemory());
-    logger.info(""Completed spilling to {}"", outputFile);
-    return newGroup;
+    return lastKnownOutcome;
   }
 
-  private SelectionVector2 newSV2() throws OutOfMemoryException, InterruptedException {
-    @SuppressWarnings(""resource"")
-    SelectionVector2 sv2 = new SelectionVector2(oAllocator);
-    if (!sv2.allocateNewSafe(incoming.getRecordCount())) {
-      try {
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        } else {
-          throw UserException.memoryError(""Unable to allocate sv2 for %d records, and not enough batchGroups to spill."",
-              incoming.getRecordCount())
-            .addContext(""batchGroups.size"", batchGroups.size())
-            .addContext(""spilledBatchGroups.size"", spilledBatchGroups.size())
-            .addContext(""allocated memory"", oAllocator.getAllocatedMemory())
-            .addContext(""allocator limit"", oAllocator.getLimit())
+  /**
+   * Handle a new schema from upstream. The ESB is quite limited in its ability
+   * to handle schema changes.
+   */
+
+  private void setupSchema()  {
+
+    // First batch: we won't have a schema.
+
+    if (schema == null) {
+      schema = incoming.getSchema();
+    } else if (incoming.getSchema().equals(schema)) {
+      // Nothing to do.  Artificial schema changes are ignored.
+    } else if (unionTypeEnabled) {
+      schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
+    } else {
+      throw UserException.unsupportedError()
+            .message(""Schema changes not supported in External Sort. Please enable Union type."")
+            .addContext(""Previous schema"", schema.toString())
+            .addContext(""Incoming schema"", incoming.getSchema().toString())
             .build(logger);
-        }
-      } catch (SchemaChangeException e) {
-        throw new RuntimeException(e);
-      }
-      int waitTime = 1;
-      while (true) {
-        try {
-          Thread.sleep(waitTime * 1000);
-        } catch(final InterruptedException e) {
-          if (!context.getExecutorState().shouldContinue()) {
-            throw e;
-          }
-        }
-        waitTime *= 2;
-        if (sv2.allocateNewSafe(incoming.getRecordCount())) {
-          break;
-        }
-        if (waitTime >= 32) {
-          throw new OutOfMemoryException(""Unable to allocate sv2 buffer after repeated attempts"");
-        }
-      }
     }
-    for (int i = 0; i < incoming.getRecordCount(); i++) {
-      sv2.setIndex(i, (char) i);
-    }
-    sv2.setRecordCount(incoming.getRecordCount());
-    return sv2;
+    sortImpl.setSchema(schema);
   }
 
-  private VectorContainer constructHyperBatch(List<BatchGroup> batchGroupList) {
-    VectorContainer cont = new VectorContainer();
-    for (MaterializedField field : schema) {
-      ValueVector[] vectors = new ValueVector[batchGroupList.size()];
-      int i = 0;
-      for (BatchGroup group : batchGroupList) {
-        vectors[i++] = group.getValueAccessorById(
-            field.getValueClass(),
-            group.getValueVectorId(SchemaPath.getSimplePath(field.getName())).getFieldIds())
-            .getValueVector();
-      }
-      cont.add(vectors);
-    }
-    cont.buildSchema(BatchSchema.SelectionVectorMode.FOUR_BYTE);
-    return cont;
+  @Override
+  public WritableBatch getWritableBatch() {
+    throw new UnsupportedOperationException(""A sort batch is not writable."");
   }
 
-  private MSorter createNewMSorter() throws ClassTransformationException, IOException, SchemaChangeException {
-    return createNewMSorter(context, this.popConfig.getOrderings(), this, MAIN_MAPPING, LEFT_MAPPING, RIGHT_MAPPING);
+  @Override
+  protected void killIncoming(boolean sendUpstream) {
+    incoming.kill(sendUpstream);
   }
 
-  private MSorter createNewMSorter(FragmentContext context, List<Ordering> orderings, VectorAccessible batch, MappingSet mainMapping, MappingSet leftMapping, MappingSet
-    rightMapping)
-          throws ClassTransformationException, IOException, SchemaChangeException {
-    CodeGenerator<MSorter> cg = CodeGenerator.get(MSorter.TEMPLATE_DEFINITION, context.getOptions());
-    ClassGenerator<MSorter> g = cg.getRoot();
-    g.setMappingSet(mainMapping);
-
-    for (Ordering od : orderings) {
-      // first, we rewrite the evaluation stack for each side of the comparison.
-      ErrorCollector collector = new ErrorCollectorImpl();
-      final LogicalExpression expr = ExpressionTreeMaterializer.materialize(od.getExpr(), batch, collector, context.getFunctionRegistry());
-      if (collector.hasErrors()) {
-        throw new SchemaChangeException(""Failure while materializing expression. "" + collector.toErrorString());
+  /**
+   * Extreme paranoia to avoid leaving resources unclosed in the case
+   * of an error. Since generally only the first error is of interest,
+   * we track only the first exception, not potential cascading downstream
+   * exceptions.
+   * <p>
+   * Some Drill code ends up calling close() two or more times. The code
+   * here protects itself from these undesirable semantics.
+   * </p>
+   */
+","[{'comment': '```suggestion\r\n\r\n```', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 
-        builder.build(container);
-        sv4 = builder.getSv4();
-        mSorter = createNewMSorter();
-        mSorter.setup(context, oAllocator, getSelectionVector4(), this.container);
+    IterOutcome result = OK;
+    for (;;) {
+      result = loadBatch();
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes setup
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SETUP);
-        mSorter.sort(this.container);
+      // NONE/EMIT means all batches have been read at this record boundary
+      if (result == NONE || result == EMIT) {
+        break; }
 
-        // sort may have prematurely exited due to should continue returning false.
-        if (!context.getExecutorState().shouldContinue()) {
-          return IterOutcome.STOP;
-        }
+      // if result is STOP that means something went wrong.
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes sorting
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SORT);
-        sv4 = mSorter.getSV4();
+      if (result == STOP) {
+        return result; }
+    }
 
-        container.buildSchema(SelectionVectorMode.FOUR_BYTE);
-      } else { // some batches were spilled
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        }
-        batchGroups.addAll(spilledBatchGroups);
-        spilledBatchGroups = null; // no need to cleanup spilledBatchGroups, all it's batches are in batchGroups now
-
-        logger.warn(""Starting to merge. {} batch groups. Current allocated memory: {}"", batchGroups.size(), oAllocator.getAllocatedMemory());
-        VectorContainer hyperBatch = constructHyperBatch(batchGroups);
-        createCopier(hyperBatch, batchGroups, container, false);
-
-        int estimatedRecordSize = 0;
-        for (VectorWrapper<?> w : batchGroups.get(0)) {
-          try {
-            estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-          } catch (UnsupportedOperationException e) {
-            estimatedRecordSize += 50;
-          }
-        }
-        targetRecordCount = Math.min(MAX_BATCH_ROW_COUNT, Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize));
-        int count = copier.next(targetRecordCount);
-        container.buildSchema(SelectionVectorMode.NONE);
-        container.setRecordCount(count);
+    // Anything to actually sort?
+    resultsIterator = sortImpl.startMerge();
+    if (! resultsIterator.next()) {
+      // If there is no records to sort and we got NONE then just return NONE
+      if (result == NONE) {
+        sortState = SortState.DONE;
+        return NONE;
       }
-
-      return IterOutcome.OK_NEW_SCHEMA;
-
-    } catch (SchemaChangeException ex) {
-      kill(false);
-      context.getExecutorState().fail(UserException.unsupportedError(ex)
-        .message(""Sort doesn't currently support sorts with changing schemas"").build(logger));
-      return IterOutcome.STOP;
-    } catch(ClassTransformationException | IOException ex) {
-      kill(false);
-      context.getExecutorState().fail(ex);
-      return IterOutcome.STOP;
-    } catch (UnsupportedOperationException e) {
-      throw new RuntimeException(e);
     }
-  }
-
-  private boolean hasMemoryForInMemorySort(int currentRecordCount) {
-    long currentlyAvailable =  popConfig.getMaxAllocation() - oAllocator.getAllocatedMemory();
 
-    long neededForInMemorySort = SortRecordBatchBuilder.memoryNeeded(currentRecordCount) +
-        MSortTemplate.memoryNeeded(currentRecordCount);
-
-    return currentlyAvailable > neededForInMemorySort;
-  }
+    // sort may have prematurely exited due to shouldContinue() returning false.
 
-  public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws SchemaChangeException {
-    logger.debug(""Copier allocator current allocation {}"", copierAllocator.getAllocatedMemory());
-    logger.debug(""mergeAndSpill: starting total size in memory = {}"", oAllocator.getAllocatedMemory());
-    VectorContainer outputContainer = new VectorContainer();
-    List<BatchGroup> batchGroupList = Lists.newArrayList();
-    int batchCount = batchGroups.size();
-    for (int i = 0; i < batchCount / 2; i++) {
-      if (batchGroups.size() == 0) {
-        break;
-      }
-      BatchGroup batch = batchGroups.pollLast();
-      assert batch != null : ""Encountered a null batch during merge and spill operation"";
-      batchGroupList.add(batch);
+    if (!context.getExecutorState().shouldContinue()) {
+      sortState = SortState.DONE;
+      return STOP;
     }
 
-    if (batchGroupList.size() == 0) {
-      return null;
-    }
-    int estimatedRecordSize = 0;
-    for (VectorWrapper<?> w : batchGroupList.get(0)) {
-      try {
-        estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-      } catch (UnsupportedOperationException e) {
-        estimatedRecordSize += 50;
-      }
-    }
-    int targetRecordCount = Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize);
-    VectorContainer hyperBatch = constructHyperBatch(batchGroupList);
-    createCopier(hyperBatch, batchGroupList, outputContainer, true);
+    // If we are here that means there is some data to be returned downstream.
+    // We have to prepare output container
+    prepareOutputContainer(resultsIterator);
+    return getFinalOutcome();
+  }
 
-    int count = copier.next(targetRecordCount);
-    assert count > 0;
+  /**
+   * Load and process a single batch, handling schema changes. In general, the
+   * external sort accepts only one schema.
+   *
+   * @return return code depending on the amount of data read from upstream
+   */
 
-    logger.debug(""mergeAndSpill: estimated record size = {}, target record count = {}"", estimatedRecordSize, targetRecordCount);
+  private IterOutcome loadBatch() {
 
-    // 1 output container is kept in memory, so we want to hold on to it and transferClone
-    // allows keeping ownership
-    VectorContainer c1 = VectorContainer.getTransferClone(outputContainer, oContext);
-    c1.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-    c1.setRecordCount(count);
+    // If this is the very first batch, then AbstractRecordBatch
+    // already loaded it for us in buildSchema().
 
-    String spillDir = dirs.next();
-    Path currSpillPath = new Path(Joiner.on(""/"").join(spillDir, fileName));
-    currSpillDirs.add(currSpillPath);
-    String outputFile = Joiner.on(""/"").join(currSpillPath, spillCount++);
-    try {
-        fs.deleteOnExit(currSpillPath);
-    } catch (IOException e) {
-        // since this is meant to be used in a batches's spilling, we don't propagate the exception
-        logger.warn(""Unable to mark spill directory "" + currSpillPath + "" for deleting on exit"", e);
+    if (sortState == SortState.START) {
+      sortState = SortState.LOAD;
+      lastKnownOutcome = OK_NEW_SCHEMA;
+    } else {
+      lastKnownOutcome = next(incoming);
     }
-    stats.setLongStat(Metric.SPILL_COUNT, spillCount);
-    BatchGroup newGroup = new BatchGroup(c1, fs, outputFile, oContext);
-    try (AutoCloseable a = AutoCloseables.all(batchGroupList)) {
-      logger.info(""Merging and spilling to {}"", outputFile);
-      while ((count = copier.next(targetRecordCount)) > 0) {
-        outputContainer.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-        outputContainer.setRecordCount(count);
-        // note that addBatch also clears the outputContainer
-        newGroup.addBatch(outputContainer);
+    switch (lastKnownOutcome) {
+    case NONE:
+    case STOP:
+      return lastKnownOutcome;
+    case OK_NEW_SCHEMA:
+      firstBatchOfSchema = true;
+      setupSchema();
+      // Fall through
+
+    case OK:
+    case EMIT:
+
+      // Add the batch to the in-memory generation, spilling if
+      // needed.
+
+      sortImpl.addBatch(incoming);
+      break;
+    case OUT_OF_MEMORY:
+
+      // Note: it is highly doubtful that this code actually works. It
+      // requires that the upstream batches got to a safe place to run
+      // out of memory and that no work was in-flight and thus abandoned.
+      // Consider removing this case once resource management is in place.
+
+      logger.error(""received OUT_OF_MEMORY, trying to spill"");
+      if (! sortImpl.forceSpill()) {
+        throw UserException.memoryError(""Received OUT_OF_MEMORY, but not enough batches to spill"")
+          .build(logger);
       }
-      injector.injectChecked(context.getExecutionControls(), INTERRUPTION_WHILE_SPILLING, IOException.class);
-      newGroup.closeOutputStream();
-    } catch (Throwable e) {
-      // we only need to cleanup newGroup if spill failed
-      try {
-        AutoCloseables.close(e, newGroup);
-      } catch (Throwable t) { /* close() may hit the same IO issue; just ignore */ }
-      throw UserException.resourceError(e)
-        .message(""External Sort encountered an error while spilling to disk"")
-              .addContext(e.getMessage() /* more detail */)
-        .build(logger);
-    } finally {
-      hyperBatch.clear();
+      break;
+    default:
+      throw new IllegalStateException(""Unexpected iter outcome: "" + lastKnownOutcome);
     }
-    logger.debug(""mergeAndSpill: final total size in memory = {}"", oAllocator.getAllocatedMemory());
-    logger.info(""Completed spilling to {}"", outputFile);
-    return newGroup;
+    return lastKnownOutcome;
   }
 
-  private SelectionVector2 newSV2() throws OutOfMemoryException, InterruptedException {
-    @SuppressWarnings(""resource"")
-    SelectionVector2 sv2 = new SelectionVector2(oAllocator);
-    if (!sv2.allocateNewSafe(incoming.getRecordCount())) {
-      try {
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        } else {
-          throw UserException.memoryError(""Unable to allocate sv2 for %d records, and not enough batchGroups to spill."",
-              incoming.getRecordCount())
-            .addContext(""batchGroups.size"", batchGroups.size())
-            .addContext(""spilledBatchGroups.size"", spilledBatchGroups.size())
-            .addContext(""allocated memory"", oAllocator.getAllocatedMemory())
-            .addContext(""allocator limit"", oAllocator.getLimit())
+  /**
+   * Handle a new schema from upstream. The ESB is quite limited in its ability
+   * to handle schema changes.
+   */
+
+  private void setupSchema()  {","[{'comment': 'Please simplify the method, for example: \r\n```java\r\n  private void setupSchema() {\r\n    final BatchSchema incomingSchema = incoming.getSchema();\r\n    if (schema == null) {\r\n      // First batch doesn\'t have a schema.\r\n      schema = incomingSchema;\r\n    } else if (!schema.equals(incomingSchema)) {\r\n      if (unionTypeEnabled) {\r\n        schema = SchemaUtil.mergeSchemas(schema, incomingSchema);\r\n      } else {\r\n        throw UserException.unsupportedError()\r\n            .message(""Schema changes not supported in External Sort. Please enable Union type."")\r\n            .addContext(""Previous schema"", schema.toString())\r\n            .addContext(""Incoming schema"", incomingSchema.toString())\r\n            .build(logger);\r\n      }\r\n    }\r\n    sortImpl.setSchema(schema);\r\n  }\r\n```\r\n', 'commenter': 'ihuzenko'}]"
1929,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java,"@@ -286,540 +291,419 @@ public void buildSchema() throws SchemaChangeException {
         state = BatchState.DONE;
         break;
       default:
-        break;
+        throw new IllegalStateException(""Unexpected iter outcome: "" + outcome);
     }
   }
 
+  /**
+   * Process each request for a batch. The first request retrieves
+   * all the incoming batches and sorts them, optionally spilling to
+   * disk as needed. Subsequent calls retrieve the sorted results in
+   * fixed-size batches.
+   */
+
   @Override
   public IterOutcome innerNext() {
-    if (schema != null) {
-      if (spillCount == 0) {
-        return (getSelectionVector4().next()) ? IterOutcome.OK : IterOutcome.NONE;
-      } else {
-        Stopwatch w = Stopwatch.createStarted();
-        int count = copier.next(targetRecordCount);
-        if (count > 0) {
-          long t = w.elapsed(TimeUnit.MICROSECONDS);
-          logger.debug(""Took {} us to merge {} records"", t, count);
-          container.setRecordCount(count);
-          return IterOutcome.OK;
-        } else {
-          logger.debug(""copier returned 0 records"");
-          return IterOutcome.NONE;
-        }
+    switch (sortState) {
+    case DONE:
+      return NONE;
+    case START:
+      return load();
+    case LOAD:
+      if (!this.retainInMemoryBatchesOnNone) {
+        resetSortState();
       }
+      return (sortState == SortState.DONE) ? NONE : load();
+    case DELIVER:
+      return nextOutputBatch();
+    default:
+      throw new IllegalStateException(""Unexpected sort state: "" + sortState);
     }
+  }
 
-    int totalCount = 0;
-    int totalBatches = 0; // total number of batches received so far
+  private IterOutcome nextOutputBatch() {
+    // Call next on outputSV4 for it's state to progress in parallel to resultsIterator state
+    outputSV4.next();
 
-    try{
-      container.clear();
-      outer: while (true) {
-        IterOutcome upstream;
-        if (first) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        } else {
-          upstream = next(incoming);
-        }
-        if (upstream == IterOutcome.OK && sorter == null) {
-          upstream = IterOutcome.OK_NEW_SCHEMA;
-        }
-        switch (upstream) {
-        case NONE:
-          if (first) {
-            return upstream;
-          }
-          break outer;
-        case NOT_YET:
-          throw new UnsupportedOperationException();
-        case STOP:
-          return upstream;
-        case OK_NEW_SCHEMA:
-        case OK:
-          VectorContainer convertedBatch;
-          // only change in the case that the schema truly changes.  Artificial schema changes are ignored.
-          if (upstream == IterOutcome.OK_NEW_SCHEMA && !incoming.getSchema().equals(schema)) {
-            if (schema != null) {
-              if (unionTypeEnabled) {
-                this.schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
-              } else {
-                throw new SchemaChangeException(""Schema changes not supported in External Sort. Please enable Union type"");
-              }
-            } else {
-              schema = incoming.getSchema();
-            }
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-            for (BatchGroup b : batchGroups) {
-              b.setSchema(schema);
-            }
-            for (BatchGroup b : spilledBatchGroups) {
-              b.setSchema(schema);
-            }
-            this.sorter = createNewSorter(context, convertedBatch);
-          } else {
-            convertedBatch = SchemaUtil.coerceContainer(incoming, schema, oContext);
-          }
-          if (first) {
-            first = false;
-          }
-          if (convertedBatch.getRecordCount() == 0) {
-            for (VectorWrapper<?> w : convertedBatch) {
-              w.clear();
-            }
-            break;
-          }
-          SelectionVector2 sv2;
-          if (incoming.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {
-            sv2 = incoming.getSelectionVector2().clone();
-          } else {
-            try {
-              sv2 = newSV2();
-            } catch(InterruptedException e) {
-              return IterOutcome.STOP;
-            } catch (OutOfMemoryException e) {
-              throw new OutOfMemoryException(e);
-            }
-          }
+    // But if results iterator next returns true that means it has more results to pass
+    if (resultsIterator.next()) {
+      container.setRecordCount(getRecordCount());
+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);
+    }
+    // getFinalOutcome will take care of returning correct IterOutcome when there is no data to pass and for
+    // EMIT/NONE scenarios
+    return getFinalOutcome();
+  }
 
-          int count = sv2.getCount();
-          totalCount += count;
-          totalBatches++;
-          sorter.setup(context, sv2, convertedBatch);
-          sorter.sort(sv2);
-          RecordBatchData rbd = new RecordBatchData(convertedBatch, oAllocator);
-          boolean success = false;
-          try {
-            rbd.setSv2(sv2);
-            batchGroups.add(new BatchGroup(rbd.getContainer(), rbd.getSv2(), oContext));
-            if (peakNumBatches < batchGroups.size()) {
-              peakNumBatches = batchGroups.size();
-              stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);
-            }
-
-            batchesSinceLastSpill++;
-            if (// If we haven't spilled so far, do we have enough memory for MSorter if this turns out to be the last incoming batch?
-                (spillCount == 0 && !hasMemoryForInMemorySort(totalCount)) ||
-                // If we haven't spilled so far, make sure we don't exceed the maximum number of batches SV4 can address
-                (spillCount == 0 && totalBatches > Character.MAX_VALUE) ||
-                // TODO(DRILL-4438) - consider setting this threshold more intelligently,
-                // lowering caused a failing low memory condition (test in BasicPhysicalOpUnitTest)
-                // to complete successfully (although it caused perf decrease as there was more spilling)
-
-                // current memory used is more than 95% of memory usage limit of this operator
-                (oAllocator.getAllocatedMemory() > .95 * oAllocator.getLimit()) ||
-                // Number of incoming batches (BatchGroups) exceed the limit and number of incoming batches accumulated
-                // since the last spill exceed the defined limit
-                (batchGroups.size() > SPILL_THRESHOLD && batchesSinceLastSpill >= SPILL_BATCH_GROUP_SIZE)) {
-
-              if (firstSpillBatchCount == 0) {
-                firstSpillBatchCount = batchGroups.size();
-              }
-
-              if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {
-                logger.info(""Merging spills"");
-                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);
-                if (merged != null) {
-                  spilledBatchGroups.addFirst(merged);
-                }
-              }
-              final BatchGroup merged = mergeAndSpill(batchGroups);
-              if (merged != null) { // make sure we don't add null to spilledBatchGroups
-                spilledBatchGroups.add(merged);
-                batchesSinceLastSpill = 0;
-              }
-            }
-            success = true;
-          } finally {
-            if (!success) {
-              rbd.clear();
-            }
-          }
-          break;
-        case OUT_OF_MEMORY:
-          logger.debug(""received OUT_OF_MEMORY, trying to spill"");
-          if (batchesSinceLastSpill > 2) {
-            final BatchGroup merged = mergeAndSpill(batchGroups);
-            if (merged != null) {
-              spilledBatchGroups.add(merged);
-              batchesSinceLastSpill = 0;
-            }
-          } else {
-            logger.debug(""not enough batches to spill, sending OUT_OF_MEMORY downstream"");
-            return IterOutcome.OUT_OF_MEMORY;
-          }
-          break;
-        default:
-          throw new UnsupportedOperationException();
-        }
-      }
+  /**
+   * Load the results and sort them. May bail out early if an exceptional
+   * condition is passed up from the input batch.
+   *
+   * @return return code: OK_NEW_SCHEMA if rows were sorted,
+   * NONE if no rows
+   */
 
-      if (totalCount == 0) {
-        return IterOutcome.NONE;
-      }
-      if (spillCount == 0) {
+  private IterOutcome load() {
+    logger.trace(""Start of load phase"");
 
-        if (builder != null) {
-          builder.clear();
-          builder.close();
-        }
-        builder = new SortRecordBatchBuilder(oAllocator);
+    // Don't clear the temporary container created by buildSchema() after each load since across EMIT outcome we have
+    // to maintain the ValueVector references for downstream operators
 
-        for (BatchGroup group : batchGroups) {
-          RecordBatchData rbd = new RecordBatchData(group.getContainer(), oAllocator);
-          rbd.setSv2(group.getSv2());
-          builder.add(rbd);
-        }
+    // Loop over all input batches
 
-        builder.build(container);
-        sv4 = builder.getSv4();
-        mSorter = createNewMSorter();
-        mSorter.setup(context, oAllocator, getSelectionVector4(), this.container);
+    IterOutcome result = OK;
+    for (;;) {
+      result = loadBatch();
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes setup
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SETUP);
-        mSorter.sort(this.container);
+      // NONE/EMIT means all batches have been read at this record boundary
+      if (result == NONE || result == EMIT) {
+        break; }
 
-        // sort may have prematurely exited due to should continue returning false.
-        if (!context.getExecutorState().shouldContinue()) {
-          return IterOutcome.STOP;
-        }
+      // if result is STOP that means something went wrong.
 
-        // For testing memory-leak purpose, inject exception after mSorter finishes sorting
-        injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_AFTER_SORT);
-        sv4 = mSorter.getSV4();
+      if (result == STOP) {
+        return result; }
+    }
 
-        container.buildSchema(SelectionVectorMode.FOUR_BYTE);
-      } else { // some batches were spilled
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        }
-        batchGroups.addAll(spilledBatchGroups);
-        spilledBatchGroups = null; // no need to cleanup spilledBatchGroups, all it's batches are in batchGroups now
-
-        logger.warn(""Starting to merge. {} batch groups. Current allocated memory: {}"", batchGroups.size(), oAllocator.getAllocatedMemory());
-        VectorContainer hyperBatch = constructHyperBatch(batchGroups);
-        createCopier(hyperBatch, batchGroups, container, false);
-
-        int estimatedRecordSize = 0;
-        for (VectorWrapper<?> w : batchGroups.get(0)) {
-          try {
-            estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-          } catch (UnsupportedOperationException e) {
-            estimatedRecordSize += 50;
-          }
-        }
-        targetRecordCount = Math.min(MAX_BATCH_ROW_COUNT, Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize));
-        int count = copier.next(targetRecordCount);
-        container.buildSchema(SelectionVectorMode.NONE);
-        container.setRecordCount(count);
+    // Anything to actually sort?
+    resultsIterator = sortImpl.startMerge();
+    if (! resultsIterator.next()) {
+      // If there is no records to sort and we got NONE then just return NONE
+      if (result == NONE) {
+        sortState = SortState.DONE;
+        return NONE;
       }
-
-      return IterOutcome.OK_NEW_SCHEMA;
-
-    } catch (SchemaChangeException ex) {
-      kill(false);
-      context.getExecutorState().fail(UserException.unsupportedError(ex)
-        .message(""Sort doesn't currently support sorts with changing schemas"").build(logger));
-      return IterOutcome.STOP;
-    } catch(ClassTransformationException | IOException ex) {
-      kill(false);
-      context.getExecutorState().fail(ex);
-      return IterOutcome.STOP;
-    } catch (UnsupportedOperationException e) {
-      throw new RuntimeException(e);
     }
-  }
-
-  private boolean hasMemoryForInMemorySort(int currentRecordCount) {
-    long currentlyAvailable =  popConfig.getMaxAllocation() - oAllocator.getAllocatedMemory();
 
-    long neededForInMemorySort = SortRecordBatchBuilder.memoryNeeded(currentRecordCount) +
-        MSortTemplate.memoryNeeded(currentRecordCount);
-
-    return currentlyAvailable > neededForInMemorySort;
-  }
+    // sort may have prematurely exited due to shouldContinue() returning false.
 
-  public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws SchemaChangeException {
-    logger.debug(""Copier allocator current allocation {}"", copierAllocator.getAllocatedMemory());
-    logger.debug(""mergeAndSpill: starting total size in memory = {}"", oAllocator.getAllocatedMemory());
-    VectorContainer outputContainer = new VectorContainer();
-    List<BatchGroup> batchGroupList = Lists.newArrayList();
-    int batchCount = batchGroups.size();
-    for (int i = 0; i < batchCount / 2; i++) {
-      if (batchGroups.size() == 0) {
-        break;
-      }
-      BatchGroup batch = batchGroups.pollLast();
-      assert batch != null : ""Encountered a null batch during merge and spill operation"";
-      batchGroupList.add(batch);
+    if (!context.getExecutorState().shouldContinue()) {
+      sortState = SortState.DONE;
+      return STOP;
     }
 
-    if (batchGroupList.size() == 0) {
-      return null;
-    }
-    int estimatedRecordSize = 0;
-    for (VectorWrapper<?> w : batchGroupList.get(0)) {
-      try {
-        estimatedRecordSize += TypeHelper.getSize(w.getField().getType());
-      } catch (UnsupportedOperationException e) {
-        estimatedRecordSize += 50;
-      }
-    }
-    int targetRecordCount = Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize);
-    VectorContainer hyperBatch = constructHyperBatch(batchGroupList);
-    createCopier(hyperBatch, batchGroupList, outputContainer, true);
+    // If we are here that means there is some data to be returned downstream.
+    // We have to prepare output container
+    prepareOutputContainer(resultsIterator);
+    return getFinalOutcome();
+  }
 
-    int count = copier.next(targetRecordCount);
-    assert count > 0;
+  /**
+   * Load and process a single batch, handling schema changes. In general, the
+   * external sort accepts only one schema.
+   *
+   * @return return code depending on the amount of data read from upstream
+   */
 
-    logger.debug(""mergeAndSpill: estimated record size = {}, target record count = {}"", estimatedRecordSize, targetRecordCount);
+  private IterOutcome loadBatch() {
 
-    // 1 output container is kept in memory, so we want to hold on to it and transferClone
-    // allows keeping ownership
-    VectorContainer c1 = VectorContainer.getTransferClone(outputContainer, oContext);
-    c1.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-    c1.setRecordCount(count);
+    // If this is the very first batch, then AbstractRecordBatch
+    // already loaded it for us in buildSchema().
 
-    String spillDir = dirs.next();
-    Path currSpillPath = new Path(Joiner.on(""/"").join(spillDir, fileName));
-    currSpillDirs.add(currSpillPath);
-    String outputFile = Joiner.on(""/"").join(currSpillPath, spillCount++);
-    try {
-        fs.deleteOnExit(currSpillPath);
-    } catch (IOException e) {
-        // since this is meant to be used in a batches's spilling, we don't propagate the exception
-        logger.warn(""Unable to mark spill directory "" + currSpillPath + "" for deleting on exit"", e);
+    if (sortState == SortState.START) {
+      sortState = SortState.LOAD;
+      lastKnownOutcome = OK_NEW_SCHEMA;
+    } else {
+      lastKnownOutcome = next(incoming);
     }
-    stats.setLongStat(Metric.SPILL_COUNT, spillCount);
-    BatchGroup newGroup = new BatchGroup(c1, fs, outputFile, oContext);
-    try (AutoCloseable a = AutoCloseables.all(batchGroupList)) {
-      logger.info(""Merging and spilling to {}"", outputFile);
-      while ((count = copier.next(targetRecordCount)) > 0) {
-        outputContainer.buildSchema(BatchSchema.SelectionVectorMode.NONE);
-        outputContainer.setRecordCount(count);
-        // note that addBatch also clears the outputContainer
-        newGroup.addBatch(outputContainer);
+    switch (lastKnownOutcome) {
+    case NONE:
+    case STOP:
+      return lastKnownOutcome;
+    case OK_NEW_SCHEMA:
+      firstBatchOfSchema = true;
+      setupSchema();
+      // Fall through
+
+    case OK:
+    case EMIT:
+
+      // Add the batch to the in-memory generation, spilling if
+      // needed.
+
+      sortImpl.addBatch(incoming);
+      break;
+    case OUT_OF_MEMORY:
+
+      // Note: it is highly doubtful that this code actually works. It
+      // requires that the upstream batches got to a safe place to run
+      // out of memory and that no work was in-flight and thus abandoned.
+      // Consider removing this case once resource management is in place.
+
+      logger.error(""received OUT_OF_MEMORY, trying to spill"");
+      if (! sortImpl.forceSpill()) {
+        throw UserException.memoryError(""Received OUT_OF_MEMORY, but not enough batches to spill"")
+          .build(logger);
       }
-      injector.injectChecked(context.getExecutionControls(), INTERRUPTION_WHILE_SPILLING, IOException.class);
-      newGroup.closeOutputStream();
-    } catch (Throwable e) {
-      // we only need to cleanup newGroup if spill failed
-      try {
-        AutoCloseables.close(e, newGroup);
-      } catch (Throwable t) { /* close() may hit the same IO issue; just ignore */ }
-      throw UserException.resourceError(e)
-        .message(""External Sort encountered an error while spilling to disk"")
-              .addContext(e.getMessage() /* more detail */)
-        .build(logger);
-    } finally {
-      hyperBatch.clear();
+      break;
+    default:
+      throw new IllegalStateException(""Unexpected iter outcome: "" + lastKnownOutcome);
     }
-    logger.debug(""mergeAndSpill: final total size in memory = {}"", oAllocator.getAllocatedMemory());
-    logger.info(""Completed spilling to {}"", outputFile);
-    return newGroup;
+    return lastKnownOutcome;
   }
 
-  private SelectionVector2 newSV2() throws OutOfMemoryException, InterruptedException {
-    @SuppressWarnings(""resource"")
-    SelectionVector2 sv2 = new SelectionVector2(oAllocator);
-    if (!sv2.allocateNewSafe(incoming.getRecordCount())) {
-      try {
-        final BatchGroup merged = mergeAndSpill(batchGroups);
-        if (merged != null) {
-          spilledBatchGroups.add(merged);
-        } else {
-          throw UserException.memoryError(""Unable to allocate sv2 for %d records, and not enough batchGroups to spill."",
-              incoming.getRecordCount())
-            .addContext(""batchGroups.size"", batchGroups.size())
-            .addContext(""spilledBatchGroups.size"", spilledBatchGroups.size())
-            .addContext(""allocated memory"", oAllocator.getAllocatedMemory())
-            .addContext(""allocator limit"", oAllocator.getLimit())
+  /**
+   * Handle a new schema from upstream. The ESB is quite limited in its ability
+   * to handle schema changes.
+   */
+
+  private void setupSchema()  {
+
+    // First batch: we won't have a schema.
+
+    if (schema == null) {
+      schema = incoming.getSchema();
+    } else if (incoming.getSchema().equals(schema)) {
+      // Nothing to do.  Artificial schema changes are ignored.
+    } else if (unionTypeEnabled) {
+      schema = SchemaUtil.mergeSchemas(schema, incoming.getSchema());
+    } else {
+      throw UserException.unsupportedError()
+            .message(""Schema changes not supported in External Sort. Please enable Union type."")
+            .addContext(""Previous schema"", schema.toString())
+            .addContext(""Incoming schema"", incoming.getSchema().toString())
             .build(logger);
-        }
-      } catch (SchemaChangeException e) {
-        throw new RuntimeException(e);
-      }
-      int waitTime = 1;
-      while (true) {
-        try {
-          Thread.sleep(waitTime * 1000);
-        } catch(final InterruptedException e) {
-          if (!context.getExecutorState().shouldContinue()) {
-            throw e;
-          }
-        }
-        waitTime *= 2;
-        if (sv2.allocateNewSafe(incoming.getRecordCount())) {
-          break;
-        }
-        if (waitTime >= 32) {
-          throw new OutOfMemoryException(""Unable to allocate sv2 buffer after repeated attempts"");
-        }
-      }
     }
-    for (int i = 0; i < incoming.getRecordCount(); i++) {
-      sv2.setIndex(i, (char) i);
-    }
-    sv2.setRecordCount(incoming.getRecordCount());
-    return sv2;
+    sortImpl.setSchema(schema);
   }
 
-  private VectorContainer constructHyperBatch(List<BatchGroup> batchGroupList) {
-    VectorContainer cont = new VectorContainer();
-    for (MaterializedField field : schema) {
-      ValueVector[] vectors = new ValueVector[batchGroupList.size()];
-      int i = 0;
-      for (BatchGroup group : batchGroupList) {
-        vectors[i++] = group.getValueAccessorById(
-            field.getValueClass(),
-            group.getValueVectorId(SchemaPath.getSimplePath(field.getName())).getFieldIds())
-            .getValueVector();
-      }
-      cont.add(vectors);
-    }
-    cont.buildSchema(BatchSchema.SelectionVectorMode.FOUR_BYTE);
-    return cont;
+  @Override
+  public WritableBatch getWritableBatch() {
+    throw new UnsupportedOperationException(""A sort batch is not writable."");
   }
 
-  private MSorter createNewMSorter() throws ClassTransformationException, IOException, SchemaChangeException {
-    return createNewMSorter(context, this.popConfig.getOrderings(), this, MAIN_MAPPING, LEFT_MAPPING, RIGHT_MAPPING);
+  @Override
+  protected void killIncoming(boolean sendUpstream) {
+    incoming.kill(sendUpstream);
   }
 
-  private MSorter createNewMSorter(FragmentContext context, List<Ordering> orderings, VectorAccessible batch, MappingSet mainMapping, MappingSet leftMapping, MappingSet
-    rightMapping)
-          throws ClassTransformationException, IOException, SchemaChangeException {
-    CodeGenerator<MSorter> cg = CodeGenerator.get(MSorter.TEMPLATE_DEFINITION, context.getOptions());
-    ClassGenerator<MSorter> g = cg.getRoot();
-    g.setMappingSet(mainMapping);
-
-    for (Ordering od : orderings) {
-      // first, we rewrite the evaluation stack for each side of the comparison.
-      ErrorCollector collector = new ErrorCollectorImpl();
-      final LogicalExpression expr = ExpressionTreeMaterializer.materialize(od.getExpr(), batch, collector, context.getFunctionRegistry());
-      if (collector.hasErrors()) {
-        throw new SchemaChangeException(""Failure while materializing expression. "" + collector.toErrorString());
+  /**
+   * Extreme paranoia to avoid leaving resources unclosed in the case
+   * of an error. Since generally only the first error is of interest,
+   * we track only the first exception, not potential cascading downstream
+   * exceptions.
+   * <p>
+   * Some Drill code ends up calling close() two or more times. The code
+   * here protects itself from these undesirable semantics.
+   * </p>
+   */
+
+  @Override
+  public void close() {","[{'comment': 'It would be good to reduce boilerplate here, for example: \r\n```java\r\n    if (closed) {\r\n      return;\r\n    }\r\n    try {\r\n      AutoCloseables.close(\r\n          resultsIterator,\r\n          sortImpl,\r\n          outputWrapperContainer::clear,\r\n          outputSV4::clear,\r\n          super::close,\r\n          oContext);\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    } finally {\r\n      resultsIterator = null;\r\n      sortImpl = null;\r\n      closed = true;\r\n    }\r\n```\r\nAlso, it would be good to make sanity check based on its own boolean `closed` field, so nobody could accidentally break the closing.', 'commenter': 'ihuzenko'}]"
1933,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetComplex.java,"@@ -835,4 +835,19 @@ public void testDictValueInFilter3() throws Exception {
         .baselineValues(2, TestBuilder.mapOfObject(""a"", 1, ""b"", 2, ""c"", 3))
         .go();
   }
+
+  @Test // DRILL-7473
+  public void testSelect() throws Exception {","[{'comment': 'Please give better naming for the test', 'commenter': 'arina-ielchiieva'}]"
1933,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/validate/BatchValidator.java,"@@ -293,14 +292,20 @@ private void validateVector(String name, ValueVector vector) {
       // structure to check.
     } else if (vector instanceof BaseRepeatedValueVector) {
       validateRepeatedVector(name, (BaseRepeatedValueVector) vector);
-    } else if (vector instanceof RepeatedMapVector) {
-      validateRepeatedMapVector(name, (RepeatedMapVector) vector);
+    } else if (vector instanceof AbstractRepeatedMapVector) {","[{'comment': 'Could you please clarify your comment below, for example\r\n\r\n>Checking ```AbstractRepeatedMapVector``` includes DictVector, in order to avoid NPE while getting value from ```NullReader```. The ```NullReader``` may be returned when index points to the absent value in dict array, example query ```SELECT dict_array[3].keyInDict ... ``` .', 'commenter': 'ihuzenko'}, {'comment': ""I've updated the comment - hope it is clearer"", 'commenter': 'KazydubB'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM","[{'comment': '```suggestion\r\n * registers the field with PMM. If the field is a variable-width field, PMM\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression","[{'comment': '```suggestion\r\n * records the expression that produces the variable-width field. The expression\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,","[{'comment': '```suggestion\r\n * fixed-width fields are just accumulated into a single total. Note: The PMM,\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);","[{'comment': '```suggestion\r\n  private static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }","[{'comment': 'After a close look at the code, I believe this enum is unnecessary. All usages of the ```ColumnWidthInfo``` constructor accept ```WidthType.VARIABLE``` and there is a block inside the update method : \r\n```java\r\n      if (columnWidthInfo.isFixedWidth()) {\r\n        // fixed width columns are accumulated in totalFixedWidthColumnWidth\r\n        ShouldNotReachHere();\r\n      } else {...}\r\n```\r\nPlease remove the enum and related code. \r\n', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {","[{'comment': '```suggestion\r\n  private enum OutputColumnType {\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {","[{'comment': '```suggestion\r\n  private static class VariableWidthColumnInfo {\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;","[{'comment': 'The ```width``` is always ```-1``` and getter is unused, so I suggest removing the field and related code.', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;","[{'comment': 'please remove the field and enum as suggested previously.', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;","[{'comment': ""Actually the field and getter aren't used, but if you think this could be useful for debugging then it can be left as is."", 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }","[{'comment': '```suggestion\r\n    OutputWidthExpression getOutputExpression() { return outputExpression; }\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }","[{'comment': 'please remove', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }","[{'comment': 'Since ```getIncomingBatch()``` already exists in the class, please remove the method and replace its usages. \r\n\r\n```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }","[{'comment': 'Since the method is already outdated (DICT missed), I would suggest replacing its usages with \r\n```java\r\nTypes.isComplex(type) || Types.isUnion(type)\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {","[{'comment': '```suggestion\r\n  private ValueVector getOutgoingValueVector(TypedFieldId fieldId) {\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }","[{'comment': '```suggestion\r\n  private static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
+                String inputColumnName, String outputColumnName) {
+    if(isFixedWidth(vv)) {
+      addFixedWidthField(vv);
+    } else {
+      addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
     }
-
-    public static String printVV(ValueVector vv) {
-        String str = ""null"";
-        if (vv != null) {
-            str = vv.getField().getName() + "" "" + vv.getField().getType();
-        }
-        return str;
+  }
+
+  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
+                                     OutputColumnType outputColumnType, String inputColumnName,
+                                     String outputColumnName) {
+    variableWidthColumnCount++;
+    ColumnWidthInfo columnWidthInfo;
+    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
+            printVV(vv), variableWidthColumnCount, outputColumnType);
+    // Variable width transfers
+    if (outputColumnType == OutputColumnType.TRANSFER) {
+      VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
+      columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
+    } else if (isComplex(vv.getField().getType())) {
+      addComplexField(vv);
+      return;
+    } else {
+      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
+      OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+      OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
+      columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
     }
-
-    void addComplexField(ValueVector vv) {
-        //Complex types are not yet supported. Just use a guess for the size
-        assert vv == null || isComplex(vv.getField().getType());
-        complexColumnsCount++;
-        // just a guess
-        totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
-        logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+    ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
+    Preconditions.checkState(existingInfo == null);
+  }
+
+  public static String printVV(ValueVector vv) {
+    String str = ""null"";
+    if (vv != null) {
+      str = vv.getField().getName() + "" "" + vv.getField().getType();
     }
-
-    void addFixedWidthField(ValueVector vv) {
-        assert isFixedWidth(vv);
-        fixedWidthColumnCount++;
-        int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
-        totalFixedWidthColumnWidth += fixedFieldWidth;
-        logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+    return str;
+  }
+
+  void addComplexField(ValueVector vv) {
+    //Complex types are not yet supported. Just use a guess for the size
+    assert vv == null || isComplex(vv.getField().getType());
+    complexColumnsCount++;
+    // just a guess
+    totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
+    logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
+            printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+  }
+
+  void addFixedWidthField(ValueVector vv) {
+    assert isFixedWidth(vv);
+    fixedWidthColumnCount++;
+    int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);","[{'comment': '```suggestion\r\n    int fixedFieldWidth = ((FixedWidthVector) vv).getValueWidth();\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }","[{'comment': 'Since there is only one usage of the method and assertion present before the call please replace the method with direct ```((FixedWidthVector) vv).getValueWidth()``` call. ', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {","[{'comment': 'Please replace the method with a shorter version: \r\n```java\r\n  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {\r\n    Preconditions.checkArgument(!Types.isVarWidthType(majorType.getMinorType()),\r\n        ""Expected fixed type but was \'%s\'."", majorType.getMinorType());\r\n    return TypeHelper.getSize(majorType);\r\n  }\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,","[{'comment': '```suggestion\r\n  private void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
+                String inputColumnName, String outputColumnName) {
+    if(isFixedWidth(vv)) {
+      addFixedWidthField(vv);
+    } else {
+      addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
     }
-
-    public static String printVV(ValueVector vv) {
-        String str = ""null"";
-        if (vv != null) {
-            str = vv.getField().getName() + "" "" + vv.getField().getType();
-        }
-        return str;
+  }
+
+  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
+                                     OutputColumnType outputColumnType, String inputColumnName,
+                                     String outputColumnName) {
+    variableWidthColumnCount++;
+    ColumnWidthInfo columnWidthInfo;
+    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
+            printVV(vv), variableWidthColumnCount, outputColumnType);
+    // Variable width transfers
+    if (outputColumnType == OutputColumnType.TRANSFER) {
+      VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
+      columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
+    } else if (isComplex(vv.getField().getType())) {
+      addComplexField(vv);
+      return;
+    } else {
+      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
+      OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+      OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
+      columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
     }
-
-    void addComplexField(ValueVector vv) {
-        //Complex types are not yet supported. Just use a guess for the size
-        assert vv == null || isComplex(vv.getField().getType());
-        complexColumnsCount++;
-        // just a guess
-        totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
-        logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+    ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
+    Preconditions.checkState(existingInfo == null);
+  }","[{'comment': 'Could be rewritten to something like:\r\n```java\r\n  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,\r\n                                     OutputColumnType outputColumnType, String inputColumnName,\r\n                                     String outputColumnName) {\r\n    variableWidthColumnCount++;\r\n    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",\r\n        printVV(vv), variableWidthColumnCount, outputColumnType);\r\n    OutputWidthExpression outWidthExpr;\r\n    if (outputColumnType == OutputColumnType.TRANSFER) {\r\n      // Variable width transfers\r\n      outWidthExpr = new VarLenReadExpr(inputColumnName);\r\n    } else if (isComplex(vv.getField().getType())) {\r\n      addComplexField(vv);\r\n      return;\r\n    } else {\r\n      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions\r\n      outWidthExpr = logicalExpression.accept(new OutputWidthVisitor(), new OutputWidthVisitorState(this));\r\n    }\r\n    VariableWidthColumnInfo columnWidthInfo = new VariableWidthColumnInfo(outWidthExpr, outputColumnType,\r\n        WidthType.VARIABLE, -1, vv);// fieldWidth has to be obtained from the OutputWidthExpression\r\n    VariableWidthColumnInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);\r\n    Preconditions.checkState(existingInfo == null);\r\n  }\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
+                String inputColumnName, String outputColumnName) {
+    if(isFixedWidth(vv)) {
+      addFixedWidthField(vv);
+    } else {
+      addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
     }
-
-    public static String printVV(ValueVector vv) {
-        String str = ""null"";
-        if (vv != null) {
-            str = vv.getField().getName() + "" "" + vv.getField().getType();
-        }
-        return str;
+  }
+
+  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
+                                     OutputColumnType outputColumnType, String inputColumnName,
+                                     String outputColumnName) {
+    variableWidthColumnCount++;
+    ColumnWidthInfo columnWidthInfo;
+    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
+            printVV(vv), variableWidthColumnCount, outputColumnType);
+    // Variable width transfers
+    if (outputColumnType == OutputColumnType.TRANSFER) {
+      VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
+      columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
+    } else if (isComplex(vv.getField().getType())) {
+      addComplexField(vv);
+      return;
+    } else {
+      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
+      OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+      OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
+      columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
     }
-
-    void addComplexField(ValueVector vv) {
-        //Complex types are not yet supported. Just use a guess for the size
-        assert vv == null || isComplex(vv.getField().getType());
-        complexColumnsCount++;
-        // just a guess
-        totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
-        logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+    ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
+    Preconditions.checkState(existingInfo == null);
+  }
+
+  public static String printVV(ValueVector vv) {","[{'comment': '```suggestion\r\n  private static String printVV(ValueVector vv) {\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
+                String inputColumnName, String outputColumnName) {
+    if(isFixedWidth(vv)) {
+      addFixedWidthField(vv);
+    } else {
+      addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
     }
-
-    public static String printVV(ValueVector vv) {
-        String str = ""null"";
-        if (vv != null) {
-            str = vv.getField().getName() + "" "" + vv.getField().getType();
-        }
-        return str;
+  }
+
+  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
+                                     OutputColumnType outputColumnType, String inputColumnName,
+                                     String outputColumnName) {
+    variableWidthColumnCount++;
+    ColumnWidthInfo columnWidthInfo;
+    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
+            printVV(vv), variableWidthColumnCount, outputColumnType);
+    // Variable width transfers
+    if (outputColumnType == OutputColumnType.TRANSFER) {
+      VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
+      columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
+    } else if (isComplex(vv.getField().getType())) {
+      addComplexField(vv);
+      return;
+    } else {
+      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
+      OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+      OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
+      columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
     }
-
-    void addComplexField(ValueVector vv) {
-        //Complex types are not yet supported. Just use a guess for the size
-        assert vv == null || isComplex(vv.getField().getType());
-        complexColumnsCount++;
-        // just a guess
-        totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
-        logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+    ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
+    Preconditions.checkState(existingInfo == null);
+  }
+
+  public static String printVV(ValueVector vv) {
+    String str = ""null"";
+    if (vv != null) {
+      str = vv.getField().getName() + "" "" + vv.getField().getType();
     }
-
-    void addFixedWidthField(ValueVector vv) {
-        assert isFixedWidth(vv);
-        fixedWidthColumnCount++;
-        int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
-        totalFixedWidthColumnWidth += fixedFieldWidth;
-        logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+    return str;","[{'comment': '```suggestion\r\n    return vv == null ? ""null"" : vv.getField().toString();\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
+                String inputColumnName, String outputColumnName) {
+    if(isFixedWidth(vv)) {
+      addFixedWidthField(vv);
+    } else {
+      addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
     }
-
-    public static String printVV(ValueVector vv) {
-        String str = ""null"";
-        if (vv != null) {
-            str = vv.getField().getName() + "" "" + vv.getField().getType();
-        }
-        return str;
+  }
+
+  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
+                                     OutputColumnType outputColumnType, String inputColumnName,
+                                     String outputColumnName) {
+    variableWidthColumnCount++;
+    ColumnWidthInfo columnWidthInfo;
+    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
+            printVV(vv), variableWidthColumnCount, outputColumnType);
+    // Variable width transfers
+    if (outputColumnType == OutputColumnType.TRANSFER) {
+      VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
+      columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
+    } else if (isComplex(vv.getField().getType())) {
+      addComplexField(vv);
+      return;
+    } else {
+      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
+      OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+      OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
+      columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
     }
-
-    void addComplexField(ValueVector vv) {
-        //Complex types are not yet supported. Just use a guess for the size
-        assert vv == null || isComplex(vv.getField().getType());
-        complexColumnsCount++;
-        // just a guess
-        totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
-        logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+    ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
+    Preconditions.checkState(existingInfo == null);
+  }
+
+  public static String printVV(ValueVector vv) {
+    String str = ""null"";
+    if (vv != null) {
+      str = vv.getField().getName() + "" "" + vv.getField().getType();
     }
-
-    void addFixedWidthField(ValueVector vv) {
-        assert isFixedWidth(vv);
-        fixedWidthColumnCount++;
-        int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
-        totalFixedWidthColumnWidth += fixedFieldWidth;
-        logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+    return str;
+  }
+
+  void addComplexField(ValueVector vv) {
+    //Complex types are not yet supported. Just use a guess for the size
+    assert vv == null || isComplex(vv.getField().getType());
+    complexColumnsCount++;
+    // just a guess
+    totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
+    logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
+            printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+  }
+
+  void addFixedWidthField(ValueVector vv) {","[{'comment': '```suggestion\r\n private void addFixedWidthField(ValueVector vv) {\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
+                String inputColumnName, String outputColumnName) {
+    if(isFixedWidth(vv)) {
+      addFixedWidthField(vv);
+    } else {
+      addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
     }
-
-    public static String printVV(ValueVector vv) {
-        String str = ""null"";
-        if (vv != null) {
-            str = vv.getField().getName() + "" "" + vv.getField().getType();
-        }
-        return str;
+  }
+
+  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
+                                     OutputColumnType outputColumnType, String inputColumnName,
+                                     String outputColumnName) {
+    variableWidthColumnCount++;
+    ColumnWidthInfo columnWidthInfo;
+    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
+            printVV(vv), variableWidthColumnCount, outputColumnType);
+    // Variable width transfers
+    if (outputColumnType == OutputColumnType.TRANSFER) {
+      VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
+      columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
+    } else if (isComplex(vv.getField().getType())) {
+      addComplexField(vv);
+      return;
+    } else {
+      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
+      OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+      OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
+      columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
     }
-
-    void addComplexField(ValueVector vv) {
-        //Complex types are not yet supported. Just use a guess for the size
-        assert vv == null || isComplex(vv.getField().getType());
-        complexColumnsCount++;
-        // just a guess
-        totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
-        logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+    ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
+    Preconditions.checkState(existingInfo == null);
+  }
+
+  public static String printVV(ValueVector vv) {
+    String str = ""null"";
+    if (vv != null) {
+      str = vv.getField().getName() + "" "" + vv.getField().getType();
     }
-
-    void addFixedWidthField(ValueVector vv) {
-        assert isFixedWidth(vv);
-        fixedWidthColumnCount++;
-        int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
-        totalFixedWidthColumnWidth += fixedFieldWidth;
-        logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+    return str;
+  }
+
+  void addComplexField(ValueVector vv) {
+    //Complex types are not yet supported. Just use a guess for the size
+    assert vv == null || isComplex(vv.getField().getType());
+    complexColumnsCount++;
+    // just a guess
+    totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
+    logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
+            printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+  }
+
+  void addFixedWidthField(ValueVector vv) {
+    assert isFixedWidth(vv);
+    fixedWidthColumnCount++;
+    int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
+    totalFixedWidthColumnWidth += fixedFieldWidth;
+    logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
+            printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+  }
+
+  public void init(RecordBatch incomingBatch, ProjectRecordBatch outgoingBatch) {
+    setIncomingBatch(incomingBatch);
+    setOutgoingBatch(outgoingBatch);
+    reset();
+
+    RecordBatchStats.printConfiguredBatchSize(outgoingBatch.getRecordBatchStatsContext(),
+      getOutputBatchSize());
+  }
+
+  private void reset() {
+    rowWidth = 0;
+    totalFixedWidthColumnWidth = 0;
+    totalComplexColumnWidth = 0;
+
+    fixedWidthColumnCount = 0;
+    complexColumnsCount = 0;
+  }
+
+  @Override
+  public void update() {
+    long updateStartTime = System.currentTimeMillis();
+    RecordBatchSizer batchSizer = new RecordBatchSizer(incomingBatch);
+    long batchSizerEndTime = System.currentTimeMillis();
+
+    setRecordBatchSizer(batchSizer);
+    rowWidth = 0;
+    int totalVariableColumnWidth = 0;
+    for (String outputColumnName : outputColumnSizes.keySet()) {
+      ColumnWidthInfo columnWidthInfo = outputColumnSizes.get(outputColumnName);
+      int width = -1;
+      if (columnWidthInfo.isFixedWidth()) {
+        // fixed width columns are accumulated in totalFixedWidthColumnWidth
+        ShouldNotReachHere();
+      } else {","[{'comment': 'please remove', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
+                String inputColumnName, String outputColumnName) {
+    if(isFixedWidth(vv)) {
+      addFixedWidthField(vv);
+    } else {
+      addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
     }
-
-    public static String printVV(ValueVector vv) {
-        String str = ""null"";
-        if (vv != null) {
-            str = vv.getField().getName() + "" "" + vv.getField().getType();
-        }
-        return str;
+  }
+
+  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
+                                     OutputColumnType outputColumnType, String inputColumnName,
+                                     String outputColumnName) {
+    variableWidthColumnCount++;
+    ColumnWidthInfo columnWidthInfo;
+    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
+            printVV(vv), variableWidthColumnCount, outputColumnType);
+    // Variable width transfers
+    if (outputColumnType == OutputColumnType.TRANSFER) {
+      VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
+      columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
+    } else if (isComplex(vv.getField().getType())) {
+      addComplexField(vv);
+      return;
+    } else {
+      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
+      OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+      OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
+      columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
     }
-
-    void addComplexField(ValueVector vv) {
-        //Complex types are not yet supported. Just use a guess for the size
-        assert vv == null || isComplex(vv.getField().getType());
-        complexColumnsCount++;
-        // just a guess
-        totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
-        logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+    ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
+    Preconditions.checkState(existingInfo == null);
+  }
+
+  public static String printVV(ValueVector vv) {
+    String str = ""null"";
+    if (vv != null) {
+      str = vv.getField().getName() + "" "" + vv.getField().getType();
     }
-
-    void addFixedWidthField(ValueVector vv) {
-        assert isFixedWidth(vv);
-        fixedWidthColumnCount++;
-        int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
-        totalFixedWidthColumnWidth += fixedFieldWidth;
-        logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+    return str;
+  }
+
+  void addComplexField(ValueVector vv) {
+    //Complex types are not yet supported. Just use a guess for the size
+    assert vv == null || isComplex(vv.getField().getType());
+    complexColumnsCount++;
+    // just a guess
+    totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
+    logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
+            printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+  }
+
+  void addFixedWidthField(ValueVector vv) {
+    assert isFixedWidth(vv);
+    fixedWidthColumnCount++;
+    int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
+    totalFixedWidthColumnWidth += fixedFieldWidth;
+    logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
+            printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+  }
+
+  public void init(RecordBatch incomingBatch, ProjectRecordBatch outgoingBatch) {
+    setIncomingBatch(incomingBatch);
+    setOutgoingBatch(outgoingBatch);
+    reset();
+
+    RecordBatchStats.printConfiguredBatchSize(outgoingBatch.getRecordBatchStatsContext(),
+      getOutputBatchSize());
+  }
+
+  private void reset() {
+    rowWidth = 0;
+    totalFixedWidthColumnWidth = 0;
+    totalComplexColumnWidth = 0;
+
+    fixedWidthColumnCount = 0;
+    complexColumnsCount = 0;
+  }
+
+  @Override
+  public void update() {
+    long updateStartTime = System.currentTimeMillis();
+    RecordBatchSizer batchSizer = new RecordBatchSizer(incomingBatch);
+    long batchSizerEndTime = System.currentTimeMillis();
+
+    setRecordBatchSizer(batchSizer);
+    rowWidth = 0;
+    int totalVariableColumnWidth = 0;
+    for (String outputColumnName : outputColumnSizes.keySet()) {
+      ColumnWidthInfo columnWidthInfo = outputColumnSizes.get(outputColumnName);
+      int width = -1;
+      if (columnWidthInfo.isFixedWidth()) {
+        // fixed width columns are accumulated in totalFixedWidthColumnWidth
+        ShouldNotReachHere();
+      } else {
+        //Walk the tree of OutputWidthExpressions to get a FixedLenExpr
+        //As the tree is walked, the RecordBatchSizer and function annotations
+        //are looked-up to come up with the final FixedLenExpr
+        OutputWidthExpression savedWidthExpr = columnWidthInfo.getOutputExpression();
+        OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+        OutputWidthExpression reducedExpr = savedWidthExpr.accept(new OutputWidthVisitor(), state);
+        width = ((FixedLenExpr)reducedExpr).getDataWidth();
+        Preconditions.checkState(width >= 0);
+        int metadataWidth = getMetadataWidth(columnWidthInfo.outputVV);
+        logger.trace(""update(): fieldName {} width: {} metadataWidth: {}"",
+                columnWidthInfo.outputVV.getField().getName(), width, metadataWidth);
+        width += metadataWidth;
+      }
+      totalVariableColumnWidth += width;
     }
-
-    public void init(RecordBatch incomingBatch, ProjectRecordBatch outgoingBatch) {
-        setIncomingBatch(incomingBatch);
-        setOutgoingBatch(outgoingBatch);
-        reset();
-
-        RecordBatchStats.printConfiguredBatchSize(outgoingBatch.getRecordBatchStatsContext(),
-          getOutputBatchSize());
+    rowWidth += totalFixedWidthColumnWidth;
+    rowWidth += totalComplexColumnWidth;
+    rowWidth += totalVariableColumnWidth;
+    int outPutRowCount;
+    if (rowWidth != 0) {
+      //if rowWidth is not zero, set the output row count in the sizer
+      setOutputRowCount(getOutputBatchSize(), rowWidth);
+      // if more rows can be allowed than the incoming row count, then set the
+      // output row count to the incoming row count.
+      outPutRowCount = Math.min(getOutputRowCount(), batchSizer.rowCount());
+    } else {
+      // if rowWidth == 0 then the memory manager does
+      // not have sufficient information to size the batch
+      // let the entire batch pass through.
+      // If incoming rc == 0, all RB Sizer look-ups will have
+      // 0 width and so total width can be 0
+      outPutRowCount = incomingBatch.getRecordCount();
     }
-
-    private void reset() {
-        rowWidth = 0;
-        totalFixedWidthColumnWidth = 0;
-        totalComplexColumnWidth = 0;
-
-        fixedWidthColumnCount = 0;
-        complexColumnsCount = 0;
+    setOutputRowCount(outPutRowCount);
+    long updateEndTime = System.currentTimeMillis();
+    logger.trace(""update() : Output RC {}, BatchSizer RC {}, incoming RC {}, width {}, total fixed width {}""
+                + "", total variable width {}, total complex width {}, batchSizer time {} ms, update time {}  ms""
+                + "", manager {}, incoming {}"",outPutRowCount, batchSizer.rowCount(), incomingBatch.getRecordCount(),
+                rowWidth, totalFixedWidthColumnWidth, totalVariableColumnWidth, totalComplexColumnWidth,
+                (batchSizerEndTime - updateStartTime),(updateEndTime - updateStartTime), this, incomingBatch);
+
+    RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT, getRecordBatchSizer(), outgoingBatch.getRecordBatchStatsContext());
+    updateIncomingStats();
+  }
+
+  public static int getMetadataWidth(ValueVector vv) {
+    int width = 0;
+    if (vv instanceof NullableVector) {
+      width += ((NullableVector)vv).getBitsVector().getPayloadByteCount(1);","[{'comment': '```suggestion\r\n      width += ((NullableVector) vv).getBitsVector().getPayloadByteCount(1);\r\n```', 'commenter': 'ihuzenko'}]"
1944,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectMemoryManager.java,"@@ -42,307 +44,310 @@
 import java.util.Map;
 
 /**
- *
- * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by ProjectRecordBatch.
- * The PMM works as follows:
- *
- * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it registers the field with PMM.
- * If the field is a variable width field, PMM records the expression that produces the variable
- * width field. The expression is a tree of LogicalExpressions. The PMM walks this tree of LogicalExpressions
- * to produce a tree of OutputWidthExpressions. The widths of Fixed width fields are just accumulated into a single
- * total. Note: The PMM, currently, cannot handle new complex fields, it just uses a hard-coded estimate for such fields.
- *
- *
- * Execution phase: Just before a batch is processed by Project, the PMM walks the tree of OutputWidthExpressions
- * and converts them to FixedWidthExpressions. It uses the RecordBatchSizer and the function annotations to do this conversion.
- * See OutputWidthVisitor for details.
+ * ProjectMemoryManager(PMM) is used to estimate the size of rows produced by
+ * ProjectRecordBatch. The PMM works as follows:
+ * <p>
+ * Setup phase: As and when ProjectRecordBatch creates or transfers a field, it
+ * registers the field with PMM. If the field is a variable width field, PMM
+ * records the expression that produces the variable width field. The expression
+ * is a tree of LogicalExpressions. The PMM walks this tree of
+ * LogicalExpressions to produce a tree of OutputWidthExpressions. The widths of
+ * Fixed width fields are just accumulated into a single total. Note: The PMM,
+ * currently, cannot handle new complex fields, it just uses a hard-coded
+ * estimate for such fields.
+ * <p>
+ * Execution phase: Just before a batch is processed by Project, the PMM walks
+ * the tree of OutputWidthExpressions and converts them to
+ * FixedWidthExpressions. It uses the RecordBatchSizer and the function
+ * annotations to do this conversion. See OutputWidthVisitor for details.
  */
 public class ProjectMemoryManager extends RecordBatchMemoryManager {
 
-    static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProjectMemoryManager.class);
-
-    public RecordBatch getIncomingBatch() {
-        return incomingBatch;
+  static final Logger logger = LoggerFactory.getLogger(ProjectMemoryManager.class);
+
+  private RecordBatch incomingBatch;
+  private ProjectRecordBatch outgoingBatch;
+
+  private int rowWidth;
+  private final Map<String, ColumnWidthInfo> outputColumnSizes;
+  // Number of variable width columns in the batch
+  private int variableWidthColumnCount;
+  // Number of fixed width columns in the batch
+  private int fixedWidthColumnCount;
+  // Number of complex columns in the batch
+  private int complexColumnsCount;
+
+  // Holds sum of all fixed width column widths
+  private int totalFixedWidthColumnWidth;
+  // Holds sum of all complex column widths
+  // Currently, this is just a guess
+  private int totalComplexColumnWidth;
+
+  private enum WidthType {
+      FIXED,
+      VARIABLE
+  }
+
+  public enum OutputColumnType {
+      TRANSFER,
+      NEW
+  }
+
+  public static class ColumnWidthInfo {
+    private final OutputWidthExpression outputExpression;
+    private final int width;
+    private final WidthType widthType;
+    private final OutputColumnType outputColumnType;
+    private final ValueVector outputVV; // for transfers, this is the transfer src
+
+
+    ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
+                    OutputColumnType outputColumnType,
+                    WidthType widthType,
+                    int fieldWidth, ValueVector outputVV) {
+      this.outputExpression = outputWidthExpression;
+      this.width = fieldWidth;
+      this.outputColumnType = outputColumnType;
+      this.widthType = widthType;
+      this.outputVV = outputVV;
     }
 
-    RecordBatch incomingBatch = null;
-    ProjectRecordBatch outgoingBatch = null;
+    public OutputWidthExpression getOutputExpression() { return outputExpression; }
 
-    int rowWidth = 0;
-    Map<String, ColumnWidthInfo> outputColumnSizes;
-    // Number of variable width columns in the batch
-    int variableWidthColumnCount = 0;
-    // Number of fixed width columns in the batch
-    int fixedWidthColumnCount = 0;
-    // Number of complex columns in the batch
-    int complexColumnsCount = 0;
+    public OutputColumnType getOutputColumnType() { return outputColumnType; }
 
+    public boolean isFixedWidth() { return widthType == WidthType.FIXED; }
 
-    // Holds sum of all fixed width column widths
-    int totalFixedWidthColumnWidth = 0;
-    // Holds sum of all complex column widths
-    // Currently, this is just a guess
-    int totalComplexColumnWidth = 0;
-
-    enum WidthType {
-        FIXED,
-        VARIABLE
-    }
-
-    enum OutputColumnType {
-        TRANSFER,
-        NEW
-    }
+    public int getWidth() { return width; }
+  }
 
-    class ColumnWidthInfo {
-        OutputWidthExpression outputExpression;
-        int width;
-        WidthType widthType;
-        OutputColumnType outputColumnType;
-        ValueVector outputVV; // for transfers, this is the transfer src
+  public RecordBatch getIncomingBatch() {
+    return incomingBatch;
+  }
 
+  void ShouldNotReachHere() {
+    throw new IllegalStateException();
+  }
 
-        ColumnWidthInfo(OutputWidthExpression outputWidthExpression,
-                        OutputColumnType outputColumnType,
-                        WidthType widthType,
-                        int fieldWidth, ValueVector outputVV) {
-            this.outputExpression = outputWidthExpression;
-            this.width = fieldWidth;
-            this.outputColumnType = outputColumnType;
-            this.widthType = widthType;
-            this.outputVV = outputVV;
-        }
+  private void setIncomingBatch(RecordBatch recordBatch) {
+    incomingBatch = recordBatch;
+  }
 
-        public OutputWidthExpression getOutputExpression() { return outputExpression; }
+  public RecordBatch incomingBatch() { return incomingBatch; }
 
-        public OutputColumnType getOutputColumnType() { return outputColumnType; }
+  private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
+    this.outgoingBatch = outgoingBatch;
+  }
 
-        boolean isFixedWidth() { return widthType == WidthType.FIXED; }
+  public ProjectMemoryManager(int configuredOutputSize) {
+    super(configuredOutputSize);
+    outputColumnSizes = new HashMap<>();
+  }
 
-        public int getWidth() { return width; }
-
-    }
-
-    void ShouldNotReachHere() {
-        throw new IllegalStateException();
-    }
-
-    private void setIncomingBatch(RecordBatch recordBatch) {
-        incomingBatch = recordBatch;
-    }
-
-    private void setOutgoingBatch(ProjectRecordBatch outgoingBatch) {
-        this.outgoingBatch = outgoingBatch;
-    }
+  public boolean isComplex(MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
+  }
 
-    public ProjectMemoryManager(int configuredOutputSize) {
-        super(configuredOutputSize);
-        outputColumnSizes = new HashMap<>();
-    }
+  boolean isFixedWidth(TypedFieldId fieldId) {
+    ValueVector vv = getOutgoingValueVector(fieldId);
+    return isFixedWidth(vv);
+  }
 
-    public boolean isComplex(MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        return minorType == MinorType.MAP || minorType == MinorType.UNION || minorType == MinorType.LIST;
-    }
+  public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
+    Class<?> clazz = fieldId.getIntermediateClass();
+    int[] fieldIds = fieldId.getFieldIds();
+    return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
+  }
 
-    boolean isFixedWidth(TypedFieldId fieldId) {
-        ValueVector vv = getOutgoingValueVector(fieldId);
-        return isFixedWidth(vv);
-    }
+  static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
 
-    public ValueVector getOutgoingValueVector(TypedFieldId fieldId) {
-        Class<?> clazz = fieldId.getIntermediateClass();
-        int[] fieldIds = fieldId.getFieldIds();
-        return outgoingBatch.getValueAccessorById(clazz, fieldIds).getValueVector();
-    }
 
-    static boolean isFixedWidth(ValueVector vv) {  return (vv instanceof FixedWidthVector); }
+  static int getNetWidthOfFixedWidthType(ValueVector vv) {
+    assert isFixedWidth(vv);
+    return ((FixedWidthVector)vv).getValueWidth();
+  }
 
+  public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
+    MinorType minorType = majorType.getMinorType();
+    final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
+            || minorType == MinorType.VARBINARY);
 
-    static int getNetWidthOfFixedWidthType(ValueVector vv) {
-        assert isFixedWidth(vv);
-        return ((FixedWidthVector)vv).getValueWidth();
+    if (isVariableWidth) {
+      throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
     }
 
-    public static int getDataWidthOfFixedWidthType(TypeProtos.MajorType majorType) {
-        MinorType minorType = majorType.getMinorType();
-        final boolean isVariableWidth  = (minorType == MinorType.VARCHAR || minorType == MinorType.VAR16CHAR
-                || minorType == MinorType.VARBINARY);
-
-        if (isVariableWidth) {
-            throw new IllegalArgumentException(""getWidthOfFixedWidthType() cannot handle variable width types"");
-        }
-
-        if (minorType == MinorType.NULL) {
-            return 0;
-        }
-
-        return TypeHelper.getSize(majorType);
+    if (minorType == MinorType.NULL) {
+      return 0;
     }
 
+    return TypeHelper.getSize(majorType);
+  }
 
-    void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
-        addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
-    }
 
-    void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
-        addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
-    }
+  void addTransferField(ValueVector vvIn, String inputColumnName, String outputColumnName) {
+    addField(vvIn, null, OutputColumnType.TRANSFER, inputColumnName, outputColumnName);
+  }
 
-    void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
-                  String inputColumnName, String outputColumnName) {
-        if(isFixedWidth(vv)) {
-            addFixedWidthField(vv);
-        } else {
-            addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
-        }
-    }
+  void addNewField(ValueVector vvOut, LogicalExpression logicalExpression) {
+    addField(vvOut, logicalExpression, OutputColumnType.NEW, null, vvOut.getField().getName());
+  }
 
-    private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
-                                       OutputColumnType outputColumnType, String inputColumnName, String outputColumnName) {
-        variableWidthColumnCount++;
-        ColumnWidthInfo columnWidthInfo;
-        logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
-                printVV(vv), variableWidthColumnCount, outputColumnType);
-        //Variable width transfers
-        if(outputColumnType == OutputColumnType.TRANSFER) {
-            VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
-            columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
-        } else if (isComplex(vv.getField().getType())) {
-            addComplexField(vv);
-            return;
-        } else {
-            // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
-            OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-            OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
-            columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
-                    WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
-        }
-        ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
-        Preconditions.checkState(existingInfo == null);
+  void addField(ValueVector vv, LogicalExpression logicalExpression, OutputColumnType outputColumnType,
+                String inputColumnName, String outputColumnName) {
+    if(isFixedWidth(vv)) {
+      addFixedWidthField(vv);
+    } else {
+      addVariableWidthField(vv, logicalExpression, outputColumnType, inputColumnName, outputColumnName);
     }
-
-    public static String printVV(ValueVector vv) {
-        String str = ""null"";
-        if (vv != null) {
-            str = vv.getField().getName() + "" "" + vv.getField().getType();
-        }
-        return str;
+  }
+
+  private void addVariableWidthField(ValueVector vv, LogicalExpression logicalExpression,
+                                     OutputColumnType outputColumnType, String inputColumnName,
+                                     String outputColumnName) {
+    variableWidthColumnCount++;
+    ColumnWidthInfo columnWidthInfo;
+    logger.trace(""addVariableWidthField(): vv {} totalCount: {} outputColumnType: {}"",
+            printVV(vv), variableWidthColumnCount, outputColumnType);
+    // Variable width transfers
+    if (outputColumnType == OutputColumnType.TRANSFER) {
+      VarLenReadExpr readExpr = new VarLenReadExpr(inputColumnName);
+      columnWidthInfo = new ColumnWidthInfo(readExpr, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the RecordBatchSizer
+    } else if (isComplex(vv.getField().getType())) {
+      addComplexField(vv);
+      return;
+    } else {
+      // Walk the tree of LogicalExpressions to get a tree of OutputWidthExpressions
+      OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+      OutputWidthExpression outputWidthExpression = logicalExpression.accept(new OutputWidthVisitor(), state);
+      columnWidthInfo = new ColumnWidthInfo(outputWidthExpression, outputColumnType,
+              WidthType.VARIABLE, -1, vv); //fieldWidth has to be obtained from the OutputWidthExpression
     }
-
-    void addComplexField(ValueVector vv) {
-        //Complex types are not yet supported. Just use a guess for the size
-        assert vv == null || isComplex(vv.getField().getType());
-        complexColumnsCount++;
-        // just a guess
-        totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
-        logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+    ColumnWidthInfo existingInfo = outputColumnSizes.put(outputColumnName, columnWidthInfo);
+    Preconditions.checkState(existingInfo == null);
+  }
+
+  public static String printVV(ValueVector vv) {
+    String str = ""null"";
+    if (vv != null) {
+      str = vv.getField().getName() + "" "" + vv.getField().getType();
     }
-
-    void addFixedWidthField(ValueVector vv) {
-        assert isFixedWidth(vv);
-        fixedWidthColumnCount++;
-        int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
-        totalFixedWidthColumnWidth += fixedFieldWidth;
-        logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
-                printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+    return str;
+  }
+
+  void addComplexField(ValueVector vv) {
+    //Complex types are not yet supported. Just use a guess for the size
+    assert vv == null || isComplex(vv.getField().getType());
+    complexColumnsCount++;
+    // just a guess
+    totalComplexColumnWidth +=  OutputSizeEstimateConstants.COMPLEX_FIELD_ESTIMATE;
+    logger.trace(""addComplexField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
+            printVV(vv), complexColumnsCount, totalComplexColumnWidth);
+  }
+
+  void addFixedWidthField(ValueVector vv) {
+    assert isFixedWidth(vv);
+    fixedWidthColumnCount++;
+    int fixedFieldWidth = getNetWidthOfFixedWidthType(vv);
+    totalFixedWidthColumnWidth += fixedFieldWidth;
+    logger.trace(""addFixedWidthField(): vv {} totalCount: {} totalComplexColumnWidth: {}"",
+            printVV(vv), fixedWidthColumnCount, totalFixedWidthColumnWidth);
+  }
+
+  public void init(RecordBatch incomingBatch, ProjectRecordBatch outgoingBatch) {
+    setIncomingBatch(incomingBatch);
+    setOutgoingBatch(outgoingBatch);
+    reset();
+
+    RecordBatchStats.printConfiguredBatchSize(outgoingBatch.getRecordBatchStatsContext(),
+      getOutputBatchSize());
+  }
+
+  private void reset() {
+    rowWidth = 0;
+    totalFixedWidthColumnWidth = 0;
+    totalComplexColumnWidth = 0;
+
+    fixedWidthColumnCount = 0;
+    complexColumnsCount = 0;
+  }
+
+  @Override
+  public void update() {
+    long updateStartTime = System.currentTimeMillis();
+    RecordBatchSizer batchSizer = new RecordBatchSizer(incomingBatch);
+    long batchSizerEndTime = System.currentTimeMillis();
+
+    setRecordBatchSizer(batchSizer);
+    rowWidth = 0;
+    int totalVariableColumnWidth = 0;
+    for (String outputColumnName : outputColumnSizes.keySet()) {
+      ColumnWidthInfo columnWidthInfo = outputColumnSizes.get(outputColumnName);
+      int width = -1;
+      if (columnWidthInfo.isFixedWidth()) {
+        // fixed width columns are accumulated in totalFixedWidthColumnWidth
+        ShouldNotReachHere();
+      } else {
+        //Walk the tree of OutputWidthExpressions to get a FixedLenExpr
+        //As the tree is walked, the RecordBatchSizer and function annotations
+        //are looked-up to come up with the final FixedLenExpr
+        OutputWidthExpression savedWidthExpr = columnWidthInfo.getOutputExpression();
+        OutputWidthVisitorState state = new OutputWidthVisitorState(this);
+        OutputWidthExpression reducedExpr = savedWidthExpr.accept(new OutputWidthVisitor(), state);
+        width = ((FixedLenExpr)reducedExpr).getDataWidth();
+        Preconditions.checkState(width >= 0);
+        int metadataWidth = getMetadataWidth(columnWidthInfo.outputVV);
+        logger.trace(""update(): fieldName {} width: {} metadataWidth: {}"",
+                columnWidthInfo.outputVV.getField().getName(), width, metadataWidth);
+        width += metadataWidth;
+      }
+      totalVariableColumnWidth += width;
     }
-
-    public void init(RecordBatch incomingBatch, ProjectRecordBatch outgoingBatch) {
-        setIncomingBatch(incomingBatch);
-        setOutgoingBatch(outgoingBatch);
-        reset();
-
-        RecordBatchStats.printConfiguredBatchSize(outgoingBatch.getRecordBatchStatsContext(),
-          getOutputBatchSize());
+    rowWidth += totalFixedWidthColumnWidth;
+    rowWidth += totalComplexColumnWidth;
+    rowWidth += totalVariableColumnWidth;
+    int outPutRowCount;
+    if (rowWidth != 0) {
+      //if rowWidth is not zero, set the output row count in the sizer
+      setOutputRowCount(getOutputBatchSize(), rowWidth);
+      // if more rows can be allowed than the incoming row count, then set the
+      // output row count to the incoming row count.
+      outPutRowCount = Math.min(getOutputRowCount(), batchSizer.rowCount());
+    } else {
+      // if rowWidth == 0 then the memory manager does
+      // not have sufficient information to size the batch
+      // let the entire batch pass through.
+      // If incoming rc == 0, all RB Sizer look-ups will have
+      // 0 width and so total width can be 0
+      outPutRowCount = incomingBatch.getRecordCount();
     }
-
-    private void reset() {
-        rowWidth = 0;
-        totalFixedWidthColumnWidth = 0;
-        totalComplexColumnWidth = 0;
-
-        fixedWidthColumnCount = 0;
-        complexColumnsCount = 0;
+    setOutputRowCount(outPutRowCount);
+    long updateEndTime = System.currentTimeMillis();
+    logger.trace(""update() : Output RC {}, BatchSizer RC {}, incoming RC {}, width {}, total fixed width {}""
+                + "", total variable width {}, total complex width {}, batchSizer time {} ms, update time {}  ms""
+                + "", manager {}, incoming {}"",outPutRowCount, batchSizer.rowCount(), incomingBatch.getRecordCount(),
+                rowWidth, totalFixedWidthColumnWidth, totalVariableColumnWidth, totalComplexColumnWidth,
+                (batchSizerEndTime - updateStartTime),(updateEndTime - updateStartTime), this, incomingBatch);
+
+    RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT, getRecordBatchSizer(), outgoingBatch.getRecordBatchStatsContext());
+    updateIncomingStats();
+  }
+
+  public static int getMetadataWidth(ValueVector vv) {
+    int width = 0;
+    if (vv instanceof NullableVector) {
+      width += ((NullableVector)vv).getBitsVector().getPayloadByteCount(1);
     }
 
-    @Override
-    public void update() {
-        long updateStartTime = System.currentTimeMillis();
-        RecordBatchSizer batchSizer = new RecordBatchSizer(incomingBatch);
-        long batchSizerEndTime = System.currentTimeMillis();
-
-        setRecordBatchSizer(batchSizer);
-        rowWidth = 0;
-        int totalVariableColumnWidth = 0;
-        for (String outputColumnName : outputColumnSizes.keySet()) {
-            ColumnWidthInfo columnWidthInfo = outputColumnSizes.get(outputColumnName);
-            int width = -1;
-            if (columnWidthInfo.isFixedWidth()) {
-                // fixed width columns are accumulated in totalFixedWidthColumnWidth
-                ShouldNotReachHere();
-            } else {
-                //Walk the tree of OutputWidthExpressions to get a FixedLenExpr
-                //As the tree is walked, the RecordBatchSizer and function annotations
-                //are looked-up to come up with the final FixedLenExpr
-                OutputWidthExpression savedWidthExpr = columnWidthInfo.getOutputExpression();
-                OutputWidthVisitorState state = new OutputWidthVisitorState(this);
-                OutputWidthExpression reducedExpr = savedWidthExpr.accept(new OutputWidthVisitor(), state);
-                width = ((FixedLenExpr)reducedExpr).getDataWidth();
-                Preconditions.checkState(width >= 0);
-                int metadataWidth = getMetadataWidth(columnWidthInfo.outputVV);
-                logger.trace(""update(): fieldName {} width: {} metadataWidth: {}"",
-                        columnWidthInfo.outputVV.getField().getName(), width, metadataWidth);
-                width += metadataWidth;
-            }
-            totalVariableColumnWidth += width;
-        }
-        rowWidth += totalFixedWidthColumnWidth;
-        rowWidth += totalComplexColumnWidth;
-        rowWidth += totalVariableColumnWidth;
-        int outPutRowCount;
-        if (rowWidth != 0) {
-            //if rowWidth is not zero, set the output row count in the sizer
-            setOutputRowCount(getOutputBatchSize(), rowWidth);
-            // if more rows can be allowed than the incoming row count, then set the
-            // output row count to the incoming row count.
-            outPutRowCount = Math.min(getOutputRowCount(), batchSizer.rowCount());
-        } else {
-            // if rowWidth == 0 then the memory manager does
-            // not have sufficient information to size the batch
-            // let the entire batch pass through.
-            // If incoming rc == 0, all RB Sizer look-ups will have
-            // 0 width and so total width can be 0
-            outPutRowCount = incomingBatch.getRecordCount();
-        }
-        setOutputRowCount(outPutRowCount);
-        long updateEndTime = System.currentTimeMillis();
-        logger.trace(""update() : Output RC {}, BatchSizer RC {}, incoming RC {}, width {}, total fixed width {}""
-                    + "", total variable width {}, total complex width {}, batchSizer time {} ms, update time {}  ms""
-                    + "", manager {}, incoming {}"",outPutRowCount, batchSizer.rowCount(), incomingBatch.getRecordCount(),
-                    rowWidth, totalFixedWidthColumnWidth, totalVariableColumnWidth, totalComplexColumnWidth,
-                    (batchSizerEndTime - updateStartTime),(updateEndTime - updateStartTime), this, incomingBatch);
-
-        RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT, getRecordBatchSizer(), outgoingBatch.getRecordBatchStatsContext());
-        updateIncomingStats();
+    if (vv instanceof VariableWidthVector) {
+      width += ((VariableWidthVector)vv).getOffsetVector().getPayloadByteCount(1);","[{'comment': '```suggestion\r\n      width += ((VariableWidthVector) vv).getOffsetVector().getPayloadByteCount(1);\r\n```', 'commenter': 'ihuzenko'}]"
1945,common/src/main/java/org/apache/drill/common/types/Types.java,"@@ -906,4 +908,16 @@ public static boolean isNullable(final MajorType type) {
         throw new UnsupportedOperationException(""Unexpected/unhandled DataMode value "" + type.getMode());
     }
   }
+
+  /**
+   * The number of minor types. Actually, the largest minor type ordinal.
+   * (There are holes in the ordering.) Update this if a new type
+   * is added. Use this value when allocating arrays to be indexed
+   * by minor type.
+   *
+   * @return the maximum minor type ordinal
+   */
+  public static final int typeCount() {","[{'comment': ""This method in the most places replaces `MinorType.values().length` occurrences, and for arrays, where it is used, are indexed using `Enum.ordinal()` method, so it should work correctly, and the existing code doesn't require updating every time when a new type is added."", 'commenter': 'vvysotskyi'}, {'comment': 'I agree it seems it *looks like* it should work. However, when a I ran a test that uses the `DICT` type I got array out of bounds errors. Reason: the number of values in the types list has holes: Type 2 is missing, as are a number of others. So, when we ask how many values exist, we get something like 40. But, `DICT` has an index of 44 (because of the hole.) So, when we request the `DICT`th entry in an array, we get an OOB error.\r\n\r\nThe other approach is to introduce a value for the missing ordinals, maybe ""UNUSEDx"" type. I\'ll try that instead.', 'commenter': 'paul-rogers'}, {'comment': 'I think there is an issue with misusing `getNumber()` method in places where `ordinal()` should be used. It should guarantee that we will use indexes less than `MinorType.values().length`.', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi, if I could e-mail you a beer, or even a pizza, I would do so. You uncovered a huge mess in the Union vector and reader. It is amazing it ever worked. Adding the `DICT` type pushed it over the edge. As it turns out, the code has long confused ordinals and Protobuf values. The problem is made worse because the Probuf-generated classes replace the `valueOf()` ordinal function with a lookup based on the Protobuf values. We had the two systems all mixed up.\r\n\r\nSo, I cleaned up all that, following your advice to use only the ordinal values. Added comments to help people avoid falling into this trap in the future.\r\n\r\nPlus, I think this accidentally fixed another issue: DRILL-7510.\r\n\r\nNot bad for a single review comment!', 'commenter': 'paul-rogers'}, {'comment': 'Thank you for finding and fixing all these issues! union data type was broken for a long time, and it is great that you will fix these code-gen issues.', 'commenter': 'vvysotskyi'}]"
1945,exec/java-exec/src/main/java/org/apache/drill/exec/expr/ClassGenerator.java,"@@ -97,31 +98,37 @@
   private JVar innerClassField;
 
   /**
-   * Assumed that field has 3 indexes within the constant pull: index of the CONSTANT_Fieldref_info +
-   * CONSTANT_Fieldref_info.name_and_type_index + CONSTANT_NameAndType_info.name_index.
-   * CONSTANT_NameAndType_info.descriptor_index has limited range of values, CONSTANT_Fieldref_info.class_index is
-   * the same for a single class, they will be taken into account later.
+   * Assumed that field has 3 indexes within the constant pull: index of the
+   * CONSTANT_Fieldref_info + CONSTANT_Fieldref_info.name_and_type_index +
+   * CONSTANT_NameAndType_info.name_index.
+   * CONSTANT_NameAndType_info.descriptor_index has limited range of values,
+   * CONSTANT_Fieldref_info.class_index is the same for a single class, they
+   * will be taken into account later.
    * <p>
    * Local variable has 1 index within the constant pool.
    * {@link org.objectweb.asm.MethodWriter#visitLocalVariable(String, String, String, Label, Label, int)}
    * <p>
-   * For upper estimation of max index value, suppose that each field and local variable uses different literal
-   * values that have two indexes, then the number of occupied indexes within the constant pull is
-   * fieldCount * 3 + fieldCount * 2 + (index - fieldCount) * 3 => fieldCount * 2 + index * 3
+   * For upper estimation of max index value, suppose that each field and local
+   * variable uses different literal values that have two indexes, then the
+   * number of occupied indexes within the constant pull is fieldCount * 3 +
+   * fieldCount * 2 + (index - fieldCount) * 3 => fieldCount * 2 + index * 3
    * <p>
-   * Assumed that method has 3 indexes within the constant pull: index of the CONSTANT_Methodref_info +
-   * CONSTANT_Methodref_info.name_and_type_index + CONSTANT_NameAndType_info.name_index.
+   * Assumed that method has 3 indexes within the constant pull: index of the
+   * CONSTANT_Methodref_info + CONSTANT_Methodref_info.name_and_type_index +
+   * CONSTANT_NameAndType_info.name_index.
    * <p>
-   * For the upper estimation of number of split methods suppose that each expression in the method uses single variable.
-   * Suppose that the max number of indexes within the constant pull occupied by fields and local variables is M,
-   * the number of split methods is N, number of abstract methods in the template is A, then splitted methods count is
-   * N = (M - A * N * 3) / 50 => N = M / (50 + A * 3)
+   * For the upper estimation of number of split methods suppose that each
+   * expression in the method uses single variable. Suppose that the max number
+   * of indexes within the constant pull occupied by fields and local variables
+   * is M, the number of split methods is N, number of abstract methods in the
+   * template is A, then splitted methods count is N = (M - A * N * 3) / 50 => N","[{'comment': 'Please leave formulas to start from the new line.', 'commenter': 'vvysotskyi'}]"
1945,exec/java-exec/src/main/java/org/apache/drill/exec/expr/EvaluationVisitor.java,"@@ -400,8 +403,10 @@ public HoldingContainer visitUnknown(LogicalExpression e, ClassGenerator<?> gene
      * seedValue0 .value = seedValue;
      * </p>
      *
-     * @param e parameter expression
-     * @param generator class generator
+     * @param e","[{'comment': 'Please revert this change, initial formatting looks better.', 'commenter': 'vvysotskyi'}]"
1945,exec/java-exec/src/main/codegen/templates/TypeHelper.java,"@@ -82,28 +82,28 @@ public static JType getHolderType(JCodeModel model, MinorType type, DataMode mod
     case MAP:
     case LIST:
       return model._ref(ComplexHolder.class);
-      
+
 <#list vv.types as type>
   <#list type.minor as minor>
-      case ${minor.class?upper_case}:
-        switch (mode) {
-          case REQUIRED:
-            return model._ref(${minor.class}Holder.class);
-          case OPTIONAL:
-            return model._ref(Nullable${minor.class}Holder.class);
-          case REPEATED:
-            return model._ref(Repeated${minor.class}Holder.class);
-        }
+    case ${minor.class?upper_case}:
+      switch (mode) {
+        case REQUIRED:
+          return model._ref(${minor.class}Holder.class);
+        case OPTIONAL:
+          return model._ref(Nullable${minor.class}Holder.class);
+        case REPEATED:
+          return model._ref(Repeated${minor.class}Holder.class);
+      }
   </#list>
 </#list>
-      case GENERIC_OBJECT:
-        return model._ref(ObjectHolder.class);
+    case GENERIC_OBJECT:
+      return model._ref(ObjectHolder.class);
     case NULL:
       return model._ref(UntypedNullHolder.class);
-      default:
-        break;
-      }
-      throw new UnsupportedOperationException(buildErrorMessage(""get holder type"", type, mode));
+    default:
+      break;
+    }
+    throw new UnsupportedOperationException(buildErrorMessage(""get holder type"", type, mode));","[{'comment': 'I think the line may be located under ```default:``` inside switch-case. ', 'commenter': 'ihuzenko'}]"
1945,exec/java-exec/src/main/java/org/apache/drill/exec/expr/EvaluationVisitor.java,"@@ -146,6 +135,19 @@ public boolean equals(Object obj) {
 
   Stack<Map<ExpressionHolder,HoldingContainer>> mapStack = new Stack<>();
 
+  public EvaluationVisitor() { }","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1945,exec/java-exec/src/main/java/org/apache/drill/exec/expr/EvaluationVisitor.java,"@@ -146,6 +135,19 @@ public boolean equals(Object obj) {
 
   Stack<Map<ExpressionHolder,HoldingContainer>> mapStack = new Stack<>();
 
+  public EvaluationVisitor() { }
+
+  public HoldingContainer addExpr(LogicalExpression e, ClassGenerator<?> generator) {
+","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1945,exec/java-exec/src/main/java/org/apache/drill/exec/expr/EvaluationVisitor.java,"@@ -117,9 +106,9 @@ public HoldingContainer addExpr(LogicalExpression e, ClassGenerator<?> generator
   }
 
   private class ExpressionHolder {","[{'comment': '```suggestion\r\n  private static class ExpressionHolder {\r\n```', 'commenter': 'ihuzenko'}]"
1945,exec/java-exec/src/main/java/org/apache/drill/exec/expr/EvaluationVisitor.java,"@@ -1282,11 +1285,9 @@ public HoldingContainer visitConvertExpression(ConvertExpression e, ClassGenerat
     }
   }
 
-
-
   private class ConstantFilter extends EvalVisitor {
 
-    private Set<LogicalExpression> constantBoundaries;
+    private final Set<LogicalExpression> constantBoundaries;
 
     public ConstantFilter(Set<LogicalExpression> constantBoundaries) {
       super();","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1945,exec/vector/src/main/codegen/templates/UnionReader.java,"@@ -31,28 +34,33 @@
 /*
  * This class is generated using freemarker and the ${.template_name} template.
  */
-@SuppressWarnings(""unused"")
 public class UnionReader extends AbstractFieldReader {
 
-  private BaseReader[] readers = new BaseReader[45];
+  private static final int TYPE_COUNT = Types.typeCount();
+  private BaseReader[] readers = new BaseReader[TYPE_COUNT];
   public UnionVector data;
-  
+
   public UnionReader(UnionVector data) {
     this.data = data;
   }
 
-  private static MajorType[] TYPES = new MajorType[45];
+  private static MajorType[] TYPES = new MajorType[TYPE_COUNT];","[{'comment': 'can be final?', 'commenter': 'ihuzenko'}]"
1945,exec/vector/src/main/codegen/templates/AbstractFieldReader.java,"@@ -28,11 +28,11 @@
 /*
  * This class is generated using freemarker and the ${.template_name} template.
  */
-@SuppressWarnings(""unused"")
 public abstract class AbstractFieldReader extends AbstractBaseReader implements FieldReader {
 
-  public AbstractFieldReader() {
-  }
+  public AbstractFieldReader() { }","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1945,exec/java-exec/src/main/codegen/templates/UnionFunctions.java,"@@ -43,15 +43,15 @@
  * Additional functions can be found in the class UnionFunctions
  */
 public class GUnionFunctions {
-
   <#list vv.types as type><#list type.minor as minor><#assign name = minor.class?cap_first />
   <#assign fields = minor.fields!type.fields />
   <#assign uncappedName = name?uncap_first/>
 
   <#if !minor.class?starts_with(""Decimal"")>
-
-  @SuppressWarnings(""unused"")
-  @FunctionTemplate(name = ""IS_${name?upper_case}"", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls=NullHandling.INTERNAL)
+  @FunctionTemplate(
+      name = ""IS_${name?upper_case}"",
+      scope = FunctionTemplate.FunctionScope.SIMPLE,
+      nulls=NullHandling.INTERNAL)","[{'comment': '```suggestion\r\n      nulls = NullHandling.INTERNAL)\r\n```', 'commenter': 'vvysotskyi'}]"
1945,exec/java-exec/src/main/codegen/templates/UnionFunctions.java,"@@ -68,8 +68,10 @@ public void eval() {
     }
   }
 
-  @SuppressWarnings(""unused"")
-  @FunctionTemplate(name = ""ASSERT_${name?upper_case}"", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls=NullHandling.INTERNAL)
+  @FunctionTemplate(
+      name = ""ASSERT_${name?upper_case}"",
+      scope = FunctionTemplate.FunctionScope.SIMPLE,
+      nulls=NullHandling.INTERNAL)","[{'comment': '```suggestion\r\n      nulls = NullHandling.INTERNAL)\r\n```', 'commenter': 'vvysotskyi'}]"
1945,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java,"@@ -324,7 +323,12 @@ protected DrillRel convertToDrel(RelNode relNode) throws SqlUnsupportedException
   /**
    * A shuttle designed to finalize all RelNodes.
    */
-  private static class PrelFinalizer extends RelShuttleImpl {
+  public static class PrelFinalizer extends RelShuttleImpl {
+
+    // The non-default constructor appears to be called from Calcite
+    // code when processing Hive-based queries.
+    public PrelFinalizer(DefaultSqlHandler handler) { }","[{'comment': ""Could you please explain it a little? I thought this is a regular visitor which is used only in places where its instance is created and used. By the way, since it doesn't store any state, its instance may be reused."", 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi, not sure I can explain this. I got a runtime exception that some code could not create an instance. The error message appeared to indicate that the code was called via introspection, with the above signature. Adding the above code made the problem disappear.\r\n\r\nI should have noted the test that failed when run in Eclipse.From the comment, it seemed to be from some of the Hive tests I ran. But, today, those tests run fine without this addition. So, I removed it.', 'commenter': 'paul-rogers'}]"
1945,logical/src/main/java/org/apache/drill/common/expression/FunctionHolderExpression.java,"@@ -21,31 +21,37 @@
 
 import org.apache.drill.common.expression.fn.FuncHolder;
 import org.apache.drill.common.expression.visitors.ExprVisitor;
-
 import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
 
+/**
+ * Represents an actual call (a reference) to a declared function.
+ * Holds the name used (functions can have multiple aliases), the
+ * function declaration, and the actual argument expressions used
+ * in this call. This might be better named
+ * <code>FunctionCallExpression</code> as it represents a use
+ * of a function. Subclasses hold references to the declaration
+ * depending on the type (Drill, Hive) of the function.
+ */
 public abstract class FunctionHolderExpression extends LogicalExpressionBase {
   public final ImmutableList<LogicalExpression> args;
   public final String nameUsed;
 
   /**
-   * A field reference identifies the output field and
-   * is used to reference that field in the generated classes.
+   * Identifies the output field. References that field in the
+   * generated classes.
    */
   private FieldReference fieldReference;
 
   public FunctionHolderExpression(String nameUsed, ExpressionPosition pos, List<LogicalExpression> args) {
     super(pos);
+    this.nameUsed = nameUsed;
     if (args == null) {
-      args = Lists.newArrayList();
-    }
-
-    if (!(args instanceof ImmutableList)) {
-      args = ImmutableList.copyOf(args);
+      this.args = ImmutableList.of();
+    } else if (args instanceof ImmutableList) {","[{'comment': 'Could you please replace these if blocks with `ImmutableList.copyOf`, it produces these checks internally to avoid copying is possible.', 'commenter': 'vvysotskyi'}]"
1945,logical/src/main/java/org/apache/drill/common/expression/FunctionHolderExpression.java,"@@ -98,4 +105,20 @@ public FieldReference getFieldReference() {
   public void setFieldReference(FieldReference fieldReference) {
     this.fieldReference = fieldReference;
   }
+
+  @Override
+  public String toString() {
+    StringBuilder buf = new StringBuilder()
+        .append(""["").append(getClass().getSimpleName())","[{'comment': 'Please separate somehow separate `getClass().getSimpleName()` and `nameUsed` in resulting string.', 'commenter': 'vvysotskyi'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -0,0 +1,368 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.Schema;
+import org.apache.avro.file.DataFileReader;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericContainer;
+import org.apache.avro.generic.GenericDatumReader;
+import org.apache.avro.generic.GenericFixed;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.mapred.FsInput;
+import org.apache.avro.util.Utf8;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.DictWriter;
+import org.apache.drill.exec.vector.accessor.ObjectWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.joda.time.DateTimeConstants;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+
+public class AvroBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(AvroBatchReader.class);
+
+  // currently config is unused but maybe used later
+  private final AvroReaderConfig config;
+
+  private Path filePath;
+  private long endPosition;
+  private DataFileReader<GenericContainer> reader;
+  private ResultSetLoader loader;
+  // re-use container instance
+  private GenericContainer container = null;","[{'comment': 'Is it possible to use `GenericRecord` type here, so we would avoid casts in `nextLine()` method?', 'commenter': 'vvysotskyi'}, {'comment': 'Good point, replace with record.', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/test/java/org/apache/drill/exec/store/avro/AvroDataGenerator.java,"@@ -0,0 +1,819 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.LogicalType;
+import org.apache.avro.LogicalTypes;
+import org.apache.avro.Schema;
+import org.apache.avro.Schema.Type;
+import org.apache.avro.SchemaBuilder;
+import org.apache.avro.file.DataFileWriter;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericData;
+import org.apache.avro.generic.GenericDatumWriter;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.drill.exec.util.JsonStringArrayList;
+import org.apache.drill.exec.util.JsonStringHashMap;
+import org.apache.drill.exec.util.Text;
+import org.apache.drill.test.BaseDirTestWatcher;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.IOException;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.file.Paths;
+import java.time.LocalDateTime;
+import java.time.ZoneOffset;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+/**
+ * Utilities for generating Avro test data.
+ */
+public class AvroDataGenerator {
+
+  public static final int RECORD_COUNT = 50;
+  public static int ARRAY_SIZE = 4;
+
+  private final BaseDirTestWatcher dirTestWatcher;
+
+  public AvroDataGenerator(BaseDirTestWatcher dirTestWatcher) {
+    this.dirTestWatcher = dirTestWatcher;
+  }
+
+  /**
+   * Class to write records to an Avro file while simultaneously
+   * constructing a corresponding list of records in the format taken in
+   * by the Drill test builder to describe expected results.
+   */
+  public static class AvroTestRecordWriter implements Closeable {
+
+    private final List<Map<String, Object>> expectedRecords;
+    private final Schema schema;
+    private final DataFileWriter<GenericData.Record> writer;
+    private final String filePath;
+    private final String fileName;
+
+    private GenericData.Record currentAvroRecord;
+    private Map<String, Object> currentExpectedRecord;
+
+    public AvroTestRecordWriter(Schema schema, File file) {
+      writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));
+      try {
+        writer.create(schema, file);
+      } catch (IOException e) {
+        throw new RuntimeException(""Error creating file in Avro test setup."", e);
+      }
+      this.schema = schema;
+      currentExpectedRecord = new TreeMap<>();
+      expectedRecords = new ArrayList<>();
+      filePath = file.getAbsolutePath();
+      fileName = file.getName();
+    }
+
+    public void startRecord() {
+      currentAvroRecord = new GenericData.Record(schema);
+      currentExpectedRecord = new TreeMap<>();
+    }
+
+    public void put(String key, Object value) {
+      currentAvroRecord.put(key, value);
+      // convert binary values into byte[], the format they will be given
+      // in the Drill result set in the test framework
+      currentExpectedRecord.put(""`"" + key + ""`"", convertAvroValToDrill(value, true));
+    }
+
+    // TODO - fix this the test wrapper to prevent the need for this hack
+    // to make the root behave differently than nested fields for String vs. Text
+    private Object convertAvroValToDrill(Object value, boolean root) {
+      if (value instanceof ByteBuffer) {
+        ByteBuffer bb = ((ByteBuffer)value);","[{'comment': '```suggestion\r\n        ByteBuffer bb = ((ByteBuffer) value);\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/test/java/org/apache/drill/exec/store/avro/AvroDataGenerator.java,"@@ -0,0 +1,819 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.LogicalType;
+import org.apache.avro.LogicalTypes;
+import org.apache.avro.Schema;
+import org.apache.avro.Schema.Type;
+import org.apache.avro.SchemaBuilder;
+import org.apache.avro.file.DataFileWriter;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericData;
+import org.apache.avro.generic.GenericDatumWriter;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.drill.exec.util.JsonStringArrayList;
+import org.apache.drill.exec.util.JsonStringHashMap;
+import org.apache.drill.exec.util.Text;
+import org.apache.drill.test.BaseDirTestWatcher;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.IOException;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.file.Paths;
+import java.time.LocalDateTime;
+import java.time.ZoneOffset;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+/**
+ * Utilities for generating Avro test data.
+ */
+public class AvroDataGenerator {
+
+  public static final int RECORD_COUNT = 50;
+  public static int ARRAY_SIZE = 4;
+
+  private final BaseDirTestWatcher dirTestWatcher;
+
+  public AvroDataGenerator(BaseDirTestWatcher dirTestWatcher) {
+    this.dirTestWatcher = dirTestWatcher;
+  }
+
+  /**
+   * Class to write records to an Avro file while simultaneously
+   * constructing a corresponding list of records in the format taken in
+   * by the Drill test builder to describe expected results.
+   */
+  public static class AvroTestRecordWriter implements Closeable {
+
+    private final List<Map<String, Object>> expectedRecords;
+    private final Schema schema;
+    private final DataFileWriter<GenericData.Record> writer;
+    private final String filePath;
+    private final String fileName;
+
+    private GenericData.Record currentAvroRecord;
+    private Map<String, Object> currentExpectedRecord;
+
+    public AvroTestRecordWriter(Schema schema, File file) {
+      writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));
+      try {
+        writer.create(schema, file);
+      } catch (IOException e) {
+        throw new RuntimeException(""Error creating file in Avro test setup."", e);
+      }
+      this.schema = schema;
+      currentExpectedRecord = new TreeMap<>();
+      expectedRecords = new ArrayList<>();
+      filePath = file.getAbsolutePath();
+      fileName = file.getName();
+    }
+
+    public void startRecord() {
+      currentAvroRecord = new GenericData.Record(schema);
+      currentExpectedRecord = new TreeMap<>();
+    }
+
+    public void put(String key, Object value) {
+      currentAvroRecord.put(key, value);
+      // convert binary values into byte[], the format they will be given
+      // in the Drill result set in the test framework
+      currentExpectedRecord.put(""`"" + key + ""`"", convertAvroValToDrill(value, true));
+    }
+
+    // TODO - fix this the test wrapper to prevent the need for this hack
+    // to make the root behave differently than nested fields for String vs. Text
+    private Object convertAvroValToDrill(Object value, boolean root) {
+      if (value instanceof ByteBuffer) {
+        ByteBuffer bb = ((ByteBuffer)value);
+        byte[] drillVal = new byte[((ByteBuffer)value).remaining()];","[{'comment': '```suggestion\r\n        byte[] drillVal = new byte[((ByteBuffer) value).remaining()];\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroDrillTable.java,"@@ -1,197 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * ""License""); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.drill.exec.store.avro;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.avro.LogicalType;
-import org.apache.avro.Schema;
-import org.apache.avro.Schema.Field;
-import org.apache.avro.file.DataFileReader;
-import org.apache.avro.generic.GenericContainer;
-import org.apache.avro.generic.GenericDatumReader;
-import org.apache.avro.mapred.FsInput;
-import org.apache.calcite.rel.type.RelDataType;
-import org.apache.calcite.rel.type.RelDataTypeFactory;
-import org.apache.calcite.sql.type.SqlTypeName;
-import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.exec.planner.logical.DrillTable;
-import org.apache.drill.exec.planner.logical.ExtendableRelDataType;
-import org.apache.drill.exec.planner.types.ExtendableRelDataTypeHolder;
-import org.apache.drill.exec.store.ColumnExplorer;
-import org.apache.drill.exec.store.SchemaConfig;
-import org.apache.drill.exec.store.dfs.FileSystemPlugin;
-import org.apache.drill.exec.store.dfs.FormatSelection;
-import org.apache.hadoop.fs.Path;
-
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
-
-public class AvroDrillTable extends DrillTable {
-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AvroDrillTable.class);
-
-  private final DataFileReader<GenericContainer> reader;
-  private final SchemaConfig schemaConfig;
-  private ExtendableRelDataTypeHolder holder;","[{'comment': 'Could you please also remove `ExtendableRelDataTypeHolder`, `ExtendableRelDataType` classes and `DrillValidator.addToSelectList()` method added in DRILL-4120 sine they are not needed with these changes.', 'commenter': 'vvysotskyi'}, {'comment': 'Thanks for pointing to this. Removed.', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -0,0 +1,368 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.Schema;
+import org.apache.avro.file.DataFileReader;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericContainer;
+import org.apache.avro.generic.GenericDatumReader;
+import org.apache.avro.generic.GenericFixed;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.mapred.FsInput;
+import org.apache.avro.util.Utf8;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.DictWriter;
+import org.apache.drill.exec.vector.accessor.ObjectWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.joda.time.DateTimeConstants;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+
+public class AvroBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(AvroBatchReader.class);
+
+  // currently config is unused but maybe used later
+  private final AvroReaderConfig config;
+
+  private Path filePath;
+  private long endPosition;
+  private DataFileReader<GenericContainer> reader;
+  private ResultSetLoader loader;
+  // re-use container instance
+  private GenericContainer container = null;
+
+  public AvroBatchReader(AvroReaderConfig config) {
+    this.config = config;
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    FileSplit split = negotiator.split();
+    filePath = split.getPath();
+
+    // Avro files are splittable, define reading start / end positions
+    long startPosition = split.getStart();
+    endPosition = startPosition + split.getLength();
+
+    logger.debug(""Processing Avro file: {}, start position: {}, end position: {}"",
+      filePath, startPosition, endPosition);
+
+    reader = prepareReader(split, negotiator.fileSystem(),
+      negotiator.userName(), negotiator.context().getFragmentContext().getQueryUserName());
+
+    logger.debug(""Avro file schema: {}"", reader.getSchema());
+    TupleMetadata schema = AvroSchemaUtil.convert(reader.getSchema());
+    logger.debug(""Avro file converted schema: {}"", schema);
+    negotiator.setTableSchema(schema, true);
+    loader = negotiator.build();
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!nextLine(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      reader.close();
+    } catch (IOException e) {
+      logger.warn(""Error closing Avro reader: {}"", e.getMessage(), e);
+    } finally {
+      reader = null;
+    }
+  }
+
+  @Override
+  public String toString() {
+    long currentPosition = -1L;
+    try {
+      if (reader != null) {
+        currentPosition = reader.tell();
+      }
+    } catch (IOException e) {
+      logger.trace(""Unable to obtain Avro reader position: {}"", e.getMessage(), e);
+    }
+    return ""AvroBatchReader[File="" + filePath
+      + "", Position="" + currentPosition
+      + ""]"";
+  }
+
+  /**
+   * Initialized Avro data reader based on given file system and file path.
+   * Moves reader to the sync point from where to start reading the data.
+   *
+   * @param fileSplit file split
+   * @param fs file system
+   * @param opUserName name of the user whom to impersonate while reading the data
+   * @param queryUserName name of the user who issues the query
+   * @return Avro file reader
+   */
+  private DataFileReader<GenericContainer> prepareReader(FileSplit fileSplit, FileSystem fs, String opUserName, String queryUserName) {
+    try {
+      UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      DataFileReader<GenericContainer> reader = ugi.doAs((PrivilegedExceptionAction<DataFileReader<GenericContainer>>) () ->
+        new DataFileReader<>(new FsInput(fileSplit.getPath(), fs.getConf()), new GenericDatumReader<GenericContainer>()));
+
+      // move to sync point from where to read the file
+      reader.sync(fileSplit.getStart());
+      return reader;
+    } catch (IOException | InterruptedException e) {
+      throw UserException.dataReadError(e)
+        .message(""Error preparing Avro reader"")
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+  }
+
+  private boolean nextLine(RowSetLoader rowWriter) {
+    try {
+      if (!reader.hasNext() || reader.pastSync(endPosition)) {
+        return false;
+      }
+      container = reader.next(container);
+    } catch (IOException e) {
+      throw UserException.dataReadError(e)
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    Schema schema = container.getSchema();
+    GenericRecord record = (GenericRecord) container;
+
+    if (Schema.Type.RECORD != schema.getType()) {
+      throw UserException.dataReadError()
+        .message(""Root object must be record type. Found: %s"", schema.getType())
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    rowWriter.start();
+    List<Schema.Field> fields = schema.getFields();
+    for (Schema.Field field : fields) {
+      String fieldName = field.name();
+      Object value = record.get(fieldName);
+      ObjectWriter writer = rowWriter.column(fieldName);
+      processRecord(writer, value, field.schema());
+    }","[{'comment': 'This code is fine; but perhaps slow. It does two hash map lookups per column: one for Avro, another fir the row writer.\r\n\r\nTuples are often defined to be accessed both by name and index. The rowwriter supports this model. I wonder if we can use indexes with Avro?\r\n\r\nLooking at the Avro [GenericData](https://github.com/apache/avro/blob/master/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java) implementation, there is a `get(int index)` method for indexed access.\r\n\r\nBut, are the column positions the same on each record? Looking at the implementation of `get(String key) `, it seems that Avro does with the row writer does: looks up the column name in the schema, and uses the schema position for column access. This means that columns must have the same position in each row.\r\n\r\nThis means we can implement the above something like:\r\n\r\n```\r\nfor (int i = o; i < rowWriter.size(); i++) {\r\n  processRecord(rowWriter.writer(i), record.get(i), fields.get(i));\r\n}\r\n```', 'commenter': 'paul-rogers'}, {'comment': 'Good point, replaced with index access. Thanks.', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -0,0 +1,368 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.Schema;
+import org.apache.avro.file.DataFileReader;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericContainer;
+import org.apache.avro.generic.GenericDatumReader;
+import org.apache.avro.generic.GenericFixed;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.mapred.FsInput;
+import org.apache.avro.util.Utf8;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.DictWriter;
+import org.apache.drill.exec.vector.accessor.ObjectWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.joda.time.DateTimeConstants;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+
+public class AvroBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(AvroBatchReader.class);
+
+  // currently config is unused but maybe used later
+  private final AvroReaderConfig config;
+
+  private Path filePath;
+  private long endPosition;
+  private DataFileReader<GenericContainer> reader;
+  private ResultSetLoader loader;
+  // re-use container instance
+  private GenericContainer container = null;
+
+  public AvroBatchReader(AvroReaderConfig config) {
+    this.config = config;
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    FileSplit split = negotiator.split();
+    filePath = split.getPath();
+
+    // Avro files are splittable, define reading start / end positions
+    long startPosition = split.getStart();
+    endPosition = startPosition + split.getLength();
+
+    logger.debug(""Processing Avro file: {}, start position: {}, end position: {}"",
+      filePath, startPosition, endPosition);
+
+    reader = prepareReader(split, negotiator.fileSystem(),
+      negotiator.userName(), negotiator.context().getFragmentContext().getQueryUserName());
+
+    logger.debug(""Avro file schema: {}"", reader.getSchema());
+    TupleMetadata schema = AvroSchemaUtil.convert(reader.getSchema());
+    logger.debug(""Avro file converted schema: {}"", schema);
+    negotiator.setTableSchema(schema, true);
+    loader = negotiator.build();
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!nextLine(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      reader.close();
+    } catch (IOException e) {
+      logger.warn(""Error closing Avro reader: {}"", e.getMessage(), e);
+    } finally {
+      reader = null;
+    }
+  }
+
+  @Override
+  public String toString() {
+    long currentPosition = -1L;
+    try {
+      if (reader != null) {
+        currentPosition = reader.tell();
+      }
+    } catch (IOException e) {
+      logger.trace(""Unable to obtain Avro reader position: {}"", e.getMessage(), e);
+    }
+    return ""AvroBatchReader[File="" + filePath
+      + "", Position="" + currentPosition
+      + ""]"";
+  }
+
+  /**
+   * Initialized Avro data reader based on given file system and file path.
+   * Moves reader to the sync point from where to start reading the data.
+   *
+   * @param fileSplit file split
+   * @param fs file system
+   * @param opUserName name of the user whom to impersonate while reading the data
+   * @param queryUserName name of the user who issues the query
+   * @return Avro file reader
+   */
+  private DataFileReader<GenericContainer> prepareReader(FileSplit fileSplit, FileSystem fs, String opUserName, String queryUserName) {
+    try {
+      UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      DataFileReader<GenericContainer> reader = ugi.doAs((PrivilegedExceptionAction<DataFileReader<GenericContainer>>) () ->
+        new DataFileReader<>(new FsInput(fileSplit.getPath(), fs.getConf()), new GenericDatumReader<GenericContainer>()));
+
+      // move to sync point from where to read the file
+      reader.sync(fileSplit.getStart());
+      return reader;
+    } catch (IOException | InterruptedException e) {
+      throw UserException.dataReadError(e)
+        .message(""Error preparing Avro reader"")
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+  }
+
+  private boolean nextLine(RowSetLoader rowWriter) {
+    try {
+      if (!reader.hasNext() || reader.pastSync(endPosition)) {
+        return false;
+      }
+      container = reader.next(container);
+    } catch (IOException e) {
+      throw UserException.dataReadError(e)
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    Schema schema = container.getSchema();
+    GenericRecord record = (GenericRecord) container;
+
+    if (Schema.Type.RECORD != schema.getType()) {
+      throw UserException.dataReadError()
+        .message(""Root object must be record type. Found: %s"", schema.getType())
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    rowWriter.start();
+    List<Schema.Field> fields = schema.getFields();
+    for (Schema.Field field : fields) {
+      String fieldName = field.name();
+      Object value = record.get(fieldName);
+      ObjectWriter writer = rowWriter.column(fieldName);
+      processRecord(writer, value, field.schema());
+    }
+    rowWriter.save();
+    return true;
+  }
+
+  private void processRecord(ObjectWriter writer, Object value, Schema schema) {","[{'comment': 'This approach obviously works, but it requires a type check per column value. We just noted that the schema for every row is the same. So, we can push the per-column checks so that they are done once at file open time rather than on the inner-loop on each column value.\r\n\r\nOne way to accomplish that is to provide a `ColumnConverter` interface along with implementations for each type of conversion. So, we might have a `UnionConverter`, a `XConverter` for each scalar type X, and so on. Each would hold its writer and index. This way, the above loop becomes:\r\n\r\n```\r\nfor (int i = o; i < rowWriter.size(); i++) {\r\n  converters[i].convert(record.get(i));\r\n}\r\n```\r\n\r\nCharles uses something similar in the PCAP and other readers. And, in fact, we advocate an earlier (non-EVF) version of the same idea in the Drill book.', 'commenter': 'paul-rogers'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -0,0 +1,368 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.Schema;
+import org.apache.avro.file.DataFileReader;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericContainer;
+import org.apache.avro.generic.GenericDatumReader;
+import org.apache.avro.generic.GenericFixed;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.mapred.FsInput;
+import org.apache.avro.util.Utf8;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.DictWriter;
+import org.apache.drill.exec.vector.accessor.ObjectWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.joda.time.DateTimeConstants;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+
+public class AvroBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(AvroBatchReader.class);
+
+  // currently config is unused but maybe used later
+  private final AvroReaderConfig config;
+
+  private Path filePath;
+  private long endPosition;
+  private DataFileReader<GenericContainer> reader;
+  private ResultSetLoader loader;
+  // re-use container instance
+  private GenericContainer container = null;
+
+  public AvroBatchReader(AvroReaderConfig config) {
+    this.config = config;
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    FileSplit split = negotiator.split();
+    filePath = split.getPath();
+
+    // Avro files are splittable, define reading start / end positions
+    long startPosition = split.getStart();
+    endPosition = startPosition + split.getLength();
+
+    logger.debug(""Processing Avro file: {}, start position: {}, end position: {}"",
+      filePath, startPosition, endPosition);
+
+    reader = prepareReader(split, negotiator.fileSystem(),
+      negotiator.userName(), negotiator.context().getFragmentContext().getQueryUserName());
+
+    logger.debug(""Avro file schema: {}"", reader.getSchema());
+    TupleMetadata schema = AvroSchemaUtil.convert(reader.getSchema());
+    logger.debug(""Avro file converted schema: {}"", schema);
+    negotiator.setTableSchema(schema, true);
+    loader = negotiator.build();
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!nextLine(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      reader.close();
+    } catch (IOException e) {
+      logger.warn(""Error closing Avro reader: {}"", e.getMessage(), e);
+    } finally {
+      reader = null;
+    }
+  }
+
+  @Override
+  public String toString() {
+    long currentPosition = -1L;
+    try {
+      if (reader != null) {
+        currentPosition = reader.tell();
+      }
+    } catch (IOException e) {
+      logger.trace(""Unable to obtain Avro reader position: {}"", e.getMessage(), e);
+    }
+    return ""AvroBatchReader[File="" + filePath
+      + "", Position="" + currentPosition
+      + ""]"";
+  }
+
+  /**
+   * Initialized Avro data reader based on given file system and file path.
+   * Moves reader to the sync point from where to start reading the data.
+   *
+   * @param fileSplit file split
+   * @param fs file system
+   * @param opUserName name of the user whom to impersonate while reading the data
+   * @param queryUserName name of the user who issues the query
+   * @return Avro file reader
+   */
+  private DataFileReader<GenericContainer> prepareReader(FileSplit fileSplit, FileSystem fs, String opUserName, String queryUserName) {
+    try {
+      UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      DataFileReader<GenericContainer> reader = ugi.doAs((PrivilegedExceptionAction<DataFileReader<GenericContainer>>) () ->
+        new DataFileReader<>(new FsInput(fileSplit.getPath(), fs.getConf()), new GenericDatumReader<GenericContainer>()));
+
+      // move to sync point from where to read the file
+      reader.sync(fileSplit.getStart());
+      return reader;
+    } catch (IOException | InterruptedException e) {
+      throw UserException.dataReadError(e)
+        .message(""Error preparing Avro reader"")
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+  }
+
+  private boolean nextLine(RowSetLoader rowWriter) {
+    try {
+      if (!reader.hasNext() || reader.pastSync(endPosition)) {
+        return false;
+      }
+      container = reader.next(container);
+    } catch (IOException e) {
+      throw UserException.dataReadError(e)
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    Schema schema = container.getSchema();
+    GenericRecord record = (GenericRecord) container;
+
+    if (Schema.Type.RECORD != schema.getType()) {
+      throw UserException.dataReadError()
+        .message(""Root object must be record type. Found: %s"", schema.getType())
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    rowWriter.start();
+    List<Schema.Field> fields = schema.getFields();
+    for (Schema.Field field : fields) {
+      String fieldName = field.name();
+      Object value = record.get(fieldName);
+      ObjectWriter writer = rowWriter.column(fieldName);
+      processRecord(writer, value, field.schema());
+    }
+    rowWriter.save();
+    return true;
+  }
+
+  private void processRecord(ObjectWriter writer, Object value, Schema schema) {
+    // skip processing record if it is null or is not projected
+    if (value == null || !writer.isProjected()) {
+      return;
+    }
+
+    switch (schema.getType()) {
+      case UNION:
+        processRecord(writer, value, AvroSchemaUtil.extractSchemaFromNullable(schema, writer.schema().name()));
+        break;
+      case RECORD:
+        TupleWriter tupleWriter = writer.tuple();
+
+        if (tupleWriter.tupleSchema().isEmpty()) {","[{'comment': 'Is this known up front? Can we build up the nested structure when we build the Drill-side schema?', 'commenter': 'paul-rogers'}, {'comment': ""For most of the cases yes and we build, we cannot build it for the case when recursive named record types are present (I believe it's a rare use-case but Avro supports this)."", 'commenter': 'arina-ielchiieva'}, {'comment': ""Do you mean nested record types? Wouldn't those translate to Drill STRUCT (MAP)s? Looking at [this spec](https://avro.apache.org/docs/current/spec.html#schemas), it would seen that an Avro record maps to a Drill tuple (that is, a top-level record or a MAP). Or am I missing something?"", 'commenter': 'paul-rogers'}, {'comment': 'No, I mean records that refer to named records that are not constructed yet.\r\nRegular, maps / nested map are constructed easily, since we know the structure beforehand:\r\nhttps://github.com/arina-ielchiieva/drill/blob/DRILL-7454/exec/java-exec/src/test/java/org/apache/drill/exec/store/avro/AvroSchemaUtilTest.java#L246\r\n\r\nThe case I am referring to is a little bit more complex, in Avro documentation they just provide one example of it:\r\n```\r\nFor example, a linked-list of 64-bit values may be defined with:\r\n{\r\n  ""type"": ""record"",\r\n  ""name"": ""LongList"",\r\n  ""aliases"": [""LinkedLongs""],                      // old name for this\r\n  ""fields"" : [\r\n    {""name"": ""value"", ""type"": ""long""},             // each element has a long\r\n    {""name"": ""next"", ""type"": [""null"", ""LongList""]} // optional next element\r\n  ]\r\n}\r\n```\r\nSimply put they allow a field of record type to refer to another record by name. The problem occurs when one record type refers to another which is not yet constructed, moreover it cannot be constructed until upper type is constructed.\r\nSome more complicated schema examples: https://github.com/arina-ielchiieva/drill/blob/DRILL-7454/exec/java-exec/src/test/java/org/apache/drill/exec/store/avro/AvroSchemaUtilTest.java#L343\r\n\r\nFor these cases, I put off such type construction till the runtime when exact schema is know, simply adding map without fields.\r\nExplanation in the code: https://github.com/arina-ielchiieva/drill/blob/DRILL-7454/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroSchemaUtil.java#L118\r\n\r\nIn reality, I believe it is a rare case but we had unit test in Drill so I had to cover this case as well.\r\n\r\n', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -0,0 +1,368 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.Schema;
+import org.apache.avro.file.DataFileReader;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericContainer;
+import org.apache.avro.generic.GenericDatumReader;
+import org.apache.avro.generic.GenericFixed;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.mapred.FsInput;
+import org.apache.avro.util.Utf8;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.DictWriter;
+import org.apache.drill.exec.vector.accessor.ObjectWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.joda.time.DateTimeConstants;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+
+public class AvroBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(AvroBatchReader.class);
+
+  // currently config is unused but maybe used later
+  private final AvroReaderConfig config;
+
+  private Path filePath;
+  private long endPosition;
+  private DataFileReader<GenericContainer> reader;
+  private ResultSetLoader loader;
+  // re-use container instance
+  private GenericContainer container = null;
+
+  public AvroBatchReader(AvroReaderConfig config) {
+    this.config = config;
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    FileSplit split = negotiator.split();
+    filePath = split.getPath();
+
+    // Avro files are splittable, define reading start / end positions
+    long startPosition = split.getStart();
+    endPosition = startPosition + split.getLength();
+
+    logger.debug(""Processing Avro file: {}, start position: {}, end position: {}"",
+      filePath, startPosition, endPosition);
+
+    reader = prepareReader(split, negotiator.fileSystem(),
+      negotiator.userName(), negotiator.context().getFragmentContext().getQueryUserName());
+
+    logger.debug(""Avro file schema: {}"", reader.getSchema());
+    TupleMetadata schema = AvroSchemaUtil.convert(reader.getSchema());
+    logger.debug(""Avro file converted schema: {}"", schema);
+    negotiator.setTableSchema(schema, true);
+    loader = negotiator.build();
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!nextLine(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      reader.close();
+    } catch (IOException e) {
+      logger.warn(""Error closing Avro reader: {}"", e.getMessage(), e);
+    } finally {
+      reader = null;
+    }
+  }
+
+  @Override
+  public String toString() {
+    long currentPosition = -1L;
+    try {
+      if (reader != null) {
+        currentPosition = reader.tell();
+      }
+    } catch (IOException e) {
+      logger.trace(""Unable to obtain Avro reader position: {}"", e.getMessage(), e);
+    }
+    return ""AvroBatchReader[File="" + filePath
+      + "", Position="" + currentPosition
+      + ""]"";
+  }
+
+  /**
+   * Initialized Avro data reader based on given file system and file path.
+   * Moves reader to the sync point from where to start reading the data.
+   *
+   * @param fileSplit file split
+   * @param fs file system
+   * @param opUserName name of the user whom to impersonate while reading the data
+   * @param queryUserName name of the user who issues the query
+   * @return Avro file reader
+   */
+  private DataFileReader<GenericContainer> prepareReader(FileSplit fileSplit, FileSystem fs, String opUserName, String queryUserName) {
+    try {
+      UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      DataFileReader<GenericContainer> reader = ugi.doAs((PrivilegedExceptionAction<DataFileReader<GenericContainer>>) () ->
+        new DataFileReader<>(new FsInput(fileSplit.getPath(), fs.getConf()), new GenericDatumReader<GenericContainer>()));
+
+      // move to sync point from where to read the file
+      reader.sync(fileSplit.getStart());
+      return reader;
+    } catch (IOException | InterruptedException e) {
+      throw UserException.dataReadError(e)
+        .message(""Error preparing Avro reader"")
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+  }
+
+  private boolean nextLine(RowSetLoader rowWriter) {
+    try {
+      if (!reader.hasNext() || reader.pastSync(endPosition)) {
+        return false;
+      }
+      container = reader.next(container);
+    } catch (IOException e) {
+      throw UserException.dataReadError(e)
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    Schema schema = container.getSchema();
+    GenericRecord record = (GenericRecord) container;
+
+    if (Schema.Type.RECORD != schema.getType()) {
+      throw UserException.dataReadError()
+        .message(""Root object must be record type. Found: %s"", schema.getType())
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    rowWriter.start();
+    List<Schema.Field> fields = schema.getFields();
+    for (Schema.Field field : fields) {
+      String fieldName = field.name();
+      Object value = record.get(fieldName);
+      ObjectWriter writer = rowWriter.column(fieldName);
+      processRecord(writer, value, field.schema());
+    }
+    rowWriter.save();
+    return true;
+  }
+
+  private void processRecord(ObjectWriter writer, Object value, Schema schema) {
+    // skip processing record if it is null or is not projected
+    if (value == null || !writer.isProjected()) {
+      return;
+    }
+
+    switch (schema.getType()) {
+      case UNION:
+        processRecord(writer, value, AvroSchemaUtil.extractSchemaFromNullable(schema, writer.schema().name()));
+        break;
+      case RECORD:
+        TupleWriter tupleWriter = writer.tuple();
+
+        if (tupleWriter.tupleSchema().isEmpty()) {
+          // fill in tuple schema for cases when there recursive named record types are present
+          TupleMetadata recordSchema = AvroSchemaUtil.convert(schema);
+          recordSchema.toMetadataList().forEach(tupleWriter::addColumn);
+        }
+
+        GenericRecord genericRecord = (GenericRecord) value;
+        schema.getFields().forEach(
+          field -> processRecord(tupleWriter.column(field.name()), genericRecord.get(field.name()), field.schema())
+        );
+        break;
+      case ARRAY:
+        ArrayWriter arrayWriter = writer.array();
+        GenericArray<?> array = (GenericArray<?>) value;
+        ObjectWriter entryWriter = arrayWriter.entry();
+        for (Object arrayValue : array) {
+          processRecord(entryWriter, arrayValue, array.getSchema().getElementType());
+          arrayWriter.save();
+        }
+        break;
+      case MAP:
+        @SuppressWarnings(""unchecked"")
+        Map<Object, Object> map = (Map<Object, Object>) value;
+        Schema valueSchema = schema.getValueType();
+
+        DictWriter dictWriter = writer.dict();
+        ScalarWriter keyWriter = dictWriter.keyWriter();
+        ObjectWriter valueWriter = dictWriter.valueWriter();
+
+        for (Map.Entry<Object, Object> mapEntry : map.entrySet()) {
+          processScalar(keyWriter, mapEntry.getKey());
+          processRecord(valueWriter, mapEntry.getValue(), valueSchema);
+          dictWriter.save();
+        }
+        break;
+      default:
+        try {
+          ScalarWriter scalarWriter = writer.scalar();
+          processScalar(scalarWriter, value);
+        } catch (UnsupportedOperationException e) {
+          throw UserException.dataReadError(e)
+            .message(""Unexpected writer type '%s', expected scalar"", writer.type())
+            .addContext(""Reader"", this)
+            .build(logger);
+        }
+    }
+  }
+
+  private void processScalar(ScalarWriter scalarWriter, Object value) {
+    ColumnMetadata columnMetadata = scalarWriter.schema();
+    switch (columnMetadata.type()) {
+      case INT:
+        scalarWriter.setInt((int) value);
+        break;
+      case BIGINT:
+        scalarWriter.setLong((long) value);","[{'comment': 'Where the Avro-provided Java object is of the type that the scalar writer wants, we can just call `setValue(Object value)` which will do the required casting for you.', 'commenter': 'paul-rogers'}, {'comment': 'Well, most of the scalar type still have their own logic so left those that can be replaced with setObject with casts for visibility. Having some casts implicitly, others not might be - a little confusing.', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -0,0 +1,368 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.Schema;
+import org.apache.avro.file.DataFileReader;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericContainer;
+import org.apache.avro.generic.GenericDatumReader;
+import org.apache.avro.generic.GenericFixed;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.mapred.FsInput;
+import org.apache.avro.util.Utf8;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.DictWriter;
+import org.apache.drill.exec.vector.accessor.ObjectWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.joda.time.DateTimeConstants;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+
+public class AvroBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(AvroBatchReader.class);
+
+  // currently config is unused but maybe used later
+  private final AvroReaderConfig config;
+
+  private Path filePath;
+  private long endPosition;
+  private DataFileReader<GenericContainer> reader;
+  private ResultSetLoader loader;
+  // re-use container instance
+  private GenericContainer container = null;
+
+  public AvroBatchReader(AvroReaderConfig config) {
+    this.config = config;
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    FileSplit split = negotiator.split();
+    filePath = split.getPath();
+
+    // Avro files are splittable, define reading start / end positions
+    long startPosition = split.getStart();
+    endPosition = startPosition + split.getLength();
+
+    logger.debug(""Processing Avro file: {}, start position: {}, end position: {}"",
+      filePath, startPosition, endPosition);
+
+    reader = prepareReader(split, negotiator.fileSystem(),
+      negotiator.userName(), negotiator.context().getFragmentContext().getQueryUserName());
+
+    logger.debug(""Avro file schema: {}"", reader.getSchema());
+    TupleMetadata schema = AvroSchemaUtil.convert(reader.getSchema());
+    logger.debug(""Avro file converted schema: {}"", schema);
+    negotiator.setTableSchema(schema, true);
+    loader = negotiator.build();
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    RowSetLoader rowWriter = loader.writer();
+    while (!rowWriter.isFull()) {
+      if (!nextLine(rowWriter)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      reader.close();
+    } catch (IOException e) {
+      logger.warn(""Error closing Avro reader: {}"", e.getMessage(), e);
+    } finally {
+      reader = null;
+    }
+  }
+
+  @Override
+  public String toString() {
+    long currentPosition = -1L;
+    try {
+      if (reader != null) {
+        currentPosition = reader.tell();
+      }
+    } catch (IOException e) {
+      logger.trace(""Unable to obtain Avro reader position: {}"", e.getMessage(), e);
+    }
+    return ""AvroBatchReader[File="" + filePath
+      + "", Position="" + currentPosition
+      + ""]"";
+  }
+
+  /**
+   * Initialized Avro data reader based on given file system and file path.
+   * Moves reader to the sync point from where to start reading the data.
+   *
+   * @param fileSplit file split
+   * @param fs file system
+   * @param opUserName name of the user whom to impersonate while reading the data
+   * @param queryUserName name of the user who issues the query
+   * @return Avro file reader
+   */
+  private DataFileReader<GenericContainer> prepareReader(FileSplit fileSplit, FileSystem fs, String opUserName, String queryUserName) {
+    try {
+      UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      DataFileReader<GenericContainer> reader = ugi.doAs((PrivilegedExceptionAction<DataFileReader<GenericContainer>>) () ->
+        new DataFileReader<>(new FsInput(fileSplit.getPath(), fs.getConf()), new GenericDatumReader<GenericContainer>()));
+
+      // move to sync point from where to read the file
+      reader.sync(fileSplit.getStart());
+      return reader;
+    } catch (IOException | InterruptedException e) {
+      throw UserException.dataReadError(e)
+        .message(""Error preparing Avro reader"")
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+  }
+
+  private boolean nextLine(RowSetLoader rowWriter) {
+    try {
+      if (!reader.hasNext() || reader.pastSync(endPosition)) {
+        return false;
+      }
+      container = reader.next(container);
+    } catch (IOException e) {
+      throw UserException.dataReadError(e)
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    Schema schema = container.getSchema();
+    GenericRecord record = (GenericRecord) container;
+
+    if (Schema.Type.RECORD != schema.getType()) {
+      throw UserException.dataReadError()
+        .message(""Root object must be record type. Found: %s"", schema.getType())
+        .addContext(""Reader"", this)
+        .build(logger);
+    }
+
+    rowWriter.start();
+    List<Schema.Field> fields = schema.getFields();
+    for (Schema.Field field : fields) {
+      String fieldName = field.name();
+      Object value = record.get(fieldName);
+      ObjectWriter writer = rowWriter.column(fieldName);
+      processRecord(writer, value, field.schema());
+    }
+    rowWriter.save();
+    return true;
+  }
+
+  private void processRecord(ObjectWriter writer, Object value, Schema schema) {
+    // skip processing record if it is null or is not projected
+    if (value == null || !writer.isProjected()) {
+      return;
+    }
+
+    switch (schema.getType()) {
+      case UNION:
+        processRecord(writer, value, AvroSchemaUtil.extractSchemaFromNullable(schema, writer.schema().name()));
+        break;
+      case RECORD:
+        TupleWriter tupleWriter = writer.tuple();
+
+        if (tupleWriter.tupleSchema().isEmpty()) {
+          // fill in tuple schema for cases when there recursive named record types are present
+          TupleMetadata recordSchema = AvroSchemaUtil.convert(schema);
+          recordSchema.toMetadataList().forEach(tupleWriter::addColumn);
+        }
+
+        GenericRecord genericRecord = (GenericRecord) value;
+        schema.getFields().forEach(
+          field -> processRecord(tupleWriter.column(field.name()), genericRecord.get(field.name()), field.schema())
+        );
+        break;
+      case ARRAY:
+        ArrayWriter arrayWriter = writer.array();
+        GenericArray<?> array = (GenericArray<?>) value;
+        ObjectWriter entryWriter = arrayWriter.entry();
+        for (Object arrayValue : array) {
+          processRecord(entryWriter, arrayValue, array.getSchema().getElementType());
+          arrayWriter.save();
+        }
+        break;
+      case MAP:
+        @SuppressWarnings(""unchecked"")
+        Map<Object, Object> map = (Map<Object, Object>) value;
+        Schema valueSchema = schema.getValueType();
+
+        DictWriter dictWriter = writer.dict();
+        ScalarWriter keyWriter = dictWriter.keyWriter();
+        ObjectWriter valueWriter = dictWriter.valueWriter();
+
+        for (Map.Entry<Object, Object> mapEntry : map.entrySet()) {
+          processScalar(keyWriter, mapEntry.getKey());
+          processRecord(valueWriter, mapEntry.getValue(), valueSchema);
+          dictWriter.save();
+        }
+        break;
+      default:
+        try {
+          ScalarWriter scalarWriter = writer.scalar();
+          processScalar(scalarWriter, value);
+        } catch (UnsupportedOperationException e) {
+          throw UserException.dataReadError(e)
+            .message(""Unexpected writer type '%s', expected scalar"", writer.type())
+            .addContext(""Reader"", this)
+            .build(logger);
+        }
+    }
+  }
+
+  private void processScalar(ScalarWriter scalarWriter, Object value) {
+    ColumnMetadata columnMetadata = scalarWriter.schema();
+    switch (columnMetadata.type()) {
+      case INT:
+        scalarWriter.setInt((int) value);
+        break;
+      case BIGINT:
+        scalarWriter.setLong((long) value);
+        break;
+      case FLOAT4:
+        scalarWriter.setDouble((float) value);
+        break;
+      case FLOAT8:
+        scalarWriter.setDouble((double) value);
+        break;
+      case VARDECIMAL:
+        BigInteger bigInteger;
+        if (value instanceof ByteBuffer) {
+          ByteBuffer decBuf = (ByteBuffer) value;
+          bigInteger = new BigInteger(decBuf.array());
+        } else {
+          GenericFixed genericFixed = (GenericFixed) value;
+          bigInteger = new BigInteger(genericFixed.bytes());
+        }
+        BigDecimal decimalValue = new BigDecimal(bigInteger, columnMetadata.scale());
+        scalarWriter.setDecimal(decimalValue);
+        break;
+      case BIT:
+        scalarWriter.setBoolean((boolean) value);
+        break;
+      case VARCHAR:
+        byte[] binary;
+        int length;
+        if (value instanceof Utf8) {
+          Utf8 utf8 = (Utf8) value;
+          binary = utf8.getBytes();
+          length = utf8.getByteLength();
+        } else {
+          binary = value.toString().getBytes(Charsets.UTF_8);
+          length = binary.length;
+        }
+        scalarWriter.setBytes(binary, length);
+        break;
+      case VARBINARY:
+        if (value instanceof ByteBuffer) {
+          ByteBuffer buf = (ByteBuffer) value;
+          scalarWriter.setBytes(buf.array(), buf.remaining());
+        } else {
+          byte[] bytes = ((GenericFixed) value).bytes();
+          scalarWriter.setBytes(bytes, bytes.length);
+        }
+        break;
+      case TIMESTAMP:
+        String avroLogicalType = columnMetadata.property(AvroSchemaUtil.AVRO_LOGICAL_TYPE_PROPERTY);
+        if (AvroSchemaUtil.TIMESTAMP_MILLIS_LOGICAL_TYPE.equals(avroLogicalType)) {
+          scalarWriter.setLong((long) value);
+        } else {
+          scalarWriter.setLong((long) value / 1000);
+        }
+        break;
+      case DATE:
+        scalarWriter.setLong((int) value * (long) DateTimeConstants.MILLIS_PER_DAY);
+        break;
+      case TIME:
+        if (value instanceof Long) {
+          scalarWriter.setInt((int) ((long) value / 1000));
+        } else {
+          scalarWriter.setInt((int) value);
+        }
+        break;
+      case INTERVAL:
+        GenericFixed genericFixed = (GenericFixed) value;
+        IntBuffer intBuf = ByteBuffer
+          .wrap(genericFixed.bytes())
+          .order(ByteOrder.LITTLE_ENDIAN)
+          .asIntBuffer();
+
+        Period period = Period
+          .months(intBuf.get(0))
+          .withDays(intBuf.get(1))
+          .withMillis(intBuf.get(2));
+
+        scalarWriter.setPeriod(period);
+        break;
+      default:
+        throw UserException.dataReadError()
+          .message(""Unexpected scalar schema type: "", columnMetadata.type())
+          .addContext(""Column"", columnMetadata)
+          .addContext(""Reader"", this)
+          .build(logger);
+    }
+  }
+
+  public static class AvroReaderConfig {","[{'comment': ""The config is generally needed only if we pass more than one item to the reader. Here we just pass the plugin, so (IIRC) the class isn't necessary; though no harm in having it."", 'commenter': 'paul-rogers'}, {'comment': ""Removed the config since we don't need it."", 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java,"@@ -17,117 +17,68 @@
  */
 package org.apache.drill.exec.store.avro;
 
-import java.io.IOException;
-import java.util.List;
-import java.util.regex.Pattern;
-
-import org.apache.drill.common.exceptions.ExecutionSetupException;
-import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.common.logical.StoragePluginConfig;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.planner.common.DrillStatsTable.TableStatistics;
-import org.apache.drill.exec.planner.logical.DrillTable;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
 import org.apache.drill.exec.proto.UserBitShared.CoreOperatorType;
 import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.RecordReader;
-import org.apache.drill.exec.store.RecordWriter;
-import org.apache.drill.exec.store.SchemaConfig;
-import org.apache.drill.exec.store.dfs.BasicFormatMatcher;
-import org.apache.drill.exec.store.dfs.DrillFileSystem;
-import org.apache.drill.exec.store.dfs.FileSelection;
-import org.apache.drill.exec.store.dfs.FileSystemPlugin;
-import org.apache.drill.exec.store.dfs.FormatMatcher;
-import org.apache.drill.exec.store.dfs.FormatSelection;
-import org.apache.drill.exec.store.dfs.MagicString;
+import org.apache.drill.exec.server.options.OptionManager;
 import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;
-import org.apache.drill.exec.store.dfs.easy.EasyWriter;
-import org.apache.drill.exec.store.dfs.easy.FileWork;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
 import org.apache.hadoop.conf.Configuration;
 
-import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
 /**
  * Format plugin for Avro data files.
  */
 public class AvroFormatPlugin extends EasyFormatPlugin<AvroFormatConfig> {
 
-  private final AvroFormatMatcher matcher;
-
-  public AvroFormatPlugin(String name, DrillbitContext context, Configuration fsConf,
-                          StoragePluginConfig storagePluginConfig) {
-    this(name, context, fsConf, storagePluginConfig, new AvroFormatConfig());
-  }
-
-  public AvroFormatPlugin(String name, DrillbitContext context, Configuration fsConf, StoragePluginConfig config, AvroFormatConfig formatPluginConfig) {
-    super(name, context, fsConf, config, formatPluginConfig, true, false, true, false, Lists.newArrayList(""avro""), ""avro"");
-    this.matcher = new AvroFormatMatcher(this);
-  }
-
-  @Override
-  public boolean supportsPushDown() {
-    return true;
-  }
-
-  @Override
-  public RecordReader getRecordReader(FragmentContext context, DrillFileSystem dfs, FileWork fileWork, List<SchemaPath> columns, String userName) throws ExecutionSetupException {
-    return new AvroRecordReader(context, fileWork.getPath(), fileWork.getStart(), fileWork.getLength(), dfs, columns,
-      userName);
-  }
-
-  @Override
-  public RecordWriter getRecordWriter(FragmentContext context, EasyWriter writer) throws IOException {
-    throw new UnsupportedOperationException(""unimplemented"");
-  }
+  public static final String DEFAULT_NAME = ""avro"";
 
-  @Override
-  public int getReaderOperatorType() {
-    return CoreOperatorType.AVRO_SUB_SCAN_VALUE;
+  public AvroFormatPlugin(String name,
+                             DrillbitContext context,
+                             Configuration fsConf,
+                             StoragePluginConfig storageConfig,
+                             AvroFormatConfig formatConfig) {
+    super(name, easyConfig(fsConf, formatConfig), context, storageConfig, formatConfig);
   }
 
-  @Override
-  public int getWriterOperatorType() {
-    throw new UnsupportedOperationException(""unimplemented"");
+  private static EasyFormatConfig easyConfig(Configuration fsConf, AvroFormatConfig formatConfig) {
+    EasyFormatConfig config = new EasyFormatConfig();
+    config.readable = true;
+    config.writable = false;
+    config.blockSplittable = true;
+    config.compressible = false;
+    config.supportsProjectPushdown = true;
+    config.extensions = formatConfig.extensions;
+    config.fsConf = fsConf;
+    config.defaultName = DEFAULT_NAME;
+    config.readerOperatorType = CoreOperatorType.AVRO_SUB_SCAN_VALUE;
+    config.useEnhancedScan = true;
+    return config;
   }
 
   @Override
-  public FormatMatcher getMatcher() {
-    return this.matcher;
+  protected FileScanFramework.FileScanBuilder frameworkBuilder(OptionManager options, EasySubScan scan) {
+    FileScanFramework.FileScanBuilder builder = new FileScanFramework.FileScanBuilder();
+    builder.setReaderFactory(new AvroReaderFactory(new AvroBatchReader.AvroReaderConfig(this)));
+    initScanBuilder(builder, scan);
+    builder.setNullType(Types.optional(TypeProtos.MinorType.VARCHAR));
+    return builder;","[{'comment': 'No harm in setting this. But, I wonder if this is the right choice for Avro? Avro has a schema which, presumably, all files should follow. If schema evolution occurred (new field added), we\'d really want to use the proper data type when reading old files without that column. This is of course a long-standing limitation of Drill\'s ""pure"" schema-on-read approach: while creating the reader is the wrong time to be guessing the schema of columns we are not going to see.', 'commenter': 'paul-rogers'}, {'comment': ""Well, I suppose if I won't set it, varchar type will be used anyway. Do you suggest to remove the assigment? Or use something else."", 'commenter': 'arina-ielchiieva'}, {'comment': 'I guess we can leave this one alone; no harm as you point out.', 'commenter': 'paul-rogers'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroSchemaUtil.java,"@@ -0,0 +1,274 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.LogicalType;
+import org.apache.avro.LogicalTypes;
+import org.apache.avro.Schema;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.DictBuilder;
+import org.apache.drill.exec.record.metadata.MapBuilder;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.RepeatedListBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.record.metadata.TupleSchema;
+import org.apache.drill.exec.vector.complex.DictVector;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/**
+ * Utility class that provides methods to interact with Avro schema.
+ */
+public class AvroSchemaUtil {
+
+  private static final Logger logger = LoggerFactory.getLogger(AvroSchemaUtil.class);
+
+  public static final String AVRO_LOGICAL_TYPE_PROPERTY = ""avro_logical_type"";
+
+  public static final String DECIMAL_LOGICAL_TYPE = ""decimal"";
+  public static final String TIMESTAMP_MICROS_LOGICAL_TYPE = ""timestamp-micros"";
+  public static final String TIMESTAMP_MILLIS_LOGICAL_TYPE = ""timestamp-millis"";
+  public static final String DATE_LOGICAL_TYPE = ""date"";
+  public static final String TIME_MICROS_LOGICAL_TYPE = ""time-micros"";
+  public static final String TIME_MILLIS_LOGICAL_TYPE = ""time-millis"";
+  public static final String DURATION_LOGICAL_TYPE = ""duration"";
+
+  /**
+   * Converts Avro schema into Drill metadata description of the schema.
+   *
+   * @param schema Avro schema
+   * @return metadata description of the schema
+   * @throws UserException if schema contains unsupported types
+   */
+  public static TupleMetadata convert(Schema schema) {
+    return SchemaConverter.INSTANCE.convert(schema);
+  }
+
+  /**
+   * Avro represents nullable type as union of null and another schema: [""null"", ""some-type""].
+   * This method extracts non-nullable schema for given union schema.
+   *
+   * @param schema Avro schema
+   * @param columnName column name
+   * @return non-nullable Avro schema
+   * @throws UserException if given schema is not a union or represents complex union
+   */
+  public static Schema extractSchemaFromNullable(Schema schema, String columnName) {
+    if (!schema.isUnion()) {
+      throw UserException.validationError()
+        .message(""Expected union type, but received: %s"", schema.getType())
+        .addContext(""Column"", columnName)
+        .build(logger);
+    }
+    List<Schema> unionSchemas = schema.getTypes();
+
+    // exclude all schemas with null type
+    List<Schema> nonNullSchemas = unionSchemas.stream()
+      .filter(unionSchema -> !Schema.Type.NULL.equals(unionSchema.getType()))
+      .collect(Collectors.toList());
+
+    // if original schema has two elements and only one non-nullable schema, this is simple nullable type
+    if (unionSchemas.size() == 2 && nonNullSchemas.size() == 1) {
+      return nonNullSchemas.get(0);
+    } else {
+      return throwUnsupportedErrorForType(""complex union"", columnName);
+    }
+  }
+
+  private static <T> T throwUnsupportedErrorForType(String type, String columnName) {
+    throw UserException.unsupportedError()
+      .message(""'%s' type is not supported"", type)
+      .addContext(""Column"", columnName)
+      .build(logger);
+  }
+
+  /**
+   * Class is responsible for converting Avro schema into Drill metadata description of the schema.
+   * It does not hold state and thus is thread-safe.
+   */
+  private static class SchemaConverter {
+
+    private static final SchemaConverter INSTANCE = new SchemaConverter();
+
+    TupleMetadata convert(Schema schema) {
+      /*
+        Avro allows to reference types by name, sometimes reference can be done to the type under construction.
+        For example, a linked-list of 64-bit values:
+        {
+          ""type"": ""record"",
+          ""name"": ""LongList"",
+          ""fields"" : [
+             {""name"": ""value"", ""type"": ""long""},             // each element has a long
+             {""name"": ""next"", ""type"": [""null"", ""LongList""]} // optional next element
+           ]
+        }
+
+        Since we cannot build record type which is not constructed yet, when such situation is detected,
+        record type is set to Drill Map without columns, columns will be detected when reading actual data.
+
+        `typeNamesUnderConstruction` is a holder to store record type names under construction to detect
+         reference to the types which are not yet constructed.
+       */
+      Set<String> typeNamesUnderConstruction = new HashSet<>();
+      TupleSchema tupleSchema = new TupleSchema();
+
+      // add current record type to the set of types under construction
+      typeNamesUnderConstruction.add(schema.getFullName());
+
+      List<Schema.Field> fields = schema.getFields();
+      fields.stream()
+        .map(field -> convert(field, typeNamesUnderConstruction))
+        .forEach(tupleSchema::add);
+      return tupleSchema;
+    }
+
+    private ColumnMetadata convert(Schema.Field field, Set<String> typeNamesUnderConstruction) {
+      Schema fieldSchema = field.schema();
+      return defineColumn(field.name(), fieldSchema, TypeProtos.DataMode.REQUIRED, typeNamesUnderConstruction);
+    }
+
+    private ColumnMetadata defineColumn(String name, Schema fieldSchema,","[{'comment': ""Are there any cases where two distinct Avro types or variations map to a single Drill type? If so, you'd later want to create distinct `ColumnConverter`s for each variation. This might be a handy place to use `ColumnMetadata` properties to capture those nuances to be used later when creating the column converter."", 'commenter': 'paul-rogers'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -0,0 +1,368 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.Schema;
+import org.apache.avro.file.DataFileReader;
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericContainer;
+import org.apache.avro.generic.GenericDatumReader;
+import org.apache.avro.generic.GenericFixed;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.mapred.FsInput;
+import org.apache.avro.util.Utf8;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.DictWriter;
+import org.apache.drill.exec.vector.accessor.ObjectWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.joda.time.DateTimeConstants;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+
+public class AvroBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(AvroBatchReader.class);
+
+  // currently config is unused but maybe used later
+  private final AvroReaderConfig config;
+
+  private Path filePath;
+  private long endPosition;
+  private DataFileReader<GenericContainer> reader;
+  private ResultSetLoader loader;
+  // re-use container instance
+  private GenericContainer container = null;
+
+  public AvroBatchReader(AvroReaderConfig config) {
+    this.config = config;
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    FileSplit split = negotiator.split();
+    filePath = split.getPath();
+
+    // Avro files are splittable, define reading start / end positions
+    long startPosition = split.getStart();
+    endPosition = startPosition + split.getLength();
+
+    logger.debug(""Processing Avro file: {}, start position: {}, end position: {}"",
+      filePath, startPosition, endPosition);
+
+    reader = prepareReader(split, negotiator.fileSystem(),
+      negotiator.userName(), negotiator.context().getFragmentContext().getQueryUserName());
+
+    logger.debug(""Avro file schema: {}"", reader.getSchema());
+    TupleMetadata schema = AvroSchemaUtil.convert(reader.getSchema());","[{'comment': ""Here we are converting the Avro schema to a Drill schema, which makes sense. If we scan a directory with multiple files, they should all have the same schema (or an evolved schema: `(A, B) --> (A,B,C) --> (B,C)`.\r\n\r\nIn the future, it would be handy to infer the schema a plan time, then pass the resulting schema to each reader so we don't have to repeat identical work in each of perhaps hundreds of readers."", 'commenter': 'paul-rogers'}, {'comment': 'Sounds good, are there examples for EVF where schema is inferred at plan time? When doing this implementation, I looked at existing EVF implementations and mostly they inferred schema in reader.', 'commenter': 'arina-ielchiieva'}, {'comment': ""I actually don't have a good example in mind. Planner-side schema planning is not really an EVF-specific thing. We used an external schema for CSV files using your provisioned schema mechanism. There is the new metadata mechanism that was recently added. We do partition and row group pruning for Parquet using external metadata.\r\n\r\nSo, planner-side schema analysis is probably something new we'd want to add. I believe that Avro, when used for RPC, defines the schema separate from the data. Is something like that available when Avro is used as a file format?\r\n\r\nFor now, perhaps we can just do the schema planning at open time for each file. Then, if that code just does Avro --> ColumnMetadata translation, the code can be moved earlier in the process if we find a way to obtain the schema earlier."", 'commenter': 'paul-rogers'}, {'comment': 'When reading Avro files we obtain schema from each file. \r\nI believe we might have list of files to read at planning time (that what we have for Parquet where we read the footers). We can open first file read the schema, convert it and then provide it when reading all files but of course we would have to check each file is compatible with such schema. Though from the EVF code, I am not sure where such approach can be added.', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/test/java/org/apache/drill/exec/store/avro/AvroSchemaUtilTest.java,"@@ -0,0 +1,431 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.LogicalType;
+import org.apache.avro.LogicalTypes;
+import org.apache.avro.Schema;
+import org.apache.avro.SchemaBuilder;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.test.BaseTest;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.ExpectedException;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+public class AvroSchemaUtilTest extends BaseTest {","[{'comment': 'Nicely done; I like how you were able to test the type conversion separate from the reader itself.', 'commenter': 'paul-rogers'}, {'comment': 'Thanks!', 'commenter': 'arina-ielchiieva'}]"
1951,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/ColumnConverter.java,"@@ -0,0 +1,261 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.avro;
+
+import org.apache.avro.generic.GenericArray;
+import org.apache.avro.generic.GenericFixed;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.util.Utf8;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.DictWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.joda.time.DateTimeConstants;
+import org.joda.time.Period;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Consumer;
+import java.util.stream.IntStream;
+
+/**
+ * Converts and sets given value into the specific column writer.
+ */
+public interface ColumnConverter {
+
+  void convert(Object value);
+
+  /**
+   * Does nothing, is used when column is not projected to avoid unnecessary
+   * column values conversions and writes.
+   */
+  class DummyColumnConverter implements ColumnConverter {
+
+    public static final DummyColumnConverter INSTANCE = new DummyColumnConverter();
+
+    @Override
+    public void convert(Object value) {
+      // do nothing
+    }
+  }
+
+  /**
+   * Converts and writes scalar values using provided {@link #valueConverter}.
+   * {@link #valueConverter} has different implementation depending
+   * on the scalar value type.
+   */
+  class ScalarColumnConverter implements ColumnConverter {
+
+    private final Consumer<Object> valueConverter;
+
+    public ScalarColumnConverter(Consumer<Object> valueConverter) {
+      this.valueConverter = valueConverter;
+    }
+
+    public static ScalarColumnConverter init(ScalarWriter writer) {","[{'comment': ""Very nice! Clean and simple; much simpler than having a class per type. If we get a chance to update the Drill book, we'll point to this as an example of how to do conversion simply."", 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.","[{'comment': 'Drill 1.17 introduces the Drill Metastore which stores the table schema and table statistics. Statistics allow Drill to better create optimal query plans.\r\n\r\nThe Metastore is an Alpha feature; it is subject to change. We encourage you to try it and provide feedback. Because the Metastore is in Beta, the SQL commands and Metastore formats may change in the next release.\r\n\r\n(Looks like this appears in Limitations below, but it should be stated right up front. Note the addition of warnings of what will change.\r\n\r\nPlease explain how this feature relates to the previous Parquet metadata feature. In particular, will enabling this feature break existing behavior?)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:","[{'comment': 'To use the Drill Metastore, you must enable it with the following command:\r\n\r\n(BTW: Why the `SET` command and not `ALTER SYSTEM`? Seems an unnecessary complexity.)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.","[{'comment': '(Ah, if it is an option, then the above command is wrong? Should it be `ALTER SESSION SET ..`?)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax","[{'comment': '(We just told the user that a metastore exists and how to enable it. Now we tell them the syntax of commands. We need a transition.)\r\n\r\nOnce you enable the Metastore, the next step is to populate it with data. Drill can query a table whether that table has a Metastore entry or not. (If you are familiar with Hive, then you know that Hive requires that all tables have Hive Metastore entries before you can query them.) In Drill, only add data to the Metastore when doing so improves query performance. In general, large tables benefit from statistics more than small tables do.\r\n\r\nUnlike Hive, Drill does not require you to declare a schema. Instead, Drill infers the schema by scanning your table. Drill not only infers the schema, it optionally computes statistics about your table.', 'commenter': 'paul-rogers'}, {'comment': '(If the above is true, it raises all sorts of questions. How do we handle conflicting types? Conflicting names (Foo, foo and FOO)? How does the user specify that ""column X is coming, when it arrives in new files, it will be of type DECIMAL""?)\r\n\r\nInferring schema is good, but a human must fine tune the schema to express intent, especially when the information is ambiguous. And, a main reason to have a schema is to deal with ambiguous data.)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table","[{'comment': '(Here it is not clear if the command gathers schema and statistics? Or, only schema by default?)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.","[{'comment': 'or if you do not have permission to read the table', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.","[{'comment': '(Capitalize The but not ""Stored"". That is, it should read: ""metadata. The stored schema..."")', 'commenter': 'paul-rogers'}, {'comment': '(Not clear here. If this command infers schema, do I specify here the only columns that Drill should infer? Does Drill then not store schema information for the other columns?\r\n\r\nOr, is this the list of columns for which Drill will gather statistics? Maybe Drill infers schema for all columns?)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.","[{'comment': ""(What is the difference between `COLUMNS NONE` and no `COLUMNS` clause at all?\r\n\r\nMight it be worth while having an `EXCLUDE` clause? If I have 12 columns, but I don't need the `$type` and `$recordNo` columns, it is easier to list the columns to exclude than to list all columns except these. This is especially true because the big benefit seems to be that Drill will infer the columns; I have to list them (but not the types), it seems we have a funny worst of both worlds situation.)"", 'commenter': 'paul-rogers'}, {'comment': 'Drill will infer schema for all columns, but gather statistics for none of the columns.', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.","[{'comment': 'varchar --> VARCHAR\r\n\r\n(Since this is a quoted string, can we make it user friendly? ""ROW_GROUP"" --> ""ROW GROUP"" (no underscore.) Actually, why can\'t this be a keyword?\r\n\r\nPlease define the terms, especially `SEGMENT`. Also, what is the advantage of one level vs. another?)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.","[{'comment': ""(This command seems to infer schema. The command is aware of partitions. Drill has a long-standing usability issue that users must do their own partition coding. If I want data from 2018-11 to 2019-02 (one quarter worth of data), I have to write the very ugly\r\n\r\n```\r\nWHERE (dir0 = 2018 AND dir1 >= 11)\r\n        OR (dir0 = 2019 AND dir1 <= 1)\r\n```\r\n\r\nWith Hive I can just write:\r\n\r\n```\r\nWHERE transDate IN ('2018-11-01', '2019-01-31')\r\n```\r\nDoes this feature have a way to infer a column name for directory names? To know how a directory maps to a date column? If so, we should explain how that this is done and anything the user must do to make it work.)"", 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.","[{'comment': 'Generates --> Computes\r\n\r\n(The command is `COMPUTE` so we should use that word here instead of ""generate."")', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.","[{'comment': ""(Please explain what `planner.enable_statistics` means.)\r\n\r\nYou cannot use this command if statistics is disabled. Statistics usage is enabled by setting the SYSTEM/SESSION option `planner.enable_statistics`. It is (what?) by default. To enable statistics:\r\n\r\n```\r\nALTER SYSTEM SET `planner.enable_statistics` = true\r\n```\r\n\r\n(Note: This behavior seems odd. Why can't I first gather stats and review them for quality before I have my users start using them? As it is, there is no ability to gather them, enable the option for a session for testing, verify that things work right, then turn it on for everyone.\r\n\r\nAlso, what happens if I turn the option on, gather stats, then turn the option off? If we refuse to gather stats, do we automatically delete them if the option is disabled?)\r\nif statistics usage is disabled, the command "", 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.","[{'comment': 'generate --> compute', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. ","[{'comment': ""(Please explain the benefit. Faster? What is the cost? Less accurate estimates? Why should I use the fast approach, when the full approach?\r\n\r\nIf the command is `SAMPLE 50 PERCENT`, won't Drill still read 100% of my data, but throw away half of it? How is this helpful? Is there an option to specify to sample 50% of *files* to reduce disk I/O?)"", 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options","[{'comment': 'Related Session/System Options\r\n\r\nThe following options are set via `ALTER SYSTEM SET` or via the Drill Web console. Note, because these options affect all users, use `ALTER SYSTEM` to set them, not `ALTER SESSION`.', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able","[{'comment': 'Enables the Drill Metastore and the `ANALYZE TABLE` command.', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.","[{'comment': 'Same options as the *level* option above.', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.","[{'comment': 'If you run the `ANALYZE TABLE` command at the same time as queries run, then the query can read incorrect or corrupt statistics. Drill will reload statistics and replan the query. This option specifies the maximum number of retry attempts.\r\n\r\n(Drill is a multi-user system. We should certainly allow concurrent queries and metadata updates!)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**","[{'comment': '(Can we eliminate the `metadata` here? The Metastore only works with metadata, so no need to add an extra layer of names.\r\n\r\nSo far in this document, we\'ve said nothing about a ""file metadata cache"" until now. We should describe it before we explain options that reference it.)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**
+Allows using file metadata cache for the case when required metadata is absent in the Metastore. Default is true.
+- **metastore.metadata.use_schema**
+Enables schema usage, stored to the Metastore. Default is `true`.","[{'comment': 'The `ANALYZE TABLE` command infers table schema as it gathers statistics. This option tells Drill to use that schema information while planning the query. (Is that where we use it?) Disable this option if Drill has inferred the schema incorrectly, or if new files arrive that have columns not in the schema. (I made up this last bit to explain why the user would care about this option. Please insert the real reason.)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**
+Allows using file metadata cache for the case when required metadata is absent in the Metastore. Default is true.
+- **metastore.metadata.use_schema**
+Enables schema usage, stored to the Metastore. Default is `true`.
+- **metastore.metadata.use_statistics**
+Enables statistics usage, stored in the Metastore, at the planning stage. Default is `true`.","[{'comment': '(How does this option relate to the planner option above? Must I enable both? Why would I disable one or the other?\r\n\r\nI wonder, should this stuff be in an appendix somewhere and not here? As a (somewhat informed) reader, I find myself becoming more and more confused about how this stuff even works! Maybe only list the key things here: options that represent knobs that the user might actually want to twiddle.)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**
+Allows using file metadata cache for the case when required metadata is absent in the Metastore. Default is true.
+- **metastore.metadata.use_schema**
+Enables schema usage, stored to the Metastore. Default is `true`.
+- **metastore.metadata.use_statistics**
+Enables statistics usage, stored in the Metastore, at the planning stage. Default is `true`.
+- **metastore.metadata.ctas.auto-collect**
+Specifies whether schema and / or column statistics will be automatically collected for every table after CTAS and CTTAS.","[{'comment': 'Drill provides the `CREATE TABLE AS` commands to create new tables. This option causes Drill to gather schema and statistics for those tables automatically as they are written.\r\n\r\n(Not CTTAS, correct? We explained above that `ANALYZE TABLE` will fail on a temp table. and the second ""T"" in ""CTTAS"" stands for ""TEMPORARY"".\r\n\r\nAlso, the text says ""schema and/or column statistics."" Why would I want to gather statistics without a schema? What does that even mean? Wouldn\'t the statistics represent an implied schema?\r\n\r\nThe default is `NONE`. But, we don\'t tell the user the other values. Please list them.)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**
+Allows using file metadata cache for the case when required metadata is absent in the Metastore. Default is true.
+- **metastore.metadata.use_schema**
+Enables schema usage, stored to the Metastore. Default is `true`.
+- **metastore.metadata.use_statistics**
+Enables statistics usage, stored in the Metastore, at the planning stage. Default is `true`.
+- **metastore.metadata.ctas.auto-collect**
+Specifies whether schema and / or column statistics will be automatically collected for every table after CTAS and CTTAS.
+This option is not active for now. Default is `'NONE'`.
+- **drill.exec.storage.implicit.last_modified_time.column.label**
+Sets the implicit column name for the last modified time (`lmt`) column. For internal usage when producing Metastore analyze.","[{'comment': '(If internal, please do not document. Please add a comment to `ExecConstants` to show the property is internal only. In fact maybe we need to add a bit of metadata to the option definition to say not to show the option in the Web UI.\r\n\r\nSame comment for the next three options.)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**
+Allows using file metadata cache for the case when required metadata is absent in the Metastore. Default is true.
+- **metastore.metadata.use_schema**
+Enables schema usage, stored to the Metastore. Default is `true`.
+- **metastore.metadata.use_statistics**
+Enables statistics usage, stored in the Metastore, at the planning stage. Default is `true`.
+- **metastore.metadata.ctas.auto-collect**
+Specifies whether schema and / or column statistics will be automatically collected for every table after CTAS and CTTAS.
+This option is not active for now. Default is `'NONE'`.
+- **drill.exec.storage.implicit.last_modified_time.column.label**
+Sets the implicit column name for the last modified time (`lmt`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_index.column.label**
+Sets the implicit column name for the row group index (`rgi`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_length.column.label**
+Sets the implicit column name for the row group length (`rgl`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_start.column.label**
+Sets the implicit column name for the row group start (`rgs`) column. For internal usage when producing Metastore analyze.
+
+## Related Commands
+
+To drop table metadata from the Metastore, the following command may be used:","[{'comment': 'Use the following command to remove a table from the Metastore:', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**
+Allows using file metadata cache for the case when required metadata is absent in the Metastore. Default is true.
+- **metastore.metadata.use_schema**
+Enables schema usage, stored to the Metastore. Default is `true`.
+- **metastore.metadata.use_statistics**
+Enables statistics usage, stored in the Metastore, at the planning stage. Default is `true`.
+- **metastore.metadata.ctas.auto-collect**
+Specifies whether schema and / or column statistics will be automatically collected for every table after CTAS and CTTAS.
+This option is not active for now. Default is `'NONE'`.
+- **drill.exec.storage.implicit.last_modified_time.column.label**
+Sets the implicit column name for the last modified time (`lmt`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_index.column.label**
+Sets the implicit column name for the row group index (`rgi`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_length.column.label**
+Sets the implicit column name for the row group length (`rgl`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_start.column.label**
+Sets the implicit column name for the row group start (`rgs`) column. For internal usage when producing Metastore analyze.
+
+## Related Commands
+
+To drop table metadata from the Metastore, the following command may be used:
+
+	ANALYZE TABLE [table_name] DROP [METADATA|STATISTICS] [IF EXISTS]","[{'comment': '(Again, why would I keep stats but remove metadata? This seems line an unnecessary complexity. Seems the valid option would be:\r\n\r\n`DROP` (all)\r\n`DROP STATISTICS` (keep the schema, clear the statistics)\r\n)', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**
+Allows using file metadata cache for the case when required metadata is absent in the Metastore. Default is true.
+- **metastore.metadata.use_schema**
+Enables schema usage, stored to the Metastore. Default is `true`.
+- **metastore.metadata.use_statistics**
+Enables statistics usage, stored in the Metastore, at the planning stage. Default is `true`.
+- **metastore.metadata.ctas.auto-collect**
+Specifies whether schema and / or column statistics will be automatically collected for every table after CTAS and CTTAS.
+This option is not active for now. Default is `'NONE'`.
+- **drill.exec.storage.implicit.last_modified_time.column.label**
+Sets the implicit column name for the last modified time (`lmt`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_index.column.label**
+Sets the implicit column name for the row group index (`rgi`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_length.column.label**
+Sets the implicit column name for the row group length (`rgl`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_start.column.label**
+Sets the implicit column name for the row group start (`rgs`) column. For internal usage when producing Metastore analyze.
+
+## Related Commands
+
+To drop table metadata from the Metastore, the following command may be used:
+
+	ANALYZE TABLE [table_name] DROP [METADATA|STATISTICS] [IF EXISTS]
+
+It will not throw an exception for absent table metadata if `IF EXISTS` clause was specified.","[{'comment': 'The command will fail if the table does not exist in the Metastore. Include the `IF EXISTS` clause to ignore a missing table.', 'commenter': 'paul-rogers'}]"
1953,_docs/sql-reference/sql-commands/007-analyze-table-refresh-metadata.md,"@@ -0,0 +1,158 @@
+---
+title: ""ANALYZE TABLE REFRESH METADATA""
+parent: ""SQL Commands""
+date: 2020-01-13
+---
+
+Starting from Drill 1.17, you can store table metadata (including schema and computed statistics) into Drill Metastore.
+This metadata will be used when querying a table for more optimal plan creation.
+
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+
+To enable Drill Metastore usage, the following option `metastore.enabled` should be set to `true`, as shown:
+
+	SET `metastore.enabled` = true;
+
+Alternatively, you can enable the option in the Drill Web UI at `http://<drill-hostname-or-ip-address>:8047/options`.
+
+## Syntax
+
+The ANALYZE TABLE REFRESH METADATA statement supports the following syntax:
+
+	ANALYZE TABLE [table_name] [COLUMNS {(col1, col2, ...) | NONE}]
+	REFRESH METADATA ['level' LEVEL]
+	[{COMPUTE | ESTIMATE} | STATISTICS [(column1, column2, ...)]
+	[ SAMPLE number PERCENT ]]
+
+## Parameters
+
+*table_name*
+The name of the table or directory for which Drill will collect table metadata. If the table does not exist, or the table
+ is temporary, the command fails and metadata is not collected and stored.
+
+*COLUMNS (col1, col2, ...)*
+Optional names of the column(s) for which Drill will generate and store metadata. the Stored schema will include all table columns.
+
+*COLUMNS NONE*
+Specifies to ignore collecting and storing metadata for all table columns.
+
+*level*
+Optional varchar literal which specifies maximum level depth for collecting metadata.
+Possible values: `TABLE`, `SEGMENT`, `PARTITION`, `FILE`, `ROW_GROUP`, `ALL`. Default is `ALL`.
+
+*COMPUTE*
+Generates statistics for the table to be stored into the Metastore.
+If statistics usage is disabled (`planner.enable_statistics` is set to `false`), an error will be thrown when this clause is specified.
+
+*ESTIMATE*
+Generates estimated statistics for the table to be stored into the Metastore. Currently is not supported.
+
+*(column1, column2, ...)*
+The name of the column(s) for which Drill will generate statistics.
+
+*SAMPLE*
+Optional. Indicates that compute statistics should run on a subset of the data.
+
+*number PERCENT*  
+An integer that specifies the percentage of data on which to compute statistics. For example, if a table has 100 rows, `SAMPLE 50 PERCENT` indicates that statistics should be computed on 50 rows. The optimizer selects the rows at random. 
+
+## Related Options
+
+- **metastore.enabled**
+Enables Drill Metastore usage to be able to store table metadata during ANALYZE TABLE commands execution and to be able
+ to read table metadata during regular queries execution or when querying some INFORMATION_SCHEMA tables. Default is `false`.
+- **metastore.metadata.store.depth_level**
+Specifies maximum level depth for collecting metadata. Default is `'ALL'`.
+- **metastore.retrieval.retry_attempts**
+Specifies the number of attempts for retrying query planning after detecting that query metadata is changed.
+If the number of retries was exceeded, query will be planned without metadata information from the Metastore. Default is 5.
+- **metastore.metadata.fallback_to_file_metadata**
+Allows using file metadata cache for the case when required metadata is absent in the Metastore. Default is true.
+- **metastore.metadata.use_schema**
+Enables schema usage, stored to the Metastore. Default is `true`.
+- **metastore.metadata.use_statistics**
+Enables statistics usage, stored in the Metastore, at the planning stage. Default is `true`.
+- **metastore.metadata.ctas.auto-collect**
+Specifies whether schema and / or column statistics will be automatically collected for every table after CTAS and CTTAS.
+This option is not active for now. Default is `'NONE'`.
+- **drill.exec.storage.implicit.last_modified_time.column.label**
+Sets the implicit column name for the last modified time (`lmt`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_index.column.label**
+Sets the implicit column name for the row group index (`rgi`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_length.column.label**
+Sets the implicit column name for the row group length (`rgl`) column. For internal usage when producing Metastore analyze.
+- **drill.exec.storage.implicit.row_group_start.column.label**
+Sets the implicit column name for the row group start (`rgs`) column. For internal usage when producing Metastore analyze.
+
+## Related Commands
+
+To drop table metadata from the Metastore, the following command may be used:
+
+	ANALYZE TABLE [table_name] DROP [METADATA|STATISTICS] [IF EXISTS]
+
+It will not throw an exception for absent table metadata if `IF EXISTS` clause was specified.
+
+	ANALYZE TABLE [workspace.]table_name COMPUTE STATISTICS [(column1, column2,...)] [SAMPLE number PERCENT]","[{'comment': '(Is `workspace.` correct? A workspace is something unique to a session. Do we mean `plugin.schema` instead?\r\n\r\nAlso, what does happen if the user works via a workspace? Does Drill work out the underlying plugin and schema name?\r\n\r\nWhat happens if the user analyzes a table, then removes or changes the plugin or schema? Does the data remain in the Metastore? Can the user remove it, given that the plugin and schema names are now invalid?\r\n\r\nSadly, altering a plugin or schema is not done with a SQL command, so there is no way to prevent changes while metadata is present, or trigger metadata removal.\r\n\r\nHow does the user clean up in this case?)', 'commenter': 'paul-rogers'}]"
1954,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/FilterEvaluatorUtils.java,"@@ -113,7 +113,9 @@ public static RowsMatch matches(LogicalExpression expr, Map<SchemaPath, ColumnSt
       StatisticsProvider<T> rangeExprEvaluator = new StatisticsProvider(columnsStatistics, rowCount);
       rowsMatch = parquetPredicate.matches(rangeExprEvaluator);
     }
-    return rowsMatch == RowsMatch.ALL && isRepeated(schemaPathsInExpr, fileMetadata) ? RowsMatch.SOME : rowsMatch;
+    return rowsMatch == RowsMatch.ALL
+        && (isRepeated(schemaPathsInExpr, fileMetadata) || isDictOrRepeatedMapChild(schemaPathsInExpr, fileMetadata))","[{'comment': 'It seems like both methods ```isRepeated(schemaPathsInExpr, fileMetadata)``` and ```isDictOrRepeatedMapChild(schemaPathsInExpr, fileMetadata))``` only exists to serve one specific purpose - avoid ```RowsMatch.ALL``` when necessary. Maybe it makes sense to combine both methods into one and rename it?  Also, the check could be placed inside the ``` if (parquetPredicate != null) {``` declared above. ', 'commenter': 'ihuzenko'}, {'comment': 'Grouped those two methods in another method.', 'commenter': 'KazydubB'}, {'comment': ""I'll better leave this check after the `if (parquetPredicate != null)` to allow `RowsMatch.NONE` to be returned."", 'commenter': 'KazydubB'}]"
1954,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java,"@@ -737,7 +737,13 @@ public static boolean containsComplexColumn(ParquetMetadata footer, List<SchemaP
       if (type == OriginalType.MAP) {
         TypeProtos.MajorType drillType = TypeProtos.MajorType.newBuilder()
             .setMinorType(TypeProtos.MinorType.DICT)
-            .setMode(TypeProtos.DataMode.OPTIONAL)
+            .setMode(TypeProtos.DataMode.REQUIRED)
+            .build();
+        result.add(drillType);
+      } else if (type == OriginalType.LIST) {
+        TypeProtos.MajorType drillType = TypeProtos.MajorType.newBuilder()
+            .setMinorType(TypeProtos.MinorType.LIST)
+            .setMode(TypeProtos.DataMode.REQUIRED)","[{'comment': 'Please update javadoc for this method. Also, loop may be shortened to : \r\n```java\r\n    for (OriginalType type : originalTypes) {\r\n      if (type == OriginalType.MAP) {\r\n        result.add(Types.required(TypeProtos.MinorType.DICT));\r\n      } else if (type == OriginalType.LIST) {\r\n        result.add(Types.required(TypeProtos.MinorType.LIST));\r\n      } else {\r\n        result.add(null);\r\n      }\r\n    }\r\n```', 'commenter': 'ihuzenko'}]"
1954,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetTableMetadataUtils.java,"@@ -507,29 +507,67 @@ private static long convertToDrillDateValue(int dateValue) {
       MetadataBase.ParquetTableMetadataBase parquetTableMetadata) {
     int precision = 0;
     int scale = 0;
-    int definitionLevel = 1;
-    int repetitionLevel = 0;
     MetadataVersion metadataVersion = new MetadataVersion(parquetTableMetadata.getMetadataVersion());
     // only ColumnTypeMetadata_v3 and ColumnTypeMetadata_v4 store information about scale, precision, repetition level and definition level
     if (parquetTableMetadata.hasColumnMetadata() && (metadataVersion.compareTo(new MetadataVersion(3, 0)) >= 0)) {
       scale = parquetTableMetadata.getScale(name);
       precision = parquetTableMetadata.getPrecision(name);
-      repetitionLevel = parquetTableMetadata.getRepetitionLevel(name);
-      definitionLevel = parquetTableMetadata.getDefinitionLevel(name);
-    }
-    TypeProtos.DataMode mode;
-    if (repetitionLevel >= 1) {
-      mode = TypeProtos.DataMode.REPEATED;
-    } else if (repetitionLevel == 0 && definitionLevel == 0) {
-      mode = TypeProtos.DataMode.REQUIRED;
-    } else {
-      mode = TypeProtos.DataMode.OPTIONAL;
     }
+
+    TypeProtos.DataMode mode = getDataMode(parquetTableMetadata, metadataVersion, name);
     return TypeProtos.MajorType.newBuilder(ParquetReaderUtility.getType(primitiveType, originalType, precision, scale))
         .setMode(mode)
         .build();
   }
 
+  /**
+   * Obtain data mode from table metadata for a column. Algorithm for retrieving data mode depends on metadata version:
+   * <ul>
+   *   <li>starting from version {@code 4.2}, Parquet's {@link org.apache.parquet.schema.Type.Repetition}
+   *   is stored in table metadata itself;</li>
+   *   <li>starting from {@code 3.0} to {@code 4.2} (exclusively) the data mode is
+   *   computed based on max {@code definition} and {@code repetition} levels
+   *   ({@link MetadataBase.ParquetTableMetadataBase#getDefinitionLevel(String[])} and
+   *   {@link MetadataBase.ParquetTableMetadataBase#getRepetitionLevel(String[])} respectively)
+   *   obtained from Parquet's schema;
+   *
+   *   <p><strong>Note:</strong> this computation may lead to erroneous results,
+   *   when there are few nesting levels.</p>
+   *   </li>
+   *   <li>prior to {@code 3.0} {@code DataMode.OPTIONAL} is returned.</li>
+   * </ul>
+   * @param tableMetadata Parquet table metadata
+   * @param metadataVersion version of Parquet table metadata
+   * @param name (leaf) column to obtain data mode for
+   * @return data mode of the specified column
+   */
+  private static TypeProtos.DataMode getDataMode(MetadataBase.ParquetTableMetadataBase tableMetadata,","[{'comment': ""Could you please simplify the method, for example: \r\n```java\r\n    TypeProtos.DataMode mode = TypeProtos.DataMode.OPTIONAL;\r\n    if (tableMetadata.hasColumnMetadata()) {\r\n      if (metadataVersion.compareTo(new MetadataVersion(4, 2)) >= 0) {\r\n        mode = ParquetReaderUtility.getDataMode(tableMetadata.getRepetition(name));\r\n      } else if (metadataVersion.compareTo(new MetadataVersion(3, 0)) >= 0) {\r\n        int repetitionLevel = tableMetadata.getRepetitionLevel(name);\r\n        if (repetitionLevel >= 1) {\r\n          mode = TypeProtos.DataMode.REPEATED;\r\n        } else if (repetitionLevel == 0 && tableMetadata.getDefinitionLevel(name) == 0) {\r\n          mode = TypeProtos.DataMode.REQUIRED;\r\n        }\r\n      }\r\n    }\r\n    return mode;\r\n```\r\nAlso, since we know that ```MetadataVersion``` is based on major & minor int numbers, I'd suggest providing a few useful methods to make comparisons easy to read. For example, we could use methods ```atLeast(major, minor)``` and ```atMost(major, minor)``` similar to Range class. We need to analyze compare to usages and figure out which method would be useful to add. "", 'commenter': 'ihuzenko'}, {'comment': 'Refactored the method.\r\nAdded convenience methods to `MetadataVersion`: `atLeast(int major, int minor)`, `isEqualTo(int major, int minor)` and `after(int major, int minor)`.', 'commenter': 'KazydubB'}]"
1954,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/FileMetadataCollector.java,"@@ -254,15 +255,9 @@ private void addColumnMetadata(String[] columnName,
     int scale;
     int repetitionLevel;
     int definitionLevel;
+    Type.Repetition repetition;
 
-    ColTypeInfo(OriginalType originalType, List<OriginalType> parentTypes,
-                int precision, int scale, int repetitionLevel, int definitionLevel) {
-      this.originalType = originalType;
-      this.parentTypes = parentTypes;
-      this.precision = precision;
-      this.scale = scale;
-      this.repetitionLevel = repetitionLevel;
-      this.definitionLevel = definitionLevel;
+    ColTypeInfo() {","[{'comment': '```suggestion\r\n```', 'commenter': 'ihuzenko'}]"
1954,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/FileMetadataCollector.java,"@@ -278,17 +273,71 @@ static ColTypeInfo of(MessageType schema, Type type, String[] path, int depth, L
         int repetitionLevel = schema.getMaxRepetitionLevel(path);
         int definitionLevel = schema.getMaxDefinitionLevel(path);
 
-        return new ColTypeInfo(type.getOriginalType(), parentTypes, precision, scale, repetitionLevel, definitionLevel);
+        Type.Repetition repetition;
+        // Check if the primitive has LIST as parent, if it does - this is an array of primitives.
+        // (See ParquetReaderUtility#isLogicalListType(GroupType) for the REPEATED field structure.)
+        if (parentTypes.size() - 2 >= 0 && parentTypes.get(parentTypes.size() - 2) == OriginalType.LIST) {","[{'comment': 'please use local variable to eliminate magic number confusion, for example: \r\n\r\n```java\r\n        int probableListIdx = parentTypes.size() - 2;\r\n        if (probableListIdx >= 0 && parentTypes.get(probableListIdx) == OriginalType.LIST) {\r\n```', 'commenter': 'ihuzenko'}]"
1954,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/FileMetadataCollector.java,"@@ -278,17 +273,71 @@ static ColTypeInfo of(MessageType schema, Type type, String[] path, int depth, L
         int repetitionLevel = schema.getMaxRepetitionLevel(path);
         int definitionLevel = schema.getMaxDefinitionLevel(path);
 
-        return new ColTypeInfo(type.getOriginalType(), parentTypes, precision, scale, repetitionLevel, definitionLevel);
+        Type.Repetition repetition;
+        // Check if the primitive has LIST as parent, if it does - this is an array of primitives.
+        // (See ParquetReaderUtility#isLogicalListType(GroupType) for the REPEATED field structure.)
+        if (parentTypes.size() - 2 >= 0 && parentTypes.get(parentTypes.size() - 2) == OriginalType.LIST) {
+          repetition = Type.Repetition.REPEATED;
+        } else {
+          repetition = primitiveType.getRepetition();
+        }
+
+        return new ColTypeInfo()","[{'comment': 'Please extract all the block inside ```if (type.isPrimitive()) {```, used to create ```ColTypeInfo``` for primitive type to separate method. ', 'commenter': 'ihuzenko'}]"
1954,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/AbstractColumnMetadata.java,"@@ -306,7 +306,7 @@ public String columnString() {
     builder.append(typeString());
 
     // Drill does not have nullability notion for complex types
-    if (!isNullable() && !isArray() && !isMap()) {
+    if (!isNullable() && !isArray() && !isMap() && !isDict()) {","[{'comment': 'Bad ""code smell"". The base class is enumerating all the ways that something can be not null. Not even sure this makes sense. Repeated items (arrays) in Drill are non-nullable. But, LISTs, oddly, are nullable. Is a List an Array? A Map is not nullable, but a union with a map element is nullable. A Dict is not nullable, but UNION or LIST of Dict is.\r\n\r\nSo, why is it that we have to check both if the type is not nullable and that it is not an array (or map or DICT?) It should be that, if we say that a DICT, say, is not nullable, then `isNullable()` should always be false for that type, or we have a logic error.\r\n\r\nNote: a while back I tried adding unit tests to this class (still need to issue that PR) and found that the semantics are getting pretty muddy: very hard to figure out what we\'re trying to do. Not in scope for this PR, but we really need to clean up our type semantics.', 'commenter': 'paul-rogers'}]"
1954,exec/vector/src/main/java/org/apache/drill/exec/record/metadata/MetadataUtils.java,"@@ -187,6 +187,10 @@ public static ColumnMetadata newMapArray(String name, TupleMetadata schema) {
     return new MapColumnMetadata(name, DataMode.REPEATED, (TupleSchema) schema);
   }
 
+  public static DictColumnMetadata newDictArray(String name, TupleMetadata schema) {
+    return new DictColumnMetadata(name, DataMode.REPEATED, (TupleSchema) schema);","[{'comment': ""Picking up on a comment in your PR description, I wonder if we have the wrong semantics here. It is true that DICT is *implemented* as a map. But, at the metadata (descriptive) level, it is not a map, and is not constructed from a map; it is instead a `<KEY,VALUE>` pair.\r\n\r\nI'm not sure we're doing ourselves a favor by exposing the implementation detail (a kind of map) in our metadata description."", 'commenter': 'paul-rogers'}, {'comment': ""@paul-rogers, let me explain: Dict stores `key` and `value` `ColumnMetadata`s in `TupleSchema` - the way Map's members are stored - but there's a validation for each of the fields (name, type). Dict does not contain a Map, it stores its `key` and `value` in its `TupleSchema schema` field.\r\n\r\nWhen `TupleMetadata` is constructed for Parquet table as part of table metadata, we loop over each Parquet field's `SchemaPath` representation (leaf fields, e.g. `` `mapcol`.`map`.`key` ``, `` `structcol`.`b` `` with the last field in schema being a primitive). Such named segments are treated as either a (Drill's) `MAP` or `DICT`, depending on parent segment type.\r\n\r\n(Parquet's) `MAP` is represented as a group (note, that nested group's name, `key_value` below, can be different, based on the system which produced the Parquet file):\r\n```\r\n<map-repetition> group <name> (MAP) {\r\n  repeated group key_value {\r\n    required <key-type> key;\r\n    <value-repetition> <value-type> value;\r\n  }\r\n}\r\n```\r\nand before changes in the PR, when `TupleMetadata` was being created for the table, if `DICT` column was encountered, it included a nested `key_value` group as Drill's `MAP` which then contained `key` and `value` fields. Thus, there is a need to skip this segment if we know that its parent's type is `DICT` to have correct `ColumnMetadata` for the `DICT` field."", 'commenter': 'KazydubB'}]"
1954,metastore/metastore-api/src/main/java/org/apache/drill/metastore/util/SchemaPathUtils.java,"@@ -50,7 +51,7 @@ public static ColumnMetadata getColumnMetadata(SchemaPath schemaPath, TupleMetad
     while (!colPath.isLastPath() && colMetadata != null) {
       if (colMetadata.isDict()) {
         // get dict's value field metadata
-        colMetadata = colMetadata.tupleSchema().metadata(0).tupleSchema().metadata(1);
+        colMetadata = colMetadata.tupleSchema().metadata(1);","[{'comment': 'This also has a bad ""code smell"". We are asking each tool that uses metadata to know how to reference members within a map or DICT. Before DICT, the only type with internal structure was a MAP, which was represented as a tuple, so the original code made sense. But, with DICT, we now no longer tie the idea of ""has named members"" with the idea of ""has a nested tuple schema"".\r\n\r\nI wonder, should we add to the `ColumnMetadata` class a `member(String/int)` method? For, `MAP`, it would turn around and call into the tuple schema. For a `DICT`, it would have a static mapping of `key`/`value` and `0`/`1`.\r\n\r\nThinking more generally, a `UNION` could could implement `member(String)` as an access to the subtype given a type.\r\n\r\nNot necessary in this PR, but perhaps we can file a JIRA for several DICT-aware cleanups.', 'commenter': 'paul-rogers'}, {'comment': ""That's a good point, a `keyMetadata()` and `valueMetadata()` can be defined in `DictColumnMetadata` (as an another to what you've suggested; but that'd require casting).\r\n\r\nI don't see a strong need to have a static mapping of `key` and `value` for `DICT`, as the existing mechanism does the work and suits well for the case (at least, to my understanding). But I do agree that hiding this implementation details is better practice. Will introduce a new method."", 'commenter': 'KazydubB'}]"
1954,metastore/metastore-api/src/main/java/org/apache/drill/metastore/util/SchemaPathUtils.java,"@@ -63,6 +64,30 @@ public static ColumnMetadata getColumnMetadata(SchemaPath schemaPath, TupleMetad
     return colMetadata;
   }
 
+  /**
+   * Checks if field indetified by the schema path is child in either {@code DICT} or {@code REPEATED MAP}.
+   * For such fields, nested in {@code DICT} or {@code REPEATED MAP},
+   * filters can't be removed based on Parquet statistics.
+   * @param schemaPath schema path used in filter
+   * @param schema schema containing all the fields in the file
+   * @return {@literal true} if field is nested inside {@code DICT} (is {@code `key`} or {@code `value`})
+   *         or inside {@code REPEATED MAP} field, {@literal false} otherwise.
+   */
+  public static boolean isFieldNestedInDictOrRepeatedMap(SchemaPath schemaPath, TupleMetadata schema) {","[{'comment': ""This is a kind of cluttered version of what was just suggested. Rather than a bunch of if-statements that answers a specific question, generalize to look up a member. You'd get the same result as this method if `member(String name)` returned `null` if no such member existed. (And, you'd do it generically without having to later extend this to `isFieldNestedInDictOrRepeatedMapOrSomeNewType()`.\r\n\r\nAlso, why only `Repeated` Map? (Non-repeated) Maps also have fields and allow `a.b` notation."", 'commenter': 'paul-rogers'}, {'comment': 'Another general observation is that we\'ve accidentally created multiple versions of the same code. The `RequestedTupleImpl` class does something very similar: it converts a `SchemaPath` into a consolidated projection set using logic much like we have here. For example, `SELECT a, a.b` is consolidated into a single column, `a`, that must be a MAP that must contain at least a `b` member. That code was very tricky to get right, as, I\'m sure, this is. That code has to be modified to handle `SELECT a, a.key`. Is `a` now a `MAP` or a `DICT`? We don\'t know, all we know is that `a` is consistent with either interpretation.\r\n\r\nDo we really need multiple copies?\r\n\r\nSome things to consolidate:\r\n\r\n* `ColumnMetadata` - which should be our go-to solution as it is the most general.\r\n* `MaterializedField` - which has all kinds of holes and problems for complex types.\r\n* `SerializedField` - Like `MaterializedField` but with buffer lengths?\r\n* `RequestedColumn` - Part of that code mentioned above that parses a project list into a consolidated set of columns.\r\n* `SchemaPath` - A description of a project list.\r\n* The logic here\r\n* ... - Probably others.\r\n\r\nWe probably want three tiers of representation:\r\n\r\n* Project list - `SchemaPath`\r\n* ""Semanticized"" project list - `RequestedTuple`\r\n* Column metadata - `ColumnMetadata`\r\n\r\nThen, we create one set of code that handles transforms and validations. (Convert project list to semantisized list, then validate that against a schema.) As it turns out, that is what the EVF does for scan projection planning; maybe we can pull that out and generalize it for use elsewhere?', 'commenter': 'paul-rogers'}, {'comment': ""This method is used to check whether a schema path in filter, e.g. `... WHERE mapcol['a'] IS NULL`, references a `DICT`'s `value` (accessed by some key). If a `value` is `OPTIONAL INT` (which is default type for absent column also), first statistics is going to be retrieved for the 'field' identified by schema path. In case when `value` is retrieved by key, previous example results to schema path being `` `mapcol`.`a` `` which is not present in statistics (but there is statistics for the value itself, which has schema path `` `mapcol`.`map`.`value` ``) and then is treated as an absent column resulting in every row matching the filter.\r\n\r\nWhile doing the changes for resolving correct metadata for the `DICT`, I've seen that `DataMode` for `key` and `value` fields are always `REPEATED`. This was because data mode was determined using Parquet's max `repetition` and `definition` level values (see changes in `ParquetTableMetadataUtils.java`), computed for the whole schema up to the leaf field. The algorith was, if `repetition >= 1` then the field is `REPEATED`. This means, if there is at least one `REPEATED` member in schema, each of its children is going to be `REPEATED`. In case of `DICT`, it has a nested `repeated group`, mentioned above, thus resulting in `key` and `value` being `REPEATED`. To retain original data mode, a Parquet's `Type.Repetition` was added to column metadata v4 and is used instead. This filtering was working before for the `DICT` because the `value` was `REPEATED` and statistics was not used for `REPEATED` fields. Now, when the data mode is retained, there is a need to handle such a case. `REPEATED MAP` is included, because its fields were determined to be `REPEATED` also, thus to preserve previous behaviuor. But currently, this method is likely to be used if the `REPEATED MAP` contains an `OPTIONAL INT`. For the case of repeated map, it's statistics is not found, because it uses simple name, e.g. `` `struct_array`.`a` `` (note that indexes are ommited, as they are not retained in Parquet schema), but it has another _actual_ structure: `` `struct_array`.`bag`.`array_element`.`a` ``. "", 'commenter': 'KazydubB'}]"
1954,metastore/metastore-api/src/main/java/org/apache/drill/metastore/util/SchemaPathUtils.java,"@@ -81,15 +106,50 @@ public static void addColumnMetadata(TupleMetadata schema, SchemaPath schemaPath
       names.add(colPath.getPath());
       colMetadata = schema.metadata(colPath.getPath());
       TypeProtos.MajorType pathType = types.get(SchemaPath.getCompoundPath(names.toArray(new String[0])));
+
+      boolean isDict = pathType != null && pathType.getMinorType() == TypeProtos.MinorType.DICT;
+      boolean isList = pathType != null && pathType.getMinorType() == TypeProtos.MinorType.LIST;
+      String name = colPath.getPath();","[{'comment': ""More bad code smell. See notes above.\r\n\r\nBTW: If you have to do a short-term fix, then this kind of error-prone, redundant code is fine. But, if we have time to fix the underlying problem, let's do it. Else, we can fix the problem in another PR."", 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVFormatPlugin.java,"@@ -15,78 +15,74 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.drill.exec.store.ltsv;
 
-import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.common.logical.StoragePluginConfig;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.planner.common.DrillStatsTable.TableStatistics;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileReaderFactory;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileScanBuilder;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
 import org.apache.drill.exec.proto.UserBitShared;
 import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.RecordReader;
-import org.apache.drill.exec.store.RecordWriter;
-import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.server.options.OptionManager;
 import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;
-import org.apache.drill.exec.store.dfs.easy.EasyWriter;
-import org.apache.drill.exec.store.dfs.easy.FileWork;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
-import java.util.List;
 
 public class LTSVFormatPlugin extends EasyFormatPlugin<LTSVFormatPluginConfig> {
 
-  private static final boolean IS_COMPRESSIBLE = true;
-
   private static final String DEFAULT_NAME = ""ltsv"";
 
-  public LTSVFormatPlugin(String name, DrillbitContext context, Configuration fsConf, StoragePluginConfig storageConfig) {
-    this(name, context, fsConf, storageConfig, new LTSVFormatPluginConfig());
-  }
+  public static class LTSVReaderFactory extends FileReaderFactory {
+    private final LTSVFormatPluginConfig ltsvFormatPluginConfig;
 
-  public LTSVFormatPlugin(String name, DrillbitContext context, Configuration fsConf, StoragePluginConfig config, LTSVFormatPluginConfig formatPluginConfig) {
-    super(name, context, fsConf, config, formatPluginConfig, true, false, false, IS_COMPRESSIBLE, formatPluginConfig.getExtensions(), DEFAULT_NAME);
-  }
+    public LTSVReaderFactory(LTSVFormatPluginConfig config) {
+      ltsvFormatPluginConfig = config;","[{'comment': 'Nit, the full name is redundant: this is the LTSV factory so this has to be the ""ltsv"" config. Suggestion: ""config"".', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVFormatPlugin.java,"@@ -15,78 +15,74 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.drill.exec.store.ltsv;
 
-import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.common.logical.StoragePluginConfig;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.planner.common.DrillStatsTable.TableStatistics;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileReaderFactory;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileScanBuilder;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
 import org.apache.drill.exec.proto.UserBitShared;
 import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.RecordReader;
-import org.apache.drill.exec.store.RecordWriter;
-import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.server.options.OptionManager;
 import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;
-import org.apache.drill.exec.store.dfs.easy.EasyWriter;
-import org.apache.drill.exec.store.dfs.easy.FileWork;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
-import java.util.List;
 
 public class LTSVFormatPlugin extends EasyFormatPlugin<LTSVFormatPluginConfig> {
 
-  private static final boolean IS_COMPRESSIBLE = true;
-
   private static final String DEFAULT_NAME = ""ltsv"";
 
-  public LTSVFormatPlugin(String name, DrillbitContext context, Configuration fsConf, StoragePluginConfig storageConfig) {
-    this(name, context, fsConf, storageConfig, new LTSVFormatPluginConfig());
-  }
+  public static class LTSVReaderFactory extends FileReaderFactory {
+    private final LTSVFormatPluginConfig ltsvFormatPluginConfig;
 
-  public LTSVFormatPlugin(String name, DrillbitContext context, Configuration fsConf, StoragePluginConfig config, LTSVFormatPluginConfig formatPluginConfig) {
-    super(name, context, fsConf, config, formatPluginConfig, true, false, false, IS_COMPRESSIBLE, formatPluginConfig.getExtensions(), DEFAULT_NAME);
-  }
+    public LTSVReaderFactory(LTSVFormatPluginConfig config) {
+      ltsvFormatPluginConfig = config;
+    }
 
-  @Override
-  public RecordReader getRecordReader(FragmentContext context, DrillFileSystem dfs, FileWork fileWork, List<SchemaPath> columns, String userName) {
-    return new LTSVRecordReader(context, fileWork.getPath(), dfs, columns);
+    @Override
+    public ManagedReader<? extends FileSchemaNegotiator> newReader() {
+      return new LTSVBatchReader(ltsvFormatPluginConfig);","[{'comment': 'So a reader normally works on a file split. But, we only give the reader the plugin config. How does it know which file and/or block to read?', 'commenter': 'paul-rogers'}, {'comment': '@paul-rogers Question... this code was cut/pasted from all the new EVF plugins.  Did I miss something here?', 'commenter': 'cgivre'}, {'comment': 'Oh, never mind. I wrote the darn thing and forgot that file info is passed in to open, not into the constructor...', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;","[{'comment': 'final, here and below', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()","[{'comment': '.dataReadError(e)', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();","[{'comment': 'Better to get this on next() rather than having to store and manage a look-ahead line.', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public boolean hasNext() {
+    return line != null;
+  }
+
+  @Override
+  public Boolean next() {","[{'comment': 'This does next line? Maybe call it `nextRow` to avoid conflicts with the `next()` which means `nextBatch()`.\r\n\r\nThen, simplify (below is abbreviated):\r\n```\r\ntry {\r\n  for (;;) {\r\n    String line = reader.readLine();\r\n    if (line == null) { return false; }\r\n    lineNumber++;\r\n    line = line.trim();\r\n    if (line.isEmpty()) { continue; }\r\n    processRow();\r\n  }\r\n} catch (Exception e) {\r\n  UserException...\r\n    .addContext(""Line"", lineNumber)\r\n}\r\n```', 'commenter': 'paul-rogers'}, {'comment': 'Boolean --> boolean. No need to use the boxed type.', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public boolean hasNext() {
+    return line != null;
+  }
+
+  @Override
+  public Boolean next() {
+    // Skip empty lines
+    if (line.trim().length() == 0) {
+      try {
+        // Advance the line to the next line
+        line = reader.readLine();
+      } catch (IOException e) {
+        throw UserException
+          .dataReadError()
+          .message(""Error reading LTSV Data: {}"", e.getMessage())
+          .build(logger);
+      }
+      return Boolean.TRUE;
+    } else if (line == null) {
+      return Boolean.FALSE;","[{'comment': 'Boolean.FALSE --> false. No need to use the boxed type. Here, above and below.', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public boolean hasNext() {
+    return line != null;
+  }
+
+  @Override
+  public Boolean next() {
+    // Skip empty lines
+    if (line.trim().length() == 0) {
+      try {
+        // Advance the line to the next line
+        line = reader.readLine();
+      } catch (IOException e) {
+        throw UserException
+          .dataReadError()
+          .message(""Error reading LTSV Data: {}"", e.getMessage())
+          .build(logger);
+      }
+      return Boolean.TRUE;
+    } else if (line == null) {
+      return Boolean.FALSE;
+    }
+
+    // Process the row
+    processRow();
+
+    // Increment record counter
+    recordCount++;","[{'comment': ""Maybe do this before? As it is, this is not useful. You can't use it in error messages because it is not a line number (we don't increment it for blank lines.) The EVF doesn't need it."", 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public boolean hasNext() {
+    return line != null;
+  }
+
+  @Override
+  public Boolean next() {
+    // Skip empty lines
+    if (line.trim().length() == 0) {
+      try {
+        // Advance the line to the next line
+        line = reader.readLine();
+      } catch (IOException e) {
+        throw UserException
+          .dataReadError()
+          .message(""Error reading LTSV Data: {}"", e.getMessage())
+          .build(logger);
+      }
+      return Boolean.TRUE;
+    } else if (line == null) {
+      return Boolean.FALSE;
+    }
+
+    // Process the row
+    processRow();
+
+    // Increment record counter
+    recordCount++;
+
+    // Get the next line
+    try {
+      line = reader.readLine();
+      if(line == null) {
+        return Boolean.FALSE;
+      }
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+    return Boolean.TRUE;
+  }
+
+  /**
+   * Function processes one row of data, splitting it up first by tabs then splitting the key/value pairs
+   * finally recording it in the current Drill row.
+   */
+  private void processRow() {
+    // Start the row
+    rowWriter.start();
+    for (String field : line.split(""\t"")) {
+      int index = field.indexOf("":"");
+      if (index <= 0) {
+        throw UserException
+          .dataReadError()
+          .message(""Invalid LTSV format at line %d: %s"", recordCount + 1, line)
+          .build(logger);
+      }
+
+      String fieldName = field.substring(0, index);
+      String fieldValue = field.substring(index + 1);
+
+      LTSVBatchReader.writeColumn(rowWriter, fieldName, fieldValue, TypeProtos.MinorType.VARCHAR);","[{'comment': ""No need to pass the type, looks like we only support one type.\r\n\r\nCan't `writeColumn()` be here rather than a static method?"", 'commenter': 'paul-rogers'}]"
1962,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/EasyEVFBatchReader.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.easy;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Iterator;
+
+public abstract class EasyEVFBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(EasyEVFBatchReader.class);
+
+  public FileSplit split;
+
+  public Iterator fileIterator;
+
+  public ResultSetLoader loader;
+
+  private RowSetLoader rowWriter;
+
+  public InputStream fsStream;
+
+  public BufferedReader reader;
+
+  public EasyEVFBatchReader() {
+  }","[{'comment': 'Delete if empty.', 'commenter': 'paul-rogers'}]"
1962,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/EasyEVFBatchReader.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.easy;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Iterator;
+
+public abstract class EasyEVFBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(EasyEVFBatchReader.class);
+
+  public FileSplit split;
+
+  public Iterator fileIterator;
+
+  public ResultSetLoader loader;
+
+  private RowSetLoader rowWriter;
+
+  public InputStream fsStream;
+
+  public BufferedReader reader;
+
+  public EasyEVFBatchReader() {
+  }
+
+  public RowSetLoader getRowWriter() {
+    return rowWriter;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    this.split = negotiator.split();
+    this.loader = negotiator.build();
+    this.rowWriter = loader.writer();
+    try {
+      this.fsStream = negotiator.fileSystem().openPossiblyCompressedStream(split.getPath());
+      this.reader = new BufferedReader(new InputStreamReader(fsStream, StandardCharsets.UTF_8));
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(String.format(""Failed to open input file: %s"", split.getPath()))
+        .build(logger);
+    }
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    while (!rowWriter.isFull()) {
+      if (!fileIterator.hasNext()) {
+        return false;
+      }
+      fileIterator.next();","[{'comment': ""As it turns out, when reading data, the `hasNext()`/`next()` pattern is not super helpful. That's why the super class here has only `next()`. Consider rewriting as:\r\n\r\n```\r\n   while (!rowWriter.isFull()) {\r\n      if (!fileIterator.next()) {\r\n        return false;\r\n      }\r\n  }\r\n```"", 'commenter': 'paul-rogers'}]"
1962,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/EasyEVFBatchReader.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.easy;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Iterator;
+
+public abstract class EasyEVFBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(EasyEVFBatchReader.class);
+
+  public FileSplit split;
+
+  public Iterator fileIterator;
+
+  public ResultSetLoader loader;
+
+  private RowSetLoader rowWriter;
+
+  public InputStream fsStream;
+
+  public BufferedReader reader;
+
+  public EasyEVFBatchReader() {
+  }
+
+  public RowSetLoader getRowWriter() {
+    return rowWriter;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    this.split = negotiator.split();
+    this.loader = negotiator.build();
+    this.rowWriter = loader.writer();
+    try {
+      this.fsStream = negotiator.fileSystem().openPossiblyCompressedStream(split.getPath());
+      this.reader = new BufferedReader(new InputStreamReader(fsStream, StandardCharsets.UTF_8));
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(String.format(""Failed to open input file: %s"", split.getPath()))
+        .build(logger);
+    }
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    while (!rowWriter.isFull()) {
+      if (!fileIterator.hasNext()) {
+        return false;
+      }
+      fileIterator.next();
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      if (reader != null) {
+        reader.close();
+        reader = null;
+      }
+      if (fsStream != null) {
+        fsStream.close();
+        fsStream = null;
+      }
+    } catch (IOException e) {
+      logger.warn(""Error closing batch Record Reader."");
+    }","[{'comment': ""This will lean the `fsStream` if `reader.close()` fails (which it shouldn't). Try using the handy\r\n\r\n```\r\nAutoCloseables.close(reader, fsStream);\r\n```"", 'commenter': 'paul-rogers'}]"
1962,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/EasyEVFBatchReader.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.easy;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Iterator;
+
+public abstract class EasyEVFBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(EasyEVFBatchReader.class);
+
+  public FileSplit split;
+
+  public Iterator fileIterator;
+
+  public ResultSetLoader loader;
+
+  private RowSetLoader rowWriter;
+
+  public InputStream fsStream;
+
+  public BufferedReader reader;
+
+  public EasyEVFBatchReader() {
+  }
+
+  public RowSetLoader getRowWriter() {
+    return rowWriter;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    this.split = negotiator.split();
+    this.loader = negotiator.build();
+    this.rowWriter = loader.writer();
+    try {
+      this.fsStream = negotiator.fileSystem().openPossiblyCompressedStream(split.getPath());
+      this.reader = new BufferedReader(new InputStreamReader(fsStream, StandardCharsets.UTF_8));
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(String.format(""Failed to open input file: %s"", split.getPath()))
+        .build(logger);
+    }
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    while (!rowWriter.isFull()) {
+      if (!fileIterator.hasNext()) {
+        return false;
+      }
+      fileIterator.next();
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      if (reader != null) {
+        reader.close();
+        reader = null;
+      }
+      if (fsStream != null) {
+        fsStream.close();
+        fsStream = null;
+      }
+    } catch (IOException e) {
+      logger.warn(""Error closing batch Record Reader."");
+    }
+  }
+
+  /**
+   * This function should be used when writing Drill columns when the schema is not fixed or known in advance. At present only simple data types are
+   * supported with this function.  If the column is not present in the schema, this function will add it first.
+   * @param rowWriter The RowSetWriter to which the data is to be written
+   * @param name The field name
+   * @param value The field value.  Cannot be a primitive.
+   * @param type The Drill data type of the field.
+   */
+  public static void writeColumn(TupleWriter rowWriter, String name, Object value, TypeProtos.MinorType type) {","[{'comment': 'Good idea, **S-L-O-W** implementation.\r\n\r\nFirst, we box each value. OK for strings, slow for `int`, etc.\r\n\r\nSecond, this code repeats what `setObject()` already does. That method is meant primarily for testing because it, like this meethod, does a switch on each value, which is slow.\r\n\r\nAnd, since this is a name-based lookup, we have to do a hash table lookup on each of billions of columns. That is also slow.\r\n\r\nFor all these reasons, I think it best to leave column-to-writer mapping to each reader. For the present LTSV reader, if column order is not guaranteed, then you need the per-column name lookup. But, you know the type is always VARCHAR.\r\n\r\nIn a case where column order is known, using ""shims"" is better: one step convert from incoming data type to writer.', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVBatchReader.java,"@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.store.easy.EasyEVFBatchReader;
+
+public class LTSVBatchReader extends EasyEVFBatchReader {
+
+  public LTSVBatchReader(LTSVFormatPluginConfig formatConfig) {","[{'comment': 'Why batch reader needs to except format plugin if it does not use it?\r\nI think Constructor is redundant at all, default can be used.', 'commenter': 'arina-ielchiieva'}, {'comment': 'For this example, the config is not needed.  However, for other plugins, it is possible that they will need to access the variables in the config file. ', 'commenter': 'cgivre'}]"
1962,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/EasyEVFBatchReader.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.easy;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Iterator;
+
+public abstract class EasyEVFBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(EasyEVFBatchReader.class);
+
+  public FileSplit split;
+
+  public Iterator fileIterator;
+
+  public ResultSetLoader loader;
+
+  private RowSetLoader rowWriter;
+
+  public InputStream fsStream;
+
+  public BufferedReader reader;
+
+  public EasyEVFBatchReader() {
+  }
+
+  public RowSetLoader getRowWriter() {
+    return rowWriter;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    this.split = negotiator.split();
+    this.loader = negotiator.build();
+    this.rowWriter = loader.writer();
+    try {
+      this.fsStream = negotiator.fileSystem().openPossiblyCompressedStream(split.getPath());
+      this.reader = new BufferedReader(new InputStreamReader(fsStream, StandardCharsets.UTF_8));
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(String.format(""Failed to open input file: %s"", split.getPath()))
+        .build(logger);
+    }
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    while (!rowWriter.isFull()) {
+      if (!fileIterator.hasNext()) {
+        return false;
+      }
+      fileIterator.next();
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      if (reader != null) {
+        reader.close();
+        reader = null;
+      }
+      if (fsStream != null) {
+        fsStream.close();
+        fsStream = null;
+      }
+    } catch (IOException e) {
+      logger.warn(""Error closing batch Record Reader."");
+    }
+  }
+
+  /**
+   * This function should be used when writing Drill columns when the schema is not fixed or known in advance. At present only simple data types are
+   * supported with this function.  If the column is not present in the schema, this function will add it first.
+   * @param rowWriter The RowSetWriter to which the data is to be written
+   * @param name The field name
+   * @param value The field value.  Cannot be a primitive.
+   * @param type The Drill data type of the field.
+   */
+  public static void writeColumn(TupleWriter rowWriter, String name, Object value, TypeProtos.MinorType type) {
+    int index = rowWriter.tupleSchema().index(name);
+    if (index == -1) {
+      ColumnMetadata colSchema = MetadataUtils.newScalar(name, type, TypeProtos.DataMode.OPTIONAL);
+      index = rowWriter.addColumn(colSchema);
+    }
+    ScalarWriter colWriter = rowWriter.scalar(index);
+    switch (type) {
+      case INT:
+        colWriter.setInt((Integer)value);
+        break;
+      case BIGINT:
+        colWriter.setLong((Long)value);
+        break;
+      case VARCHAR:
+        colWriter.setString((String)value);
+        break;
+      case FLOAT4:
+      case FLOAT8:
+        colWriter.setDouble((Double)value);
+        break;
+      case DATE:
+        colWriter.setDate((LocalDate) value);
+        break;
+      case BIT:
+        colWriter.setBoolean((Boolean)value);
+        break;
+      case TIMESTAMP:
+      case TIMESTAMPTZ:
+        colWriter.setTimestamp((Instant) value);
+        break;
+      case TIME:
+        colWriter.setTime((LocalTime) value);
+        break;
+      case INTERVAL:
+        colWriter.setPeriod((Period)value);
+        break;
+      default:
+        logger.warn(""Does not support data type {}"", type.toString());","[{'comment': 'We can add more details, column name for instance...', 'commenter': 'arina-ielchiieva'}]"
1962,contrib/format-ltsv/src/test/java/org/apache/drill/exec/store/ltsv/TestLTSVRecordReader.java,"@@ -17,84 +17,190 @@
  */
 package org.apache.drill.exec.store.ltsv;
 
+import org.apache.drill.categories.RowSetTests;
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.ExecTest;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetBuilder;
 import org.apache.drill.exec.proto.UserBitShared;
-import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.dfs.FileSystemConfig;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.ZipCodec;
 import org.apache.drill.test.ClusterFixture;
 import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryBuilder;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
 import org.junit.BeforeClass;
 import org.junit.Test;
+import org.junit.experimental.categories.Category;
 
-import java.util.HashMap;
-import java.util.Map;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.nio.file.Paths;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
+@Category(RowSetTests.class)
 public class TestLTSVRecordReader extends ClusterTest {
 
   @BeforeClass
   public static void setup() throws Exception {
-    startCluster(ClusterFixture.builder(dirTestWatcher));
-
-    DrillbitContext context = cluster.drillbit().getContext();
-    FileSystemConfig original = (FileSystemConfig) context.getStorage().getPlugin(""cp"").getConfig();
-    Map<String, FormatPluginConfig> newFormats = new HashMap<>(original.getFormats());
-    newFormats.put(""ltsv"", new LTSVFormatPluginConfig());
-    FileSystemConfig pluginConfig = new FileSystemConfig(original.getConnection(), original.getConfig(), original.getWorkspaces(), newFormats);
-    pluginConfig.setEnabled(true);
-    context.getStorage().createOrUpdate(""cp"", pluginConfig, true);
+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));","[{'comment': '```suggestion\r\n    startCluster(ClusterFixture.builder(dirTestWatcher));\r\n```', 'commenter': 'arina-ielchiieva'}]"
1962,contrib/format-ltsv/src/test/java/org/apache/drill/exec/store/ltsv/TestLTSVRecordReader.java,"@@ -17,84 +17,190 @@
  */
 package org.apache.drill.exec.store.ltsv;
 
+import org.apache.drill.categories.RowSetTests;
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.ExecTest;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetBuilder;
 import org.apache.drill.exec.proto.UserBitShared;
-import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.dfs.FileSystemConfig;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.ZipCodec;
 import org.apache.drill.test.ClusterFixture;
 import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryBuilder;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
 import org.junit.BeforeClass;
 import org.junit.Test;
+import org.junit.experimental.categories.Category;
 
-import java.util.HashMap;
-import java.util.Map;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.nio.file.Paths;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
+@Category(RowSetTests.class)
 public class TestLTSVRecordReader extends ClusterTest {
 
   @BeforeClass
   public static void setup() throws Exception {
-    startCluster(ClusterFixture.builder(dirTestWatcher));
-
-    DrillbitContext context = cluster.drillbit().getContext();
-    FileSystemConfig original = (FileSystemConfig) context.getStorage().getPlugin(""cp"").getConfig();
-    Map<String, FormatPluginConfig> newFormats = new HashMap<>(original.getFormats());
-    newFormats.put(""ltsv"", new LTSVFormatPluginConfig());
-    FileSystemConfig pluginConfig = new FileSystemConfig(original.getConnection(), original.getConfig(), original.getWorkspaces(), newFormats);
-    pluginConfig.setEnabled(true);
-    context.getStorage().createOrUpdate(""cp"", pluginConfig, true);
+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));
+
+    LTSVFormatPluginConfig formatConfig = new LTSVFormatPluginConfig();
+    cluster.defineFormat(""cp"", ""ltsv"", formatConfig);
+
+    // Needed for compressed file unit test
+    dirTestWatcher.copyResourceToRoot(Paths.get(""ltsv/""));
   }
 
   @Test
   public void testWildcard() throws Exception {
-    testBuilder()
-      .sqlQuery(""SELECT * FROM cp.`simple.ltsv`"")
-      .unOrdered()
-      .baselineColumns(""host"", ""forwardedfor"", ""req"", ""status"", ""size"", ""referer"", ""ua"", ""reqtime"", ""apptime"", ""vhost"")
-      .baselineValues(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/xxx HTTP/1.1"", ""200"", ""4968"", ""-"", ""Java/1.8.0_131"", ""2.532"", ""2.532"", ""api.example.com"")
-      .baselineValues(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
-      .go();
+    String sql = ""SELECT * FROM cp.`ltsv/simple.ltsv`"";
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""host"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""forwardedfor"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""req"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""status"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""size"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""referer"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""ua"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""reqtime"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""apptime"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""vhost"",  TypeProtos.MinorType.VARCHAR)
+      .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), expectedSchema)
+      .addRow(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/xxx HTTP/1.1"", ""200"", ""4968"", ""-"", ""Java/1.8.0_131"", ""2.532"", ""2.532"", ""api.example.com"")
+      .addRow(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
   }
 
   @Test
   public void testSelectColumns() throws Exception {
-    testBuilder()
-      .sqlQuery(""SELECT ua, reqtime FROM cp.`simple.ltsv`"")
-      .unOrdered()
-      .baselineColumns(""ua"", ""reqtime"")
-      .baselineValues(""Java/1.8.0_131"", ""2.532"")
-      .baselineValues(""Java/1.8.0_201"", ""3.580"")
-      .go();
+    String sql = ""SELECT ua, reqtime FROM cp.`ltsv/simple.ltsv`"";
+
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""ua"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""reqtime"",  TypeProtos.MinorType.VARCHAR)
+      .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), expectedSchema)
+      .addRow(""Java/1.8.0_131"", ""2.532"")
+      .addRow(""Java/1.8.0_201"", ""3.580"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
   }
 
   @Test
   public void testQueryWithConditions() throws Exception {
-    testBuilder()
-      .sqlQuery(""SELECT * FROM cp.`simple.ltsv` WHERE reqtime > 3.0"")
-      .unOrdered()
-      .baselineColumns(""host"", ""forwardedfor"", ""req"", ""status"", ""size"", ""referer"", ""ua"", ""reqtime"", ""apptime"", ""vhost"")
-      .baselineValues(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
-      .go();
+    String sql = ""SELECT * FROM cp.`ltsv/simple.ltsv` WHERE reqtime > 3.0"";
+
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""host"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""forwardedfor"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""req"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""status"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""size"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""referer"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""ua"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""reqtime"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""apptime"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""vhost"",  TypeProtos.MinorType.VARCHAR)
+      .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), expectedSchema)
+      .addRow(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
   }
 
   @Test
   public void testSkipEmptyLines() throws Exception {
-    assertEquals(2, queryBuilder().sql(""SELECT * FROM cp.`emptylines.ltsv`"").run().recordCount());
+    assertEquals(2, queryBuilder().sql(""SELECT * FROM cp.`ltsv/emptylines.ltsv`"").run().recordCount());
   }
 
   @Test
   public void testReadException() throws Exception {
     try {
-      run(""SELECT * FROM cp.`invalid.ltsv`"");
+      run(""SELECT * FROM cp.`ltsv/invalid.ltsv`"");
       fail();
     } catch (UserException e) {
       assertEquals(UserBitShared.DrillPBError.ErrorType.DATA_READ, e.getErrorType());
-      assertTrue(e.getMessage().contains(""Failure while reading messages from /invalid.ltsv. Record reader was at record: 1""));
+      assertTrue(e.getMessage().contains(""Invalid LTSV format at line 1: time:30/Nov/2016:00:55:08 +0900""));
     }
   }
 
+  @Test
+  public void testSerDe() throws Exception {
+    String sql = ""SELECT COUNT(*) as cnt FROM cp.`ltsv/simple.ltsv`"";
+    String plan = queryBuilder().sql(sql).explainJson();
+    long cnt = queryBuilder().physical(plan).singletonLong();
+    assertEquals(""Counts should match"",2L, cnt);
+  }
+
+  @Test
+  public void testSelectColumnWithCompressedFile() throws Exception {
+    generateCompressedFile(""ltsv/simple.ltsv"", ""zip"", ""ltsv/simple.ltsv.zip"" );
+
+    String sql = ""SELECT ua, reqtime FROM dfs.`ltsv/simple.ltsv.zip`"";
+
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""ua"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""reqtime"",  TypeProtos.MinorType.VARCHAR)
+      .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), expectedSchema)
+      .addRow(""Java/1.8.0_131"", ""2.532"")
+      .addRow(""Java/1.8.0_201"", ""3.580"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
+  }
+
+  private void generateCompressedFile(String fileName, String codecName, String outFileName) throws IOException {","[{'comment': ""You have already repeated this code in several test, let's move code to the method `ClusterFixture` and reuse it. Also clean usage in other tests."", 'commenter': 'arina-ielchiieva'}]"
1962,contrib/format-ltsv/src/test/java/org/apache/drill/exec/store/ltsv/TestLTSVRecordReader.java,"@@ -17,84 +17,190 @@
  */
 package org.apache.drill.exec.store.ltsv;
 
+import org.apache.drill.categories.RowSetTests;
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.ExecTest;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetBuilder;
 import org.apache.drill.exec.proto.UserBitShared;
-import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.dfs.FileSystemConfig;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.ZipCodec;
 import org.apache.drill.test.ClusterFixture;
 import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryBuilder;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
 import org.junit.BeforeClass;
 import org.junit.Test;
+import org.junit.experimental.categories.Category;
 
-import java.util.HashMap;
-import java.util.Map;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.nio.file.Paths;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
+@Category(RowSetTests.class)
 public class TestLTSVRecordReader extends ClusterTest {
 
   @BeforeClass
   public static void setup() throws Exception {
-    startCluster(ClusterFixture.builder(dirTestWatcher));
-
-    DrillbitContext context = cluster.drillbit().getContext();
-    FileSystemConfig original = (FileSystemConfig) context.getStorage().getPlugin(""cp"").getConfig();
-    Map<String, FormatPluginConfig> newFormats = new HashMap<>(original.getFormats());
-    newFormats.put(""ltsv"", new LTSVFormatPluginConfig());
-    FileSystemConfig pluginConfig = new FileSystemConfig(original.getConnection(), original.getConfig(), original.getWorkspaces(), newFormats);
-    pluginConfig.setEnabled(true);
-    context.getStorage().createOrUpdate(""cp"", pluginConfig, true);
+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));
+
+    LTSVFormatPluginConfig formatConfig = new LTSVFormatPluginConfig();
+    cluster.defineFormat(""cp"", ""ltsv"", formatConfig);
+
+    // Needed for compressed file unit test
+    dirTestWatcher.copyResourceToRoot(Paths.get(""ltsv/""));
   }
 
   @Test
   public void testWildcard() throws Exception {
-    testBuilder()
-      .sqlQuery(""SELECT * FROM cp.`simple.ltsv`"")
-      .unOrdered()
-      .baselineColumns(""host"", ""forwardedfor"", ""req"", ""status"", ""size"", ""referer"", ""ua"", ""reqtime"", ""apptime"", ""vhost"")
-      .baselineValues(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/xxx HTTP/1.1"", ""200"", ""4968"", ""-"", ""Java/1.8.0_131"", ""2.532"", ""2.532"", ""api.example.com"")
-      .baselineValues(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
-      .go();
+    String sql = ""SELECT * FROM cp.`ltsv/simple.ltsv`"";
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""host"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""forwardedfor"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""req"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""status"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""size"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""referer"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""ua"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""reqtime"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""apptime"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""vhost"",  TypeProtos.MinorType.VARCHAR)
+      .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), expectedSchema)
+      .addRow(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/xxx HTTP/1.1"", ""200"", ""4968"", ""-"", ""Java/1.8.0_131"", ""2.532"", ""2.532"", ""api.example.com"")
+      .addRow(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
   }
 
   @Test
   public void testSelectColumns() throws Exception {
-    testBuilder()
-      .sqlQuery(""SELECT ua, reqtime FROM cp.`simple.ltsv`"")
-      .unOrdered()
-      .baselineColumns(""ua"", ""reqtime"")
-      .baselineValues(""Java/1.8.0_131"", ""2.532"")
-      .baselineValues(""Java/1.8.0_201"", ""3.580"")
-      .go();
+    String sql = ""SELECT ua, reqtime FROM cp.`ltsv/simple.ltsv`"";
+
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""ua"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""reqtime"",  TypeProtos.MinorType.VARCHAR)
+      .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), expectedSchema)
+      .addRow(""Java/1.8.0_131"", ""2.532"")
+      .addRow(""Java/1.8.0_201"", ""3.580"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
   }
 
   @Test
   public void testQueryWithConditions() throws Exception {
-    testBuilder()
-      .sqlQuery(""SELECT * FROM cp.`simple.ltsv` WHERE reqtime > 3.0"")
-      .unOrdered()
-      .baselineColumns(""host"", ""forwardedfor"", ""req"", ""status"", ""size"", ""referer"", ""ua"", ""reqtime"", ""apptime"", ""vhost"")
-      .baselineValues(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
-      .go();
+    String sql = ""SELECT * FROM cp.`ltsv/simple.ltsv` WHERE reqtime > 3.0"";
+
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""host"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""forwardedfor"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""req"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""status"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""size"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""referer"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""ua"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""reqtime"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""apptime"",  TypeProtos.MinorType.VARCHAR)
+      .addNullable(""vhost"",  TypeProtos.MinorType.VARCHAR)
+      .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), expectedSchema)
+      .addRow(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
   }
 
   @Test
   public void testSkipEmptyLines() throws Exception {
-    assertEquals(2, queryBuilder().sql(""SELECT * FROM cp.`emptylines.ltsv`"").run().recordCount());
+    assertEquals(2, queryBuilder().sql(""SELECT * FROM cp.`ltsv/emptylines.ltsv`"").run().recordCount());
   }
 
   @Test
   public void testReadException() throws Exception {
     try {
-      run(""SELECT * FROM cp.`invalid.ltsv`"");
+      run(""SELECT * FROM cp.`ltsv/invalid.ltsv`"");
       fail();
     } catch (UserException e) {
       assertEquals(UserBitShared.DrillPBError.ErrorType.DATA_READ, e.getErrorType());
-      assertTrue(e.getMessage().contains(""Failure while reading messages from /invalid.ltsv. Record reader was at record: 1""));
+      assertTrue(e.getMessage().contains(""Invalid LTSV format at line 1: time:30/Nov/2016:00:55:08 +0900""));
     }
   }
 
+  @Test
+  public void testSerDe() throws Exception {
+    String sql = ""SELECT COUNT(*) as cnt FROM cp.`ltsv/simple.ltsv`"";
+    String plan = queryBuilder().sql(sql).explainJson();
+    long cnt = queryBuilder().physical(plan).singletonLong();
+    assertEquals(""Counts should match"",2L, cnt);
+  }
+
+  @Test","[{'comment': 'Also please add test with provided schema.', 'commenter': 'arina-ielchiieva'}]"
1962,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/EasyEVFBatchReader.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.easy;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Iterator;
+
+public abstract class EasyEVFBatchReader implements ManagedReader<FileSchemaNegotiator> {","[{'comment': 'Please add java doc.', 'commenter': 'arina-ielchiieva'}]"
1962,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/EasyEVFBatchReader.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.easy;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.joda.time.Instant;
+import org.joda.time.LocalDate;
+import org.joda.time.LocalTime;
+import org.joda.time.Period;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Iterator;
+
+public abstract class EasyEVFBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(EasyEVFBatchReader.class);","[{'comment': 'This abstraction applies only for the LTSV reader? Is there any other formats already written on EVF can use it?', 'commenter': 'arina-ielchiieva'}, {'comment': ""@arina-ielchiieva \r\nI wanted to try to hide as much of the boiler plate code for format plugins as possible.  I wanted to start with the LTSV plugin since it is relatively simple.  I think this would be useful for simple formats (image, httpd, etc) and future formats.  \r\n\r\nI don't think there is any point in converting EVF plugins to use this, but my goal was to make it easier for future EVF based plugins. \r\n\r\nLTSV doesn't know the schema in advance, so this one just generates the columns on the fly.  The next iteration of this will include features to define the schema in advance, like HTTPD does."", 'commenter': 'cgivre'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())","[{'comment': 'This is not a logger, use %s', 'commenter': 'arina-ielchiieva'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public boolean hasNext() {
+    return line != null;
+  }
+
+  @Override
+  public Boolean next() {
+    // Skip empty lines
+    if (line.trim().length() == 0) {
+      try {
+        // Advance the line to the next line
+        line = reader.readLine();
+      } catch (IOException e) {
+        throw UserException
+          .dataReadError()
+          .message(""Error reading LTSV Data: {}"", e.getMessage())","[{'comment': 'The same here.', 'commenter': 'arina-ielchiieva'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public boolean hasNext() {
+    return line != null;
+  }
+
+  @Override
+  public Boolean next() {
+    // Skip empty lines
+    if (line.trim().length() == 0) {
+      try {
+        // Advance the line to the next line
+        line = reader.readLine();
+      } catch (IOException e) {
+        throw UserException
+          .dataReadError()
+          .message(""Error reading LTSV Data: {}"", e.getMessage())
+          .build(logger);
+      }
+      return Boolean.TRUE;
+    } else if (line == null) {
+      return Boolean.FALSE;
+    }
+
+    // Process the row
+    processRow();
+
+    // Increment record counter
+    recordCount++;
+
+    // Get the next line
+    try {
+      line = reader.readLine();
+      if(line == null) {
+        return Boolean.FALSE;
+      }
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())","[{'comment': 'The same here.', 'commenter': 'arina-ielchiieva'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVFormatPlugin.java,"@@ -15,78 +15,74 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.drill.exec.store.ltsv;
 
-import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.common.logical.StoragePluginConfig;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.planner.common.DrillStatsTable.TableStatistics;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileReaderFactory;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileScanBuilder;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
 import org.apache.drill.exec.proto.UserBitShared;
 import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.RecordReader;
-import org.apache.drill.exec.store.RecordWriter;
-import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.server.options.OptionManager;
 import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;
-import org.apache.drill.exec.store.dfs.easy.EasyWriter;
-import org.apache.drill.exec.store.dfs.easy.FileWork;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
-import java.util.List;
 
 public class LTSVFormatPlugin extends EasyFormatPlugin<LTSVFormatPluginConfig> {","[{'comment': 'Would be handy to reference [the spec](http://ltsv.org/) for us newbies not familiar with the format.', 'commenter': 'paul-rogers'}]"
1962,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVRecordIterator.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.Iterator;
+
+public class LTSVRecordIterator implements Iterator {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVRecordIterator.class);
+
+  private RowSetLoader rowWriter;
+
+  private BufferedReader reader;
+
+  private String line;
+
+  private int recordCount;
+
+  public LTSVRecordIterator(RowSetLoader rowWriter, BufferedReader reader) {
+    this.rowWriter = rowWriter;
+    this.reader = reader;
+
+    // Get the first line
+    try {
+      line = reader.readLine();
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public boolean hasNext() {
+    return line != null;
+  }
+
+  @Override
+  public Boolean next() {
+    // Skip empty lines
+    if (line.trim().length() == 0) {
+      try {
+        // Advance the line to the next line
+        line = reader.readLine();
+      } catch (IOException e) {
+        throw UserException
+          .dataReadError()
+          .message(""Error reading LTSV Data: {}"", e.getMessage())
+          .build(logger);
+      }
+      return Boolean.TRUE;
+    } else if (line == null) {
+      return Boolean.FALSE;
+    }
+
+    // Process the row
+    processRow();
+
+    // Increment record counter
+    recordCount++;
+
+    // Get the next line
+    try {
+      line = reader.readLine();
+      if(line == null) {
+        return Boolean.FALSE;
+      }
+    } catch (IOException e) {
+      throw UserException
+        .dataReadError()
+        .message(""Error reading LTSV Data: {}"", e.getMessage())
+        .build(logger);
+    }
+    return Boolean.TRUE;
+  }
+
+  /**
+   * Function processes one row of data, splitting it up first by tabs then splitting the key/value pairs
+   * finally recording it in the current Drill row.
+   */
+  private void processRow() {
+    // Start the row
+    rowWriter.start();","[{'comment': 'By the way, I recently was reminded that `start()` returns a `boolean`. We usually write\r\n\r\n```\r\n  while (!rs.isFull()) {\r\n    rowWriter.start();\r\n    ...\r\n  }\r\n```\r\n\r\nI now remember that the code was designed to allow:\r\n\r\n```\r\n   while (rowWriter.start()) {\r\n      // Write stuff\r\n      rowWriter.end();\r\n  }\r\n```', 'commenter': 'paul-rogers'}]"
1972,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/auth/DrillHttpSecurityHandlerProvider.java,"@@ -191,6 +200,7 @@ public boolean isFormEnabled() {
             AuthStringUtil.asSet(config.getStringList(ExecConstants.HTTP_AUTHENTICATION_MECHANISMS)));
       } else {
         // For backward compatibility
+        configuredMechs.add(Constraint.__BASIC_AUTH);","[{'comment': ""Please don't enable basic by default. "", 'commenter': 'ihuzenko'}]"
1972,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/auth/DrillHttpSecurityHandlerProvider.java,"@@ -139,10 +140,14 @@ public void handle(String target, Request baseRequest, HttpServletRequest reques
       if (isSpnegoEnabled() && (!isFormEnabled() || uri.equals(WebServerConstants.SPENGO_LOGIN_RESOURCE_PATH))) {
         securityHandler = securityHandlers.get(Constraint.__SPNEGO_AUTH);
         securityHandler.handle(target, baseRequest, request, response);
+      } else if(isBasicEnabled() && request.getHeader(HttpHeader.AUTHORIZATION.asString()) != null) {
+        final DrillHttpConstraintSecurityHandler basicSecurityHandler = securityHandlers.get(Constraint.__BASIC_AUTH);
+        basicSecurityHandler.handle(target, baseRequest, request, response);","[{'comment': 'please reuse already defined ```securityHandler``` variable as was done for other cases. ', 'commenter': 'ihuzenko'}]"
1972,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/auth/HttpBasicAuthSecurityHandler.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.server.rest.auth;
+
+import org.apache.drill.common.exceptions.DrillException;
+import org.apache.drill.exec.rpc.security.plain.PlainFactory;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.eclipse.jetty.security.authentication.BasicAuthenticator;
+import org.eclipse.jetty.util.security.Constraint;
+
+/**
+ * Implement HTTP Basic authentication for REST API access
+ */
+public class HttpBasicAuthSecurityHandler extends DrillHttpConstraintSecurityHandler {
+  @Override
+  public String getImplName() {
+    return Constraint.__BASIC_AUTH;
+  }
+
+  @Override
+  public void doSetup(DrillbitContext dbContext) throws DrillException {
+
+    // Check if PAMAuthenticator is available or not which is required for FORM authentication
+    if (!dbContext.getAuthProvider().containsFactory(PlainFactory.SIMPLE_NAME)) {
+      throw new DrillException(""BASIC auth mechanism was configured but PLAIN mechanism is not enabled to provide an "" +
+        ""authenticator. Please configure user authentication with PLAIN mechanism and authenticator to use "" +
+        ""BASIC authentication"");
+    }","[{'comment': 'this is very similar to check done in  ```FormSecurityHanlder``` , maybe makes sense to create protected method in base class and use it for both cases?', 'commenter': 'ihuzenko'}, {'comment': ""I don't think it's worth creating whole class just to reuse what amounts to basically 2 lines of code.  Maybe something for another day."", 'commenter': 'dobesv'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *","[{'comment': 'Nit: Javadoc needs an HTML `<p>` to separate paragraphs.', 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You","[{'comment': 'Would a better default be `${DRILL_HOME}/conf`? This allows easier setup on an informal install where Drill is dropped into a home folder. It would be more consistent with our other config files. Of course, if Drill is installed in `/opt/drill`, then the result is the same.\r\n\r\n`${DRILL_HOME}/conf` is on the class path, so you can load the file as a resource by default. If the config value is set, then load it from the local file system as in the present code.', 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);","[{'comment': ""Please use just `Logger` and `LoggerFactory` and import the needed sflj classes. (We have lots of mechanically-edited code we're trying to clean up.) Also, please use your own class for the logger, `HtpasswdFileUserAuthenticator`. That way we know where to look in the code to match up log messages."", 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;","[{'comment': 'Please use modifiers, `private` preferred.', 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  long lastModified;
+  HashMap<String, String> userToPassword;
+
+  @Override
+  public void setup(DrillConfig drillConfig) throws DrillbitStartupException {
+    if (drillConfig.hasPath(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH)) {
+      path = drillConfig.getString(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH);
+    }
+  }
+
+","[{'comment': 'Nit: one blank like between methods. These open-source coding standards are a hassle, but necessary to keep us all pointed in the same direction.', 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  long lastModified;
+  HashMap<String, String> userToPassword;","[{'comment': 'A good general rule is to declare variables with the base type, `Map`, but allocate the implementation you want, here `HashMap`.', 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  long lastModified;
+  HashMap<String, String> userToPassword;
+
+  @Override
+  public void setup(DrillConfig drillConfig) throws DrillbitStartupException {
+    if (drillConfig.hasPath(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH)) {
+      path = drillConfig.getString(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH);
+    }
+  }
+
+
+  /**
+   * Check password against hash read from the file
+   *
+   * @param password User provided password
+   * @param hash     Hash stored in the htpasswd file
+   * @return true if the password matched the hash
+   */
+  public static boolean checkPassword(String password, String hash) {","[{'comment': ""Nit: best to use a name that flows in an if-statement. Maybe `isPasswordValid`. So:\r\n\r\n```\r\nif (isPasswordValid(...)) {\r\n  // Welcome!\r\n} else {\r\n  // You're outta here!\r\n}\r\n```"", 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  long lastModified;
+  HashMap<String, String> userToPassword;
+
+  @Override
+  public void setup(DrillConfig drillConfig) throws DrillbitStartupException {
+    if (drillConfig.hasPath(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH)) {
+      path = drillConfig.getString(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH);
+    }
+  }
+
+
+  /**
+   * Check password against hash read from the file
+   *
+   * @param password User provided password
+   * @param hash     Hash stored in the htpasswd file
+   * @return true if the password matched the hash
+   */
+  public static boolean checkPassword(String password, String hash) {
+    if (hash.startsWith(""$apr1$"")) {
+      return hash.equals(Md5Crypt.apr1Crypt(password, hash));
+    } else if (hash.startsWith(""$1$"")) {
+      return hash.equals(Md5Crypt.md5Crypt(password.getBytes(Charsets.UTF_8), hash));
+    } else if (hash.startsWith(""{SHA}"")) {
+      return hash.substring(5).equals(Base64.getEncoder().encodeToString(DigestUtils.sha1(password)));
+    } else if (hash.startsWith(""$2y$"")) {
+      // bcrypt not supported currently
+      return false;
+    } else {
+      return hash.equals(password);
+    }
+  }
+
+  /**
+   * Validate the given username and password against the password file
+   *
+   * @param username Username provided
+   * @param password Password provided
+   * @throws UserAuthenticationException If the username and password could not be validated
+   */
+  @Override
+  public void authenticate(String username, String password) throws UserAuthenticationException {
+    read();
+    String hash = this.userToPassword.get(username);
+    logger.error(""Testing "" + hash + "" against password "" + password);
+    boolean credentialsAccepted = (hash != null && this.checkPassword(password, hash));
+    if (!credentialsAccepted) {
+      throw new UserAuthenticationException(String.format(""htpasswd auth failed for user '%s'"",
+        username));
+    }
+  }
+
+  /**
+   * Read the password file into the map, if the file has changed since we last read it
+   */
+  protected synchronized void read() {
+    File file = new File(path);","[{'comment': 'Here is where you\'ll need a different implementation if the file is a resource vs. a file system path. Since the code cleverly checks for file changes, the best path would be to convert the resource URL into a `File` if using the default location:\r\n\r\n```\r\n    File file;\r\n    URL url = getClass().getResource(""htpasswd"");\r\n    if (url != null) {\r\n      file = new File(url.getPath());\r\n    } else {\r\n      file = new File(customPath);\r\n    }\r\n```\r\n\r\nThere should be a constant for ""htpasswd"".\r\n\r\nNote: it may be necessary to prefix the resource name with a slash: ""/htpasswd"". There is some strangeness in how Drill locates resources at the top of the class path that I don\'t fully understand.', 'commenter': 'paul-rogers'}, {'comment': ""I tried this but couldn't get it to work, I think it's beyond my depth.  Perhaps someone can add this feature later as a follow-up task, if it's useful to them.\r\n"", 'commenter': 'dobesv'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  long lastModified;
+  HashMap<String, String> userToPassword;
+
+  @Override
+  public void setup(DrillConfig drillConfig) throws DrillbitStartupException {
+    if (drillConfig.hasPath(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH)) {
+      path = drillConfig.getString(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH);
+    }
+  }
+
+
+  /**
+   * Check password against hash read from the file
+   *
+   * @param password User provided password
+   * @param hash     Hash stored in the htpasswd file
+   * @return true if the password matched the hash
+   */
+  public static boolean checkPassword(String password, String hash) {
+    if (hash.startsWith(""$apr1$"")) {
+      return hash.equals(Md5Crypt.apr1Crypt(password, hash));
+    } else if (hash.startsWith(""$1$"")) {
+      return hash.equals(Md5Crypt.md5Crypt(password.getBytes(Charsets.UTF_8), hash));
+    } else if (hash.startsWith(""{SHA}"")) {
+      return hash.substring(5).equals(Base64.getEncoder().encodeToString(DigestUtils.sha1(password)));
+    } else if (hash.startsWith(""$2y$"")) {
+      // bcrypt not supported currently
+      return false;
+    } else {
+      return hash.equals(password);
+    }
+  }
+
+  /**
+   * Validate the given username and password against the password file
+   *
+   * @param username Username provided
+   * @param password Password provided
+   * @throws UserAuthenticationException If the username and password could not be validated
+   */
+  @Override
+  public void authenticate(String username, String password) throws UserAuthenticationException {
+    read();
+    String hash = this.userToPassword.get(username);
+    logger.error(""Testing "" + hash + "" against password "" + password);
+    boolean credentialsAccepted = (hash != null && this.checkPassword(password, hash));
+    if (!credentialsAccepted) {
+      throw new UserAuthenticationException(String.format(""htpasswd auth failed for user '%s'"",
+        username));
+    }
+  }
+
+  /**
+   * Read the password file into the map, if the file has changed since we last read it
+   */
+  protected synchronized void read() {
+    File file = new File(path);
+    if (file.exists() && (userToPassword == null || file.lastModified() != lastModified)) {","[{'comment': ""Nit: if this form of authentication is enabled (which, I presume, is why we're in this class), but the file is not found or not readable, maybe log a message? Else, it will be a mystery why things don't work."", 'commenter': 'paul-rogers'}, {'comment': 'This logging is already present in the `else` clause below.\r\n', 'commenter': 'dobesv'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  long lastModified;
+  HashMap<String, String> userToPassword;
+
+  @Override
+  public void setup(DrillConfig drillConfig) throws DrillbitStartupException {
+    if (drillConfig.hasPath(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH)) {
+      path = drillConfig.getString(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH);
+    }
+  }
+
+
+  /**
+   * Check password against hash read from the file
+   *
+   * @param password User provided password
+   * @param hash     Hash stored in the htpasswd file
+   * @return true if the password matched the hash
+   */
+  public static boolean checkPassword(String password, String hash) {
+    if (hash.startsWith(""$apr1$"")) {
+      return hash.equals(Md5Crypt.apr1Crypt(password, hash));
+    } else if (hash.startsWith(""$1$"")) {
+      return hash.equals(Md5Crypt.md5Crypt(password.getBytes(Charsets.UTF_8), hash));
+    } else if (hash.startsWith(""{SHA}"")) {
+      return hash.substring(5).equals(Base64.getEncoder().encodeToString(DigestUtils.sha1(password)));
+    } else if (hash.startsWith(""$2y$"")) {
+      // bcrypt not supported currently
+      return false;
+    } else {
+      return hash.equals(password);
+    }
+  }
+
+  /**
+   * Validate the given username and password against the password file
+   *
+   * @param username Username provided
+   * @param password Password provided
+   * @throws UserAuthenticationException If the username and password could not be validated
+   */
+  @Override
+  public void authenticate(String username, String password) throws UserAuthenticationException {
+    read();
+    String hash = this.userToPassword.get(username);
+    logger.error(""Testing "" + hash + "" against password "" + password);
+    boolean credentialsAccepted = (hash != null && this.checkPassword(password, hash));
+    if (!credentialsAccepted) {
+      throw new UserAuthenticationException(String.format(""htpasswd auth failed for user '%s'"",
+        username));
+    }
+  }
+
+  /**
+   * Read the password file into the map, if the file has changed since we last read it
+   */
+  protected synchronized void read() {
+    File file = new File(path);
+    if (file.exists() && (userToPassword == null || file.lastModified() != lastModified)) {
+      HashMap<String, String> newMap = new HashMap<String, String>();
+      Pattern entry = Pattern.compile(""^([^:]+):([^:]+)"");
+      try (FileInputStream stream = new FileInputStream(file); Scanner scanner = new Scanner(stream)) {","[{'comment': 'A simpler solution is `BufferedReader reader = new BufferedReader(new FileReader(file))`. The buffered reader will read line-by-line by calling `readLine()`. See https://docs.oracle.com/javase/8/docs/api/java/io/BufferedReader.html', 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  long lastModified;
+  HashMap<String, String> userToPassword;
+
+  @Override
+  public void setup(DrillConfig drillConfig) throws DrillbitStartupException {
+    if (drillConfig.hasPath(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH)) {
+      path = drillConfig.getString(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH);
+    }
+  }
+
+
+  /**
+   * Check password against hash read from the file
+   *
+   * @param password User provided password
+   * @param hash     Hash stored in the htpasswd file
+   * @return true if the password matched the hash
+   */
+  public static boolean checkPassword(String password, String hash) {
+    if (hash.startsWith(""$apr1$"")) {
+      return hash.equals(Md5Crypt.apr1Crypt(password, hash));
+    } else if (hash.startsWith(""$1$"")) {
+      return hash.equals(Md5Crypt.md5Crypt(password.getBytes(Charsets.UTF_8), hash));
+    } else if (hash.startsWith(""{SHA}"")) {
+      return hash.substring(5).equals(Base64.getEncoder().encodeToString(DigestUtils.sha1(password)));
+    } else if (hash.startsWith(""$2y$"")) {
+      // bcrypt not supported currently
+      return false;
+    } else {
+      return hash.equals(password);
+    }
+  }
+
+  /**
+   * Validate the given username and password against the password file
+   *
+   * @param username Username provided
+   * @param password Password provided
+   * @throws UserAuthenticationException If the username and password could not be validated
+   */
+  @Override
+  public void authenticate(String username, String password) throws UserAuthenticationException {
+    read();
+    String hash = this.userToPassword.get(username);
+    logger.error(""Testing "" + hash + "" against password "" + password);
+    boolean credentialsAccepted = (hash != null && this.checkPassword(password, hash));
+    if (!credentialsAccepted) {
+      throw new UserAuthenticationException(String.format(""htpasswd auth failed for user '%s'"",
+        username));
+    }
+  }
+
+  /**
+   * Read the password file into the map, if the file has changed since we last read it
+   */
+  protected synchronized void read() {
+    File file = new File(path);
+    if (file.exists() && (userToPassword == null || file.lastModified() != lastModified)) {
+      HashMap<String, String> newMap = new HashMap<String, String>();
+      Pattern entry = Pattern.compile(""^([^:]+):([^:]+)"");
+      try (FileInputStream stream = new FileInputStream(file); Scanner scanner = new Scanner(stream)) {
+        while (scanner.hasNextLine()) {
+          String line = scanner.nextLine().trim();
+          if (!line.isEmpty() && !line.startsWith(""#"")) {
+            Matcher m = entry.matcher(line);
+            if (m.matches()) {
+              newMap.put(m.group(1), m.group(2));
+            }
+          }
+        }
+      } catch (Exception e) {
+        logger.error(MessageFormat.format(""Failed to read htpasswd file at path {0}"", file), e);","[{'comment': 'Maybe differentiate the two cases: 1) failed to read the file the first time, and so auth will be broken, or 2) failed to read an update; continuing to read the previous version.', 'commenter': 'paul-rogers'}, {'comment': ""I think it's better to completely fail in this case, otherwise they won't even realize they messed up the file and won't understand why their changes did not take effect.\r\n"", 'commenter': 'dobesv'}, {'comment': ""Might be good to fail if the file can't be read at all, but log an error if the file previously existed, but is now invalid. That way, an error won't take down the cluster.\r\n\r\nOf course, in your target use case, Docker, the file won't change within a single run; there is no mechanism to push an updated file to all Drillbits. (You could host-mount the location, and change the file on the host, but this seems to be far too complex and error prone.) The behavior seems fine: to change security, create an updated file and relaunch the container.\r\n\r\nAn ideal solution would be to allow auth as a plugin to call out to an auth service elsewhere on the cluster. I don't know enough about how that works in practice to suggest how that might work."", 'commenter': 'paul-rogers'}, {'comment': ""The logic currently will just log an error if the file fails to load for some reason.  If the file is updated, it could log another error if it still invalid, or it will load it successfully.  It doesn't shut down the drillbit in any case.\r\n\r\nI am planning to mount the file from a kubernetes secret, which means I can update it without restarting the drillbits.  It seems like a pain to restart the drillbits after adding/removing a user.  It can take a while to roll all the drillbits and may interrupt running operations and delete query profiles if you do that.\r\n"", 'commenter': 'dobesv'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Scanner;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ *
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ *
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ *
+ * By default this loads the passwords from the path <code>/opt/drill/conf/htpasswd</code>.  You
+ * can change the path by setting <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ *
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Pam4jUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  long lastModified;
+  HashMap<String, String> userToPassword;
+
+  @Override
+  public void setup(DrillConfig drillConfig) throws DrillbitStartupException {
+    if (drillConfig.hasPath(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH)) {
+      path = drillConfig.getString(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH);
+    }
+  }
+
+
+  /**
+   * Check password against hash read from the file
+   *
+   * @param password User provided password
+   * @param hash     Hash stored in the htpasswd file
+   * @return true if the password matched the hash
+   */
+  public static boolean checkPassword(String password, String hash) {
+    if (hash.startsWith(""$apr1$"")) {
+      return hash.equals(Md5Crypt.apr1Crypt(password, hash));
+    } else if (hash.startsWith(""$1$"")) {
+      return hash.equals(Md5Crypt.md5Crypt(password.getBytes(Charsets.UTF_8), hash));
+    } else if (hash.startsWith(""{SHA}"")) {
+      return hash.substring(5).equals(Base64.getEncoder().encodeToString(DigestUtils.sha1(password)));
+    } else if (hash.startsWith(""$2y$"")) {
+      // bcrypt not supported currently
+      return false;
+    } else {
+      return hash.equals(password);
+    }
+  }
+
+  /**
+   * Validate the given username and password against the password file
+   *
+   * @param username Username provided
+   * @param password Password provided
+   * @throws UserAuthenticationException If the username and password could not be validated
+   */
+  @Override
+  public void authenticate(String username, String password) throws UserAuthenticationException {
+    read();
+    String hash = this.userToPassword.get(username);
+    logger.error(""Testing "" + hash + "" against password "" + password);
+    boolean credentialsAccepted = (hash != null && this.checkPassword(password, hash));
+    if (!credentialsAccepted) {
+      throw new UserAuthenticationException(String.format(""htpasswd auth failed for user '%s'"",
+        username));
+    }
+  }
+
+  /**
+   * Read the password file into the map, if the file has changed since we last read it
+   */
+  protected synchronized void read() {
+    File file = new File(path);
+    if (file.exists() && (userToPassword == null || file.lastModified() != lastModified)) {
+      HashMap<String, String> newMap = new HashMap<String, String>();
+      Pattern entry = Pattern.compile(""^([^:]+):([^:]+)"");
+      try (FileInputStream stream = new FileInputStream(file); Scanner scanner = new Scanner(stream)) {
+        while (scanner.hasNextLine()) {
+          String line = scanner.nextLine().trim();
+          if (!line.isEmpty() && !line.startsWith(""#"")) {
+            Matcher m = entry.matcher(line);
+            if (m.matches()) {
+              newMap.put(m.group(1), m.group(2));
+            }
+          }
+        }
+      } catch (Exception e) {
+        logger.error(MessageFormat.format(""Failed to read htpasswd file at path {0}"", file), e);
+      }
+      lastModified = file.lastModified();
+      userToPassword = newMap;
+    } else if (userToPassword == null) {
+      logger.error(MessageFormat.format(""htpasswd file not found at path {0}"", file));
+      userToPassword = new HashMap();
+    }
+  }
+
+  /**
+   * Free resources associated with this authenticator
+   */
+  @Override
+  public void close() throws IOException {
+    lastModified = 0;
+    userToPassword = null;","[{'comment': 'Since no exception is actually thrown, you can leave off the `throws` declaration.', 'commenter': 'paul-rogers'}]"
1977,exec/java-exec/src/test/java/org/apache/drill/exec/rpc/user/security/TestHtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,154 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.drill.common.config.DrillProperties;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClientFixture;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.Test;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.Arrays;
+import java.util.List;
+
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class TestHtpasswdFileUserAuthenticator extends ClusterTest {
+  private File tempPasswdFile;
+
+
+  private void setupCluster(String passwdContent) throws IOException {
+    tempPasswdFile = new File(dirTestWatcher.getTmpDir(), ""htpasswd."" + System.currentTimeMillis());
+    Files.write(tempPasswdFile.toPath(), passwdContent.getBytes());
+
+    cluster = ClusterFixture.bareBuilder(dirTestWatcher)
+      .clusterSize(3)
+      .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true)
+      .configProperty(ExecConstants.USER_AUTHENTICATION_ENABLED, true)
+      .configProperty(ExecConstants.USER_AUTHENTICATOR_IMPL, ""htpasswd"")
+      .configProperty(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH, tempPasswdFile.toString())
+      .build();
+  }
+
+
+  @Test
+  public void passwordChecksGiveCorrectResults() throws Exception {
+    String passwdContent = ""alice:pass1\n"" +
+      ""bob:buzzkill\n"" +
+      ""jane:$apr1$PrwDfXy9$ajkhotQW6RFnoVQtPKoW4/\n"" +
+      ""john:$apr1$UxZgBU8k$K4UzdubNa741TnWAZY2QV0\n"";
+    setupCluster(passwdContent);
+
+
+    assertTrue(true);
+
+    tryCredentials(""alice"", ""pass1"", cluster, true);
+    tryCredentials(""bob"", ""buzzkill"", cluster, true);
+    tryCredentials(""notalice"", ""pass1"", cluster, false);
+    tryCredentials(""notbob"", ""buzzkill"", cluster, false);
+    tryCredentials(""alice"", ""wrong"", cluster, false);
+    tryCredentials(""bob"", ""incorrect"", cluster, false);
+    tryCredentials(""jane"", ""pass"", cluster, true);
+    tryCredentials(""john"", ""foobar"", cluster, true);
+    tryCredentials(""jane"", ""wrong"", cluster, false);
+    tryCredentials(""john"", ""incorrect1"", cluster, false);
+  }
+
+  @Test
+  public void rejectsLoginsWhenHtpasswdFileMissing() throws Exception {
+    cluster = ClusterFixture.bareBuilder(dirTestWatcher)
+      .clusterSize(3)
+      .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true)
+      .configProperty(ExecConstants.USER_AUTHENTICATION_ENABLED, true)
+      .configProperty(ExecConstants.USER_AUTHENTICATOR_IMPL, ""htpasswd"")
+      .configProperty(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH, ""/nonexistant-file"")
+      .build();
+    tryCredentials(""bob"", ""bob"", cluster, false);
+  }
+
+  @Test
+  public void detectsChanges() throws Exception {
+    String passwdContent = ""alice:pass1\nbob:buzzkill\n"";
+    setupCluster(passwdContent);
+
+    tryCredentials(""alice"", ""pass1"", cluster, true);
+    tryCredentials(""alice"", ""pass2"", cluster, false);
+    tryCredentials(""bob"", ""buzzkill"", cluster, true);
+    tryCredentials(""bob"", ""yolo"", cluster, false);
+
+    String passwdContent2 = ""alice:pass2\nbob:yolo\n"";
+    Files.write(tempPasswdFile.toPath(), passwdContent2.getBytes());
+    Thread.sleep(10);
+
+    tryCredentials(""alice"", ""pass1"", cluster, false);
+    tryCredentials(""alice"", ""pass2"", cluster, true);
+    tryCredentials(""bob"", ""buzzkill"", cluster, false);
+    tryCredentials(""bob"", ""yolo"", cluster, true);
+
+    // Invalid file is treated as empty
+    String passwdContent3 = ""invalid file"";
+    Files.write(tempPasswdFile.toPath(), passwdContent3.getBytes());
+    Thread.sleep(10);
+
+    tryCredentials(""alice"", ""pass1"", cluster, false);
+    tryCredentials(""alice"", ""pass2"", cluster, false);","[{'comment': ""Hmm so this is the test that fails in 1.8, but not the other JVM versions.  That would seem to imply that it did not pick up the change to the file this time around, but I don't have any new hypothesis as to why.  I had previously added some `Thread.sleep` calls, thinking maybe it was some sort of timing issue.\r\n\r\nAlso, the first change was apparently picked up successfully, so the change detection is working to some degree here.  I wonder if maybe the granularity of the `lastModified` could be the issue - maybe it's rounded to the nearest second or something?\r\n\r\n"", 'commenter': 'dobesv'}, {'comment': ""A bit of googling turns up a lack of subsecond precision in Java 1.8 and below, so I've added a 1 second delay in those versions"", 'commenter': 'dobesv'}]"
1977,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/security/HtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,145 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.commons.codec.digest.DigestUtils;
+import org.apache.commons.codec.digest.Md5Crypt;
+import org.apache.commons.io.Charsets;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.exception.DrillbitStartupException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.text.MessageFormat;
+import java.util.Base64;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Implementation of UserAuthenticator that reads passwords from an htpasswd
+ * formatted file.
+ * <p>
+ * Currently supports MD5, SHA-1, and plaintext passwords.
+ * <p>
+ * Use the htpasswd command line tool to create and modify htpasswd files.
+ * <p>
+ * By default this loads the passwords from <code>/opt/drill/conf/htpasswd</code>.  Users can change the path by
+ * putting the absolute file path as <code>drill.exec.security.user.auth.htpasswd.path</code> in
+ * <code>drill-override.conf</code>.
+ * <p>
+ * This is intended for situations where the list of users is relatively static, and you are running
+ * drill in a container so using pam is not convenient.
+ */
+@UserAuthenticatorTemplate(type = ""htpasswd"")
+public class HtpasswdFileUserAuthenticator implements UserAuthenticator {
+  private static final Logger logger = LoggerFactory.getLogger(HtpasswdFileUserAuthenticator.class);
+  public static final String DEFAULT_HTPASSWD_AUTHENTICATOR_PATH = ""/opt/drill/conf/htpasswd"";
+
+  private String path = DEFAULT_HTPASSWD_AUTHENTICATOR_PATH;
+  private long lastModified;
+  private Map<String, String> userToPassword;
+
+  @Override
+  public void setup(DrillConfig drillConfig) throws DrillbitStartupException {
+    if (drillConfig.hasPath(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH)) {
+      path = drillConfig.getString(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH);
+    }
+  }
+
+  /**
+   * Check password against hash read from the file
+   *
+   * @param password User provided password
+   * @param hash     Hash stored in the htpasswd file
+   * @return true if the password matched the hash
+   */
+  public static boolean isPasswordValid(String password, String hash) {
+    if (hash.startsWith(""$apr1$"")) {
+      return hash.equals(Md5Crypt.apr1Crypt(password, hash));
+    } else if (hash.startsWith(""$1$"")) {
+      return hash.equals(Md5Crypt.md5Crypt(password.getBytes(Charsets.UTF_8), hash));
+    } else if (hash.startsWith(""{SHA}"")) {
+      return hash.substring(5).equals(Base64.getEncoder().encodeToString(DigestUtils.sha1(password)));
+    } else if (hash.startsWith(""$2y$"")) {
+      // bcrypt not supported currently
+      return false;
+    } else {
+      return hash.equals(password);
+    }
+  }
+
+  /**
+   * Validate the given username and password against the password file
+   *
+   * @param username Username provided
+   * @param password Password provided
+   * @throws UserAuthenticationException If the username and password could not be validated
+   */
+  @Override
+  public void authenticate(String username, String password) throws UserAuthenticationException {
+    read();
+    String hash = this.userToPassword.get(username);
+    boolean credentialsAccepted = (hash != null && isPasswordValid(password, hash));
+    if (!credentialsAccepted) {
+      throw new UserAuthenticationException(String.format(""htpasswd auth failed for user '%s'"",
+        username));
+    }
+  }
+
+  /**
+   * Read the password file into the map, if the file has changed since we last read it
+   */
+  protected synchronized void read() {
+    File file = new File(path);
+    long newLastModified = file.exists() ? file.lastModified() : 0;
+    if (userToPassword == null || file.lastModified() != lastModified) {
+      HashMap<String, String> newMap = new HashMap<>();
+      Pattern entry = Pattern.compile(""^([^:]+):([^:]+)"");
+      try (BufferedReader reader = new BufferedReader(new FileReader(file))) {
+        String line;
+        while ((line = reader.readLine()) != null) {
+          if (!line.isEmpty() && !line.startsWith(""#"")) {
+            Matcher m = entry.matcher(line);
+            if (m.matches()) {
+              newMap.put(m.group(1), m.group(2));
+            }
+          }
+        }
+      } catch (Exception e) {
+        logger.error(MessageFormat.format(""Failed to read htpasswd file at path {0}"", file), e);
+      }
+      lastModified = file.lastModified();","[{'comment': 'Potential race condition: this could be a new modified time, later than the one checked above. It would be better to save the time in a var above, then set it here if things succeed.', 'commenter': 'paul-rogers'}, {'comment': 'I had even created a local variable above just for this purpose.  Silly mistake, thanks for catching it!\r\n', 'commenter': 'dobesv'}]"
1977,exec/java-exec/src/test/java/org/apache/drill/exec/rpc/user/security/TestHtpasswdFileUserAuthenticator.java,"@@ -0,0 +1,157 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc.user.security;
+
+import org.apache.drill.common.config.DrillProperties;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClientFixture;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.Test;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.Arrays;
+import java.util.List;
+
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class TestHtpasswdFileUserAuthenticator extends ClusterTest {
+  private File tempPasswdFile;
+
+
+  private void setupCluster(String passwdContent) throws IOException {
+    tempPasswdFile = new File(dirTestWatcher.getTmpDir(), ""htpasswd."" + System.currentTimeMillis());
+    Files.write(tempPasswdFile.toPath(), passwdContent.getBytes());
+
+    cluster = ClusterFixture.bareBuilder(dirTestWatcher)
+      .clusterSize(3)
+      .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true)
+      .configProperty(ExecConstants.USER_AUTHENTICATION_ENABLED, true)
+      .configProperty(ExecConstants.USER_AUTHENTICATOR_IMPL, ""htpasswd"")
+      .configProperty(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH, tempPasswdFile.toString())
+      .build();
+  }
+
+
+  @Test
+  public void passwordChecksGiveCorrectResults() throws Exception {
+    String passwdContent = ""alice:pass1\n"" +
+      ""bob:buzzkill\n"" +
+      ""jane:$apr1$PrwDfXy9$ajkhotQW6RFnoVQtPKoW4/\n"" +
+      ""john:$apr1$UxZgBU8k$K4UzdubNa741TnWAZY2QV0\n"";
+    setupCluster(passwdContent);
+
+
+    assertTrue(true);
+
+    tryCredentials(""alice"", ""pass1"", cluster, true);
+    tryCredentials(""bob"", ""buzzkill"", cluster, true);
+    tryCredentials(""notalice"", ""pass1"", cluster, false);
+    tryCredentials(""notbob"", ""buzzkill"", cluster, false);
+    tryCredentials(""alice"", ""wrong"", cluster, false);
+    tryCredentials(""bob"", ""incorrect"", cluster, false);
+    tryCredentials(""jane"", ""pass"", cluster, true);
+    tryCredentials(""john"", ""foobar"", cluster, true);
+    tryCredentials(""jane"", ""wrong"", cluster, false);
+    tryCredentials(""john"", ""incorrect1"", cluster, false);
+  }
+
+  @Test
+  public void rejectsLoginsWhenHtpasswdFileMissing() throws Exception {
+    cluster = ClusterFixture.bareBuilder(dirTestWatcher)
+      .clusterSize(3)
+      .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true)
+      .configProperty(ExecConstants.USER_AUTHENTICATION_ENABLED, true)
+      .configProperty(ExecConstants.USER_AUTHENTICATOR_IMPL, ""htpasswd"")
+      .configProperty(ExecConstants.HTPASSWD_AUTHENTICATOR_PATH, ""/nonexistant-file"")
+      .build();
+    tryCredentials(""bob"", ""bob"", cluster, false);
+  }
+
+  @Test
+  public void detectsChanges() throws Exception {
+    String passwdContent = ""alice:pass1\nbob:buzzkill\n"";
+    setupCluster(passwdContent);
+
+    tryCredentials(""alice"", ""pass1"", cluster, true);
+    tryCredentials(""alice"", ""pass2"", cluster, false);
+    tryCredentials(""bob"", ""buzzkill"", cluster, true);
+    tryCredentials(""bob"", ""yolo"", cluster, false);
+
+    // Wait a second between changes on older VMs that lack subsecond precision file modification time
+    if(System.getProperty(""java.version"").startsWith(""1."")) Thread.sleep(1000); ","[{'comment': 'I disagree with this change. Please avoid adding `Thread.sleep` in unit tests. If it is required, the file modification time may be changed from the code.', 'commenter': 'vvysotskyi'}, {'comment': ""I've changed it to detect file size changes in addition to mod time, so that should catch the change here even though mtime doesn't update."", 'commenter': 'dobesv'}]"
1980,_docs/developer-information/rest-api/010-rest-api-introduction.md,"@@ -481,4 +481,71 @@ Enclose option values of kind STRING in double quotation marks.
 
  -->
 
+## Authenticating REST API requests
+
+If drill has authentication enabled, you will have to supply credentials when you use the REST API.
+
+### Basic authentication
+
+Apache Drill versions 1.18 and higher support HTTP's ""Basic"" authentication system, sending the username & password in the `Authorization` header, encoded to base64 and joined using `:`.
+
+Basic authentication support is controlled using `drill-override.conf`.  Add the string `""BASIC""` to `http.auth.mechanisms`.  Note that if the field is not currently set, it defaults to having `""FORM""` in it, so you probably want to include `""FORM""` if you set this field, so that Web UI users can still use the login form.
+
+Example:
+
+```
+http: {
+    enabled: true,
+    auth: {
+        # Http Auth mechanisms to configure. If not provided but user.auth is enabled
+        # then default value is [""FORM""].
+        mechanisms: [""BASIC"", ""FORM""]
+    }
+}
+```
+
+To authenticate requests using Basic authentication, send the appropriate `Authorization` header with each request using your HTTP client's options:
+
+    curl -kv \
+           -u drilluser:drillpassword  \
+           -X POST \
+           -H ""Content-Type: application/json"" \
+           -d '{""queryType"":""SQL"", ""query"": ""select * from sys.version""}' \
+           http://localhost:8047/query.json
+
+### Form based authentication
+
+Form based authentication is enabled or disabled using `drill-override.conf`.  Add the string `""FORM""` to `http.auth.mechanisms` if it is set.  If `http.auth.mechanisms` is not set, `""FORM""` is enabled by default.
+
+Example:
+
+```
+http: {
+    enabled: true,
+    auth: {
+        # Http Auth mechanisms to configure. If not provided but user.auth is enabled
+        # then default value is [""FORM""].
+        mechanisms: [""BASIC"", ""FORM""]
+    }
+}
+```
+
+To authenticate requests using form-based authentication, you must use an HTTP client that saves cookies between requests.  Simulate a form submission to the same URL used in the Web UI / Console (`/j_security_check`)
+","[{'comment': 'Three-ticks, here and below, since this is a code example.', 'commenter': 'paul-rogers'}, {'comment': 'Is that a new requirement ?  Most of the examples above use indentation, like this one.  It is formatted the same as three ticks.', 'commenter': 'dobesv'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java,"@@ -297,62 +294,64 @@ public IterOutcome innerNext() {
       }
     }
 
-    if (wasKilled) { // if kill() was called before, then finish up
+    if (wasKilled) { // if cancel() was called before, then finish up
       aggregator.cleanup();
-      incoming.kill(false);
       return IterOutcome.NONE;
     }
 
     // Read and aggregate records
-    // ( may need to run again if the spilled partition that was read
-    //   generated new partitions that were all spilled )
+    // (may need to run again if the spilled partition that was read
+    //  generated new partitions that were all spilled)
     AggOutcome out;
     do {
-      //
       //  Read incoming batches and process their records
-      //
       out = aggregator.doWork();
     } while (out == AggOutcome.CALL_WORK_AGAIN);
 
     switch (out) {
-    case CLEANUP_AND_RETURN:
-      container.zeroVectors();
-      aggregator.cleanup();
-      state = BatchState.DONE;
-      // fall through
-    case RETURN_OUTCOME:
-      // rebuilds the schema in the case of complex writer expressions,
-      // since vectors would be added to batch run-time
-      IterOutcome outcome = aggregator.getOutcome();
-      switch (outcome) {
-        case OK:
-        case OK_NEW_SCHEMA:
-          if (firstBatch) {
-            if (CollectionUtils.isNotEmpty(complexWriters)) {
-              container.buildSchema(SelectionVectorMode.NONE);
-              outcome = IterOutcome.OK_NEW_SCHEMA;
+      case CLEANUP_AND_RETURN:
+        container.zeroVectors();
+        aggregator.cleanup();
+        state = BatchState.DONE;
+        // fall through
+      case RETURN_OUTCOME:
+        // rebuilds the schema in the case of complex writer expressions,
+        // since vectors would be added to batch run-time
+        IterOutcome outcome = aggregator.getOutcome();
+        switch (outcome) {
+          case OK:
+          case OK_NEW_SCHEMA:
+            if (firstBatch) {
+              if (CollectionUtils.isNotEmpty(complexWriters)) {
+                container.buildSchema(SelectionVectorMode.NONE);
+                // You'd be forgiven for thinking we should always return
+                // OK_NEW_SCHEMA for the first batch. It turns out, when
+                // two hash aggs are stacked, we get an error if the
+                // upstream one returns OK_NEW_SCHEMA first. Not sure the
+                // details, only know several tests fail.
+                outcome = IterOutcome.OK_NEW_SCHEMA;
+              }
+              firstBatch = false;
             }
-            firstBatch = false;
-          }
-          // fall thru
-        default:
-          return outcome;
-      }
-
-    case UPDATE_AGGREGATOR:
-      throw UserException.unsupportedError()
-          .message(SchemaChangeException.schemaChanged(
-              ""Hash aggregate does not support schema change"",
-              incomingSchema,
-              incoming.getSchema()).getMessage())
-          .build(logger);
-    default:
-      throw new IllegalStateException(String.format(""Unknown state %s."", out));
+            break;
+          default:
+        }
+        return outcome;
+
+      case UPDATE_AGGREGATOR:
+        throw UserException.unsupportedError()
+            .message(SchemaChangeException.schemaChanged(
+                ""Hash aggregate does not support schema change"",
+                incomingSchema,
+                incoming.getSchema()).getMessage())
+            .build(logger);
+      default:
+        throw new IllegalStateException(String.format(""Unknown state %s."", out));
     }
   }
 
   /**
-   * Creates a new Aggregator based on the current schema. If setup fails, this
+   * Creates a new aggregator based on the current schema. If setup fails, this","[{'comment': '```suggestion\r\n   * Creates a new {@link #aggregator} based on the current schema. If setup fails, this\r\n```', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/EvaluationPredicate.java,"@@ -17,14 +17,6 @@
  */
 package org.apache.drill.exec.physical.impl.filter;
 
-import org.apache.drill.exec.record.selection.SelectionVector2;
-
 public class EvaluationPredicate {
-  private SelectionVector2 vector;
-
-  EvaluationPredicate(String pred){
-
-  }
-
-
+  EvaluationPredicate(String pred) { }","[{'comment': ""It seems like the class is not used anywhere. Let's remove it :)"", 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -190,35 +195,44 @@
 
   private final JoinControl joinControl;
 
-  // An iterator over the build side hash table (only applicable for row-key joins)
+  // An iterator over the build side hash table (only applicable for row-key
+  // joins)
   private boolean buildComplete;
 
   // indicates if we have previously returned an output batch
   private boolean firstOutputBatch = true;
 
   private int rightHVColPosition;
   private final BufferAllocator allocator;
-  // Local fields for left/right incoming - may be replaced when reading from spilled
+  // Local fields for left/right incoming - may be replaced when reading from
+  // spilled
   private RecordBatch buildBatch;
   private RecordBatch probeBatch;
 
   /**
-   * Flag indicating whether or not the first data holding build batch needs to be fetched.
+   * Flag indicating whether or not the first data holding build batch needs to
+   * be fetched.
    */
   private final MutableBoolean prefetchedBuild = new MutableBoolean(false);
   /**
-   * Flag indicating whether or not the first data holding probe batch needs to be fetched.
+   * Flag indicating whether or not the first data holding probe batch needs to
+   * be fetched.
    */
   private final MutableBoolean prefetchedProbe = new MutableBoolean(false);
 
   // For handling spilling
   private final SpillSet spillSet;
   HashJoinPOP popConfig;
 
-  private final int originalPartition = -1; // the partition a secondary reads from
-  IntVector read_right_HV_vector; // HV vector that was read from the spilled batch
+  private final int originalPartition = -1; // the partition a secondary reads
+                                            // from
+  IntVector read_right_HV_vector; // HV vector that was read from the spilled
+                                  // batch
   private final int maxBatchesInMemory;
-  private final List<String> probeFields = new ArrayList<>(); // keep the same sequence with the bloomFilters
+  private final List<String> probeFields = new ArrayList<>(); // keep the same
+                                                              // sequence with
+                                                              // the
+                                                              // bloomFilters","[{'comment': 'Please convert such one-line comments with introduced line breaks to regular Javadoc comments. ', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -308,35 +324,28 @@ public boolean hasPartitionLimit() {
    */
   private final SpilledState<HashJoinSpilledPartition> spilledState = new SpilledState<>();
   private final HashJoinUpdater spilledStateUpdater = new HashJoinUpdater();
-  private HashJoinSpilledPartition spilledInners[]; // for the outer to find the partition
+  private HashJoinSpilledPartition spilledInners[]; // for the outer to find the
+                                                    // partition
 
   public enum Metric implements MetricDef {
-    NUM_BUCKETS,
-    NUM_ENTRIES,
-    NUM_RESIZING,
-    RESIZING_TIME_MS,
-    NUM_PARTITIONS,
-    SPILLED_PARTITIONS, // number of original partitions spilled to disk
-    SPILL_MB,         // Number of MB of data spilled to disk. This amount is first written,
-                      // then later re-read. So, disk I/O is twice this amount.
-    SPILL_CYCLE,       // 0 - no spill, 1 - spill, 2 - SECONDARY, 3 - TERTIARY
-    LEFT_INPUT_BATCH_COUNT,
-    LEFT_AVG_INPUT_BATCH_BYTES,
-    LEFT_AVG_INPUT_ROW_BYTES,
-    LEFT_INPUT_RECORD_COUNT,
-    RIGHT_INPUT_BATCH_COUNT,
-    RIGHT_AVG_INPUT_BATCH_BYTES,
-    RIGHT_AVG_INPUT_ROW_BYTES,
-    RIGHT_INPUT_RECORD_COUNT,
-    OUTPUT_BATCH_COUNT,
-    AVG_OUTPUT_BATCH_BYTES,
-    AVG_OUTPUT_ROW_BYTES,
-    OUTPUT_RECORD_COUNT;
+    NUM_BUCKETS, NUM_ENTRIES, NUM_RESIZING, RESIZING_TIME_MS, NUM_PARTITIONS,
+    // number of original partitions spilled to disk
+    SPILLED_PARTITIONS,
+    SPILL_MB, // Number of MB of data spilled to disk. This amount is first
+              // written,
+              // then later re-read. So, disk I/O is twice this amount.
+    SPILL_CYCLE, // 0 - no spill, 1 - spill, 2 - SECONDARY, 3 - TERTIARY
+    LEFT_INPUT_BATCH_COUNT, LEFT_AVG_INPUT_BATCH_BYTES, LEFT_AVG_INPUT_ROW_BYTES,
+    LEFT_INPUT_RECORD_COUNT, RIGHT_INPUT_BATCH_COUNT, RIGHT_AVG_INPUT_BATCH_BYTES,
+    RIGHT_AVG_INPUT_ROW_BYTES, RIGHT_INPUT_RECORD_COUNT, OUTPUT_BATCH_COUNT,
+    AVG_OUTPUT_BATCH_BYTES, AVG_OUTPUT_ROW_BYTES, OUTPUT_RECORD_COUNT;","[{'comment': 'Please revert the change. Although this version is more compact, it is less readable now. ', 'commenter': 'ihuzenko'}, {'comment': 'Argh... An auto-format of a comment got loose and did the whole file. Fixed.', 'commenter': 'paul-rogers'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -386,52 +398,52 @@ protected void buildSchema() {
    * Prefetches the first build side data holding batch.
    */
   private void prefetchFirstBuildBatch() {
-    rightUpstream = prefetchFirstBatch(rightUpstream,
-      prefetchedBuild,
-      buildSideIsEmpty,
-      RIGHT_INDEX,
-      buildBatch,
-      () -> {
-        batchMemoryManager.update(RIGHT_INDEX, 0, true);
-        RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT_RIGHT,
-            batchMemoryManager.getRecordBatchSizer(RIGHT_INDEX),
-            getRecordBatchStatsContext());
-      });
+    rightUpstream = prefetchFirstBatch(rightUpstream, prefetchedBuild,
+        buildSideIsEmpty, RIGHT_INDEX, buildBatch, () -> {
+          batchMemoryManager.update(RIGHT_INDEX, 0, true);
+          RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT_RIGHT,
+              batchMemoryManager.getRecordBatchSizer(RIGHT_INDEX),
+              getRecordBatchStatsContext());
+        });","[{'comment': 'It would be nice to extract the creation of the last lambda to a new method and have here and in method _prefetchFirstProbeBatch()_ something like: \r\n```java\r\n    leftUpstream = prefetchFirstBatch(leftUpstream, prefetchedProbe,\r\n        probeSideIsEmpty, LEFT_INDEX, probeBatch, probeSideMemoryUpdater());\r\n```', 'commenter': 'ihuzenko'}, {'comment': 'Good suggestion. For the most part, I tried not to tinker with the code here unless necessary. A few of the adjustments to remove STOP resulted in subtle errors that took hours to find. In general, this class (and several others) are trying to do too much: there are too many variables to think about. Better to split up responsibilities as I did years ago in the external sort. (For fun, go back and look at the original version.)', 'commenter': 'paul-rogers'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -386,52 +398,52 @@ protected void buildSchema() {
    * Prefetches the first build side data holding batch.
    */
   private void prefetchFirstBuildBatch() {
-    rightUpstream = prefetchFirstBatch(rightUpstream,
-      prefetchedBuild,
-      buildSideIsEmpty,
-      RIGHT_INDEX,
-      buildBatch,
-      () -> {
-        batchMemoryManager.update(RIGHT_INDEX, 0, true);
-        RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT_RIGHT,
-            batchMemoryManager.getRecordBatchSizer(RIGHT_INDEX),
-            getRecordBatchStatsContext());
-      });
+    rightUpstream = prefetchFirstBatch(rightUpstream, prefetchedBuild,
+        buildSideIsEmpty, RIGHT_INDEX, buildBatch, () -> {
+          batchMemoryManager.update(RIGHT_INDEX, 0, true);
+          RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT_RIGHT,
+              batchMemoryManager.getRecordBatchSizer(RIGHT_INDEX),
+              getRecordBatchStatsContext());
+        });
   }
 
   /**
    * Prefetches the first build side data holding batch.
    */
   private void prefetchFirstProbeBatch() {
-    leftUpstream =  prefetchFirstBatch(leftUpstream,
-      prefetchedProbe,
-      probeSideIsEmpty,
-      LEFT_INDEX,
-      probeBatch,
-      () -> {
-        batchMemoryManager.update(LEFT_INDEX, 0);
-        RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT_LEFT,
-            batchMemoryManager.getRecordBatchSizer(LEFT_INDEX),
-            getRecordBatchStatsContext());
-      });
+    leftUpstream = prefetchFirstBatch(leftUpstream, prefetchedProbe,
+        probeSideIsEmpty, LEFT_INDEX, probeBatch, () -> {
+          batchMemoryManager.update(LEFT_INDEX, 0);
+          RecordBatchStats.logRecordBatchStats(RecordBatchIOType.INPUT_LEFT,
+              batchMemoryManager.getRecordBatchSizer(LEFT_INDEX),
+              getRecordBatchStatsContext());
+        });
   }
 
   /**
-   * Used to fetch the first data holding batch from either the build or probe side.
-   * @param outcome The current upstream outcome for either the build or probe side.
-   * @param prefetched A flag indicating if we have already done a prefetch of the first data holding batch for the probe or build side.
-   * @param isEmpty A flag indicating if the probe or build side is empty.
-   * @param index The upstream index of the probe or build batch.
-   * @param batch The probe or build batch itself.
-   * @param memoryManagerUpdate A lambda function to execute the memory manager update for the probe or build batch.
-   * @return The current {@link org.apache.drill.exec.record.RecordBatch.IterOutcome}.
+   * Used to fetch the first data holding batch from either the build or probe
+   * side.
+   *
+   * @param outcome
+   *          The current upstream outcome for either the build or probe side.
+   * @param prefetched
+   *          A flag indicating if we have already done a prefetch of the first
+   *          data holding batch for the probe or build side.
+   * @param isEmpty
+   *          A flag indicating if the probe or build side is empty.
+   * @param index
+   *          The upstream index of the probe or build batch.
+   * @param batch
+   *          The probe or build batch itself.
+   * @param memoryManagerUpdate
+   *          A lambda function to execute the memory manager update for the
+   *          probe or build batch.
+   * @return The current
+   *         {@link org.apache.drill.exec.record.RecordBatch.IterOutcome}.
    */
   private IterOutcome prefetchFirstBatch(IterOutcome outcome,
-                                         MutableBoolean prefetched,
-                                         MutableBoolean isEmpty,
-                                         int index,
-                                         RecordBatch batch,
-                                         Runnable memoryManagerUpdate) {
+      MutableBoolean prefetched, MutableBoolean isEmpty, int index,","[{'comment': 'Such shallow indentations may confuse readers since at first glance look like method body started here. I would suggest \r\n\r\n```java\r\n  private IterOutcome prefetchFirstBatch(IterOutcome outcome, MutableBoolean prefetched,\r\n                                         MutableBoolean isEmpty, int index, RecordBatch batch,\r\n                                         Runnable memoryManagerUpdate) {\r\n```', 'commenter': 'ihuzenko'}, {'comment': 'Reverted, Another over-aggressive bit of help from the code formatter.', 'commenter': 'paul-rogers'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -445,32 +457,34 @@ private IterOutcome prefetchFirstBatch(IterOutcome outcome,
       outcome = sniffNonEmptyBatch(outcome, index, batch);
     }
 
-    isEmpty.setValue(outcome == IterOutcome.NONE); // If we received NONE there is no data.
+    isEmpty.setValue(outcome == IterOutcome.NONE); // If we received NONE there
+                                                   // is no data.
 
-    if (outcome == IterOutcome.STOP) {
-      // We reached a termination state
-      state = BatchState.STOP;
-    } else {
-      // Got our first batch(es)
-      if (spilledState.isFirstCycle()) {
-        // Only collect stats for the first cycle
-        memoryManagerUpdate.run();
-      }
-      state = BatchState.FIRST;
+    // Got our first batch(es)
+    if (spilledState.isFirstCycle()) {
+      // Only collect stats for the first cycle
+      memoryManagerUpdate.run();
     }
-
+    state = BatchState.FIRST;
     return outcome;
   }
 
   /**
-   * Currently in order to accurately predict memory usage for spilling, the first non-empty build or probe side batch is needed. This method
-   * fetches the first non-empty batch from the probe or build side.
-   * @param curr The current outcome.
-   * @param inputIndex Index specifying whether to work with the prorbe or build input.
-   * @param recordBatch The probe or build record batch.
-   * @return The {@link org.apache.drill.exec.record.RecordBatch.IterOutcome} for the left or right record batch.
+   * Currently in order to accurately predict memory usage for spilling, the
+   * first non-empty build or probe side batch is needed. This method fetches
+   * the first non-empty batch from the probe or build side.
+   *
+   * @param curr
+   *          The current outcome.
+   * @param inputIndex
+   *          Index specifying whether to work with the prorbe or build input.
+   * @param recordBatch
+   *          The probe or build record batch.
+   * @return The {@link org.apache.drill.exec.record.RecordBatch.IterOutcome}
+   *         for the left or right record batch.
    */
-  private IterOutcome sniffNonEmptyBatch(IterOutcome curr, int inputIndex, RecordBatch recordBatch) {
+  private IterOutcome sniffNonEmptyBatch(IterOutcome curr, int inputIndex,
+      RecordBatch recordBatch) {","[{'comment': 'same as above', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -445,32 +457,34 @@ private IterOutcome prefetchFirstBatch(IterOutcome outcome,
       outcome = sniffNonEmptyBatch(outcome, index, batch);
     }
 
-    isEmpty.setValue(outcome == IterOutcome.NONE); // If we received NONE there is no data.
+    isEmpty.setValue(outcome == IterOutcome.NONE); // If we received NONE there
+                                                   // is no data.
 
-    if (outcome == IterOutcome.STOP) {
-      // We reached a termination state
-      state = BatchState.STOP;
-    } else {
-      // Got our first batch(es)
-      if (spilledState.isFirstCycle()) {
-        // Only collect stats for the first cycle
-        memoryManagerUpdate.run();
-      }
-      state = BatchState.FIRST;
+    // Got our first batch(es)
+    if (spilledState.isFirstCycle()) {
+      // Only collect stats for the first cycle
+      memoryManagerUpdate.run();
     }
-
+    state = BatchState.FIRST;
     return outcome;
   }
 
   /**
-   * Currently in order to accurately predict memory usage for spilling, the first non-empty build or probe side batch is needed. This method
-   * fetches the first non-empty batch from the probe or build side.
-   * @param curr The current outcome.
-   * @param inputIndex Index specifying whether to work with the prorbe or build input.
-   * @param recordBatch The probe or build record batch.
-   * @return The {@link org.apache.drill.exec.record.RecordBatch.IterOutcome} for the left or right record batch.
+   * Currently in order to accurately predict memory usage for spilling, the
+   * first non-empty build or probe side batch is needed. This method fetches
+   * the first non-empty batch from the probe or build side.
+   *
+   * @param curr","[{'comment': 'Is this curry? :) Please rename the parameter. ', 'commenter': 'ihuzenko'}, {'comment': 'This code is full of such abbreviations. Out of scope to fix in this go-round.', 'commenter': 'paul-rogers'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -479,34 +493,41 @@ private IterOutcome sniffNonEmptyBatch(IterOutcome curr, int inputIndex, RecordB
       curr = next(inputIndex, recordBatch);
 
       switch (curr) {
-        case OK:
-          // We got a data batch
-          break;
-        case NOT_YET:
-          // We need to try again
-          break;
-        case EMIT:
-          throw new UnsupportedOperationException(""We do not support "" + EMIT);
-        default:
-          // Other cases are termination conditions
-          return curr;
+      case OK:
+        // We got a data batch
+        break;
+      case NOT_YET:
+        // We need to try again
+        break;
+      case EMIT:
+        throw new UnsupportedOperationException(""We do not support "" + EMIT);
+      default:
+        // Other cases are termination conditions
+        return curr;
       }
     }
   }
 
   /**
-   * Determines the memory calculator to use. If maxNumBatches is configured simple batch counting is used to spill. Otherwise
-   * memory calculations are used to determine when to spill.
+   * Determines the memory calculator to use. If maxNumBatches is configured
+   * simple batch counting is used to spill. Otherwise memory calculations are
+   * used to determine when to spill.
+   *
    * @return The memory calculator to use.
    */
   public HashJoinMemoryCalculator getCalculatorImpl() {
     if (maxBatchesInMemory == 0) {
-      double safetyFactor = context.getOptions().getDouble(ExecConstants.HASHJOIN_SAFETY_FACTOR_KEY);
-      double fragmentationFactor = context.getOptions().getDouble(ExecConstants.HASHJOIN_FRAGMENTATION_FACTOR_KEY);
-      double hashTableDoublingFactor = context.getOptions().getDouble(ExecConstants.HASHJOIN_HASH_DOUBLE_FACTOR_KEY);
-      String hashTableCalculatorType = context.getOptions().getString(ExecConstants.HASHJOIN_HASHTABLE_CALC_TYPE_KEY);
-
-      return new HashJoinMemoryCalculatorImpl(safetyFactor, fragmentationFactor, hashTableDoublingFactor, hashTableCalculatorType, semiJoin);
+      double safetyFactor = context.getOptions()
+          .getDouble(ExecConstants.HASHJOIN_SAFETY_FACTOR_KEY);
+      double fragmentationFactor = context.getOptions()
+          .getDouble(ExecConstants.HASHJOIN_FRAGMENTATION_FACTOR_KEY);
+      double hashTableDoublingFactor = context.getOptions()
+          .getDouble(ExecConstants.HASHJOIN_HASH_DOUBLE_FACTOR_KEY);
+      String hashTableCalculatorType = context.getOptions()
+          .getString(ExecConstants.HASHJOIN_HASHTABLE_CALC_TYPE_KEY);
+
+      return new HashJoinMemoryCalculatorImpl(safetyFactor, fragmentationFactor,
+          hashTableDoublingFactor, hashTableCalculatorType, semiJoin);","[{'comment': 'In order to shorten the method could be rewritten to : \r\n\r\n```java\r\n  public HashJoinMemoryCalculator getCalculatorImpl() {\r\n    if (maxBatchesInMemory != 0) {\r\n      return new HashJoinMechanicalMemoryCalculator(maxBatchesInMemory);\r\n    }\r\n    OptionManager opts = context.getOptions();\r\n    double safety = opts.getDouble(ExecConstants.HASHJOIN_SAFETY_FACTOR_KEY);\r\n    double fragmentation = opts.getDouble(ExecConstants.HASHJOIN_FRAGMENTATION_FACTOR_KEY);\r\n    double hashTableDoubling = opts.getDouble(ExecConstants.HASHJOIN_HASH_DOUBLE_FACTOR_KEY);\r\n    String hashTableCalcType = opts.getString(ExecConstants.HASHJOIN_HASHTABLE_CALC_TYPE_KEY);\r\n    return new HashJoinMemoryCalculatorImpl(safety, fragmentation, hashTableDoubling, hashTableCalcType, semiJoin);\r\n  }\r\n``` ', 'commenter': 'ihuzenko'}, {'comment': 'Another good suggestion. After this PR is in, do you want to take a crack at fixing some of this stuff?', 'commenter': 'paul-rogers'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -515,99 +536,95 @@ public HashJoinMemoryCalculator getCalculatorImpl() {
   @Override
   public IterOutcome innerNext() {
     if (wasKilled) {
-      // We have received a kill signal. We need to stop processing.
-      this.cleanup();
-      super.close();
+      // We have received a cancel signal. We need to stop processing.
+      cleanup();
       return IterOutcome.NONE;
     }
 
     prefetchFirstBuildBatch();
 
     if (rightUpstream.isError()) {
-      // A termination condition was reached while prefetching the first build side data holding batch.
+      // A termination condition was reached while prefetching the first build
+      // side data holding batch.","[{'comment': 'merge with line below', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -515,99 +536,95 @@ public HashJoinMemoryCalculator getCalculatorImpl() {
   @Override
   public IterOutcome innerNext() {
     if (wasKilled) {
-      // We have received a kill signal. We need to stop processing.
-      this.cleanup();
-      super.close();
+      // We have received a cancel signal. We need to stop processing.
+      cleanup();
       return IterOutcome.NONE;
     }
 
     prefetchFirstBuildBatch();
 
     if (rightUpstream.isError()) {
-      // A termination condition was reached while prefetching the first build side data holding batch.
+      // A termination condition was reached while prefetching the first build
+      // side data holding batch.
       // We need to terminate.
       return rightUpstream;
     }
 
     try {
-      /* If we are here for the first time, execute the build phase of the
-       * hash join and setup the run time generated class for the probe side
+      /*
+       * If we are here for the first time, execute the build phase of the hash
+       * join and setup the run time generated class for the probe side
        */
       if (state == BatchState.FIRST) {
         // Build the hash table, using the build side record batches.
         IterOutcome buildExecuteTermination = executeBuildPhase();
 
         if (buildExecuteTermination != null) {
-          // A termination condition was reached while executing the build phase.
+          // A termination condition was reached while executing the build
+          // phase.","[{'comment': 'merge with line below', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -515,99 +536,95 @@ public HashJoinMemoryCalculator getCalculatorImpl() {
   @Override
   public IterOutcome innerNext() {
     if (wasKilled) {
-      // We have received a kill signal. We need to stop processing.
-      this.cleanup();
-      super.close();
+      // We have received a cancel signal. We need to stop processing.
+      cleanup();
       return IterOutcome.NONE;
     }
 
     prefetchFirstBuildBatch();
 
     if (rightUpstream.isError()) {
-      // A termination condition was reached while prefetching the first build side data holding batch.
+      // A termination condition was reached while prefetching the first build
+      // side data holding batch.
       // We need to terminate.
       return rightUpstream;
     }
 
     try {
-      /* If we are here for the first time, execute the build phase of the
-       * hash join and setup the run time generated class for the probe side
+      /*
+       * If we are here for the first time, execute the build phase of the hash
+       * join and setup the run time generated class for the probe side
        */
       if (state == BatchState.FIRST) {
         // Build the hash table, using the build side record batches.
         IterOutcome buildExecuteTermination = executeBuildPhase();
 
         if (buildExecuteTermination != null) {
-          // A termination condition was reached while executing the build phase.
+          // A termination condition was reached while executing the build
+          // phase.
           // We need to terminate.
           return buildExecuteTermination;
         }
 
         buildComplete = true;
 
         if (isRowKeyJoin) {
-          // discard the first left batch which was fetched by buildSchema, and get the new
+          // discard the first left batch which was fetched by buildSchema, and
+          // get the new","[{'comment': 'merge with line below', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -515,99 +536,95 @@ public HashJoinMemoryCalculator getCalculatorImpl() {
   @Override
   public IterOutcome innerNext() {
     if (wasKilled) {
-      // We have received a kill signal. We need to stop processing.
-      this.cleanup();
-      super.close();
+      // We have received a cancel signal. We need to stop processing.
+      cleanup();
       return IterOutcome.NONE;
     }
 
     prefetchFirstBuildBatch();
 
     if (rightUpstream.isError()) {
-      // A termination condition was reached while prefetching the first build side data holding batch.
+      // A termination condition was reached while prefetching the first build
+      // side data holding batch.
       // We need to terminate.
       return rightUpstream;
     }
 
     try {
-      /* If we are here for the first time, execute the build phase of the
-       * hash join and setup the run time generated class for the probe side
+      /*
+       * If we are here for the first time, execute the build phase of the hash
+       * join and setup the run time generated class for the probe side
        */
       if (state == BatchState.FIRST) {
         // Build the hash table, using the build side record batches.
         IterOutcome buildExecuteTermination = executeBuildPhase();
 
         if (buildExecuteTermination != null) {
-          // A termination condition was reached while executing the build phase.
+          // A termination condition was reached while executing the build
+          // phase.
           // We need to terminate.
           return buildExecuteTermination;
         }
 
         buildComplete = true;
 
         if (isRowKeyJoin) {
-          // discard the first left batch which was fetched by buildSchema, and get the new
+          // discard the first left batch which was fetched by buildSchema, and
+          // get the new
           // one based on rowkey join
           leftUpstream = next(left);
-
-          if (leftUpstream == IterOutcome.STOP || rightUpstream == IterOutcome.STOP) {
-            state = BatchState.STOP;
-            return leftUpstream;
-          }
         }
 
         // Update the hash table related stats for the operator
         updateStats();
       }
 
       // Try to probe and project, or recursively handle a spilled partition
-      if (!buildSideIsEmpty.booleanValue() ||  // If there are build-side rows
-          joinIsLeftOrFull) {  // or if this is a left/full outer join
+      if (!buildSideIsEmpty.booleanValue() || // If there are build-side rows
+          joinIsLeftOrFull) { // or if this is a left/full outer join
 
         prefetchFirstProbeBatch();
 
-        if (leftUpstream.isError() ||
-            ( leftUpstream == NONE && ! joinIsRightOrFull )) {
-          // A termination condition was reached while prefetching the first probe side data holding batch.
+        if (leftUpstream.isError()
+            || (leftUpstream == NONE && !joinIsRightOrFull)) {
+          // A termination condition was reached while prefetching the first
+          // probe side data holding batch.","[{'comment': 'merge with line below', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -679,67 +715,91 @@ public IterOutcome innerNext() {
   }
 
   /**
-   * In case an upstream data is no longer needed, send a kill and flush any remaining batch
+   * In case an upstream data is no longer needed, send a kill and flush any
+   * remaining batch
    *
-   * @param batch probe or build batch
-   * @param upstream which upstream
-   * @param isLeft is it the left or right
+   * @param batch
+   *          probe or build batch
+   * @param upstream
+   *          which upstream
+   * @param isLeft
+   *          is it the left or right
    */
-  private void killAndDrainUpstream(RecordBatch batch, IterOutcome upstream, boolean isLeft) {
-    batch.kill(true);
-    while (upstream == IterOutcome.OK_NEW_SCHEMA || upstream == IterOutcome.OK) {
+  private void killAndDrainUpstream(RecordBatch batch, IterOutcome upstream,
+      boolean isLeft) {","[{'comment': 'it would be better to accept int and pass HashJoinHelper.RIGHT_INPUT or  HashJoinHelper.LEFT_INPUT when invoked. ', 'commenter': 'ihuzenko'}, {'comment': 'Another good suggestion for a refactoring pass.', 'commenter': 'paul-rogers'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -679,67 +715,91 @@ public IterOutcome innerNext() {
   }
 
   /**
-   * In case an upstream data is no longer needed, send a kill and flush any remaining batch
+   * In case an upstream data is no longer needed, send a kill and flush any
+   * remaining batch
    *
-   * @param batch probe or build batch
-   * @param upstream which upstream
-   * @param isLeft is it the left or right
+   * @param batch
+   *          probe or build batch
+   * @param upstream
+   *          which upstream
+   * @param isLeft
+   *          is it the left or right
    */
-  private void killAndDrainUpstream(RecordBatch batch, IterOutcome upstream, boolean isLeft) {
-    batch.kill(true);
-    while (upstream == IterOutcome.OK_NEW_SCHEMA || upstream == IterOutcome.OK) {
+  private void killAndDrainUpstream(RecordBatch batch, IterOutcome upstream,
+      boolean isLeft) {
+    batch.cancel();
+    while (upstream == IterOutcome.OK_NEW_SCHEMA
+        || upstream == IterOutcome.OK) {
       VectorAccessibleUtilities.clear(batch);
-      upstream = next( isLeft ? HashJoinHelper.LEFT_INPUT : HashJoinHelper.RIGHT_INPUT, batch);
+      upstream = next(
+          isLeft ? HashJoinHelper.LEFT_INPUT : HashJoinHelper.RIGHT_INPUT,
+          batch);
     }
   }
 
-  private void killAndDrainLeftUpstream() { killAndDrainUpstream(probeBatch, leftUpstream, true); }
-  private void killAndDrainRightUpstream() { killAndDrainUpstream(buildBatch, rightUpstream, false); }
+  private void killAndDrainLeftUpstream() {
+    killAndDrainUpstream(probeBatch, leftUpstream, true);
+  }
+
+  private void killAndDrainRightUpstream() {
+    killAndDrainUpstream(buildBatch, rightUpstream, false);
+  }
 
   private void setupHashTable() {
-    List<Comparator> comparators = Lists.newArrayListWithExpectedSize(conditions.size());
-    conditions.forEach(cond->comparators.add(JoinUtils.checkAndReturnSupportedJoinComparator(cond)));
+    List<Comparator> comparators = Lists
+        .newArrayListWithExpectedSize(conditions.size());
+    conditions.forEach(cond -> comparators
+        .add(JoinUtils.checkAndReturnSupportedJoinComparator(cond)));","[{'comment': 'makes sense to collect comparators right before HashTableConfig(...) constructor call, using : \r\n```java\r\n    List<Comparator> comparators = conditions.stream()\r\n        .map(JoinUtils::checkAndReturnSupportedJoinComparator)\r\n        .collect(Collectors.toList());\r\n``` ', 'commenter': 'ihuzenko'}, {'comment': 'As above.', 'commenter': 'paul-rogers'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -812,37 +877,43 @@ private void delayedSetup() {
    * Initialize fields (that may be reused when reading spilled partitions)
    */
   private void initializeBuild() {
-    baseHashTable.updateIncoming(buildBatch, probeBatch); // in case we process the spilled files
+    baseHashTable.updateIncoming(buildBatch, probeBatch); // in case we process
+                                                          // the spilled files
     // Recreate the partitions every time build is initialized
-    for (int part = 0; part < numPartitions; part++ ) {
-      partitions[part] = new HashPartition(context, allocator, baseHashTable, buildBatch, probeBatch, semiJoin,
-        RECORDS_PER_BATCH, spillSet, part, spilledState.getCycle(), numPartitions);
+    for (int part = 0; part < numPartitions; part++) {
+      partitions[part] = new HashPartition(context, allocator, baseHashTable,
+          buildBatch, probeBatch, semiJoin, RECORDS_PER_BATCH, spillSet, part,
+          spilledState.getCycle(), numPartitions);
     }
 
     spilledInners = new HashJoinSpilledPartition[numPartitions];
 
   }
 
   /**
-   * Note:
-   * This method can not be called again as part of recursive call of executeBuildPhase() to handle spilled build partitions.
+   * Note: This method can not be called again as part of recursive call of
+   * executeBuildPhase() to handle spilled build partitions.
    */
   private void initializeRuntimeFilter() {
     if (!enableRuntimeFilter || bloomFiltersGenerated) {
       return;
     }
-    runtimeFilterReporter = new RuntimeFilterReporter((ExecutorFragmentContext) context);
+    runtimeFilterReporter = new RuntimeFilterReporter(
+        (ExecutorFragmentContext) context);","[{'comment': 'There are a lot of new line breaks introduced in the class in situations where the line is not actually long. Could you please revert the changes? I would suggest considering line with length up to 100 symbols as normal. Phew... Java is so verbal:)', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -1222,63 +1329,81 @@ public HashJoinBatch(HashJoinPOP popConfig, FragmentContext context,
 
     for (int i = 0; i < conditions.size(); i++) {
       SchemaPath rightPath = (SchemaPath) conditions.get(i).getRight();
-      PathSegment.NameSegment nameSegment = (PathSegment.NameSegment)rightPath.getLastSegment();
+      PathSegment.NameSegment nameSegment = (PathSegment.NameSegment) rightPath
+          .getLastSegment();
       buildJoinColumns.add(nameSegment.getPath());
       String refName = ""build_side_"" + i;
-      rightExpr.add(new NamedExpression(conditions.get(i).getRight(), new FieldReference(refName)));
+      rightExpr.add(new NamedExpression(conditions.get(i).getRight(),
+          new FieldReference(refName)));
     }
 
     this.allocator = oContext.getAllocator();
 
-    numPartitions = (int)context.getOptions().getOption(ExecConstants.HASHJOIN_NUM_PARTITIONS_VALIDATOR);
-    if ( numPartitions == 1 ) { //
-      disableSpilling(""Spilling is disabled due to configuration setting of num_partitions to 1"");
+    numPartitions = (int) context.getOptions()
+        .getOption(ExecConstants.HASHJOIN_NUM_PARTITIONS_VALIDATOR);
+    if (numPartitions == 1) { //","[{'comment': '```suggestion\r\n    if (numPartitions == 1) {\r\n```', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -1344,74 +1483,95 @@ private void updateStats() {
     stats.setLongStat(Metric.NUM_RESIZING, htStats.numResizing);
     stats.setLongStat(Metric.RESIZING_TIME_MS, htStats.resizingTime);
     stats.setLongStat(Metric.NUM_PARTITIONS, numPartitions);
-    stats.setLongStat(Metric.SPILL_CYCLE, spilledState.getCycle()); // Put 0 in case no spill
+    stats.setLongStat(Metric.SPILL_CYCLE, spilledState.getCycle()); // Put 0 in
+                                                                    // case no
+                                                                    // spill
     stats.setLongStat(Metric.SPILLED_PARTITIONS, numSpilled);
   }
 
   /**
-   * Get the hash table iterator that is created for the build side of the hash join if
-   * this hash join was instantiated as a row-key join.
-   * @return hash table iterator or null if this hash join was not a row-key join or if it
-   * was a row-key join but the build has not yet completed.
+   * Get the hash table iterator that is created for the build side of the hash
+   * join if this hash join was instantiated as a row-key join.
+   *
+   * @return hash table iterator or null if this hash join was not a row-key
+   *         join or if it was a row-key join but the build has not yet
+   *         completed.
    */
   @Override
   public Pair<ValueVector, Integer> nextRowKeyBatch() {
     if (buildComplete) {
-      // partition 0 because Row Key Join has only a single partition - no spilling
+      // partition 0 because Row Key Join has only a single partition - no
+      // spilling
       Pair<VectorContainer, Integer> pp = partitions[0].nextBatch();
       if (pp != null) {
         VectorWrapper<?> vw = Iterables.get(pp.getLeft(), 0);
         ValueVector vv = vw.getValueVector();
         return Pair.of(vv, pp.getRight());
       }
-    } else if(partitions == null && firstOutputBatch) { //if there is data coming to right(build) side in build Schema stage, use it.
+    } else if (partitions == null && firstOutputBatch) { // if there is data
+                                                         // coming to
+                                                         // right(build) side in
+                                                         // build Schema stage,
+                                                         // use it.","[{'comment': 'maybe simply put line below ```else if``` ?', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java,"@@ -1344,74 +1483,95 @@ private void updateStats() {
     stats.setLongStat(Metric.NUM_RESIZING, htStats.numResizing);
     stats.setLongStat(Metric.RESIZING_TIME_MS, htStats.resizingTime);
     stats.setLongStat(Metric.NUM_PARTITIONS, numPartitions);
-    stats.setLongStat(Metric.SPILL_CYCLE, spilledState.getCycle()); // Put 0 in case no spill
+    stats.setLongStat(Metric.SPILL_CYCLE, spilledState.getCycle()); // Put 0 in
+                                                                    // case no
+                                                                    // spill
     stats.setLongStat(Metric.SPILLED_PARTITIONS, numSpilled);
   }
 
   /**
-   * Get the hash table iterator that is created for the build side of the hash join if
-   * this hash join was instantiated as a row-key join.
-   * @return hash table iterator or null if this hash join was not a row-key join or if it
-   * was a row-key join but the build has not yet completed.
+   * Get the hash table iterator that is created for the build side of the hash
+   * join if this hash join was instantiated as a row-key join.
+   *
+   * @return hash table iterator or null if this hash join was not a row-key
+   *         join or if it was a row-key join but the build has not yet
+   *         completed.
    */
   @Override
   public Pair<ValueVector, Integer> nextRowKeyBatch() {
     if (buildComplete) {
-      // partition 0 because Row Key Join has only a single partition - no spilling
+      // partition 0 because Row Key Join has only a single partition - no
+      // spilling
       Pair<VectorContainer, Integer> pp = partitions[0].nextBatch();
       if (pp != null) {
         VectorWrapper<?> vw = Iterables.get(pp.getLeft(), 0);
         ValueVector vv = vw.getValueVector();
         return Pair.of(vv, pp.getRight());
       }
-    } else if(partitions == null && firstOutputBatch) { //if there is data coming to right(build) side in build Schema stage, use it.
+    } else if (partitions == null && firstOutputBatch) { // if there is data
+                                                         // coming to
+                                                         // right(build) side in
+                                                         // build Schema stage,
+                                                         // use it.
       firstOutputBatch = false;
-      if ( right.getRecordCount() > 0 ) {
+      if (right.getRecordCount() > 0) {
         VectorWrapper<?> vw = Iterables.get(right, 0);
         ValueVector vv = vw.getValueVector();
-        return Pair.of(vv, right.getRecordCount()-1);
+        return Pair.of(vv, right.getRecordCount() - 1);
       }
     }
     return null;
   }
 
-  @Override    // implement RowKeyJoin interface
+  @Override // implement RowKeyJoin interface
   public boolean hasRowKeyBatch() {
     return buildComplete;
   }
 
-  @Override   // implement RowKeyJoin interface
+  @Override // implement RowKeyJoin interface
   public BatchState getBatchState() {
     return state;
   }
 
-  @Override  // implement RowKeyJoin interface
+  @Override // implement RowKeyJoin interface
   public void setBatchState(BatchState newState) {
     state = newState;
   }
 
   @Override
-  public void killIncoming(boolean sendUpstream) {
+  protected void cancelIncoming() {
     wasKilled = true;
-    probeBatch.kill(sendUpstream);
-    buildBatch.kill(sendUpstream);
+    probeBatch.cancel();
+    buildBatch.cancel();
   }
 
   public void updateMetrics() {
-    stats.setLongStat(HashJoinBatch.Metric.LEFT_INPUT_BATCH_COUNT, batchMemoryManager.getNumIncomingBatches(LEFT_INDEX));
-    stats.setLongStat(HashJoinBatch.Metric.LEFT_AVG_INPUT_BATCH_BYTES, batchMemoryManager.getAvgInputBatchSize(LEFT_INDEX));
-    stats.setLongStat(HashJoinBatch.Metric.LEFT_AVG_INPUT_ROW_BYTES, batchMemoryManager.getAvgInputRowWidth(LEFT_INDEX));
-    stats.setLongStat(HashJoinBatch.Metric.LEFT_INPUT_RECORD_COUNT, batchMemoryManager.getTotalInputRecords(LEFT_INDEX));
-
-    stats.setLongStat(HashJoinBatch.Metric.RIGHT_INPUT_BATCH_COUNT, batchMemoryManager.getNumIncomingBatches(RIGHT_INDEX));
-    stats.setLongStat(HashJoinBatch.Metric.RIGHT_AVG_INPUT_BATCH_BYTES, batchMemoryManager.getAvgInputBatchSize(RIGHT_INDEX));
-    stats.setLongStat(HashJoinBatch.Metric.RIGHT_AVG_INPUT_ROW_BYTES, batchMemoryManager.getAvgInputRowWidth(RIGHT_INDEX));
-    stats.setLongStat(HashJoinBatch.Metric.RIGHT_INPUT_RECORD_COUNT, batchMemoryManager.getTotalInputRecords(RIGHT_INDEX));
-
-    stats.setLongStat(HashJoinBatch.Metric.OUTPUT_BATCH_COUNT, batchMemoryManager.getNumOutgoingBatches());
-    stats.setLongStat(HashJoinBatch.Metric.AVG_OUTPUT_BATCH_BYTES, batchMemoryManager.getAvgOutputBatchSize());
-    stats.setLongStat(HashJoinBatch.Metric.AVG_OUTPUT_ROW_BYTES, batchMemoryManager.getAvgOutputRowWidth());
-    stats.setLongStat(HashJoinBatch.Metric.OUTPUT_RECORD_COUNT, batchMemoryManager.getTotalOutputRecords());
+    stats.setLongStat(HashJoinBatch.Metric.LEFT_INPUT_BATCH_COUNT,
+        batchMemoryManager.getNumIncomingBatches(LEFT_INDEX));
+    stats.setLongStat(HashJoinBatch.Metric.LEFT_AVG_INPUT_BATCH_BYTES,
+        batchMemoryManager.getAvgInputBatchSize(LEFT_INDEX));
+    stats.setLongStat(HashJoinBatch.Metric.LEFT_AVG_INPUT_ROW_BYTES,
+        batchMemoryManager.getAvgInputRowWidth(LEFT_INDEX));
+    stats.setLongStat(HashJoinBatch.Metric.LEFT_INPUT_RECORD_COUNT,
+        batchMemoryManager.getTotalInputRecords(LEFT_INDEX));
+
+    stats.setLongStat(HashJoinBatch.Metric.RIGHT_INPUT_BATCH_COUNT,
+        batchMemoryManager.getNumIncomingBatches(RIGHT_INDEX));
+    stats.setLongStat(HashJoinBatch.Metric.RIGHT_AVG_INPUT_BATCH_BYTES,
+        batchMemoryManager.getAvgInputBatchSize(RIGHT_INDEX));
+    stats.setLongStat(HashJoinBatch.Metric.RIGHT_AVG_INPUT_ROW_BYTES,
+        batchMemoryManager.getAvgInputRowWidth(RIGHT_INDEX));
+    stats.setLongStat(HashJoinBatch.Metric.RIGHT_INPUT_RECORD_COUNT,
+        batchMemoryManager.getTotalInputRecords(RIGHT_INDEX));
+
+    stats.setLongStat(HashJoinBatch.Metric.OUTPUT_BATCH_COUNT,
+        batchMemoryManager.getNumOutgoingBatches());
+    stats.setLongStat(HashJoinBatch.Metric.AVG_OUTPUT_BATCH_BYTES,
+        batchMemoryManager.getAvgOutputBatchSize());
+    stats.setLongStat(HashJoinBatch.Metric.AVG_OUTPUT_ROW_BYTES,
+        batchMemoryManager.getAvgOutputRowWidth());
+    stats.setLongStat(HashJoinBatch.Metric.OUTPUT_RECORD_COUNT,
+        batchMemoryManager.getTotalOutputRecords());","[{'comment': 'With little trick may be : \r\n\r\n```java\r\n  public void updateMetrics() {\r\n    RecordBatchMemoryManager bmm = batchMemoryManager;\r\n    stats.setLongStat(Metric.LEFT_INPUT_BATCH_COUNT, bmm.getNumIncomingBatches(LEFT_INDEX));\r\n    stats.setLongStat(Metric.LEFT_AVG_INPUT_BATCH_BYTES, bmm.getAvgInputBatchSize(LEFT_INDEX));\r\n    stats.setLongStat(Metric.LEFT_AVG_INPUT_ROW_BYTES, bmm.getAvgInputRowWidth(LEFT_INDEX));\r\n    stats.setLongStat(Metric.LEFT_INPUT_RECORD_COUNT, bmm.getTotalInputRecords(LEFT_INDEX));\r\n    \r\n    stats.setLongStat(Metric.RIGHT_INPUT_BATCH_COUNT, bmm.getNumIncomingBatches(RIGHT_INDEX));\r\n    stats.setLongStat(Metric.RIGHT_AVG_INPUT_BATCH_BYTES, bmm.getAvgInputBatchSize(RIGHT_INDEX));\r\n    stats.setLongStat(Metric.RIGHT_AVG_INPUT_ROW_BYTES, bmm.getAvgInputRowWidth(RIGHT_INDEX));\r\n    stats.setLongStat(Metric.RIGHT_INPUT_RECORD_COUNT, bmm.getTotalInputRecords(RIGHT_INDEX));\r\n    \r\n    stats.setLongStat(Metric.OUTPUT_BATCH_COUNT, bmm.getNumOutgoingBatches());\r\n    stats.setLongStat(Metric.AVG_OUTPUT_BATCH_BYTES, bmm.getAvgOutputBatchSize());\r\n    stats.setLongStat(Metric.AVG_OUTPUT_ROW_BYTES, bmm.getAvgOutputRowWidth());\r\n    stats.setLongStat(Metric.OUTPUT_RECORD_COUNT, bmm.getTotalOutputRecords());\r\n  }\r\n```', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinMemoryCalculatorImpl.java,"@@ -361,13 +358,13 @@ private void calculateMemoryUsage()
         partitionProbeBatchSize = probeSizePredictor.predictBatchSize(recordsPerPartitionBatchProbe, reserveHash);
       }
 
-      maxOutputBatchSize = (long) ((double)outputBatchSize * fragmentationFactor * safetyFactor);
+      maxOutputBatchSize = (long) (outputBatchSize * fragmentationFactor * safetyFactor);
 
       long probeReservedMemory = 0;
 
       for (partitions = initialPartitions;; partitions /= 2) {
         // The total amount of memory to reserve for incomplete batches across all partitions
-        long incompletePartitionsBatchSizes = ((long) partitions) * partitionBuildBatchSize;
+        long incompletePartitionsBatchSizes = (partitions) * partitionBuildBatchSize;","[{'comment': '```suggestion\r\n        long incompletePartitionsBatchSizes = partitions * partitionBuildBatchSize;\r\n```', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinMemoryCalculatorImpl.java,"@@ -448,7 +445,7 @@ public boolean shouldSpill() {
 
       if (reserveHash) {
         // Include the hash sizes for the batch
-        consumedMemory += ((long) IntVector.VALUE_WIDTH) * partitionStatsSet.getNumInMemoryRecords();
+        consumedMemory += (IntVector.VALUE_WIDTH) * partitionStatsSet.getNumInMemoryRecords();","[{'comment': '```suggestion\r\n        consumedMemory += IntVector.VALUE_WIDTH * partitionStatsSet.getNumInMemoryRecords();\r\n```', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/test/java/org/apache/drill/test/PhysicalOpUnitTestBase.java,"@@ -145,8 +145,7 @@ public boolean hasNext() {
             lastResultOutcome = operator.next();
             needToGrabNext = false;
           }
-          if (lastResultOutcome == RecordBatch.IterOutcome.NONE
-            || lastResultOutcome == RecordBatch.IterOutcome.STOP) {
+          if (lastResultOutcome == RecordBatch.IterOutcome.NONE) {","[{'comment': '```suggestion\r\n          return lastResultOutcome != RecordBatch.IterOutcome.NONE;\r\n```', 'commenter': 'ihuzenko'}]"
1981,exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestBroadcastExchange.java,"@@ -88,6 +88,7 @@ public void TestMultipleSendLocationBroadcastExchange() throws Exception {
         }
         b.release();
       }
+      // Nothing done with count?","[{'comment': 'maybe check count > 0 ? :)', 'commenter': 'ihuzenko'}]"
1987,exec/java-exec/src/test/java/org/apache/drill/exec/udf/dynamic/TestDynamicUDFSupport.java,"@@ -104,8 +104,10 @@ public static void buildAndStoreDefaultJars() throws IOException {
   @Before
   public void setupNewDrillbit() throws Exception {
     udfDir = dirTestWatcher.makeSubDir(Paths.get(""udf""));
+    File udfLocalDir = dirTestWatcher.makeSubDir(Paths.get(""udf"", ""local""));","[{'comment': ""The `DirTestWatcher` has internal support for each of Drill's working directories. Might we want to add another directory for UDF files?"", 'commenter': 'paul-rogers'}, {'comment': 'Good idea, thanks, added directory for UDF files.', 'commenter': 'vvysotskyi'}, {'comment': 'Nice set of improvements in setting up the UDF dir. Looks very clean.\r\n\r\nShould this code then use, say, `dirTestWatcher.getUdfDir()`? Or,\r\n\r\n```\r\nFile udfLocalDir = new File(dirTestWatcher.getUdfDir(), ""local"");\r\n```\r\n\r\nOr, since the UDF code works from the locations set in the conf file, should this be something like:\r\n\r\n```\r\nFile udfLocalDir = new File(config.getString(ExecConstants.UDF_DIRECTORY_LOCAL));\r\n```', 'commenter': 'paul-rogers'}, {'comment': 'Thanks, replaced it with\r\n```\r\nFile udfLocalDir = new File(dirTestWatcher.getUdfDir(), ""local"");\r\n```', 'commenter': 'vvysotskyi'}]"
1987,exec/java-exec/src/test/java/org/apache/drill/exec/udf/dynamic/TestDynamicUDFSupport.java,"@@ -104,8 +104,10 @@ public static void buildAndStoreDefaultJars() throws IOException {
   @Before
   public void setupNewDrillbit() throws Exception {
     udfDir = dirTestWatcher.makeSubDir(Paths.get(""udf""));
+    File udfLocalDir = dirTestWatcher.makeSubDir(Paths.get(""udf"", ""local""));
     Properties overrideProps = new Properties();
     overrideProps.setProperty(ExecConstants.UDF_DIRECTORY_ROOT, udfDir.getAbsolutePath());
+    overrideProps.setProperty(ExecConstants.UDF_DIRECTORY_LOCAL, udfLocalDir.getAbsolutePath());","[{'comment': 'We\'ve got lots of local directory properties. Hard to keep them all in sync. I wonder if we can use a feature of HOCON to default them to a known structure:\r\n\r\n```\r\nexec: {\r\n   ...\r\n   local: {\r\n     baseDir: ""/tmp/drill"",\r\n     udfDir: ""${drill.exec.local.baseDir}/udf"",\r\n     pluginDir: ""${drill.exec.local.baseDir}/plugins"",\r\n     ...\r\n},\r\n```\r\n\r\nProbably some setup to do in the `ClusterFixture` and `DirTestWatcher` to get everything set up.', 'commenter': 'paul-rogers'}, {'comment': ""Could you please clarify it a little bit?\r\nAre you proposing to create one more test conf file with these configs or updating one of the existing ones?\r\nIn the first case, I'm afraid we can face up with the problem connected with the order of reading conf files: the same config values would depend on the order of reading config files.\r\nIn the second case, it may affect other tests."", 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi, glad you were able to simplify the test. I realize you didn\'t write them, so I appreciate taking the time to clean them up.\r\n\r\nThe random thought on the directories was that we have 4 or 5 (more) local directories that we set up. It seems we configure each separately. For example, from `drill-module.conf`:\r\n\r\n```\r\ndrill.tmp-dir: ""/tmp""\r\ndrill.tmp-dir: ${?DRILL_TMP_DIR}\r\n...\r\n  sys.store.provider: {\r\n    local: {\r\n      path: ""/tmp/drill"",\r\n    }\r\n  trace: {\r\n    directory: ""/tmp/drill-trace"",\r\n    filesystem: ""file:///""\r\n  },\r\n  tmp: {\r\n    directories: [""/tmp/drill""],\r\n    filesystem: ""drill-local:///""\r\n  },\r\n  compile: {\r\n    code_dir: ""/tmp/drill/codegen""\r\n...\r\n  spill: {\r\n    // *** Options common to all the operators that may spill\r\n    // File system to use. Local file system by default.\r\n    fs: ""file:///"",\r\n    // List of directories to use. Directories are created\r\n    // if they do not exist.\r\n    directories: [ ""/tmp/drill/spill"" ]\r\n...\r\n  udf: {\r\n    directory: {\r\n      // Base directory for remote and local udf directories, unique among clusters.\r\n```\r\n\r\nAnd probably more. To move where Drill stores temp files, the user must change all of these properties.\r\n\r\nFortunately, @arina-ielchiieva did a nice job with the UDF directories: they all are computed from the base directory:\r\n\r\n```\r\n   directory: {\r\n      // Base directory for remote and local udf directories, unique among clusters.\r\n      base: ${drill.exec.zk.root}""/udf"",\r\n\r\n      // Path to local udf directory, always created on local file system.\r\n      // Root for these directory is generated at runtime unless Drill temporary directory is set.\r\n      local: ${drill.exec.udf.directory.base}""/udf/local"",\r\n\r\n      // Set this property if custom file system should be used to create remote directories, ex: fs: ""file:///"".\r\n      // fs: """",\r\n      // Set this property if custom absolute root should be used for remote directories, ex: root: ""/app/drill"".\r\n      // root: """",\r\n\r\n      // Relative path to all remote udf directories.\r\n      // Directories are created under default file system taken from Hadoop configuration\r\n      // unless ${drill.exec.udf.directory.fs} is set.\r\n      // User home directory is used as root unless ${drill.exec.udf.directory.root} is set.\r\n      staging: ${drill.exec.udf.directory.base}""/staging"",\r\n      registry: ${drill.exec.udf.directory.base}""/registry"",\r\n      tmp: ${drill.exec.udf.directory.base}""/tmp""\r\n    }\r\n```\r\n\r\nSo, my question was, can we do the same thing for all the other local directories? Allow each to be custom-set, but default them to be computed from a single base directory. That way, if a unit test or install wants to move the Drill local directories to, say, `/var/drill/tmp`, they only need change a single config line and everything else follows automatically.\r\n\r\nThis can be done in the existing conf file as was done for UDFs. And, I guess to preserve compatibility, we\'d have to leave the properties where they are; we\'d just change their values.\r\n\r\nIf you feel this is a bit out of scope for this PR then we can file a Jira for the change.\r\n\r\n``` ', 'commenter': 'paul-rogers'}, {'comment': '@paul-rogers, thanks for the explanation and pointing to some places of `drill-module.conf`.\r\nI have also looked at it and found one more place:\r\n```\r\n  zk: {\r\n    connect: ""localhost:2181"",\r\n    root: ""drill"",\r\n```\r\n`drill.exec.zk.root` is used in several places as a base directory, and I think most of the users use its default value, so changing it may cause problems. Regarding other places, it would be easier to update their values without affecting users, but I think it would be better to do it in the scope of another Jira.', 'commenter': 'vvysotskyi'}, {'comment': 'The `zk.root` path is a path on ZK, not the file system. So, my thought is that the ZK path would not be set as part of the proposed system.\r\n\r\nI do agree that we should tackle any fix as another PR. Filed DRILL-7593 to track the issue.', 'commenter': 'paul-rogers'}, {'comment': 'But in some places, it is used as a path on the file system, so I mentioned it. Thanks for logging the new Jira ticket!', 'commenter': 'vvysotskyi'}, {'comment': 'Thanks for the explanation. Since that field is supposed to be relative to the ZK root, it probably should not also be used as a file system path. I suspect that, somewhere, sometime, somebody got confused.\r\n\r\nIf you know where those usages are, please add a note to the JIRA ticket; we should fix those at the same time to us some other config variable for the file system.', 'commenter': 'paul-rogers'}]"
1987,exec/java-exec/src/test/java/org/apache/drill/exec/work/filter/BloomFilterTest.java,"@@ -133,214 +135,227 @@ public boolean hasFailed() {
     }
   }
 
-
   @Test
   public void testNotExist() throws Exception {
-    Drillbit bit = new Drillbit(c, RemoteServiceSet.getLocalServiceSet(), ClassPathScanner.fromPrescan(c));
-    bit.run();
-    DrillbitContext bitContext = bit.getContext();
-    FunctionImplementationRegistry registry = bitContext.getFunctionImplementationRegistry();
-    FragmentContextImpl context = new FragmentContextImpl(bitContext, BitControl.PlanFragment.getDefaultInstance(), null, registry);
-    BufferAllocator bufferAllocator = bitContext.getAllocator();
-    //create RecordBatch
-    VarCharVector vector = new VarCharVector(SchemaBuilder.columnSchema(""a"", TypeProtos.MinorType.VARCHAR, TypeProtos.DataMode.REQUIRED), bufferAllocator);
-    vector.allocateNew();
-    int valueCount = 3;
-    VarCharVector.Mutator mutator = vector.getMutator();
-    mutator.setSafe(0, ""a"".getBytes());
-    mutator.setSafe(1, ""b"".getBytes());
-    mutator.setSafe(2, ""c"".getBytes());
-    mutator.setValueCount(valueCount);
-    VectorContainer vectorContainer = new VectorContainer();
-    TypedFieldId fieldId = vectorContainer.add(vector);
-    RecordBatch recordBatch = new TestRecordBatch(vectorContainer);
-    //construct hash64
-    ValueVectorReadExpression exp = new ValueVectorReadExpression(fieldId);
-    LogicalExpression[] expressions = new LogicalExpression[1];
-    expressions[0] = exp;
-    TypedFieldId[] fieldIds = new TypedFieldId[1];
-    fieldIds[0] = fieldId;
-    ValueVectorHashHelper valueVectorHashHelper = new ValueVectorHashHelper(recordBatch, context);
-    ValueVectorHashHelper.Hash64 hash64 = valueVectorHashHelper.getHash64(expressions, fieldIds);
-
-    //construct BloomFilter
-    int numBytes = BloomFilter.optimalNumOfBytes(3, 0.03);
-
-    BloomFilter bloomFilter = new BloomFilter(numBytes, bufferAllocator);
-    for (int i = 0; i < valueCount; i++) {
-      long hashCode = hash64.hash64Code(i, 0, 0);
-      bloomFilter.insert(hashCode);
+    int userPort = QueryTestUtil.getFreePortNumber(31170, 300);
+    int bitPort = QueryTestUtil.getFreePortNumber(31180, 300);
+    ClusterFixtureBuilder clusterFixtureBuilder = ClusterFixture.bareBuilder(dirTestWatcher)","[{'comment': 'Do you need a full cluster for this? There is a `SubOperatorTest` that will give you a fragment context and allocator so you can create vectors and invoke ""sub-operator"" functionality such as the BloomFilter stuff.\r\n\r\nIf any of the code under tests needs the `DrillbitContext`, perhaps look at modifying so that it doesn\'t. There is nothing a Bloom filter should need.', 'commenter': 'paul-rogers'}, {'comment': ""That's brilliant! Thanks for pointing to `SubOperatorTest`, it really simplifies code a lot!"", 'commenter': 'vvysotskyi'}]"
1987,exec/java-exec/src/test/java/org/apache/drill/exec/work/filter/BloomFilterTest.java,"@@ -133,214 +135,227 @@ public boolean hasFailed() {
     }
   }
 
-
   @Test
   public void testNotExist() throws Exception {
-    Drillbit bit = new Drillbit(c, RemoteServiceSet.getLocalServiceSet(), ClassPathScanner.fromPrescan(c));
-    bit.run();
-    DrillbitContext bitContext = bit.getContext();
-    FunctionImplementationRegistry registry = bitContext.getFunctionImplementationRegistry();
-    FragmentContextImpl context = new FragmentContextImpl(bitContext, BitControl.PlanFragment.getDefaultInstance(), null, registry);
-    BufferAllocator bufferAllocator = bitContext.getAllocator();
-    //create RecordBatch
-    VarCharVector vector = new VarCharVector(SchemaBuilder.columnSchema(""a"", TypeProtos.MinorType.VARCHAR, TypeProtos.DataMode.REQUIRED), bufferAllocator);
-    vector.allocateNew();
-    int valueCount = 3;
-    VarCharVector.Mutator mutator = vector.getMutator();
-    mutator.setSafe(0, ""a"".getBytes());
-    mutator.setSafe(1, ""b"".getBytes());
-    mutator.setSafe(2, ""c"".getBytes());
-    mutator.setValueCount(valueCount);
-    VectorContainer vectorContainer = new VectorContainer();
-    TypedFieldId fieldId = vectorContainer.add(vector);
-    RecordBatch recordBatch = new TestRecordBatch(vectorContainer);
-    //construct hash64
-    ValueVectorReadExpression exp = new ValueVectorReadExpression(fieldId);
-    LogicalExpression[] expressions = new LogicalExpression[1];
-    expressions[0] = exp;
-    TypedFieldId[] fieldIds = new TypedFieldId[1];
-    fieldIds[0] = fieldId;
-    ValueVectorHashHelper valueVectorHashHelper = new ValueVectorHashHelper(recordBatch, context);
-    ValueVectorHashHelper.Hash64 hash64 = valueVectorHashHelper.getHash64(expressions, fieldIds);
-
-    //construct BloomFilter
-    int numBytes = BloomFilter.optimalNumOfBytes(3, 0.03);
-
-    BloomFilter bloomFilter = new BloomFilter(numBytes, bufferAllocator);
-    for (int i = 0; i < valueCount; i++) {
-      long hashCode = hash64.hash64Code(i, 0, 0);
-      bloomFilter.insert(hashCode);
+    int userPort = QueryTestUtil.getFreePortNumber(31170, 300);
+    int bitPort = QueryTestUtil.getFreePortNumber(31180, 300);
+    ClusterFixtureBuilder clusterFixtureBuilder = ClusterFixture.bareBuilder(dirTestWatcher)
+        .configProperty(ExecConstants.INITIAL_USER_PORT, userPort)
+        .configProperty(ExecConstants.INITIAL_BIT_PORT, bitPort)
+        .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true);
+    try (ClusterFixture cluster = clusterFixtureBuilder.build()) {
+      Drillbit bit = cluster.drillbit();
+      DrillbitContext bitContext = bit.getContext();
+      FunctionImplementationRegistry registry = bitContext.getFunctionImplementationRegistry();
+      FragmentContextImpl context = new FragmentContextImpl(bitContext, BitControl.PlanFragment.getDefaultInstance(), null, registry);
+      BufferAllocator bufferAllocator = bitContext.getAllocator();
+      //create RecordBatch
+      VarCharVector vector = new VarCharVector(SchemaBuilder.columnSchema(""a"", TypeProtos.MinorType.VARCHAR, TypeProtos.DataMode.REQUIRED), bufferAllocator);
+      vector.allocateNew();
+      int valueCount = 3;
+      VarCharVector.Mutator mutator = vector.getMutator();
+      mutator.setSafe(0, ""a"".getBytes());
+      mutator.setSafe(1, ""b"".getBytes());
+      mutator.setSafe(2, ""c"".getBytes());
+      mutator.setValueCount(valueCount);
+      VectorContainer vectorContainer = new VectorContainer();
+      TypedFieldId fieldId = vectorContainer.add(vector);
+      RecordBatch recordBatch = new TestRecordBatch(vectorContainer);
+      //construct hash64
+      ValueVectorReadExpression exp = new ValueVectorReadExpression(fieldId);
+      LogicalExpression[] expressions = new LogicalExpression[1];
+      expressions[0] = exp;
+      TypedFieldId[] fieldIds = new TypedFieldId[1];
+      fieldIds[0] = fieldId;
+      ValueVectorHashHelper valueVectorHashHelper = new ValueVectorHashHelper(recordBatch, context);
+      ValueVectorHashHelper.Hash64 hash64 = valueVectorHashHelper.getHash64(expressions, fieldIds);
+
+      //construct BloomFilter
+      int numBytes = BloomFilter.optimalNumOfBytes(3, 0.03);
+
+      BloomFilter bloomFilter = new BloomFilter(numBytes, bufferAllocator);
+      for (int i = 0; i < valueCount; i++) {
+        long hashCode = hash64.hash64Code(i, 0, 0);
+        bloomFilter.insert(hashCode);
+      }
+
+      //-----------------create probe side RecordBatch---------------------
+      VarCharVector probeVector = new VarCharVector(SchemaBuilder.columnSchema(""a"", TypeProtos.MinorType.VARCHAR, TypeProtos.DataMode.REQUIRED), bufferAllocator);
+      probeVector.allocateNew();
+      int probeValueCount = 1;
+      VarCharVector.Mutator mutator1 = probeVector.getMutator();
+      mutator1.setSafe(0, ""f"".getBytes());
+      mutator1.setValueCount(probeValueCount);
+      VectorContainer probeVectorContainer = new VectorContainer();
+      TypedFieldId probeFieldId = probeVectorContainer.add(probeVector);
+      RecordBatch probeRecordBatch = new TestRecordBatch(probeVectorContainer);
+      ValueVectorReadExpression probExp = new ValueVectorReadExpression(probeFieldId);
+      LogicalExpression[] probExpressions = new LogicalExpression[1];
+      probExpressions[0] = probExp;
+      TypedFieldId[] probeFieldIds = new TypedFieldId[1];
+      probeFieldIds[0] = probeFieldId;
+      ValueVectorHashHelper probeValueVectorHashHelper = new ValueVectorHashHelper(probeRecordBatch, context);
+      ValueVectorHashHelper.Hash64 probeHash64 = probeValueVectorHashHelper.getHash64(probExpressions, probeFieldIds);
+      long hashCode = probeHash64.hash64Code(0, 0, 0);
+      boolean contain = bloomFilter.find(hashCode);
+      Assert.assertFalse(contain);
+      bloomFilter.getContent().close();
+      vectorContainer.clear();
+      probeVectorContainer.clear();
+      context.close();
     }
-
-    //-----------------create probe side RecordBatch---------------------
-    VarCharVector probeVector = new VarCharVector(SchemaBuilder.columnSchema(""a"", TypeProtos.MinorType.VARCHAR, TypeProtos.DataMode.REQUIRED), bufferAllocator);
-    probeVector.allocateNew();
-    int probeValueCount = 1;
-    VarCharVector.Mutator mutator1 = probeVector.getMutator();
-    mutator1.setSafe(0, ""f"".getBytes());
-    mutator1.setValueCount(probeValueCount);
-    VectorContainer probeVectorContainer = new VectorContainer();
-    TypedFieldId probeFieldId = probeVectorContainer.add(probeVector);
-    RecordBatch probeRecordBatch = new TestRecordBatch(probeVectorContainer);
-    ValueVectorReadExpression probExp = new ValueVectorReadExpression(probeFieldId);
-    LogicalExpression[] probExpressions = new LogicalExpression[1];
-    probExpressions[0] = probExp;
-    TypedFieldId[] probeFieldIds = new TypedFieldId[1];
-    probeFieldIds[0] = probeFieldId;
-    ValueVectorHashHelper probeValueVectorHashHelper = new ValueVectorHashHelper(probeRecordBatch, context);
-    ValueVectorHashHelper.Hash64 probeHash64 = probeValueVectorHashHelper.getHash64(probExpressions, probeFieldIds);
-    long hashCode = probeHash64.hash64Code(0, 0, 0);
-    boolean contain = bloomFilter.find(hashCode);
-    Assert.assertFalse(contain);
-    bloomFilter.getContent().close();
-    vectorContainer.clear();
-    probeVectorContainer.clear();
-    context.close();
-    bitContext.close();
-    bit.close();
   }
 
 
   @Test
   public void testExist() throws Exception {
-
-    Drillbit bit = new Drillbit(c, RemoteServiceSet.getLocalServiceSet(), ClassPathScanner.fromPrescan(c));
-    bit.run();
-    DrillbitContext bitContext = bit.getContext();
-    FunctionImplementationRegistry registry = bitContext.getFunctionImplementationRegistry();
-    FragmentContextImpl context = new FragmentContextImpl(bitContext, BitControl.PlanFragment.getDefaultInstance(), null, registry);
-    BufferAllocator bufferAllocator = bitContext.getAllocator();
-    //create RecordBatch
-    VarCharVector vector = new VarCharVector(SchemaBuilder.columnSchema(""a"", TypeProtos.MinorType.VARCHAR, TypeProtos.DataMode.REQUIRED), bufferAllocator);
-    vector.allocateNew();
-    int valueCount = 3;
-    VarCharVector.Mutator mutator = vector.getMutator();
-    mutator.setSafe(0, ""a"".getBytes());
-    mutator.setSafe(1, ""b"".getBytes());
-    mutator.setSafe(2, ""c"".getBytes());
-    mutator.setValueCount(valueCount);
-    VectorContainer vectorContainer = new VectorContainer();
-    TypedFieldId fieldId = vectorContainer.add(vector);
-    RecordBatch recordBatch = new TestRecordBatch(vectorContainer);
-    //construct hash64
-    ValueVectorReadExpression exp = new ValueVectorReadExpression(fieldId);
-    LogicalExpression[] expressions = new LogicalExpression[1];
-    expressions[0] = exp;
-    TypedFieldId[] fieldIds = new TypedFieldId[1];
-    fieldIds[0] = fieldId;
-    ValueVectorHashHelper valueVectorHashHelper = new ValueVectorHashHelper(recordBatch, context);
-    ValueVectorHashHelper.Hash64 hash64 = valueVectorHashHelper.getHash64(expressions, fieldIds);
-
-    //construct BloomFilter
-    int numBytes = BloomFilter.optimalNumOfBytes(3, 0.03);
-
-    BloomFilter bloomFilter = new BloomFilter(numBytes, bufferAllocator);
-    for (int i = 0; i < valueCount; i++) {
-      long hashCode = hash64.hash64Code(i, 0, 0);
-      bloomFilter.insert(hashCode);
+    int userPort = QueryTestUtil.getFreePortNumber(31170, 300);
+    int bitPort = QueryTestUtil.getFreePortNumber(31180, 300);
+    ClusterFixtureBuilder clusterFixtureBuilder = ClusterFixture.bareBuilder(dirTestWatcher)
+        .configProperty(ExecConstants.INITIAL_USER_PORT, userPort)
+        .configProperty(ExecConstants.INITIAL_BIT_PORT, bitPort)
+        .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true);
+    try (ClusterFixture cluster = clusterFixtureBuilder.build()) {
+      Drillbit bit = cluster.drillbit();
+      DrillbitContext bitContext = bit.getContext();
+      FunctionImplementationRegistry registry = bitContext.getFunctionImplementationRegistry();
+      FragmentContextImpl context = new FragmentContextImpl(bitContext, BitControl.PlanFragment.getDefaultInstance(), null, registry);
+      BufferAllocator bufferAllocator = bitContext.getAllocator();
+      //create RecordBatch
+      VarCharVector vector = new VarCharVector(SchemaBuilder.columnSchema(""a"", TypeProtos.MinorType.VARCHAR, TypeProtos.DataMode.REQUIRED), bufferAllocator);
+      vector.allocateNew();
+      int valueCount = 3;
+      VarCharVector.Mutator mutator = vector.getMutator();
+      mutator.setSafe(0, ""a"".getBytes());
+      mutator.setSafe(1, ""b"".getBytes());
+      mutator.setSafe(2, ""c"".getBytes());
+      mutator.setValueCount(valueCount);
+      VectorContainer vectorContainer = new VectorContainer();
+      TypedFieldId fieldId = vectorContainer.add(vector);
+      RecordBatch recordBatch = new TestRecordBatch(vectorContainer);
+      //construct hash64
+      ValueVectorReadExpression exp = new ValueVectorReadExpression(fieldId);
+      LogicalExpression[] expressions = new LogicalExpression[1];
+      expressions[0] = exp;
+      TypedFieldId[] fieldIds = new TypedFieldId[1];
+      fieldIds[0] = fieldId;
+      ValueVectorHashHelper valueVectorHashHelper = new ValueVectorHashHelper(recordBatch, context);
+      ValueVectorHashHelper.Hash64 hash64 = valueVectorHashHelper.getHash64(expressions, fieldIds);
+
+      //construct BloomFilter
+      int numBytes = BloomFilter.optimalNumOfBytes(3, 0.03);
+
+      BloomFilter bloomFilter = new BloomFilter(numBytes, bufferAllocator);
+      for (int i = 0; i < valueCount; i++) {
+        long hashCode = hash64.hash64Code(i, 0, 0);
+        bloomFilter.insert(hashCode);
+      }
+
+      //-----------------create probe side RecordBatch---------------------
+      VarCharVector probeVector = new VarCharVector(SchemaBuilder.columnSchema(""a"", TypeProtos.MinorType.VARCHAR, TypeProtos.DataMode.REQUIRED), bufferAllocator);","[{'comment': 'This code looks highly redundant. In general copy/paste is a poor form of reuse. Can any of this be pulled out into a method? If not a method, can you create a ""fixture"" class to hold the common bits?\r\n\r\nAlso if you use the `RowSet` mechanism to create your vectors, you can use some handy utilities such as `RowSetUtilities.setFromInt()` to set (just about) any data type from an int value. See `TestCopier` for an example where this is used.\r\n\r\nThe result should be that each test case reduces to a few setup lines.', 'commenter': 'paul-rogers'}, {'comment': 'Rewritten tests to use row sets and moved out common code.', 'commenter': 'vvysotskyi'}]"
1987,exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java,"@@ -73,31 +73,39 @@ private static void enableDrillPortHunting(ClusterFixtureBuilder builder) {
     builder.configBuilder.put(ExecConstants.DRILL_PORT_HUNT, true);
     builder.configBuilder.put(ExecConstants.GRACE_PERIOD, 500);
     builder.configBuilder.put(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true);
+
+    setTestDirectories(builder);
+  }
+
+  private static void setTestDirectories(ClusterFixtureBuilder builder) {
+    builder.configBuilder.put(ExecConstants.DRILL_TMP_DIR, dirTestWatcher.getTmpDir().getAbsolutePath());
+    builder.configBuilder.put(ExecConstants.SYS_STORE_PROVIDER_LOCAL_PATH, dirTestWatcher.getStoreDir().getAbsolutePath());","[{'comment': ""Can this be done in `ClusterFixture` or its builder so we use a consistent set of directories everywhere? I've been burned by these being a bit ill-defined."", 'commenter': 'paul-rogers'}, {'comment': '`ClusterFixture` has already set these directories with correct values. It was done here since instead on builder with test configuration was used `bareBuilder`.\r\nI have checked, and it also works with regular builder. So I have removed these configs and replaced `bareBuilder` with `builder`.', 'commenter': 'vvysotskyi'}]"
1987,exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java,"@@ -262,17 +265,15 @@ private boolean waitAndAssertDrillbitCount(ClusterFixture cluster, int zkRefresh
   }
 
   private static void setupFile(int file_num) throws Exception {
-    final String file = ""employee""+file_num+"".json"";
-    final Path path = dirTestWatcher.getRootDir().toPath().resolve(file);
-    try(PrintWriter out = new PrintWriter(new BufferedWriter(new FileWriter(path.toFile(), true)))) {
+    String file = ""employee"" + file_num + "".json"";
+    Path path = dirTestWatcher.getRootDir().toPath().resolve(file);
+    try (PrintWriter out = new PrintWriter(new BufferedWriter(new FileWriter(path.toFile(), true)))) {","[{'comment': 'I realize the code here is original; but it might be a bit cleaner to put the data in a resource file than in Java.', 'commenter': 'paul-rogers'}, {'comment': 'Looks like these lines are first 7 lines from `employee.json`. I have replaced it by reading its content.', 'commenter': 'vvysotskyi'}]"
1987,exec/java-exec/src/test/java/org/apache/drill/exec/cache/TestWriteToDisk.java,"@@ -18,95 +18,62 @@
 package org.apache.drill.exec.cache;
 
 import java.io.File;
-import java.util.List;
 
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSets;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
 import org.apache.drill.shaded.guava.com.google.common.io.Files;
-import org.apache.drill.common.config.DrillConfig;
 import org.apache.drill.common.types.TypeProtos;
-import org.apache.drill.common.types.Types;
-import org.apache.drill.test.TestTools;
+import org.apache.drill.test.SubOperatorTest;
 import org.apache.drill.exec.ExecTest;
-import org.apache.drill.exec.expr.TypeHelper;
-import org.apache.drill.exec.record.MaterializedField;
 import org.apache.drill.exec.record.VectorContainer;
 import org.apache.drill.exec.record.WritableBatch;
-import org.apache.drill.exec.server.Drillbit;
-import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.server.RemoteServiceSet;
-import org.apache.drill.exec.vector.AllocationHelper;
-import org.apache.drill.exec.vector.IntVector;
-import org.apache.drill.exec.vector.ValueVector;
-import org.apache.drill.exec.vector.VarBinaryVector;
+import org.apache.drill.test.rowSet.RowSetUtilities;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.junit.Rule;
 import org.junit.Test;
 
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
-import org.junit.rules.TestRule;
-
-public class TestWriteToDisk extends ExecTest {
-  @Rule public final TestRule TIMEOUT = TestTools.getTimeoutRule(90000); // 90secs
+public class TestWriteToDisk extends SubOperatorTest {
 
   @Test
-  @SuppressWarnings(""static-method"")
   public void test() throws Exception {
-    final List<ValueVector> vectorList = Lists.newArrayList();
-    final DrillConfig config = DrillConfig.create();
-    try (final RemoteServiceSet serviceSet = RemoteServiceSet
-        .getLocalServiceSet();
-        final Drillbit bit = new Drillbit(config, serviceSet)) {
-      bit.run();
-      final DrillbitContext context = bit.getContext();
-
-      final MaterializedField intField = MaterializedField.create(""int"", Types.required(TypeProtos.MinorType.INT));
-      final MaterializedField binField = MaterializedField.create(""binary"", Types.required(TypeProtos.MinorType.VARBINARY));
-      try (final IntVector intVector = (IntVector) TypeHelper.getNewVector(intField, context.getAllocator());
-          final VarBinaryVector binVector =
-              (VarBinaryVector) TypeHelper.getNewVector(binField, context.getAllocator())) {
-        AllocationHelper.allocate(intVector, 4, 4);
-        AllocationHelper.allocate(binVector, 4, 5);
-        vectorList.add(intVector);
-        vectorList.add(binVector);
+    VectorContainer container = expectedRowSet().container();
 
-        intVector.getMutator().setSafe(0, 0);
-        binVector.getMutator().setSafe(0, ""ZERO"".getBytes());
-        intVector.getMutator().setSafe(1, 1);
-        binVector.getMutator().setSafe(1, ""ONE"".getBytes());
-        intVector.getMutator().setSafe(2, 2);
-        binVector.getMutator().setSafe(2, ""TWO"".getBytes());
-        intVector.getMutator().setSafe(3, 3);
-        binVector.getMutator().setSafe(3, ""THREE"".getBytes());
-        intVector.getMutator().setValueCount(4);
-        binVector.getMutator().setValueCount(4);
+    WritableBatch batch = WritableBatch.getBatchNoHVWrap(container.getRecordCount(), container, false);
 
-        VectorContainer container = new VectorContainer();
-        container.addCollection(vectorList);
-        container.setRecordCount(4);
-        WritableBatch batch = WritableBatch.getBatchNoHVWrap(
-            container.getRecordCount(), container, false);
-        VectorAccessibleSerializable wrap = new VectorAccessibleSerializable(
-            batch, context.getAllocator());
+    VectorAccessibleSerializable wrap = new VectorAccessibleSerializable(batch, fixture.allocator());
 
-        final VectorAccessibleSerializable newWrap = new VectorAccessibleSerializable(
-            context.getAllocator());
-        try (final FileSystem fs = getLocalFileSystem()) {
-          final File tempDir = Files.createTempDir();
-          tempDir.deleteOnExit();
-          final Path path = new Path(tempDir.getAbsolutePath(), ""drillSerializable"");
-          try (final FSDataOutputStream out = fs.create(path)) {
-            wrap.writeToStream(out);
-          }
-
-          try (final FSDataInputStream in = fs.open(path)) {
-            newWrap.readFromStream(in);
-          }
-        }
+    VectorAccessibleSerializable newWrap = new VectorAccessibleSerializable(fixture.allocator());
+    try (FileSystem fs = ExecTest.getLocalFileSystem()) {
+      File tempDir = Files.createTempDir();","[{'comment': 'I think the idea of the `DirTestWatcher` is that it will provide you with a temp dir and will ensure the directory is cleaned on exit. Also, the temp dir will be in the `target` folder, so it is cleaned on the next build, even if the code crashes. I think you want to use `BaseTestDirWatcher.getTmpDir()`, or a variation.', 'commenter': 'paul-rogers'}, {'comment': 'Agree that using `tmp` dir from `dirTestWatcher` would be better, thanks for pointing this, replaced as you have suggested.', 'commenter': 'vvysotskyi'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryResources.java,"@@ -96,10 +101,13 @@ public QueryResult submitQueryJSON(QueryWrapper query) throws Exception {
   @Produces(MediaType.TEXT_HTML)
   public Viewable submitQuery(@FormParam(""query"") String query,
                               @FormParam(""queryType"") String queryType,
-                              @FormParam(""autoLimit"") String autoLimit) throws Exception {
+                              @FormParam(""autoLimit"") String autoLimit,
+                              Form form) throws Exception {","[{'comment': 'Ah, so you do have a solution for session options. Suggestion: since the two changes affect exactly the same code, just go ahead and combine them. You can create a roll-up JIRA that has the two specific JIRA tickets as sub-tasks.', 'commenter': 'paul-rogers'}, {'comment': 'Hmm I originally had them all together and pulled them apart for separate feedback & tracking.  Oops ... maybe.  I personally do prefer small PRs that add specific functionality, though.', 'commenter': 'dobesv'}, {'comment': 'You have the right idea in general. If we could approve and commit PRs quickly, then smaller would absolutely be better. Unfortunately, it typically takes a week (sometimes several) to get reviews done.\r\n\r\nFor historical reasons, commits are done only after running tests on a non-public cluster. The folks who have access to that cluster run tests only about once a week, typically on Fridays. To reduce work, that person does a ""batch commit"", combines that week\'s commits into a single branch so tests can be run once. It ain\'t pretty, but it is who things have been done for a long time.\r\n\r\nIf your code conflicts with another PR (such as your own other change in the same place), the committer will pick one of them, ask you to resolve conflicts, and will then commit the other in the next commit batch a week or two later.\r\n\r\nGiven our (again, not pretty) process, you are better off combining conflicting changes into a single PR.\r\n\r\nThis is, by the way, why some of my PRs are big: I can\'t wait months to get it done in small pieces the way I ""should"" do it -- I\'d completely forget what I was doing several 2-3 week cycles.\r\n\r\nWe absolutely want to change it, but don\'t have a good way to run the ""pre-commit"" tests except on that non-public cluster.\r\n\r\nAny suggestions you\'ve seen work on other projects?', 'commenter': 'paul-rogers'}, {'comment': ""Well, seems like you've covered it - the tests should be run more often so problems can be picked up faster and merges done whenever people are ready."", 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryResources.java,"@@ -110,6 +118,48 @@ public Viewable submitQuery(@FormParam(""query"") String query,
     }
   }
 
+  @Nonnull
+  private Map<String, Object> readOptionsFromForm(Form form) {
+    Map<String, Object> options = new HashMap<String, Object>();
+    SessionOptionManager sessionOptionManager = webUserConnection.getSession().getOptions();
+    for (Map.Entry<String, List<String>> pair : form.asMap().entrySet()) {
+      String name = pair.getKey();
+      OptionDefinition definition = sessionOptionManager.getOptionDefinition(name);
+      if (definition != null) {
+        String valueStr = pair.getValue().get(0);
+        if (!valueStr.isEmpty()) {
+          try {
+            Object value = null;
+            switch (definition.getValidator().getKind()) {
+              case BOOLEAN:","[{'comment': 'Nit: `value = Boolean.parseBoolean(valueStr);` (Or `valueOf()`, which seems to be the new style.)', 'commenter': 'paul-rogers'}, {'comment': 'I considered it, but I wanted to validate that the input was either `true` or `false`, so I wrote my own algorithm.  `parseBoolean` will just return `false` if the input isn\'t `""true""` but otherwise it accepts anything.', 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryWrapper.java,"@@ -131,6 +146,52 @@ public QueryResult run(final WorkManager workManager, final WebUserConnection we
     return new QueryResult(queryId, webUserConnection, webUserConnection.results);
   }
 
+  private void applyOptions(WebUserConnection webUserConnection) throws BadRequestException {","[{'comment': 'This and the above can probably be simplified. Just keep the options as a string map until here. Then use `OptionManager.setLocalOption(String name, String value)`. (Why it is called ""local"" I don\'t know, nor do I know why we have one that takes an `Object` and another that takes a `String`, seems an unsafe overloading to me.) Anyway, that method does the very same string parsing this code does above, and avoids the need for the double type check.', 'commenter': 'paul-rogers'}, {'comment': 'When the options come from the form, they do arrive as strings.  But if it is a JSON payload, then the options object values could be boolean, number, etc... any JSON value would be possible there.  I wanted to accept actual boolean / numeric values from a JSON payload rather than have them provide the booleans / number as string.', 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/main/resources/rest/query/query.ftl,"@@ -83,6 +83,16 @@
       Submit
     </button>
     <input type=""checkbox"" name=""forceLimit"" value=""limit"" <#if model.isAutoLimitEnabled()>checked</#if>> Limit results to <input type=""text"" id=""autoLimit"" name=""autoLimit"" min=""0"" value=""${model.getDefaultRowsAutoLimited()?c}"" size=""6"" pattern=""[0-9]*""> rows <span class=""glyphicon glyphicon-info-sign"" title=""Limits the number of records retrieved in the query. Ignored if query has a limit already"" style=""cursor:pointer""></span>
+    <label> Store Format
+      <select name=""store.format"">
+        <option value=""parquet"">Parquet</option>
+        <option value=""json"">JSON</option>
+        <option value=""csv"">Comma-Separated Values</option>
+        <option value=""psv"">Pipe-Separated Values</option>
+        <option value=""tsv"">Tab-Separated Values</option>","[{'comment': 'Should these be hard-coded? Other format plugins can offer the ability to write to that format. Can they be used for a `CREATE TABLE`?\r\n\r\nAlso, it is annoying, but each plugin declares the formats that it supports. As a result, the user could select `csv`, but that format might not be available for a given file storage plugin. Or, there might be two: `csv` and `csvh`.\r\n\r\nLong story short, should we get this list from the plugin which can get it from its list of supported writable formats?', 'commenter': 'paul-rogers'}, {'comment': ""That could be an interesting bit of future work for someone - I'm not up for it personally."", 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryResources.java,"@@ -110,6 +118,48 @@ public Viewable submitQuery(@FormParam(""query"") String query,
     }
   }
 
+  @Nonnull","[{'comment': 'Please remove, we should avoid dependency to `javax.annotation`.', 'commenter': 'arina-ielchiieva'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryResources.java,"@@ -110,6 +118,48 @@ public Viewable submitQuery(@FormParam(""query"") String query,
     }
   }
 
+  @Nonnull
+  private Map<String, Object> readOptionsFromForm(Form form) {
+    Map<String, Object> options = new HashMap<String, Object>();","[{'comment': '```suggestion\r\n    Map<String, Object> options = new HashMap<>();\r\n```', 'commenter': 'arina-ielchiieva'}]"
1997,exec/java-exec/src/main/resources/rest/query/query.ftl,"@@ -83,6 +83,16 @@
       Submit
     </button>
     <input type=""checkbox"" name=""forceLimit"" value=""limit"" <#if model.isAutoLimitEnabled()>checked</#if>> Limit results to <input type=""text"" id=""autoLimit"" name=""autoLimit"" min=""0"" value=""${model.getDefaultRowsAutoLimited()?c}"" size=""6"" pattern=""[0-9]*""> rows <span class=""glyphicon glyphicon-info-sign"" title=""Limits the number of records retrieved in the query. Ignored if query has a limit already"" style=""cursor:pointer""></span>
+    <label> Store Format","[{'comment': 'We should remove this part since store format names depend on the file storage plugin configuration and current default schema thus they cannot be hard-coded.', 'commenter': 'arina-ielchiieva'}, {'comment': ""This should work for 99% of users, I think.  If they select something that isn't supported they will just get an error.  The alternative would be a free-form text input, which would still just report an error if you input an unsupported value.  This drop-down can be updated over time to include all known supported output formats of all the plugins and the user can more easily pick a format using a drop-down than a text input.\r\n"", 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryWrapper.java,"@@ -48,13 +51,21 @@
   private final String queryType;
   private final int autoLimitRowCount;
 
+  @Nullable","[{'comment': 'Please remove.', 'commenter': 'arina-ielchiieva'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryWrapper.java,"@@ -131,6 +146,52 @@ public QueryResult run(final WorkManager workManager, final WebUserConnection we
     return new QueryResult(queryId, webUserConnection, webUserConnection.results);
   }
 
+  private void applyOptions(WebUserConnection webUserConnection) throws BadRequestException {
+    if (options != null && !options.isEmpty()) {
+      SessionOptionManager sessionOptionManager = webUserConnection.getSession().getOptions();
+      for (Map.Entry<String, Object> entry : options.entrySet()) {
+        String name = entry.getKey();
+        OptionDefinition definition = sessionOptionManager.getOptionDefinition(name);
+        if (definition == null) {
+          throw UserException.validationError().message(""Unsupported option '%s'"", name).build(logger);
+        }
+        if (!(definition.getMetaData().getAccessibleScopes().inScopeOf(OptionValue.OptionScope.SESSION) || definition.getMetaData().getAccessibleScopes().inScopeOf(OptionValue.OptionScope.QUERY))) {
+          throw UserException.validationError().message(""Option '%s' is not a session / query option"", name).build(logger);
+        }
+        if (definition.getMetaData().isInternal()) {
+          throw UserException.validationError().message(""Internal option '%s' cannot be set with query"", name).build(logger);
+        }
+        Object value = entry.getValue();
+        switch (definition.getValidator().getKind()) {
+          case BOOLEAN:
+            if (!(value instanceof Boolean)) {
+              throw UserException.validationError().message(""Expected boolean value for option '%s'"", name).build(logger);
+            }
+            sessionOptionManager.setLocalOption(name, (Boolean) value);
+            break;
+          case DOUBLE:
+            if (!(value instanceof Number)) {
+              throw UserException.validationError().message(""Expected number value for option '%s'"", name).build(logger);
+            }
+            sessionOptionManager.setLocalOption(name, ((Number) value).doubleValue());
+            break;
+          case STRING:
+            if (!(value instanceof String)) {
+              throw UserException.validationError().message(""Expected string value for option '%s'"", name).build(logger);
+            }
+            sessionOptionManager.setLocalOption(name, (String) value);
+            break;
+          case LONG:
+            if (!(value instanceof Number)) {
+              throw UserException.validationError().message(""Expected number value for option '%s'"", name).build(logger);
+            }
+            sessionOptionManager.setLocalOption(name, ((Number) value).longValue());
+            break;","[{'comment': 'Please add behavior in case of we encounter unexpected kind.', 'commenter': 'arina-ielchiieva'}]"
1997,exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/TestQueryWrapper.java,"@@ -0,0 +1,224 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.server.rest;
+
+import io.netty.channel.DefaultChannelPromise;
+import io.netty.channel.local.LocalAddress;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.proto.UserBitShared;
+import org.apache.drill.exec.rpc.user.UserSession;
+import org.apache.drill.exec.server.Drillbit;
+import org.apache.drill.exec.server.options.SystemOptionManager;
+import org.apache.drill.exec.server.rest.auth.DrillUserPrincipal;
+import org.apache.drill.exec.work.WorkManager;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.eclipse.jetty.servlet.ServletContextHandler;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.apache.drill.exec.ExecConstants.ENABLE_VERBOSE_ERRORS_KEY;
+import static org.apache.drill.exec.ExecConstants.OUTPUT_FORMAT_OPTION;
+import static org.apache.drill.exec.ExecConstants.PARQUET_FLAT_BATCH_MEMORY_SIZE_VALIDATOR;
+import static org.apache.drill.exec.ExecConstants.QUERY_MAX_ROWS;
+import static org.apache.drill.exec.ExecConstants.TEXT_ESTIMATED_ROW_SIZE;
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotEquals;
+import static org.junit.Assert.assertThat;
+import static org.junit.Assert.fail;
+
+public class TestQueryWrapper extends ClusterTest {","[{'comment': ""What if we move server setup into WebClusterTest class and all tests that would test web part will extend it and thus won't have repeat conf code part?"", 'commenter': 'arina-ielchiieva'}, {'comment': 'Hmm ... maybe ?  This might be beyond my drill budget right now, though.', 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/TestQueryWrapper.java,"@@ -0,0 +1,224 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.server.rest;
+
+import io.netty.channel.DefaultChannelPromise;
+import io.netty.channel.local.LocalAddress;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.proto.UserBitShared;
+import org.apache.drill.exec.rpc.user.UserSession;
+import org.apache.drill.exec.server.Drillbit;
+import org.apache.drill.exec.server.options.SystemOptionManager;
+import org.apache.drill.exec.server.rest.auth.DrillUserPrincipal;
+import org.apache.drill.exec.work.WorkManager;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.eclipse.jetty.servlet.ServletContextHandler;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.apache.drill.exec.ExecConstants.ENABLE_VERBOSE_ERRORS_KEY;
+import static org.apache.drill.exec.ExecConstants.OUTPUT_FORMAT_OPTION;
+import static org.apache.drill.exec.ExecConstants.PARQUET_FLAT_BATCH_MEMORY_SIZE_VALIDATOR;
+import static org.apache.drill.exec.ExecConstants.QUERY_MAX_ROWS;
+import static org.apache.drill.exec.ExecConstants.TEXT_ESTIMATED_ROW_SIZE;
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotEquals;
+import static org.junit.Assert.assertThat;
+import static org.junit.Assert.fail;
+
+public class TestQueryWrapper extends ClusterTest {
+
+  private static DrillRestServer server;
+  private static WorkManager workManager;
+  private static WebServer webServer;
+
+  private void setupServer() throws IOException {
+    boolean impersonationEnabled = false;
+    setupServer(impersonationEnabled);
+  }
+
+  private void setupServer(boolean impersonationEnabled) {
+    cluster = ClusterFixture.bareBuilder(dirTestWatcher)
+      .clusterSize(1)
+      .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true)
+      .configProperty(ExecConstants.IMPERSONATION_ENABLED, impersonationEnabled)
+      .build();
+    Drillbit drillbit = cluster.drillbit();
+    workManager = drillbit.getManager();
+    webServer = drillbit.getWebServer();
+    final ServletContextHandler servletContextHandler = new ServletContextHandler(ServletContextHandler.SESSIONS);
+    servletContextHandler.setContextPath(""/"");
+    server = new DrillRestServer(workManager, servletContextHandler.getServletContext(), drillbit);
+  }
+
+  private QueryWrapper.QueryResult run(String sql) throws Exception {
+    return runWithOptions(sql, null);
+  }
+
+  private QueryWrapper.QueryResult runWithOptions(String sql, Map<String, Object> options) throws Exception {
+    if (cluster == null) {","[{'comment': 'Same as in previous PR, better to setup cluster before the tests.', 'commenter': 'arina-ielchiieva'}, {'comment': 'In the other PR I setup the cluster with different options in different tests (e.g. with or without impersonation).  I might as well keep that structure here in case all the PRs are merged that capability will be preserved.', 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryResources.java,"@@ -110,6 +118,48 @@ public Viewable submitQuery(@FormParam(""query"") String query,
     }
   }
 
+  @Nonnull
+  private Map<String, Object> readOptionsFromForm(Form form) {
+    Map<String, Object> options = new HashMap<String, Object>();
+    SessionOptionManager sessionOptionManager = webUserConnection.getSession().getOptions();
+    for (Map.Entry<String, List<String>> pair : form.asMap().entrySet()) {
+      String name = pair.getKey();
+      OptionDefinition definition = sessionOptionManager.getOptionDefinition(name);
+      if (definition != null) {
+        String valueStr = pair.getValue().get(0);
+        if (!valueStr.isEmpty()) {
+          try {
+            Object value = null;
+            switch (definition.getValidator().getKind()) {","[{'comment': 'You do iteration over switch by kind twice here and in  `QueryWrapper` class, I think you can do only once in QueryWrapper.', 'commenter': 'arina-ielchiieva'}, {'comment': 'When I parse the form the inputs are all strings, but when the input comes from JSON they can be mixed types.  I think it is better this way despite the apparent duplication.', 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryWrapper.java,"@@ -48,13 +51,21 @@
   private final String queryType;
   private final int autoLimitRowCount;
 
+  @Nullable
+  private final Map<String, Object> options;
+
   private static MemoryMXBean memMXBean = ManagementFactory.getMemoryMXBean();
 
   @JsonCreator
-  public QueryWrapper(@JsonProperty(""query"") String query, @JsonProperty(""queryType"") String queryType, @JsonProperty(""autoLimit"") String autoLimit) {
+  public QueryWrapper(
+    @JsonProperty(""query"") String query,
+    @Nullable @JsonProperty(""queryType"") String queryType,","[{'comment': 'Please remove nullable annotation, `javax.annotation` is a separate dependency and it is not built in.', 'commenter': 'arina-ielchiieva'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryResources.java,"@@ -110,6 +117,47 @@ public Viewable submitQuery(@FormParam(""query"") String query,
     }
   }
 
+  private Map<String, Object> readOptionsFromForm(Form form) {
+    Map<String, Object> options = new HashMap<>();
+    SessionOptionManager sessionOptionManager = webUserConnection.getSession().getOptions();
+    for (Map.Entry<String, List<String>> pair : form.asMap().entrySet()) {
+      String name = pair.getKey();
+      OptionDefinition definition = sessionOptionManager.getOptionDefinition(name);
+      if (definition != null) {
+        String valueStr = pair.getValue().get(0);
+        if (!valueStr.isEmpty()) {
+          try {
+            Object value = null;
+            switch (definition.getValidator().getKind()) {
+              case BOOLEAN:
+                if (""true"".equals(valueStr)) {
+                  value = true;
+                } else if (""false"".equals(valueStr)) {
+                  value = false;
+                } else {
+                  throw new BadRequestException(String.format(""Invalid boolean option value for %s: %s"", name, valueStr));
+                }
+                break;
+              case DOUBLE:
+                value = Double.valueOf(valueStr);
+                break;
+              case STRING:
+                value = valueStr;
+                break;
+              case LONG:
+                value = Long.valueOf(valueStr);
+                break;
+            }
+            options.put(name, value);","[{'comment': '`value` will be `null` here for an unknown type. Also, there is probably an IDE warning for missing `default` clause. Maybe add one that ignores (or logs) the ""should never occur"" case of an unknown option type.', 'commenter': 'paul-rogers'}, {'comment': ""I'm not exactly sure what an unknown type is - the switch/case is exhaustive for that enum currently:\r\n\r\n```\r\npublic enum Kind {\r\n    BOOLEAN, LONG, STRING, DOUBLE\r\n  }\r\n```\r\n\r\nI have added a default clause anyway."", 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryWrapper.java,"@@ -76,7 +85,11 @@ public QueryResult run(final WorkManager workManager, final WebUserConnection we
         .setAutolimitRowcount(autoLimitRowCount)
         .build();
 
-    int defaultMaxRows = webUserConnection.getSession().getOptions().getOption(ExecConstants.QUERY_MAX_ROWS).num_val.intValue();
+    applyOptions(webUserConnection);
+
+    SessionOptionManager options = webUserConnection.getSession().getOptions();
+
+    int defaultMaxRows = options.getOption(ExecConstants.QUERY_MAX_ROWS).num_val.intValue();","[{'comment': 'The modern way to do this is:\r\n\r\n```\r\nOptionManager options = ...\r\nint defaultMaxRows = (int) options.getLong(ExecConstants.QUERY_MAX_ROWS);\r\n```\r\n\r\nYes, we should have a `getInt()` method...', 'commenter': 'paul-rogers'}]"
1997,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/QueryWrapper.java,"@@ -131,6 +144,54 @@ public QueryResult run(final WorkManager workManager, final WebUserConnection we
     return new QueryResult(queryId, webUserConnection, webUserConnection.results);
   }
 
+  private void applyOptions(WebUserConnection webUserConnection) throws BadRequestException {
+    if (options != null && !options.isEmpty()) {
+      SessionOptionManager sessionOptionManager = webUserConnection.getSession().getOptions();
+      for (Map.Entry<String, Object> entry : options.entrySet()) {","[{'comment': 'Beyond the scope of this PR, but a better approach would be to implement a `setOption(String name, Object value)` method in `BaseOptionManager`. Then, build on that with a `setOptions<Map<String, Object>)` method. Then, we reuse these methods in the several other places where we do such things. Filed DRILL-7611 to track this request.', 'commenter': 'paul-rogers'}, {'comment': ""Hmm, maybe.  I'm not sure that would ever be used anywhere else but in here, though.  Could probably hold off on that refactoring until you have at least two places where it would be useful."", 'commenter': 'dobesv'}]"
1997,exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/TestQueryWrapper.java,"@@ -0,0 +1,224 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.server.rest;
+
+import io.netty.channel.DefaultChannelPromise;
+import io.netty.channel.local.LocalAddress;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.proto.UserBitShared;
+import org.apache.drill.exec.rpc.user.UserSession;
+import org.apache.drill.exec.server.Drillbit;
+import org.apache.drill.exec.server.options.SystemOptionManager;
+import org.apache.drill.exec.server.rest.auth.DrillUserPrincipal;
+import org.apache.drill.exec.work.WorkManager;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.eclipse.jetty.servlet.ServletContextHandler;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.apache.drill.exec.ExecConstants.ENABLE_VERBOSE_ERRORS_KEY;
+import static org.apache.drill.exec.ExecConstants.OUTPUT_FORMAT_OPTION;
+import static org.apache.drill.exec.ExecConstants.PARQUET_FLAT_BATCH_MEMORY_SIZE_VALIDATOR;
+import static org.apache.drill.exec.ExecConstants.QUERY_MAX_ROWS;
+import static org.apache.drill.exec.ExecConstants.TEXT_ESTIMATED_ROW_SIZE;
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotEquals;
+import static org.junit.Assert.assertThat;
+import static org.junit.Assert.fail;
+
+public class TestQueryWrapper extends ClusterTest {
+
+  private static DrillRestServer server;
+  private static WorkManager workManager;
+  private static WebServer webServer;
+
+  private void setupServer() throws IOException {
+    boolean impersonationEnabled = false;
+    setupServer(impersonationEnabled);
+  }","[{'comment': 'I think I saw a comment from @arina-ielchiieva about this. The ideomatic way to set up a server is to do it in a `@BeforeClass` method. You can call the code you have below. This structure runs in parallel with the `@AfterClass` logic in the base class.\r\n\r\nIf you want to start the server with impersonation enabled in some tests, disabled in others, then you can put the setup logic in a base class (without the `@BeforeClass` annotation), then have each test call the `setupServer()` method in its own `@BeforeClass`. See the `BaseCsvTest` and its subclases for an example.', 'commenter': 'paul-rogers'}, {'comment': 'I think I have long exceeded my budget for this task ... is this a hard requirement?', 'commenter': 'dobesv'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/ops/DataTunnelStatusHandler.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.ops;
+
+import io.netty.buffer.ByteBuf;
+import org.apache.drill.exec.proto.BitData;
+import org.apache.drill.exec.rpc.Acks;
+import org.apache.drill.exec.rpc.RpcException;
+import org.apache.drill.exec.rpc.RpcOutcomeListener;
+
+/**
+ * Listener that keeps track of the status of batches sent, and updates the SendingAccountor when status is received
+ * for each batch
+ */
+public class DataTunnelStatusHandler implements RpcOutcomeListener<BitData.AckWithCredit> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DataTunnelStatusHandler.class);","[{'comment': 'Nit: please use imports. All these fully qualified forms you see are the result of some script taking the easy way out.', 'commenter': 'paul-rogers'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/ops/DataTunnelStatusHandler.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.ops;
+
+import io.netty.buffer.ByteBuf;
+import org.apache.drill.exec.proto.BitData;
+import org.apache.drill.exec.rpc.Acks;
+import org.apache.drill.exec.rpc.RpcException;
+import org.apache.drill.exec.rpc.RpcOutcomeListener;
+
+/**
+ * Listener that keeps track of the status of batches sent, and updates the SendingAccountor when status is received
+ * for each batch
+ */
+public class DataTunnelStatusHandler implements RpcOutcomeListener<BitData.AckWithCredit> {
+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DataTunnelStatusHandler.class);
+  private final SendingAccountor sendingAccountor;
+  private final Consumer<RpcException> consumer;
+
+  public DataTunnelStatusHandler(Consumer<RpcException> consumer, SendingAccountor sendingAccountor) {
+    this.consumer = consumer;
+    this.sendingAccountor = sendingAccountor;
+  }
+
+  @Override
+  public void failed(RpcException ex) {
+    sendingAccountor.decrement();
+    consumer.accept(ex);
+  }
+
+  @Override
+  public void success(BitData.AckWithCredit value, ByteBuf buffer) {
+    sendingAccountor.decrement();
+    if (value.getAllowedCredit() != Acks.FAIL_CREDIT) {
+      return;
+    }
+
+    logger.error(""Data not accepted downstream. Stopping future sends."");","[{'comment': 'Looking at the code further down, it appears that the fail case indicates a query failure. The issue then, is not that the receiver did not accept the data. Rather, the receiver is telling us the query has died and future sends are irrelevant.\r\n\r\nMaybe adjust the log message to indicate that the receiver has informed us the query has failed.', 'commenter': 'paul-rogers'}, {'comment': 'This class is mainly copied from the StatusHandler. The log message seems OK. A clear message will be better.', 'commenter': 'weijietong'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/DynamicSemaphore.java,"@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.rpc;
+
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.ReentrantLock;
+","[{'comment': ""This class would benefit from a description, perhaps a form of that in your commit message.\r\n\r\nOne bit of info missing is the goal of the credit. I gather it is to speed up the query by allowing the upstream to get further ahead by using receiver-side memory to buffer results. That is, it is a performance/memory tradeoff.\r\n\r\nLooks like the credit itself is computed elsewhere, so I'm hoping for a description of the algorithm there."", 'commenter': 'paul-rogers'}, {'comment': ""Yes, I would do that. Besides the tradeoff you mentioned above, the main problem is to send data \r\n with a speed according to their real data size. To some queries, such as: select user_id from table \r\nand select * from table, the former's batch size is normally smaller than the latter one. So receiver could receive more batch to the former query. Of course the sender would send their data more rapidly, and then accelerate the execution speed without more time spending to wait for the semaphore.  This would also make the scanner storage plugin to release their resource more sooner to support more queries. "", 'commenter': 'weijietong'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/rpc/data/AckSender.java,"@@ -56,8 +60,27 @@ void clear() {
    * response upstream.
    */
   public void sendOk() {
+    sendOk(Acks.NO_SUGGESTED_CREDIT);
+  }
+
+  /**
+   * Decrement the number of references still holding on to this response. When the number of references hit zero, send
+   * response upstream. Ack for the dynamic credit model
+   * credit == -1 means Fail
+   * @param credit
+   */
+  public void sendOk(int credit) {
+    everLargestAdviceCredit = credit > everLargestAdviceCredit ? credit : everLargestAdviceCredit;","[{'comment': 'This might be a bit clearer:\r\n```\r\neverLargestAdviceCredit = Math.max(everLargestAdviceCredit, credit);\r\n```', 'commenter': 'paul-rogers'}]"
2000,protocol/src/main/protobuf/BitData.proto,"@@ -50,3 +52,7 @@ message RuntimeFilterBDef{
   optional int32 hj_op_id = 7; // the operator id of the HashJoin which generates this RuntimeFilter
   optional int64 rf_identifier = 8; // the runtime filter identifier
 }
+
+message AckWithCredit{","[{'comment': 'Although it is not a good design, Drill clients use the same RPC protocol as Drillbits. We recently with Drill 1.17 had an issue where we changed a protobuf in a way that broke the C++ ODBC driver.\r\n\r\nAt present the project is thinly staffed; we lack C++ expertise and it may take a while to get the ODBC driver updated. Further, as Drill is rolled out in organizations, we cannot expect clients to update in sync with each server release. Also, we may have people using a single client to speak to Drillbits of different versions.\r\n\r\nAll of is a preface to asking: is this a safe change? Is it backward compatible? If not, is there a way to use optional fields in an existing message to accomplish the same thing without breaking backward compatibility?\r\n\r\nI understand hacking the current protocol is not elegant. But, is it possible to ensure compatibility?', 'commenter': 'paul-rogers'}, {'comment': ""The protocol here changed at the DataTunnel between the execution nodes. The client uses the UserChannel to talk with the execution cluster(see the ScreenCreator's userConnection). So the client still uses the ACK to talk with the cluster not the 'AckWithCredit'  and it's backward compatibility.\r\n\r\nTo the execution cluster nodes inner, it's not possible to keep compatibility by using different ack protocol. So the cluster manager should upgrade their execution nodes at the same time.\r\n\r\n"", 'commenter': 'weijietong'}, {'comment': ""Thanks for the explanation. As it turns out, the only consumer of the C++ code is the ODBC client, which is why I asked when I saw the C++ files changed.\r\n\r\nWhile forcing an all-in-one cluster upgrade is not good, it should be OK. Although the Drill Web console suggests that one can do a -1/+1 version upgrade, I don't think such an upgrade is ever tested. Let's just make sure we document the all-at-once upgrade requirement in the release notes of the next release."", 'commenter': 'paul-rogers'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -32,12 +32,22 @@
   private final int softlimit;
   private final int startlimit;
 
+  private int runtimeSoftLimit = -1;
+  private int runtimeAckCredit = 10;
+  private int sampleTimes = 0;
+  private long totalBatchSize = 0l;","[{'comment': 'Nit: Octal 1? Maybe just plain `1` will be clearer.', 'commenter': 'paul-rogers'}, {'comment': 'just a meaningless initial value , 1 will be ok.', 'commenter': 'weijietong'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -32,12 +32,22 @@
   private final int softlimit;
   private final int startlimit;
 
+  private int runtimeSoftLimit = -1;","[{'comment': 'These would greatly benefit from an explanation: either per-variable, or as a class comment to explain the algorithm.\r\n\r\nAs I read the code, the idea is:\r\n\r\n* Something sets a memory limit per receiver.\r\n* The goal is to fill that memory up to 40% by adjusting the number of batches buffered on the receiver side, which is the same as the number of batches the sender can send before receiving an ACK.\r\n* After seeing the first `sampleTimes` batches, use the average of those batches to determine how many batches can fit in the available receiver memory.\r\n\r\nIs this roughly right?', 'commenter': 'paul-rogers'}, {'comment': 'yes ', 'commenter': 'weijietong'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -90,14 +100,39 @@ public boolean isEmpty() {
 
     @Override
     public void add(RawFragmentBatch batch) {
+      int recordCount = batch.getHeader().getDef().getRecordCount();
+      long bathByteSize = batch.getByteCount();","[{'comment': '`bathByteSize` --> `batchByteSize` (typo)', 'commenter': 'paul-rogers'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -90,14 +100,39 @@ public boolean isEmpty() {
 
     @Override
     public void add(RawFragmentBatch batch) {
+      int recordCount = batch.getHeader().getDef().getRecordCount();
+      long bathByteSize = batch.getByteCount();
+      if (recordCount != 0) {
+        //skip first header batch
+        totalBatchSize += bathByteSize;
+        sampleTimes++;
+      }
+      if (sampleTimes == maxSampleTimes) {
+        long averageBathSize = totalBatchSize / sampleTimes;
+        //make a decision
+        long limit = context.getAllocator().getLimit();","[{'comment': ""Unless this has been adjusted, the limit is generally meaningless; there is no code that sets this to any reasonable value (unless something has changed recently.) It generally defaults to 10 GB, which is far too large.\r\n\r\nIf we use the default, will each receiver try to use the 10 GB of memory? More than 1 or 2 queries will cause OOM on many installations.\r\n\r\nTo provide a bit more background. Unless things changed recently, if the query queue is off, there is no memory budget per query. But, if queuing is on, there is code that allocates memory to the buffering operators (sort, join, etc.) but not (last I looked) to exchanges. (It clearly should do such an allocation.) So, if we use the full 10GB (or whatever value) here, the query may far exceed the budget and the queuing mechanism will still cause the system to exhaust memory before we hit the query limit.\r\n\r\nIn short, this memory needs to be allocated somehow as part of memory planning. I don't see any code in this PR which will do so."", 'commenter': 'paul-rogers'}, {'comment': ""Another issue is the question of how many of these receivers exist per Drillbit. I don't know the answer. If I have 5 minor fragments on this Drillbit, will all 5 have their own flow control calcs? Will I have 5 fragments each trying to use 50% of 10GB for a total of 20GB of buffering? Will this be a problem?\r\n\r\nAlso, how can this algorithm go wrong? Suppose I have a set of files organized by time. I do a time range query. The first few batches might have very few rows because the filter is picking up just a few early arrivals. We see three batches, say, where the filter had low selectivity, of a few dozen rows, then decide we can hold many batches.\r\n\r\nLater, the scan hits the bulk of my time ranges and the batches have far fewer rows filtered out. Suddenly, we need far more memory for these low-selectivity batches.\r\n\r\nDo we need a safety valve that says that we will back off if we suddenly see large batches?"", 'commenter': 'paul-rogers'}, {'comment': ""A good valuable question!  thanks, but still a tough question. It is a resource management problem.  Firstly, I did not know the memory limit using a default 10 gb value until you mentioned it. Thanks for pointing out this. \r\n\r\nTo drill's random statistic based memory allocation mechanism, it could not manage one drillbit's memory allocation accurately. Whatever we do ,we will risk OOM.  Think about what other system do, they will account for the total memory used by one node, if the node has not enough memory to satisfy the query, the resource manager will not assign a query to that node. After a node has accepted a query, they normally specify the network memory space and the execution memory space size separately.  So the system would be memory resource safety. \r\n\r\nTo solve the memory risk not only to this PR but also to current implementation, maybe we need a centralized resource manager not current peer node sketchy resource management model.\r\n\r\nWe could specify a configure item to set the max network memory usage to an appropriate value (according to our cluster hardware memory size )to a fragment node. But we could not control the parallel receiver nodes happened at one drillbit. \r\n\r\nTo your last question, the sender's current implementation will hold a calculated number rows to flush out a batch. So the row number of batch would not be a problem.  But to some situation, like the row has a text column, it's possible that the text size will not be uniform at different batches, it's possible to OOM. \r\n\r\nMaybe we could check for OOM .If that happened, we could notify the sender to change to a half credit value ? \r\n\r\n"", 'commenter': 'weijietong'}, {'comment': 'Or rollback to the initial 3 credit.', 'commenter': 'weijietong'}, {'comment': ""One more answer to this question, to current SpoolingRawBatchBuffer, though it could spill to disk, but when to spill is still not memory safe. The reason is still we could not control the parallel receiver nodes one drillbit.  \r\n\r\nI also find a bug to its implementation. It calculates the memory usage of one batch by calling the `RawFragmentBatchWrapper.getBodySize()` .  That method would always return 0. As the Drillbuf's writerIndex and readerIndex are all zero at that time.\r\n\r\n\r\n"", 'commenter': 'weijietong'}, {'comment': 'As it turns out, it is VERY hard to compute the memory used for a batch. The ""batch sizer"" classes use a mechanism that dives below the level of vectors into the ledgers. See `ValueVector.void collectLedgers(Set<BufferLedger> ledgers)`. This turned out to be the only solution because deserialized data shares buffers, as I recall, and counting buffer sizes gives the wrong answer. (It\'s been a couple of years since we we added that code, so I can\'t remember the details.)\r\n\r\nThere are two better solutions. One would be to carry the data size along with each batch. But, even that has the problems described in my earlier note.\r\n\r\nThe best solution would be to have an RM solution set the target batch size. Each operator would work to produce batches of that size. (The scan can do so using the Result Set Loader. Other operators can use the ""batch sizer"" method.) That way memory calcs are easier not just for this feature, but also for sorts and other buffering operators.\r\n\r\nSince adding a target batch size is beyond the scope of this PR, I\'d suggest adding some config options instead:\r\n\r\n* Enable/disable this feature\r\n* Target memory limit for buffering\r\n\r\nThat way, until we can control batch sizes, we can at least adjust the buffering here to deal with different workloads.\r\n\r\nIt is probably fine to make these config (boot) options rather than system options.\r\n\r\nFinally, my (limited) understanding is that spooling for the receiver exists, but has never been used. First, it is very complex. Second, it could actually slow query execution because of the need to write to, then read from disk. Third, we\'d need a way to manage maximum disk space used for spooling.\r\n\r\nSo, given that the code has never been used (I believe), it is not surprising that it contains bugs. Thanks for finding them.\r\n\r\nLast I checked (my knowledge may be old), only sort, join and hash agg can spill to disk well.', 'commenter': 'paul-rogers'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -90,14 +100,39 @@ public boolean isEmpty() {
 
     @Override
     public void add(RawFragmentBatch batch) {
+      int recordCount = batch.getHeader().getDef().getRecordCount();
+      long bathByteSize = batch.getByteCount();
+      if (recordCount != 0) {
+        //skip first header batch
+        totalBatchSize += bathByteSize;
+        sampleTimes++;
+      }
+      if (sampleTimes == maxSampleTimes) {
+        long averageBathSize = totalBatchSize / sampleTimes;
+        //make a decision
+        long limit = context.getAllocator().getLimit();
+        long thresholdNetworkMem = (long) (limit * 0.40);","[{'comment': 'This code should handle an undersize limit. If the limit can be set, someone will set it to smaller than a single batch. (I had to deal with this issue when fixing the external sort several years ago.)\r\n\r\nSo, the math needs to handle the case where the first batches are already close to the limit.\r\n\r\nIn particular, the ""available memory"" seems to be assumed at 40% of the limit, or 4 GB for a default limit. The average batch size could, in theory, be larger, leaving a `runtimeSoftLimit` of 0. So, seems it should be clamped at 1.', 'commenter': 'paul-rogers'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -90,14 +100,39 @@ public boolean isEmpty() {
 
     @Override
     public void add(RawFragmentBatch batch) {
+      int recordCount = batch.getHeader().getDef().getRecordCount();
+      long bathByteSize = batch.getByteCount();
+      if (recordCount != 0) {
+        //skip first header batch
+        totalBatchSize += bathByteSize;
+        sampleTimes++;
+      }
+      if (sampleTimes == maxSampleTimes) {
+        long averageBathSize = totalBatchSize / sampleTimes;
+        //make a decision
+        long limit = context.getAllocator().getLimit();
+        long thresholdNetworkMem = (long) (limit * 0.40);
+        if (averageBathSize > 0) {
+          runtimeSoftLimit = (int) (thresholdNetworkMem / averageBathSize);","[{'comment': '`averageBatchSize` (typo)', 'commenter': 'paul-rogers'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -90,14 +100,39 @@ public boolean isEmpty() {
 
     @Override
     public void add(RawFragmentBatch batch) {
+      int recordCount = batch.getHeader().getDef().getRecordCount();
+      long bathByteSize = batch.getByteCount();
+      if (recordCount != 0) {
+        //skip first header batch
+        totalBatchSize += bathByteSize;
+        sampleTimes++;
+      }
+      if (sampleTimes == maxSampleTimes) {
+        long averageBathSize = totalBatchSize / sampleTimes;
+        //make a decision
+        long limit = context.getAllocator().getLimit();
+        long thresholdNetworkMem = (long) (limit * 0.40);
+        if (averageBathSize > 0) {
+          runtimeSoftLimit = (int) (thresholdNetworkMem / averageBathSize);
+          runtimeAckCredit = runtimeSoftLimit / fragmentCount;
+          runtimeAckCredit = runtimeAckCredit > 0 ? runtimeAckCredit : 1;","[{'comment': '`runtimeAckCredit = Math.min(1, runtimeAckCredit);`', 'commenter': 'paul-rogers'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -90,14 +100,39 @@ public boolean isEmpty() {
 
     @Override
     public void add(RawFragmentBatch batch) {
+      int recordCount = batch.getHeader().getDef().getRecordCount();
+      long bathByteSize = batch.getByteCount();
+      if (recordCount != 0) {
+        //skip first header batch
+        totalBatchSize += bathByteSize;
+        sampleTimes++;
+      }
+      if (sampleTimes == maxSampleTimes) {","[{'comment': 'Suggestion: this method is about adding (buffering) a batch. Perhaps move the credit calculation to new method to make things a bit simpler.', 'commenter': 'paul-rogers'}, {'comment': 'agree', 'commenter': 'weijietong'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -90,14 +100,39 @@ public boolean isEmpty() {
 
     @Override
     public void add(RawFragmentBatch batch) {
+      int recordCount = batch.getHeader().getDef().getRecordCount();
+      long bathByteSize = batch.getByteCount();
+      if (recordCount != 0) {
+        //skip first header batch
+        totalBatchSize += bathByteSize;
+        sampleTimes++;
+      }
+      if (sampleTimes == maxSampleTimes) {
+        long averageBathSize = totalBatchSize / sampleTimes;
+        //make a decision
+        long limit = context.getAllocator().getLimit();
+        long thresholdNetworkMem = (long) (limit * 0.40);
+        if (averageBathSize > 0) {
+          runtimeSoftLimit = (int) (thresholdNetworkMem / averageBathSize);
+          runtimeAckCredit = runtimeSoftLimit / fragmentCount;
+          runtimeAckCredit = runtimeAckCredit > 0 ? runtimeAckCredit : 1;
+        }
+      }
+      if (runtimeSoftLimit > 0) {","[{'comment': 'As discussed above, `runtimeSoftLimit` can be 0 even if we do the calcs under certain conditions. Maybe check `runtimeAckCredit` instead, which we have forced to be non-zero?', 'commenter': 'paul-rogers'}, {'comment': 'If we get a 0 runtimeSoftLimit, we do nothing just to keep the original static credit flow control. ', 'commenter': 'weijietong'}]"
2000,exec/java-exec/src/main/resources/drill-module.conf,"@@ -248,6 +248,9 @@ drill.exec: {
     spooling: {
       delete: true,
       size: 100000000
+    },
+    unlimited: {
+      size: 500000000","[{'comment': 'The name seems confusing. ""Unlmited"" means (except in American cell phone plans) that there is no limit. Yet, we see a limit with the `size`. Is this the `receive_buffer_size`? ', 'commenter': 'paul-rogers'}]"
2000,exec/java-exec/src/main/java/org/apache/drill/exec/work/batch/UnlimitedRawBatchBuffer.java,"@@ -32,12 +33,24 @@
   private final int softlimit;
   private final int startlimit;
 
-  public UnlimitedRawBatchBuffer(FragmentContext context, int fragmentCount) {
-    super(context, fragmentCount);
+  private int runtimeSoftLimit = -1;
+  private int runtimeAckCredit = 1;
+  private int sampleTimes = 0;
+  private long totalBatchSize = 0l;
+  private final int fragmentCount;
+  private final int maxSampleTimes;
+  private final long thresholdNetworkMem;
+
+  public UnlimitedRawBatchBuffer(FragmentContext context, int fragmentCount, boolean enableDynamicFC) {
+    super(context, fragmentCount, enableDynamicFC);
     this.softlimit = bufferSizePerSocket * fragmentCount;
     this.startlimit = Math.max(softlimit/2, 1);
     logger.trace(""softLimit: {}, startLimit: {}"", softlimit, startlimit);
     this.bufferQueue = new UnlimitedBufferQueue();
+    this.fragmentCount = fragmentCount;
+    this.sampleTimes = fragmentCount;
+    this.maxSampleTimes = fragmentCount;
+    this.thresholdNetworkMem = context.getConfig().getLong(ExecConstants.UNLIMITED__BUFFER_MAX_MEMORY_SIZE);","[{'comment': 'I\'m concerned that this feature is always enabled with a default size of 500MB. If we assume that most batches are usually < 10MB, we\'ve gone from a potential buffer size of 30MB to one of 500MB. In large queries, this could cause unexpected OOM errors.\r\n\r\nMaybe define this so that 0 is the default, and 0 means ""don\'t dynamically change buffer size."" This allows us to try out the feature, and for you to use the feature, without running the risk of destabilizing other production sites.', 'commenter': 'paul-rogers'}, {'comment': 'Since we already have a configure item to control this feature. Why we still need to set the limited buffer size to 0?', 'commenter': 'weijietong'}]"
2030,_docs/connect-a-data-source/plugins/080-rdbms-storage-plugin.md,"@@ -9,14 +9,25 @@ As with any source, Drill supports joins within and between all systems. Drill a
 
 ## Using the RDBMS Storage Plugin
 
-Drill is designed to work with any relational datastore that provides a JDBC driver. Drill is actively tested with Postgres, MySQL, Oracle, MSSQL and Apache Derby. For each system, you will follow three basic steps for setup:
+Drill is designed to work with any relational datastore that provides a JDBC driver. Drill is actively tested with
+ Postgres, MySQL, Oracle, MSSQL, Apache Derby and H2. For each system, you will follow three basic steps for setup:
 
   1. [Install Drill]({{ site.baseurl }}/docs/installing-drill-in-embedded-mode), if you do not already have it installed.
   2. Copy your database's JDBC driver into the jars/3rdparty directory. (You'll need to do this on every node.)  
   3. Restart Drill. See [Starting Drill in Distributed Mode]({{site.baseurl}}/docs/starting-drill-in-distributed-mode/).
-  4. Add a new storage configuration to Drill through the Web UI. Example configurations for [Oracle](#Example-Oracle-Configuration), [SQL Server](#Example-SQL-Server-Configuration), [MySQL](#Example-MySQL-Configuration) and [Postgres](#Example-Postgres-Configuration) are provided below.
-  
-**Example: Working with MySQL**
+  4. Add a new storage configuration to Drill through the Web UI. Example configurations for [Oracle](#example-oracle-configuration), [SQL Server](#example-sql-server-configuration), [MySQL](#example-mysql-configuration) and [Postgres](#example-postgres-configuration) are provided below.
+
+## Setting data source parameters in the storage plugin configuration
+
+Starting from Drill 1.18.0, new JDBC storage plugin configuration property `sourceParameters` was introduced to allow
+ setting data source parameters described in [HikariCP](https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby).
+ Parameters names with incorrect naming will be ignored during plugin configuration,","[{'comment': 'This is not true anymore, plugin setup will fail.\r\nhttps://github.com/apache/drill/blob/master/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestDataSource.java#L95', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks for pointing to this, updated sentence to reflect current behavior.', 'commenter': 'vvysotskyi'}]"
2030,_docs/connect-a-data-source/plugins/080-rdbms-storage-plugin.md,"@@ -142,4 +153,20 @@ You may need to qualify a table name with a schema name for Drill to return data
        | 2 	| 1.2.3.5  |
        +-------+----------+
 
+### Example of Postgres Configuration with `sourceParameters` configuration property
 
+    {
+      type: ""jdbc"",
+      enabled: true,
+      driver: ""org.postgresql.Driver"",
+      url:""jdbc:postgresql://1.2.3.4/mydatabase?defaultRowFetchSize=2"",
+      username:""user"",
+      password:""password"",
+      sourceParameters: {
+        minimumIdle : 5,
+        autoCommit: false,
+        connectionTestQuery: ""select version() as postgresql_version"",","[{'comment': 'Missing quotes.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks, fixed.', 'commenter': 'vvysotskyi'}]"
2030,_docs/performance-tuning/drill-metastore/010-using-drill-metastore.md,"@@ -1,14 +1,16 @@
 ---
 title: ""Using Drill Metastore""
 parent: ""Drill Metastore""
-date: 2020-03-03
+date: 2020-03-17
 ---
 
 Drill 1.17 introduces the Drill Metastore which stores the table schema and table statistics. Statistics allow Drill to better create optimal query plans.
 
 The Metastore is a Beta feature; it is subject to change. We encourage you to try it and provide feedback.
 Because the Metastore is in Beta, the SQL commands and Metastore formats may change in the next release.
-{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.
+Starting from Drill 1.18, this feature is supported for all **format** plugins.","[{'comment': 'Well, not all, for example MaprDB format does not support it.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks, added ""except for MaprDB"".', 'commenter': 'vvysotskyi'}]"
2030,_docs/performance-tuning/drill-metastore/010-using-drill-metastore.md,"@@ -117,6 +119,13 @@ Drill can connect to any number of data sources, each of which may have its own
 As a result, the Metastore labels tables with a combination of (plugin configuration name, workspace name, table name).
 Note that if before renaming any of these items, you must delete table's Metadata entry and recreate it after renaming.
 
+### Using schema provisioning feature with Drill Metastore
+
+Drill Metastore cooperates with [Schema provisioning]({{site.baseurl}}/docs/create-or-replace-schema/#usage-notes) feature.","[{'comment': 'I think instead of cooperates, it better to expand: Drill Metastore allows to specify schema using the same syntax as Schema provisioning feature when used as table function (add link to the docs).', 'commenter': 'arina-ielchiieva'}, {'comment': 'Agree, it looks more clear. Thanks, fixed.', 'commenter': 'vvysotskyi'}]"
2030,_docs/performance-tuning/drill-metastore/010-using-drill-metastore.md,"@@ -250,6 +259,9 @@ A table can be divided into directories, called ""partitions"". The `PARTITIONS` t
  - Applies to tables stored as Parquet files and only when stored in the `DFS` storage plugin.
  - Disabled by default. You must enable this feature through the `metastore.enabled` system/session option.
 
+### Limitations of the 1.18 release
+ - Applies to all storage formats of file system storage plugin tables only.","[{'comment': 'Not all.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks, fixed it.', 'commenter': 'vvysotskyi'}]"
2030,_docs/connect-a-data-source/035-plugin-configuration-basics.md,"@@ -147,6 +149,45 @@ fieldDelimiter => ',', extractHeader => true))``
 
 For more information about format plugin configuration see [""Text Files: CSV, TSV, PSV""]({{site.baseurl}}{{site.baseurl}}/docs/text-files-csv-tsv-psv/).  
 
+## Specifying the Schema as Table Function Parameter
+
+Starting from Drill 1.17, table schema may be indicated in the query using table function.
+
+It is useful when the user does not want to persist schema in table root location or when reading from file, not folder.
+Schema parameter can be used as an individual unit or in together with for format plugin table properties.","[{'comment': '```suggestion\r\nSchema parameter can be used as an individual unit or together with for format plugin table properties.\r\n```', 'commenter': 'arina-ielchiieva'}, {'comment': 'Thanks. fixed.', 'commenter': 'vvysotskyi'}]"
2030,_docs/connect-a-data-source/plugins/080-rdbms-storage-plugin.md,"@@ -21,8 +21,8 @@ Drill is designed to work with any relational datastore that provides a JDBC dri
 
 Starting from Drill 1.18.0, new JDBC storage plugin configuration property `sourceParameters` was introduced to allow
  setting data source parameters described in [HikariCP](https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby).
- Parameters names with incorrect naming will be ignored during plugin configuration,
- parameter value which are of incorrect data type or illegal will fail storage plugin to start up.
+ Parameters names with incorrect naming and parameter value which are of incorrect data type or illegal will fail","[{'comment': '```suggestion\r\n Parameters names with incorrect naming and parameter values which are of incorrect data type or illegal will fail\r\n```', 'commenter': 'arina-ielchiieva'}, {'comment': 'Fixed.', 'commenter': 'vvysotskyi'}]"
2030,_docs/connect-a-data-source/035-plugin-configuration-basics.md,"@@ -147,6 +149,45 @@ fieldDelimiter => ',', extractHeader => true))``
 
 For more information about format plugin configuration see [""Text Files: CSV, TSV, PSV""]({{site.baseurl}}{{site.baseurl}}/docs/text-files-csv-tsv-psv/).  
 
+## Specifying the Schema as Table Function Parameter
+
+Starting from Drill 1.17, table schema may be indicated in the query using table function.
+
+It is useful when the user does not want to persist schema in table root location or when reading from file, not folder.
+Schema parameter can be used as an individual unit or together with for format plugin table properties.","[{'comment': '```suggestion\r\nSchema parameter can be used as an individual unit or together with format plugin table properties.\r\n```', 'commenter': 'arina-ielchiieva'}]"
2030,_docs/sql-reference/sql-commands/021-create-schema.md,"@@ -4,9 +4,9 @@ date: 2019-05-31
 parent: ""SQL Commands""
 ---
 
-Starting in Drill 1.16, you can define a schema for text files using the CREATE OR REPLACE SCHEMA command. Schema is only available for tables represented by a directory. To use this feature with a single file, put the file inside a directory, and use the directory name to query the table.
+Starting in Drill 1.16, you can define a schema for text files using the `CREATE OR REPLACE SCHEMA` command. Such schema is only available for tables represented by a directory. To use this feature with a single file, put the file inside a directory, and use the directory name to query the table.","[{'comment': '```suggestion\r\nStarting in Drill 1.16, you can define a schema for text files using the `CREATE OR REPLACE SCHEMA` command. Such schema is only available for tables represented by a directory. To use this feature with a single file, put the file inside a directory, and use the directory name to query the table or use table function with schema parameter instead.\r\n```', 'commenter': 'arina-ielchiieva'}]"
2030,_docs/connect-a-data-source/035-plugin-configuration-basics.md,"@@ -147,6 +149,45 @@ fieldDelimiter => ',', extractHeader => true))``
 
 For more information about format plugin configuration see [""Text Files: CSV, TSV, PSV""]({{site.baseurl}}{{site.baseurl}}/docs/text-files-csv-tsv-psv/).  
 
+## Specifying the Schema as Table Function Parameter
+
+Starting from Drill 1.17, table schema may be indicated in the query using table function.
+
+It is useful when the user does not want to persist schema in table root location or when reading from file, not folder.
+Schema parameter can be used as an individual unit or together with format plugin table properties.","[{'comment': '(Combine four paragraphs.) Table schemas normally reside in the root folder of each table. You an also specify a schema for an individual query using a table function and specifying the `SCHEMA` property. You can combine the schema with format plugin properties. The syntax is similar...', 'commenter': 'paul-rogers'}]"
2030,_docs/connect-a-data-source/035-plugin-configuration-basics.md,"@@ -147,6 +149,45 @@ fieldDelimiter => ',', extractHeader => true))``
 
 For more information about format plugin configuration see [""Text Files: CSV, TSV, PSV""]({{site.baseurl}}{{site.baseurl}}/docs/text-files-csv-tsv-psv/).  
 
+## Specifying the Schema as Table Function Parameter
+
+Starting from Drill 1.17, table schema may be indicated in the query using table function.
+
+It is useful when the user does not want to persist schema in table root location or when reading from file, not folder.
+Schema parameter can be used as an individual unit or together with format plugin table properties.
+
+Schema can be provided in the `SCHEMA` property inline or using the file.
+
+The syntax for inline schema is similar to the [CREATE OR REPLACE SCHEMA]({{site.baseurl}}/docs/create-or-replace-schema/#syntax):
+
+```
+SELECT a, b FROM TABLE (table_name(
+SCHEMA => 'inline=(column_name data_type [nullability] [format] [default] [properties {prop='val', ...})]'))
+```
+
+Example of usage:
+
+```
+select * from table(dfs.tmp.`text_table`(
+schema => 'inline=(col1 date properties {`drill.format` = `yyyy-MM-dd`}) 
+properties {`drill.strict` = `false`}'))
+```
+
+The syntax for indicating schema using the path:","[{'comment': 'Alternatively, can also specify the path to a schema file. For example:', 'commenter': 'paul-rogers'}]"
2030,_docs/connect-a-data-source/035-plugin-configuration-basics.md,"@@ -147,6 +149,45 @@ fieldDelimiter => ',', extractHeader => true))``
 
 For more information about format plugin configuration see [""Text Files: CSV, TSV, PSV""]({{site.baseurl}}{{site.baseurl}}/docs/text-files-csv-tsv-psv/).  
 
+## Specifying the Schema as Table Function Parameter
+
+Starting from Drill 1.17, table schema may be indicated in the query using table function.
+
+It is useful when the user does not want to persist schema in table root location or when reading from file, not folder.
+Schema parameter can be used as an individual unit or together with format plugin table properties.
+
+Schema can be provided in the `SCHEMA` property inline or using the file.
+
+The syntax for inline schema is similar to the [CREATE OR REPLACE SCHEMA]({{site.baseurl}}/docs/create-or-replace-schema/#syntax):
+
+```
+SELECT a, b FROM TABLE (table_name(
+SCHEMA => 'inline=(column_name data_type [nullability] [format] [default] [properties {prop='val', ...})]'))
+```
+
+Example of usage:","[{'comment': 'You can specify the schema inline within the query. For example:', 'commenter': 'paul-rogers'}]"
2030,_docs/connect-a-data-source/035-plugin-configuration-basics.md,"@@ -147,6 +149,45 @@ fieldDelimiter => ',', extractHeader => true))``
 
 For more information about format plugin configuration see [""Text Files: CSV, TSV, PSV""]({{site.baseurl}}{{site.baseurl}}/docs/text-files-csv-tsv-psv/).  
 
+## Specifying the Schema as Table Function Parameter
+
+Starting from Drill 1.17, table schema may be indicated in the query using table function.
+
+It is useful when the user does not want to persist schema in table root location or when reading from file, not folder.
+Schema parameter can be used as an individual unit or together with format plugin table properties.
+
+Schema can be provided in the `SCHEMA` property inline or using the file.
+
+The syntax for inline schema is similar to the [CREATE OR REPLACE SCHEMA]({{site.baseurl}}/docs/create-or-replace-schema/#syntax):
+
+```
+SELECT a, b FROM TABLE (table_name(
+SCHEMA => 'inline=(column_name data_type [nullability] [format] [default] [properties {prop='val', ...})]'))
+```
+
+Example of usage:
+
+```
+select * from table(dfs.tmp.`text_table`(
+schema => 'inline=(col1 date properties {`drill.format` = `yyyy-MM-dd`}) 
+properties {`drill.strict` = `false`}'))
+```
+
+The syntax for indicating schema using the path:
+
+```
+select * from table(dfs.tmp.`text_table`(schema => 'path=`/tmp/my_schema`'))
+```
+
+The following example demonstrates applying provided schema alongside with format plugin table function parameters.
+Assuming that the user has CSV file with headers with extension that does not comply to a default text file with headers extension (ex: `cars.csvh-test`):","[{'comment': 'Suppose that you have a CSV file with headers and with a custom extension: `csvh-test`. You can combine the schema with format plugin properties:', 'commenter': 'paul-rogers'}]"
2030,_docs/connect-a-data-source/plugins/080-rdbms-storage-plugin.md,"@@ -9,14 +9,25 @@ As with any source, Drill supports joins within and between all systems. Drill a
 
 ## Using the RDBMS Storage Plugin
 
-Drill is designed to work with any relational datastore that provides a JDBC driver. Drill is actively tested with Postgres, MySQL, Oracle, MSSQL and Apache Derby. For each system, you will follow three basic steps for setup:
+Drill is designed to work with any relational datastore that provides a JDBC driver. Drill is actively tested with
+ Postgres, MySQL, Oracle, MSSQL, Apache Derby and H2. For each system, you will follow three basic steps for setup:
 
   1. [Install Drill]({{ site.baseurl }}/docs/installing-drill-in-embedded-mode), if you do not already have it installed.
   2. Copy your database's JDBC driver into the jars/3rdparty directory. (You'll need to do this on every node.)  
   3. Restart Drill. See [Starting Drill in Distributed Mode]({{site.baseurl}}/docs/starting-drill-in-distributed-mode/).
-  4. Add a new storage configuration to Drill through the Web UI. Example configurations for [Oracle](#Example-Oracle-Configuration), [SQL Server](#Example-SQL-Server-Configuration), [MySQL](#Example-MySQL-Configuration) and [Postgres](#Example-Postgres-Configuration) are provided below.
-  
-**Example: Working with MySQL**
+  4. Add a new storage configuration to Drill through the Web UI. Example configurations for [Oracle](#example-oracle-configuration), [SQL Server](#example-sql-server-configuration), [MySQL](#example-mysql-configuration) and [Postgres](#example-postgres-configuration) are provided below.
+
+## Setting data source parameters in the storage plugin configuration
+
+Starting from Drill 1.18.0, new JDBC storage plugin configuration property `sourceParameters` was introduced to allow","[{'comment': 'Drill\'s JDBC storage plugin configuration allows you to specify database parameters as JSON key/value pairs. Drill 1.18 introduced a new JDBC storage plugin property called `sourceParameters` to handle query parameter names which are not valid JSON identifiers. See  [HikariCP](https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby) for details. See [Example of Postgres Configuration with `sourceParameters` configuration property](#example-of-postgres-configuration-with-sourceparameters-configuration-property)\r\nfor an example.\r\n\r\n(Note: please specify which parameters we\'re talking about. I made up the ""database parameter"" part; please replace with an accurate description.)', 'commenter': 'paul-rogers'}]"
2030,_docs/connect-a-data-source/plugins/080-rdbms-storage-plugin.md,"@@ -101,9 +112,9 @@ For MySQL, Drill has been tested with MySQL's [mysql-connector-java-5.1.37-bin.j
       password:""password""
     }  
 
-**Example Postgres Configuration**
+### Example Postgres Configuration
 
-For Postgres, Drill has been tested with Postgres's [9.1-901-1.jdbc4](http://central.maven.org/maven2/org/postgresql/postgresql/) driver (any recent driver should work). Copy this driver file to all nodes.
+For Postgres, Drill has been tested with Postgres's [42.2.11](https://mvnrepository.com/artifact/org.postgresql/postgresql) driver (any recent driver should work). Copy this driver file to all nodes.","[{'comment': 'Drill is tested with the Postgres driver version ... Copy this driver jar (?) to the (which?) folder on all nodes.', 'commenter': 'paul-rogers'}]"
2030,_docs/connect-a-data-source/plugins/080-rdbms-storage-plugin.md,"@@ -142,4 +153,20 @@ You may need to qualify a table name with a schema name for Drill to return data
        | 2 	| 1.2.3.5  |
        +-------+----------+
 
+### Example of Postgres Configuration with `sourceParameters` configuration property
 
+    {
+      type: ""jdbc"",
+      enabled: true,
+      driver: ""org.postgresql.Driver"",
+      url:""jdbc:postgresql://1.2.3.4/mydatabase?defaultRowFetchSize=2"",
+      username:""user"",
+      password:""password"",","[{'comment': '(Nit: for display formatting, please include a space after the colon.)', 'commenter': 'paul-rogers'}]"
2030,_docs/connect-a-data-source/plugins/114-image-metadata-format-plugin.md,"@@ -49,54 +49,53 @@ fileSystemMetadata|true|Set to true to extract filesystem metadata including the
 descriptive|true|Set to true to extract metadata in a human-readable string format. Set false to extract metadata in a machine-readable typed format.
 timeZone|null|Specify the time zone to interpret the timestamp with no time zone information. If the timestamp includes the time zone information, this value is ignored. If null is set, the local time zone is used.  
 
-##Examples  
+## Examples  
+
+Download the following image and place it to the `/tmp` folder to follow the examples.","[{'comment': 'To follow along with the examples, start by downloading the following image to your `\\tmp` directory.\r\n\r\n(The documentation seems more friendly and approachable if we address the user directly and avoid the passive voice.)', 'commenter': 'paul-rogers'}]"
2030,_docs/connect-a-data-source/plugins/114-image-metadata-format-plugin.md,"@@ -49,54 +49,53 @@ fileSystemMetadata|true|Set to true to extract filesystem metadata including the
 descriptive|true|Set to true to extract metadata in a human-readable string format. Set false to extract metadata in a machine-readable typed format.
 timeZone|null|Specify the time zone to interpret the timestamp with no time zone information. If the timestamp includes the time zone information, this value is ignored. If null is set, the local time zone is used.  
 
-##Examples  
+## Examples  
+
+Download the following image and place it to the `/tmp` folder to follow the examples.
+
+[![image]({{ site.baseurl }}/images/7671b34d6e8a4d050f75278f10f1a08.jpg)]({{ site.baseurl }}/images/7671b34d6e8a4d050f75278f10f1a08.jpg)
 
 A Drill query on a JPEG file with the property descriptive: true
 
-       0: jdbc:drill:zk=local> select FileName, * from dfs.`4349313028_f69ffa0257_o.jpg`;  
-       +----------+----------+--------------+--------+------------+-------------+--------------+----------+-----------+------------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+------+------+----------+------------+------------------+-----+---------------+-----------+------+---------+----------+
-       | FileName | FileSize | FileDateTime | Format | PixelWidth | PixelHeight | BitsPerPixel | DPIWidth | DPIHeight | Orientaion | ColorMode | HasAlpha | Duration | VideoCodec | FrameRate | AudioCodec | AudioSampleSize | AudioSampleRate | JPEG | JFIF | ExifIFD0 | ExifSubIFD | Interoperability | GPS | ExifThumbnail | Photoshop | IPTC | Huffman | FileType |
-       +----------+----------+--------------+--------+------------+-------------+--------------+----------+-----------+------------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+------+------+----------+------------+------------------+-----+---------------+-----------+------+---------+----------+
-       | 4349313028_f69ffa0257_o.jpg | 257213 bytes | Fri Mar 09 12:09:34 +08:00 2018 | JPEG | 1199 | 800 | 24 | 96 | 96 | Unknown (0) | RGB | false | 00:00:00 | Unknown | 0 | Unknown | 0 | 0 | {""CompressionType"":""Baseline"",""DataPrecision"":""8 bits"",""ImageHeight"":""800 pixels"",""ImageWidth"":""1199 pixels"",""NumberOfComponents"":""3"",""Component1"":""Y component: Quantization table 0, Sampling factors 2 horiz/2 vert"",""Component2"":""Cb component: Quantization table 1, Sampling factors 1 horiz/1 vert"",""Component3"":""Cr component: Quantization table 1, Sampling factors 1 horiz/1 vert""} | {""Version"":""1.1"",""ResolutionUnits"":""inch"",""XResolution"":""96 dots"",""YResolution"":""96 dots"",""ThumbnailWidthPixels"":""0"",""ThumbnailHeightPixels"":""0""} | {""Software"":""Picasa 3.0""} | {""ExifVersion"":""2.10"",""UniqueImageID"":""d65e93b836d15a0c5e041e6b7258c76e""} | {""InteroperabilityIndex"":""Unknown (    )"",""InteroperabilityVersion"":""1.00""} | {""GPSVersionID"":"".022"",""GPSLatitudeRef"":""N"",""GPSLatitude"":""47° 32' 15.98\"""",""GPSLongitudeRef"":""W"",""GPSLongitude"":""-122° 2' 6.37\"""",""GPSAltitudeRef"":""Sea level"",""GPSAltitude"":""0 metres""} | {""Compression"":""JPEG (old-style)"",""XResolution"":""72 dots per inch"",""YResolution"":""72 dots per inch"",""ResolutionUnit"":""Inch"",""ThumbnailOffset"":""414 bytes"",""ThumbnailLength"":""7213 bytes""} | {} | {""Keywords"":""135;2002;issaquah;police car;wa;washington""} | {""NumberOfTables"":""4 Huffman tables""} | {""DetectedFileTypeName"":""JPEG"",""DetectedFileTypeLongName"":""Joint Photographic Experts Group"",""DetectedMIMEType"":""image/jpeg"",""ExpectedFileNameExtension"":""jpg""} |
-       +----------+----------+--------------+--------+------------+-------------+--------------+----------+-----------+------------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+------+------+----------+------------+------------------+-----+---------------+-----------+------+---------+----------+
- 
+        select FileName, * from dfs.tmp.`7671b34d6e8a4d050f75278f10f1a08.jpg`;
+        +-------------------------------------+-------------+---------------------------------+--------+------------+-------------+--------------+-------------+----------+-----------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+----------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------+----------------------------------------------------------------------------------+
+        |              FileName               |  FileSize   |          FileDateTime           | Format | PixelWidth | PixelHeight | BitsPerPixel | Orientaion  | DPIWidth | DPIHeight | ColorMode | HasAlpha | Duration | VideoCodec | FrameRate | AudioCodec | AudioSampleSize | AudioSampleRate |                                       JPEG                                       |                                 JpegComment                                  |                                       JFIF                                       |                                 ExifIFD0                                  |                                       GPS                                        |                Huffman                |                                     FileType                                     |
+        +-------------------------------------+-------------+---------------------------------+--------+------------+-------------+--------------+-------------+----------+-----------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+----------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------+----------------------------------------------------------------------------------+
+        | 7671b34d6e8a4d050f75278f10f1a08.jpg | 45877 bytes | Tue Mar 17 21:37:09 +02:00 2020 | JPEG   | 604        | 453         | 24           | Unknown (0) | 0        | 0         | RGB       | false    | 00:00:00 | Unknown    | 0         | Unknown    | 0               | 0               | {""CompressionType"":""Baseline"",""DataPrecision"":""8 bits"",""ImageHeight"":""453 pixels"",""ImageWidth"":""604 pixels"",""NumberOfComponents"":""3"",""Component1"":""Y component: Quantization table 0, Sampling factors 2 horiz/2 vert"",""Component2"":""Cb component: Quantization table 1, Sampling factors 1 horiz/1 vert"",""Component3"":""Cr component: Quantization table 1, Sampling factors 1 horiz/1 vert""} | {""JPEGComment"":""CREATOR: gd-jpeg v1.0 (using IJG JPEG v62), quality = 90\n""} | {""Version"":""1.1"",""ResolutionUnits"":""none"",""XResolution"":""1 dot"",""YResolution"":""1 dot"",""ThumbnailWidthPixels"":""0"",""ThumbnailHeightPixels"":""0""} | {""ResolutionUnit"":""(No unit)"",""YCbCrPositioning"":""Center of pixel array""} | {""GPSLatitudeRef"":""N"",""GPSLatitude"":""50° 27' 48.8\"""",""GPSLongitudeRef"":""E"",""GPSLongitude"":""30° 30' 31.21\""""} | {""NumberOfTables"":""4 Huffman tables""} | {""DetectedFileTypeName"":""JPEG"",""DetectedFileTypeLongName"":""Joint Photographic Experts Group"",""DetectedMIMEType"":""image/jpeg"",""ExpectedFileNameExtension"":""jpg""} |
+        +-------------------------------------+-------------+---------------------------------+--------+------------+-------------+--------------+-------------+----------+-----------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+----------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------+----------------------------------------------------------------------------------+
 
 A Drill query on a JPEG file with the property descriptive: false    
 
-       0: jdbc:drill:zk=local> select FileName, * from dfs.`4349313028_f69ffa0257_o.jpg`;  
-       +----------+----------+--------------+--------+------------+-------------+--------------+----------+-----------+------------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+------+------+----------+------------+------------------+-----+---------------+-----------+------+---------+----------+
-       | FileName | FileSize | FileDateTime | Format | PixelWidth | PixelHeight | BitsPerPixel | DPIWidth | DPIHeight | Orientaion | ColorMode | HasAlpha | Duration | VideoCodec | FrameRate | AudioCodec | AudioSampleSize | AudioSampleRate | JPEG | JFIF | ExifIFD0 | ExifSubIFD | Interoperability | GPS | ExifThumbnail | Photoshop | IPTC | Huffman | FileType |
-       +----------+----------+--------------+--------+------------+-------------+--------------+----------+-----------+------------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+------+------+----------+------------+------------------+-----+---------------+-----------+------+---------+----------+
-       | 4349313028_f69ffa0257_o.jpg | 257213 | 2018-03-09 04:09:34.0 | JPEG | 1199 | 800 | 24 | 96.0 | 96.0 | 0 | RGB | false | 0 | Unknown | 0.0 | Unknown | 0 | 0.0 | {""CompressionType"":0,""DataPrecision"":8,""ImageHeight"":800,""ImageWidth"":1199,""NumberOfComponents"":3,""Component1"":{""ComponentId"":1,""HorizontalSamplingFactor"":2,""VerticalSamplingFactor"":2,""QuantizationTableNumber"":0},""Component2"":{""ComponentId"":2,""HorizontalSamplingFactor"":1,""VerticalSamplingFactor"":1,""QuantizationTableNumber"":1},""Component3"":{""ComponentId"":3,""HorizontalSamplingFactor"":1,""VerticalSamplingFactor"":1,""QuantizationTableNumber"":1}} | {""Version"":257,""ResolutionUnits"":1,""XResolution"":96,""YResolution"":96,""ThumbnailWidthPixels"":0,""ThumbnailHeightPixels"":0} | {""Software"":""Picasa 3.0""} | {""ExifVersion"":""0210"",""UniqueImageID"":""d65e93b836d15a0c5e041e6b7258c76e""} | {""InteroperabilityIndex"":""    "",""InteroperabilityVersion"":""0100""} | {""GPSVersionID"":[0,0,2,2],""GPSLatitudeRef"":""N"",""GPSLatitude"":47.53777313232332,""GPSLongitudeRef"":""W"",""GPSLongitude"":-122.03510284423795,""GPSAltitudeRef"":0,""GPSAltitude"":0.0} | {""Compression"":6,""XResolution"":72.0,""YResolution"":72.0,""ResolutionUnit"":2,""ThumbnailOffset"":414,""ThumbnailLength"":7213} | {} | {""Keywords"":[""135"",""2002"",""issaquah"",""police car"",""wa"",""washington""]} | {""NumberOfTables"":4} | {""DetectedFileTypeName"":""JPEG"",""DetectedFileTypeLongName"":""Joint Photographic Experts Group"",""DetectedMIMEType"":""image/jpeg"",""ExpectedFileNameExtension"":""jpg""} |
-       +----------+----------+--------------+--------+------------+-------------+--------------+----------+-----------+------------+-----------+----------+----------+------------+-----------+------------+-----  
+        select FileName, * from dfs.tmp.`7671b34d6e8a4d050f75278f10f1a08.jpg`;
+        +-------------------------------------+----------+-----------------------+--------+------------+-------------+--------------+------------+----------+-----------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+----------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-------------------------------------------+----------------------------------------------------------------------------------+----------------------+----------------------------------------------------------------------------------+
+        |              FileName               | FileSize |     FileDateTime      | Format | PixelWidth | PixelHeight | BitsPerPixel | Orientaion | DPIWidth | DPIHeight | ColorMode | HasAlpha | Duration | VideoCodec | FrameRate | AudioCodec | AudioSampleSize | AudioSampleRate |                                       JPEG                                       |                                 JpegComment                                  |                                       JFIF                                       |                 ExifIFD0                  |                                       GPS                                        |       Huffman        |                                     FileType                                     |
+        +-------------------------------------+----------+-----------------------+--------+------------+-------------+--------------+------------+----------+-----------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+----------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-------------------------------------------+----------------------------------------------------------------------------------+----------------------+----------------------------------------------------------------------------------+
+        | 7671b34d6e8a4d050f75278f10f1a08.jpg | 45877    | 2020-03-17 19:37:09.0 | JPEG   | 604        | 453         | 24           | 0          | 0.0      | 0.0       | RGB       | false    | 0        | Unknown    | 0.0       | Unknown    | 0               | 0.0             | {""CompressionType"":0,""DataPrecision"":8,""ImageHeight"":453,""ImageWidth"":604,""NumberOfComponents"":3,""Component1"":{""ComponentId"":1,""HorizontalSamplingFactor"":2,""VerticalSamplingFactor"":2,""QuantizationTableNumber"":0},""Component2"":{""ComponentId"":2,""HorizontalSamplingFactor"":1,""VerticalSamplingFactor"":1,""QuantizationTableNumber"":1},""Component3"":{""ComponentId"":3,""HorizontalSamplingFactor"":1,""VerticalSamplingFactor"":1,""QuantizationTableNumber"":1}} | {""JPEGComment"":""CREATOR: gd-jpeg v1.0 (using IJG JPEG v62), quality = 90\n""} | {""Version"":257,""ResolutionUnits"":0,""XResolution"":1,""YResolution"":1,""ThumbnailWidthPixels"":0,""ThumbnailHeightPixels"":0} | {""ResolutionUnit"":1,""YCbCrPositioning"":1} | {""GPSLatitudeRef"":""N"",""GPSLatitude"":50.46355547157135,""GPSLongitudeRef"":""E"",""GPSLongitude"":30.508668422733077} | {""NumberOfTables"":4} | {""DetectedFileTypeName"":""JPEG"",""DetectedFileTypeLongName"":""Joint Photographic Experts Group"",""DetectedMIMEType"":""image/jpeg"",""ExpectedFileNameExtension"":""jpg""} |
+        +-------------------------------------+----------+-----------------------+--------+------------+-------------+--------------+------------+----------+-----------+-----------+----------+----------+------------+-----------+------------+-----------------+-----------------+----------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-------------------------------------------+----------------------------------------------------------------------------------+----------------------+----------------------------------------------------------------------------------+
 
 Retrieving GPS location data from the Exif metadata for the use of GIS functions.
 
-       0: jdbc:drill:zk=local> select t.GPS.GPSLatitude as lat, t.GPS.GPSLongitude as lon from dfs.`4349313028_f69ffa0257_o.jpg` t;
-       +--------------------+----------------------+
-       |        lat         |         lon          |
-       +--------------------+----------------------+
-       | 47.53777313232332  | -122.03510284423795  |
-       +--------------------+----------------------+  
-       
-Retrieving the images that are larger than 640 x 480 pixels.
-       
-       0: jdbc:drill:zk=local> select FileName, PixelWidth, PixelHeight from dfs.`/images/*.png` where PixelWidth >= 640 and PixelHeight >= 480;
-       +--------------------------+-------------+--------------+
-       |         FileName         | PixelWidth  | PixelHeight  |
-       +--------------------------+-------------+--------------+
-       | 1.png                    | 2788        | 1758         |
-       | 1500x500.png             | 1500        | 500          |
-       | 2.png                    | 2788        | 1758         |
-       | 9784873116914_1.png      | 874         | 1240         |
-       | Driven-Example-Load.png  | 1208        | 970          |
-       | features-diagram.png     | 1170        | 644          |
-       | hal1.png                 | 1223        | 772          |
-       | hal2.png                 | 1184        | 768          |
-       | image-3.png              | 1200        | 771          |
-       | image-4.png              | 1200        | 771          |
-       | image002.png             | 1689        | 695          |
-       +--------------------------+-------------+--------------+  
+        select t.GPS.GPSLatitude as lat, t.GPS.GPSLongitude as lon from dfs.tmp.`7671b34d6e8a4d050f75278f10f1a08.jpg` t;
+        +-------------------+--------------------+
+        |        lat        |        lon         |
+        +-------------------+--------------------+
+        | 50.46355547157135 | 30.508668422733077 |
+        +-------------------+--------------------+
+
+Download all `png` images from [Logos]({{ site.baseurl }}/images/logos/) page and place them to `/tmp/logos`
+ directory to examine the following example. 
+
+Retrieving the images that are less than 640 x 480 pixels.","[{'comment': 'Example query to retrieve the images... pixels:', 'commenter': 'paul-rogers'}]"
2030,_docs/performance-tuning/drill-metastore/010-using-drill-metastore.md,"@@ -1,14 +1,16 @@
 ---
 title: ""Using Drill Metastore""
 parent: ""Drill Metastore""
-date: 2020-03-03
+date: 2020-03-17
 ---
 
 Drill 1.17 introduces the Drill Metastore which stores the table schema and table statistics. Statistics allow Drill to better create optimal query plans.
 
 The Metastore is a Beta feature; it is subject to change. We encourage you to try it and provide feedback.
 Because the Metastore is in Beta, the SQL commands and Metastore formats may change in the next release.
-{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.{% include endnote.html %}
+{% include startnote.html %}In Drill 1.17, this feature is supported for Parquet tables only and is disabled by default.
+Starting from Drill 1.18, this feature is supported for all **format** plugins except for MaprDB.
+{% include endnote.html %}","[{'comment': 'The Metastore is a beta feature and is subject to change. In particular, the SQL commands and Metastore format may change based on your experience and feedback.\r\n* In Drill 1.17, Metastore supports only tables in Parquet format. The feature is disabled by default.\r\n* In Drill 1.18, Metastore supports all format plugins (except MaprDB) for the file system plugin. The feature is still disabled by default.', 'commenter': 'paul-rogers'}]"
2030,_docs/performance-tuning/drill-metastore/010-using-drill-metastore.md,"@@ -117,6 +119,14 @@ Drill can connect to any number of data sources, each of which may have its own
 As a result, the Metastore labels tables with a combination of (plugin configuration name, workspace name, table name).
 Note that if before renaming any of these items, you must delete table's Metadata entry and recreate it after renaming.
 
+### Using schema provisioning feature with Drill Metastore
+
+Drill Metastore allows specifying schema using the same syntax as
+ [Schema provisioning]({{site.baseurl}}/docs/plugin-configuration-basics/#specifying-the-schema-as-table-function-parameter) feature when used as a table function.
+User can specify table schema in the `ANALYZE` command, so it will be used for collecting table statistics and will be stored
+ to Drill Metastore to be used when submitting queries for this table similar to the case when user specifies schema
+ explicitly in the table function.","[{'comment': ""The Drill Metastore holds both schema and statistics information for a table. The `ANALYZE` command can infer the table schema for well-defined tables (such as many Parquet tables). Some tables are too complex or variable for Drill's schema inference to work well. For example, JSON tables often omit fields or have long runs of nulls so that Drill cannot determine column types. In these cases you can specify the correct schema based on your knowledge of the a table's structure. You specify a schema in the `ANALYZE` command using the \r\n [Schema provisioning]({{site.baseurl}}/docs/plugin-configuration-basics/#specifying-the-schema-as-table-function-parameter) syntax.\r\n\r\n(Please provide an example.)"", 'commenter': 'paul-rogers'}]"
2030,_docs/performance-tuning/drill-metastore/010-using-drill-metastore.md,"@@ -442,3 +455,114 @@ apache drill (information_schema)> SELECT * FROM INFORMATION_SCHEMA.`COLUMNS` WH
 +---------------+--------------+------------+-----------------+------------------+----------------+-------------+-------------------+--------------------------+------------------------+-------------------+-------------------------+---------------+--------------------+---------------+--------------------+-------------+---------------+-----------+---------+---------+-----------+-------------------+-----------+
 17 rows selected (0.183 seconds)
 ```
+
+### Provisioning schema for Drill Metastore
+
+#### Directory and File Setup
+
+Set up storage plugin for desired file system, as described here:
+ [Connecting Drill to a File System]({{site.baseurl}}/docs/file-system-storage-plugin/#connecting-drill-to-a-file-system).","[{'comment': 'Ensure you have configured the file system storage plugin as described here: ...', 'commenter': 'paul-rogers'}]"
2030,_docs/performance-tuning/drill-metastore/010-using-drill-metastore.md,"@@ -442,3 +455,114 @@ apache drill (information_schema)> SELECT * FROM INFORMATION_SCHEMA.`COLUMNS` WH
 +---------------+--------------+------------+-----------------+------------------+----------------+-------------+-------------------+--------------------------+------------------------+-------------------+-------------------------+---------------+--------------------+---------------+--------------------+-------------+---------------+-----------+---------+---------+-----------+-------------------+-----------+
 17 rows selected (0.183 seconds)
 ```
+
+### Provisioning schema for Drill Metastore
+
+#### Directory and File Setup
+
+Set up storage plugin for desired file system, as described here:
+ [Connecting Drill to a File System]({{site.baseurl}}/docs/file-system-storage-plugin/#connecting-drill-to-a-file-system).
+
+Set `store.format` to `csvh`:
+
+```
+SET `store.format`='csvh';
++------+-----------------------+
+|  ok  |        summary        |
++------+-----------------------+
+| true | store.format updated. |
++------+-----------------------+
+```
+
+Create text table based on the sample `/tpch/nation.parquet` table from `cp` plugin:","[{'comment': 'Create a text table...', 'commenter': 'paul-rogers'}]"
2030,_docs/performance-tuning/drill-metastore/010-using-drill-metastore.md,"@@ -442,3 +455,114 @@ apache drill (information_schema)> SELECT * FROM INFORMATION_SCHEMA.`COLUMNS` WH
 +---------------+--------------+------------+-----------------+------------------+----------------+-------------+-------------------+--------------------------+------------------------+-------------------+-------------------------+---------------+--------------------+---------------+--------------------+-------------+---------------+-----------+---------+---------+-----------+-------------------+-----------+
 17 rows selected (0.183 seconds)
 ```
+
+### Provisioning schema for Drill Metastore
+
+#### Directory and File Setup
+
+Set up storage plugin for desired file system, as described here:
+ [Connecting Drill to a File System]({{site.baseurl}}/docs/file-system-storage-plugin/#connecting-drill-to-a-file-system).
+
+Set `store.format` to `csvh`:
+
+```
+SET `store.format`='csvh';
++------+-----------------------+
+|  ok  |        summary        |
++------+-----------------------+
+| true | store.format updated. |
++------+-----------------------+
+```
+
+Create text table based on the sample `/tpch/nation.parquet` table from `cp` plugin:
+
+```
+create table dfs.tmp.text_nation as (select * from cp.`/tpch/nation.parquet`);
++----------+---------------------------+
+| Fragment | Number of records written |
++----------+---------------------------+
+| 0_0      | 25                        |
++----------+---------------------------+
+```
+
+Query the table `text_nation`:
+
+```
+SELECT count(*) FROM dfs.tmp.`text_nation`;
++--------+
+| EXPR$0 |
++--------+
+| 25     |
++--------+
+```","[{'comment': '(Suggestion: since we are applying a schema, show the original types using the clunky `typeof()` functions. This will show that the columns start as `VARCHAR`, but that applying the schema gives them more useful types. Otherwise, I think the point may be lost on most users.\r\n\r\nAnd, yes, we should have a `DESCRIBE TABLE` to do the job instead of `SELECT typeof(n_nationkey), typeof(...`)', 'commenter': 'paul-rogers'}]"
2040,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/TimeBucketFunctions.java,"@@ -102,4 +104,80 @@ public void eval() {
       out.value = timestamp - (timestamp % intervalToAdd);
     }
   }
+
+  /**
+   * This function is used for facilitating time series analysis by creating buckets of time intervals.  See
+   * https://blog.timescale.com/blog/simplified-time-series-analytics-using-the-time_bucket-function/ for usage. The function takes two arguments:
+   * 1. The timestamp (as a Drill timestamp)
+   * 2. The desired bucket interval IN milliseconds
+   *
+   * The function returns a BIGINT of the nearest time bucket.
+   */
+  @FunctionTemplate(name = ""time_bucket"",
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class TimestampTimeBucketFunction implements DrillSimpleFunc {
+
+    @Param
+    TimeStampHolder inputDate;
+
+    @Param
+    BigIntHolder interval;
+
+    @Output
+    BigIntHolder out;
+
+    @Override
+    public void setup() {
+    }
+
+    @Override
+    public void eval() {
+      // Get the timestamp in milliseconds
+      long timestamp = inputDate.value;
+
+      // Get the interval in milliseconds
+      long intervalToAdd = interval.value;","[{'comment': 'This is not an interval to ""add"" so much as a group-by interval.', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2040,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/TimeBucketFunctions.java,"@@ -102,4 +104,80 @@ public void eval() {
       out.value = timestamp - (timestamp % intervalToAdd);
     }
   }
+
+  /**
+   * This function is used for facilitating time series analysis by creating buckets of time intervals.  See
+   * https://blog.timescale.com/blog/simplified-time-series-analytics-using-the-time_bucket-function/ for usage. The function takes two arguments:
+   * 1. The timestamp (as a Drill timestamp)
+   * 2. The desired bucket interval IN milliseconds
+   *
+   * The function returns a BIGINT of the nearest time bucket.
+   */
+  @FunctionTemplate(name = ""time_bucket"",
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class TimestampTimeBucketFunction implements DrillSimpleFunc {
+
+    @Param
+    TimeStampHolder inputDate;
+
+    @Param
+    BigIntHolder interval;
+
+    @Output
+    BigIntHolder out;
+
+    @Override
+    public void setup() {
+    }
+
+    @Override
+    public void eval() {
+      // Get the timestamp in milliseconds
+      long timestamp = inputDate.value;
+
+      // Get the interval in milliseconds
+      long intervalToAdd = interval.value;
+
+      out.value = timestamp - (timestamp % intervalToAdd);
+    }
+  }
+
+  /**
+   * This function is used for facilitating time series analysis by creating buckets of time intervals.  See
+   * https://blog.timescale.com/blog/simplified-time-series-analytics-using-the-time_bucket-function/ for usage. The function takes two arguments:
+   * 1. The timestamp (as a Drill timestamp)
+   * 2. The desired bucket interval IN milliseconds
+   *
+   * The function returns a BIGINT of the nearest time bucket.
+   */
+  @FunctionTemplate(name = ""time_bucket"",
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class DoubleTimeBucketFunction implements DrillSimpleFunc {
+
+    @Param
+    Float8Holder inputDate;
+
+    @Param
+    BigIntHolder interval;
+
+    @Output
+    BigIntHolder out;
+
+    @Override
+    public void setup() {
+    }
+
+    @Override
+    public void eval() {
+      // Get the timestamp in milliseconds
+      long timestamp = (long)inputDate.value;","[{'comment': '`Math.round(inputDate.value)`. We would want 4.99999999.... to be treated as 5.', 'commenter': 'paul-rogers'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2040,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/TimeBucketFunctions.java,"@@ -97,9 +99,85 @@ public void eval() {
       long timestamp = inputDate.value;
 
       // Get the interval in milliseconds
-      long intervalToAdd = interval.value;
+      long groupByInterval = interval.value;
 
-      out.value = timestamp - (timestamp % intervalToAdd);
+      out.value = timestamp - (timestamp % groupByInterval);
+    }
+  }
+
+  /**
+   * This function is used for facilitating time series analysis by creating buckets of time intervals.  See
+   * https://blog.timescale.com/blog/simplified-time-series-analytics-using-the-time_bucket-function/ for usage. The function takes two arguments:
+   * 1. The timestamp (as a Drill timestamp)
+   * 2. The desired bucket interval IN milliseconds
+   *
+   * The function returns a BIGINT of the nearest time bucket.
+   */
+  @FunctionTemplate(name = ""time_bucket"",
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class TimestampTimeBucketFunction implements DrillSimpleFunc {
+
+    @Param
+    TimeStampHolder inputDate;
+
+    @Param
+    BigIntHolder interval;
+
+    @Output
+    BigIntHolder out;","[{'comment': 'Sorry; I wonder if the `TimeStamp` version should emit a timestamp. The interval is a valid ms-since-the-epoch number; seems to make sense to truncate a TS to another TS.', 'commenter': 'paul-rogers'}, {'comment': ""I was thinking the same thing actually... of course right after I submitted.  I'll convert to TS."", 'commenter': 'cgivre'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2040,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/TimeBucketFunctions.java,"@@ -97,9 +99,88 @@ public void eval() {
       long timestamp = inputDate.value;
 
       // Get the interval in milliseconds
-      long intervalToAdd = interval.value;
+      long groupByInterval = interval.value;
 
-      out.value = timestamp - (timestamp % intervalToAdd);
+      out.value = timestamp - (timestamp % groupByInterval);
+    }
+  }
+
+  /**
+   * This function is used for facilitating time series analysis by creating buckets of time intervals.  See
+   * https://blog.timescale.com/blog/simplified-time-series-analytics-using-the-time_bucket-function/ for usage. The function takes two arguments:
+   * 1. The timestamp (as a Drill timestamp)
+   * 2. The desired bucket interval IN milliseconds
+   *
+   * The function returns a BIGINT of the nearest time bucket.
+   */
+  @FunctionTemplate(name = ""time_bucket"",
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class TimestampTimeBucketFunction implements DrillSimpleFunc {
+
+    @Param
+    TimeStampHolder inputDate;
+
+    @Param
+    BigIntHolder interval;
+
+    @Output
+    TimeStampHolder out;
+
+    @Override
+    public void setup() {
+    }
+
+    @Override
+    public void eval() {
+      // Get the timestamp in milliseconds
+      long timestamp = inputDate.value;
+
+      // Get the interval in milliseconds
+      long groupByInterval = interval.value;
+
+      java.time.Instant instant = java.time.Instant.ofEpochMilli(timestamp - (timestamp % groupByInterval));
+      java.time.LocalDateTime localDate = instant.atZone(java.time.ZoneId.of(""UTC"")).toLocalDateTime();
+
+      out.value = localDate.atZone(java.time.ZoneId.of(""UTC"")).toInstant().toEpochMilli();","[{'comment': ""@cgivre, could you please explain, what happens here? Initially, you calculate the required milliseconds, after that creates `Instant` instance based on that, converts it to `LocalDateTime` at `UTC` timezone, converts it to `ZonedDateTime`, converts it to `Instant` and after that converts back to milliseconds.\r\n\r\nAre all these transformations required? Usually, UDF shouldn't apply timezone to the values they handle."", 'commenter': 'vvysotskyi'}, {'comment': 'Nope.  Not sure why I did that, but I simplified and tested this with a few different timezones on my computer. ', 'commenter': 'cgivre'}]"
2040,contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestTimeBucketFunction.java,"@@ -92,6 +94,28 @@ public void testTimeBucket() throws Exception {
       .go();
   }
 
+  @Test
+  public void testDoubleTimeBucket() throws Exception {
+    String query = ""SELECT time_bucket(CAST(1451606760 AS DOUBLE), 300000) AS high FROM (values(1))"";
+    testBuilder()
+      .sqlQuery(query)
+      .ordered()
+      .baselineColumns(""high"")
+      .baselineValues(1451400000L)
+      .go();
+  }
+
+  @Test
+  public void testTimeBucketTimestamp() throws Exception {
+    String query = ""SELECT time_bucket(CAST(1585272833845 AS TIMESTAMP), 300000) AS high FROM (values(1))"";","[{'comment': 'Could you please replace it with string representation, it looks more friendly and helps to understand the source value:\r\n```suggestion\r\n    String query = ""SELECT time_bucket(timestamp \'2020-03-27 01:33:53.845\', 300000) AS high"";\r\n```\r\n\r\nAlso, there is no need to specify `FROM (values(1))` if it is not used.', 'commenter': 'vvysotskyi'}, {'comment': 'Removed.', 'commenter': 'cgivre'}]"
2040,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/TimeBucketFunctions.java,"@@ -97,9 +99,86 @@ public void eval() {
       long timestamp = inputDate.value;
 
       // Get the interval in milliseconds
-      long intervalToAdd = interval.value;
+      long groupByInterval = interval.value;
 
-      out.value = timestamp - (timestamp % intervalToAdd);
+      out.value = timestamp - (timestamp % groupByInterval);
+    }
+  }
+
+  /**
+   * This function is used for facilitating time series analysis by creating buckets of time intervals.  See
+   * https://blog.timescale.com/blog/simplified-time-series-analytics-using-the-time_bucket-function/ for usage. The function takes two arguments:
+   * 1. The timestamp (as a Drill timestamp)
+   * 2. The desired bucket interval IN milliseconds
+   *
+   * The function returns a BIGINT of the nearest time bucket.
+   */
+  @FunctionTemplate(name = ""time_bucket"",
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class TimestampTimeBucketFunction implements DrillSimpleFunc {
+
+    @Param
+    TimeStampHolder inputDate;
+
+    @Param
+    BigIntHolder interval;
+
+    @Output
+    TimeStampHolder out;
+
+    @Override
+    public void setup() {
+    }
+
+    @Override
+    public void eval() {
+      // Get the timestamp in milliseconds
+      long timestamp = inputDate.value;
+
+      // Get the interval in milliseconds
+      long groupByInterval = interval.value;
+
+      java.time.Instant instant = java.time.Instant.ofEpochMilli(timestamp - (timestamp % groupByInterval));","[{'comment': 'Looks like creating `Instant` here may be also omitted.', 'commenter': 'vvysotskyi'}, {'comment': 'Done.', 'commenter': 'cgivre'}]"
2044,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/UserAgentUtils.java,"@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.udfs;
+
+import nl.basjes.parse.useragent.UserAgentAnalyzer;","[{'comment': 'Nit:   Please indent with 2 spaces rather than 4. ', 'commenter': 'cgivre'}, {'comment': 'Done', 'commenter': 'nielsbasjes'}, {'comment': 'Thanks!', 'commenter': 'cgivre'}]"
2044,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/UserAgentUtils.java,"@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.udfs;
+
+import nl.basjes.parse.useragent.UserAgentAnalyzer;
+
+public class UserAgentUtils {
+
+  private UserAgentUtils(){}
+
+  private static UserAgentAnalyzer instance = null;
+
+  public static synchronized UserAgentAnalyzer getInstance() {","[{'comment': ""It's better to use here DCL to avoid constant synchronization.\r\n\r\n@vvysotskyi @paul-rogers it is ok to call such methods from UDFs? I recall there was some discussions in Jira about scalar replacement. Does it apply here?"", 'commenter': 'arina-ielchiieva'}, {'comment': 'What do you mean with DCL?\r\nI happy to change the code if needed.', 'commenter': 'nielsbasjes'}, {'comment': 'Double-checked locking but even better to use holder on demand.\r\nMaybe this article can help - https://www.baeldung.com/java-singleton-double-checked-locking', 'commenter': 'arina-ielchiieva'}, {'comment': ""I know Double checked locking, I was just confused about 'DCL'."", 'commenter': 'nielsbasjes'}, {'comment': ""No problem, it would be nice if you'll use here holder on demand, looks like it suits here perfectly."", 'commenter': 'arina-ielchiieva'}, {'comment': '@arina-ielchiieva, in this case, it is safe to call such methods from UDFs. Problem with scalar replacement occurs when holder is passed to the method.', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi thanks for confirmation.', 'commenter': 'arina-ielchiieva'}, {'comment': 'Note that this is called only during the `setup()` so I expect the number of calls to this to be very small and as such not a performance problem.', 'commenter': 'nielsbasjes'}, {'comment': 'Just to clarify you are not going to implement holder on demand?', 'commenter': 'arina-ielchiieva'}, {'comment': '@arina-ielchiieva 2 things: \r\n1. If you want it using a ""holder on demand"" then I\'ll implement it. This is your project, I\'m just passing by to help on this single aspect. Your call.\r\n2. My knowledge of the inner workings of Drill is too limited to understand how you want it. I have no clue what a ""holder on demand"" looks like. Sorry.', 'commenter': 'nielsbasjes'}, {'comment': 'Yes, it would be really good to use holder on demand implementation because initialization-on-demand holder is always best practice for implementing singleton pattern.\r\n\r\nRegarding how to implement it, example how to implement it was present in the link I have posted previously: `3.2. Initialization on Demand`.\r\n\r\nTo make things quicker, I have already adopted it to your use case, please replace your class `UserAgentUtils` with new class `UserAgentAnalyzerProvider` and call `UserAgentAnalyzerProvider.getInstance()` in your UDFs code.\r\n```\r\npublic class UserAgentAnalyzerProvider {\r\n\r\n  public static UserAgentAnalyzer getInstance() {\r\n    return UserAgentAnalyzerHolder.INSTANCE;\r\n  }\r\n\r\n  private static class UserAgentAnalyzerHolder {\r\n    private static final UserAgentAnalyzer INSTANCE = UserAgentAnalyzer.newBuilder()\r\n      .dropTests()\r\n      .hideMatcherLoadStats()\r\n      .immediateInitialization()\r\n      .build();\r\n  }\r\n}\r\n```', 'commenter': 'arina-ielchiieva'}, {'comment': ""Thanks! I've updated the code, works on my machine and pushed it.\r\n"", 'commenter': 'nielsbasjes'}]"
2044,contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestUserAgentFunctions.java,"@@ -102,7 +102,7 @@ public void testEmptyFieldName() throws Exception {
   @Test
   public void testNullUserAgent() throws Exception {
     String query = ""SELECT parse_user_agent(CAST(null as VARCHAR)) AS agent FROM (values(1))"";
-    Map emptyMap = new HashMap();
+    Map<?, ?> emptyMap = new HashMap<>();","[{'comment': '```suggestion\r\n    Map<?, ?> emptyMap = Collections.emptyMap();\r\n```', 'commenter': 'arina-ielchiieva'}, {'comment': 'Fixed.', 'commenter': 'nielsbasjes'}]"
2044,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/UserAgentAnalyzerProvider.java,"@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.udfs;
+
+import nl.basjes.parse.useragent.UserAgentAnalyzer;
+
+public class UserAgentAnalyzerProvider {
+
+  public static UserAgentAnalyzer getInstance() {
+    return UserAgentAnalyzerHolder.INSTANCE;
+  }
+
+  private static class UserAgentAnalyzerHolder {
+    private static final UserAgentAnalyzer INSTANCE = UserAgentAnalyzer.newBuilder()","[{'comment': ""Jumping in a bit late here. Isn't one layer of class sufficient to hold the static instance?\r\n\r\n```\r\npublic class UserAgentAnalyzerProvider {\r\n  private static final INSTANCE = UserAgentAnalyzer.newBuilder()\r\n\t    .dropTests()\r\n            .hideMatcherLoadStats()\r\n            .immediateInitialization()\r\n            .build();\r\n\r\n  public static UserAgentAnalyzer getInstance() {\r\n    return INSTANCE;\r\n  }\r\n}\r\n```"", 'commenter': 'paul-rogers'}, {'comment': ""@paul-rogers may I answer this question.\r\nYour suggestion will also work but will contradict with holder on demand singleton approach, i.e. it's not lazy init approach. Your example will init singleton instance when JVM starts while having inner class will init singleton only during first call of `getInstance` method."", 'commenter': 'arina-ielchiieva'}, {'comment': ""@arina-ielchiieva, thanks for the explanation. I understand the reasoning. However, what I see, at least in the debugger, is that statics are not initialized until the first use of the class. A reference is not a use, only a call is.\r\n\r\nHowever, this behavior may be unique to the debugger, perhaps the non-debug execution works differently. Still, easy enough to test so we can know for certain. Would be good to nail down the exact behavior so we don't add excess complexity unnecessarily. "", 'commenter': 'paul-rogers'}, {'comment': 'Did a little experiment to see if I was confused. Looks like a single layer of provider will work fine.\r\n\r\nTwo classes:\r\n\r\n```\r\npublic class DummyProvider {\r\n  private static int instance = 10;\r\n  static {\r\n    System.out.println(""DummyProvider loaded"");\r\n  }\r\n\r\n  public static int instance() { return instance; }\r\n}\r\n```\r\n\r\nAnd:\r\n\r\n```\r\npublic class DummyClient {\r\n  public static void main(String[] args) {\r\n    System.out.println(""Main starting"");\r\n    System.out.println(""Getting instance"");\r\n    System.out.println(DummyProvider.instance());\r\n    System.out.println(""Exiting"");\r\n  }\r\n}\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nMain starting\r\nGetting instance\r\nDummyProvider loaded\r\n10\r\nExiting\r\n```\r\n\r\nRan in both the debugger and normally. Both produced the same output.\r\n\r\nThis suggests that, indeed, the class is not loaded, and statics initialized, until first use (call).', 'commenter': 'paul-rogers'}, {'comment': '@paul-rogers here is some explanation that you might find useful: https://en.m.wikipedia.org/wiki/Initialization-on-demand_holder_idiom', 'commenter': 'arina-ielchiieva'}, {'comment': ""@arina-ielchiieva, thanks; that article is accurate in the case that class `Something` is loaded earlier than the first use of `getInstance()`. Maybe there are other static functions or constants that force earlier class loading.\r\n\r\nIn this specific case, the only method is `getInstance()` and so the outer class won't be loaded until that time. This means that the inner and outer classes are load at essentially the same time: on that first call to `getInstance()`.\r\n\r\nWe can commit his PR with the code as it is; it works fine. My concern, however, is that we use static instances all over, and we've kind of relied on the behavior I've been describing. To be consistent, we'd want to go and change all the other uses as well. But, doing so would be unnecessary work in cases, like this one, where a single layer works fine.\r\n\r\nLet's go ahead and commit this, then we can continue the discussion without slowing this PR. "", 'commenter': 'paul-rogers'}, {'comment': ""@paul-rogers totally agree with you, since there is not other static stuff there class won't be loaded prematurely, unless with time something static is added there. Anyway, having inner class does not give much complexity and since you ok, we can leave it as it to comply with pattern code template."", 'commenter': 'arina-ielchiieva'}]"
2044,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/UserAgentAnalyzerProvider.java,"@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.udfs;
+
+import nl.basjes.parse.useragent.UserAgentAnalyzer;
+
+public class UserAgentAnalyzerProvider {
+
+  public static UserAgentAnalyzer getInstance() {
+    return UserAgentAnalyzerHolder.INSTANCE;
+  }
+
+  private static class UserAgentAnalyzerHolder {
+    private static final UserAgentAnalyzer INSTANCE = UserAgentAnalyzer.newBuilder()
+            .dropTests()
+            .hideMatcherLoadStats()
+            .immediateInitialization()
+            .build();
+  }
+}","[{'comment': 'Nit: newline', 'commenter': 'paul-rogers'}, {'comment': 'Fixed.', 'commenter': 'nielsbasjes'}]"
2044,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/UserAgentFunctions.java,"@@ -46,11 +46,14 @@
     DrillBuf outBuffer;
 
     @Workspace
-    nl.basjes.parse.useragent.UserAgentAnalyzerDirect uaa;
+    nl.basjes.parse.useragent.UserAgentAnalyzer uaa;
+
+    @Workspace
+    java.util.List<String> allFields;
 
     public void setup() {
-      uaa = nl.basjes.parse.useragent.UserAgentAnalyzerDirect.newBuilder().dropTests().hideMatcherLoadStats().build();
-      uaa.getAllPossibleFieldNamesSorted();
+      uaa = org.apache.drill.exec.udfs.UserAgentAnalyzerProvider.getInstance();
+      allFields = uaa.getAllPossibleFieldNamesSorted();","[{'comment': ""Thinking this through... Drill is multi-threaded. We can have, say, 20 threads all running the same query, evaluating the same expression. Having the list here seems thread-safe. I wonder, however, is the list of fields essentially static? Seems to be, there are no parameters passed to the instance. If so, does it make sense to store the field list in the provider so we don't have to build it for every thread of every query?"", 'commenter': 'paul-rogers'}, {'comment': 'The list of field names is essentially static.', 'commenter': 'nielsbasjes'}]"
2044,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/UserAgentFunctions.java,"@@ -92,11 +95,14 @@ public void eval() {
     DrillBuf outBuffer;
 
     @Workspace
-    nl.basjes.parse.useragent.UserAgentAnalyzerDirect uaa;
+    nl.basjes.parse.useragent.UserAgentAnalyzer uaa;
+
+    @Workspace
+    java.util.List<String> allFields;
 
     public void setup() {
-      uaa = nl.basjes.parse.useragent.UserAgentAnalyzerDirect.newBuilder().dropTests().hideMatcherLoadStats().build();
-      uaa.getAllPossibleFieldNamesSorted();
+      uaa = org.apache.drill.exec.udfs.UserAgentAnalyzerProvider.getInstance();
+      allFields = uaa.getAllPossibleFieldNamesSorted();","[{'comment': 'Same suggestion', 'commenter': 'paul-rogers'}]"
2055,common/src/main/java/org/apache/drill/common/util/ProtobufPatcher.java,"@@ -57,15 +60,15 @@ public static synchronized void patch() {
   private static void patchByteString() {
     try {
       ClassPool classPool = getClassPool();
-      CtClass byteString = classPool.get(""com.google.protobuf.ByteString"");
+      CtClass byteString = classPool.get(protobufPackage + ""ByteString"");","[{'comment': ""I'm afraid it will break JDBC client. As you know, we shade protobuf jars into JDBC driver and relocate them from `com.google` to `oadd.com.google`.\r\nShade plugin traverses all classes and replaces packages with updated ones. String literal with full class name also will be updated, but with your change, it breaks this behavior, so classes wouldn't be shades.\r\nSee the description of this behavior here: https://issues.apache.org/jira/browse/MSHADE-156. \r\n\r\nAfter fixing this issue, please ensure that the custom authenticator works correctly with your changes (this one should break it)."", 'commenter': 'vvysotskyi'}, {'comment': 'Verified with different JDBC clients w/ and w/o security, with pam4j and custom auth module, everything is working fine. Also, by decompilation of ProtobufPatcher class from the driver I can see that all strings with package names were renamed with addition of ""oadd."" prefix.', 'commenter': 'agozhiy'}, {'comment': 'Thanks for checking. Looks like I observed this issue for another case of using a fully qualified name. ', 'commenter': 'vvysotskyi'}]"
2060,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SchemaUtilites.java,"@@ -218,7 +218,7 @@ private static AbstractSchema resolveToDrillSchemaInternal (SchemaPlus defaultSc
 
     if (isRootSchema(schema)) {
       throw UserException.validationError()
-          .message(""Root schema is immutable. Creating or dropping tables/views is not allowed in root schema."" +
+          .message(""Root schema is immutable. Creating or dropping tables/views is not allowed in root schema. "" +","[{'comment': 'Maybe: ""Drill does not allow creating or deleting tables or views in the root schema.""', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.
+
+### Tables creation
+
+RDBMS Metastore is using [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)
+to create all needed tables during RDBMS Metastore initialization, users should not create any tables manually.
+
+### Database schema
+
+Liquibase is using yaml configuration file to apply changes into database schema: `src/main/resources/db/changelog/chnagelog.yaml`.","[{'comment': '```suggestion\r\nLiquibase is using yaml configuration file to apply changes into database schema: `src/main/resources/db/changelog/changelog.yaml`.\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Liquibase ~~is using~~ **uses a** yaml ... ~~into~~ **to the** database ...', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/src/main/resources/db/changelog/changes/initial_ddls.yaml,"@@ -0,0 +1,397 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Creates 5 RDBMS Metastore tables component tables: TABLES, SEGMENTS, FILES, ROW_GROUPS, PARTITIONS.
+databaseChangeLog:
+    # to preserve upper case naming in some DBs (ex: PostgreSQL) and quote reserved column names (ex: COLUMN)
+  - objectQuotingStrategy: QUOTE_ALL_OBJECTS
+  - changeSet:
+      id: 1
+      author: arina
+      changes:
+        - createTable:
+            tableName: TABLES
+            columns:
+              - column:
+                  name: STORAGE_PLUGIN
+                  type: VARCHAR(100)
+                  constraints:
+                    primaryKey: true
+                    nullable: false
+              - column:
+                  name: WORKSPACE
+                  type: VARCHAR(100)
+                  constraints:
+                    primaryKey: true
+                    nullable: false
+              - column:
+                  name: TABLE_NAME
+                  type: VARCHAR(200)","[{'comment': 'Small assumption: table path may be specified as a table name, for example ````dfs.`/tmp/a/b/c/nation.parquet` ```` so I would recommend making its size closer to the `LOCATION` size.', 'commenter': 'vvysotskyi'}]"
2060,metastore/rdbms-metastore/src/main/resources/db/changelog/changes/initial_ddls.yaml,"@@ -0,0 +1,397 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Creates 5 RDBMS Metastore tables component tables: TABLES, SEGMENTS, FILES, ROW_GROUPS, PARTITIONS.
+databaseChangeLog:
+    # to preserve upper case naming in some DBs (ex: PostgreSQL) and quote reserved column names (ex: COLUMN)
+  - objectQuotingStrategy: QUOTE_ALL_OBJECTS
+  - changeSet:
+      id: 1
+      author: arina
+      changes:
+        - createTable:
+            tableName: TABLES
+            columns:
+              - column:
+                  name: STORAGE_PLUGIN
+                  type: VARCHAR(100)
+                  constraints:
+                    primaryKey: true
+                    nullable: false
+              - column:
+                  name: WORKSPACE
+                  type: VARCHAR(100)
+                  constraints:
+                    primaryKey: true
+                    nullable: false
+              - column:
+                  name: TABLE_NAME
+                  type: VARCHAR(200)
+                  constraints:
+                    primaryKey: true
+                    nullable: false
+              - column:
+                  name: OWNER
+                  type: VARCHAR(100)
+              - column:
+                  name: TABLE_TYPE
+                  type: VARCHAR(100)
+              - column:
+                  name: METADATA_KEY
+                  type: VARCHAR(100)
+                  constraints:
+                    nullable: false
+              - column:
+                  name: METADATA_TYPE
+                  type: VARCHAR(100)
+                  constraints:
+                    nullable: false
+              - column:
+                  name: LOCATION
+                  type: VARCHAR(500)","[{'comment': 'Realy a good choice for this value. Though some linux-based systems limit its size to 256 bytes, other systems like Windows removed the existing 260 characters limit. So I think 500 value is fine.', 'commenter': 'vvysotskyi'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.","[{'comment': '**The** RDBMS Metastore implementation allows **you** store Drill Metastore metadata in **a** configured RDBMS.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.","[{'comment': 'Currently, **the** RDBMS Metastore is not **the** default ~~Drill Metastore~~ implementation.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:","[{'comment': 'To enable **the** RDBMS Metastore**,** create **the** `drill-metastore-override.conf` **file in your config directory** and ~~indicate~~ **specify the** RDBMS Metastore class:', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.","[{'comment': 'Use the connection properties to specify how Drill should connect to your Metastore database.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.","[{'comment': 'Note ~~,~~ **: the** driver **class** must be included into **the** Drill classpath ~~prior to start up for all databases except of SQLite~~ . The easiest way to do that is to put the driver jar file into the `$DRILL_HOME/jars/3rdparty` folder. Or, to make upgrades easier, in your `$DRILL_SITE/jars` folder. Drill includes the driver for SQLite.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.","[{'comment': 'Out of the box, **the** Drill RDBMS Metastore is configured to use **the** embedded file system based SQLite database.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.","[{'comment': ""((This is an awkward location. `zk.root` is a path in ZK, not on the file system. The location should be relative to $DRILL_CONFIG or $DRILL_SITE so that it resides in the home directory, which should be separate from the Drill install in a production system. I don't recall how to get that location in Drill, it is given by `DRILL_CONF_DIR` in the shell scripts.))"", 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.","[{'comment': 'SQLite is an embedded database; is not distributed. SQLite is good for trying out the feature, for testing, for a running Drill in embedded mode, and perhaps for a single-node Drill ""cluster"". If should not be used in a multi-node cluster. Each Drillbit will have its own version of the truth and behavior will be undefined and incorrect.\r\n\r\n((Question: can we detect that more than one Drillbit is running and fail or log an error if using Sqllite?))', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.","[{'comment': 'Note: **as mentioned above, the** PostgreSQL JDBC driver must be present in **the** Drill classpath.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.","[{'comment': '((As above.))', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,","[{'comment': '...use **the** `com...`', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.","[{'comment': '**The** Drill Metastore ~~consists of components~~ **stores several types of metadata, called compoents**. Currently, only **the** `tables` component is implemented. ((In the RDBS implementation or overall?))', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.","[{'comment': '~~This component~~ **The `tables` component** provides...', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.","[{'comment': '((Sorry, didn\'t understand this. Does this mean that `TableMetadataUnit` implements the tables component? Describes it? What does ""Unit"" mean? And, is the **Table** metadata unit applicable for any metadata type in the sense of those others that are not implemented yet? Or, is this the class that holds fields for all of the objects within the table component, meaning it has the union of fields for tables, segments, files, row groups and partitions? If so, I might have a suggestion about an improved design...))', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.","[{'comment': '((This does indicate that it is a union of all fields. If so, then:)) The `TableMetadataUnit` class holds fields for all five metadata types within the `tables` component. Any fields not applicable to a particular metadata type are simply ignored and remain unset.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.","[{'comment': 'In the RDBMS implementation of the Drill Metastore, the `tables` component includes five tables, one for each metadata type. The five tables are: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, **and** `PARTITIONS`. See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for the schema and indexes of each table.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.
+
+### Tables creation
+
+RDBMS Metastore is using [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)
+to create all needed tables during RDBMS Metastore initialization, users should not create any tables manually.","[{'comment': 'Table ~~s~~ creation\r\n\r\n**The** RDBMS Metastore ~~is using~~ **uses** [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)\r\nto create ~~all~~ **the** needed tables during RDBMS Metastore initialization ~~, users~~ **. Users** should not create ~~any~~ tables manually.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.
+
+### Tables creation
+
+RDBMS Metastore is using [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)
+to create all needed tables during RDBMS Metastore initialization, users should not create any tables manually.
+
+### Database schema
+
+Liquibase is using yaml configuration file to apply changes into database schema: `src/main/resources/db/changelog/chnagelog.yaml`.
+It converts yaml content into DDL / DML commands suitable to database syntax based on connection details.
+See list of supported databases: https://www.liquibase.org/databases.html.","[{'comment': '~~It~~ Liquibase converts **the** yaml ~~content~~ **specification** into **the** DDL ~~/~~ **and** DML commands ~~suitable to database syntax to database syntax based on connection details.~~ **required for the configured database.**', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.
+
+### Tables creation
+
+RDBMS Metastore is using [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)
+to create all needed tables during RDBMS Metastore initialization, users should not create any tables manually.
+
+### Database schema
+
+Liquibase is using yaml configuration file to apply changes into database schema: `src/main/resources/db/changelog/chnagelog.yaml`.
+It converts yaml content into DDL / DML commands suitable to database syntax based on connection details.
+See list of supported databases: https://www.liquibase.org/databases.html.
+
+All Drill Metastore tables will be created in database default schema, unless schema is set in the connection url. 
+It's recommended prior to using RDBMS Metastore to create dedicated Drill Metastore schema, for example, `drill_metastore`
+and indicate it in the connection url.","[{'comment': 'The Drill Metastore tables are created in the database schema indicated in the connection URL. This will be the default schema unless you specify a different schema. Drill will not create the schema, however. Best practice is to create a schema within your database for the Drill metastore before initializing the metastore.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.
+
+### Tables creation
+
+RDBMS Metastore is using [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)
+to create all needed tables during RDBMS Metastore initialization, users should not create any tables manually.
+
+### Database schema
+
+Liquibase is using yaml configuration file to apply changes into database schema: `src/main/resources/db/changelog/chnagelog.yaml`.
+It converts yaml content into DDL / DML commands suitable to database syntax based on connection details.
+See list of supported databases: https://www.liquibase.org/databases.html.
+
+All Drill Metastore tables will be created in database default schema, unless schema is set in the connection url. 
+It's recommended prior to using RDBMS Metastore to create dedicated Drill Metastore schema, for example, `drill_metastore`
+and indicate it in the connection url.
+
+Example:
+
+PostgreSQL: `jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore`
+
+MySQL: `jdbc:mysql://localhost:1234/drill_metastore`
+
+Note: database user must have not only read / write permission in Drill Metastore schema 
+but also permission to create / modify database objects.","[{'comment': 'Since Drill will create the required tables, ensure that the database user has the following permissions in the metastore schema:\r\n\r\n* Read and write tables\r\n* Create and modify database objects (tables, indexes, views, etc.)', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.
+
+### Tables creation
+
+RDBMS Metastore is using [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)
+to create all needed tables during RDBMS Metastore initialization, users should not create any tables manually.
+
+### Database schema
+
+Liquibase is using yaml configuration file to apply changes into database schema: `src/main/resources/db/changelog/chnagelog.yaml`.
+It converts yaml content into DDL / DML commands suitable to database syntax based on connection details.
+See list of supported databases: https://www.liquibase.org/databases.html.
+
+All Drill Metastore tables will be created in database default schema, unless schema is set in the connection url. 
+It's recommended prior to using RDBMS Metastore to create dedicated Drill Metastore schema, for example, `drill_metastore`
+and indicate it in the connection url.
+
+Example:
+
+PostgreSQL: `jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore`
+
+MySQL: `jdbc:mysql://localhost:1234/drill_metastore`
+
+Note: database user must have not only read / write permission in Drill Metastore schema 
+but also permission to create / modify database objects.
+
+### Liquibase tables
+
+During Drill RDBMS Metastore initialization, Liquibase will create two internal tracking tables:
+`DATABASECHANGELOG` and `DATABASECHANGELOGLOCK`. They are needed to track schema changes and concurrent updates.
+See https://www.liquibase.org/get_started/how-lb-works.html for more details.","[{'comment': ""((Sorry, I've gotten this far, but it is not clear the actual steps I should perform. What I gather so far is that I should:\r\n\r\n* Select a (distributed) database\r\n* Create a DB schema (AKA database) for the Drill metastore.\r\n* Create a user that has the required rights in that database.\r\n* Specify metastore configuration in the indicated file.\r\n\r\nThen the description gives great background information, but leaves me hanging: what do I do next? How do I initialize the DB? How do I verify it works? What do I check if something goes wrong (which it always does with DB-related stuff)?\r\n\r\nMaybe move the topics around to give a list of steps, then have topics that explain the theory of operation? ))"", 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.
+
+### Tables creation
+
+RDBMS Metastore is using [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)
+to create all needed tables during RDBMS Metastore initialization, users should not create any tables manually.
+
+### Database schema
+
+Liquibase is using yaml configuration file to apply changes into database schema: `src/main/resources/db/changelog/chnagelog.yaml`.
+It converts yaml content into DDL / DML commands suitable to database syntax based on connection details.
+See list of supported databases: https://www.liquibase.org/databases.html.
+
+All Drill Metastore tables will be created in database default schema, unless schema is set in the connection url. 
+It's recommended prior to using RDBMS Metastore to create dedicated Drill Metastore schema, for example, `drill_metastore`
+and indicate it in the connection url.
+
+Example:
+
+PostgreSQL: `jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore`
+
+MySQL: `jdbc:mysql://localhost:1234/drill_metastore`
+
+Note: database user must have not only read / write permission in Drill Metastore schema 
+but also permission to create / modify database objects.
+
+### Liquibase tables
+
+During Drill RDBMS Metastore initialization, Liquibase will create two internal tracking tables:
+`DATABASECHANGELOG` and `DATABASECHANGELOGLOCK`. They are needed to track schema changes and concurrent updates.
+See https://www.liquibase.org/get_started/how-lb-works.html for more details.
+
+## Query execution
+
+SQL queries issued to RDBMS Metastore tables are generated using [JOOQ](https://www.jooq.org/doc/3.13/manual/getting-started/).
+JOOQ allows programmatically construct SQL queries at runtime and provides executor to run generated queries.","[{'comment': 'Drill uses the open-source version of JOOQ to generate the queries sent to the configured metastore database.', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/README.md,"@@ -0,0 +1,140 @@
+# RDBMS Metastore
+
+RDBMS Metastore implementation allows to store Drill Metastore metadata in configured RDBMS.
+
+## Configuration
+
+Currently, RDBMS Metastore is not default Drill Metastore implementation.
+To enable RDBMS Metastore create `drill-metastore-override.conf` and indicate RDBMS Metastore class:
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore""
+}
+```
+
+### Connection properties
+
+Data source connection properties allows to indicate how to connect to Drill Metastore database.
+
+`drill.metastore.rdbms.data_source.driver` - driver class name. Required. 
+Note, driver must be included into Drill classpath prior to start up for all databases except of SQLite.
+
+`drill.metastore.rdbms.data_source.url` - connection url. Required.
+
+`drill.metastore.rdbms.data_source.username` - database user on whose behalf the connection is
+being made. Optional, if database does not require user to connect. 
+
+`drill.metastore.rdbms.data_source.password` - database user's password. 
+Optional, if database does not require user's password to connect.
+
+`drill.metastore.rdbms.data_source.properties` - specifies properties which will be used
+during data source creation. See list of available [Hikari properties](https://github.com/brettwooldridge/HikariCP)
+for more details.
+
+### Default configuration 
+
+Out of the box, Drill RDBMS Metastore is configured to use embedded file system based SQLite database.
+It will be created locally in user's home directory under `${drill.exec.zk.root}""/metastore` location.
+
+Default setup can be used only in Drill embedded mode. 
+If SQLite setup will be used in distributed mode, each drillbit will have it's own SQLite instance
+which will lead to bogus results during queries execution.
+In distributed mode, database instance must be accessible for all drillbits.
+
+### Custom configuration
+
+`drill-metastore-override.conf` is used to customize connection details to the Drill Metastore database.
+See `drill-metastore-override-example.conf` for more details.
+
+#### Example of PostgreSQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""org.postgresql.Driver"",
+      url: ""jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: PostgreSQL JDBC driver must be present in Drill classpath.
+
+#### Example of MySQL configuration
+
+```
+drill.metastore: {
+  implementation.class: ""org.apache.drill.metastore.rdbms.RdbmsMetastore"",
+  rdbms: {
+    data_source: {
+      driver: ""com.mysql.cj.jdbc.Driver"",
+      url: ""jdbc:mysql://localhost:1234/drill_metastore"",
+      username: ""user"",
+      password: ""password""
+    }
+  }
+}
+```
+
+Note: MySQL JDBC driver must be present in Drill classpath.
+
+##### Driver version
+
+For MySQL connector version 6+, use `com.mysql.cj.jdbc.Driver` driver class,
+for older versions use `com.mysql.jdbc.Driver`.
+
+## Tables structure
+
+Drill Metastore consists of components. Currently, only `tables` component is implemented.
+This component provides metadata about Drill tables, including their segments, files, row groups and partitions.
+In Drill `tables` component unit is represented by `TableMetadataUnit` class which is applicable to any metadata type.
+Fields which are not applicable to particular metadata type, remain unset.
+
+In RDBMS Drill Metastore each `tables` component metadata type has it's own table.
+There are five tables: `TABLES`, `SEGMENTS`, `FILES`, `ROW_GROUPS`, `PARTITIONS`.
+These tables structure and primary keys are defined based on fields specific for each metadata type.
+See `src/main/resources/db/changelog/changes/initial_ddls.yaml` for more details.
+
+### Tables creation
+
+RDBMS Metastore is using [Liquibase](https://www.liquibase.org/documentation/core-concepts/index.html)
+to create all needed tables during RDBMS Metastore initialization, users should not create any tables manually.
+
+### Database schema
+
+Liquibase is using yaml configuration file to apply changes into database schema: `src/main/resources/db/changelog/chnagelog.yaml`.
+It converts yaml content into DDL / DML commands suitable to database syntax based on connection details.
+See list of supported databases: https://www.liquibase.org/databases.html.
+
+All Drill Metastore tables will be created in database default schema, unless schema is set in the connection url. 
+It's recommended prior to using RDBMS Metastore to create dedicated Drill Metastore schema, for example, `drill_metastore`
+and indicate it in the connection url.
+
+Example:
+
+PostgreSQL: `jdbc:postgresql://localhost:1234/mydb?currentSchema=drill_metastore`
+
+MySQL: `jdbc:mysql://localhost:1234/drill_metastore`
+
+Note: database user must have not only read / write permission in Drill Metastore schema 
+but also permission to create / modify database objects.
+
+### Liquibase tables
+
+During Drill RDBMS Metastore initialization, Liquibase will create two internal tracking tables:
+`DATABASECHANGELOG` and `DATABASECHANGELOGLOCK`. They are needed to track schema changes and concurrent updates.
+See https://www.liquibase.org/get_started/how-lb-works.html for more details.
+
+## Query execution
+
+SQL queries issued to RDBMS Metastore tables are generated using [JOOQ](https://www.jooq.org/doc/3.13/manual/getting-started/).
+JOOQ allows programmatically construct SQL queries at runtime and provides executor to run generated queries.
+
+JOOQ generates SQL statements based on SQL dialect determined by database connection details.
+List of supported dialects: https://www.jooq.org/javadoc/3.13.x/org.jooq/org/jooq/SQLDialect.html.
+Note: dialects annotated with `@Pro` are not supported, since open-source version of JOOQ is used.","[{'comment': ""((This part is confusing. I'd like to know the list of supported DBs. Am I to understand that I can use any DB for which a JDBC driver is available (so I can do that part of the config), which has Liquibase support, and which has open-source JOOQ support?\r\n\r\nIs that rather too open ended? Should we list those that Drill tests? Maybe say that any other should work if there is Liquibase and JOOQ support? Maybe add that such support is a community effort: posts questions and suggestions on the user mailing list.))"", 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/src/main/java/org/apache/drill/metastore/rdbms/QueryExecutorProvider.java,"@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.metastore.rdbms;
+
+import com.zaxxer.hikari.HikariDataSource;
+import org.jooq.DSLContext;
+import org.jooq.SQLDialect;
+import org.jooq.impl.DSL;
+import org.jooq.tools.jdbc.JDBCUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.sql.Connection;
+import java.sql.SQLException;
+
+/**
+ * Provides SQL queries executor configured based on given data source and SQL dialect.
+ */
+public class QueryExecutorProvider implements AutoCloseable {
+
+  private static final Logger logger = LoggerFactory.getLogger(QueryExecutorProvider.class);
+
+  private final HikariDataSource dataSource;
+  private final SQLDialect dialect;
+
+  public QueryExecutorProvider(HikariDataSource dataSource) {
+    this.dataSource = dataSource;
+    this.dialect = defineDialect();
+  }
+
+  /**
+   * Provides query executor which can be used to execute various SQL statements.
+   * Executor transforms programmatically created queries into configured SQL dialect,
+   * executes them using connections from provided data source. Allows to execute
+   * SQL queries in transaction.
+   * Note: always close executor to release open connections.
+   *
+   * @return query executor
+   */
+  public DSLContext executor() {
+    return DSL.using(dataSource, dialect);
+  }
+
+  @Override
+  public void close() {
+    dataSource.close();
+  }
+
+  /**
+   * Defines SQL dialect based on data source connection.
+   * If unable to define the dialect, uses {@link SQLDialect#DEFAULT}.
+   *
+   * @return SQL dialect
+   */
+  private SQLDialect defineDialect() {
+    SQLDialect dialect = SQLDialect.DEFAULT;
+    try (Connection connection = dataSource.getConnection()) {
+      dialect = JDBCUtils.dialect(connection);
+    } catch (SQLException e) {
+      logger.debug(""Unable to connect to data source in order to define SQL dialect: {}"", e.getMessage(), e);","[{'comment': 'This seems like exactly the kind of problem a typical user (that would be me) will have. Stuff won\'t work for some dumb reason, but the only indication is a debug-level log message.\r\n\r\nShould we:\r\n\r\n* Dedicate a logger to narrating metastore start up and errors?\r\n* Explain in the user guide how to turn on the logger, and show what a successful startup is like.\r\n\r\nExample:\r\n\r\n... Found drill-metastore-override.conf in /path/to/file.\r\n... Metastore is configured to use the MySQL database with url ""..."", user ""bob"".\r\n... ERROR: MySQL JDBC driver not found on the class path.\r\n\r\nOr...\r\n\r\n... Connection to MySQL with url ""..."" failed. Is user ""bob"", no password correct?\r\n\r\nOr...\r\n\r\n\r\n... Database Oracle7 is not supported by Liquibase.\r\n\r\nOr\r\n\r\n... Unable to create Drill metastore table TABLES. Does user \'bob\' have correct permissions?', 'commenter': 'paul-rogers'}]"
2060,metastore/rdbms-metastore/src/main/java/org/apache/drill/metastore/rdbms/RdbmsMetastore.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.metastore.rdbms;
+
+import com.typesafe.config.Config;
+import com.zaxxer.hikari.HikariConfig;
+import com.zaxxer.hikari.HikariDataSource;
+import liquibase.Liquibase;
+import liquibase.database.Database;
+import liquibase.database.DatabaseFactory;
+import liquibase.database.jvm.JdbcConnection;
+import liquibase.resource.ClassLoaderResourceAccessor;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.metastore.Metastore;
+import org.apache.drill.metastore.components.tables.Tables;
+import org.apache.drill.metastore.components.views.Views;
+import org.apache.drill.metastore.config.MetastoreConfigConstants;
+import org.apache.drill.metastore.rdbms.components.tables.RdbmsTables;
+import org.apache.drill.metastore.rdbms.config.RdbmsConfigConstants;
+import org.apache.drill.metastore.rdbms.exception.RdbmsMetastoreException;
+import org.jooq.SQLDialect;
+import org.jooq.tools.jdbc.JDBCUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.sql.DataSource;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.sql.Connection;
+import java.util.Properties;
+
+/**
+ * RDBMS Drill Metastore implementation that creates necessary tables using Liquibase,
+ * initializes data source using provided config.
+ */
+public class RdbmsMetastore implements Metastore {
+
+  private static final Logger logger = LoggerFactory.getLogger(RdbmsMetastore.class);
+
+  private static final String LIQUIBASE_CHANGELOG_FILE = ""db/changelog/changelog.yaml"";
+
+  private final QueryExecutorProvider executorProvider;
+
+  public RdbmsMetastore(DrillConfig config) {
+    HikariDataSource dataSource = dataSource(config);
+    this.executorProvider = new QueryExecutorProvider(dataSource);
+    initTables(dataSource);
+  }
+
+  @Override
+  public Tables tables() {
+    return new RdbmsTables(executorProvider);
+  }
+
+  @Override
+  public Views views() {
+    throw new UnsupportedOperationException(""Views metadata support is not implemented"");
+  }
+
+  @Override
+  public void close() {
+    executorProvider.close();
+  }
+
+  /**
+   * Prepares database before initializing data source based on its type,
+   * initializes {@link HikariDataSource} instance and configures it based on given
+   * Metastore configuration.
+   * Basic parameters such as driver, url, user name and password are set using setters.
+   * Other source parameters are set dynamically through the properties. See the list
+   * of available Hikari properties: <a href=""https://github.com/brettwooldridge/HikariCP"">.
+   *
+   * @param config Metastore config
+   * @return Hikari data source instance
+   * @throws RdbmsMetastoreException if unable to configure Hikari data source
+   */
+  private HikariDataSource dataSource(DrillConfig config) {
+    prepareDatabase(config);
+    try {
+      Properties properties = new Properties();
+      if (config.hasPath(RdbmsConfigConstants.DATA_SOURCE_PROPERTIES)) {
+        Config propertiesConfig = config.getConfig(RdbmsConfigConstants.DATA_SOURCE_PROPERTIES);
+        propertiesConfig.entrySet().forEach(e -> properties.put(e.getKey(), e.getValue().unwrapped()));
+      }
+      HikariConfig hikariConfig = new HikariConfig(properties);
+      hikariConfig.setDriverClassName(config.getString(RdbmsConfigConstants.DATA_SOURCE_DRIVER));
+      hikariConfig.setJdbcUrl(config.getString(RdbmsConfigConstants.DATA_SOURCE_URL));
+      if (config.hasPath(RdbmsConfigConstants.DATA_SOURCE_USER_NAME)) {
+        hikariConfig.setUsername(config.getString(RdbmsConfigConstants.DATA_SOURCE_USER_NAME));
+      }
+      if (config.hasPath(RdbmsConfigConstants.DATA_SOURCE_PASSWORD)) {
+        hikariConfig.setPassword(config.getString(RdbmsConfigConstants.DATA_SOURCE_PASSWORD));
+      }
+      return new HikariDataSource(hikariConfig);
+    } catch (RuntimeException e) {
+      throw new RdbmsMetastoreException(""Unable to init RDBMS Metastore data source: "" + e.getMessage(), e);","[{'comment': ""This is exactly the kind of information we'll need when a user on the mailing list runs into problems. But, we're not providing much context. Consider using `UserException` so that we can add lots of context and to ensure the error is logged."", 'commenter': 'paul-rogers'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -147,10 +185,15 @@ private void openFile(FileScanFramework.FileSchemaNegotiator negotiator) {
       fsStream = negotiator.fileSystem().openPossiblyCompressedStream(split.getPath());
 
       // Open streaming reader
-      workbook = StreamingReader.builder()
+      Workbook workbook = StreamingReader.builder()
         .rowCacheSize(ROW_CACHE_SIZE)
         .bufferSize(BUFFER_SIZE)
+        .setReadCoreProperties(true)
         .open(fsStream);
+
+      swb =(StreamingWorkbook)workbook;","[{'comment': '```suggestion\r\n      swb = (StreamingWorkbook) workbook;\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -75,7 +83,9 @@
 
   private Row currentRow;
 
-  private Workbook workbook;
+  private StreamingWorkbook swb;","[{'comment': 'Is there any reason for using specific implementation instead of the interface here?', 'commenter': 'vvysotskyi'}, {'comment': 'The initial name was clearer.', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi \r\nThe way the underlying library is written, in order to access the metadata, we need the workbook to be a `StreamingWorkbook` type, but for whatever reason, the reader only returns the `Workbook` type.  ', 'commenter': 'cgivre'}, {'comment': 'Renamed `streamingWorkbook`', 'commenter': 'cgivre'}, {'comment': ""This class uses only methods from the `Workbook` interface, so I don't see any reason for narrowing down the type to `StreamingWorkbook` here and adding explicit casts for this type."", 'commenter': 'vvysotskyi'}, {'comment': ""The `getCoreProperties()` unfortunately is not in the `Workbook` class.  I'm remembering now that was why I had to make that cast. "", 'commenter': 'cgivre'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -75,7 +83,9 @@
 
   private Row currentRow;
 
-  private Workbook workbook;
+  private StreamingWorkbook swb;
+
+  private CoreProperties fileMetadata;","[{'comment': 'This field is obtained from the previous one. Are there any reasons for adding it?', 'commenter': 'vvysotskyi'}, {'comment': 'Yeah, so both the workbook and the `fileMetadata` fields are called a few times later in the reader.  It seemed to make sense to create class variables for them.', 'commenter': 'cgivre'}, {'comment': 'It is called in a single method only. May be declared a local variable in this method instead of adding the class field.', 'commenter': 'vvysotskyi'}, {'comment': 'Refactored with `fileMetadata` as local variable.', 'commenter': 'cgivre'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -199,7 +245,7 @@ private void getColumnHeaders(SchemaBuilder builder) {
         i++;
       }
       columnWriters = new ArrayList<>(columnCount);
-
+      metadataColumnWriters = new ArrayList<>();","[{'comment': 'This field is initialized here, and after the if-else block, it is initialized with the same value. And `columnWriters` value also.', 'commenter': 'vvysotskyi'}, {'comment': 'Moved initialization to top of method.', 'commenter': 'cgivre'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -334,9 +382,18 @@ private boolean nextLine(RowSetLoader rowWriter) {
       colPosition++;
     }
 
+    if (firstLine) {","[{'comment': 'This if may be combined with the next one.', 'commenter': 'vvysotskyi'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -348,6 +405,27 @@ private boolean nextLine(RowSetLoader rowWriter) {
     }
   }
 
+  private void populateMetadata() {
+    stringMetadata = new HashMap<>();
+    dateMetadata = new HashMap<>();
+
+    stringMetadata.put(IMPLICIT_STRING_COLUMN_NAMES.get(0), fileMetadata.getCategory());","[{'comment': 'Please do not rely on the index of the specific column. You may define an enum with implicit excel field names and use its elements here and iterate through all its elements where required instead of using the list.', 'commenter': 'vvysotskyi'}, {'comment': 'Refactored with enum.', 'commenter': 'cgivre'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -420,15 +510,57 @@ private void addColumnToArray(TupleWriter rowWriter, String name, TypeProtos.Min
     }
   }
 
+  private void addMetadataWriters() {
+    for (String colName : IMPLICIT_STRING_COLUMN_NAMES) {
+      addMetadataColumnsToArray(rowWriter, colName, MinorType.VARCHAR);
+    }
+    for (String colName : IMPLICIT_TIMESTAMP_COLUMN_NAMES) {
+      addMetadataColumnsToArray(rowWriter, colName, MinorType.TIMESTAMP);
+    }
+  }
+
+  private void addMetadataColumnsToArray(TupleWriter rowWriter, String name, MinorType type) {
+    int index = rowWriter.tupleSchema().index(name);
+    if (index == -1) {
+      ColumnMetadata colSchema = MetadataUtils.newScalar(name, type, TypeProtos.DataMode.OPTIONAL);
+      colSchema.setBooleanProperty(ColumnMetadata.EXCLUDE_FROM_WILDCARD, true);
+      index = rowWriter.addColumn(colSchema);
+    } else {
+      return;
+    }","[{'comment': 'This part of the code is similar to the code from `addColumnToArray` method. Please refactor it to avoid code duplication.', 'commenter': 'vvysotskyi'}, {'comment': 'I consolidated these two methods. ', 'commenter': 'cgivre'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -420,15 +510,57 @@ private void addColumnToArray(TupleWriter rowWriter, String name, TypeProtos.Min
     }
   }
 
+  private void addMetadataWriters() {
+    for (String colName : IMPLICIT_STRING_COLUMN_NAMES) {
+      addMetadataColumnsToArray(rowWriter, colName, MinorType.VARCHAR);
+    }
+    for (String colName : IMPLICIT_TIMESTAMP_COLUMN_NAMES) {
+      addMetadataColumnsToArray(rowWriter, colName, MinorType.TIMESTAMP);
+    }
+  }
+
+  private void addMetadataColumnsToArray(TupleWriter rowWriter, String name, MinorType type) {
+    int index = rowWriter.tupleSchema().index(name);
+    if (index == -1) {
+      ColumnMetadata colSchema = MetadataUtils.newScalar(name, type, TypeProtos.DataMode.OPTIONAL);
+      colSchema.setBooleanProperty(ColumnMetadata.EXCLUDE_FROM_WILDCARD, true);
+      index = rowWriter.addColumn(colSchema);
+    } else {
+      return;
+    }
+    metadataColumnWriters.add(rowWriter.scalar(index));
+  }
+
+  private void writeMetadata() {
+    for (int i = 0; i < metadataColumnWriters.size(); i++) {","[{'comment': '`metadataColumnWriters` is populated using `IMPLICIT_STRING_COLUMN_NAMES` and `IMPLICIT_TIMESTAMP_COLUMN_NAMES` lists. Is it possible instead of iterating through `metadataColumnWriters` values and adding a check for the index, iterate through values of the lists only?', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi \r\nI refactored this method considerably now that it uses enums.  I also followed @Sanel's advice and used a much simpler counter. "", 'commenter': 'cgivre'}, {'comment': 'I meant something like this:\r\n```\r\n    for (IMPLICIT_STRING_COLUMNS column : IMPLICIT_STRING_COLUMNS.values()) {\r\n      String value = stringMetadata.get(column);\r\n      int index = column.ordinal();\r\n      if (value == null) {\r\n        metadataColumnWriters.get(index).setNull();\r\n      } else {\r\n        metadataColumnWriters.get(index).setString(value);\r\n      }\r\n    }\r\n\r\n    for (IMPLICIT_TIMESTAMP_COLUMNS column : IMPLICIT_TIMESTAMP_COLUMNS.values()) {\r\n      Date timeValue = dateMetadata.get(column);\r\n      int index = column.ordinal() + IMPLICIT_STRING_COLUMNS.values().length;\r\n      if (timeValue == null) {\r\n        metadataColumnWriters.get(index).setNull();\r\n      } else {\r\n        metadataColumnWriters.get(index).setTimestamp(new Instant(timeValue));\r\n      }\r\n    }\r\n```', 'commenter': 'vvysotskyi'}]"
2069,contrib/format-excel/src/test/java/org/apache/drill/exec/store/excel/TestExcelFormat.java,"@@ -112,6 +112,42 @@ public void testExplicitAllQuery() throws RpcException {
     new RowSetComparison(expected).verifyAndClearAll(results);
   }
 
+
+  @Test
+  public void testExplicitMetadataQuery() throws RpcException {
+    String sql =
+      ""SELECT _category, _content_status, _content_type, _creator, _description, _identifier, _keywords, _last_modified_by_user, _revision, _subject, _title, _created,"" +
+        ""_last_printed, _modified FROM cp.`excel/test_data.xlsx` LIMIT 1"";
+
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+    results.print();","[{'comment': 'Please remove printing the result.', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -420,15 +510,57 @@ private void addColumnToArray(TupleWriter rowWriter, String name, TypeProtos.Min
     }
   }
 
+  private void addMetadataWriters() {
+    for (String colName : IMPLICIT_STRING_COLUMN_NAMES) {
+      addMetadataColumnsToArray(rowWriter, colName, MinorType.VARCHAR);
+    }
+    for (String colName : IMPLICIT_TIMESTAMP_COLUMN_NAMES) {
+      addMetadataColumnsToArray(rowWriter, colName, MinorType.TIMESTAMP);
+    }
+  }
+
+  private void addMetadataColumnsToArray(TupleWriter rowWriter, String name, MinorType type) {
+    int index = rowWriter.tupleSchema().index(name);
+    if (index == -1) {
+      ColumnMetadata colSchema = MetadataUtils.newScalar(name, type, TypeProtos.DataMode.OPTIONAL);
+      colSchema.setBooleanProperty(ColumnMetadata.EXCLUDE_FROM_WILDCARD, true);
+      index = rowWriter.addColumn(colSchema);
+    } else {
+      return;
+    }
+    metadataColumnWriters.add(rowWriter.scalar(index));
+  }
+
+  private void writeMetadata() {
+    for (int i = 0; i < metadataColumnWriters.size(); i++) {
+      if (i < IMPLICIT_STRING_COLUMN_NAMES.size()) {","[{'comment': ""Why just don't say something like:\r\n\r\n```java\r\nint n = i - IMPLICIT_STRING_COLUMN_NAMES.size();\r\nif (n < 0) {\r\n ...\r\n} else {\r\n ...\r\n ... = dateMetadata.get(IMPLICIT_TIMESTAMP_COLUMN_NAMES.get(n);\r\n...\r\n}\r\n```"", 'commenter': 'sanel'}, {'comment': '@Sanel, I like that.  Done!\r\n', 'commenter': 'cgivre'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -56,60 +61,70 @@
   private static final Logger logger = org.slf4j.LoggerFactory.getLogger(ExcelBatchReader.class);
 
   private static final String SAFE_WILDCARD = ""_$"";
-
   private static final String SAFE_SEPARATOR = ""_"";
-
   private static final String PARSER_WILDCARD = "".*"";
-
   private static final String HEADER_NEW_LINE_REPLACEMENT = ""__"";
-
   private static final String MISSING_FIELD_NAME_HEADER = ""field_"";
 
-  private static final int ROW_CACHE_SIZE = 100;
+  private enum IMPLICIT_STRING_COLUMNS {
+    _category,","[{'comment': 'Please use naming according to JCC.', 'commenter': 'vvysotskyi'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -56,60 +61,70 @@
   private static final Logger logger = org.slf4j.LoggerFactory.getLogger(ExcelBatchReader.class);
 
   private static final String SAFE_WILDCARD = ""_$"";
-
   private static final String SAFE_SEPARATOR = ""_"";
-
   private static final String PARSER_WILDCARD = "".*"";
-
   private static final String HEADER_NEW_LINE_REPLACEMENT = ""__"";
-
   private static final String MISSING_FIELD_NAME_HEADER = ""field_"";
 
-  private static final int ROW_CACHE_SIZE = 100;
+  private enum IMPLICIT_STRING_COLUMNS {","[{'comment': '```suggestion\r\n  private enum IMPLICIT_STRING_COLUMN {\r\n```', 'commenter': 'vvysotskyi'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -56,60 +61,70 @@
   private static final Logger logger = org.slf4j.LoggerFactory.getLogger(ExcelBatchReader.class);
 
   private static final String SAFE_WILDCARD = ""_$"";
-
   private static final String SAFE_SEPARATOR = ""_"";
-
   private static final String PARSER_WILDCARD = "".*"";
-
   private static final String HEADER_NEW_LINE_REPLACEMENT = ""__"";
-
   private static final String MISSING_FIELD_NAME_HEADER = ""field_"";
 
-  private static final int ROW_CACHE_SIZE = 100;
+  private enum IMPLICIT_STRING_COLUMNS {
+    _category,
+    _content_status,
+    _content_type,
+    _creator,
+    _description,
+    _identifier,
+    _keywords,
+    _last_modified_by_user,
+    _revision,
+    _subject,
+    _title
+  }
+
+  private enum IMPLICIT_TIMESTAMP_COLUMNS {","[{'comment': '```suggestion\r\n  private enum IMPLICIT_TIMESTAMP_COLUMN {\r\n```', 'commenter': 'vvysotskyi'}]"
2069,contrib/format-excel/src/main/java/org/apache/drill/exec/store/excel/ExcelBatchReader.java,"@@ -56,60 +61,70 @@
   private static final Logger logger = org.slf4j.LoggerFactory.getLogger(ExcelBatchReader.class);
 
   private static final String SAFE_WILDCARD = ""_$"";
-
   private static final String SAFE_SEPARATOR = ""_"";
-
   private static final String PARSER_WILDCARD = "".*"";
-
   private static final String HEADER_NEW_LINE_REPLACEMENT = ""__"";
-
   private static final String MISSING_FIELD_NAME_HEADER = ""field_"";
 
-  private static final int ROW_CACHE_SIZE = 100;
+  private enum IMPLICIT_STRING_COLUMNS {
+    _category,
+    _content_status,
+    _content_type,
+    _creator,
+    _description,
+    _identifier,
+    _keywords,
+    _last_modified_by_user,
+    _revision,
+    _subject,
+    _title
+  }
+
+  private enum IMPLICIT_TIMESTAMP_COLUMNS {
+    /**
+     * The file created date
+     */
+    _created,","[{'comment': 'And here also.', 'commenter': 'vvysotskyi'}]"
2075,exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java,"@@ -250,7 +250,7 @@ private void showSingleBatch (VectorAccessibleSerializable vcSerializable, boole
       System.out.println(getBatchMetaInfo(vcSerializable).toString());
 
       System.out.println(""Schema Information"");
-      for (final VectorWrapper w : vectorContainer) {
+      for (final VectorWrapper<?> w : vectorContainer) {
         final MaterializedField field = w.getValueVector().getField();
         System.out.println (String.format(""name : %s, minor_type : %s, data_mode : %s"",","[{'comment': 'Do we want a `System.out.println()` here?', 'commenter': 'cgivre'}, {'comment': '`DumpCat` is a command-line tool. All I did here was fix some warnings.', 'commenter': 'paul-rogers'}, {'comment': 'Ah ok.  ', 'commenter': 'cgivre'}]"
2075,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/LogsResources.java,"@@ -190,9 +190,9 @@ public int compareTo(Log log) {
 
   @XmlRootElement
   public class LogContent {
-    private String name;
-    private Collection<String> lines;
-    private int maxLines;
+    private final String name;
+    private final Collection<String> lines;
+    private final int maxLines;
 
     @JsonCreator
     public LogContent (@JsonProperty(""name"") String name, @JsonProperty(""lines"") Collection<String> lines, @JsonProperty(""maxLines"") int maxLines) {","[{'comment': 'nit: New lines here perhaps?', 'commenter': 'cgivre'}]"
2076,exec/vector/src/main/java/org/apache/drill/exec/vector/DateUtilities.java,"@@ -195,7 +197,27 @@ public static int timeToMillis(int hours, int minutes, int seconds, int millis)
    * @param localTime Java local time
    * @return Drill form of the time","[{'comment': '@paul-rogers \r\nWould you mind please adding a brief javadoc to this class explaining what a Drill form of the time is?', 'commenter': 'cgivre'}, {'comment': 'Turns out the format is explained two lines above, but repeated it in the `@return` as requested.', 'commenter': 'paul-rogers'}]"
2076,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/scan/convert/StandardConversions.java,"@@ -166,6 +171,12 @@ public static DirectConverter newInstance(
     try {
       final Constructor<? extends DirectConverter> ctor = conversionClass.getDeclaredConstructor(ScalarWriter.class, Map.class);
       return ctor.newInstance(baseWriter, properties);
+    } catch (final InvocationTargetException e) {
+      throw UserException.validationError(e.getTargetException())
+        .addContext(""Converter setup failed"")
+        .addContext(""Conversion class"" + conversionClass.getName())
+        // .addContext(errorContext) // Add after merge","[{'comment': 'Should we add this line back in?', 'commenter': 'cgivre'}, {'comment': 'As the comment (tersely) suggests, the error context can be added after the merge of a different PR which provides the error context. This comment will show up in a merge conflict and remind me to make that change.', 'commenter': 'paul-rogers'}]"
2078,exec/java-exec/src/main/java/org/apache/calcite/jdbc/DynamicRootSchema.java,"@@ -46,12 +47,14 @@
 public class DynamicRootSchema extends DynamicSchema {
   private static final Logger logger = LoggerFactory.getLogger(DynamicRootSchema.class);
 
+  private static final String ROOT_SCHEMA_NAME = """";","[{'comment': ""@vvysotskyi \r\nIs there a way to get this PR to do what you want without having to maintain a modified version of Calcite?  My concern is that as Drill progresses, it will be much easier to maintain if we don't have to maintain a custom version of Calcite. "", 'commenter': 'cgivre'}, {'comment': 'This is not a Calcite class. It is just Drill class, placed in the Calcite package to be able to access some classes and fields. It was introduced in fix for DRILL-5089.', 'commenter': 'vvysotskyi'}, {'comment': 'Ah ok.  In that case, LGTM +1', 'commenter': 'cgivre'}]"
2096,_docs/configure-drill/configuration-options/030-planning-and-exececution-options.md,"@@ -27,11 +27,11 @@ You can run the following query to see a list of options:
 The query returns a table that lists options with descriptions and other details. As of Drill 1.15, there are 179 options:  
 
 	SELECT COUNT() AS num_of_sysopts FROM sys.options;
-	+-----------------+
+	|-----------------|
 	| num_of_sysopts  |
-	+-----------------+
+	|-----------------|
 	| 179             |
-	+-----------------+  
+	|-----------------|  ","[{'comment': 'Nit: `###Drill-override.conf` Row Limit Settings is not formatted correctly.', 'commenter': 'cgivre'}, {'comment': ""I couldn't see any _major_ problem here.  I've corrected `###Drill-override...` to `### Drill-override...`"", 'commenter': 'jnturton'}]"
2096,_docs/connect-a-data-source/plugins/114-image-metadata-format-plugin.md,"@@ -1,6 +1,6 @@
 ---
 title: ""Image Metadata Format Plugin""","[{'comment': 'IMHO, We could use a little re-organization here.  This particular page is about querying image metadata via Drill. We should collect any pages about formats under the section about querying the file system instead of treating this like it was a storage plugin.', 'commenter': 'cgivre'}, {'comment': ""Absolutely.  I've moved the format plugins away from the storage plugins, and under Data Sources and File Formats which seems to me to be the best available place.  I think there are further other reorganisation opportunities like this, but they can be for future PRs."", 'commenter': 'jnturton'}]"
2112,contrib/format-httpd/README.md,"@@ -0,0 +1,32 @@
+# Web Server Log Format Plugin (HTTPD)
+This plugin enables Drill to read and query httpd (Apache Web Server) and nginx logs natively. This plugin uses the work by [Niels Basjes](https://github.com/nielsbasjes) which is available here: https://github.com/nielsbasjes/logparser.
+
+## Configuration
+There are three fields which you will need to configure in order for Drill to read web server logs which are:
+* **`logFormat`**:  The log format string is the format string found in your web server configuration.
+* **`timestampFormat`**:  The format of time stamps in your log files.
+* **`extensions`**:  The file extension of your web server logs.
+* **`maxErrors`**:  Sets the plugin error tolerence. When set to any value less than `0`, Drill will ignore all errors. ","[{'comment': 'tolerance', 'commenter': 'nielsbasjes'}, {'comment': 'Add something like: `If unspecified then maxErrors is 0 which will cause the query to fail on the first error.`', 'commenter': 'nielsbasjes'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2112,contrib/format-httpd/README.md,"@@ -0,0 +1,32 @@
+# Web Server Log Format Plugin (HTTPD)
+This plugin enables Drill to read and query httpd (Apache Web Server) and nginx logs natively. This plugin uses the work by [Niels Basjes](https://github.com/nielsbasjes) which is available here: https://github.com/nielsbasjes/logparser.
+
+## Configuration
+There are three fields which you will need to configure in order for Drill to read web server logs which are:
+* **`logFormat`**:  The log format string is the format string found in your web server configuration.","[{'comment': 'Add something like: `If you have multiple sufficiently different logFormats then you can add all of them in this single parameter separated by a newline. The parser will automatically select the first matching format.`', 'commenter': 'nielsbasjes'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2112,contrib/format-httpd/README.md,"@@ -0,0 +1,32 @@
+# Web Server Log Format Plugin (HTTPD)
+This plugin enables Drill to read and query httpd (Apache Web Server) and nginx logs natively. This plugin uses the work by [Niels Basjes](https://github.com/nielsbasjes) which is available here: https://github.com/nielsbasjes/logparser.
+
+## Configuration
+There are three fields which you will need to configure in order for Drill to read web server logs which are:
+* **`logFormat`**:  The log format string is the format string found in your web server configuration.
+* **`timestampFormat`**:  The format of time stamps in your log files.","[{'comment': 'Add something like: `This setting is optional and is almost never needed.`', 'commenter': 'nielsbasjes'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2112,contrib/format-httpd/README.md,"@@ -0,0 +1,32 @@
+# Web Server Log Format Plugin (HTTPD)
+This plugin enables Drill to read and query httpd (Apache Web Server) and nginx logs natively. This plugin uses the work by [Niels Basjes](https://github.com/nielsbasjes) which is available here: https://github.com/nielsbasjes/logparser.
+
+## Configuration
+There are three fields which you will need to configure in order for Drill to read web server logs which are:
+* **`logFormat`**:  The log format string is the format string found in your web server configuration.
+* **`timestampFormat`**:  The format of time stamps in your log files.
+* **`extensions`**:  The file extension of your web server logs.","[{'comment': 'Add something like: `If unspecified the default file extension .httpd is assumed.`', 'commenter': 'nielsbasjes'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2112,contrib/format-httpd/README.md,"@@ -0,0 +1,32 @@
+# Web Server Log Format Plugin (HTTPD)
+This plugin enables Drill to read and query httpd (Apache Web Server) and nginx logs natively. This plugin uses the work by [Niels Basjes](https://github.com/nielsbasjes) which is available here: https://github.com/nielsbasjes/logparser.
+
+## Configuration
+There are three fields which you will need to configure in order for Drill to read web server logs which are:
+* **`logFormat`**:  The log format string is the format string found in your web server configuration.
+* **`timestampFormat`**:  The format of time stamps in your log files.
+* **`extensions`**:  The file extension of your web server logs.
+* **`maxErrors`**:  Sets the plugin error tolerence. When set to any value less than `0`, Drill will ignore all errors. 
+
+```json
+""httpd"" : {
+  ""type"" : ""httpd"",
+  ""logFormat"" : ""%h %l %u %t \""%r\"" %s %b \""%{Referer}i\"" \""%{User-agent}i\"""",
+  ""timestampFormat"" : ""dd/MMM/yyyy:HH:mm:ss ZZ"",
+  ""maxErrors"": 0
+}","[{'comment': 'I would remove the timestampFormat and add extensions and a second logformat to this example.', 'commenter': 'nielsbasjes'}, {'comment': 'I removed the default. ', 'commenter': 'cgivre'}]"
2112,contrib/format-httpd/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java,"@@ -17,33 +17,44 @@
  */
 package org.apache.drill.exec.store.httpd;
 
-import java.util.Objects;
-
-import org.apache.drill.common.PlanStringBuilder;
-import org.apache.drill.common.logical.FormatPluginConfig;
-
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
 
-@JsonTypeName(""httpd"")
+@JsonTypeName(HttpdLogFormatPlugin.DEFAULT_NAME)
 @JsonInclude(JsonInclude.Include.NON_DEFAULT)
 public class HttpdLogFormatConfig implements FormatPluginConfig {
 
   public static final String DEFAULT_TS_FORMAT = ""dd/MMM/yyyy:HH:mm:ss ZZ"";
-
-  // No extensions?
   private final String logFormat;
   private final String timestampFormat;
+  private final List<String> extensions;
+  private final int maxErrors;
+
 
   @JsonCreator
   public HttpdLogFormatConfig(
+      @JsonProperty(""extensions"") List<String> extensions,
       @JsonProperty(""logFormat"") String logFormat,
-      @JsonProperty(""timestampFormat"") String timestampFormat) {
+      @JsonProperty(""timestampFormat"") String timestampFormat,
+      @JsonProperty(""maxErrors"") int maxErrors
+  ) {
+
+    this.extensions = extensions == null
+      ? Collections.singletonList(""httpd"")
+      : ImmutableList.copyOf(extensions);
     this.logFormat = logFormat;
     this.timestampFormat = timestampFormat == null","[{'comment': ""I would simplify this to:     this.timestampFormat = timestampFormat;\r\nThe underlying logparser already handles the 'null' and 'empty' cases.\r\nSee: https://github.com/nielsbasjes/logparser/blob/master/httpdlog/httpdlog-parser/src/main/java/nl/basjes/parse/httpdlog/dissectors/TimeStampDissector.java#L65\r\n\r\nI haven't looked yet but I assume there are tests for these kinds of scenarios?\r\n"", 'commenter': 'nielsbasjes'}]"
2112,contrib/format-httpd/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java,"@@ -17,33 +17,44 @@
  */
 package org.apache.drill.exec.store.httpd;
 
-import java.util.Objects;
-
-import org.apache.drill.common.PlanStringBuilder;
-import org.apache.drill.common.logical.FormatPluginConfig;
-
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
 
-@JsonTypeName(""httpd"")
+@JsonTypeName(HttpdLogFormatPlugin.DEFAULT_NAME)
 @JsonInclude(JsonInclude.Include.NON_DEFAULT)
 public class HttpdLogFormatConfig implements FormatPluginConfig {
 
   public static final String DEFAULT_TS_FORMAT = ""dd/MMM/yyyy:HH:mm:ss ZZ"";","[{'comment': 'Remove (see comment below)', 'commenter': 'nielsbasjes'}, {'comment': 'Default removed', 'commenter': 'cgivre'}]"
2112,contrib/format-httpd/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java,"@@ -0,0 +1,188 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.httpd;
+
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
+import nl.basjes.parse.core.Casts;
+import nl.basjes.parse.core.Parser;
+import nl.basjes.parse.core.exceptions.DissectionFailure;
+import nl.basjes.parse.core.exceptions.InvalidDissectorException;
+import nl.basjes.parse.core.exceptions.MissingDissectorsException;
+import nl.basjes.parse.httpdlog.HttpdLoglineParser;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+public class HttpdParser {
+
+  private static final Logger logger = LoggerFactory.getLogger(HttpdParser.class);
+
+  public static final String PARSER_WILDCARD = "".*"";
+  public static final String REMAPPING_FLAG = ""#"";
+
+  private final Parser<HttpdLogRecord> parser;
+  private Map<String, String> requestedPaths;
+  private final Map<String, MinorType> mappedColumns;
+  private EnumSet<Casts> casts;
+  private final HttpdLogRecord record;
+  private final String logFormat;
+
+
+  public HttpdParser(final String logFormat, final String timestampFormat) {
+
+    Preconditions.checkArgument(logFormat != null && !logFormat.trim().isEmpty(), ""logFormat cannot be null or empty"");
+
+    this.logFormat = logFormat;
+    this.record = new HttpdLogRecord(timestampFormat);
+    this.parser = new HttpdLoglineParser<>(HttpdLogRecord.class, logFormat, timestampFormat);
+    if (timestampFormat != null && !timestampFormat.trim().isEmpty()) {
+      logger.info(""Custom timestamp format has been specified. This is an informational note only as custom timestamps is rather unusual."");
+    }
+    if (logFormat.contains(""\n"")) {
+      logger.info(""Specified logformat is a multiline log format: {}"", logFormat);
+    }
+
+    mappedColumns = new HashMap<>();
+  }
+
+  /**
+   * We do not expose the underlying parser or the record which is used to manage the writers.
+   *
+   * @param line log line to tear apart.
+   * @throws DissectionFailure
+   * @throws InvalidDissectorException
+   * @throws MissingDissectorsException
+   */
+  public void parse(final String line) throws DissectionFailure, InvalidDissectorException, MissingDissectorsException {
+    parser.parse(record, line);
+    record.finishRecord();
+  }
+
+  /*
+   * The parser deals with dots unlike Drill wanting underscores request_referer. For the sake of simplicity we are
+   * going replace the dots. The resultant output field will look like: request.referer.<br>
+   * Additionally, wild cards will get replaced with .*
+   *
+   * @param drillFieldName name to be cleansed.
+   * @return cleaned string
+   */
+ /* public static String parserFormattedFieldName(String drillFieldName) {
+    String tempFieldName;
+    tempFieldName = LOGFIELDS.get(drillFieldName);
+    return tempFieldName.replace(SAFE_WILDCARD, PARSER_WILDCARD).replaceAll(SAFE_SEPARATOR, ""."").replaceAll(""\\.\\."", ""_"");
+  }*/
+
+  public TupleMetadata setupParser()
+          throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {
+
+    SchemaBuilder builder = new SchemaBuilder();
+
+    /*
+     * If the user has selected fields, then we will use them to configure the parser because this would be the most
+     * efficient way to parse the log.
+     */
+    List<String> allParserPaths = parser.getPossiblePaths();
+
+    /*
+     * Use all possible paths that the parser has determined from the specified log format.
+     */
+
+    requestedPaths = Maps.newHashMap();
+    for (final String parserPath : allParserPaths) {
+      requestedPaths.put(HttpdUtils.drillFormattedFieldName(parserPath), parserPath);
+    }
+
+    /*
+     * By adding the parse target to the dummy instance we activate it for use. Which we can then use to find out which
+     * paths cast to which native data types. After we are done figuring this information out, we throw this away
+     * because this will be the slowest parsing path possible for the specified format.
+     */
+    Parser<Object> dummy = new HttpdLoglineParser<>(Object.class, logFormat);
+
+    // TODO Don't we want requested paths here... not the all possible","[{'comment': 'If I understand this TODO correctly then this would be a rather big performance issue.', 'commenter': 'nielsbasjes'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2112,contrib/format-httpd/README.md,"@@ -0,0 +1,32 @@
+# Web Server Log Format Plugin (HTTPD)
+This plugin enables Drill to read and query httpd (Apache Web Server) and nginx logs natively. This plugin uses the work by [Niels Basjes](https://github.com/nielsbasjes) which is available here: https://github.com/nielsbasjes/logparser.","[{'comment': 'It only does Access logs (no error logs or any other kind of logs)', 'commenter': 'nielsbasjes'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2117,exec/vector/src/main/java/org/apache/drill/exec/record/MaterializedField.java,"@@ -189,7 +189,7 @@ public MaterializedField getOtherNullableVersion() {
 
   @Override
   public int hashCode() {
-    return Objects.hash(this.name, this.type, this.children);
+    return Objects.hash(this.name.toLowerCase(), this.type);","[{'comment': 'is it guaranteed that name is never null? ', 'commenter': 'ihuzenko'}]"
2122,contrib/format-httpd/README.md,"@@ -10,15 +10,16 @@ There are five fields which you can to configure in order for Drill to read web
 * **`extensions`**:  The file extension of your web server logs.  Defaults to `httpd`.
 * **`maxErrors`**:  Sets the plugin error tolerance. When set to any value less than `0`, Drill will ignore all errors. If unspecified then maxErrors is 0 which will cause the query to fail on the first error.
 * **`flattenWildcards`**: There are a few variables which Drill extracts into maps.  Defaults to `false`.
-
+* **`enableUserAgentParser`**:  This enables the additional Yauaa useragent parser. If unspecified this is disabled because there is a noticeable startup overhead of this plugin, even if not used.
 ","[{'comment': 'I wonder if we could do something here to only activate this function if:\r\n1.  The user executes a star query OR\r\n2.  They specified a field with `user-agent` in the field name?\r\n\r\nDo you think that would work?  \r\n\r\n', 'commenter': 'cgivre'}, {'comment': 'It might work.\r\n\r\nThe underlying logparser works on the idea of a type. The useragent parser hooks to the ""HTTP.USERAGENT"" type and allows dissecting anything of that type, regardless of the name. \r\n\r\nPractically speaking:\r\nThe logparser code only maps the request header ""user-agent"" to field ""request.user-agent"" with type ""HTTP.USERAGENT""\r\nSo unless someone uses the type remapping to extract the useragent from a different field this is the only naturally occurring place.\r\n\r\nIf you do it that way you are working under an assumption about the name of the column instead of the internal types.\r\n\r\nYou then also have to take into account that these do not need the extra step:\r\n\r\n    request_user-agent\r\n    request_user-agent_last\r\n\r\nand these do\r\n\r\n    request_user-agent_operating__system__version__major\r\n    request_user-agent_last_operating__system__version__major\r\n\r\nSo what I think is that you can activate it automatically if someone does `*` or asks for something as mentioned above.\r\nYet I do think that if any type remapping is done it should also be activated if something is mapped to the input type of the UserAgentDissector.\r\n\r\nIf there are cases where you activate the plugin when it is not needed then the only downside of this is that you\'ll have extra startup time and extra memory usage.\r\n\r\nThe actual parsing speed is not affected because the logparser will simply not do any of the Yauaa code if it not needed.\r\n\r\n', 'commenter': 'nielsbasjes'}]"
2122,contrib/format-httpd/README.md,"@@ -41,24 +45,67 @@ FROM dfs.test.`logfile.httpd` AS mylogs
 
 ```
 In this example, we assign an alias of `mylogs` to the table, the column name is `request_firstline_uri_query_$` and then the individual field within that mapping is `username
-`.  This particular example enables you to analyze items in query strings.  
+`.  This particular example enables you to analyze items in query strings.
 
 ### Flattening Maps
-In the event that you have a map field that you would like broken into columns rather than getting the nested fields, you can set the `flattenWildcards` option to `true` and 
-Drill will create columns for these fields.  For example if you have a URI Query option called `username`.  If you selected the `flattedWildcards` option, Drill will create a 
-field called `request_firstline_uri_query_username`.  
+In the event that you have a map field that you would like broken into columns rather than getting the nested fields, you can set the `flattenWildcards` option to `true` and
+Drill will create columns for these fields.  For example if you have a URI Query option called `username`.  If you selected the `flattedWildcards` option, Drill will create a
+field called `request_firstline_uri_query_username`.
 
-** Note that underscores in the field name are replaced with double underscores ** 
- 
- ## Useful Functions
+** Note that underscores in the field name are replaced with double underscores **
+
+## Useful Functions
  If you are using Drill to analyze web access logs, there are a few other useful functions which you should know about:
- 
+
  * `parse_url(<url>)`: This function accepts a URL as an argument and returns a map of the URL's protocol, authority, host, and path.
  * `parse_query(<query_string>)`: This function accepts a query string and returns a key/value pairing of the variables submitted in the request.
  * `parse_user_agent(<user agent>)`, `parse_user_agent( <useragent field>, <desired field> )`: The function parse_user_agent() takes a user agent string as an argument and
-  returns a map of the available fields. Note that not every field will be present in every user agent string. 
+  returns a map of the available fields. Note that not every field will be present in every user agent string.
   [Complete Docs Here](https://github.com/apache/drill/tree/master/contrib/udfs#user-agent-functions)
- 
+
+## LogParser type remapping
+**Advanced feature**
+The underlying [logparser](https://github.com/nielsbasjes/logparser) supports something called type remapping.
+Essentially it means that an extracted value which would normally be treated as an unparsable STRING can now be 'cast' to something
+that can be further cut into relevant pieces.
+
+The parameter string is a `;` separated list of mappings.
+Each mapping is a `:` separated list of
+- the name of the underlying logparser field (which is different from th Drill column name),
+- the underlying `type` which is used to determine which additional Dissectors can be applied.
+- optionally the `cast` (one of `STRING`, `LONG`, `DOUBLE`) which may impact the type of the Drill column
+
+Examples:
+- If you have a query parameter in the URL called `ua` which is really the UserAgent string and you would like to parse this you can add
+`request.firstline.uri.query.ua:HTTP.USERAGENT`
+- If you have a query parameter in the URL called `timestamp` which is really the numerical timestamp (epoch milliseconds).
+The additional ""LONG"" will cause the returned value be a long which tells Drill the `TIME.EPOCH` is to be interpreted as a `TIMESTAMP` column.
+`request.firstline.uri.query.timestamp:TIME.EPOCH:LONG`
+","[{'comment': 'This is slick! 🥇 ', 'commenter': 'cgivre'}, {'comment': 'Yes, I was hoping you would like this feature to be available in Drill.\r\nNote that the name of the parameter does not matter. If it is called `foo` you can do `request.firstline.uri.query.foo:TIME.EPOCH:LONG` and it will work aswell.\r\n\r\nHaving this is especially useful if you are parsing web analytics `beacon` logs (where the real data is all in query parameters).\r\nIf you have a query parameter that is really the URL of your site then simply remap it to `HTTP.URI` and go from there.\r\n`request.firstline.uri.query.g:HTTP.URI`\r\n\r\nAnother nice example of this is what I do here with the MOD_UNIQUE_ID field ... for which a built in Dissector is available:\r\n\r\nhttps://github.com/apache/drill/blob/29edfec469c7de4cdd621fd4914dfb143d9a2784/contrib/format-httpd/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReaderUserAgent.java#L194\r\n\r\n', 'commenter': 'nielsbasjes'}, {'comment': ""Have a look at this file as an example of what I mean with a `beacon` log:\r\nhttps://github.com/nielsbasjes/logparser/blob/master/httpdlog/httpdlog-pigloader/src/test/resources/omniture-access.log\r\nHere the `g` query parameter is the real URL ... which in itself has query parameters of its own that I'm interested in for my analysis.\r\n\r\nThis is the kind of logfile that actually triggered me to write this library all those years ago.\r\n"", 'commenter': 'nielsbasjes'}]"
2122,contrib/format-httpd/README.md,"@@ -1,35 +1,39 @@
 # Web Server Log Format Plugin (HTTPD)
 This plugin enables Drill to read and query httpd (Apache Web Server) and nginx access logs natively. This plugin uses the work by [Niels Basjes](https://github.com/nielsbasjes
-) which is available here: https://github.com/nielsbasjes/logparser.
+) which is available here: https://github.com/nielsbasjes/logparser .","[{'comment': 'Nit:  Why the extra space?', 'commenter': 'cgivre'}, {'comment': 'Yes, sorry. I thought it would break the link.', 'commenter': 'nielsbasjes'}]"
2122,contrib/format-httpd/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java,"@@ -35,45 +36,61 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.ArrayList;
 import java.util.EnumSet;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.TreeMap;
+
+import static nl.basjes.parse.core.Casts.DOUBLE;
+import static nl.basjes.parse.core.Casts.DOUBLE_ONLY;
+import static nl.basjes.parse.core.Casts.LONG;
+import static nl.basjes.parse.core.Casts.LONG_ONLY;
+import static nl.basjes.parse.core.Casts.STRING;
+import static nl.basjes.parse.core.Casts.STRING_ONLY;
 
 public class HttpdParser {
 
   private static final Logger logger = LoggerFactory.getLogger(HttpdParser.class);
 
   public static final String PARSER_WILDCARD = "".*"";
-  public static final String REMAPPING_FLAG = ""#"";
   private final Parser<HttpdLogRecord> parser;
   private final List<SchemaPath> requestedColumns;
   private final Map<String, MinorType> mappedColumns;
+  private final Map<String, Casts> columnCasts;
   private final HttpdLogRecord record;
   private final String logFormat;
+  private final boolean parseUserAgent;
+  private final String logParserRemapping;
   private Map<String, String> requestedPaths;
-  private EnumSet<Casts> casts;
-
 
-  public HttpdParser(final String logFormat, final String timestampFormat, final boolean flattenWildcards, final EasySubScan scan) {
+  public HttpdParser(
+          final String logFormat,
+          final String timestampFormat,
+          final boolean flattenWildcards,
+          final boolean parseUserAgent,
+          final String logParserRemapping,
+          final EasySubScan scan) {
 
     Preconditions.checkArgument(logFormat != null && !logFormat.trim().isEmpty(), ""logFormat cannot be null or empty"");
 
     this.logFormat = logFormat;
+    this.parseUserAgent = parseUserAgent;
     this.record = new HttpdLogRecord(timestampFormat, flattenWildcards);
 
-    if (timestampFormat == null) {
-      this.parser = new HttpdLoglineParser<>(HttpdLogRecord.class, logFormat);
-    } else {
-      this.parser = new HttpdLoglineParser<>(HttpdLogRecord.class, logFormat, timestampFormat);","[{'comment': ""Will the parser throw errors or warnings if the `timestampFormat` is `null`?   I know you don't recommend that the user provides a custom `timestampFormat` so I was trying to optimize the code for that. "", 'commenter': 'cgivre'}, {'comment': 'No, the parser already handles this case: https://github.com/nielsbasjes/logparser/blob/master/httpdlog/httpdlog-parser/src/main/java/nl/basjes/parse/httpdlog/dissectors/TimeStampDissector.java#L65', 'commenter': 'nielsbasjes'}, {'comment': 'Perfect!', 'commenter': 'cgivre'}]"
2122,contrib/format-httpd/src/main/resources/bootstrap-format-plugins.json,"@@ -5,7 +5,7 @@
       ""formats"": {
         ""httpd"" : {
           ""type"" : ""httpd"",
-          ""logFormat"" : ""%h %l %u %t \""%r\"" %s %b \""%{Referer}i\"" \""%{User-agent}i\"""",
+          ""logFormat"" : ""common\ncombined"",","[{'comment': 'Nice!  I like this a lot better than having to specify the exact config.  Can we put something in the `README` about this?  Are there other canned configs we could use?', 'commenter': 'cgivre'}, {'comment': 'I already did that. Please check if it is enough documentation (and clear enough for others).', 'commenter': 'nielsbasjes'}]"
2122,contrib/format-httpd/README.md,"@@ -1,35 +1,39 @@
 # Web Server Log Format Plugin (HTTPD)
 This plugin enables Drill to read and query httpd (Apache Web Server) and nginx access logs natively. This plugin uses the work by [Niels Basjes](https://github.com/nielsbasjes
-) which is available here: https://github.com/nielsbasjes/logparser.
+) which is available here: https://github.com/nielsbasjes/logparser .
 
 ## Configuration
-There are five fields which you can to configure in order for Drill to read web server logs.  In general the defaults should be fine, however the fields are:
+There are several fields which you can specify in order for Drill to read web server logs. In general the defaults should be fine, however the fields are:
 * **`logFormat`**:  The log format string is the format string found in your web server configuration. If you have multiple logFormats then you can add all of them in this
  single parameter separated by a newline (`\n`). The parser will automatically select the first matching format.
+ Note that the well known formats `common`, `combined`, `combinedio`, `referer` and `agent` are also accepted as logFormat.","[{'comment': 'Here I already put some documentation about the builtin formats. Is this enough?', 'commenter': 'nielsbasjes'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";","[{'comment': 'Please use `ALL_CAPS` for constant names. ', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;","[{'comment': 'These two vars do not appear to be used.  Please remove if unnecessary.  Specifically `opUserName` and `queryUserName`.', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();","[{'comment': ""After you've defined the `split`, you should also define the `errorContext`.\r\n```java\r\nerrorContext = negotiator.parentErrorContext();\r\n```\r\nOnce you've done that, you can include the `errorContext` in all exceptions which gives the user a lot more information about the exception. \r\n"", 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(","[{'comment': 'Please consider throwing a `UserException` here (and elsewhere).', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);","[{'comment': 'You should set the `isComplete` parameter here to `true` because you are not adding columns to the schema.  ', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);
+    logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    // open Sequencefile
+    try {
+      processReader(negotiator);
+    } catch (ExecutionSetupException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Unable to open Sequencefile %s"", split.getPath())
+        .addContext(e.getMessage())","[{'comment': ""You can remove the call to `e.getMessage()` here as it will be overwritten in the following `addContext()` call.  Also, I'd suggest moving the `e.getMessage()` to the actual `message()` call.  The file information will be provided via the `errorContext` so you don't need to include that in the message."", 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+","[{'comment': ""I'd add a variable for the limit pushdown. \r\nSo... here add\r\n```java\r\nprivate final int maxRecords;\r\n```\r\n\r\nThen in the constructor, add:\r\n```java\r\nmaxRecords = scan.getMaxRecords();\r\n```"", 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);
+    logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    // open Sequencefile
+    try {
+      processReader(negotiator);
+    } catch (ExecutionSetupException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Unable to open Sequencefile %s"", split.getPath())
+        .addContext(e.getMessage())
+        .addContext(errorContext)
+        .build(logger);
+    }
+    setLoader = negotiator.build();","[{'comment': 'I think `setLoader` can be local. ', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);
+    logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    // open Sequencefile
+    try {
+      processReader(negotiator);
+    } catch (ExecutionSetupException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Unable to open Sequencefile %s"", split.getPath())
+        .addContext(e.getMessage())
+        .addContext(errorContext)
+        .build(logger);
+    }
+    setLoader = negotiator.build();
+    loader = setLoader.writer();
+    keyWriter = loader.scalar(keySchema);
+    valueWriter = loader.scalar(valueSchema);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    int recordCount = 0;
+    if (watch == null) {
+      watch = Stopwatch.createStarted();
+    }
+    try {
+      while (!loader.isFull()) {
+        if (reader.next(key, value)) {","[{'comment': 'I strongly recommend adding the limit pushdown code.   All you need to do is add the code I referred to earlier and then in this loop add the following:\r\n```java\r\nif (loader.limitReached(maxRecords) ) {\r\n  return false;\r\n}', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);
+    logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    // open Sequencefile
+    try {
+      processReader(negotiator);
+    } catch (ExecutionSetupException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Unable to open Sequencefile %s"", split.getPath())
+        .addContext(e.getMessage())
+        .addContext(errorContext)
+        .build(logger);
+    }
+    setLoader = negotiator.build();
+    loader = setLoader.writer();
+    keyWriter = loader.scalar(keySchema);
+    valueWriter = loader.scalar(valueSchema);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    int recordCount = 0;
+    if (watch == null) {
+      watch = Stopwatch.createStarted();
+    }
+    try {
+      while (!loader.isFull()) {
+        if (reader.next(key, value)) {
+          loader.start();
+          keyWriter.setBytes(key.getBytes(), key.getLength());
+          valueWriter.setBytes(value.getBytes(), value.getLength());
+          loader.save();
+          ++ recordCount;
+        } else {
+          logger.debug(""Read {} records in {} ms"", recordCount, watch.elapsed(TimeUnit.MILLISECONDS));
+          return false;
+        }
+      }
+    } catch (IOException e) {
+      close();
+      throw UserException
+              .dataReadError(e)","[{'comment': ""Please make sure you add a message as to what went wrong here.    You'll want to make sure every error message has:\r\n* a clear message in the `message()` call and\r\n* a call to the `addContext(errorContext)`\r\n\r\nHere and elsewhere.\r\n"", 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);
+    logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    // open Sequencefile
+    try {
+      processReader(negotiator);
+    } catch (ExecutionSetupException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Unable to open Sequencefile %s"", split.getPath())
+        .addContext(e.getMessage())
+        .addContext(errorContext)
+        .build(logger);
+    }
+    setLoader = negotiator.build();
+    loader = setLoader.writer();
+    keyWriter = loader.scalar(keySchema);
+    valueWriter = loader.scalar(valueSchema);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    int recordCount = 0;
+    if (watch == null) {
+      watch = Stopwatch.createStarted();
+    }
+    try {
+      while (!loader.isFull()) {
+        if (reader.next(key, value)) {
+          loader.start();
+          keyWriter.setBytes(key.getBytes(), key.getLength());
+          valueWriter.setBytes(value.getBytes(), value.getLength());
+          loader.save();
+          ++ recordCount;
+        } else {
+          logger.debug(""Read {} records in {} ms"", recordCount, watch.elapsed(TimeUnit.MILLISECONDS));
+          return false;
+        }
+      }
+    } catch (IOException e) {
+      close();","[{'comment': 'The `close()` call is not needed here.', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);
+    logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    // open Sequencefile
+    try {
+      processReader(negotiator);
+    } catch (ExecutionSetupException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Unable to open Sequencefile %s"", split.getPath())
+        .addContext(e.getMessage())
+        .addContext(errorContext)
+        .build(logger);
+    }
+    setLoader = negotiator.build();
+    loader = setLoader.writer();
+    keyWriter = loader.scalar(keySchema);
+    valueWriter = loader.scalar(valueSchema);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    int recordCount = 0;
+    if (watch == null) {
+      watch = Stopwatch.createStarted();
+    }
+    try {
+      while (!loader.isFull()) {
+        if (reader.next(key, value)) {
+          loader.start();
+          keyWriter.setBytes(key.getBytes(), key.getLength());
+          valueWriter.setBytes(value.getBytes(), value.getLength());
+          loader.save();
+          ++ recordCount;
+        } else {
+          logger.debug(""Read {} records in {} ms"", recordCount, watch.elapsed(TimeUnit.MILLISECONDS));
+          return false;
+        }
+      }
+    } catch (IOException e) {
+      close();
+      throw UserException
+              .dataReadError(e)
+              .addContext(""File Path"", split.getPath())
+              .build(logger);
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      if (reader != null) {","[{'comment': 'Does the reader implement `AutoCloseable`?  If so you might want to use that instead.', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);
+    logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    // open Sequencefile
+    try {
+      processReader(negotiator);
+    } catch (ExecutionSetupException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Unable to open Sequencefile %s"", split.getPath())
+        .addContext(e.getMessage())
+        .addContext(errorContext)
+        .build(logger);
+    }
+    setLoader = negotiator.build();
+    loader = setLoader.writer();
+    keyWriter = loader.scalar(keySchema);
+    valueWriter = loader.scalar(valueSchema);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    int recordCount = 0;
+    if (watch == null) {
+      watch = Stopwatch.createStarted();
+    }
+    try {
+      while (!loader.isFull()) {
+        if (reader.next(key, value)) {
+          loader.start();
+          keyWriter.setBytes(key.getBytes(), key.getLength());
+          valueWriter.setBytes(value.getBytes(), value.getLength());
+          loader.save();
+          ++ recordCount;
+        } else {
+          logger.debug(""Read {} records in {} ms"", recordCount, watch.elapsed(TimeUnit.MILLISECONDS));
+          return false;
+        }
+      }
+    } catch (IOException e) {
+      close();
+      throw UserException
+              .dataReadError(e)
+              .addContext(""File Path"", split.getPath())
+              .build(logger);
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      if (reader != null) {
+        reader.close();
+        reader = null;
+      }
+    } catch (IOException e) {
+      logger.warn(""Exception closing reader: {}"", e);
+    }
+  }
+
+  @Override
+  public String toString() {","[{'comment': 'The `toString()` function is not really needed for the BatchReader classes. ', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileBatchReader.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.sequencefile;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.ImpersonationUtil;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SequenceFileBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(SequenceFileBatchReader.class);
+
+  private final SequenceFileFormatConfig config;
+  private final EasySubScan scan;
+  private FileSplit split;
+  private String queryUserName;
+  private String opUserName;
+  private final String keySchema = ""binary_key"";
+  private final String valueSchema = ""binary_value"";
+  private final BytesWritable key = new BytesWritable();
+  private final BytesWritable value = new BytesWritable();
+  private ResultSetLoader setLoader;
+  private RowSetLoader loader;
+  private ScalarWriter keyWriter;
+  private ScalarWriter valueWriter;
+  private RecordReader<BytesWritable, BytesWritable> reader;
+  private CustomErrorContext errorContext;
+  private Stopwatch watch;
+
+  public SequenceFileBatchReader(SequenceFileFormatConfig config, EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    builder.addNullable(keySchema, MinorType.VARBINARY);
+    builder.addNullable(valueSchema, MinorType.VARBINARY);
+    return builder.buildSchema();
+  }
+
+  private void processReader(FileSchemaNegotiator negotiator) throws ExecutionSetupException {
+    final SequenceFileAsBinaryInputFormat inputFormat = new SequenceFileAsBinaryInputFormat();
+    split = negotiator.split();
+    final JobConf jobConf = new JobConf(negotiator.fileSystem().getConf());
+    jobConf.setInputFormat(inputFormat.getClass());
+    reader = getRecordReader(inputFormat, jobConf);
+  }
+
+  private RecordReader<BytesWritable, BytesWritable> getRecordReader(
+    final InputFormat<BytesWritable, BytesWritable> inputFormat, final JobConf jobConf)
+    throws ExecutionSetupException {
+    try {
+      final UserGroupInformation ugi = ImpersonationUtil.createProxyUgi(opUserName, queryUserName);
+      return ugi.doAs(new PrivilegedExceptionAction<RecordReader<BytesWritable, BytesWritable>>() {
+        @Override
+        public RecordReader<BytesWritable, BytesWritable> run() throws Exception {
+          return inputFormat.getRecordReader(split, jobConf, Reporter.NULL);
+        }
+      });
+    } catch (IOException | InterruptedException e) {
+      throw new ExecutionSetupException(
+        String.format(""Error in creating sequencefile reader for file: %s, start: %d, length: %d"",
+          split.getPath(), split.getStart(), split.getLength()), e);
+    }
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    negotiator.tableSchema(defineMetadata(), false);
+    logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    // open Sequencefile
+    try {
+      processReader(negotiator);
+    } catch (ExecutionSetupException e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Unable to open Sequencefile %s"", split.getPath())
+        .addContext(e.getMessage())
+        .addContext(errorContext)
+        .build(logger);
+    }
+    setLoader = negotiator.build();
+    loader = setLoader.writer();
+    keyWriter = loader.scalar(keySchema);
+    valueWriter = loader.scalar(valueSchema);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    int recordCount = 0;
+    if (watch == null) {
+      watch = Stopwatch.createStarted();
+    }
+    try {
+      while (!loader.isFull()) {
+        if (reader.next(key, value)) {
+          loader.start();
+          keyWriter.setBytes(key.getBytes(), key.getLength());
+          valueWriter.setBytes(value.getBytes(), value.getLength());
+          loader.save();
+          ++ recordCount;
+        } else {
+          logger.debug(""Read {} records in {} ms"", recordCount, watch.elapsed(TimeUnit.MILLISECONDS));
+          return false;
+        }
+      }
+    } catch (IOException e) {
+      close();
+      throw UserException
+              .dataReadError(e)
+              .addContext(""File Path"", split.getPath())
+              .build(logger);
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    try {
+      if (reader != null) {
+        reader.close();
+        reader = null;
+      }
+    } catch (IOException e) {
+      logger.warn(""Exception closing reader: {}"", e);
+    }
+  }
+
+  @Override
+  public String toString() {
+    long position = -1L;
+    try {
+      if (reader != null) {
+        position = reader.getPos();
+      }
+    } catch (IOException e) {
+      logger.trace(""Unable to obtain reader position."", e);
+    }
+    return String.format(""SequenceFileBatchReader[File=%s, Position=%s]"", split.getPath(), position);
+  }
+","[{'comment': 'NIT:  Remove extra line.', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/sequencefile/SequenceFileFormatPlugin.java,"@@ -17,92 +17,77 @@
  */
 package org.apache.drill.exec.store.easy.sequencefile;
 
-import java.io.IOException;
-import java.util.List;
-
 import org.apache.drill.common.exceptions.ExecutionSetupException;
-import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.common.logical.StoragePluginConfig;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.physical.base.AbstractGroupScan;
-import org.apache.drill.exec.planner.common.DrillStatsTable.TableStatistics;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileReaderFactory;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileScanBuilder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
 import org.apache.drill.exec.proto.UserBitShared.CoreOperatorType;
 import org.apache.drill.exec.server.DrillbitContext;
-import org.apache.drill.exec.store.RecordReader;
-import org.apache.drill.exec.store.RecordWriter;
-import org.apache.drill.exec.store.dfs.DrillFileSystem;
-import org.apache.drill.exec.store.dfs.FileSelection;
+import org.apache.drill.exec.server.options.OptionManager;
 import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;
-import org.apache.drill.exec.store.dfs.easy.EasyGroupScan;
-import org.apache.drill.exec.store.dfs.easy.EasyWriter;
-import org.apache.drill.exec.store.dfs.easy.FileWork;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.FileSplit;
 
 public class SequenceFileFormatPlugin extends EasyFormatPlugin<SequenceFileFormatConfig> {
-  public SequenceFileFormatPlugin(String name, DrillbitContext context, Configuration fsConf,
-                                  StoragePluginConfig storageConfig) {
-    this(name, context, fsConf, storageConfig, new SequenceFileFormatConfig(null));
-  }
 
-  public SequenceFileFormatPlugin(String name, DrillbitContext context, Configuration fsConf,
-                                  StoragePluginConfig storageConfig, SequenceFileFormatConfig formatConfig) {
-    super(name, context, fsConf, storageConfig, formatConfig,
-      true, false, /* splittable = */ true, /* compressible = */ true,
-      formatConfig.getExtensions(), ""sequencefile"");
+  public SequenceFileFormatPlugin(String name,
+                                  DrillbitContext context,
+                                  Configuration fsConf,
+                                  StoragePluginConfig storageConfig,
+                                  SequenceFileFormatConfig formatConfig) {
+    super(name, easyConfig(fsConf, formatConfig), context, storageConfig, formatConfig);
   }
 
-  @Override
-  public boolean supportsPushDown() {
-    return true;
+  private static EasyFormatConfig easyConfig(Configuration fsConf, SequenceFileFormatConfig pluginConfig) {
+    EasyFormatConfig config = new EasyFormatConfig();
+    config.readable = true;
+    config.writable = false;
+    config.blockSplittable = true;
+    config.compressible = true;
+    config.extensions = pluginConfig.getExtensions();
+    config.fsConf = fsConf;
+    config.readerOperatorType = CoreOperatorType.SEQUENCE_SUB_SCAN_VALUE;
+    config.useEnhancedScan = true;
+    config.supportsProjectPushdown = true;
+    config.defaultName = SequenceFileFormatConfig.NAME;","[{'comment': 'If you want to support the limit pushdown, you should add\r\n```\r\nconfig.supportsLimitPushdown = true;\r\n```\r\nto this method.', 'commenter': 'cgivre'}]"
2125,exec/java-exec/src/test/java/org/apache/drill/exec/store/sequencefile/TestSequenceFileReader.java,"@@ -17,32 +17,59 @@
  */
 package org.apache.drill.exec.store.sequencefile;
 
-import java.io.DataOutputStream;
 import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.nio.file.Paths;
 
-import org.junit.Test;
-import org.apache.drill.test.BaseTestQuery;
+import org.apache.drill.categories.RowSetTests;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryRowSetIterator;
 import org.apache.hadoop.io.BytesWritable;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
 
-public class TestSequenceFileReader extends BaseTestQuery {
+@Category(RowSetTests.class)
+public class TestSequenceFileReader extends ClusterTest {
 
-  public static String byteWritableString(String input) throws Exception {
-    final ByteArrayOutputStream bout = new ByteArrayOutputStream();
-    DataOutputStream out = new DataOutputStream(bout);
-    final BytesWritable writable = new BytesWritable(input.getBytes(""UTF-8""));
-    writable.write(out);
-    return new String(bout.toByteArray());
+  @BeforeClass
+  public static void setup() throws Exception {
+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""sequencefiles/""));
   }
 
   @Test
   public void testSequenceFileReader() throws Exception {
     testBuilder()
-      .sqlQuery(""select convert_from(t.binary_key, 'UTF8') as k, convert_from(t.binary_value, 'UTF8') as v "" +
-        ""from cp.`sequencefiles/simple.seq` t"")
+      .sqlQuery(""select convert_from(t.binary_key, 'UTF8') as k, convert_from(t.binary_value, 'UTF8') as v ""
+        + ""from cp.`sequencefiles/simple.seq` t"")
       .ordered()
       .baselineColumns(""k"", ""v"")
       .baselineValues(byteWritableString(""key0""), byteWritableString(""value0""))
       .baselineValues(byteWritableString(""key1""), byteWritableString(""value1""))
-      .build().run();
+      .build()
+      .run();
+  }
+
+  @Test
+  public void testOutput() {
+    String sql = ""select convert_from(t.binary_key, 'UTF8'), convert_from(t.binary_value, 'UTF8')""
+        + ""from cp.`sequencefiles/simple.seq` t"";
+    QueryRowSetIterator iterator = queryBuilder().sql(sql).rowSetIterator();
+    for (RowSet rowset : iterator) {
+      rowset.print();
+      rowset.clear();
+    }
+  }
+
+  private static String byteWritableString(String input) throws Exception {
+    final ByteArrayOutputStream bout = new ByteArrayOutputStream();
+    DataOutputStream out = new DataOutputStream(bout);
+    final BytesWritable writable = new BytesWritable(input.getBytes(""UTF-8""));
+    writable.write(out);
+    return new String(bout.toByteArray());
   }
+","[{'comment': ""I'd recommend some additional work on the unit tests.  First, please remove any code in the unit tests that produces output to STDOUT.  (IE any print statements) \r\n\r\nSecondly, take a look at the test below:  I'd recommend refactoring all the unit tests for this in the pattern below. This pattern will test that the schema is being generated correctly, and that the outputs are properly mapped using EVF.\r\n\r\nI'd also add unit tests for:\r\n* Star Query\r\n* Explicit Field Query\r\n* Limit Pushdown (Plan only)\r\n* SerDe \r\n* Compressed File (If applicable)\r\n\r\nYou can pretty much cut/paste from the file below for all those tests. \r\n\r\n\r\nhttps://github.com/apache/drill/blob/f79587ed14767463129f8905d2e36e55563655f2/contrib/format-spss/src/test/java/org/apache/drill/exec/store/spss/TestSpssReader.java#L110-L131"", 'commenter': 'cgivre'}]"
2130,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcapng/PcapColumn.java,"@@ -0,0 +1,1031 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcapng;
+
+import java.util.Collections;
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+import org.apache.drill.common.types.TypeProtos.MajorType;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.store.pcap.PcapFormatUtils;
+import org.apache.drill.exec.store.pcapng.decoder.PacketDecoder;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.joda.time.Instant;
+
+import fr.bmartel.pcapdecoder.structure.options.inter.IOptionsStatisticsHeader;
+import fr.bmartel.pcapdecoder.structure.types.IPcapngType;
+import fr.bmartel.pcapdecoder.structure.types.inter.IDescriptionBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.IEnhancedPacketBLock;
+import fr.bmartel.pcapdecoder.structure.types.inter.INameResolutionBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.ISectionHeaderBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.IStatisticsBlock;
+
+public abstract class PcapColumn {
+
+  private static final Map<String, PcapColumn> columns = new LinkedHashMap<>();
+  private static final Map<String, PcapColumn> summary_columns = new LinkedHashMap<>();
+  public static final String DUMMY_NAME = ""dummy"";
+  public static final String PATH_NAME = ""path"";
+
+  static {
+    // Basic
+    columns.put(""timestamp"", new PcapTimestamp());
+    columns.put(""packet_length"", new PcapPacketLength());
+    columns.put(""type"", new PcapType());
+    columns.put(""src_ip"", new PcapSrcIp());
+    columns.put(""dst_ip"", new PcapDstIp());
+    columns.put(""src_port"", new PcapSrcPort());
+    columns.put(""dst_port"", new PcapDstPort());
+    columns.put(""src_mac_address"", new PcapSrcMac());
+    columns.put(""dst_mac_address"", new PcapDstMac());
+    columns.put(""tcp_session"", new PcapTcpSession());
+    columns.put(""tcp_ack"", new PcapTcpAck());
+    columns.put(""tcp_flags"", new PcapTcpFlags());
+    columns.put(""tcp_flags_ns"", new PcapTcpFlagsNs());
+    columns.put(""tcp_flags_cwr"", new PcapTcpFlagsCwr());
+    columns.put(""tcp_flags_ece"", new PcapTcpFlagsEce());
+    columns.put(""tcp_flags_ece_ecn_capable"", new PcapTcpFlagsEceEcnCapable());
+    columns.put(""tcp_flags_ece_congestion_experienced"", new PcapTcpFlagsEceCongestionExperienced());
+    columns.put(""tcp_flags_urg"", new PcapTcpFlagsUrg());
+    columns.put(""tcp_flags_ack"", new PcapTcpFlagsAck());
+    columns.put(""tcp_flags_psh"", new PcapTcpFlagsPsh());
+    columns.put(""tcp_flags_rst"", new PcapTcpFlagsRst());
+    columns.put(""tcp_flags_syn"", new PcapTcpFlagsSyn());
+    columns.put(""tcp_flags_fin"", new PcapTcpFlagsFin());
+    columns.put(""tcp_parsed_flags"", new PcapTcpParsedFlags());
+    columns.put(""packet_data"", new PcapPacketData());
+
+    // Extensions
+    summary_columns.put(""path"", new PcapStatPath());
+    // Section Header Block
+    summary_columns.put(""shb_hardware"", new PcapHardware());
+    summary_columns.put(""shb_os"", new PcapOS());
+    summary_columns.put(""shb_userappl"", new PcapUserAppl());
+    // Interface Description Block
+    summary_columns.put(""if_name"", new PcapIfName());
+    summary_columns.put(""if_description"", new PcapIfDescription());
+    summary_columns.put(""if_ipv4addr"", new PcapIfIPv4addr());
+    summary_columns.put(""if_ipv6addr"", new PcapIfIPv6addr());
+    summary_columns.put(""if_macaddr"", new PcapIfMACaddr());
+    summary_columns.put(""if_euiaddr"", new PcapIfEUIaddr());
+    summary_columns.put(""if_speed"", new PcapIfSpeed());
+    summary_columns.put(""if_tsresol"", new PcapIfTsresol());
+    summary_columns.put(""if_tzone"", new PcapIfTzone());
+    summary_columns.put(""if_os"", new PcapIfOS());
+    summary_columns.put(""if_fcslen"", new PcapIfFcslen());
+    summary_columns.put(""if_tsoffset"", new PcapIfTsOffset());
+    // Name Resolution Block
+    summary_columns.put(""ns_dnsname"", new PcapDnsName());
+    summary_columns.put(""ns_dnsip4addr"", new PcapDnsIP4addr());
+    summary_columns.put(""ns_dnsip6addr"", new PcapDnsIP6addr());
+    // Interface Statistics Block
+    summary_columns.put(""isb_starttime"", new PcapIsbStarttime());
+    summary_columns.put(""isb_endtime"", new PcapIsbEndtime());
+    summary_columns.put(""isb_ifrecv"", new PcapIsbIfrecv());
+    summary_columns.put(""isb_ifdrop"", new PcapIsbIfdrop());
+    summary_columns.put(""isb_filteraccept"", new PcapIsbFilterAccept());
+    summary_columns.put(""isb_osdrop"", new PcapIsbOSdrop());
+    summary_columns.put(""isb_usrdeliv"", new PcapIsbUsrdeliv());","[{'comment': ""I'm getting an IDE warning here about possibly causing class deadlock.  Could you please take a look?"", 'commenter': 'cgivre'}]"
2130,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcapng/PcapColumn.java,"@@ -0,0 +1,1031 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcapng;
+
+import java.util.Collections;
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+import org.apache.drill.common.types.TypeProtos.MajorType;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.store.pcap.PcapFormatUtils;
+import org.apache.drill.exec.store.pcapng.decoder.PacketDecoder;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.joda.time.Instant;
+
+import fr.bmartel.pcapdecoder.structure.options.inter.IOptionsStatisticsHeader;
+import fr.bmartel.pcapdecoder.structure.types.IPcapngType;
+import fr.bmartel.pcapdecoder.structure.types.inter.IDescriptionBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.IEnhancedPacketBLock;
+import fr.bmartel.pcapdecoder.structure.types.inter.INameResolutionBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.ISectionHeaderBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.IStatisticsBlock;
+
+public abstract class PcapColumn {
+
+  private static final Map<String, PcapColumn> columns = new LinkedHashMap<>();
+  private static final Map<String, PcapColumn> summary_columns = new LinkedHashMap<>();
+  public static final String DUMMY_NAME = ""dummy"";
+  public static final String PATH_NAME = ""path"";
+
+  static {
+    // Basic
+    columns.put(""timestamp"", new PcapTimestamp());
+    columns.put(""packet_length"", new PcapPacketLength());
+    columns.put(""type"", new PcapType());
+    columns.put(""src_ip"", new PcapSrcIp());
+    columns.put(""dst_ip"", new PcapDstIp());
+    columns.put(""src_port"", new PcapSrcPort());
+    columns.put(""dst_port"", new PcapDstPort());
+    columns.put(""src_mac_address"", new PcapSrcMac());
+    columns.put(""dst_mac_address"", new PcapDstMac());
+    columns.put(""tcp_session"", new PcapTcpSession());
+    columns.put(""tcp_ack"", new PcapTcpAck());
+    columns.put(""tcp_flags"", new PcapTcpFlags());
+    columns.put(""tcp_flags_ns"", new PcapTcpFlagsNs());
+    columns.put(""tcp_flags_cwr"", new PcapTcpFlagsCwr());
+    columns.put(""tcp_flags_ece"", new PcapTcpFlagsEce());
+    columns.put(""tcp_flags_ece_ecn_capable"", new PcapTcpFlagsEceEcnCapable());
+    columns.put(""tcp_flags_ece_congestion_experienced"", new PcapTcpFlagsEceCongestionExperienced());
+    columns.put(""tcp_flags_urg"", new PcapTcpFlagsUrg());
+    columns.put(""tcp_flags_ack"", new PcapTcpFlagsAck());
+    columns.put(""tcp_flags_psh"", new PcapTcpFlagsPsh());
+    columns.put(""tcp_flags_rst"", new PcapTcpFlagsRst());
+    columns.put(""tcp_flags_syn"", new PcapTcpFlagsSyn());
+    columns.put(""tcp_flags_fin"", new PcapTcpFlagsFin());
+    columns.put(""tcp_parsed_flags"", new PcapTcpParsedFlags());
+    columns.put(""packet_data"", new PcapPacketData());
+
+    // Extensions
+    summary_columns.put(""path"", new PcapStatPath());
+    // Section Header Block
+    summary_columns.put(""shb_hardware"", new PcapHardware());
+    summary_columns.put(""shb_os"", new PcapOS());
+    summary_columns.put(""shb_userappl"", new PcapUserAppl());
+    // Interface Description Block
+    summary_columns.put(""if_name"", new PcapIfName());
+    summary_columns.put(""if_description"", new PcapIfDescription());
+    summary_columns.put(""if_ipv4addr"", new PcapIfIPv4addr());
+    summary_columns.put(""if_ipv6addr"", new PcapIfIPv6addr());
+    summary_columns.put(""if_macaddr"", new PcapIfMACaddr());
+    summary_columns.put(""if_euiaddr"", new PcapIfEUIaddr());
+    summary_columns.put(""if_speed"", new PcapIfSpeed());
+    summary_columns.put(""if_tsresol"", new PcapIfTsresol());
+    summary_columns.put(""if_tzone"", new PcapIfTzone());
+    summary_columns.put(""if_os"", new PcapIfOS());
+    summary_columns.put(""if_fcslen"", new PcapIfFcslen());
+    summary_columns.put(""if_tsoffset"", new PcapIfTsOffset());
+    // Name Resolution Block
+    summary_columns.put(""ns_dnsname"", new PcapDnsName());
+    summary_columns.put(""ns_dnsip4addr"", new PcapDnsIP4addr());
+    summary_columns.put(""ns_dnsip6addr"", new PcapDnsIP6addr());
+    // Interface Statistics Block
+    summary_columns.put(""isb_starttime"", new PcapIsbStarttime());
+    summary_columns.put(""isb_endtime"", new PcapIsbEndtime());
+    summary_columns.put(""isb_ifrecv"", new PcapIsbIfrecv());
+    summary_columns.put(""isb_ifdrop"", new PcapIsbIfdrop());
+    summary_columns.put(""isb_filteraccept"", new PcapIsbFilterAccept());
+    summary_columns.put(""isb_osdrop"", new PcapIsbOSdrop());
+    summary_columns.put(""isb_usrdeliv"", new PcapIsbUsrdeliv());
+  }
+
+  abstract MajorType getType();
+
+  abstract void process(IPcapngType block, ScalarWriter writer);
+
+  public static Map<String, PcapColumn> getColumns() {
+    return Collections.unmodifiableMap(columns);
+  }
+
+  public static Map<String, PcapColumn> getSummaryColumns() {
+    return Collections.unmodifiableMap(summary_columns);
+  }
+
+  static class PcapDummy extends PcapColumn {
+
+    @Override
+    MajorType getType() {
+      return Types.optional(MinorType.INT);
+    }
+
+    @Override
+    void process(IPcapngType block, ScalarWriter writer) { }
+  }
+
+  static class PcapDummyArray extends PcapColumn {","[{'comment': ""This class doesn't appear to be used.  Can you please remove if it is unnecessary?"", 'commenter': 'cgivre'}]"
2130,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcapng/PcapColumn.java,"@@ -0,0 +1,1031 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcapng;
+
+import java.util.Collections;
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+import org.apache.drill.common.types.TypeProtos.MajorType;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.store.pcap.PcapFormatUtils;
+import org.apache.drill.exec.store.pcapng.decoder.PacketDecoder;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.joda.time.Instant;
+
+import fr.bmartel.pcapdecoder.structure.options.inter.IOptionsStatisticsHeader;
+import fr.bmartel.pcapdecoder.structure.types.IPcapngType;
+import fr.bmartel.pcapdecoder.structure.types.inter.IDescriptionBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.IEnhancedPacketBLock;
+import fr.bmartel.pcapdecoder.structure.types.inter.INameResolutionBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.ISectionHeaderBlock;
+import fr.bmartel.pcapdecoder.structure.types.inter.IStatisticsBlock;
+
+public abstract class PcapColumn {
+
+  private static final Map<String, PcapColumn> columns = new LinkedHashMap<>();
+  private static final Map<String, PcapColumn> summary_columns = new LinkedHashMap<>();
+  public static final String DUMMY_NAME = ""dummy"";
+  public static final String PATH_NAME = ""path"";
+
+  static {
+    // Basic
+    columns.put(""timestamp"", new PcapTimestamp());
+    columns.put(""packet_length"", new PcapPacketLength());
+    columns.put(""type"", new PcapType());
+    columns.put(""src_ip"", new PcapSrcIp());
+    columns.put(""dst_ip"", new PcapDstIp());
+    columns.put(""src_port"", new PcapSrcPort());
+    columns.put(""dst_port"", new PcapDstPort());
+    columns.put(""src_mac_address"", new PcapSrcMac());
+    columns.put(""dst_mac_address"", new PcapDstMac());
+    columns.put(""tcp_session"", new PcapTcpSession());
+    columns.put(""tcp_ack"", new PcapTcpAck());
+    columns.put(""tcp_flags"", new PcapTcpFlags());
+    columns.put(""tcp_flags_ns"", new PcapTcpFlagsNs());
+    columns.put(""tcp_flags_cwr"", new PcapTcpFlagsCwr());
+    columns.put(""tcp_flags_ece"", new PcapTcpFlagsEce());
+    columns.put(""tcp_flags_ece_ecn_capable"", new PcapTcpFlagsEceEcnCapable());
+    columns.put(""tcp_flags_ece_congestion_experienced"", new PcapTcpFlagsEceCongestionExperienced());
+    columns.put(""tcp_flags_urg"", new PcapTcpFlagsUrg());
+    columns.put(""tcp_flags_ack"", new PcapTcpFlagsAck());
+    columns.put(""tcp_flags_psh"", new PcapTcpFlagsPsh());
+    columns.put(""tcp_flags_rst"", new PcapTcpFlagsRst());
+    columns.put(""tcp_flags_syn"", new PcapTcpFlagsSyn());
+    columns.put(""tcp_flags_fin"", new PcapTcpFlagsFin());
+    columns.put(""tcp_parsed_flags"", new PcapTcpParsedFlags());
+    columns.put(""packet_data"", new PcapPacketData());","[{'comment': 'In the regular PCAP reader, we added a boolean column called `is_corrupt` or something like that.  Basically, instead of crashing on corrupt data, Drill catches is, marks that flag as true and keeps reading.  Could we add that column here as well?', 'commenter': 'cgivre'}, {'comment': ""@luocooong \r\nI saw your response.  Basically, what we did in the original reader was put a lot of code in try/catch blocks and if it found errors, set `is_corrupt` to `true` and move on to the next packet.  Have you tested the PCAP-NG with a corrupt packet?  Does the `parse()` method throw an exception if the packet is corrupt?\r\nThe reason I'm asking is that it is often better for the user to get a result, rather than a huge java stacktrace, so if we can avoid that, it would be good. \r\n"", 'commenter': 'cgivre'}]"
2130,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcapng/PcapngBatchReader.java,"@@ -0,0 +1,270 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcapng;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.Utilities;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import fr.bmartel.pcapdecoder.PcapDecoder;
+import fr.bmartel.pcapdecoder.structure.types.IPcapngType;
+import fr.bmartel.pcapdecoder.structure.types.inter.IEnhancedPacketBLock;
+
+public class PcapngBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapngBatchReader.class);
+
+  private final PcapngFormatConfig config;
+  private final EasySubScan scan;
+  private final int maxRecords;
+  private CustomErrorContext errorContext;
+  private List<SchemaPath> columns;
+  private List<ColumnDefn> projectedColumns;
+  private Iterator<IPcapngType> pcapIterator;
+  private RowSetLoader loader;
+  private InputStream in;
+  private Path path;
+
+  public PcapngBatchReader(final PcapngFormatConfig config, final EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+    this.maxRecords = scan.getMaxRecords();
+    this.columns = scan.getColumns();
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    try {
+      // init InputStream for pcap file
+      errorContext = negotiator.parentErrorContext();
+      DrillFileSystem dfs = negotiator.fileSystem();
+      path = dfs.makeQualified(negotiator.split().getPath());
+      in = dfs.openPossiblyCompressedStream(path);
+      // decode the pcap file
+      PcapDecoder decoder = new PcapDecoder(IOUtils.toByteArray(in));
+      decoder.decode();
+      pcapIterator = decoder.getSectionList().iterator();
+      logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), columns);
+    } catch (IOException e) {
+      throw UserException
+             .dataReadError(e)
+             .message(""Failure in initial pcapng inputstream. "" + e.getMessage())
+             .addContext(errorContext)
+             .build(logger);
+    } catch (Exception e) {
+      throw UserException
+             .dataReadError(e)
+             .message(""Failed to decode the pcapng file. "" + e.getMessage())
+             .addContext(errorContext)
+             .build(logger);
+    }
+    // define the schema
+    negotiator.tableSchema(defineMetadata(), true);
+    ResultSetLoader resultSetLoader = negotiator.build();
+    loader = resultSetLoader.writer();
+    // bind the writer for columns
+    bindColumns(loader);
+    return true;
+  }
+
+  /**
+   * The default of the `stat` parameter is false,
+   * which means that the packet data is parsed and returned,
+   * but if true, will return the statistics data about the each pcapng file only
+   * (consist of the information about collect devices and the summary of the packet data above).
+   *
+   * In addition, a pcapng file contains a single Section Header Block (SHB),
+   * a single Interface Description Block (IDB) and a few Enhanced Packet Blocks (EPB).
+   * <pre>
+   * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+   * | SHB | IDB | EPB | EPB |    ...    | EPB |
+   * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+   * </pre>
+   * https://pcapng.github.io/pcapng/draft-tuexen-opsawg-pcapng.html#name-physical-file-layout
+   */
+  @Override
+  public boolean next() {
+    while (!loader.isFull()) {
+      while (pcapIterator.hasNext()) {","[{'comment': ""I think this loop could be refactored a bit as it's a bit confusing.  This function is intended to process 1 row. \r\n\r\nThe format I've generally used is like this:\r\n```java\r\n  @Override\r\n  public boolean next() {\r\n    while (!rowWriter.isFull()) {\r\n      if (!processNextRow()) {\r\n        return false;\r\n      }\r\n    }\r\n    return true;\r\n  }\r\n```\r\n\r\nThen the `processNextRow()` function checks to see if the limit has been reached, loading the data into the vectors etc.   The important thing is that this function needs to return `true` when there is more data but the loader is full (so new batches are started) and it needs to return `false` when there is no more data. \r\n\r\nI'm getting an IDE warning as well with the outer loop."", 'commenter': 'cgivre'}]"
2130,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcapng/PcapngBatchReader.java,"@@ -0,0 +1,270 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcapng;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.Utilities;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import fr.bmartel.pcapdecoder.PcapDecoder;
+import fr.bmartel.pcapdecoder.structure.types.IPcapngType;
+import fr.bmartel.pcapdecoder.structure.types.inter.IEnhancedPacketBLock;
+
+public class PcapngBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapngBatchReader.class);
+
+  private final PcapngFormatConfig config;
+  private final EasySubScan scan;
+  private final int maxRecords;
+  private CustomErrorContext errorContext;
+  private List<SchemaPath> columns;
+  private List<ColumnDefn> projectedColumns;
+  private Iterator<IPcapngType> pcapIterator;
+  private RowSetLoader loader;
+  private InputStream in;
+  private Path path;
+
+  public PcapngBatchReader(final PcapngFormatConfig config, final EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+    this.maxRecords = scan.getMaxRecords();
+    this.columns = scan.getColumns();
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    try {
+      // init InputStream for pcap file
+      errorContext = negotiator.parentErrorContext();
+      DrillFileSystem dfs = negotiator.fileSystem();
+      path = dfs.makeQualified(negotiator.split().getPath());
+      in = dfs.openPossiblyCompressedStream(path);
+      // decode the pcap file
+      PcapDecoder decoder = new PcapDecoder(IOUtils.toByteArray(in));
+      decoder.decode();
+      pcapIterator = decoder.getSectionList().iterator();
+      logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), columns);
+    } catch (IOException e) {
+      throw UserException
+             .dataReadError(e)
+             .message(""Failure in initial pcapng inputstream. "" + e.getMessage())
+             .addContext(errorContext)
+             .build(logger);
+    } catch (Exception e) {
+      throw UserException
+             .dataReadError(e)
+             .message(""Failed to decode the pcapng file. "" + e.getMessage())
+             .addContext(errorContext)
+             .build(logger);
+    }
+    // define the schema
+    negotiator.tableSchema(defineMetadata(), true);
+    ResultSetLoader resultSetLoader = negotiator.build();
+    loader = resultSetLoader.writer();
+    // bind the writer for columns
+    bindColumns(loader);
+    return true;
+  }
+
+  /**
+   * The default of the `stat` parameter is false,
+   * which means that the packet data is parsed and returned,
+   * but if true, will return the statistics data about the each pcapng file only
+   * (consist of the information about collect devices and the summary of the packet data above).
+   *
+   * In addition, a pcapng file contains a single Section Header Block (SHB),
+   * a single Interface Description Block (IDB) and a few Enhanced Packet Blocks (EPB).
+   * <pre>
+   * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+   * | SHB | IDB | EPB | EPB |    ...    | EPB |
+   * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+   * </pre>
+   * https://pcapng.github.io/pcapng/draft-tuexen-opsawg-pcapng.html#name-physical-file-layout
+   */
+  @Override
+  public boolean next() {
+    while (!loader.isFull()) {
+      while (pcapIterator.hasNext()) {
+        IPcapngType block = pcapIterator.next();
+        if (config.getStat()) {
+          if (block instanceof IEnhancedPacketBLock) {
+            continue;
+          }
+        } else {
+          if (!(block instanceof IEnhancedPacketBLock)) {
+            continue;
+          }
+        }
+        loader.start();
+        for (ColumnDefn columnDefn : projectedColumns) {
+          // pcapng file name
+          if (columnDefn.getName().equals(PcapColumn.PATH_NAME)) {
+            columnDefn.load(path.getName());
+          } else {
+            columnDefn.load(block);
+          }
+        }
+        loader.save();
+        if (loader.limitReached(maxRecords)) {
+          return false;
+        }
+      }
+      return false;
+    }
+    return true;
+  }
+
+  @Override
+  public void close() {
+    AutoCloseables.closeSilently(in);
+  }
+
+  private boolean isSkipQuery() {
+    return columns.isEmpty();
+  }
+
+  private boolean isStarQuery() {
+    return Utilities.isStarQuery(columns);
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    processProjected(columns);
+    for (ColumnDefn columnDefn : projectedColumns) {
+      columnDefn.define(builder);
+    }
+    return builder.buildSchema();
+  }
+
+  /**
+   * <b> Define the schema based on projected </b><br/>
+   * 1. SkipQuery: no field specified, such as count(*) <br/>
+   * 2. StarQuery: select * <br/>
+   * 3. ProjectPushdownQuery: select a,b,c <br/>
+   */
+  private List<ColumnDefn> processProjected(List<SchemaPath> columns) {","[{'comment': 'Should this function be `void`?  The return value is not used.', 'commenter': 'cgivre'}]"
2130,exec/java-exec/src/test/java/org/apache/drill/exec/store/pcapng/TestPcapngRecordReader.java,"@@ -17,84 +17,178 @@
  */
 package org.apache.drill.exec.store.pcapng;
 
-import org.apache.drill.PlanTestBase;
+import static org.junit.Assert.assertEquals;
+
+import java.nio.file.Paths;
+
+import org.apache.drill.categories.RowSetTests;
 import org.apache.drill.common.exceptions.UserRemoteException;
-import org.junit.Assert;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryBuilder;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.joda.time.Instant;
 import org.junit.BeforeClass;
 import org.junit.Test;
+import org.junit.experimental.categories.Category;
 
-import java.nio.file.Paths;
+@Category(RowSetTests.class)
+public class TestPcapngRecordReader extends ClusterTest {
 
-public class TestPcapngRecordReader extends PlanTestBase {
   @BeforeClass
-  public static void setupTestFiles() {
+  public static void setup() throws Exception {
+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));
     dirTestWatcher.copyResourceToRoot(Paths.get(""store"", ""pcapng""));
   }
 
   @Test
   public void testStarQuery() throws Exception {
-    Assert.assertEquals(123, testSql(""select * from dfs.`store/pcapng/sniff.pcapng`""));
-    Assert.assertEquals(1, testSql(""select * from dfs.`store/pcapng/example.pcapng`""));
+    String sql = ""select * from dfs.`store/pcapng/sniff.pcapng`"";
+    QueryBuilder builder = client.queryBuilder().sql(sql);
+    RowSet sets = builder.rowSet();
+
+    assertEquals(123, sets.rowCount());
+    sets.clear();
   }
 
   @Test
-  public void testProjectingByName() throws Exception {
-    Assert.assertEquals(123, testSql(""select `timestamp`, packet_data, type from dfs.`store/pcapng/sniff.pcapng`""));
-    Assert.assertEquals(1, testSql(""select src_ip, dst_ip, `timestamp` from dfs.`store/pcapng/example.pcapng`""));
+  public void testExplicitQuery() throws Exception {
+    String sql = ""select type, packet_length, `timestamp` from dfs.`store/pcapng/sniff.pcapng` where type = 'ARP'"";
+    QueryBuilder builder = client.queryBuilder().sql(sql);
+    RowSet sets = builder.rowSet();
+
+    TupleMetadata schema = new SchemaBuilder()
+        .addNullable(""type"", MinorType.VARCHAR)
+        .add(""packet_length"", MinorType.INT)
+        .add(""timestamp"", MinorType.TIMESTAMP)
+        .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), schema)
+        .addRow(""ARP"", 90, Instant.ofEpochMilli(1518010669927L))
+        .addRow(""ARP"", 90, Instant.ofEpochMilli(1518010671874L))
+        .build();
+
+    assertEquals(2, sets.rowCount());
+    new RowSetComparison(expected).verifyAndClearAll(sets);
+  }
+
+  @Test
+  public void testLimitPushdown() throws Exception {
+    String sql = ""select * from dfs.`store/pcapng/sniff.pcapng` where type = 'UDP' limit 10 offset 65"";
+    QueryBuilder builder = client.queryBuilder().sql(sql);
+    RowSet sets = builder.rowSet();
+
+    assertEquals(6, sets.rowCount());
+    sets.clear();
   }
 
   @Test
-  public void testDiffCaseQuery() throws Exception {
-    Assert.assertEquals(123, testSql(""select `timestamp`, paCket_dAta, TyPe from dfs.`store/pcapng/sniff.pcapng`""));
-    Assert.assertEquals(1, testSql(""select src_ip, dst_ip, `Timestamp` from dfs.`store/pcapng/example.pcapng`""));
+  public void testSerDe() throws Exception {
+    String sql = ""select count(*) from dfs.`store/pcapng/example.pcapng`"";
+    String plan = queryBuilder().sql(sql).explainJson();
+    long cnt = queryBuilder().physical(plan).singletonLong();
+
+    assertEquals(""Counts should match"", 1, cnt);
   }
 
   @Test
-  public void testProjectingMissColls() throws Exception {
-    Assert.assertEquals(123, testSql(""select `timestamp`, `name`, `color` from dfs.`store/pcapng/sniff.pcapng`""));
-    Assert.assertEquals(1, testSql(""select src_ip, `time` from dfs.`store/pcapng/example.pcapng`""));
+  public void testCaseInsensitiveQuery() throws Exception {
+    String sql = ""select `timestamp`, paCket_dAta, TyPe from dfs.`store/pcapng/sniff.pcapng`"";
+    QueryBuilder builder = client.queryBuilder().sql(sql);
+    RowSet sets = builder.rowSet();
+
+    assertEquals(123, sets.rowCount());
+    sets.clear();
   }
 
+  @Test","[{'comment': ""Could you please add a unit test for compressed files?\r\nHere's an example:\r\n\r\nhttps://github.com/apache/drill/blob/8f892b3c9b04e5e0ff1973681ff862da857d22ef/contrib/format-spss/src/test/java/org/apache/drill/exec/store/spss/TestSpssReader.java#L141-L162\r\n\r\n\r\n"", 'commenter': 'cgivre'}]"
2130,exec/java-exec/src/main/java/org/apache/drill/exec/store/pcapng/PcapngBatchReader.java,"@@ -0,0 +1,271 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.pcapng;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.util.Utilities;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.hadoop.fs.Path;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import fr.bmartel.pcapdecoder.PcapDecoder;
+import fr.bmartel.pcapdecoder.structure.types.IPcapngType;
+import fr.bmartel.pcapdecoder.structure.types.inter.IEnhancedPacketBLock;
+
+public class PcapngBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PcapngBatchReader.class);
+
+  private final PcapngFormatConfig config;
+  private final EasySubScan scan;
+  private final int maxRecords;
+  private CustomErrorContext errorContext;
+  private List<SchemaPath> columns;
+  private List<ColumnDefn> projectedColumns;
+  private Iterator<IPcapngType> pcapIterator;
+  private RowSetLoader loader;
+  private InputStream in;
+  private Path path;
+
+  public PcapngBatchReader(final PcapngFormatConfig config, final EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+    this.maxRecords = scan.getMaxRecords();
+    this.columns = scan.getColumns();
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    try {
+      // init InputStream for pcap file
+      errorContext = negotiator.parentErrorContext();
+      DrillFileSystem dfs = negotiator.fileSystem();
+      path = dfs.makeQualified(negotiator.split().getPath());
+      in = dfs.openPossiblyCompressedStream(path);
+      // decode the pcap file
+      PcapDecoder decoder = new PcapDecoder(IOUtils.toByteArray(in));
+      decoder.decode();
+      pcapIterator = decoder.getSectionList().iterator();
+      logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), columns);
+    } catch (IOException e) {
+      throw UserException
+             .dataReadError(e)
+             .message(""Failure in initial pcapng inputstream. "" + e.getMessage())
+             .addContext(errorContext)
+             .build(logger);
+    } catch (Exception e) {
+      throw UserException
+             .dataReadError(e)
+             .message(""Failed to decode the pcapng file. "" + e.getMessage())
+             .addContext(errorContext)
+             .build(logger);
+    }
+    // define the schema
+    negotiator.tableSchema(defineMetadata(), true);
+    ResultSetLoader resultSetLoader = negotiator.build();
+    loader = resultSetLoader.writer();
+    // bind the writer for columns
+    bindColumns(loader);
+    return true;
+  }
+
+  /**
+   * The default of the `stat` parameter is false,
+   * which means that the packet data is parsed and returned,
+   * but if true, will return the statistics data about the each pcapng file only
+   * (consist of the information about collect devices and the summary of the packet data above).
+   *
+   * In addition, a pcapng file contains a single Section Header Block (SHB),
+   * a single Interface Description Block (IDB) and a few Enhanced Packet Blocks (EPB).
+   * <pre>
+   * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+   * | SHB | IDB | EPB | EPB |    ...    | EPB |
+   * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+   * </pre>
+   * https://pcapng.github.io/pcapng/draft-tuexen-opsawg-pcapng.html#name-physical-file-layout
+   */
+  @Override
+  public boolean next() {
+    while (!loader.isFull()) {
+      if (pcapIterator.hasNext()) {
+        IPcapngType block = pcapIterator.next();
+        if (config.getStat()) {","[{'comment': ""@luocooong \r\nThanks for addressing the review comments.   I have a few more minor comments and we should be good to go.  This particular logic here is a bit difficult to follow.  Could we refactor the if-statements so that it is a little more clear?\r\n\r\nI'm thinking (pseudocode)\r\n```java\r\nif (config.getStat() && isIEnhancedPacketBlock()) {\r\n   // do something\r\n} else if (! config.getStat() && !isEnhancedPacketBlock()) {\r\n  // Do something else\r\n}\r\n```\r\n\r\nMy personal preference is to put the check to see if the parser has more records at the beginning of the loop.  So it would look something like this: (pseudocode)\r\n\r\n```java\r\nwhile (!loader.isFull() ) {\r\n  // No more records\r\n  if (!parser.hasNext()) {\r\n    return false;\r\n  } else if (config.getStat() && isIEnhancedPacketBlock()) {\r\n     // do something\r\n  } else if (! config.getStat() && !isEnhancedPacketBlock()) {\r\n    // Do something else\r\n  }\r\n}\r\n\r\n// process the row\r\nloader.start()\r\n}\r\n```"", 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageBatchReader.java,"@@ -0,0 +1,461 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Date;
+import java.util.LinkedHashMap;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.drew.imaging.FileType;
+import com.drew.imaging.FileTypeDetector;
+import com.drew.imaging.ImageMetadataReader;
+import com.drew.imaging.ImageProcessingException;
+import com.drew.metadata.Directory;
+import com.drew.metadata.Metadata;
+import com.drew.metadata.eps.EpsDirectory;
+import com.drew.metadata.exif.ExifIFD0Directory;
+import com.drew.metadata.xmp.XmpDirectory;
+
+public class ImageBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(ImageBatchReader.class);","[{'comment': 'For loggers, please write out the full class name here and elsewhere. \r\nIE\r\n```java\r\n org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ImageBatchReader.class);\r\n```\r\n', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageBatchReader.java,"@@ -0,0 +1,461 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Date;
+import java.util.LinkedHashMap;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.drew.imaging.FileType;
+import com.drew.imaging.FileTypeDetector;
+import com.drew.imaging.ImageMetadataReader;
+import com.drew.imaging.ImageProcessingException;
+import com.drew.metadata.Directory;
+import com.drew.metadata.Metadata;
+import com.drew.metadata.eps.EpsDirectory;
+import com.drew.metadata.exif.ExifIFD0Directory;
+import com.drew.metadata.xmp.XmpDirectory;
+
+public class ImageBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(ImageBatchReader.class);
+
+  private final ImageFormatConfig config;
+  private final EasySubScan scan;
+  private CustomErrorContext errorContext;
+  private Path path;
+  private FileStatus fileStatus;
+  private BufferedInputStream metaInputStream;
+  private RowSetLoader loader;
+  private LinkedHashMap<String, ColumnDefn> genericColumns;
+  private Metadata metadata;
+
+  public ImageBatchReader(final ImageFormatConfig config, final EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    try {
+      errorContext = negotiator.parentErrorContext();
+      DrillFileSystem dfs = negotiator.fileSystem();
+      path = dfs.makeQualified(negotiator.split().getPath());
+      fileStatus = dfs.getFileStatus(path);
+      metaInputStream = new BufferedInputStream(dfs.openPossiblyCompressedStream(path));
+      logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failure in initial image inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    // define the schema
+    negotiator.tableSchema(defineMetadata(), false);
+    ResultSetLoader resultSetLoader = negotiator.build();
+    loader = resultSetLoader.writer();
+    // bind the writer for generic columns
+    bindColumns(loader);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    try {
+      loader.start();
+      // process generic metadata
+      processGenericMetadata();
+      // process external metadata
+      processExtenalMetadata();
+      loader.save();
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to estimates the file type. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (ImageProcessingException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in reading metadata from inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (Exception e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in processing metadata directory. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    return false;
+  }
+
+  @Override
+  public void close() {
+    AutoCloseables.closeSilently(metaInputStream);
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    genericColumns = new LinkedHashMap<>();
+    Collection<String> tags = GenericMetadataDirectory._tagNameMap.values();
+    for (String tagName : tags) {
+      if (!config.hasFileSystemMetadata() && ImageMetadataUtils.isSkipTag(tagName)) {
+        continue;
+      }
+      ColumnDefn columnDefn = new GenericColumnDefn(tagName);
+      if (config.isDescriptive()) {
+        columnDefn.defineText(builder);
+      } else {
+        columnDefn.define(builder);
+      }
+      genericColumns.put(ImageMetadataUtils.formatName(tagName), columnDefn);
+    }
+    Collections.unmodifiableMap(genericColumns);
+    return builder.buildSchema();
+  }
+
+  private void bindColumns(RowSetLoader loader) {
+    for (ColumnDefn columnDefn : genericColumns.values()) {
+      columnDefn.bind(loader);
+    }
+  }
+
+  private void processGenericMetadata() throws IOException, ImageProcessingException {
+    FileType fileType = FileTypeDetector.detectFileType(metaInputStream);
+    metadata = ImageMetadataReader.readMetadata(metaInputStream);
+    // Read for generic metadata at first
+    new GenericMetadataReader().read(fileType, fileStatus, metadata);
+    GenericMetadataDirectory genericMetadata = metadata.getFirstDirectoryOfType(GenericMetadataDirectory.class);
+    // Process the `Generic Metadata Directory`
+    ImageDirectoryProcessor.processGenericMetadataDirectory(genericMetadata, genericColumns, config);
+  }
+
+  private void processExtenalMetadata() {
+    boolean skipEPSPreview = false;
+    for (Directory directory : metadata.getDirectories()) {
+      // Skip the `Generic Metadata Directory`
+      String dictName = ImageMetadataUtils.formatName(directory.getName());
+      if (directory instanceof GenericMetadataDirectory) {
+        continue;
+      }
+      if (directory instanceof ExifIFD0Directory && skipEPSPreview) {
+        skipEPSPreview = false;
+        continue;
+      }
+      if (directory instanceof EpsDirectory) {
+        // If an EPS file contains a TIFF preview, skip the next IFD0
+        skipEPSPreview = directory.containsTag(EpsDirectory.TAG_TIFF_PREVIEW_SIZE);
+      }
+      // Process the `External Metadata Directory`
+      MapColumnDefn columnDefn = new MapColumnDefn(dictName).builder(loader);
+      ImageDirectoryProcessor.processDirectory(columnDefn, directory, metadata, config);
+      // Continue to process XmpDirectory if exists
+      if (directory instanceof XmpDirectory) {
+        ImageDirectoryProcessor.processXmpDirectory(columnDefn, (XmpDirectory) directory);
+      }
+    }
+  }
+
+  protected abstract static class ColumnDefn {
+
+    private final String name;
+    private final String originName; // not format
+    private ScalarWriter writer;
+
+    public ColumnDefn(String name) {
+      this.originName = name;
+      this.name = ImageMetadataUtils.formatName(name);
+    }
+
+    public String getName() {
+      return name;
+    }
+
+    public String getOriginName() {
+      return originName;
+    }
+
+    public ScalarWriter getWriter() {
+      return writer;
+    }
+
+    public void bind(RowSetLoader loader) {
+      writer = loader.scalar(name);
+    }
+
+    public void defineText(SchemaBuilder builder) {
+      builder.add(getName(), Types.optional(MinorType.VARCHAR));
+    }
+
+    public abstract void define(SchemaBuilder builder);
+
+    public abstract void load(Object value);
+
+    public ScalarWriter addText(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public ArrayWriter addList(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public ArrayWriter addListMap(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public TupleWriter addMap(String name) {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  protected static class GenericColumnDefn extends ColumnDefn {
+
+    public GenericColumnDefn(String name) {
+      super(name);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      if (ImageMetadataUtils.isVarchar(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.VARCHAR));
+      } else if (ImageMetadataUtils.isInt(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.INT));
+      } else if (ImageMetadataUtils.isLong(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.BIGINT));
+      } else if (ImageMetadataUtils.isDouble(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.FLOAT8));
+      } else if (ImageMetadataUtils.isBoolean(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.BIT));
+      } else if (ImageMetadataUtils.isDate(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.TIMESTAMP));
+      }
+    }
+
+    @Override
+    public void load(Object value) {
+      if (value instanceof Date) {
+        getWriter().setTimestamp(Instant.ofEpochMilli(((Date) value).getTime()));","[{'comment': 'Can we add this to the writer object? ', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageBatchReader.java,"@@ -0,0 +1,461 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Date;
+import java.util.LinkedHashMap;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.drew.imaging.FileType;
+import com.drew.imaging.FileTypeDetector;
+import com.drew.imaging.ImageMetadataReader;
+import com.drew.imaging.ImageProcessingException;
+import com.drew.metadata.Directory;
+import com.drew.metadata.Metadata;
+import com.drew.metadata.eps.EpsDirectory;
+import com.drew.metadata.exif.ExifIFD0Directory;
+import com.drew.metadata.xmp.XmpDirectory;
+
+public class ImageBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(ImageBatchReader.class);
+
+  private final ImageFormatConfig config;
+  private final EasySubScan scan;
+  private CustomErrorContext errorContext;
+  private Path path;
+  private FileStatus fileStatus;
+  private BufferedInputStream metaInputStream;
+  private RowSetLoader loader;
+  private LinkedHashMap<String, ColumnDefn> genericColumns;
+  private Metadata metadata;
+
+  public ImageBatchReader(final ImageFormatConfig config, final EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    try {
+      errorContext = negotiator.parentErrorContext();
+      DrillFileSystem dfs = negotiator.fileSystem();
+      path = dfs.makeQualified(negotiator.split().getPath());
+      fileStatus = dfs.getFileStatus(path);
+      metaInputStream = new BufferedInputStream(dfs.openPossiblyCompressedStream(path));
+      logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failure in initial image inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    // define the schema
+    negotiator.tableSchema(defineMetadata(), false);
+    ResultSetLoader resultSetLoader = negotiator.build();
+    loader = resultSetLoader.writer();
+    // bind the writer for generic columns
+    bindColumns(loader);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    try {
+      loader.start();
+      // process generic metadata
+      processGenericMetadata();
+      // process external metadata
+      processExtenalMetadata();
+      loader.save();
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to estimates the file type. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (ImageProcessingException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in reading metadata from inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (Exception e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in processing metadata directory. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    return false;
+  }
+
+  @Override
+  public void close() {
+    AutoCloseables.closeSilently(metaInputStream);
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    genericColumns = new LinkedHashMap<>();
+    Collection<String> tags = GenericMetadataDirectory._tagNameMap.values();
+    for (String tagName : tags) {
+      if (!config.hasFileSystemMetadata() && ImageMetadataUtils.isSkipTag(tagName)) {
+        continue;
+      }
+      ColumnDefn columnDefn = new GenericColumnDefn(tagName);
+      if (config.isDescriptive()) {
+        columnDefn.defineText(builder);
+      } else {
+        columnDefn.define(builder);
+      }
+      genericColumns.put(ImageMetadataUtils.formatName(tagName), columnDefn);
+    }
+    Collections.unmodifiableMap(genericColumns);
+    return builder.buildSchema();
+  }
+
+  private void bindColumns(RowSetLoader loader) {
+    for (ColumnDefn columnDefn : genericColumns.values()) {
+      columnDefn.bind(loader);
+    }
+  }
+
+  private void processGenericMetadata() throws IOException, ImageProcessingException {
+    FileType fileType = FileTypeDetector.detectFileType(metaInputStream);
+    metadata = ImageMetadataReader.readMetadata(metaInputStream);
+    // Read for generic metadata at first
+    new GenericMetadataReader().read(fileType, fileStatus, metadata);
+    GenericMetadataDirectory genericMetadata = metadata.getFirstDirectoryOfType(GenericMetadataDirectory.class);
+    // Process the `Generic Metadata Directory`
+    ImageDirectoryProcessor.processGenericMetadataDirectory(genericMetadata, genericColumns, config);
+  }
+
+  private void processExtenalMetadata() {
+    boolean skipEPSPreview = false;
+    for (Directory directory : metadata.getDirectories()) {
+      // Skip the `Generic Metadata Directory`
+      String dictName = ImageMetadataUtils.formatName(directory.getName());
+      if (directory instanceof GenericMetadataDirectory) {
+        continue;
+      }
+      if (directory instanceof ExifIFD0Directory && skipEPSPreview) {
+        skipEPSPreview = false;
+        continue;
+      }
+      if (directory instanceof EpsDirectory) {
+        // If an EPS file contains a TIFF preview, skip the next IFD0
+        skipEPSPreview = directory.containsTag(EpsDirectory.TAG_TIFF_PREVIEW_SIZE);
+      }
+      // Process the `External Metadata Directory`
+      MapColumnDefn columnDefn = new MapColumnDefn(dictName).builder(loader);
+      ImageDirectoryProcessor.processDirectory(columnDefn, directory, metadata, config);
+      // Continue to process XmpDirectory if exists
+      if (directory instanceof XmpDirectory) {
+        ImageDirectoryProcessor.processXmpDirectory(columnDefn, (XmpDirectory) directory);
+      }
+    }
+  }
+
+  protected abstract static class ColumnDefn {","[{'comment': 'I wonder if we could add these abstract classes to the `EasyFormatPlugin` classes so that they are usable for future storage plugins.  \r\n\r\nIf you take a look here: https://github.com/apache/drill/pull/1962\r\n\r\nI was working on refactoring them a bit to reduce the cut/paste code.\r\n', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageBatchReader.java,"@@ -0,0 +1,461 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Date;
+import java.util.LinkedHashMap;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.drew.imaging.FileType;
+import com.drew.imaging.FileTypeDetector;
+import com.drew.imaging.ImageMetadataReader;
+import com.drew.imaging.ImageProcessingException;
+import com.drew.metadata.Directory;
+import com.drew.metadata.Metadata;
+import com.drew.metadata.eps.EpsDirectory;
+import com.drew.metadata.exif.ExifIFD0Directory;
+import com.drew.metadata.xmp.XmpDirectory;
+
+public class ImageBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(ImageBatchReader.class);
+
+  private final ImageFormatConfig config;
+  private final EasySubScan scan;
+  private CustomErrorContext errorContext;
+  private Path path;
+  private FileStatus fileStatus;
+  private BufferedInputStream metaInputStream;
+  private RowSetLoader loader;
+  private LinkedHashMap<String, ColumnDefn> genericColumns;
+  private Metadata metadata;
+
+  public ImageBatchReader(final ImageFormatConfig config, final EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    try {
+      errorContext = negotiator.parentErrorContext();
+      DrillFileSystem dfs = negotiator.fileSystem();
+      path = dfs.makeQualified(negotiator.split().getPath());
+      fileStatus = dfs.getFileStatus(path);
+      metaInputStream = new BufferedInputStream(dfs.openPossiblyCompressedStream(path));
+      logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failure in initial image inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    // define the schema
+    negotiator.tableSchema(defineMetadata(), false);
+    ResultSetLoader resultSetLoader = negotiator.build();
+    loader = resultSetLoader.writer();
+    // bind the writer for generic columns
+    bindColumns(loader);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    try {
+      loader.start();
+      // process generic metadata
+      processGenericMetadata();
+      // process external metadata
+      processExtenalMetadata();
+      loader.save();
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to estimates the file type. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (ImageProcessingException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in reading metadata from inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (Exception e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in processing metadata directory. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    return false;
+  }
+
+  @Override
+  public void close() {
+    AutoCloseables.closeSilently(metaInputStream);
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    genericColumns = new LinkedHashMap<>();
+    Collection<String> tags = GenericMetadataDirectory._tagNameMap.values();
+    for (String tagName : tags) {
+      if (!config.hasFileSystemMetadata() && ImageMetadataUtils.isSkipTag(tagName)) {
+        continue;
+      }
+      ColumnDefn columnDefn = new GenericColumnDefn(tagName);
+      if (config.isDescriptive()) {
+        columnDefn.defineText(builder);
+      } else {
+        columnDefn.define(builder);
+      }
+      genericColumns.put(ImageMetadataUtils.formatName(tagName), columnDefn);
+    }
+    Collections.unmodifiableMap(genericColumns);
+    return builder.buildSchema();
+  }
+
+  private void bindColumns(RowSetLoader loader) {
+    for (ColumnDefn columnDefn : genericColumns.values()) {
+      columnDefn.bind(loader);
+    }
+  }
+
+  private void processGenericMetadata() throws IOException, ImageProcessingException {
+    FileType fileType = FileTypeDetector.detectFileType(metaInputStream);
+    metadata = ImageMetadataReader.readMetadata(metaInputStream);
+    // Read for generic metadata at first
+    new GenericMetadataReader().read(fileType, fileStatus, metadata);
+    GenericMetadataDirectory genericMetadata = metadata.getFirstDirectoryOfType(GenericMetadataDirectory.class);
+    // Process the `Generic Metadata Directory`
+    ImageDirectoryProcessor.processGenericMetadataDirectory(genericMetadata, genericColumns, config);
+  }
+
+  private void processExtenalMetadata() {
+    boolean skipEPSPreview = false;
+    for (Directory directory : metadata.getDirectories()) {
+      // Skip the `Generic Metadata Directory`
+      String dictName = ImageMetadataUtils.formatName(directory.getName());
+      if (directory instanceof GenericMetadataDirectory) {
+        continue;
+      }
+      if (directory instanceof ExifIFD0Directory && skipEPSPreview) {
+        skipEPSPreview = false;
+        continue;
+      }
+      if (directory instanceof EpsDirectory) {
+        // If an EPS file contains a TIFF preview, skip the next IFD0
+        skipEPSPreview = directory.containsTag(EpsDirectory.TAG_TIFF_PREVIEW_SIZE);
+      }
+      // Process the `External Metadata Directory`
+      MapColumnDefn columnDefn = new MapColumnDefn(dictName).builder(loader);
+      ImageDirectoryProcessor.processDirectory(columnDefn, directory, metadata, config);
+      // Continue to process XmpDirectory if exists
+      if (directory instanceof XmpDirectory) {
+        ImageDirectoryProcessor.processXmpDirectory(columnDefn, (XmpDirectory) directory);
+      }
+    }
+  }
+
+  protected abstract static class ColumnDefn {
+
+    private final String name;
+    private final String originName; // not format
+    private ScalarWriter writer;
+
+    public ColumnDefn(String name) {
+      this.originName = name;
+      this.name = ImageMetadataUtils.formatName(name);
+    }
+
+    public String getName() {
+      return name;
+    }
+
+    public String getOriginName() {
+      return originName;
+    }
+
+    public ScalarWriter getWriter() {
+      return writer;
+    }
+
+    public void bind(RowSetLoader loader) {
+      writer = loader.scalar(name);
+    }
+
+    public void defineText(SchemaBuilder builder) {
+      builder.add(getName(), Types.optional(MinorType.VARCHAR));
+    }
+
+    public abstract void define(SchemaBuilder builder);
+
+    public abstract void load(Object value);
+
+    public ScalarWriter addText(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public ArrayWriter addList(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public ArrayWriter addListMap(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public TupleWriter addMap(String name) {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  protected static class GenericColumnDefn extends ColumnDefn {","[{'comment': 'Can you please add some Javadocs for these holder classes?', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageFormatConfig.java,"@@ -15,59 +15,58 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.drill.exec.store.image;
 
 import java.util.List;
 import java.util.Objects;
 
 import org.apache.drill.common.PlanStringBuilder;
 import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
 
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.fasterxml.jackson.annotation.JsonInclude.Include;
 import com.fasterxml.jackson.annotation.JsonTypeName;
-import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
 
-@JsonTypeName(""image"") @JsonInclude(Include.NON_DEFAULT)
+@JsonTypeName(ImageFormatConfig.NAME)","[{'comment': 'When we move this to `contrib` , can we please add a `README.md` file to explain the config options?', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageDirectoryProcessor.java,"@@ -0,0 +1,256 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import java.util.Date;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.TimeZone;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.store.image.ImageBatchReader.ColumnDefn;
+import org.apache.drill.exec.store.image.ImageBatchReader.ListColumnDefn;
+import org.apache.drill.exec.store.image.ImageBatchReader.MapColumnDefn;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.adobe.internal.xmp.XMPException;
+import com.adobe.internal.xmp.XMPIterator;
+import com.adobe.internal.xmp.XMPMeta;
+import com.adobe.internal.xmp.options.IteratorOptions;
+import com.adobe.internal.xmp.properties.XMPPropertyInfo;
+import com.drew.lang.KeyValuePair;
+import com.drew.lang.Rational;
+import com.drew.metadata.Directory;
+import com.drew.metadata.Metadata;
+import com.drew.metadata.StringValue;
+import com.drew.metadata.Tag;
+import com.drew.metadata.exif.ExifIFD0Directory;
+import com.drew.metadata.exif.ExifSubIFDDirectory;
+import com.drew.metadata.exif.GpsDirectory;
+import com.drew.metadata.jpeg.JpegComponent;
+import com.drew.metadata.png.PngDirectory;
+import com.drew.metadata.xmp.XmpDirectory;
+
+public class ImageDirectoryProcessor {","[{'comment': 'Can you explain what this class does exactly?  ', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageMetadataUtils.java,"@@ -0,0 +1,191 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import java.util.HashMap;
+
+import com.drew.metadata.Directory;
+import com.drew.metadata.exif.ExifInteropDirectory;
+import com.drew.metadata.exif.ExifSubIFDDirectory;
+import com.drew.metadata.exif.PanasonicRawIFD0Directory;
+import com.drew.metadata.exif.makernotes.FujifilmMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.NikonType2MakernoteDirectory;
+import com.drew.metadata.exif.makernotes.OlympusCameraSettingsMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.OlympusEquipmentMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.OlympusFocusInfoMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.OlympusImageProcessingMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.OlympusMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.OlympusRawDevelopment2MakernoteDirectory;
+import com.drew.metadata.exif.makernotes.OlympusRawDevelopmentMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.OlympusRawInfoMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.PanasonicMakernoteDirectory;
+import com.drew.metadata.exif.makernotes.SamsungType2MakernoteDirectory;
+import com.drew.metadata.exif.makernotes.SonyType6MakernoteDirectory;
+import com.drew.metadata.icc.IccDirectory;
+import com.drew.metadata.photoshop.PhotoshopDirectory;
+import com.drew.metadata.png.PngDirectory;
+
+public class ImageMetadataUtils {
+
+  public static boolean isVarchar(String name) {
+    HashMap<Integer, String> tags = GenericMetadataDirectory._tagNameMap;
+    // Format,Color Mode,Video Codec,Audio Codec
+    if (name.equals(tags.get(GenericMetadataDirectory.TAG_FORMAT))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_COLOR_MODE))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_VIDEO_CODEC))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_AUDIO_CODEC))) {
+      return true;
+    }
+    return false;
+  }
+
+  public static boolean isInt(String name) {
+    HashMap<Integer, String> tags = GenericMetadataDirectory._tagNameMap;
+    // Pixel Width,Pixel Height,Orientation,Bits Per Pixel,Audio Sample Size
+    if (name.equals(tags.get(GenericMetadataDirectory.TAG_PIXEL_WIDTH))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_PIXEL_HEIGHT))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_ORIENTATION))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_BITS_PER_PIXEL))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_AUDIO_SAMPLE_SIZE))) {
+      return true;
+    }
+    return false;
+  }
+
+  public static boolean isLong(String name) {
+    HashMap<Integer, String> tags = GenericMetadataDirectory._tagNameMap;
+    // File Size,Duration
+    if (name.equals(tags.get(GenericMetadataDirectory.TAG_FILE_SIZE))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_DURATION))) {
+      return true;
+    }
+    return false;
+  }
+
+  public static boolean isDouble(String name) {
+    HashMap<Integer, String> tags = GenericMetadataDirectory._tagNameMap;
+    // DPI Width,DPI Height,Frame Rate,Audio Sample Rate
+    if (name.equals(tags.get(GenericMetadataDirectory.TAG_DPI_WIDTH))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_DPI_HEIGHT))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_FRAME_RATE))
+        || name.equals(tags.get(GenericMetadataDirectory.TAG_AUDIO_SAMPLE_RATE))) {
+      return true;
+    }
+    return false;
+  }
+
+  public static boolean isBoolean(String name) {
+    HashMap<Integer, String> tags = GenericMetadataDirectory._tagNameMap;
+    // Has Alpha
+    if (name.equals(tags.get(GenericMetadataDirectory.TAG_HAS_ALPHA))) {
+      return true;
+    }
+    return false;
+  }
+
+  public static boolean isDate(String name) {
+    HashMap<Integer, String> tags = GenericMetadataDirectory._tagNameMap;
+    // File Date Time
+    if (name.equals(tags.get(GenericMetadataDirectory.TAG_FILE_DATE_TIME))) {
+      return true;
+    }
+    return false;
+  }
+
+  /**
+   * Format the tag name (remove the spaces and special characters)
+   * @param tagName
+   * @return
+   */
+  public static String formatName(final String tagName) {
+    StringBuilder builder = new StringBuilder();
+    boolean upperCase = true;
+    for (char c : tagName.toCharArray()) {
+      if (c == ' ' || c == '-' || c == '/') {
+        upperCase = true;
+      } else {
+        builder.append(upperCase ? Character.toUpperCase(c) : c);
+        upperCase = false;
+      }
+    }
+    return builder.toString();
+  }
+
+  /**
+   * Skip the tag if parameter (fileSystemMetadata) is false
+   * @param name
+   * @return
+   */
+  public static boolean isSkipTag(String name) {
+    HashMap<Integer, String> tags = GenericMetadataDirectory._tagNameMap;
+    if (name.equals(tags.get(GenericMetadataDirectory.TAG_FILE_SIZE))","[{'comment': 'I think you can remove these `if` statements. IE:\r\n```\r\n// Instead of \r\nif (x || y )\r\n\r\n// Just have \r\nreturn x || y\r\n```\r\nHere and elsewhere in this class.', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageFormatConfig.java,"@@ -15,59 +15,58 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.drill.exec.store.image;
 
 import java.util.List;
 import java.util.Objects;
 
 import org.apache.drill.common.PlanStringBuilder;
 import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
 
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.fasterxml.jackson.annotation.JsonInclude.Include;
 import com.fasterxml.jackson.annotation.JsonTypeName;
-import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
 
-@JsonTypeName(""image"") @JsonInclude(Include.NON_DEFAULT)
+@JsonTypeName(ImageFormatConfig.NAME)
+@JsonInclude(JsonInclude.Include.NON_DEFAULT)
 public class ImageFormatConfig implements FormatPluginConfig {
 
+  public static final String NAME = ""image"";
   private final List<String> extensions;
   private final boolean fileSystemMetadata;
   private final boolean descriptive;
   private final String timeZone;
 
-  public ImageFormatConfig() {
-    this(null, null, null, null);
-  }
-
   @JsonCreator
   public ImageFormatConfig(
       @JsonProperty(""extensions"") List<String> extensions,
       @JsonProperty(""fileSystemMetadata"") Boolean fileSystemMetadata,
       @JsonProperty(""descriptive"") Boolean descriptive,
       @JsonProperty(""timeZone"") String timeZone) {
-    this.extensions = extensions == null ?
-        ImmutableList.of() : ImmutableList.copyOf(extensions);
+    this.extensions = extensions == null ? ImmutableList.of() : ImmutableList.copyOf(extensions);
     this.fileSystemMetadata = fileSystemMetadata == null ? true : fileSystemMetadata;","[{'comment': 'This can be simplified to:\r\n```\r\nthis.fileSystemMetadata = fileSystemMetadata == null || fileSystemMetadata;\r\n```', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageBatchReader.java,"@@ -0,0 +1,461 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Date;
+import java.util.LinkedHashMap;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.drew.imaging.FileType;
+import com.drew.imaging.FileTypeDetector;
+import com.drew.imaging.ImageMetadataReader;
+import com.drew.imaging.ImageProcessingException;
+import com.drew.metadata.Directory;
+import com.drew.metadata.Metadata;
+import com.drew.metadata.eps.EpsDirectory;
+import com.drew.metadata.exif.ExifIFD0Directory;
+import com.drew.metadata.xmp.XmpDirectory;
+
+public class ImageBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(ImageBatchReader.class);
+
+  private final ImageFormatConfig config;
+  private final EasySubScan scan;
+  private CustomErrorContext errorContext;
+  private Path path;
+  private FileStatus fileStatus;
+  private BufferedInputStream metaInputStream;
+  private RowSetLoader loader;
+  private LinkedHashMap<String, ColumnDefn> genericColumns;
+  private Metadata metadata;
+
+  public ImageBatchReader(final ImageFormatConfig config, final EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    try {
+      errorContext = negotiator.parentErrorContext();
+      DrillFileSystem dfs = negotiator.fileSystem();
+      path = dfs.makeQualified(negotiator.split().getPath());
+      fileStatus = dfs.getFileStatus(path);
+      metaInputStream = new BufferedInputStream(dfs.openPossiblyCompressedStream(path));
+      logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failure in initial image inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    // define the schema
+    negotiator.tableSchema(defineMetadata(), false);
+    ResultSetLoader resultSetLoader = negotiator.build();
+    loader = resultSetLoader.writer();
+    // bind the writer for generic columns
+    bindColumns(loader);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    try {
+      loader.start();
+      // process generic metadata
+      processGenericMetadata();
+      // process external metadata
+      processExtenalMetadata();
+      loader.save();
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to estimates the file type. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (ImageProcessingException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in reading metadata from inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (Exception e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in processing metadata directory. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    return false;
+  }
+
+  @Override
+  public void close() {
+    AutoCloseables.closeSilently(metaInputStream);
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    genericColumns = new LinkedHashMap<>();
+    Collection<String> tags = GenericMetadataDirectory._tagNameMap.values();
+    for (String tagName : tags) {
+      if (!config.hasFileSystemMetadata() && ImageMetadataUtils.isSkipTag(tagName)) {
+        continue;
+      }
+      ColumnDefn columnDefn = new GenericColumnDefn(tagName);
+      if (config.isDescriptive()) {
+        columnDefn.defineText(builder);
+      } else {
+        columnDefn.define(builder);
+      }
+      genericColumns.put(ImageMetadataUtils.formatName(tagName), columnDefn);
+    }
+    Collections.unmodifiableMap(genericColumns);","[{'comment': 'Do we need this?  The result is not assigned to anything.', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/main/java/org/apache/drill/exec/store/image/ImageBatchReader.java,"@@ -0,0 +1,461 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Date;
+import java.util.LinkedHashMap;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
+import org.apache.drill.exec.vector.accessor.ArrayWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.joda.time.Instant;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.drew.imaging.FileType;
+import com.drew.imaging.FileTypeDetector;
+import com.drew.imaging.ImageMetadataReader;
+import com.drew.imaging.ImageProcessingException;
+import com.drew.metadata.Directory;
+import com.drew.metadata.Metadata;
+import com.drew.metadata.eps.EpsDirectory;
+import com.drew.metadata.exif.ExifIFD0Directory;
+import com.drew.metadata.xmp.XmpDirectory;
+
+public class ImageBatchReader implements ManagedReader<FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(ImageBatchReader.class);
+
+  private final ImageFormatConfig config;
+  private final EasySubScan scan;
+  private CustomErrorContext errorContext;
+  private Path path;
+  private FileStatus fileStatus;
+  private BufferedInputStream metaInputStream;
+  private RowSetLoader loader;
+  private LinkedHashMap<String, ColumnDefn> genericColumns;
+  private Metadata metadata;
+
+  public ImageBatchReader(final ImageFormatConfig config, final EasySubScan scan) {
+    this.config = config;
+    this.scan = scan;
+  }
+
+  @Override
+  public boolean open(FileSchemaNegotiator negotiator) {
+    try {
+      errorContext = negotiator.parentErrorContext();
+      DrillFileSystem dfs = negotiator.fileSystem();
+      path = dfs.makeQualified(negotiator.split().getPath());
+      fileStatus = dfs.getFileStatus(path);
+      metaInputStream = new BufferedInputStream(dfs.openPossiblyCompressedStream(path));
+      logger.debug(""The config is {}, root is {}, columns has {}"", config, scan.getSelectionRoot(), scan.getColumns());
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failure in initial image inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    // define the schema
+    negotiator.tableSchema(defineMetadata(), false);
+    ResultSetLoader resultSetLoader = negotiator.build();
+    loader = resultSetLoader.writer();
+    // bind the writer for generic columns
+    bindColumns(loader);
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    try {
+      loader.start();
+      // process generic metadata
+      processGenericMetadata();
+      // process external metadata
+      processExtenalMetadata();
+      loader.save();
+    } catch (IOException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to estimates the file type. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (ImageProcessingException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in reading metadata from inputstream. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    } catch (Exception e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Error in processing metadata directory. "" + e.getMessage())
+              .addContext(errorContext)
+              .build(logger);
+    }
+    return false;
+  }
+
+  @Override
+  public void close() {
+    AutoCloseables.closeSilently(metaInputStream);
+  }
+
+  private TupleMetadata defineMetadata() {
+    SchemaBuilder builder = new SchemaBuilder();
+    genericColumns = new LinkedHashMap<>();
+    Collection<String> tags = GenericMetadataDirectory._tagNameMap.values();
+    for (String tagName : tags) {
+      if (!config.hasFileSystemMetadata() && ImageMetadataUtils.isSkipTag(tagName)) {
+        continue;
+      }
+      ColumnDefn columnDefn = new GenericColumnDefn(tagName);
+      if (config.isDescriptive()) {
+        columnDefn.defineText(builder);
+      } else {
+        columnDefn.define(builder);
+      }
+      genericColumns.put(ImageMetadataUtils.formatName(tagName), columnDefn);
+    }
+    Collections.unmodifiableMap(genericColumns);
+    return builder.buildSchema();
+  }
+
+  private void bindColumns(RowSetLoader loader) {
+    for (ColumnDefn columnDefn : genericColumns.values()) {
+      columnDefn.bind(loader);
+    }
+  }
+
+  private void processGenericMetadata() throws IOException, ImageProcessingException {
+    FileType fileType = FileTypeDetector.detectFileType(metaInputStream);
+    metadata = ImageMetadataReader.readMetadata(metaInputStream);
+    // Read for generic metadata at first
+    new GenericMetadataReader().read(fileType, fileStatus, metadata);
+    GenericMetadataDirectory genericMetadata = metadata.getFirstDirectoryOfType(GenericMetadataDirectory.class);
+    // Process the `Generic Metadata Directory`
+    ImageDirectoryProcessor.processGenericMetadataDirectory(genericMetadata, genericColumns, config);
+  }
+
+  private void processExtenalMetadata() {
+    boolean skipEPSPreview = false;
+    for (Directory directory : metadata.getDirectories()) {
+      // Skip the `Generic Metadata Directory`
+      String dictName = ImageMetadataUtils.formatName(directory.getName());
+      if (directory instanceof GenericMetadataDirectory) {
+        continue;
+      }
+      if (directory instanceof ExifIFD0Directory && skipEPSPreview) {
+        skipEPSPreview = false;
+        continue;
+      }
+      if (directory instanceof EpsDirectory) {
+        // If an EPS file contains a TIFF preview, skip the next IFD0
+        skipEPSPreview = directory.containsTag(EpsDirectory.TAG_TIFF_PREVIEW_SIZE);
+      }
+      // Process the `External Metadata Directory`
+      MapColumnDefn columnDefn = new MapColumnDefn(dictName).builder(loader);
+      ImageDirectoryProcessor.processDirectory(columnDefn, directory, metadata, config);
+      // Continue to process XmpDirectory if exists
+      if (directory instanceof XmpDirectory) {
+        ImageDirectoryProcessor.processXmpDirectory(columnDefn, (XmpDirectory) directory);
+      }
+    }
+  }
+
+  protected abstract static class ColumnDefn {
+
+    private final String name;
+    private final String originName; // not format
+    private ScalarWriter writer;
+
+    public ColumnDefn(String name) {
+      this.originName = name;
+      this.name = ImageMetadataUtils.formatName(name);
+    }
+
+    public String getName() {
+      return name;
+    }
+
+    public String getOriginName() {
+      return originName;
+    }
+
+    public ScalarWriter getWriter() {
+      return writer;
+    }
+
+    public void bind(RowSetLoader loader) {
+      writer = loader.scalar(name);
+    }
+
+    public void defineText(SchemaBuilder builder) {
+      builder.add(getName(), Types.optional(MinorType.VARCHAR));
+    }
+
+    public abstract void define(SchemaBuilder builder);
+
+    public abstract void load(Object value);
+
+    public ScalarWriter addText(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public ArrayWriter addList(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public ArrayWriter addListMap(String name) {
+      throw new UnsupportedOperationException();
+    }
+
+    public TupleWriter addMap(String name) {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  protected static class GenericColumnDefn extends ColumnDefn {
+
+    public GenericColumnDefn(String name) {
+      super(name);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      if (ImageMetadataUtils.isVarchar(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.VARCHAR));
+      } else if (ImageMetadataUtils.isInt(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.INT));
+      } else if (ImageMetadataUtils.isLong(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.BIGINT));
+      } else if (ImageMetadataUtils.isDouble(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.FLOAT8));
+      } else if (ImageMetadataUtils.isBoolean(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.BIT));
+      } else if (ImageMetadataUtils.isDate(getOriginName())) {
+        builder.add(getName(), Types.optional(MinorType.TIMESTAMP));
+      }
+    }
+
+    @Override
+    public void load(Object value) {
+      if (value instanceof Date) {
+        getWriter().setTimestamp(Instant.ofEpochMilli(((Date) value).getTime()));
+      } else {
+        getWriter().setObject(value);
+      }
+    }
+  }
+
+  protected static class MapColumnDefn extends ColumnDefn {
+
+    private int index;
+    private TupleWriter writer;
+
+    public MapColumnDefn(String name) {
+      super(name);
+    }
+
+    @Override
+    public void bind(RowSetLoader loader) {
+      index = loader.tupleSchema().index(getName());
+      if (index == -1) {
+        index = loader.addColumn(SchemaBuilder.columnSchema(getName(), MinorType.MAP, DataMode.REQUIRED));
+      }
+      writer = loader.tuple(index);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) { }
+
+    @Override
+    public void load(Object value) { }
+
+    public MapColumnDefn builder(RowSetLoader loader) {
+      bind(loader);
+      return this;
+    }
+
+    public MapColumnDefn builder(TupleWriter writer) {
+      this.writer = writer;
+      return this;
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, b : ""2"" }
+     */
+    @Override
+    public ScalarWriter addText(String name) {
+      index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, MinorType.VARCHAR, DataMode.OPTIONAL));
+      } else { // rewrite the value
+        writer.column(name).events().restartRow();
+      }
+      return writer.scalar(index);
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, [ b : ""2"" ] }
+     */
+    @Override
+    public ArrayWriter addList(String name) {
+      index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, MinorType.VARCHAR, DataMode.REPEATED));
+      }
+      return writer.array(index);
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, [ { b : 2 } ] }
+     */
+    @Override
+    public ArrayWriter addListMap(String name) {
+      index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, MinorType.MAP, DataMode.REPEATED));
+      }
+      return writer.array(index);
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, { b : 2 } }
+     */
+    @Override
+    public TupleWriter addMap(String name) {
+      index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, MinorType.MAP, DataMode.REQUIRED));
+        return writer.tuple(index);
+      }
+      return writer.tuple(name);
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, [ 0, -1, 0, -1 ] }
+     */
+    public ArrayWriter addListByte(String name) {
+      index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, MinorType.TINYINT, DataMode.REPEATED));
+      }
+      return writer.array(index);
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, b : 2.0 }
+     */
+    public ScalarWriter addDouble(String name) {
+      index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, MinorType.FLOAT8, DataMode.OPTIONAL));
+      }
+      return writer.scalar(index);
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, b : date() }
+     */
+    public ScalarWriter addDate(String name) {
+      index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, MinorType.TIMESTAMP, DataMode.OPTIONAL));
+      }
+      return writer.scalar(index);
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, b : object() }
+     */
+    public ScalarWriter addObject(String name, MinorType type) {
+      index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, type, DataMode.OPTIONAL));
+      }
+      return writer.scalar(index);
+    }
+
+    /**
+     * example : { a : 1 } > { a : 1, b : 2 }
+     */
+    public ScalarWriter addIntToMap(TupleWriter writer, String name) {
+      int index = writer.tupleSchema().index(name);
+      if (index == -1) {
+        index = writer.addColumn(SchemaBuilder.columnSchema(name, MinorType.INT, DataMode.OPTIONAL));
+      }
+      return writer.scalar(index);
+    }
+  }
+
+  protected static class ListColumnDefn extends ColumnDefn {
+
+    private int index;
+    private ArrayWriter writer;
+
+    public ListColumnDefn(String name) {
+      super(name);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) { }
+
+    @Override
+    public void load(Object value) { }
+
+    public ListColumnDefn builder(ArrayWriter writer) {
+      this.writer = writer;
+      return this;
+    }
+
+    /**
+     * example : [ ] > [ { a : ""1"" } ]
+     */
+    @Override
+    public ScalarWriter addText(String name) {
+      TupleWriter map = writer.tuple();
+      index = map.tupleSchema().index(name);","[{'comment': '`index` can be local to this method.', 'commenter': 'cgivre'}]"
2141,exec/java-exec/src/test/java/org/apache/drill/exec/store/image/TestImageBatchReader.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.image;
+
+import static org.apache.drill.test.rowSet.RowSetUtilities.mapArray;
+import static org.apache.drill.test.rowSet.RowSetUtilities.mapValue;
+import static org.apache.drill.test.rowSet.RowSetUtilities.singleMap;
+import static org.apache.drill.test.rowSet.RowSetUtilities.strArray;
+import static org.junit.Assert.assertEquals;
+
+import java.nio.file.Paths;
+import java.util.Arrays;
+
+import org.apache.drill.categories.RowSetTests;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryBuilder;
+import org.apache.drill.test.QueryTestUtil;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(RowSetTests.class)
+public class TestImageBatchReader extends ClusterTest {
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));
+    cluster.defineFormat(""dfs"", ""image"", new ImageFormatConfig(Arrays.asList(""bmp"", ""jpg"", ""mp4""), false, false, null));","[{'comment': 'Can we add some tests to test out the various configuration options?', 'commenter': 'cgivre'}]"
2142,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/profile/CoreOperatorType.java,"@@ -0,0 +1,342 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.server.rest.profile;
+
+
+import java.util.Arrays;
+
+/**
+ * This class is used for backward compatibility when reading older query profiles that
+ * stored operator id instead of its name.
+ * <b>Please do not update this class. It will be removed for Drill 2.0</b>
+ */
+public enum CoreOperatorType {
+
+  /**
+   * <code>SINGLE_SENDER = 0;</code>
+   */
+  SINGLE_SENDER(0),
+  /**
+   * <code>BROADCAST_SENDER = 1;</code>
+   */
+  BROADCAST_SENDER(1),
+  /**
+   * <code>FILTER = 2;</code>
+   */
+  FILTER(2),
+  /**
+   * <code>HASH_AGGREGATE = 3;</code>
+   */
+  HASH_AGGREGATE(3),
+  /**
+   * <code>HASH_JOIN = 4;</code>
+   */
+  HASH_JOIN(4),
+  /**
+   * <code>MERGE_JOIN = 5;</code>
+   */
+  MERGE_JOIN(5),
+  /**
+   * <code>HASH_PARTITION_SENDER = 6;</code>
+   */
+  HASH_PARTITION_SENDER(6),
+  /**
+   * <code>LIMIT = 7;</code>
+   */
+  LIMIT(7),
+  /**
+   * <code>MERGING_RECEIVER = 8;</code>
+   */
+  MERGING_RECEIVER(8),
+  /**
+   * <code>ORDERED_PARTITION_SENDER = 9;</code>
+   */
+  ORDERED_PARTITION_SENDER(9),
+  /**
+   * <code>PROJECT = 10;</code>
+   */
+  PROJECT(10),
+  /**
+   * <code>UNORDERED_RECEIVER = 11;</code>
+   */
+  UNORDERED_RECEIVER(11),
+  /**
+   * <code>RANGE_PARTITION_SENDER = 12;</code>
+   */
+  RANGE_PARTITION_SENDER(12),
+  /**
+   * <code>SCREEN = 13;</code>
+   */
+  SCREEN(13),
+  /**
+   * <code>SELECTION_VECTOR_REMOVER = 14;</code>
+   */
+  SELECTION_VECTOR_REMOVER(14),
+  /**
+   * <code>STREAMING_AGGREGATE = 15;</code>
+   */
+  STREAMING_AGGREGATE(15),
+  /**
+   * <code>TOP_N_SORT = 16;</code>
+   */
+  TOP_N_SORT(16),
+  /**
+   * <code>EXTERNAL_SORT = 17;</code>
+   */
+  EXTERNAL_SORT(17),
+  /**
+   * <code>TRACE = 18;</code>
+   */
+  TRACE(18),
+  /**
+   * <code>UNION = 19;</code>
+   */
+  UNION(19),
+  /**
+   * <code>OLD_SORT = 20;</code>
+   */
+  OLD_SORT(20),
+  /**
+   * <code>PARQUET_ROW_GROUP_SCAN = 21;</code>
+   */
+  PARQUET_ROW_GROUP_SCAN(21),
+  /**
+   * <code>HIVE_SUB_SCAN = 22;</code>
+   */
+  HIVE_SUB_SCAN(22),
+  /**
+   * <code>SYSTEM_TABLE_SCAN = 23;</code>
+   */
+  SYSTEM_TABLE_SCAN(23),
+  /**
+   * <code>MOCK_SUB_SCAN = 24;</code>
+   */
+  MOCK_SUB_SCAN(24),
+  /**
+   * <code>PARQUET_WRITER = 25;</code>
+   */
+  PARQUET_WRITER(25),
+  /**
+   * <code>DIRECT_SUB_SCAN = 26;</code>
+   */
+  DIRECT_SUB_SCAN(26),
+  /**
+   * <code>TEXT_WRITER = 27;</code>
+   */
+  TEXT_WRITER(27),
+  /**
+   * <code>TEXT_SUB_SCAN = 28;</code>
+   */
+  TEXT_SUB_SCAN(28),
+  /**
+   * <code>JSON_SUB_SCAN = 29;</code>
+   */
+  JSON_SUB_SCAN(29),
+  /**
+   * <code>INFO_SCHEMA_SUB_SCAN = 30;</code>
+   */
+  INFO_SCHEMA_SUB_SCAN(30),
+  /**
+   * <code>COMPLEX_TO_JSON = 31;</code>
+   */
+  COMPLEX_TO_JSON(31),
+  /**
+   * <code>PRODUCER_CONSUMER = 32;</code>
+   */
+  PRODUCER_CONSUMER(32),
+  /**
+   * <code>HBASE_SUB_SCAN = 33;</code>
+   */
+  HBASE_SUB_SCAN(33),
+  /**
+   * <code>WINDOW = 34;</code>
+   */
+  WINDOW(34),
+  /**
+   * <code>NESTED_LOOP_JOIN = 35;</code>
+   */
+  NESTED_LOOP_JOIN(35),
+  /**
+   * <code>AVRO_SUB_SCAN = 36;</code>
+   */
+  AVRO_SUB_SCAN(36),
+  /**
+   * <code>PCAP_SUB_SCAN = 37;</code>
+   */
+  PCAP_SUB_SCAN(37),
+  /**
+   * <code>KAFKA_SUB_SCAN = 38;</code>
+   */
+  KAFKA_SUB_SCAN(38),
+  /**
+   * <code>KUDU_SUB_SCAN = 39;</code>
+   */
+  KUDU_SUB_SCAN(39),
+  /**
+   * <code>FLATTEN = 40;</code>
+   */
+  FLATTEN(40),
+  /**
+   * <code>LATERAL_JOIN = 41;</code>
+   */
+  LATERAL_JOIN(41),
+  /**
+   * <code>UNNEST = 42;</code>
+   */
+  UNNEST(42),
+  /**
+   * <code>HIVE_DRILL_NATIVE_PARQUET_ROW_GROUP_SCAN = 43;</code>
+   */
+  HIVE_DRILL_NATIVE_PARQUET_ROW_GROUP_SCAN(43),
+  /**
+   * <code>JDBC_SCAN = 44;</code>
+   */
+  JDBC_SCAN(44),
+  /**
+   * <code>REGEX_SUB_SCAN = 45;</code>
+   */
+  REGEX_SUB_SCAN(45),
+  /**
+   * <code>MAPRDB_SUB_SCAN = 46;</code>
+   */
+  MAPRDB_SUB_SCAN(46),
+  /**
+   * <code>MONGO_SUB_SCAN = 47;</code>
+   */
+  MONGO_SUB_SCAN(47),
+  /**
+   * <code>KUDU_WRITER = 48;</code>
+   */
+  KUDU_WRITER(48),
+  /**
+   * <code>OPEN_TSDB_SUB_SCAN = 49;</code>
+   */
+  OPEN_TSDB_SUB_SCAN(49),
+  /**
+   * <code>JSON_WRITER = 50;</code>
+   */
+  JSON_WRITER(50),
+  /**
+   * <code>HTPPD_LOG_SUB_SCAN = 51;</code>
+   */
+  HTPPD_LOG_SUB_SCAN(51),
+  /**
+   * <code>IMAGE_SUB_SCAN = 52;</code>
+   */
+  IMAGE_SUB_SCAN(52),
+  /**
+   * <code>SEQUENCE_SUB_SCAN = 53;</code>
+   */
+  SEQUENCE_SUB_SCAN(53),
+  /**
+   * <code>PARTITION_LIMIT = 54;</code>
+   */
+  PARTITION_LIMIT(54),
+  /**
+   * <code>PCAPNG_SUB_SCAN = 55;</code>
+   */
+  PCAPNG_SUB_SCAN(55),
+  /**
+   * <code>RUNTIME_FILTER = 56;</code>
+   */
+  RUNTIME_FILTER(56),
+  /**
+   * <code>ROWKEY_JOIN = 57;</code>
+   */
+  ROWKEY_JOIN(57),
+  /**
+   * <code>SYSLOG_SUB_SCAN = 58;</code>
+   */
+  SYSLOG_SUB_SCAN(58),
+  /**
+   * <code>STATISTICS_AGGREGATE = 59;</code>
+   */
+  STATISTICS_AGGREGATE(59),
+  /**
+   * <code>UNPIVOT_MAPS = 60;</code>
+   */
+  UNPIVOT_MAPS(60),
+  /**
+   * <code>STATISTICS_MERGE = 61;</code>
+   */
+  STATISTICS_MERGE(61),
+  /**
+   * <code>LTSV_SUB_SCAN = 62;</code>
+   */
+  LTSV_SUB_SCAN(62),
+  /**
+   * <code>HDF5_SUB_SCAN = 63;</code>
+   */
+  HDF5_SUB_SCAN(63),
+  /**
+   * <code>EXCEL_SUB_SCAN = 64;</code>
+   */
+  EXCEL_SUB_SCAN(64),
+  /**
+   * <code>SHP_SUB_SCAN = 65;</code>
+   */
+  SHP_SUB_SCAN(65),
+  /**
+   * <code>METADATA_HANDLER = 66;</code>
+   */
+  METADATA_HANDLER(66),
+  /**
+   * <code>METADATA_CONTROLLER = 67;</code>
+   */
+  METADATA_CONTROLLER(67),
+  /**
+   * <code>DRUID_SUB_SCAN = 68;</code>
+   */
+  DRUID_SUB_SCAN(68),
+  /**
+   * <code>SPSS_SUB_SCAN = 69;</code>
+   */
+  SPSS_SUB_SCAN(69),
+  /**
+   * <code>HTTP_SUB_SCAN = 70;</code>
+   */
+  HTTP_SUB_SCAN(70),
+  /**
+   * <code>XML_SUB_SCAN = 71;</code>
+   */
+  XML_SUB_SCAN(71);
+
+  private final int value;
+
+  CoreOperatorType(int value) {
+    this.value = value;
+  }
+
+  public int getId() {
+    return value;
+  }
+
+  public static CoreOperatorType valueOf(int id) {
+    if (id >= 0 && id <= XML_SUB_SCAN.getId()) {","[{'comment': '@vvysotskyi \r\nThanks for the PR.  If someone submits a new format or storage plugin, do they have to modify this file?', 'commenter': 'cgivre'}, {'comment': ""No, this class shouldn't be modified anymore. It is used for backward compatibility only. I've tried to state that in its Javadoc, but please let me know if it should be extended with some additional details.\r\n\r\nIf someone submits a format or plugin, they will specify a string that will be used as operator type in the query profile."", 'commenter': 'vvysotskyi'}, {'comment': 'Perfect', 'commenter': 'cgivre'}]"
2142,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/profile/OperatorWrapper.java,"@@ -303,27 +313,27 @@ public void addSummary(TableBuilder tb, Map<String, Long> majorFragmentBusyTally
    * @param operatorType
    * @return index of spill metric
    */
-  private int getSpillCycleMetricIndex(CoreOperatorType operatorType) {
+  private int getSpillCycleMetricIndex(String operatorType) {
     // TODO: DRILL-6642, replace null values for ProtocolMessageEnum with UNRECOGNIZED NullValue to avoid null checks
     if (operatorType == null) {","[{'comment': 'Do you think it would be worth addressing DRILL-6642 in this PR?', 'commenter': 'cgivre'}, {'comment': ""Looks like some other Jira ticket number was meant there since DRILL-6642 is resolved. \r\nWe can't use the non-null value for operator type (it is string now) until we preserve backward compatibility for reading older profiles. With the changes in this PR, we check whether the operator type string is set in the profile, and if not, we will read the operator id that was used before these changes. Having some default value will enforce us to move additional checks to other places, so I don't think that we should address that ticket until we use both values. Maybe for Drill 2.0 when we will be able to break the compatibility, it may be addressed."", 'commenter': 'vvysotskyi'}]"
2142,exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/scan/v3/ScanFixture.java,"@@ -81,8 +81,8 @@ public ScanFixture build() {
       Scan scanConfig = new AbstractSubScan(""bob"") {
 ","[{'comment': 'Bob?', 'commenter': 'cgivre'}, {'comment': ""It was added in the scope of from DRILL-7701, so I'm not sure about the intention, perhaps some random name that may be also used in some tests.\r\nOr this name is used intentionally if we also have Alice somewhere in the code 🙂"", 'commenter': 'vvysotskyi'}]"
2143,pom.xml,"@@ -47,9 +47,9 @@
     <junit.version>4.12</junit.version>
     <slf4j.version>1.7.26</slf4j.version>
     <shaded.guava.version>28.2-jre</shaded.guava.version>
-    <guava.version>19.0</guava.version>
+    <guava.version>19.0</guava.version> <!--todo: 28.2-jre guava can be used here-->","[{'comment': 'Can we create a JIRA for this?', 'commenter': 'cgivre'}, {'comment': ""Let's create a ticket instead of adding the comment. Also, there are newer versions of Guava."", 'commenter': 'vvysotskyi'}, {'comment': 'ok. Yes, I know about newer version. But the newer version can bring some new other issues for sure.\r\nThe plan to check `<guava.version>28.2-jre</guava.version>` at first. If it is fine then to drop `<shaded.guava.version>` at all. And then to update guava version to the newest one', 'commenter': 'vdiravka'}, {'comment': ""Actually, there's a CVE for guava < 29.  Could we upgrade to guava 30-jre?"", 'commenter': 'cgivre'}, {'comment': 'WIP: [DRILL-7904: Update to 30-jre Guava version](https://issues.apache.org/jira/browse/DRILL-7904)', 'commenter': 'vdiravka'}, {'comment': 'WIP: [DRILL-7904: Update to 30-jre Guava version](https://issues.apache.org/jira/browse/DRILL-7904)', 'commenter': 'vdiravka'}, {'comment': 'So the comment can be removed?', 'commenter': 'vvysotskyi'}]"
2143,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetGroupConverter.java,"@@ -328,24 +329,26 @@ protected PrimitiveConverter getConverterForType(String name, PrimitiveType type
         }
       }
       case FIXED_LEN_BYTE_ARRAY:
-        switch (type.getOriginalType()) {
-          case DECIMAL: {
-            ParquetReaderUtility.checkDecimalTypeEnabled(options);
-            return getVarDecimalConverter(name, type);
-          }
-          case INTERVAL: {
-            IntervalWriter writer = type.isRepetition(Repetition.REPEATED)
-                ? getWriter(name, (m, f) -> m.list(f).interval(), l -> l.list().interval())
-                : getWriter(name, (m, f) -> m.interval(f), l -> l.interval());
-            return new DrillFixedLengthByteArrayToInterval(writer);
-          }
-          default: {
-            VarBinaryWriter writer = type.isRepetition(Repetition.REPEATED)
-                ? getWriter(name, (m, f) -> m.list(f).varBinary(), l -> l.list().varBinary())
-                : getWriter(name, (m, f) -> m.varBinary(f), l -> l.varBinary());
-            return new DrillFixedBinaryToVarbinaryConverter(writer, type.getTypeLength(), mutator.getManagedBuffer());
+        // TODO: to follow the latest parquet code, rewrite it by using LogicalTypeAnnotation instead of OriginalType
+        OriginalType originalType = type.getOriginalType();
+        if( originalType != null) {
+          switch (type.getOriginalType()) {
+            case DECIMAL: {
+              ParquetReaderUtility.checkDecimalTypeEnabled(options);
+              return getVarDecimalConverter(name, type);
+            }
+            case INTERVAL: {
+              IntervalWriter writer = type.isRepetition(Repetition.REPEATED)
+                      ? getWriter(name, (m, f) -> m.list(f).interval(), l -> l.list().interval())
+                      : getWriter(name, (m, f) -> m.interval(f), l -> l.interval());
+              return new DrillFixedLengthByteArrayToInterval(writer);
+            }
           }
         }
+        VarBinaryWriter writer = type.isRepetition(Repetition.REPEATED)
+                ? getWriter(name, (m, f) -> m.list(f).varBinary(), l -> l.list().varBinary())
+                : getWriter(name, MapWriter::varBinary, ListWriter::varBinary);
+        return new DrillFixedBinaryToVarbinaryConverter(writer, type.getTypeLength(), mutator.getManagedBuffer());","[{'comment': ""Don't you want to implement this TODO? :) Looks like it may be a simple visitor (by the way outer switch case may be rewritten in a similar way).\r\n```suggestion\r\n        LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveConverter> typeAnnotationVisitor = new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveConverter>() {\r\n          @Override\r\n          public Optional<PrimitiveConverter> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {\r\n            ParquetReaderUtility.checkDecimalTypeEnabled(options);\r\n            return Optional.of(getVarDecimalConverter(name, type));\r\n          }\r\n\r\n          @Override\r\n          public Optional<PrimitiveConverter> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType) {\r\n            IntervalWriter writer = type.isRepetition(Repetition.REPEATED)\r\n                ? getWriter(name, (m, f) -> m.list(f).interval(), l -> l.list().interval())\r\n                : getWriter(name, MapWriter::interval, ListWriter::interval);\r\n            return Optional.of(new DrillFixedLengthByteArrayToInterval(writer));\r\n          }\r\n        };\r\n\r\n        LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();\r\n        if (logicalTypeAnnotation != null) {\r\n          logicalTypeAnnotation.accept(typeAnnotationVisitor).orElseGet(() -> {\r\n            VarBinaryWriter writer = type.isRepetition(Repetition.REPEATED)\r\n                ? getWriter(name, (m, f) -> m.list(f).varBinary(), l -> l.list().varBinary())\r\n                : getWriter(name, MapWriter::varBinary, ListWriter::varBinary);\r\n            return new DrillFixedBinaryToVarbinaryConverter(writer, type.getTypeLength(), mutator.getManagedBuffer());\r\n          });\r\n        }\r\n```"", 'commenter': 'vvysotskyi'}, {'comment': 'ok, will do', 'commenter': 'vdiravka'}]"
2143,exec/java-exec/src/main/java/org/apache/parquet/hadoop/ParquetColumnChunkPageWriteStore.java,"@@ -260,14 +260,16 @@ public long getMemSize() {
     }
 
     /**
-     * Writes a number of pages within corresponding column chunk
+     * Writes a number of pages within corresponding column chunk <br>
+     * // TODO: the Bloom Filter can be useful in filtering entire row groups,
+     *     see <a href=""https://issues.apache.org/jira/browse/DRILL-7895"">DRILL-7895</a>","[{'comment': 'This class was created as a copy of the ColumnChunkPageWriteStore class from the parquet library (see DRILL-5544 for details)\r\n\r\nSince it is a copy, it is better to sync it with the original version instead of adding TODO with adding some specific features from it...', 'commenter': 'vvysotskyi'}, {'comment': ""Yes, you are right. The best way to remove Drill's copy of this class, we have `todo` about in `ParquetRecordWriter#256`, but we can't to it before [PARQUET-1006](https://issues.apache.org/jira/browse/PARQUET-1006) resolving. Adding the latest functionality from Parquet version of this class requires some deeper involving into it (it requires proper instantiating of `ParquetColumnChunkPageWriteStore`) and doesn't related to `UUID` logical type. That's why DRILL-7895 is created"", 'commenter': 'vdiravka'}, {'comment': ""I mean applying the same changes that were done earlier to the copy of this class to the newer version, it shouldn't be too complex..."", 'commenter': 'vvysotskyi'}, {'comment': 'I double checked Parquet `ColumnChunkPageWriteStore` and looks like we still use `ParquetDirectByteBufferAllocator` and allocate `DrillBuf` due to initializing `ParquetProperties` with proper allocator (see `ParquetRecordWriter`#`258`). I also debug `TestParquetWriter.testTPCHReadWriteRunRepeated` test case and found that Drill allocates the same memory for `byte[]` in Heap with `ColumnChunkPageWriteStore` and old `ParquetColumnChunkPageWriteStore` (~50% for my default settings).\r\nSo we can update `ParquetRecordWriter` with `ColumnChunkPageWriteStore`', 'commenter': 'vdiravka'}, {'comment': '@vdiravka, are you sure that heap memory usage is the same? I assumed that the main reason for using `ParquetColumnChunkPageWriteStore` was to use direct memory instead of heap one...\r\nFrom the code perspective, it looks like nothing was done in this direction for `ColumnChunkPageWriteStore`, it is still using the `ConcatenatingByteArrayCollector` for collecting data before writing it to the file, but our version uses `CapacityByteArrayOutputStream` that uses provided allocator.', 'commenter': 'vvysotskyi'}, {'comment': '1. I have checked heap memory after creating `ColumnChunkPageWriteStore` with `VisualVM` the size is the same:\r\nhttps://ibb.co/xFYqC0m\r\nhttps://ibb.co/fNB7MBq\r\n2. The allocator is passed to `ColumnChunkPageWriteStore` and `ColumnChunkPageWriter` too and really DrillBuf is used in process of writing the parquet file.\r\n3. And we converted that `buf` to bytes via `BytesInput.from(buf)` and `compressedBytes.writeAllTo(buf)`. So all data still placed in heap.\r\n4. We already have several other places, where `ColumnChunkPageWriteStore` is used not directly\r\n\r\nSo looks like updated `ColumnChunkPageWriteStore` will menage heap memory even better in process of creating parquet files via Drill and we safe here to go with current change.\r\n\r\nAnd the proper way to use Direct memory more that now is to make improvements in Parquet. One of them is [PARQUET-1771](https://github.com/apache/parquet-mr/pull/749), but that one will not help here. So I want to proceed with PARQUET-1006. Looks like we can use direct memory `buf` for `ColumnChunkPageWriteStore`, `ParquetFileWriter` and for `ByteArrayOutputStream`. I am planning to ask community about it.', 'commenter': 'vdiravka'}, {'comment': ""@vdiravka, thanks for sharing screenshots and providing more details.\r\n\r\n> 3. And we converted that buf to bytes via BytesInput.from(buf) and compressedBytes.writeAllTo(buf). So all data still placed in heap.\r\n\r\nPlease note, that when calling `BytesInput.from(buf)`, it doesn't convert all bytes of the buffer at the same time, it creates `CapacityBAOSBytesInput` that wraps provided `CapacityByteArrayOutputStream` and uses it when writing to the OutputStream.\r\nRegarding the `compressedBytes.writeAllTo(buf)` call this is fine to have bytes here since GC will take care of them, no reasons for possible leaks, data that should be processed later will be stored in direct memory.\r\n\r\nBut when using `ConcatenatingByteArrayCollector`, all bytes will be stored in heap (including data that should be processed later) so GC has no power here.\r\n\r\nNot sure why the heap usage you provided is similar, perhaps it may make difference when we will have more data, or GC will do its work right before flushing data from the `buf`... "", 'commenter': 'vvysotskyi'}]"
2143,exec/java-exec/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java,"@@ -0,0 +1,1633 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.parquet.hadoop;
+
+import static org.apache.parquet.format.Util.writeFileCryptoMetaData;
+import static org.apache.parquet.format.Util.writeFileMetaData;
+import static org.apache.parquet.format.converter.ParquetMetadataConverter.MAX_STATS_SIZE;
+import static org.apache.parquet.hadoop.ParquetWriter.DEFAULT_BLOCK_SIZE;
+import static org.apache.parquet.hadoop.ParquetWriter.MAX_PADDING_SIZE_DEFAULT;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.nio.charset.StandardCharsets;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.zip.CRC32;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+
+import org.apache.parquet.Preconditions;
+import org.apache.parquet.Version;
+import org.apache.parquet.bytes.BytesInput;
+import org.apache.parquet.bytes.BytesUtils;
+import org.apache.parquet.column.ColumnDescriptor;
+import org.apache.parquet.column.Encoding;
+import org.apache.parquet.column.EncodingStats;
+import org.apache.parquet.column.ParquetProperties;
+import org.apache.parquet.column.page.DictionaryPage;
+import org.apache.parquet.column.statistics.Statistics;
+import org.apache.parquet.column.values.bloomfilter.BloomFilter;
+import org.apache.parquet.crypto.AesCipher;
+import org.apache.parquet.crypto.ColumnEncryptionProperties;
+import org.apache.parquet.crypto.FileEncryptionProperties;
+import org.apache.parquet.crypto.InternalColumnEncryptionSetup;
+import org.apache.parquet.crypto.InternalFileEncryptor;
+import org.apache.parquet.crypto.ModuleCipherFactory;
+import org.apache.parquet.crypto.ModuleCipherFactory.ModuleType;
+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;
+import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel;
+import org.apache.parquet.hadoop.metadata.ColumnPath;
+import org.apache.parquet.format.BlockCipher;
+import org.apache.parquet.format.Util;
+import org.apache.parquet.format.converter.ParquetMetadataConverter;
+import org.apache.parquet.hadoop.metadata.BlockMetaData;
+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+import org.apache.parquet.hadoop.metadata.StrictKeyValueMetadataMergeStrategy;
+import org.apache.parquet.hadoop.metadata.FileMetaData;
+import org.apache.parquet.hadoop.metadata.GlobalMetaData;
+import org.apache.parquet.hadoop.metadata.KeyValueMetadataMergeStrategy;
+import org.apache.parquet.hadoop.metadata.ParquetMetadata;
+import org.apache.parquet.hadoop.util.HadoopOutputFile;
+import org.apache.parquet.hadoop.util.HadoopStreams;
+import org.apache.parquet.internal.column.columnindex.ColumnIndex;
+import org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder;
+import org.apache.parquet.internal.column.columnindex.OffsetIndex;
+import org.apache.parquet.internal.column.columnindex.OffsetIndexBuilder;
+import org.apache.parquet.internal.hadoop.metadata.IndexReference;
+import org.apache.parquet.io.InputFile;
+import org.apache.parquet.io.OutputFile;
+import org.apache.parquet.io.SeekableInputStream;
+import org.apache.parquet.io.ParquetEncodingException;
+import org.apache.parquet.io.PositionOutputStream;
+import org.apache.parquet.schema.MessageType;
+import org.apache.parquet.schema.PrimitiveType;
+import org.apache.parquet.schema.TypeUtil;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Internal implementation of the Parquet file writer as a block container<br>
+ * Note: this is temporary Drill-Parquet class needed to write empty parquet files. Details in
+ * <a href=""https://issues.apache.org/jira/browse/PARQUET-2026"">PARQUET-2026</a>
+ */
+public class ParquetFileWriter {","[{'comment': 'Is it possible somehow to reuse the original ParquetFileWriter to avoid so large code duplication?', 'commenter': 'vvysotskyi'}, {'comment': ""I wanted and tried to create a class extended from an original one and override `endBlock()` method. But unfortunately there are several private fields, which can be used only in scope of original class. So extending can't help. Anyway it is temporary solution, since the ticket to revisit dropping empty files is created and even if it will be rejected I'll create other ticket to make `endBlock()` be possible to be overridden. "", 'commenter': 'vdiravka'}]"
2143,exec/jdbc-all/pom.xml,"@@ -575,7 +575,7 @@
 
         <build>
           <plugins>
-            <plugin>
+            <plugin> <!-- TODO: this plugin has common things with default profile. Factor out this common things to avoid duplicate code -->","[{'comment': ""Could you please implement this TODO? It doesn't look to be complicated."", 'commenter': 'vvysotskyi'}, {'comment': 'ok :)', 'commenter': 'vdiravka'}, {'comment': '`maven-enforcer-plugin` is removed from `mapr` profile, because there is fully the same plugin in default scope.\r\nThere is also very similar `maven-shade-plugin`, but there are some differences. So before merging this plugin it is better to check it on `mapr` cluster, I think.', 'commenter': 'vdiravka'}, {'comment': 'Ok, thanks!', 'commenter': 'vvysotskyi'}]"
2143,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java,"@@ -263,10 +258,11 @@ private void newSchema() throws IOException {
         .withAllocator(new ParquetDirectByteBufferAllocator(oContext))
         .withValuesWriterFactory(new DefaultV1ValuesWriterFactory())
         .build();
-    pageStore = new ParquetColumnChunkPageWriteStore(codecFactory.getCompressor(codec), schema, initialSlabSize,
-        pageSize, parquetProperties.getAllocator(), parquetProperties.getPageWriteChecksumEnabled(),
-        parquetProperties.getColumnIndexTruncateLength()
-    );
+    // TODO: Replace ParquetColumnChunkPageWriteStore with ColumnChunkPageWriteStore from parquet library
+    // once PARQUET-1006 will be resolved","[{'comment': '@vdiravka Could you please create a JIRA for this and any other TODOs from this PR?', 'commenter': 'cgivre'}, {'comment': '[DRILL-7906](https://issues.apache.org/jira/browse/DRILL-7906) and [DRILL-7907](https://issues.apache.org/jira/browse/DRILL-7907) created', 'commenter': 'vdiravka'}]"
2143,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetSimpleTestFileGenerator.java,"@@ -46,7 +46,8 @@
  * that are supported by Drill. Embedded types specified in the Parquet specification are not covered by the
  * examples but can be added.
  * To create a new parquet file, define a schema, create a GroupWriter based on the schema, then add values
- * for individual records to the GroupWriter.
+ * for individual records to the GroupWriter.<br>
+ *     TODO: to run this tool please use 28.2-jre <guava.version> instead of 19.0 in main POM file","[{'comment': 'See comment above re: TODOs.  I know you already created one, but could you please update the comment with the JIRA?', 'commenter': 'cgivre'}]"
2160,exec/java-exec/src/test/java/org/apache/drill/exec/store/log/TestLogReader.java,"@@ -765,4 +777,16 @@ public void testFirewallSchema() throws RpcException {
     RowSetUtilities.verify(expected, result);
     result.clear();
   }
-}
+
+  @Test
+  public void testIssue7853() throws Exception {
+    thrownException.expect(UserRemoteException.class);
+    thrownException.expectMessage(""is not valid for type TIMESTAMP"");
+    String sql = ""SELECT * FROM cp.`regex/issue7853.log3`"";
+    QueryBuilder builder = client.queryBuilder().sql(sql);
+    RowSet sets = builder.rowSet();
+
+    assertEquals(2, sets.rowCount());
+    sets.clear();
+  }
+}","[{'comment': 'Missing newline', 'commenter': 'paul-rogers'}]"
2160,exec/java-exec/src/test/java/org/apache/drill/exec/store/log/TestLogReader.java,"@@ -177,6 +180,15 @@ private static LogFormatConfig firewallConfig() {
         regex, ""ssdlog"", null, schema);
   }
 
+  // DRILL-7853
+  private static LogFormatConfig issue7853Config() {
+    String regex = ""([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) ([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \\\""([^ ]*) ([^ ]*) (- |[^ ]*)\\\"" \\\""([^\\\""]*)\\\"" ([A-Z0-9-]+) ([A-Za-z0-9.-]*) ([^ ]*) \\\""([^\\\""]*)\\\"" \\\""([^\\\""]*)\\\"" \\\""([^\\\""]*)\\\"" ([-.0-9]*) ([^ ]*) \\\""([^\\\""]*)\\\""($| \\\""[^ ]*\\\"")(.*)"";
+    List<LogFormatField> schema = Lists.newArrayList(
+        new LogFormatField(""type"", ""VARCHAR""),
+        new LogFormatField(""time"", ""TIMESTAMP"", ""yyyy-MM-dd''T''HH:mm:ss.SSSSSSZ""));","[{'comment': ""The original source of the error is likely that the user's log format included a Joda-specific time format that must be changed to use a Java time format. Makes sense to better report the error. I suppose we should also add this change to the docs. I had forgotten that the log reader allowed Joda formats. No tests caught this issue. Thanks for adding this test."", 'commenter': 'paul-rogers'}]"
2160,exec/java-exec/src/test/java/org/apache/drill/exec/store/log/TestLogReader.java,"@@ -177,6 +180,15 @@ private static LogFormatConfig firewallConfig() {
         regex, ""ssdlog"", null, schema);
   }
 
+  // DRILL-7853
+  private static LogFormatConfig issue7853Config() {
+    String regex = ""([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) ([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \\\""([^ ]*) ([^ ]*) (- |[^ ]*)\\\"" \\\""([^\\\""]*)\\\"" ([A-Z0-9-]+) ([A-Za-z0-9.-]*) ([^ ]*) \\\""([^\\\""]*)\\\"" \\\""([^\\\""]*)\\\"" \\\""([^\\\""]*)\\\"" ([-.0-9]*) ([^ ]*) \\\""([^\\\""]*)\\\""($| \\\""[^ ]*\\\"")(.*)"";","[{'comment': 'Nit: to make the pattern more readable, use `\\d*` in place of `[0-9]*` and `\\W*` in place of `[(^ ]*)`.\r\n\r\nOne of the problems with the log format reader is that the above long regex is nearly unreadable. Better, in code, to provide the poor reader some hints (as we do elsewhere):\r\n\r\n```\r\n   regex = ""\\\\W*""  // field 1 name\r\n             + "" \\\\W*"" // field 2 name\r\n             ...\r\n```\r\n\r\nOr, even better:\r\n\r\n```\r\n    String plain = ""\\\\W*"";\r\n    String integer = ""\\\\d*"";\r\n    String sep = "" "";\r\n    ...\r\n    regex = plain // field1\r\n              + sep + plain // field 2\r\n     ...\r\n```\r\n\r\nNot pretty, but makes the code more readable. And, as you create the format, you can comment out all but a few fields.\r\n\r\nFor production use, there should be some simple way to provide this in the config since huge amount of time will be wasted trying to get the long regex correct in the current design.', 'commenter': 'paul-rogers'}, {'comment': 'Even better, whittle this test, and the required log file, down to the one field which failed: the date field. Then, the code here, and the test code below, can simply verify that the log reader is correctly parsing the date field (using Java date/time formats.)\r\n\r\nIf you make the log file itself simpler, then you can write the log file in the test. See the various `TestCsv*` tests for an example of how to do this.', 'commenter': 'paul-rogers'}]"
2160,exec/java-exec/src/test/java/org/apache/drill/exec/store/log/TestLogReader.java,"@@ -765,4 +777,16 @@ public void testFirewallSchema() throws RpcException {
     RowSetUtilities.verify(expected, result);
     result.clear();
   }
-}
+
+  @Test
+  public void testIssue7853() throws Exception {
+    thrownException.expect(UserRemoteException.class);
+    thrownException.expectMessage(""is not valid for type TIMESTAMP"");
+    String sql = ""SELECT * FROM cp.`regex/issue7853.log3`"";
+    QueryBuilder builder = client.queryBuilder().sql(sql);
+    RowSet sets = builder.rowSet();
+
+    assertEquals(2, sets.rowCount());
+    sets.clear();","[{'comment': 'This is fine. There is a simpler solution:\r\n\r\n```\r\n   QuerySummary result = client.queryBuilder().sql(sql).run();\r\n   assertEquals(2, result.records);\r\n```\r\n\r\nHowever, all this test does is assert not crash. What you really want to know is that the format worked. So, we want to build a result set with expected results as done in other tests. To avoid having to crate a zillion fields, change your query to select only the fields of interest.', 'commenter': 'paul-rogers'}]"
2160,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/scan/convert/StandardConversions.java,"@@ -181,6 +181,10 @@ public DirectConverter newInstance(
       final Constructor<? extends DirectConverter> ctor = conversionClass.getDeclaredConstructor(ScalarWriter.class, Map.class);
       return ctor.newInstance(baseWriter, mergeProperties(properties));
     } catch (final ReflectiveOperationException e) {
+      // There is no need to continue","[{'comment': 'Better comment might be:\r\n\r\n```\r\n  // Not a real reflection error: pass along underlying cause.\r\n```', 'commenter': 'paul-rogers'}]"
2169,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java,"@@ -804,4 +804,59 @@ public void testConvertFromJson() throws Exception {
         .baselineValues(""2"", ""abc"")
         .go();
   }
+  @Test // DRILL-7821","[{'comment': '@mkusnir Minor thing... Missing the new line between the  `}` and `@Test`.\r\nYou can push a revision again (please squash commits), then ready to merge, thanks', 'commenter': 'luocooong'}, {'comment': '@luocooong Sorry about that, just pushed the fix. Also, thanks for the rebase instructions you gave me on #2170.', 'commenter': 'mkusnir'}]"
2178,README.md,"@@ -15,21 +15,21 @@ Please read [Environment.md](docs/dev/Environment.md) for setting up and running
 ## More Information
 Please see the [Apache Drill Website](http://drill.apache.org/) or the [Apache Drill Documentation](http://drill.apache.org/docs/) for more information including:
 
- * Remote Execution Installation Instructions
- * [Running Drill on Docker instructions](https://drill.apache.org/docs/running-drill-on-docker/)
- * Information about how to submit logical and distributed physical plans
- * More example queries and sample data
- * Find out ways to be involved or discuss Drill
+* Remote Execution Installation Instructions
+* [Running Drill on Docker instructions](docs/dev/Docker.md)","[{'comment': ""Initially, the link was correct, but for some reason, it became broken... Could you please instead fix the page for the Drill website? Here is the corresponding doc: https://github.com/apache/drill/blob/gh-pages/_docs/install/installing-drill-in-embedded-mode/011-running-drill-on-docker.md\r\n\r\nWe shouldn't use the relative link here since this README.md is also displayed in https://hub.docker.com/r/apache/drill and this link will be broken there."", 'commenter': 'vvysotskyi'}, {'comment': ""Ok got it. As this seems  to be an issue related to another branch, so I'll have to close this ticket and then make another pr for the gh-pages."", 'commenter': 'eevanwong'}]"
2191,contrib/format-xml/src/main/java/org/apache/drill/exec/store/xml/XMLBatchReader.java,"@@ -89,9 +89,9 @@ private void openFile(FileScanFramework.FileSchemaNegotiator negotiator) {
       reader = new XMLReader(fsStream, dataLevel, maxRecords);
       reader.open(rootRowWriter, errorContext);
     } catch (Exception e) {
-      throw UserException
+      throw UserException //lgtm[java/unused-format-argument]","[{'comment': ""If you've fixed the cause of the lgtm warning, do we still need the comment?"", 'commenter': 'cgivre'}, {'comment': 'yep, my bad, forgot to remove it.', 'commenter': 'eevanwong'}]"
2191,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaPartitionScanSpec.java,"@@ -22,7 +22,7 @@
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 
-public class KafkaPartitionScanSpec {
+public class KafkaPartitionScanSpec { //lgtm[java/inconsistent-equals-and-hashcode]","[{'comment': 'We should probably fix this.  Inconsistent equals and hashcode could probably result in weird errors. ', 'commenter': 'cgivre'}, {'comment': ""Yeah definitely. I've been a bit split in terms of work so I'll look into this later."", 'commenter': 'eevanwong'}]"
2191,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java,"@@ -431,7 +431,7 @@ public void eval() {
         int power = 3 - i;
         try {
           int ip = Integer.parseInt(ipAddressInArray[i]);
-          result += ip * Math.pow(256, power);
+          result += (long)(ip * Math.pow(256, power)); //lgtm[java/implicit-cast-in-compound-assignment]","[{'comment': 'Do we still need the comment?', 'commenter': 'cgivre'}, {'comment': 'Nope. Will change.', 'commenter': 'eevanwong'}]"
2194,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/stream/StreamingHttpConnection.java,"@@ -219,12 +230,39 @@ private void emitRows(RowSetReader batchReader) throws IOException {
    */
   public void finish() throws IOException {
     JsonOutput gen = writer.jsonOutput();
-    gen.writeEndArray();
-    writeNewline(gen);
+    if (batchCount == 0) {
+      startHeader();
+      if (options != null &&
+          Boolean.parseBoolean(options.get(ExecConstants.ENABLE_VERBOSE_ERRORS_KEY))) {
+        emitErrorInfo();
+      }
+    } else {
+      gen.writeEndArray();
+      writeNewline(gen);
+    }
     gen.writeFieldName(""queryState"");
     gen.writeVarChar(getQueryState());
     writeNewline(gen);
     gen.writeEndObject();
     writeNewline(gen);
   }
+
+  private void emitErrorInfo() throws IOException {
+    JsonOutput gen = writer.jsonOutput();
+    Throwable exception = DrillExceptionUtil.getThrowable(error.getException());
+    if (exception != null) {
+      gen.writeFieldName(""exception"");
+      gen.writeVarChar(exception.getClass().getName());
+      writeNewline(gen);
+      gen.writeFieldName(""errorMessage"");
+      gen.writeVarChar(exception.getMessage());
+      writeNewline(gen);
+      gen.writeFieldName(""stackTrace"");","[{'comment': 'Stack traces are generally very large. Do you want to a) represent the items as a list, or b) prune away the uninteresting top-level bits?', 'commenter': 'paul-rogers'}, {'comment': 'Good point, thanks, updated the code to pass stack trace items as a list and updated the PR description with the example of how it will look like.', 'commenter': 'vvysotskyi'}]"
2194,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/stream/StreamingHttpConnection.java,"@@ -219,12 +230,39 @@ private void emitRows(RowSetReader batchReader) throws IOException {
    */
   public void finish() throws IOException {
     JsonOutput gen = writer.jsonOutput();
-    gen.writeEndArray();
-    writeNewline(gen);
+    if (batchCount == 0) {
+      startHeader();
+      if (options != null &&
+          Boolean.parseBoolean(options.get(ExecConstants.ENABLE_VERBOSE_ERRORS_KEY))) {","[{'comment': 'Do we want to reuse the rather verbose key here? It might be confusing as I would think, from the name, that I can also get the new behavior by setting the given config item.\r\n\r\nIn fact, I think we have a way to pass session options in a JSON query. Would it be better to simply use that mechanism here? Check if a new ""drill.rest.errors.verbose"" (or some such) option is set? This would let a site set the option by default, and would allow a future version of Drill make the verbose option the default.\r\n\r\nOtherwise, if we want to add the one-off options here, maybe use short names such as ""verbose"". Someone might add a ""schema-first"" option to export the schema before the data to avoid client-side buffering.', 'commenter': 'paul-rogers'}, {'comment': 'Agree, thanks. Updated the code to use a new session option which also can be passed in the query request.', 'commenter': 'vvysotskyi'}]"
2194,exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java,"@@ -1216,6 +1216,11 @@ public static String bootDefaultFor(String name) {
           ""the sender to send out its data more rapidly, but you should know that it has a risk to OOM when the system is solving parallel "" +
           ""large queries until we have a more accurate resource manager.""));
 
+
+  public static final String ENABLE_REST_VERBOSE_ERRORS_KEY = ""rest.errors.verbose"";","[{'comment': 'The name here should be consistent with other similar keys. For example:\r\n\r\n```\r\n  public static final String HTTP_ENABLE = ""drill.exec.http.enabled"";\r\n```\r\n\r\nSo, maybe, `drill.exec.http.rest.errors.verbose`. Kind of ugly.\r\n\r\nWe had talked about reworking the keys now that we know what they all are. But, until that is done as a unified task, best to maintain the existing naming conventions so we don\'t make things more of a mess than they already are.', 'commenter': 'paul-rogers'}, {'comment': 'Thanks, renamed', 'commenter': 'vvysotskyi'}]"
2194,exec/java-exec/src/main/java/org/apache/drill/exec/server/options/SystemOptionManager.java,"@@ -257,6 +257,7 @@
       new OptionDefinition(ClassCompilerSelector.JAVA_COMPILER_JANINO_MAXSIZE),
       new OptionDefinition(ClassCompilerSelector.JAVA_COMPILER_DEBUG),
       new OptionDefinition(ExecConstants.ENABLE_VERBOSE_ERRORS),
+      new OptionDefinition(ExecConstants.ENABLE_REST_VERBOSE_ERRORS),","[{'comment': 'There is one more step: add the default value to the `exec` version of `drill-module.conf`.', 'commenter': 'paul-rogers'}, {'comment': 'Yes, without this step everything will fail.', 'commenter': 'vvysotskyi'}]"
2194,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/BaseQueryRunner.java,"@@ -45,7 +45,7 @@
 
   protected final WorkManager workManager;
   protected final WebUserConnection webUserConnection;
-  private final OptionSet options;
+  protected final OptionSet options;","[{'comment': 'Probably not needed. See the `applyOptions` method and below.', 'commenter': 'paul-rogers'}]"
2194,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/stream/QueryRunner.java,"@@ -48,7 +48,7 @@ public void start(QueryWrapper query) throws ValidationException {
     startQuery(QueryType.valueOf(query.getQueryType()),
         query.getQuery(),
         userConn);
-    userConn.onStart(queryId, maxRows);
+    userConn.onStart(queryId, maxRows, options);","[{'comment': 'Not needed. The connection already has this member:\r\n\r\n```\r\n  protected WebSessionResources webSessionResources;\r\n```\r\n\r\nWhich provides:\r\n\r\n```\r\n  private final UserSession webUserSession;\r\n```\r\n\r\nWhich provides `getOptions()`', 'commenter': 'paul-rogers'}, {'comment': 'Thanks, used this method', 'commenter': 'vvysotskyi'}]"
2194,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/stream/StreamingHttpConnection.java,"@@ -80,9 +85,10 @@ public StreamingHttpConnection(WebSessionResources webSessionResources) {
    * Provide query info once the query starts. Sent from the REST request
    * thread.
    */
-  public void onStart(QueryId queryId, int rowLimit) {
+  public void onStart(QueryId queryId, int rowLimit, OptionSet options) {","[{'comment': 'Not needed. See above.', 'commenter': 'paul-rogers'}]"
2200,contrib/storage-kafka/pom.xml,"@@ -86,6 +86,25 @@
       <version>${project.version}</version>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>com.101tec</groupId>
+      <artifactId>zkclient</artifactId>
+      <version>0.11</version>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.kafka</groupId>
+      <artifactId>kafka_2.12</artifactId>
+      <version>${kafka.version}</version>
+      <classifier>test</classifier>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.servicemix.bundles</groupId>
+      <artifactId>org.apache.servicemix.bundles.kafka_2.12</artifactId>
+      <version>2.3.1_1</version>
+      <scope>test</scope>
+    </dependency>","[{'comment': 'Could you please revert these changes, since they are not necessary?', 'commenter': 'vvysotskyi'}, {'comment': 'Done!', 'commenter': 'cgivre'}]"
2200,contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaFilterPushdownTest.java,"@@ -419,13 +419,13 @@ public void testNoPushdownOfOffsetWithNonMetadataField() throws Exception {
     final int expectedRowCount = 30;
 
     final String queryString = String.format(TestQueryConstants.QUERY_TEMPLATE_AND_OR_PATTERN_2,
-        TestQueryConstants.JSON_PUSHDOWN_TOPIC, predicate1, predicate2, predicate3);
+      TestQueryConstants.JSON_PUSHDOWN_TOPIC, predicate1, predicate2, predicate3);
 
     runKafkaSQLVerifyCount(queryString,expectedRowCount);
     queryBuilder()
-        .sql(queryString)
-        .jsonPlanMatcher()
-        .include(String.format(EXPECTED_PATTERN, expectedRowCountInPlan))
-        .match();
+      .sql(queryString)
+      .jsonPlanMatcher()
+      .include(String.format(EXPECTED_PATTERN, expectedRowCountInPlan))
+      .match();
   }
-}
+}","[{'comment': 'Please revert formatting changes here and in other places. It is fine to have 4 spaces for the case when moving builder methods or arguments on the new line and having an empty line at the end of the class.', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi \r\nI put the version of this file from `master` back in this PR, so this **should** be what was originally there.  I didn't modify this file at all, so I'm not sure why the spacing got all messed up in the first place.  I hope that's ok. "", 'commenter': 'cgivre'}]"
2202,distribution/src/assemble/component.xml,"@@ -90,9 +90,6 @@
       <outputDirectory>jars</outputDirectory>
       <unpack>false</unpack>
       <useProjectArtifact>false</useProjectArtifact>
-      <includes>
-        <include>org.apache.drill:drill-shaded-guava:jar</include>
-      </includes>
     </dependencySet>
 
     <dependencySet>","[{'comment': 'Please remove the whole `dependencySet` block, since it is not needed without `includes`.', 'commenter': 'vvysotskyi'}, {'comment': 'right, thanks!', 'commenter': 'vdiravka'}]"
2202,docs/dev/ArtidfactsPublishing.md,"@@ -1,22 +1,8 @@
-# How to upgrade Guava in Drill","[{'comment': 'Please remove whole the file, since except for shaded guava, there is no need anymore for publishing libraries manual.', 'commenter': 'vvysotskyi'}, {'comment': ""I thought to do it too. But this info can be useful for any other library shade, can't it be?"", 'commenter': 'vdiravka'}]"
2202,pom.xml,"@@ -1047,8 +1046,9 @@
     </dependency>
 
     <dependency>
-      <groupId>org.apache.drill</groupId>
-      <artifactId>drill-shaded-guava</artifactId>
+      <groupId>com.google.guava</groupId>
+      <artifactId>guava</artifactId>
+      <version>${guava.version}</version>","[{'comment': 'Please remove the `version` line, version should be obtained from the dependencies management block.', 'commenter': 'vvysotskyi'}]"
2204,_docs/developer-information/contribute-to-drill/020-apache-drill-contribution-ideas.md,"@@ -84,8 +77,8 @@ Initially, concentrate on basics:
 
 Implement custom storage plugins for the following non-Hadoop data sources:
 
-  * NoSQL databases (such as Mongo, Cassandra, Couch etc)
-  * Search engines (such as Solr, Lucidworks, Elastic Search etc)
+  * NoSQL databases (such as Mongo, Couch etc)","[{'comment': ""Mongo is implemented.... maybe remove?  There actually is a Couchbase plugin out there also that someone implemented.  I've been trying to get them to commit that. "", 'commenter': 'cgivre'}, {'comment': 'what about clickhouse?', 'commenter': 'Leon-WTF'}, {'comment': 'Sure... Please add to the list!', 'commenter': 'cgivre'}, {'comment': ""I'd also add better support for streaming data such as Apache Flink and Pulsar."", 'commenter': 'cgivre'}, {'comment': '@Leon-WTF Nice idea. then you can create a JIRA about the `Add Storage Plugin for Clickhouse` directly, we assign to you.', 'commenter': 'luocooong'}, {'comment': 'Clickhouse runs SQL, even if their dialect does also include a good serving of nonstandard stuff, and has a JDBC driver so I think that it should already work to a significant extent...', 'commenter': 'jnturton'}, {'comment': 'Sorry for the misunderstand, I push two different commits together, and I will roll back the update of apache-drill-contribution-ideas.md, and we could use another PR to update it specificly.', 'commenter': 'kingswanwho'}]"
2204,_docs/developer-information/contribute-to-drill/020-apache-drill-contribution-ideas.md,"@@ -12,9 +12,6 @@ parent: ""Contribute to Drill""
     * BI Tool testing
   * General CLI improvements 
   * Eco system integrations
-    * MapReduce
-    * Hive views
-    * YARN
     * Spark
     * Hue","[{'comment': 'Hue?  ', 'commenter': 'cgivre'}, {'comment': 'Because of the reason I submitted two commits by accident, I will close this PR, and use two different PR for apache-drill-contribution-ideas.md and 050-value-vectors.md Update respectively.', 'commenter': 'kingswanwho'}]"
2204,_docs/developer-information/contribute-to-drill/020-apache-drill-contribution-ideas.md,"@@ -84,8 +77,8 @@ Initially, concentrate on basics:
 
 Implement custom storage plugins for the following non-Hadoop data sources:
 
-  * NoSQL databases (such as Mongo, Cassandra, Couch etc)
-  * Search engines (such as Solr, Lucidworks, Elastic Search etc)
+  * NoSQL databases (such as Mongo, Couch etc)
+  * Search engines (such as Solr, Lucidworks etc)
   * SQL databases (MySQL< PostGres etc)
   * Generic JDBC/ODBC data sources
   * HTTP URL","[{'comment': ""Since this was written, most of these are done... You can query most relational DBs via the JDBC plugin.  We don't have a generic ODBC plugin, so maybe leave that.  We do have an HTTP URL plugin. "", 'commenter': 'cgivre'}, {'comment': 'Sure, I will change back this part, and use another PR for the update of apache-drill-contribution-ideas.md.', 'commenter': 'kingswanwho'}]"
2216,contrib/storage-kudu/pom.xml,"@@ -57,7 +57,7 @@
     <dependency>
       <groupId>org.apache.kudu</groupId>
       <artifactId>kudu-client</artifactId>
-      <version>1.3.0</version>
+      <version>1.14.0</version>","[{'comment': 'Could you please confirm that with the update Drill is able to connect to Kudu? Also, there are no enabled unit tests for Kudu, existing ones are disabled and require running Kudu server.\r\nCould you please update tests to start Kudu using testcontainers, so we will be sure that it is still working correctly?', 'commenter': 'vvysotskyi'}, {'comment': ""> Could you please confirm that with the update Drill is able to connect to Kudu? Also, there are no enabled unit tests for Kudu, existing ones are disabled and require running Kudu server.\r\n> Could you please update tests to start Kudu using testcontainers, so we will be sure that it is still working correctly?\r\n\r\nSure!  I've never really used Kudu and don't know much about how it works, but I think I can figure out `testcontainers` and there is a docker container for Kudu.  I'll follow the examples in the Splunk plugin."", 'commenter': 'cgivre'}, {'comment': ""I did build this and tried it out on my local machine, and successfully got it to connect to Kudu, but I didn't have any data in Kudu... so I couldn't try queries."", 'commenter': 'cgivre'}, {'comment': 'We have two ignored test classes here, and one of them has code that creates tables in Kudu, and another one queries them. Could you please enable and update them to use the Kudu instance from test containers?', 'commenter': 'vvysotskyi'}, {'comment': ""> We have two ignored test classes here, and one of them has code that creates tables in Kudu, and another one queries them. Could you please enable and update them to use the Kudu instance from test containers?\r\n\r\nThe unit tests for this plugin are frankly pretty poor.  I'll figure out `testcontainers` and update.  Do you think I should break that out in a separate PR?  I'd really like to refactor the unit tests for Kudu if we're going to continue supporting it,  and that represents a lot more work than I intended in this PR.  "", 'commenter': 'cgivre'}]"
2217,pom.xml,"@@ -110,16 +110,18 @@
     <codemodel.version>2.6</codemodel.version>
     <joda.version>2.10.5</joda.version>
     <javax.el.version>3.0.0</javax.el.version>
+    <jna.version>5.8.0</jna.version>","[{'comment': 'Could you please explain that the function of this library?', 'commenter': 'luocooong'}, {'comment': 'JNA is an alternative of JNI that is used by other libraries to communicate with native libraries.\r\nThere has been an improvement to JNA (and transitively to TestContainers) via https://github.com/testcontainers/testcontainers-java/issues/3610', 'commenter': 'martin-g'}, {'comment': 'Here is a build at Travis that fails due to the old JNA version in that branch:  https://travis-ci.com/github/apache/drill/jobs/504592762', 'commenter': 'martin-g'}]"
2217,pom.xml,"@@ -110,16 +110,18 @@
     <codemodel.version>2.6</codemodel.version>
     <joda.version>2.10.5</joda.version>
     <javax.el.version>3.0.0</javax.el.version>
+    <jna.version>5.8.0</jna.version>
     <surefire.version>3.0.0-M4</surefire.version>
     <commons.compress.version>1.20</commons.compress.version>
     <hikari.version>3.4.2</hikari.version>
-    <netty.version>4.1.59.Final</netty.version>
+    <netty.version>4.1.63.Final</netty.version>","[{'comment': 'Netty is core of Drill RPC layer. So, Please separate this change into a new PR if the upgrade is necessary (for the aarch64).', 'commenter': 'luocooong'}, {'comment': ""I'll revert it. There is nothing aarch64 related in this version bump. The aarch64 improvements are only in netty-tcnative only."", 'commenter': 'martin-g'}]"
2217,pom.xml,"@@ -1161,6 +1165,16 @@
   <!-- Managed Dependencies -->
   <dependencyManagement>
     <dependencies>
+      <dependency>","[{'comment': 'Can you add these library to the specified sub-module (Unless all the sub-moudle depend on it)?', 'commenter': 'luocooong'}, {'comment': 'JNA comes as a transitive dependency of:\r\n- org.kohsuke:libpam4j in drill-java-exec and drill-jdbc-all\r\n- testcontainers/docker-java-api in all modules which use TestContainers (i.e. MySQL, Mongo, Splunk, Cassandra and Java Exec\r\n\r\nI think defining it in the root pom is the best!', 'commenter': 'martin-g'}]"
2217,pom.xml,"@@ -1726,7 +1740,7 @@
       <dependency>
         <groupId>io.netty</groupId>
         <artifactId>netty-tcnative</artifactId>
-        <version>2.0.1.Final</version>
+        <version>2.0.39.Final</version>","[{'comment': 'Could you please explain the reasons for this upgrade?', 'commenter': 'luocooong'}, {'comment': 'Netty-tcnative 2.0.31.Final has started providing aarch64 binaries for BoringSSL.\r\nSee https://github.com/netty/netty-tcnative/issues/552 and https://github.com/netty/netty-tcnative/pull/517', 'commenter': 'martin-g'}]"
2217,contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java,"@@ -51,51 +49,57 @@
 @Category(JdbcStorageTest.class)
 public class TestJdbcPluginWithMySQLIT extends ClusterTest {
 
-  private static EmbeddedMysql mysqld;
+  private static JdbcDatabaseContainer<?> jdbcContainer;
 
   @BeforeClass
   public static void initMysql() throws Exception {
     startCluster(ClusterFixture.builder(dirTestWatcher));
+    String osName = System.getProperty(""os.name"").toLowerCase();
     String mysqlDBName = ""drill_mysql_test"";
-    int mysqlPort = QueryTestUtil.getFreePortNumber(2215, 300);
 
-    MysqldConfig config = MysqldConfig.aMysqldConfig(Version.v5_7_27)
-        .withPort(mysqlPort)
-        .withUser(""mysqlUser"", ""mysqlPass"")
-        .withTimeZone(DateTimeZone.UTC.toTimeZone())
-        .build();
+    DockerImageName imageName;
+    if (osName.startsWith(""linux"") && ""aarch64"".equals(System.getProperty(""os.arch""))) {
+      imageName = DockerImageName.parse(""mariadb:10.6.0"").asCompatibleSubstituteFor(""mysql"");
+    } else {
+      imageName = DockerImageName.parse(""mysql:5.7.27"");","[{'comment': 'Could you please defined the version of db in the final variable?', 'commenter': 'luocooong'}, {'comment': 'Done!', 'commenter': 'martin-g'}]"
2217,pom.xml,"@@ -450,6 +452,8 @@
             <exclude>**/*.prj</exclude>
             <exclude>**/*.shp</exclude>
             <exclude>**/*.dbf</exclude>
+            <!-- DRILL-7911 MySQL config overrides -->","[{'comment': ""@martin-g It's ready to merge. Could you please resolve the conflict and remove this comment (don't need it anymore)? thanks"", 'commenter': 'luocooong'}, {'comment': ""@luocooong Let's wait for a successful TravisCI build. It just failed due to some problem with TestContainers-Ryuk. I've updated TestContainers-Vault to 1.15.3.\r\n\r\nThis comment is documenting what is .cnf file. But I will remove it!"", 'commenter': 'martin-g'}]"
2225,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/profile/ProfileResources.java,"@@ -83,6 +86,8 @@
   @Inject
   HttpServletRequest request;
 
+  static String importedProfile = null;","[{'comment': ""We are using a variable to store the profile and avoid loading the profile from disk, that's good. But how to do that if multiple users use the function at the same time..."", 'commenter': 'luocooong'}]"
2225,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/profile/ProfileResources.java,"@@ -395,6 +406,22 @@ public Viewable getProfile(@PathParam(""queryid"") String queryId){
     }
   }
 
+  @POST
+  @Path(""/profiles/view"")","[{'comment': 'Good naming : `view`, not the `import`. Could you please rename all names related to the ""import"" to ""view"" ?', 'commenter': 'luocooong'}]"
2225,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/profile/ProfileResources.java,"@@ -395,6 +406,22 @@ public Viewable getProfile(@PathParam(""queryid"") String queryId){
     }
   }
 
+  @POST
+  @Path(""/profiles/view"")
+  @Consumes(MediaType.MULTIPART_FORM_DATA)
+  @Produces(MediaType.TEXT_HTML)
+  public Viewable viewProfile(@FormDataParam(""profileFile"") String content){","[{'comment': 'It is possible to rename the ""profileFile"" to ""profile_object"" or ""profile_data"" ... ?', 'commenter': 'luocooong'}]"
2225,exec/java-exec/src/main/resources/rest/profile/list.ftl,"@@ -84,6 +84,13 @@
         $(""#queryCancelModal"").modal(""show"");
     }
 
+    function uploadProfile() {","[{'comment': 'Add a comment above the function.', 'commenter': 'luocooong'}]"
2225,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/profile/ProfileResources.java,"@@ -395,6 +406,22 @@ public Viewable getProfile(@PathParam(""queryid"") String queryId){
     }
   }
 
+  @POST
+  @Path(""/profiles/view"")
+  @Consumes(MediaType.MULTIPART_FORM_DATA)
+  @Produces(MediaType.TEXT_HTML)
+  public Viewable viewProfile(@FormDataParam(""profileFile"") String content){
+    try {
+      importedProfile = content;
+      QueryProfile profile = work.getContext().getProfileStoreContext().getProfileStoreConfig().getSerializer().deserialize(content.getBytes());
+      ProfileWrapper wrapper = new ProfileWrapper(profile, work.getContext().getConfig(), request);
+      return ViewableWithPermissions.create(authEnabled.get(), ""/rest/profile/profile.ftl"", sc, wrapper);
+    } catch (Exception | Error e) {
+      logger.error(""Exception was thrown when parsing profile {}:\n{}"", content, e);","[{'comment': 'Add a space between the ""profile {}"" and "":"".', 'commenter': 'luocooong'}]"
2225,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServerConstants.java,"@@ -45,4 +45,7 @@ private WebServerConstants() {}
 
   // Name of the CSRF protection token attribute
   public static final String CSRF_TOKEN = ""csrfToken"";
+
+  // Key of the profile data to view
+  public static final String PROFILE_DATA = ""profile_data"";","[{'comment': 'Good define.', 'commenter': 'luocooong'}]"
2225,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUtils.java,"@@ -55,7 +55,10 @@
   public static String getCsrfTokenFromHttpRequest(HttpServletRequest request) {
     // No need to create a session if not present (i.e. if a user is logged in)
     HttpSession session = request.getSession(false);
-    return session == null ? """" : (String) session.getAttribute(WebServerConstants.CSRF_TOKEN);
+    String res = session == null ? """" :
+      (String) session.getAttribute(WebServerConstants.CSRF_TOKEN);
+    // In case session is created when authentication is disabled
+    return res == null ? """" : res;","[{'comment': 'What happens without these changes? Is it possible to revert these if no effect for anything?', 'commenter': 'luocooong'}, {'comment': 'if drill.exec.security.user.auth.enabled=false, WebServerConstants.CSRF_TOKEN attribute will not be added into the session, after I created the session, this function will return null, which will cause exception when rendering the profile page', 'commenter': 'Leon-WTF'}]"
2225,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/profile/ProfileResources.java,"@@ -373,9 +379,18 @@ private QueryProfile getQueryProfile(String queryId) {
   @GET
   @Path(""/profiles/{queryid}.json"")
   @Produces(MediaType.APPLICATION_JSON)
-  public String getProfileJSON(@PathParam(""queryid"") String queryId) {
+  public String getProfileJSON(@PathParam(""queryid"") String queryId,
+                               @Context HttpServletRequest req) {
     try {
-      return new String(work.getContext().getProfileStoreContext().getProfileStoreConfig().getSerializer().serialize(getQueryProfile(queryId)));
+      HttpSession session = req.getSession(false);
+      if (session == null || session.getAttribute(PROFILE_DATA) == null) {","[{'comment': 'I think this session is not null here, because the `Cookie: JSESSIONID` is not null at this time. I recommend that :\r\n```java\r\nif(req.getSession().getAttribute(PROFILE_DATA) == null) { }\r\n```', 'commenter': 'luocooong'}, {'comment': 'if drill.exec.security.user.auth.enabled=false, session will be null', 'commenter': 'Leon-WTF'}]"
2225,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/profile/ProfileResources.java,"@@ -395,6 +410,33 @@ public Viewable getProfile(@PathParam(""queryid"") String queryId){
     }
   }
 
+  @POST
+  @Path(""/profiles/view"")
+  @Consumes(MediaType.MULTIPART_FORM_DATA)
+  @Produces(MediaType.TEXT_HTML)
+  public Viewable viewProfile(@FormDataParam(PROFILE_DATA) String content,
+                              @Context HttpServletRequest req){
+    try {
+      HttpSession session = req.getSession(true);
+      if (session.isNew()) {
+        session.setMaxInactiveInterval(work.getContext().getConfig()
+          .getInt(ExecConstants.HTTP_SESSION_MAX_IDLE_SECS));
+      }
+      session.setAttribute(PROFILE_DATA, content);","[{'comment': 'As the above said, I recommend that :\r\n```java\r\nreq.getSession().setAttribute(PROFILE_DATA, content)\r\n```', 'commenter': 'luocooong'}, {'comment': 'same as above', 'commenter': 'Leon-WTF'}]"
2236,exec/java-exec/pom.xml,"@@ -502,6 +502,11 @@
         </exclusion>
       </exclusions>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-yarn-api</artifactId>
+      <scope>test</scope>
+    </dependency>","[{'comment': 'Is this needed here? Drill-on-YARN uses the YARN API. We did try hard to avoid adding a YARN dependency to Drill itself, since Drill can run stand-alone, under YARN, under K8s, etc.', 'commenter': 'paul-rogers'}, {'comment': 'Unfortunately the mini dfs cluster tests failed without this dependency, which is why this a test dependency and not a compile/runtime one luckily...', 'commenter': 'laurentgo'}, {'comment': 'Thanks for the explanation. Not much we can do, I suppose.', 'commenter': 'paul-rogers'}]"
2236,drill-yarn/src/main/java/org/apache/drill/yarn/appMaster/http/WebServer.java,"@@ -306,18 +304,18 @@ private ConstraintSecurityHandler createSecurityHandler() {
   }
 
   /**
-   * It creates A {@link SessionHandler} which contains a {@link HashSessionManager}
+   * It creates A {@link SessionHandler}","[{'comment': 'Sorry to make you do this twice. The DoY web server was pretty much a copy/paste clone of the Drill exec version. The goal was to factor out a common set of code, but that never quite happened...', 'commenter': 'paul-rogers'}]"
2236,pom.xml,"@@ -2850,43 +2850,10 @@
           <!--Eclipse Jetty dependecies-->
           <dependency>
             <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-server</artifactId>
-            <version>${jetty.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-servlet</artifactId>
-            <version>${jetty.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-servlets</artifactId>
-            <version>${jetty.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-security</artifactId>
-            <version>${jetty.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-util</artifactId>
-            <version>${jetty.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-io</artifactId>
-            <version>${jetty.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-webapp</artifactId>
-            <version>${jetty.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.eclipse.jetty</groupId>
-            <artifactId>jetty-xml</artifactId>
+            <artifactId>jetty-bom</artifactId>
             <version>${jetty.version}</version>
+            <type>pom</type>
+            <scope>import</scope>","[{'comment': 'Nice simplification!', 'commenter': 'paul-rogers'}, {'comment': 'There are more simplifications like this for Jersey and Jackson', 'commenter': 'laurentgo'}]"
2238,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java,"@@ -745,4 +747,41 @@ public void testLimitMultipleRowGroupsBeyondRowCount() throws Exception {
     assertTrue(String.format(""Number of records in output is wrong: expected=%d, actual=%s"", 300, recordsInOutput), 300 == recordsInOutput);
   }
 
+  @Test
+  public void testTypeNull() throws Exception {
+    /* the `features` schema is:","[{'comment': 'Could you please write the comments to PR? Not in here. Add the `// DRILL-7934` at the header. thanks', 'commenter': 'luocooong'}]"
2238,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScanStatistics.java,"@@ -115,7 +118,7 @@ public void collect(Collection<T> metadataList) {
           previousCount.setValue(Statistic.NO_COLUMN_STATS);
         }
         ColumnMetadata columnMetadata = SchemaPathUtils.getColumnMetadata(schemaPath, metadata.getSchema());
-        TypeProtos.MajorType majorType = columnMetadata != null ? columnMetadata.majorType() : null;
+        TypeProtos.MajorType majorType = columnMetadata != null ? columnMetadata.majorType() : NULL;","[{'comment': 'Specifying the `NULL` type here may cause issues later when this type is used for obtaining values comparator... \r\nBut do we actually support partitioning list columns, i.e. if `majorType` is null, maybe we should set `partitionColumn` to false instead of calling the `checkForPartitionColumn()` method?', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi I've merge it to 1.18 and run in a testing environment, I didn't find any NullPointerException or other subsequent exceptions after merge. Drill can return data normally.\r\nIn my opinion, this class is mainly used to obtain the statistical information of each column in parquet. It may be necessary to keep the statistics of the columns. \r\nI will modify other questions about test cases according to your reviewed."", 'commenter': 'cdmikechen'}, {'comment': 'It is used to collect metadata for partition columns only. Even if some specific field is not determined as a partition, it still may have its metadata and be used in row group pruning later. But if you will set the type to NULL, it may cause issues for this case when it is used later for some specific types like `FIXED_LEN_BYTE_ARRAY` or `BINARY`. So it is better do not mark such field as partition at all and avoid possible issues with determining comparator (see `ParquetGroupScanStatistics.getTypeForColumn()` method usage).', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi \r\nI've add a if condition in `checkForPartitionColumn`. Do you mean like this?"", 'commenter': 'cdmikechen'}, {'comment': 'Not completely, in this case, no need to change this line, but add a simple check for `null` in `checkForPartitionColumn()` or before calling it (something like `majorType != null && checkForPartitionColumn...`).', 'commenter': 'vvysotskyi'}]"
2238,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java,"@@ -745,4 +747,41 @@ public void testLimitMultipleRowGroupsBeyondRowCount() throws Exception {
     assertTrue(String.format(""Number of records in output is wrong: expected=%d, actual=%s"", 300, recordsInOutput), 300 == recordsInOutput);
   }
 
+  @Test
+  public void testTypeNull() throws Exception {","[{'comment': ""Please note that this class has `@Ignore` annotation, so its tests wouldn't be running. Please add the test to another class."", 'commenter': 'vvysotskyi'}]"
2238,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java,"@@ -745,4 +747,41 @@ public void testLimitMultipleRowGroupsBeyondRowCount() throws Exception {
     assertTrue(String.format(""Number of records in output is wrong: expected=%d, actual=%s"", 300, recordsInOutput), 300 == recordsInOutput);
   }
 
+  @Test
+  public void testTypeNull() throws Exception {
+    /* the `features` schema is:
+    optional group features {
+      required int32 type (INTEGER(8,true));
+      optional int32 size;
+      optional group indices (LIST) {
+        repeated group list {
+          required int32 element;
+        }
+      }
+      optional group values (LIST) {
+        repeated group list {
+          required double element;
+        }
+      }
+    }
+    base on metastore/metastore-api/src/main/java/org/apache/drill/metastore/util/SchemaPathUtils.java
+    list schema is skipped, so that in ParquetGroupScanStatistics drill can not get ColumnMetadata by schemaPath
+    */
+    List<QueryDataBatch> results = testSqlWithResults(""SELECT * FROM cp.`parquet/test_type_null.parquet`"");","[{'comment': 'Please use `testBuilder()` for running and verifying query results.', 'commenter': 'vvysotskyi'}]"
2238,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScanStatistics.java,"@@ -115,7 +118,11 @@ public void collect(Collection<T> metadataList) {
           previousCount.setValue(Statistic.NO_COLUMN_STATS);
         }
         ColumnMetadata columnMetadata = SchemaPathUtils.getColumnMetadata(schemaPath, metadata.getSchema());
-        TypeProtos.MajorType majorType = columnMetadata != null ? columnMetadata.majorType() : null;
+        // DRILL-7934
+        // base on metastore/metastore-api/src/main/java/org/apache/drill/metastore/util/SchemaPathUtils.java#145","[{'comment': 'I recommend move the line 121-124 to description section of PR (On Github page).', 'commenter': 'luocooong'}]"
2238,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetScan.java,"@@ -51,4 +53,57 @@ public void testSuccessFile() throws Exception {
         .build()
         .run();
   }
+
+  @Test","[{'comment': 'I recommend that :\r\n```java\r\n// DRILL-7934: Fix NullPointerException error when reading parquet files\r\n@Test\r\n```', 'commenter': 'luocooong'}]"
2238,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetScan.java,"@@ -51,4 +53,57 @@ public void testSuccessFile() throws Exception {
         .build()
         .run();
   }
+
+  @Test
+  public void testTypeNull() throws Exception {
+    /* the `features` schema is:
+    optional group features {
+      required int32 type (INTEGER(8,true));
+      optional int32 size;
+      optional group indices (LIST) {
+        repeated group list {
+          required int32 element;
+        }
+      }
+      optional group values (LIST) {
+        repeated group list {
+          required double element;
+        }
+      }
+    }
+    */
+    String sql = ""SELECT * FROM cp.`parquet/test_type_null.parquet`"";
+    testBuilder()
+            .sqlQuery(sql)
+            .unOrdered()
+            .baselineColumns(""label"", ""features"")
+            .baselineValues(0.0d, new JsonStringHashMap<String, Object>() {{
+              put(""type"", 1);
+              put(""indices"", new JsonStringArrayList<Double>());
+              put(""values"", new JsonStringArrayList<Double>() {{
+                add(112.0d);
+                add(213.0d);
+                add(213.0d);
+              }});
+            }})
+            .baselineValues(0.0d, new JsonStringHashMap<String, Object>() {{
+              put(""type"", 1);
+              put(""indices"", new JsonStringArrayList<Double>());
+              put(""values"", new JsonStringArrayList<Double>() {{
+                add(213.0d);
+                add(123.0d);
+                add(123.0d);
+              }});
+            }})
+            .baselineValues(2.0d, new JsonStringHashMap<String, Object>() {{
+              put(""type"", 1);
+              put(""indices"", new JsonStringArrayList<Double>());
+              put(""values"", new JsonStringArrayList<Double>() {{
+                add(333.0d);
+                add(333.0d);
+                add(333.0d);
+              }});
+            }})","[{'comment': 'Please use `TestBuilder.listOf()` and `TestBuilder.mapOf()` methods, it will be mor ecompact:\r\n```suggestion\r\n        .baselineValues(0.0d,\r\n            mapOf(\r\n                ""type"", 1,\r\n                ""indices"", listOf(),\r\n                ""values"", listOf(112.0d, 213.0d, 213.0d)))\r\n        .baselineValues(0.0d,\r\n            mapOf(\r\n                ""type"", 1,\r\n                ""indices"", listOf(),\r\n                ""values"", listOf(213.0d, 123.0d, 123.0d)))\r\n        .baselineValues(2.0d,\r\n            mapOf(\r\n                ""type"", 1,\r\n                ""indices"", listOf(),\r\n                ""values"", listOf(333.0d, 333.0d, 333.0d)))\r\n```', 'commenter': 'vvysotskyi'}]"
2239,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaRecordReader.java,"@@ -17,131 +17,105 @@
  */
 package org.apache.drill.exec.store.kafka;
 
-import java.io.IOException;
-import java.util.Collection;
-import java.util.LinkedHashSet;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.TimeUnit;
-
+import org.apache.drill.common.exceptions.ChildErrorContext;
+import org.apache.drill.common.exceptions.CustomErrorContext;
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.ops.OperatorContext;
-import org.apache.drill.exec.physical.impl.OutputMutator;
-import org.apache.drill.exec.store.AbstractRecordReader;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.impl.scan.framework.SchemaNegotiator;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.server.options.OptionManager;
 import org.apache.drill.exec.store.kafka.decoders.MessageReader;
 import org.apache.drill.exec.store.kafka.decoders.MessageReaderFactory;
-import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import java.io.IOException;
 
-public class KafkaRecordReader extends AbstractRecordReader {
+public class KafkaRecordReader implements ManagedReader<SchemaNegotiator> {
   private static final Logger logger = LoggerFactory.getLogger(KafkaRecordReader.class);
 
-  private static final long DEFAULT_MESSAGES_PER_BATCH = 4000;
-
   private final ReadOptions readOptions;
   private final KafkaStoragePlugin plugin;
   private final KafkaPartitionScanSpec subScanSpec;
+  private final int maxRecords;
 
-  private VectorContainerWriter writer;
   private MessageReader messageReader;
-
   private long currentOffset;
   private MessageIterator msgItr;
-  private int currentMessageCount;
 
-  public KafkaRecordReader(KafkaPartitionScanSpec subScanSpec, List<SchemaPath> projectedColumns,
-      FragmentContext context, KafkaStoragePlugin plugin) {
-    setColumns(projectedColumns);
-    this.readOptions = new ReadOptions(context.getOptions());
+  public KafkaRecordReader(KafkaPartitionScanSpec subScanSpec, OptionManager options, KafkaStoragePlugin plugin, int maxRecords) {
+    this.readOptions = new ReadOptions(options);
     this.plugin = plugin;
     this.subScanSpec = subScanSpec;
+    this.maxRecords = maxRecords;
   }
 
   @Override
-  protected Collection<SchemaPath> transformColumns(Collection<SchemaPath> projectedColumns) {
-    Set<SchemaPath> transformed = new LinkedHashSet<>();
-    if (isStarQuery()) {
-      transformed.add(SchemaPath.STAR_COLUMN);
-    } else {
-      transformed.addAll(projectedColumns);
-    }
-    return transformed;
-  }
+  public boolean open(SchemaNegotiator negotiator) {
+    CustomErrorContext errorContext = new ChildErrorContext(negotiator.parentErrorContext()) {
+      @Override
+      public void addContext(UserException.Builder builder) {
+        super.addContext(builder);
+        builder.addContext(""topic_name"", subScanSpec.getTopicName());
+      }
+    };
+    negotiator.setErrorContext(errorContext);
 
-  @Override
-  public void setup(OperatorContext context, OutputMutator output) {
-    this.writer = new VectorContainerWriter(output, readOptions.isEnableUnionType());
     messageReader = MessageReaderFactory.getMessageReader(readOptions.getMessageReader());
-    messageReader.init(context.getManagedBuffer(), Lists.newArrayList(getColumns()), writer, readOptions);
+    messageReader.init(negotiator, readOptions, plugin);
     msgItr = new MessageIterator(messageReader.getConsumer(plugin), subScanSpec, readOptions.getPollTimeOut());
+
+    return true;
   }
 
   /**
    * KafkaConsumer.poll will fetch 500 messages per poll call. So hasNext will
    * take care of polling multiple times for this given batch next invocation
    */
   @Override
-  public int next() {
-    writer.allocate();
-    writer.reset();
-    Stopwatch watch = logger.isDebugEnabled() ? Stopwatch.createStarted() : null;
-    currentMessageCount = 0;
-
-    try {
-      while (currentOffset < subScanSpec.getEndOffset() && msgItr.hasNext()) {
-        ConsumerRecord<byte[], byte[]> consumerRecord = msgItr.next();
-        currentOffset = consumerRecord.offset();
-        writer.setPosition(currentMessageCount);
-        boolean status = messageReader.readMessage(consumerRecord);
-        // increment record count only if message was read successfully
-        if (status) {
-          if (++currentMessageCount >= DEFAULT_MESSAGES_PER_BATCH) {
-            break;
-          }
-        }
+  public boolean next() {
+    RowSetLoader rowWriter = messageReader.getResultSetLoader().writer();","[{'comment': '@vvysotskyi \r\nDoes this line belong in the `setup()` method?  In the Splunk plugin for example:\r\n\r\nhttps://github.com/apache/drill/blob/bc53c8e6c4a24b7cbce2112690a7d3e77e55fe41/contrib/storage-splunk/src/main/java/org/apache/drill/exec/store/splunk/SplunkBatchReader.java#L122-L129\r\n\r\nThen in each iteration of the `next()` method it calls `rowWriter.start()`.  Or does this work a little differently?\r\n', 'commenter': 'cgivre'}, {'comment': 'Yes, it works slightly differently, `rowWriter.start()` is called in `MessageReader.readMessage()` method after all checks below for batch size are passed.', 'commenter': 'vvysotskyi'}, {'comment': 'Got it.  Thanks!', 'commenter': 'cgivre'}]"
2239,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/decoders/JsonMessageReader.java,"@@ -55,61 +47,78 @@
 public class JsonMessageReader implements MessageReader {
 
   private static final Logger logger = LoggerFactory.getLogger(JsonMessageReader.class);
-  private JsonReader jsonReader;
-  private VectorContainerWriter writer;
-  private ObjectMapper objectMapper;
+
+  private final SingleElementIterator<InputStream> stream = new SingleElementIterator<>();
+
+  private KafkaJsonLoader kafkaJsonLoader;
+  private ResultSetLoader resultSetLoader;
+  private SchemaNegotiator negotiator;
+  private ReadOptions readOptions;
+  private Properties kafkaConsumerProps;
 
   @Override
-  public void init(DrillBuf buf, List<SchemaPath> columns, VectorContainerWriter writer, ReadOptions readOptions) {
-    // set skipOuterList to false as it doesn't applicable for JSON records and it's only applicable for JSON files.
-    this.jsonReader = new JsonReader.Builder(buf)
-      .schemaPathColumns(columns)
-      .allTextMode(readOptions.isAllTextMode())
-      .readNumbersAsDouble(readOptions.isReadNumbersAsDouble())
-      .enableNanInf(readOptions.isAllowNanInf())
-      .enableEscapeAnyChar(readOptions.isAllowEscapeAnyChar())
-      .build();
-    jsonReader.setIgnoreJSONParseErrors(readOptions.isSkipInvalidRecords());
-    this.writer = writer;
-    this.objectMapper = BaseJsonProcessor.getDefaultMapper()
-      .configure(JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS, readOptions.isAllowNanInf())
-      .configure(JsonParser.Feature.ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER, readOptions.isAllowEscapeAnyChar());
+  public void init(SchemaNegotiator negotiator, ReadOptions readOptions, KafkaStoragePlugin plugin) {
+    this.negotiator = negotiator;
+    this.resultSetLoader = negotiator.build();
+    this.readOptions = readOptions;
+    this.kafkaConsumerProps = plugin.getConfig().getKafkaConsumerProps();
   }
 
   @Override
-  public boolean readMessage(ConsumerRecord<?, ?> record) {
+  public void readMessage(ConsumerRecord<?, ?> record) {
     byte[] recordArray = (byte[]) record.value();
-    String data = new String(recordArray, Charsets.UTF_8);
     try {
-      JsonNode jsonNode = objectMapper.readTree(data);
-      if (jsonNode != null && jsonNode.isObject()) {
-        ObjectNode objectNode = (ObjectNode) jsonNode;
-        objectNode.put(KAFKA_TOPIC.getFieldName(), record.topic());
-        objectNode.put(KAFKA_PARTITION_ID.getFieldName(), record.partition());
-        objectNode.put(KAFKA_OFFSET.getFieldName(), record.offset());
-        objectNode.put(KAFKA_TIMESTAMP.getFieldName(), record.timestamp());
-        objectNode.put(KAFKA_MSG_KEY.getFieldName(), record.key() != null ? record.key().toString() : null);
-      } else {
-        throw new IOException(""Unsupported node type: "" + (jsonNode == null ? ""NO CONTENT"" : jsonNode.getNodeType()));
+      parseAndWrite(record, recordArray);
+    } catch (TokenIterator.RecoverableJsonException e) {
+      if (!readOptions.isSkipInvalidRecords()) {
+        throw e;","[{'comment': 'Do you think we should throw a `UserException` here with an explanation and `errorContext`?', 'commenter': 'cgivre'}, {'comment': 'Thanks, done', 'commenter': 'vvysotskyi'}]"
2239,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/parser/JsonStructureOptions.java,"@@ -47,6 +47,13 @@
    */
   public boolean skipMalformedRecords;
 
+  /**
+   * This property works only when {@link #skipMalformedRecords} enabled.
+   * If true, {@link TokenIterator.RecoverableJsonException} will be populated for the case of
+   * malformed empty document, so it will be possible to handle this exception by caller.
+   */
+  public boolean skipMalformedDocument;","[{'comment': 'Is there a system option for this?  Also is this documented anywhere?', 'commenter': 'cgivre'}, {'comment': 'There is no option for this. It is for internal usage only.', 'commenter': 'vvysotskyi'}]"
2239,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaRecordReader.java,"@@ -17,131 +17,105 @@
  */
 package org.apache.drill.exec.store.kafka;
 
-import java.io.IOException;
-import java.util.Collection;
-import java.util.LinkedHashSet;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.TimeUnit;
-
+import org.apache.drill.common.exceptions.ChildErrorContext;
+import org.apache.drill.common.exceptions.CustomErrorContext;
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.exec.ops.FragmentContext;
-import org.apache.drill.exec.ops.OperatorContext;
-import org.apache.drill.exec.physical.impl.OutputMutator;
-import org.apache.drill.exec.store.AbstractRecordReader;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.impl.scan.framework.SchemaNegotiator;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.server.options.OptionManager;
 import org.apache.drill.exec.store.kafka.decoders.MessageReader;
 import org.apache.drill.exec.store.kafka.decoders.MessageReaderFactory;
-import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import java.io.IOException;
 
-public class KafkaRecordReader extends AbstractRecordReader {
+public class KafkaRecordReader implements ManagedReader<SchemaNegotiator> {
   private static final Logger logger = LoggerFactory.getLogger(KafkaRecordReader.class);
 
-  private static final long DEFAULT_MESSAGES_PER_BATCH = 4000;
-
   private final ReadOptions readOptions;
   private final KafkaStoragePlugin plugin;
   private final KafkaPartitionScanSpec subScanSpec;
+  private final int maxRecords;
 
-  private VectorContainerWriter writer;
   private MessageReader messageReader;
-
   private long currentOffset;
   private MessageIterator msgItr;
-  private int currentMessageCount;
 
-  public KafkaRecordReader(KafkaPartitionScanSpec subScanSpec, List<SchemaPath> projectedColumns,
-      FragmentContext context, KafkaStoragePlugin plugin) {
-    setColumns(projectedColumns);
-    this.readOptions = new ReadOptions(context.getOptions());
+  public KafkaRecordReader(KafkaPartitionScanSpec subScanSpec, OptionManager options, KafkaStoragePlugin plugin, int maxRecords) {
+    this.readOptions = new ReadOptions(options);
     this.plugin = plugin;
     this.subScanSpec = subScanSpec;
+    this.maxRecords = maxRecords;
   }
 
   @Override
-  protected Collection<SchemaPath> transformColumns(Collection<SchemaPath> projectedColumns) {
-    Set<SchemaPath> transformed = new LinkedHashSet<>();
-    if (isStarQuery()) {
-      transformed.add(SchemaPath.STAR_COLUMN);
-    } else {
-      transformed.addAll(projectedColumns);
-    }
-    return transformed;
-  }
+  public boolean open(SchemaNegotiator negotiator) {
+    CustomErrorContext errorContext = new ChildErrorContext(negotiator.parentErrorContext()) {
+      @Override
+      public void addContext(UserException.Builder builder) {
+        super.addContext(builder);
+        builder.addContext(""topic_name"", subScanSpec.getTopicName());
+      }
+    };
+    negotiator.setErrorContext(errorContext);
 
-  @Override
-  public void setup(OperatorContext context, OutputMutator output) {
-    this.writer = new VectorContainerWriter(output, readOptions.isEnableUnionType());
     messageReader = MessageReaderFactory.getMessageReader(readOptions.getMessageReader());
-    messageReader.init(context.getManagedBuffer(), Lists.newArrayList(getColumns()), writer, readOptions);
+    messageReader.init(negotiator, readOptions, plugin);
     msgItr = new MessageIterator(messageReader.getConsumer(plugin), subScanSpec, readOptions.getPollTimeOut());
+
+    return true;
   }
 
   /**
    * KafkaConsumer.poll will fetch 500 messages per poll call. So hasNext will
    * take care of polling multiple times for this given batch next invocation
    */
   @Override
-  public int next() {
-    writer.allocate();
-    writer.reset();
-    Stopwatch watch = logger.isDebugEnabled() ? Stopwatch.createStarted() : null;
-    currentMessageCount = 0;
-
-    try {
-      while (currentOffset < subScanSpec.getEndOffset() && msgItr.hasNext()) {
-        ConsumerRecord<byte[], byte[]> consumerRecord = msgItr.next();
-        currentOffset = consumerRecord.offset();
-        writer.setPosition(currentMessageCount);
-        boolean status = messageReader.readMessage(consumerRecord);
-        // increment record count only if message was read successfully
-        if (status) {
-          if (++currentMessageCount >= DEFAULT_MESSAGES_PER_BATCH) {
-            break;
-          }
-        }
+  public boolean next() {
+    RowSetLoader rowWriter = messageReader.getResultSetLoader().writer();
+    while (!rowWriter.isFull()) {
+      if (!nextLine(rowWriter)) {
+        return false;
       }
+    }
+    return messageReader.endBatch();
+  }
 
-      if (currentMessageCount > 0) {
-        messageReader.ensureAtLeastOneField();
-      }
-      writer.setValueCount(currentMessageCount);
-      if (watch != null) {
-        logger.debug(""Took {} ms to process {} records."", watch.elapsed(TimeUnit.MILLISECONDS), currentMessageCount);
-      }
-      logger.debug(""Last offset consumed for {}:{} is {}"", subScanSpec.getTopicName(), subScanSpec.getPartitionId(),
-          currentOffset);
-      return currentMessageCount;
-    } catch (Exception e) {
-      String msg = ""Failure while reading messages from kafka. Record reader was at record: "" + (currentMessageCount + 1);
-      throw UserException.dataReadError(e)
-        .message(msg)
-        .addContext(e.getMessage())
-        .build(logger);
+  private boolean nextLine(RowSetLoader rowWriter) {
+    if (rowWriter.limitReached(maxRecords)) {
+      return false;
+    }
+
+    if (currentOffset >= subScanSpec.getEndOffset() || !msgItr.hasNext()) {
+      return false;
     }
+    ConsumerRecord<byte[], byte[]> consumerRecord = msgItr.next();
+    currentOffset = consumerRecord.offset();
+    messageReader.readMessage(consumerRecord);
+    return true;
   }
 
   @Override
-  public void close() throws IOException {
+  public void close() {
     logger.debug(""Last offset processed for {}:{} is - {}"", subScanSpec.getTopicName(), subScanSpec.getPartitionId(),
         currentOffset);
     logger.debug(""Total time to fetch messages from {}:{} is - {} milliseconds"", subScanSpec.getTopicName(),
         subScanSpec.getPartitionId(), msgItr.getTotalFetchTime());
     plugin.registerToClose(msgItr);
-    messageReader.close();
+    try {
+      messageReader.close();
+    } catch (IOException e) {
+      logger.warn(""Error closing Kafka message reader: {}"", e.getMessage(), e);
+    }
   }
 
   @Override
   public String toString() {
     return ""KafkaRecordReader[readOptions="" + readOptions","[{'comment': 'Should we update this to use the `PlanStringBuilder`?  Here and elsewhere?', 'commenter': 'cgivre'}, {'comment': 'Thanks, used it here.', 'commenter': 'vvysotskyi'}]"
2244,common/pom.xml,"@@ -37,11 +37,21 @@
       <artifactId>drill-protocol</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <dependency>
+      <groupId>org.junit.jupiter</groupId>
+      <artifactId>junit-jupiter-engine</artifactId>
+      <version>${junit.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>org.junit.vintage</groupId>
+      <artifactId>junit-vintage-engine</artifactId>
+      <version>${junit.version}</version>
+    </dependency>
     <dependency>
       <!-- add as provided scope so that we can compile TestTools.  Should only be ever used in a test scenario where someone else is bringing JUnit in. -->
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
-      <version>${junit.version}</version>
+      <scope>provided</scope>","[{'comment': 'Please remove junit / jupiter dependencies from here and move `RepeatTestRule` to the tests folder.', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi RepeatTestRule is currently at common/src/main/java/org/apache/drill/common/util/RepeatTestRule.java\r\nBut it is used by:\r\n1) exec/java-exec/src/test/java/org/apache/drill/exec/server/TestDrillbitResilience.java\r\n2) common/src/test/java/org/apache/drill/test/TestTools.java\r\n\r\nSo, to do what you suggest I have to copy it in two modules - java-exec and common.\r\nOr to move it in common/src/test/... and then in java-exec/pom.xml to add a dependency to common-tests.jar ', 'commenter': 'martin-g'}, {'comment': '`exec` module is the parent for `java-exec` and it contains the dependency to `drill-common` with `tests` classifier, so it should be enaugh just to move `common/src/main/java/org/apache/drill/common/util/RepeatTestRule.java` class to `common/test/main/java/org/apache/drill/common/util/RepeatTestRule.java`.', 'commenter': 'vvysotskyi'}, {'comment': 'Done!', 'commenter': 'martin-g'}]"
2244,common/pom.xml,"@@ -37,11 +37,21 @@
       <artifactId>drill-protocol</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <dependency>","[{'comment': ""I'm not sure why the engine has to be included as a dependency vs adding `org.junit.jupiter:junit-jupiter-api` which contains the JUnit API parts (like the annotations/assertions). \r\n\r\nIt seems the surefire should be able to pick up the engine based on the actual JUnit APIs being used in the codebase (but I guess the surefire JUnit5 page at https://maven.apache.org/surefire/maven-surefire-plugin/examples/junit-platform.html can be quite confusing).\r\n\r\nOn another project I'm part of, we actually had some issues between surefire plugin and junit engines being added as depedencies were the versions were not in sync, and it seemed the safest way was to let the surefire plugin decides which junit5 engine version to use for running the tests"", 'commenter': 'laurentgo'}]"
2244,common/pom.xml,"@@ -37,11 +37,21 @@
       <artifactId>drill-protocol</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <dependency>
+      <groupId>org.junit.jupiter</groupId>
+      <artifactId>junit-jupiter-engine</artifactId>
+      <version>${junit.version}</version>","[{'comment': ""Shouldn't we use dependencyManagement for this? (It seems the project uses it sometimes but not always)?\r\nLooks like JUnit project publishes a BOM artifact (org.junit:junit-bom) to make sure that all JUnit dependencies are on the same version.\r\n\r\nAlso, shouldn't this dependencies being provided as test dependencies (since no main code seems to reference junit5 api). In which case, I guess they could even be dropped since they are declared with the right scope in the top level pom.xml this module is a child of?"", 'commenter': 'laurentgo'}, {'comment': ""@laurentgo Very good suggestions!\r\nI've pushed a new commit that uses the junit-bom in dependencyManagement, and uses junit-jupiter instead of junit-jupiter-engine. This way junit-jupiter-api, junit-jupiter-engine and junit-jupiter-params are included as transitive dependencies and one could write use the full power of JUnit 5.x when writing tests."", 'commenter': 'martin-g'}]"
2263,_docs/query-data/query-a-file-system/040-querying-directories.md,"@@ -152,3 +152,22 @@ Starting in Drill 1.16, Drill uses a Value operator instead of a Scan operator t
  
 You can use [query directory functions]({{site.baseurl}}/docs/query-directory-functions/) to restrict a query to one of a number of subdirectories and to prevent Drill from scanning all data in directories.
 
+##Querying Multiple Directories in One Query by Wildcard Matching","[{'comment': 'Put a whitespace between the `##` and `Querying`.', 'commenter': 'luocooong'}, {'comment': 'Hi cong, I did the change as you required. Please take a look', 'commenter': 'kingswanwho'}]"
2266,zh/index.html,"@@ -79,39 +78,39 @@ <h2 id=""sub-headline"">Schema-free SQL Query Engine <br class=""mobile-break"" />fo
 <div class=""mw introWrapper"">
   <table class=""intro"" cellpadding=""0"" cellspacing=""0"" align=""center"">
     <tbody>
-      <tr>
-        <td class=""ag"">
-          <h1>敏捷</h1>
-          <p>Get faster insights without the overhead (data loading, schema creation and maintenance, transformations, etc.)</p>
-        </td>
-        <td class=""fl"">
-          <h1>Flexibility</h1>
-          <p>Analyze the multi-structured and nested data in non-relational datastores directly without transforming or restricting the data</p>
-        </td>
-        <td class=""fam"">
-          <h1>Familiarity</h1>
-          <p>Leverage your existing SQL skillsets and BI tools including Tableau, Qlikview, MicroStrategy, Spotfire, Excel and more</p>
-        </td>
-      </tr>
+    <tr>
+      <td class=""ag"">
+        <h1>敏捷性</h1>
+        <p>快速的挖掘数据而省去重复的操作(如加载数据、表单的创建与维护、数据类型转换等等)</p>","[{'comment': 'Recommended to replace that :\n获得更快的洞察力又省去繁琐的前置处理（Schema创建和维护、数据加载、转换等ETL操作）', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -79,39 +78,39 @@ <h2 id=""sub-headline"">Schema-free SQL Query Engine <br class=""mobile-break"" />fo
 <div class=""mw introWrapper"">
   <table class=""intro"" cellpadding=""0"" cellspacing=""0"" align=""center"">
     <tbody>
-      <tr>
-        <td class=""ag"">
-          <h1>敏捷</h1>
-          <p>Get faster insights without the overhead (data loading, schema creation and maintenance, transformations, etc.)</p>
-        </td>
-        <td class=""fl"">
-          <h1>Flexibility</h1>
-          <p>Analyze the multi-structured and nested data in non-relational datastores directly without transforming or restricting the data</p>
-        </td>
-        <td class=""fam"">
-          <h1>Familiarity</h1>
-          <p>Leverage your existing SQL skillsets and BI tools including Tableau, Qlikview, MicroStrategy, Spotfire, Excel and more</p>
-        </td>
-      </tr>
+    <tr>
+      <td class=""ag"">
+        <h1>敏捷性</h1>
+        <p>快速的挖掘数据而省去重复的操作(如加载数据、表单的创建与维护、数据类型转换等等)</p>
+      </td>
+      <td class=""fl"">
+        <h1>灵活性</h1>
+        <p>不必进行类型转换或限制数据类型便可通过非关系型数据存储来分析多结构数据及嵌套数据</p>","[{'comment': 'Recommended to replace that :\n直接分析NoSQL中的复杂结构和嵌套数据（无需转换和要求数据格式）', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -79,39 +78,39 @@ <h2 id=""sub-headline"">Schema-free SQL Query Engine <br class=""mobile-break"" />fo
 <div class=""mw introWrapper"">
   <table class=""intro"" cellpadding=""0"" cellspacing=""0"" align=""center"">
     <tbody>
-      <tr>
-        <td class=""ag"">
-          <h1>敏捷</h1>
-          <p>Get faster insights without the overhead (data loading, schema creation and maintenance, transformations, etc.)</p>
-        </td>
-        <td class=""fl"">
-          <h1>Flexibility</h1>
-          <p>Analyze the multi-structured and nested data in non-relational datastores directly without transforming or restricting the data</p>
-        </td>
-        <td class=""fam"">
-          <h1>Familiarity</h1>
-          <p>Leverage your existing SQL skillsets and BI tools including Tableau, Qlikview, MicroStrategy, Spotfire, Excel and more</p>
-        </td>
-      </tr>
+    <tr>
+      <td class=""ag"">
+        <h1>敏捷性</h1>
+        <p>快速的挖掘数据而省去重复的操作(如加载数据、表单的创建与维护、数据类型转换等等)</p>
+      </td>
+      <td class=""fl"">
+        <h1>灵活性</h1>
+        <p>不必进行类型转换或限制数据类型便可通过非关系型数据存储来分析多结构数据及嵌套数据</p>
+      </td>
+      <td class=""fam"">
+        <h1>易上手</h1>","[{'comment': 'Recommended to replace that :\n易用性', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -79,39 +78,39 @@ <h2 id=""sub-headline"">Schema-free SQL Query Engine <br class=""mobile-break"" />fo
 <div class=""mw introWrapper"">
   <table class=""intro"" cellpadding=""0"" cellspacing=""0"" align=""center"">
     <tbody>
-      <tr>
-        <td class=""ag"">
-          <h1>敏捷</h1>
-          <p>Get faster insights without the overhead (data loading, schema creation and maintenance, transformations, etc.)</p>
-        </td>
-        <td class=""fl"">
-          <h1>Flexibility</h1>
-          <p>Analyze the multi-structured and nested data in non-relational datastores directly without transforming or restricting the data</p>
-        </td>
-        <td class=""fam"">
-          <h1>Familiarity</h1>
-          <p>Leverage your existing SQL skillsets and BI tools including Tableau, Qlikview, MicroStrategy, Spotfire, Excel and more</p>
-        </td>
-      </tr>
+    <tr>
+      <td class=""ag"">
+        <h1>敏捷性</h1>
+        <p>快速的挖掘数据而省去重复的操作(如加载数据、表单的创建与维护、数据类型转换等等)</p>
+      </td>
+      <td class=""fl"">
+        <h1>灵活性</h1>
+        <p>不必进行类型转换或限制数据类型便可通过非关系型数据存储来分析多结构数据及嵌套数据</p>
+      </td>
+      <td class=""fam"">
+        <h1>易上手</h1>
+        <p>充分利用你已具备的SQL技术栈和BI工具，包括Tableau, Qlikview, MicroStrategy, Spotfire, Excel等等。</a></p>
+      </td>
+    </tr>
     </tbody>
   </table>
 </div>
 
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-any.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Query any non-relational datastore (well, almost...)</h1>
-    <p>Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.</p>
-    <p>Drill's datastore-aware optimizer automatically restructures a query plan to leverage the datastore's internal processing capabilities. In addition, Drill supports data locality, so it's a good idea to co-locate Drill and the datastore on the same nodes.</p>
+    <h1>无所不能：几乎可以对任何非关系型数据库进行查询</h1>","[{'comment': 'Recommended to replace that :\n无处不在：几乎可以查询任何类型的NoSQL', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -79,39 +78,39 @@ <h2 id=""sub-headline"">Schema-free SQL Query Engine <br class=""mobile-break"" />fo
 <div class=""mw introWrapper"">
   <table class=""intro"" cellpadding=""0"" cellspacing=""0"" align=""center"">
     <tbody>
-      <tr>
-        <td class=""ag"">
-          <h1>敏捷</h1>
-          <p>Get faster insights without the overhead (data loading, schema creation and maintenance, transformations, etc.)</p>
-        </td>
-        <td class=""fl"">
-          <h1>Flexibility</h1>
-          <p>Analyze the multi-structured and nested data in non-relational datastores directly without transforming or restricting the data</p>
-        </td>
-        <td class=""fam"">
-          <h1>Familiarity</h1>
-          <p>Leverage your existing SQL skillsets and BI tools including Tableau, Qlikview, MicroStrategy, Spotfire, Excel and more</p>
-        </td>
-      </tr>
+    <tr>
+      <td class=""ag"">
+        <h1>敏捷性</h1>
+        <p>快速的挖掘数据而省去重复的操作(如加载数据、表单的创建与维护、数据类型转换等等)</p>
+      </td>
+      <td class=""fl"">
+        <h1>灵活性</h1>
+        <p>不必进行类型转换或限制数据类型便可通过非关系型数据存储来分析多结构数据及嵌套数据</p>
+      </td>
+      <td class=""fam"">
+        <h1>易上手</h1>
+        <p>充分利用你已具备的SQL技术栈和BI工具，包括Tableau, Qlikview, MicroStrategy, Spotfire, Excel等等。</a></p>
+      </td>
+    </tr>
     </tbody>
   </table>
 </div>
 
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-any.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Query any non-relational datastore (well, almost...)</h1>
-    <p>Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.</p>
-    <p>Drill's datastore-aware optimizer automatically restructures a query plan to leverage the datastore's internal processing capabilities. In addition, Drill supports data locality, so it's a good idea to co-locate Drill and the datastore on the same nodes.</p>
+    <h1>无所不能：几乎可以对任何非关系型数据库进行查询</h1>
+    <p>Drill支持多种类型的NoSQL数据库和文件系统, 包括 HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS和本地文件。单个查询可以连接来自不同数据存储的数据。 比如, 你可以连接来自MongoDB的用户档案和来自Hadoop的事件日志。</p>","[{'comment': 'Recommended to replace that :\n... 包含 Hbase、MongoDB、ElasticSearch、Cassandra、Druid、Kudu、Kafka、OpenTSDB、HDFS、Amazon S3、Azure Blob Storage、Google Cloud Storage、Swift、NAS和本地文件。可以在单次查询中组合多个数据源（联邦查询）。（后续文字删除）', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -79,39 +78,39 @@ <h2 id=""sub-headline"">Schema-free SQL Query Engine <br class=""mobile-break"" />fo
 <div class=""mw introWrapper"">
   <table class=""intro"" cellpadding=""0"" cellspacing=""0"" align=""center"">
     <tbody>
-      <tr>
-        <td class=""ag"">
-          <h1>敏捷</h1>
-          <p>Get faster insights without the overhead (data loading, schema creation and maintenance, transformations, etc.)</p>
-        </td>
-        <td class=""fl"">
-          <h1>Flexibility</h1>
-          <p>Analyze the multi-structured and nested data in non-relational datastores directly without transforming or restricting the data</p>
-        </td>
-        <td class=""fam"">
-          <h1>Familiarity</h1>
-          <p>Leverage your existing SQL skillsets and BI tools including Tableau, Qlikview, MicroStrategy, Spotfire, Excel and more</p>
-        </td>
-      </tr>
+    <tr>
+      <td class=""ag"">
+        <h1>敏捷性</h1>
+        <p>快速的挖掘数据而省去重复的操作(如加载数据、表单的创建与维护、数据类型转换等等)</p>
+      </td>
+      <td class=""fl"">
+        <h1>灵活性</h1>
+        <p>不必进行类型转换或限制数据类型便可通过非关系型数据存储来分析多结构数据及嵌套数据</p>
+      </td>
+      <td class=""fam"">
+        <h1>易上手</h1>
+        <p>充分利用你已具备的SQL技术栈和BI工具，包括Tableau, Qlikview, MicroStrategy, Spotfire, Excel等等。</a></p>
+      </td>
+    </tr>
     </tbody>
   </table>
 </div>
 
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-any.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Query any non-relational datastore (well, almost...)</h1>
-    <p>Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.</p>
-    <p>Drill's datastore-aware optimizer automatically restructures a query plan to leverage the datastore's internal processing capabilities. In addition, Drill supports data locality, so it's a good idea to co-locate Drill and the datastore on the same nodes.</p>
+    <h1>无所不能：几乎可以对任何非关系型数据库进行查询</h1>
+    <p>Drill支持多种类型的NoSQL数据库和文件系统, 包括 HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS和本地文件。单个查询可以连接来自不同数据存储的数据。 比如, 你可以连接来自MongoDB的用户档案和来自Hadoop的事件日志。</p>
+    <p>Drill的""数据存储感知""优化控制器利用数据存储内部的处理能力自动重构查询方案。不仅如此, Drill支持数据本地化, 所以将Drill和数据存储配置在同一节点可以充分优化性能。</p>","[{'comment': 'Recommended to replace that :\nDrill的存储感知优化器会利用数据存储的内部处理能力来重构查询计划。Drill还支持数据本地性（Data Locality），所以将Drill和数据节点部署在一起可以充分优化性能。', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -79,39 +78,39 @@ <h2 id=""sub-headline"">Schema-free SQL Query Engine <br class=""mobile-break"" />fo
 <div class=""mw introWrapper"">
   <table class=""intro"" cellpadding=""0"" cellspacing=""0"" align=""center"">
     <tbody>
-      <tr>
-        <td class=""ag"">
-          <h1>敏捷</h1>
-          <p>Get faster insights without the overhead (data loading, schema creation and maintenance, transformations, etc.)</p>
-        </td>
-        <td class=""fl"">
-          <h1>Flexibility</h1>
-          <p>Analyze the multi-structured and nested data in non-relational datastores directly without transforming or restricting the data</p>
-        </td>
-        <td class=""fam"">
-          <h1>Familiarity</h1>
-          <p>Leverage your existing SQL skillsets and BI tools including Tableau, Qlikview, MicroStrategy, Spotfire, Excel and more</p>
-        </td>
-      </tr>
+    <tr>
+      <td class=""ag"">
+        <h1>敏捷性</h1>
+        <p>快速的挖掘数据而省去重复的操作(如加载数据、表单的创建与维护、数据类型转换等等)</p>
+      </td>
+      <td class=""fl"">
+        <h1>灵活性</h1>
+        <p>不必进行类型转换或限制数据类型便可通过非关系型数据存储来分析多结构数据及嵌套数据</p>
+      </td>
+      <td class=""fam"">
+        <h1>易上手</h1>
+        <p>充分利用你已具备的SQL技术栈和BI工具，包括Tableau, Qlikview, MicroStrategy, Spotfire, Excel等等。</a></p>
+      </td>
+    </tr>
     </tbody>
   </table>
 </div>
 
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-any.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Query any non-relational datastore (well, almost...)</h1>
-    <p>Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.</p>
-    <p>Drill's datastore-aware optimizer automatically restructures a query plan to leverage the datastore's internal processing capabilities. In addition, Drill supports data locality, so it's a good idea to co-locate Drill and the datastore on the same nodes.</p>
+    <h1>无所不能：几乎可以对任何非关系型数据库进行查询</h1>
+    <p>Drill支持多种类型的NoSQL数据库和文件系统, 包括 HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS和本地文件。单个查询可以连接来自不同数据存储的数据。 比如, 你可以连接来自MongoDB的用户档案和来自Hadoop的事件日志。</p>
+    <p>Drill的""数据存储感知""优化控制器利用数据存储内部的处理能力自动重构查询方案。不仅如此, Drill支持数据本地化, 所以将Drill和数据存储配置在同一节点可以充分优化性能。</p>
   </div>
   <div class=""small""><img src=""{{ site.baseurl }}/images/home-any.png"" style=""width:300px"" /></div>
 </div>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>Kiss the overhead goodbye and enjoy data agility</h1>
-    <p>Traditional query engines demand significant IT intervention before data can be queried. Drill gets rid of all that overhead so that users can just query the raw data in-situ. There's no need to load the data, create and maintain schemas, or transform the data before it can be processed. Instead, simply include the path to a Hadoop directory, MongoDB collection or S3 bucket in the SQL query.</p>
-    <p>Drill leverages advanced query compilation and re-compilation techniques to maximize performance without requiring up-front schema knowledge.</p>
+    <h1>告别繁冗：充分享受敏捷数据处理</h1>","[{'comment': 'Recommended to replace that :\n告别繁冗：充分享受数据应用的敏捷性', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -79,39 +78,39 @@ <h2 id=""sub-headline"">Schema-free SQL Query Engine <br class=""mobile-break"" />fo
 <div class=""mw introWrapper"">
   <table class=""intro"" cellpadding=""0"" cellspacing=""0"" align=""center"">
     <tbody>
-      <tr>
-        <td class=""ag"">
-          <h1>敏捷</h1>
-          <p>Get faster insights without the overhead (data loading, schema creation and maintenance, transformations, etc.)</p>
-        </td>
-        <td class=""fl"">
-          <h1>Flexibility</h1>
-          <p>Analyze the multi-structured and nested data in non-relational datastores directly without transforming or restricting the data</p>
-        </td>
-        <td class=""fam"">
-          <h1>Familiarity</h1>
-          <p>Leverage your existing SQL skillsets and BI tools including Tableau, Qlikview, MicroStrategy, Spotfire, Excel and more</p>
-        </td>
-      </tr>
+    <tr>
+      <td class=""ag"">
+        <h1>敏捷性</h1>
+        <p>快速的挖掘数据而省去重复的操作(如加载数据、表单的创建与维护、数据类型转换等等)</p>
+      </td>
+      <td class=""fl"">
+        <h1>灵活性</h1>
+        <p>不必进行类型转换或限制数据类型便可通过非关系型数据存储来分析多结构数据及嵌套数据</p>
+      </td>
+      <td class=""fam"">
+        <h1>易上手</h1>
+        <p>充分利用你已具备的SQL技术栈和BI工具，包括Tableau, Qlikview, MicroStrategy, Spotfire, Excel等等。</a></p>
+      </td>
+    </tr>
     </tbody>
   </table>
 </div>
 
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-any.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Query any non-relational datastore (well, almost...)</h1>
-    <p>Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.</p>
-    <p>Drill's datastore-aware optimizer automatically restructures a query plan to leverage the datastore's internal processing capabilities. In addition, Drill supports data locality, so it's a good idea to co-locate Drill and the datastore on the same nodes.</p>
+    <h1>无所不能：几乎可以对任何非关系型数据库进行查询</h1>
+    <p>Drill支持多种类型的NoSQL数据库和文件系统, 包括 HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS和本地文件。单个查询可以连接来自不同数据存储的数据。 比如, 你可以连接来自MongoDB的用户档案和来自Hadoop的事件日志。</p>
+    <p>Drill的""数据存储感知""优化控制器利用数据存储内部的处理能力自动重构查询方案。不仅如此, Drill支持数据本地化, 所以将Drill和数据存储配置在同一节点可以充分优化性能。</p>
   </div>
   <div class=""small""><img src=""{{ site.baseurl }}/images/home-any.png"" style=""width:300px"" /></div>
 </div>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>Kiss the overhead goodbye and enjoy data agility</h1>
-    <p>Traditional query engines demand significant IT intervention before data can be queried. Drill gets rid of all that overhead so that users can just query the raw data in-situ. There's no need to load the data, create and maintain schemas, or transform the data before it can be processed. Instead, simply include the path to a Hadoop directory, MongoDB collection or S3 bucket in the SQL query.</p>
-    <p>Drill leverages advanced query compilation and re-compilation techniques to maximize performance without requiring up-front schema knowledge.</p>
+    <h1>告别繁冗：充分享受敏捷数据处理</h1>
+    <p>传统的检索引擎需要大量的IT交互复杂及处理，才可以实现查询。Drill帮你解决了所有这些冗余的操作，使用户可以对原始数据进行一站式查询。不必再进行数据加载，创建及维护表格，以及数据类型转换。仅需在SQL查询语句里指定好Hadoop，MongoDB, 或者S3的文件路径。剩下的工作全部交给Drill即可。</p>","[{'comment': 'Recommended to replace that :\n传统的查询引擎需要大量的IT交互才允许查询数据。Drill直接省去了这些冗余，可以快速原地查询这些原始数据。没有Schema创建和维护，也没有数据加载、转换和ETL操作。只需要在查询语句中指定数据的位置，如 Hadoop、S3或MongoDB。', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -127,18 +126,18 @@ <h1>Kiss the overhead goodbye and enjoy data agility</h1>
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Treat your data like a table even when it's not</h1>
-    <p>Drill features a JSON data model that enables queries on complex/nested data as well as rapidly evolving structures commonly seen in modern applications and non-relational datastores. Drill also provides intuitive extensions to SQL so that you can easily query complex data.
-    <p>Drill is the only columnar query engine that supports complex data. It features an in-memory shredded columnar representation for complex data which allows Drill to achieve columnar speed with the flexibility of an internal JSON document model.</p>
+    <h1>化繁为简：非表格类数据也可以像表格数据一样处理</h1>","[{'comment': 'Recommended to replace that :\n化繁为简：将任何数据当表格形式一样使用', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -127,18 +126,18 @@ <h1>Kiss the overhead goodbye and enjoy data agility</h1>
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Treat your data like a table even when it's not</h1>
-    <p>Drill features a JSON data model that enables queries on complex/nested data as well as rapidly evolving structures commonly seen in modern applications and non-relational datastores. Drill also provides intuitive extensions to SQL so that you can easily query complex data.
-    <p>Drill is the only columnar query engine that supports complex data. It features an in-memory shredded columnar representation for complex data which allows Drill to achieve columnar speed with the flexibility of an internal JSON document model.</p>
+    <h1>化繁为简：非表格类数据也可以像表格数据一样处理</h1>
+    <p>Drill可以抽象出JSON数据的模型，使你可以对如今所有流行以及快速衍变的非关系型数据存储的复杂嵌套数据格式进行查询。同时，Drill提供了对SQL的直观拓宽，使你可以对复杂的数据直接进行SQL查询。","[{'comment': 'Recommended to replace that :\nDrill设计了专有的JSON数据模型，能够支持复杂/嵌套数据的查询，对现代应用程序以及NoSQL中进化的数据结构进行分析。不仅于此，还提供了SQL的扩展性，轻松查询更复杂的数据结构。', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -127,18 +126,18 @@ <h1>Kiss the overhead goodbye and enjoy data agility</h1>
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Treat your data like a table even when it's not</h1>
-    <p>Drill features a JSON data model that enables queries on complex/nested data as well as rapidly evolving structures commonly seen in modern applications and non-relational datastores. Drill also provides intuitive extensions to SQL so that you can easily query complex data.
-    <p>Drill is the only columnar query engine that supports complex data. It features an in-memory shredded columnar representation for complex data which allows Drill to achieve columnar speed with the flexibility of an internal JSON document model.</p>
+    <h1>化繁为简：非表格类数据也可以像表格数据一样处理</h1>
+    <p>Drill可以抽象出JSON数据的模型，使你可以对如今所有流行以及快速衍变的非关系型数据存储的复杂嵌套数据格式进行查询。同时，Drill提供了对SQL的直观拓宽，使你可以对复杂的数据直接进行SQL查询。
+    <p>Drill是列式查询引擎中唯一支持复杂数据检索的. Drill通过将复杂数据转换为存内的碎片列数据格式，使复杂的内部JSON文档模型也可以实现列数据的检索速度。</p>","[{'comment': 'Recommended to replace that :\nDrill是一款支持复杂数据的列式查询引擎。而且支持在内存中用列式表达复杂数据，所以查询JSON数据模型的速度可以媲美列式格式。', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -127,18 +126,18 @@ <h1>Kiss the overhead goodbye and enjoy data agility</h1>
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Treat your data like a table even when it's not</h1>
-    <p>Drill features a JSON data model that enables queries on complex/nested data as well as rapidly evolving structures commonly seen in modern applications and non-relational datastores. Drill also provides intuitive extensions to SQL so that you can easily query complex data.
-    <p>Drill is the only columnar query engine that supports complex data. It features an in-memory shredded columnar representation for complex data which allows Drill to achieve columnar speed with the flexibility of an internal JSON document model.</p>
+    <h1>化繁为简：非表格类数据也可以像表格数据一样处理</h1>
+    <p>Drill可以抽象出JSON数据的模型，使你可以对如今所有流行以及快速衍变的非关系型数据存储的复杂嵌套数据格式进行查询。同时，Drill提供了对SQL的直观拓宽，使你可以对复杂的数据直接进行SQL查询。
+    <p>Drill是列式查询引擎中唯一支持复杂数据检索的. Drill通过将复杂数据转换为存内的碎片列数据格式，使复杂的内部JSON文档模型也可以实现列数据的检索速度。</p>
   </div>
   <div class=""small""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
 </div>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>Keep using the BI tools you love</h1>
-    <p>Drill supports standard SQL. Business users, analysts and data scientists can use standard BI/analytics tools such as Tableau, Qlik, MicroStrategy, Spotfire, SAS and Excel to interact with non-relational datastores by leveraging Drill's JDBC and ODBC drivers. Developers can leverage Drill's simple REST API in their custom applications to create beautiful visualizations.</p>
-    <p>Drill's virtual datasets allow even the most complex, non-relational data to be mapped into BI-friendly structures which users can explore and visualize using their tool of choice.</p>
+    <h1>简单易学：持续兼容你熟悉的BI工具</h1>","[{'comment': 'Recommended to replace that :\n简单易用：继续使用你喜欢的BI工具', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -127,18 +126,18 @@ <h1>Kiss the overhead goodbye and enjoy data agility</h1>
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Treat your data like a table even when it's not</h1>
-    <p>Drill features a JSON data model that enables queries on complex/nested data as well as rapidly evolving structures commonly seen in modern applications and non-relational datastores. Drill also provides intuitive extensions to SQL so that you can easily query complex data.
-    <p>Drill is the only columnar query engine that supports complex data. It features an in-memory shredded columnar representation for complex data which allows Drill to achieve columnar speed with the flexibility of an internal JSON document model.</p>
+    <h1>化繁为简：非表格类数据也可以像表格数据一样处理</h1>
+    <p>Drill可以抽象出JSON数据的模型，使你可以对如今所有流行以及快速衍变的非关系型数据存储的复杂嵌套数据格式进行查询。同时，Drill提供了对SQL的直观拓宽，使你可以对复杂的数据直接进行SQL查询。
+    <p>Drill是列式查询引擎中唯一支持复杂数据检索的. Drill通过将复杂数据转换为存内的碎片列数据格式，使复杂的内部JSON文档模型也可以实现列数据的检索速度。</p>
   </div>
   <div class=""small""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
 </div>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>Keep using the BI tools you love</h1>
-    <p>Drill supports standard SQL. Business users, analysts and data scientists can use standard BI/analytics tools such as Tableau, Qlik, MicroStrategy, Spotfire, SAS and Excel to interact with non-relational datastores by leveraging Drill's JDBC and ODBC drivers. Developers can leverage Drill's simple REST API in their custom applications to create beautiful visualizations.</p>
-    <p>Drill's virtual datasets allow even the most complex, non-relational data to be mapped into BI-friendly structures which users can explore and visualize using their tool of choice.</p>
+    <h1>简单易学：持续兼容你熟悉的BI工具</h1>
+    <p>Drill支持标准的SQL。商务用户，分析师，数据科学家可以通过Drill的JDBC或者ODBC接口继续使用标准的BI以及分析工具，例如:Tableau, Qlik, MicroStrategy, Spotfire, SAS and Excel，来继续和非关系型数据存储交互。开发者可以在他们自定义的应用中利用Drill简单的REST API将数据可视化。</p>","[{'comment': 'Recommended to replace that :\nDrill支持标准的 ANSI SQL...\n...JDBC或者ODBC驱动在标准的BI和分析工具上运行。\n...和 Excel，开发者也可以在他们的应用中使用RESTful API（支持流式响应）来定制数据可视化。', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -127,18 +126,18 @@ <h1>Kiss the overhead goodbye and enjoy data agility</h1>
 <div class=""home-row"">
   <div class=""big""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
   <div class=""description"">
-    <h1>Treat your data like a table even when it's not</h1>
-    <p>Drill features a JSON data model that enables queries on complex/nested data as well as rapidly evolving structures commonly seen in modern applications and non-relational datastores. Drill also provides intuitive extensions to SQL so that you can easily query complex data.
-    <p>Drill is the only columnar query engine that supports complex data. It features an in-memory shredded columnar representation for complex data which allows Drill to achieve columnar speed with the flexibility of an internal JSON document model.</p>
+    <h1>化繁为简：非表格类数据也可以像表格数据一样处理</h1>
+    <p>Drill可以抽象出JSON数据的模型，使你可以对如今所有流行以及快速衍变的非关系型数据存储的复杂嵌套数据格式进行查询。同时，Drill提供了对SQL的直观拓宽，使你可以对复杂的数据直接进行SQL查询。
+    <p>Drill是列式查询引擎中唯一支持复杂数据检索的. Drill通过将复杂数据转换为存内的碎片列数据格式，使复杂的内部JSON文档模型也可以实现列数据的检索速度。</p>
   </div>
   <div class=""small""><img src=""{{ site.baseurl }}/images/home-json.png"" style=""width:300px"" /></div>
 </div>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>Keep using the BI tools you love</h1>
-    <p>Drill supports standard SQL. Business users, analysts and data scientists can use standard BI/analytics tools such as Tableau, Qlik, MicroStrategy, Spotfire, SAS and Excel to interact with non-relational datastores by leveraging Drill's JDBC and ODBC drivers. Developers can leverage Drill's simple REST API in their custom applications to create beautiful visualizations.</p>
-    <p>Drill's virtual datasets allow even the most complex, non-relational data to be mapped into BI-friendly structures which users can explore and visualize using their tool of choice.</p>
+    <h1>简单易学：持续兼容你熟悉的BI工具</h1>
+    <p>Drill支持标准的SQL。商务用户，分析师，数据科学家可以通过Drill的JDBC或者ODBC接口继续使用标准的BI以及分析工具，例如:Tableau, Qlik, MicroStrategy, Spotfire, SAS and Excel，来继续和非关系型数据存储交互。开发者可以在他们自定义的应用中利用Drill简单的REST API将数据可视化。</p>
+    <p>Drill的虚拟数据集可以将最复杂的非关系型数据对应到兼容BI的结构，帮助用户挖掘和可视化数据。</p>","[{'comment': 'Recommended to replace that :\n...将复杂的NoSQL数据结构对应到...', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -148,9 +147,9 @@ <h1>Keep using the BI tools you love</h1>
 $ cd apache-drill-&lt;version&gt;
 $ bin/drill-embedded</pre></div>
   <div class=""description"">
-    <h1>Scale from one laptop to 1000s of servers</h1>
-    <p>We made it easy to download and run Drill on your laptop. It runs on Mac, Windows and Linux, and within a minute or two you'll be exploring your data. When you're ready for prime time, deploy Drill on a cluster of commodity servers and take advantage of the world's most scalable and high performance execution engine.
-    <p>Drill's symmetrical architecture (all nodes are the same) and simple installation make it easy to deploy and operate very large clusters.</p>
+    <h1>易于扩展：从一台主机到一千台服务器</h1>","[{'comment': 'Recommended to replace that :\n水平扩展...', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -148,9 +147,9 @@ <h1>Keep using the BI tools you love</h1>
 $ cd apache-drill-&lt;version&gt;
 $ bin/drill-embedded</pre></div>
   <div class=""description"">
-    <h1>Scale from one laptop to 1000s of servers</h1>
-    <p>We made it easy to download and run Drill on your laptop. It runs on Mac, Windows and Linux, and within a minute or two you'll be exploring your data. When you're ready for prime time, deploy Drill on a cluster of commodity servers and take advantage of the world's most scalable and high performance execution engine.
-    <p>Drill's symmetrical architecture (all nodes are the same) and simple installation make it easy to deploy and operate very large clusters.</p>
+    <h1>易于扩展：从一台主机到一千台服务器</h1>
+    <p>Drill开箱即用。无论是在Mac，Windows，或者Linux系统上，只需一两分钟，就可以开始挖掘你的数据。如果你想更进一步领略Drill的优势，可以将Drill部署在商用服务器集群上，便可以充分发挥Drill这款易于扩展的高性能查询引擎。</p>","[{'comment': 'Recommended to replace that :\nDrill开箱即用的特点可以支持在Mac、Windows和Linux上快速完成安装（分钟级）。针对大规模的数据集，也可以部署到商用服务器上，充分利用高性能引擎。', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -160,13 +159,13 @@ <h1>Scale from one laptop to 1000s of servers</h1>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>No more waiting for coffee</h1>
-    <p>Drill isn't the world's first query engine, but it's the first that combines both flexibility and speed. To achieve this, Drill features a radically different architecture that enables record-breaking performance without sacrificing the flexibility offered by the JSON document model. Drill's design includes:<ul>
-<li>Columnar execution engine (the first ever to support complex data!)</li>
-<li>Data-driven compilation and recompilation at execution time</li>
-<li>Specialized memory management that reduces memory footprint and eliminates garbage collections</li>
-<li>Locality-aware execution that reduces network traffic when Drill is co-located with the datastore</li>
-<li>Advanced cost-based optimizer that pushes processing into the datastore when possible</li></ul></p>
+    <h1>快速响应：泡制一杯咖啡的时间</h1>","[{'comment': 'Recommended to replace that :\n快速响应：不需要等待你的咖啡', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -160,13 +159,13 @@ <h1>Scale from one laptop to 1000s of servers</h1>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>No more waiting for coffee</h1>
-    <p>Drill isn't the world's first query engine, but it's the first that combines both flexibility and speed. To achieve this, Drill features a radically different architecture that enables record-breaking performance without sacrificing the flexibility offered by the JSON document model. Drill's design includes:<ul>
-<li>Columnar execution engine (the first ever to support complex data!)</li>
-<li>Data-driven compilation and recompilation at execution time</li>
-<li>Specialized memory management that reduces memory footprint and eliminates garbage collections</li>
-<li>Locality-aware execution that reduces network traffic when Drill is co-located with the datastore</li>
-<li>Advanced cost-based optimizer that pushes processing into the datastore when possible</li></ul></p>
+    <h1>快速响应：泡制一杯咖啡的时间</h1>
+    <p>Drill不是第一款查询引擎, 但却是第一款兼顾了复杂性和检索速度的查询引擎。为了实现这一点, Drill设计了完全不同的架构，使得在不牺牲由JSON文档模型带来的复杂性前提上，实现了创纪录的检索速度。Drill的新设计包括：<ul>","[{'comment': 'Recommended to replace that :\nDrill并不是世界上第一款查询引擎，却是第一个兼顾数据复杂性和查询速度的MPP引擎。Drill设计了与众不同的架构，不仅能够支持JSON数据模型，还做到了非凡的响应速度。', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -160,13 +159,13 @@ <h1>Scale from one laptop to 1000s of servers</h1>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>No more waiting for coffee</h1>
-    <p>Drill isn't the world's first query engine, but it's the first that combines both flexibility and speed. To achieve this, Drill features a radically different architecture that enables record-breaking performance without sacrificing the flexibility offered by the JSON document model. Drill's design includes:<ul>
-<li>Columnar execution engine (the first ever to support complex data!)</li>
-<li>Data-driven compilation and recompilation at execution time</li>
-<li>Specialized memory management that reduces memory footprint and eliminates garbage collections</li>
-<li>Locality-aware execution that reduces network traffic when Drill is co-located with the datastore</li>
-<li>Advanced cost-based optimizer that pushes processing into the datastore when possible</li></ul></p>
+    <h1>快速响应：泡制一杯咖啡的时间</h1>
+    <p>Drill不是第一款查询引擎, 但却是第一款兼顾了复杂性和检索速度的查询引擎。为了实现这一点, Drill设计了完全不同的架构，使得在不牺牲由JSON文档模型带来的复杂性前提上，实现了创纪录的检索速度。Drill的新设计包括：<ul>
+    <li>列式执行引擎 (第一款支持复杂数据的列式执行引擎)</li>
+    <li>在执行时以数据驱动的编译和重编译</li>","[{'comment': 'Recommended to replace that :\n支持运行时期的 Data-driven 编译和重编译。', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -160,13 +159,13 @@ <h1>Scale from one laptop to 1000s of servers</h1>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>No more waiting for coffee</h1>
-    <p>Drill isn't the world's first query engine, but it's the first that combines both flexibility and speed. To achieve this, Drill features a radically different architecture that enables record-breaking performance without sacrificing the flexibility offered by the JSON document model. Drill's design includes:<ul>
-<li>Columnar execution engine (the first ever to support complex data!)</li>
-<li>Data-driven compilation and recompilation at execution time</li>
-<li>Specialized memory management that reduces memory footprint and eliminates garbage collections</li>
-<li>Locality-aware execution that reduces network traffic when Drill is co-located with the datastore</li>
-<li>Advanced cost-based optimizer that pushes processing into the datastore when possible</li></ul></p>
+    <h1>快速响应：泡制一杯咖啡的时间</h1>
+    <p>Drill不是第一款查询引擎, 但却是第一款兼顾了复杂性和检索速度的查询引擎。为了实现这一点, Drill设计了完全不同的架构，使得在不牺牲由JSON文档模型带来的复杂性前提上，实现了创纪录的检索速度。Drill的新设计包括：<ul>
+    <li>列式执行引擎 (第一款支持复杂数据的列式执行引擎)</li>
+    <li>在执行时以数据驱动的编译和重编译</li>
+    <li>独特的内存管理来减少内存印记以避免垃圾回收</li>","[{'comment': 'Recommended to replace that :\n专有的内存管理技术以减少内存占用和避免频繁垃圾回收', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -160,13 +159,13 @@ <h1>Scale from one laptop to 1000s of servers</h1>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>No more waiting for coffee</h1>
-    <p>Drill isn't the world's first query engine, but it's the first that combines both flexibility and speed. To achieve this, Drill features a radically different architecture that enables record-breaking performance without sacrificing the flexibility offered by the JSON document model. Drill's design includes:<ul>
-<li>Columnar execution engine (the first ever to support complex data!)</li>
-<li>Data-driven compilation and recompilation at execution time</li>
-<li>Specialized memory management that reduces memory footprint and eliminates garbage collections</li>
-<li>Locality-aware execution that reduces network traffic when Drill is co-located with the datastore</li>
-<li>Advanced cost-based optimizer that pushes processing into the datastore when possible</li></ul></p>
+    <h1>快速响应：泡制一杯咖啡的时间</h1>
+    <p>Drill不是第一款查询引擎, 但却是第一款兼顾了复杂性和检索速度的查询引擎。为了实现这一点, Drill设计了完全不同的架构，使得在不牺牲由JSON文档模型带来的复杂性前提上，实现了创纪录的检索速度。Drill的新设计包括：<ul>
+    <li>列式执行引擎 (第一款支持复杂数据的列式执行引擎)</li>
+    <li>在执行时以数据驱动的编译和重编译</li>
+    <li>独特的内存管理来减少内存印记以避免垃圾回收</li>
+    <li>通过将Drill与数据存储协同配置以实现本地化感知执行来减少网络通讯</li>","[{'comment': 'Recommended to replace that :\n通过将Drill和数据节点部署在一起来支持数据本地性（Data Locality）', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -160,13 +159,13 @@ <h1>Scale from one laptop to 1000s of servers</h1>
 
 <div class=""home-row"">
   <div class=""description"">
-    <h1>No more waiting for coffee</h1>
-    <p>Drill isn't the world's first query engine, but it's the first that combines both flexibility and speed. To achieve this, Drill features a radically different architecture that enables record-breaking performance without sacrificing the flexibility offered by the JSON document model. Drill's design includes:<ul>
-<li>Columnar execution engine (the first ever to support complex data!)</li>
-<li>Data-driven compilation and recompilation at execution time</li>
-<li>Specialized memory management that reduces memory footprint and eliminates garbage collections</li>
-<li>Locality-aware execution that reduces network traffic when Drill is co-located with the datastore</li>
-<li>Advanced cost-based optimizer that pushes processing into the datastore when possible</li></ul></p>
+    <h1>快速响应：泡制一杯咖啡的时间</h1>
+    <p>Drill不是第一款查询引擎, 但却是第一款兼顾了复杂性和检索速度的查询引擎。为了实现这一点, Drill设计了完全不同的架构，使得在不牺牲由JSON文档模型带来的复杂性前提上，实现了创纪录的检索速度。Drill的新设计包括：<ul>
+    <li>列式执行引擎 (第一款支持复杂数据的列式执行引擎)</li>
+    <li>在执行时以数据驱动的编译和重编译</li>
+    <li>独特的内存管理来减少内存印记以避免垃圾回收</li>
+    <li>通过将Drill与数据存储协同配置以实现本地化感知执行来减少网络通讯</li>
+    <li>先进的资源开销优化器来促使数据处理在数据存储中完成</li></ul></p>","[{'comment': 'Recommended to replace that :\n利用查询优化器的CBO和RBO技术将查询尽可能下推到数据库', 'commenter': 'luocooong'}]"
2266,zh/index.html,"@@ -14,60 +14,59 @@
 
 <script type=""text/javascript"">
 
-$(document).ready(function() {
-  $("".various"").fancybox({
-    fitToView: true,
-    autoSize: true,
-    beforeLoad: function(){
-      var url= $(this.element).attr(""href"");
-      url = url.replace(new RegExp(""watch\\?v="", ""i""), 'v/');
-      url += '?fs=1&autoplay=1';
-      this.href = url
-    }
+  $(document).ready(function() {
+    $("".various"").fancybox({
+      fitToView: true,
+      autoSize: true,
+      beforeLoad: function(){
+        var url= $(this.element).attr(""href"");
+        url = url.replace(new RegExp(""watch\\?v="", ""i""), 'v/');
+        url += '?fs=1&autoplay=1';
+        this.href = url
+      }
+    });
+
+    $('div#video-slider').slick({
+      autoplay: true,
+      autoplaySpeed: 5000,
+      dots: true
+    });
   });
 
-  $('div#video-slider').slick({
-    autoplay: true,
-    autoplaySpeed: 5000,
-    dots: true
-  });
-});
-
 </script>
 
 <div id=""header"" class=""mw"">
 
-<div class=""nav-circlepop"">
-  <a class=""aLeft prev""><span class=""icon-wrap""></span></a>
-  <a class=""aRight next""><span class=""icon-wrap""></span></a>
-</div>
+  <div class=""nav-circlepop"">
+    <a class=""aLeft prev""><span class=""icon-wrap""></span></a>
+    <a class=""aRight next""><span class=""icon-wrap""></span></a>
+  </div>
 
-<div class=""dots""></div>
-<div class=""scroller"">
-  <div class=""item"">
-    <div class=""headlines tc"">
-      <div id=""video-slider"" class=""slider"">
-        <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/UOmlhExchpk""><img src=""{{ site.baseurl }}/images/thumbnail-0rurIzOkTIg.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">Overview of Apache Drill Query Execution</div></div>
-        <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/O6WeniFSa7c""><img src=""{{ site.baseurl }}/images/thumbnail-lslA8kDr_jQ.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">SQL Queries on Parquet Data </div></div>
-        <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/EjxCy7RRUgM""><img src=""{{ site.baseurl }}/images/thumbnail-65c42i7Xg7Q.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">The Rise of the Non-Relational Datastore</div></div>
-        <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/hv_hf_juEiQ""><img src=""{{ site.baseurl }}/images/thumbnail-MYY51kiFPTk.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">Deployment Options and BI Tools</div></div>
-        <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/CGkCvgRwkbs""><img src=""{{ site.baseurl }}/images/thumbnail-bhmNbH2yzhM.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">Connecting to Data Sources</div></div>
-        <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/evQwRwXZaVk""><img src=""{{ site.baseurl }}/images/thumbnail-6pGeQOXDdD8.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">High Performance with a JSON Data Model</div></div>
+  <div class=""dots""></div>
+  <div class=""scroller"">
+    <div class=""item"">
+      <div class=""headlines tc"">
+        <div id=""video-slider"" class=""slider"">
+          <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/UOmlhExchpk""><img src=""{{ site.baseurl }}/images/thumbnail-0rurIzOkTIg.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">Overview of Apache Drill Query Execution</div></div>
+          <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/O6WeniFSa7c""><img src=""{{ site.baseurl }}/images/thumbnail-lslA8kDr_jQ.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">SQL Queries on Parquet Data </div></div>
+          <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/EjxCy7RRUgM""><img src=""{{ site.baseurl }}/images/thumbnail-65c42i7Xg7Q.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">The Rise of the Non-Relational Datastore</div></div>
+          <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/hv_hf_juEiQ""><img src=""{{ site.baseurl }}/images/thumbnail-MYY51kiFPTk.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">Deployment Options and BI Tools</div></div>
+          <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/CGkCvgRwkbs""><img src=""{{ site.baseurl }}/images/thumbnail-bhmNbH2yzhM.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">Connecting to Data Sources</div></div>
+          <div class=""slide""><a class=""various fancybox.iframe"" href=""https://www.youtube.com/embed/evQwRwXZaVk""><img src=""{{ site.baseurl }}/images/thumbnail-6pGeQOXDdD8.jpg"" class=""thumbnail"" /><img src=""{{ site.baseurl }}/images/play-mq.png"" class=""play"" /></a><div class=""title"">High Performance with a JSON Data Model</div></div>
+        </div>
+        <h1 class=""main-headline"">Apache Drill</h1>
+        <h2 id=""sub-headline"">模式自由的SQL查询引擎，<br class=""mobile-break"" />专为Hadoop，NoSQL和<br class=""mobile-break"" />云存储设计</h2>","[{'comment': 'Recommended to replace that :\nSchema-free 类型的SQL引擎', 'commenter': 'luocooong'}]"
2278,logical/src/main/java/org/apache/drill/common/logical/StoragePluginConfig.java,"@@ -29,6 +31,13 @@
   // DO NOT include enabled status in equality and hash
   // comparisons; doing so will break the plugin registry.
   private Boolean enabled;
+  protected final Integer reconnectRetries;","[{'comment': 'This property is used for the Splunk plugin only, maybe it makes sense to move it there instead of declaring in a base class?', 'commenter': 'vvysotskyi'}, {'comment': ""It is better to place it here, because this can be used for any other datasource event now. It is easy to add that for any of them this functionality. But since the implementation for any of them is specific and requires test cases, I didn't add changes for other plugins connections.\r\nTherefore it is a common config for all plugins, like `enabled`"", 'commenter': 'vdiravka'}, {'comment': 'Some plugins might not use this property, or it may be invalid for them, for example, there is no reason to use it for `sys` or `cp` plugins, but they all will have it... If someone else will be willing to use it in one more plugin, it is possible to move it to the common parent class, but for now, it is better to leave it in Splunk config only.', 'commenter': 'vvysotskyi'}]"
2278,contrib/storage-splunk/src/test/java/org/apache/drill/exec/store/splunk/SplunkTestSuite.java,"@@ -71,7 +79,9 @@ public static void initSplunk() throws Exception {
         String hostname = splunk.getHost();
         Integer port = splunk.getFirstMappedPort();
         StoragePluginRegistry pluginRegistry = cluster.drillbit().getContext().getStorage();
-        SPLUNK_STORAGE_PLUGIN_CONFIG = new SplunkPluginConfig(SPLUNK_LOGIN, SPLUNK_PASS, hostname, port, ""1"", ""now"", null);
+        JsonNode storagePluginJson = mapper.readTree(new File(Resources.getResource(""bootstrap-storage-plugins.json"").toURI()));
+        SPLUNK_STORAGE_PLUGIN_CONFIG = mapper.treeToValue(storagePluginJson.get(""storage"").get(""splunk""), SplunkPluginConfig.class);
+        setPort(SPLUNK_STORAGE_PLUGIN_CONFIG, port);","[{'comment': 'Could you please explain the reason for this change? The initial version where the constructor is used looks more clear and simple.', 'commenter': 'vvysotskyi'}, {'comment': ""1. The main reason is:\r\nIn the runtime we don't use the constructor for instantiating PluginConfigs. We deserialize them from JSON.\r\nSo right now the tests and `SplunkTestSuite.initSplunk()` reflects this too.\r\n2. The other reason not all configs can be specified via constructor."", 'commenter': 'vdiravka'}, {'comment': ""1. But the way you are deserializing slightly differs from how it is deserialized in runtime. For example, we don't hack port at runtime, use configured object mapper, etc. Also, you are setting the port number through the reflection, but what if it wasn't deserialized correctly, so it wouldn't be checked.\r\nIf we want to test the config deserializer for it, let's do it in a separate test, but this change only makes things more complex.\r\n2. I don't see how Jackson will set the correct `reconnectRetries` value to the `SplunkPluginConfig` instance. It will call the `SplunkPluginConfig` constructor, marked with `@JsonCreator` annotation, this constructor will call parent one that has the hardcoded number of the `reconnectRetries` value, so no actual value should be passed..."", 'commenter': 'vvysotskyi'}]"
2278,contrib/storage-splunk/src/main/java/org/apache/drill/exec/store/splunk/SplunkConnection.java,"@@ -41,11 +41,14 @@
   private final String hostname;
   private final int port;
   private Service service;
+  private Integer reconnectRetries;
+  private int counter;","[{'comment': 'We could use a single variable here, for example, `connectionAttempts`, decrease its value every time we are trying to connect and check in the catch block whether it is larger than 0.', 'commenter': 'vvysotskyi'}, {'comment': 'Nice suggestion. Will do.\r\nThanks!', 'commenter': 'vdiravka'}]"
2278,contrib/storage-splunk/src/main/resources/bootstrap-storage-plugins.json,"@@ -8,7 +8,8 @@
       ""port"": 8089,
       ""earliestTime"": ""-14d"",
       ""latestTime"": ""now"",
-      ""enabled"": false
+      ""enabled"": false,","[{'comment': 'Is it possible to put the `enabled` at the end?', 'commenter': 'luocooong'}, {'comment': 'What about placing in the beginning of the config?', 'commenter': 'vdiravka'}]"
2278,contrib/storage-splunk/src/main/java/org/apache/drill/exec/store/splunk/SplunkConnection.java,"@@ -71,10 +73,13 @@ public Service connect() {
     loginArgs.setPort(port);
     loginArgs.setPassword(credentials.getPassword());
     loginArgs.setUsername(credentials.getUsername());
-
     try {
+      connectionAttempts--;
       service = Service.connect(loginArgs);
     } catch (Exception e) {","[{'comment': 'Does it need to sleeping for a few seconds?', 'commenter': 'luocooong'}, {'comment': ""Good suggestion. I'll add:\r\n`TimeUnit.SECONDS.sleep(2);`"", 'commenter': 'vdiravka'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcCatalogSchema.java,"@@ -53,11 +54,12 @@
     try (Connection con = source.getConnection();
          ResultSet set = con.getMetaData().getCatalogs()) {
       connectionSchemaName = con.getSchema();
-      while (set.next()) {
-        final String catalogName = set.getString(1);
-        CapitalizingJdbcSchema schema = new CapitalizingJdbcSchema(
-            getSchemaPath(), catalogName, source, dialect, convention, catalogName, null, caseSensitive);
-        schemaMap.put(schema.getName(), schema);
+      if (!ClickhouseConstant.PRODUCT_NAME.equals(con.getMetaData().getDatabaseProductName())) {","[{'comment': 'Let\'s think about this approach a bit. There is no comment here (there should be), so I\'ll infer that Clickhouse somehow does something special. Cool. So I\'m working on Postgres, or DB X and I want to do something similar. Do we end up with a bit set of if-statements? If I work on DB X, how do I ensure I don\'t break Clickhouse, since I can\'t run that code?\r\n\r\nCan we think about a way of abstracting this kind of thing into a DB-specific helper class? The default helper does ""stock"" JDBC. Clickhouse (and my DB X) code go into DB-specific helpers. Now, to get DB X to work, I don\'t have to worry about breaking Clickhouse.\r\n\r\nSee if you can work out a ""mini-plugin"" for such code. Happy to provide suggestions if you like.', 'commenter': 'paul-rogers'}, {'comment': 'I have added JdbcSchemaFactory and JdbcSqlGenerator, please have a look.', 'commenter': 'Leon-WTF'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcCatalogSchema.java,"@@ -108,7 +113,7 @@ private boolean addSchemas(DataSource source, SqlDialect dialect, DrillJdbcConve
 
         String parentKey = StringUtils.lowerCase(catalogName);
         CapitalizingJdbcSchema parentSchema = schemaMap.get(parentKey);
-        if (parentSchema == null) {
+        if (parentSchema == null || isFirstLevel) {","[{'comment': 'Nit: comment to explain why the first level matters.', 'commenter': 'paul-rogers'}, {'comment': 'See above', 'commenter': 'Leon-WTF'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcPrel.java,"@@ -39,6 +39,9 @@
 import org.apache.drill.exec.planner.physical.visitor.PrelVisitor;
 import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
 import org.apache.drill.exec.store.SubsetRemover;
+import org.apache.drill.exec.store.jdbc.clickhouse.ClickhouseConstant;
+import org.apache.drill.exec.store.jdbc.clickhouse.ClickhouseDialect;
+import org.apache.drill.exec.store.jdbc.clickhouse.ClickhouseJdbcImplementor;","[{'comment': 'See comment above. The generic `JdbcPrel` is not the place to add vendor-specific code. However, this class can call a vendor-specific helper.', 'commenter': 'paul-rogers'}, {'comment': 'See above', 'commenter': 'Leon-WTF'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/clickhouse/ClickhouseConstant.java,"@@ -0,0 +1,27 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.jdbc.clickhouse;
+
+/**
+ * @author feiteng.wtf
+ * @date 2021-08-08
+ */","[{'comment': 'Generally Drill does not include the author and date. What we do include is an explanation of what the class is about.', 'commenter': 'paul-rogers'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/clickhouse/ClickhouseDialect.java,"@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.jdbc.clickhouse;
+
+import org.apache.calcite.rel.type.RelDataType;
+import org.apache.calcite.sql.SqlBasicTypeNameSpec;
+import org.apache.calcite.sql.SqlDataTypeSpec;
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.calcite.sql.SqlNode;
+import org.apache.calcite.sql.SqlUserDefinedTypeNameSpec;
+import org.apache.calcite.sql.SqlWriter;
+import org.apache.calcite.sql.parser.SqlParserPos;
+import org.apache.calcite.sql.type.BasicSqlType;
+
+/**
+ * @author feiteng.wtf
+ * @date 2021-08-08
+ */","[{'comment': 'See comment above.', 'commenter': 'paul-rogers'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/clickhouse/ClickhouseDialect.java,"@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.jdbc.clickhouse;
+
+import org.apache.calcite.rel.type.RelDataType;
+import org.apache.calcite.sql.SqlBasicTypeNameSpec;
+import org.apache.calcite.sql.SqlDataTypeSpec;
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.calcite.sql.SqlNode;
+import org.apache.calcite.sql.SqlUserDefinedTypeNameSpec;
+import org.apache.calcite.sql.SqlWriter;
+import org.apache.calcite.sql.parser.SqlParserPos;
+import org.apache.calcite.sql.type.BasicSqlType;
+
+/**
+ * @author feiteng.wtf
+ * @date 2021-08-08
+ */
+public class ClickhouseDialect extends SqlDialect {","[{'comment': 'This is a great example of a vendor-specific implementation. Can we follow the same pattern elsewhere?', 'commenter': 'paul-rogers'}, {'comment': 'I\'m not sure if this is relevant here or not, but Calcite actually has a Clickhouse dialect.[1]. Does that ""answer the mail"" for this?\r\n\r\n[1]: https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/sql/dialect/ClickHouseSqlDialect.java', 'commenter': 'cgivre'}, {'comment': 'Thanks, but as it is not included in the current version of calcite used in drill, so I will need to do a cherry pick on DrillCalcite1.21.0 of https://github.com/vvysotskyi/drill-calcite.git and update to 1.21.0-drill-r4, right?', 'commenter': 'Leon-WTF'}, {'comment': 'To update Calcite - yes. Please do it via PR\r\nHere is the process how to do that: https://github.com/apache/drill/blob/master/docs/dev/Calcite.md#process-of-updating-calcite-version\r\nYou can ask me or @vvysotskyi to follow you in this', 'commenter': 'vdiravka'}, {'comment': 'Please have a look at https://github.com/vvysotskyi/drill-calcite/pull/5', 'commenter': 'Leon-WTF'}, {'comment': 'Your Calcite PR: https://github.com/vvysotskyi/drill-calcite/pull/5 is merged, git tag is created and Maven artifacts are available in Jitpack:\r\nhttps://jitpack.io/#vvysotskyi/drill-calcite', 'commenter': 'vdiravka'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/clickhouse/ClickhouseJdbcImplementor.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.jdbc.clickhouse;
+
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+import org.apache.calcite.adapter.java.JavaTypeFactory;
+import org.apache.calcite.adapter.jdbc.JdbcImplementor;
+import org.apache.calcite.adapter.jdbc.JdbcTableScan;
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.calcite.sql.SqlIdentifier;
+
+import java.util.Iterator;
+
+/**
+ * @author feiteng.wtf
+ * @date 2021-07-26
+ */","[{'comment': 'Ditto.', 'commenter': 'paul-rogers'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/writers/JdbcBigintWriter.java,"@@ -31,9 +31,9 @@ public JdbcBigintWriter(String colName, RowSetLoader rowWriter, int columnIndex)
 
   @Override
   public void load(ResultSet results) throws SQLException {
-    boolean b = results.wasNull();
-    if (! results.wasNull()) {
-      long value = results.getLong(columnIndex);
+    // clickhouse requires getting the column before checking nullability
+    long value = results.getLong(columnIndex);
+    if (!results.wasNull()) {","[{'comment': 'Good catch! As it turns out, the original code is wrong for all DBs, not just Clickhouse. See [this article](https://schneide.blog/2021/02/22/jdbcs-wasnull-method-pitfall/).\r\n\r\nSuggestion, change the comment to something like ""JDBC reports nullability only after getting the column value."" Here and below.', 'commenter': 'paul-rogers'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcSchemaFactory.java,"@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.jdbc;
+
+import org.apache.calcite.schema.SchemaPlus;
+import org.apache.drill.exec.store.SchemaConfig;
+import org.apache.drill.exec.store.jdbc.clickhouse.ClickhouseCatalogSchema;
+
+public class JdbcSchemaFactory {
+  private final JdbcStoragePlugin plugin;
+
+  public JdbcSchemaFactory (JdbcStoragePlugin plugin) {
+    this.plugin = plugin;
+  }
+
+  public void registerSchemas(SchemaConfig config, SchemaPlus parent) {
+    if (plugin.getConfig().isClickhouse()) {","[{'comment': 'Here we\'ve added an `isClickhouse()` method to the base JDBC plugin. We might add five more ""isDruid()"", ""isAthena()"", ""isNextBigThing()"", ... Then we add logic in various places that use those queries.\r\n\r\nIn doing so, we couple all these dialects and make the code harder to maintain.\r\n\r\nSuggestion: add a `getDialect()` method to the config. It returns a `JdbcDialect` subclass. `getDialect()` does what is needed to identify Clickhouse (or whatever) and return the specific dialect. Else, it returns a `GenericDialect` instance.\r\n\r\nOne of the methods on `JdbcDialect` is `registerSchemas()`. The ""then"" part of this code goes in the Clickhouse version, the ""else"" part of this code goes into the `GenericDialect` version.\r\n\r\nVoila! No more tight coupling of dialects.\r\n\r\nOf course, if there is a better way to solve the problem, feel free to use that instead; the above is a quick & dirty example.', 'commenter': 'paul-rogers'}, {'comment': ""@paul-rogers \r\nAlternatively, the dialect is actually stored in the Plugin file, not the config file.   More good news is that there already is a method in the Plugin file:\r\n\r\nhttps://github.com/apache/drill/blob/39b565f112122734c080324fdcbef518ced16507/contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcStoragePlugin.java#L81-L83\r\n\r\nIf I'm tracking the logic correctly, you could use this logic to get the dialect without having to modify the config at all. \r\n"", 'commenter': 'cgivre'}, {'comment': '@cgivre I think @paul-rogers means a new ""dialect"" interface to be implemented to handle the differrent behaviors for databases connected using JdbcStoragePlugin, as registerSchemas and generateSql. ', 'commenter': 'Leon-WTF'}]"
2290,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcSqlGenerator.java,"@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.jdbc;
+
+import org.apache.calcite.adapter.java.JavaTypeFactory;
+import org.apache.calcite.adapter.jdbc.JdbcImplementor;
+import org.apache.calcite.plan.RelOptCluster;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.drill.exec.store.SubsetRemover;
+import org.apache.drill.exec.store.jdbc.clickhouse.ClickhouseJdbcImplementor;
+
+public class JdbcSqlGenerator {
+  /**
+   * Generate sql for different jdbc databases
+   */
+  public static String generateSql(JdbcStoragePlugin plugin,
+                                   RelOptCluster cluster, RelNode input) {
+    final SqlDialect dialect = plugin.getDialect();
+    final JdbcImplementor jdbcImplementor;
+    if (plugin.getConfig().isClickhouse()) {","[{'comment': ""Here, we'd add a `generateSQL` method to our `JdbcDialect` interface, moving this code to subclasses as described above."", 'commenter': 'paul-rogers'}]"
2290,logical/src/main/java/org/apache/drill/common/logical/StoragePluginConfig.java,"@@ -19,11 +19,9 @@
 
 
 import com.fasterxml.jackson.annotation.JsonIgnore;
-import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonTypeInfo;
 
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = ""type"")
-@JsonInclude(JsonInclude.Include.NON_DEFAULT)","[{'comment': 'Please revert these changes. It helps to preserve backward compatibility for the case when the new option was added, the user started a newer version of Drill, but after that decided to use previous one.', 'commenter': 'vvysotskyi'}, {'comment': 'Got it, this is for fixing one ut case, I have tried another way.', 'commenter': 'Leon-WTF'}]"
2299,exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/VectorOutput.java,"@@ -248,8 +249,8 @@ public void writeTimestamp(boolean isNull) throws IOException {
           ts.writeTimeStamp(dt.getMillis());
           break;
         case VALUE_STRING:
-          OffsetDateTime originalDateTime = OffsetDateTime.parse(parser.getValueAsString(), DateUtility.isoFormatTimeStamp);
-          OffsetDateTime utcDateTime = OffsetDateTime.of(originalDateTime.toLocalDateTime(), ZoneOffset.UTC);   // strips the time zone from the original
+          LocalDateTime localDateTime = LocalDateTime.parse(parser.getValueAsString(), DateUtility.UTC_FORMATTER);
+          OffsetDateTime utcDateTime = OffsetDateTime.of(localDateTime, ZoneOffset.UTC);   // strips the time zone from the original","[{'comment': 'This one is going to be tricky! The incoming date is in UTC. So, we have to use the offset for the local time zone to convert the UTC date/time to local time. We then write local time into the value vector. See [this file](https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/values/TimestampValueListener.java) for how the new JSON parser does this task.', 'commenter': 'paul-rogers'}, {'comment': ""Also, please add a comment to say why we're doing things this way. Perhaps reference the Mongo specs. Links are in the file mentioned in the previous comment."", 'commenter': 'paul-rogers'}]"
2299,exec/java-exec/src/test/java/org/apache/drill/TestFrameworkTest.java,"@@ -219,7 +221,8 @@ public void testBaselineValsVerificationWithComplexAndNulls() throws Exception {
                             mapOf(""pink"", ""purple"")),
                         listOf(4l, 2l),
                         listOf(listOf(2l, 1l),
-                            listOf(4l, 6l)))
+                            listOf(4l, 6l)),
+                        LocalDateTime.parse(""2019-09-30T20:47:43.123Z"", DateUtility.UTC_FORMATTER))","[{'comment': ""This isn't right. The data file has this same timestamp in UTC. The date that should appear in Drill is that same time converted to the local time zone. See [this test](https://github.com/apache/drill/blob/master/exec/java-exec/src/test/java/org/apache/drill/exec/store/easy/json/loader/TestExtendedTypes.java#L180) for how the work is done for the new JSON parser."", 'commenter': 'paul-rogers'}]"
2309,_docs/en/connect-a-data-source/plugins/111-OCI-OS-storage-plugin.md,"@@ -0,0 +1,44 @@
+---
+title: ""OCI OS Storage Plugin""
+slug: ""OCI OS Storage Plugin""
+parent: ""Connect a Data Source""
+---
+Similar to S3 Storage Plugin Drill can be configured to query Oracle Cloud Infrastructure (OCI) Object Storage (OS). 
+The ability to query this cloud storage is implemented by using Oracle HDFS library.
+
+To connect Drill to OCI OS:  
+
+- Provide your OCI credentials.   
+- Configure the OCI OS storage plugin with an OS bucket name.  
+
+For additional information, refer to the [HDFS Connector for Object Storage](https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/hdfsconnector.htm) documentation.   
+
+## Configuring the OCI OS Storage Plugin
+
+The **Storage** page in the Drill Web UI provides an OS storage plugin that you configure to connect Drill to the OS distributed file system registered in `core-site.xml`. If you did not define your OCI credentials in the `core-site.xml` file, you can define them in the storage plugin configuration. You can define the credentials directly in the OS storage plugin configuration, or you can configure the OS storage plugin to use an external provider.
+
+To configure the OCI OS storage plugin, log in to the Drill Web UI at `http://<drill-hostname>:8047`. The `drill-hostname` is a node on which Drill is running. Go to the **Storage** page and click **Update** next to the OS storage plugin option or **Create** new if it doesn't exist yet. 
+
+	{
+ 	""type"": ""file"",
+	""connection"": ""oci://{bucket_name}@{namespace}/"",
+	""config"": {
+		""fs.oci.client.hostname"": ""https://objectstorage.us-ashburn-1.oraclecloud.com"",
+		""fs.oci.client.auth.tenantId"": ""ocid1.tenancy.oc1..exampleuniqueID"",
+		""fs.oci.client.auth.userId"": ""ocid1.user.oc1..exampleuniqueID"",
+		""fs.oci.client.auth.fingerprint"": ""20:3b:97:13:55:1c:5b:0d:d3:37:d8:50:4e:c5:3a:34"",","[{'comment': '@vdiravka \r\nIs this fingerprint ok to publish here?', 'commenter': 'cgivre'}, {'comment': 'Yes, it is :)\r\nIt is an example from https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/hdfsconnector.htm#ariaid-title4', 'commenter': 'vdiravka'}]"
2309,_docs/en/connect-a-data-source/plugins/111-OCI-OS-storage-plugin.md,"@@ -0,0 +1,44 @@
+---
+title: ""OCI OS Storage Plugin""
+slug: ""OCI OS Storage Plugin""
+parent: ""Connect a Data Source""
+---
+Similar to S3 Storage Plugin Drill can be configured to query Oracle Cloud Infrastructure (OCI) Object Storage (OS). 
+The ability to query this cloud storage is implemented by using Oracle HDFS library.
+
+To connect Drill to OCI OS:  
+
+- Provide your OCI credentials.   
+- Configure the OCI OS storage plugin with an OS bucket name.  
+
+For additional information, refer to the [HDFS Connector for Object Storage](https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/hdfsconnector.htm) documentation.   
+
+## Configuring the OCI OS Storage Plugin
+
+The **Storage** page in the Drill Web UI provides an OS storage plugin that you configure to connect Drill to the OS distributed file system registered in `core-site.xml`. If you did not define your OCI credentials in the `core-site.xml` file, you can define them in the storage plugin configuration. You can define the credentials directly in the OS storage plugin configuration, or you can configure the OS storage plugin to use an external provider.
+
+To configure the OCI OS storage plugin, log in to the Drill Web UI at `http://<drill-hostname>:8047`. The `drill-hostname` is a node on which Drill is running. Go to the **Storage** page and click **Update** next to the OS storage plugin option or **Create** new if it doesn't exist yet. 
+
+	{
+ 	""type"": ""file"",
+	""connection"": ""oci://{bucket_name}@{namespace}/"",
+	""config"": {
+		""fs.oci.client.hostname"": ""https://objectstorage.us-ashburn-1.oraclecloud.com"",
+		""fs.oci.client.auth.tenantId"": ""ocid1.tenancy.oc1..exampleuniqueID"",
+		""fs.oci.client.auth.userId"": ""ocid1.user.oc1..exampleuniqueID"",
+		""fs.oci.client.auth.fingerprint"": ""20:3b:97:13:55:1c:5b:0d:d3:37:d8:50:4e:c5:3a:34"",
+		""fs.oci.client.auth.pemfilepath"": ""/opt/drill/conf/oci_api_key.pem""
+	    },
+	  ""workspaces"": {
+	    ...
+	  }  
+
+**Note:** The `""config""` block in the OS storage plugin configuration contains properties to define your OCI credentials. Do not include the `""config""` block in your OS storage plugin configuration if you defined your OCI credentials in the `core-site.xml` file.
+
+To configure the plugin in core-site.xml file, navigate to the $DRILL_HOME/conf or $DRILL_SITE directory, and rename the core-site-example.xml file to core-site.xml
+
+Configure the OS storage plugin configuration to use an external provider for credentials or directly add the credentials in the configuration itself, as described below. Click **Update** to save the configuration when done.
+
+## Providing OCI OS Credentials","[{'comment': 'I know it is documented elsewhere, but could we provide information about how to use the password vault and/or an example here as well?', 'commenter': 'cgivre'}, {'comment': ""I'll set up Vault and add the plugin configs example here then"", 'commenter': 'vdiravka'}]"
2310,exec/java-exec/src/main/java/org/apache/drill/exec/store/base/filter/FilterPushDownListener.java,"@@ -43,7 +43,7 @@
  * <dl>
  * <p>
  * In both cases, the conditions are in the form of a
- * {@link ColRelOpConst} in which one side refers to a column in the scan
+ * {@link  } in which one side refers to a column in the scan","[{'comment': 'What is `_`?', 'commenter': 'vdiravka'}, {'comment': '```suggestion\r\n * {@link ExprNode.ColRelOpConstNode} in which one side refers to a column in the scan\r\n```', 'commenter': 'vdiravka'}]"
2310,exec/java-exec/src/main/java/org/apache/drill/exec/store/base/filter/FilterPushDownListener.java,"@@ -100,10 +100,6 @@
      * If so, return an equivalent RelOp with the value normalized to what
      * the plugin needs. The returned value may be the same as the original
      * one if the value is already normalized.
-     *
-     * @param groupScan the scan element. Use {@code scan.getGroupScan()}
-     * to get the group scan
-     * @param relOp the description of the relational operator expression","[{'comment': 'Need to add javadoc for `conjunct` param instead of the removed ones', 'commenter': 'vdiravka'}]"
2310,exec/java-exec/src/main/java/org/apache/drill/exec/store/base/filter/FilterPushDownListener.java,"@@ -126,13 +122,7 @@
      * to leave in the query. Those terms can be the ones passed in, or
      * new terms to handle special needs.
      *
-     * @param groupScan the scan node
-     * @param andTerms a list of the CNF (AND) terms, in which each is given
-     * by the Calcite AND node and the derived RelOp expression.
-     * @param orTerm the DNF (OR) term, if any, that includes the Calcite
-     * node for that term and the set of OR terms. Only provided if the OR
-     * term represents a simple list of values (all OR clauses are on the
-     * same column). The OR term itself is AND'ed with the CNF terms.
+     * @param expr","[{'comment': ""Could you describe all newly added empty params? We usually don't add empty param docs\r\nPlease refer to the doc of `AndNode` to accomplish that"", 'commenter': 'vdiravka'}]"
2310,contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/JsonTableGroupScan.java,"@@ -377,7 +377,11 @@ private ScanStats indexScanStats() {
     int totalColNum = STAR_COLS;
     PluginCost pluginCostModel = formatPlugin.getPluginCostModel();
     final int avgColumnSize = pluginCostModel.getAverageColumnSize(this);
-    boolean filterPushed = (scanSpec.getSerializedFilter() != null);
+    boolean filterPushed;
+    if (scanSpec == null) {","[{'comment': 'Looks like `scanSpec` is required field for this class and it is used everywhere in the class, but never checked for the `null` value. So it looks like a redundant check.', 'commenter': 'vdiravka'}]"
2310,common/src/main/java/org/apache/drill/common/HistoricalLog.java,"@@ -119,16 +119,10 @@ public void buildHistory(final StringBuilder sb, boolean includeStackTrace) {
    * events with their stack traces.
    *
    * @param sb {@link StringBuilder} to write to
-   * @param additional an extra string that will be written between the identifying
-   *     information and the history; often used for a current piece of state
-   */
-
-  /**
-   *
-   * @param sb
-   * @param indexLevel
+   * @param indent
    * @param includeStackTrace
    */
+","[{'comment': 'redundant space', 'commenter': 'vdiravka'}]"
2310,drill-yarn/src/main/java/org/apache/drill/yarn/appMaster/DrillControllerFactory.java,"@@ -185,7 +185,7 @@ public Dispatcher build() throws ControllerFactoryException {
    * This class is very Linux-specific. The usual adjustments must be made to
    * adapt it to Windows.
    *
-   * @param config
+   * @param resources
    * @return
    * @throws DoyConfigException
    */","[{'comment': 'What should the description be for the config param on line 360?', 'commenter': 'estherbuchwalter'}, {'comment': 'Perhaps `Map of local resource configurations`?', 'commenter': 'cgivre'}]"
2310,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/VarLenOverflowReader.java,"@@ -39,8 +39,8 @@
   /**
    * CTOR.
    * @param buffer byte buffer for data buffering (within CPU cache)
-   * @param pageInfo page being processed information
-   * @param columnPrecInfo column precision information
+   * @param containerCallback","[{'comment': ""I think we need some descriptor here as well.  As I'm not familiar with the code, perhaps just something obvious like \r\n`callbackContainer The call back container`.  Not particularly helpful, but at least there is something. "", 'commenter': 'cgivre'}]"
2310,exec/java-exec/src/main/java/org/apache/drill/exec/util/record/RecordBatchStats.java,"@@ -49,7 +49,8 @@
     private final String contextOperatorId;
 
     /**
-     * @param options options manager
+     * @param context
+     * @param oContext","[{'comment': 'Here as well... ', 'commenter': 'cgivre'}]"
2310,logical/src/main/java/org/apache/drill/common/graph/GraphAlgos.java,"@@ -69,8 +69,7 @@ private void visit(AdjacencyList<V>.Node n) {
      *
      * @param graph
      *          The adjacency list for the DAG.
-     * @param sourceNodes
-     *          List of nodes that
+     * @param reverse","[{'comment': 'And finally here. ', 'commenter': 'cgivre'}, {'comment': ""@estherbuchwalter \r\nIf you can take care of this and the other empty JavaDoc flagged here, I think we'll be good to go and we can merge this. \r\n"", 'commenter': 'cgivre'}, {'comment': ""Would this be a proper definition for reverse: 'true if reversed, otherwise false'?"", 'commenter': 'estherbuchwalter'}, {'comment': 'Sure', 'commenter': 'cgivre'}]"
2310,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/SingleVectorState.java,"@@ -258,10 +258,12 @@ public int allocate(int cardinality) {
    * row, or for some previous row, depending on exactly when and where the
    * overflow occurs.
    *
-   * @param sourceStartIndex the index of the row that caused the overflow, the
+   * sourceStartIndex: the index of the row that caused the overflow, the
    * values of which should be copied to a new ""look-ahead"" vector. If the
    * vector is an array, then the overflowIndex is the position of the first
    * element to be moved, and multiple elements may need to move
+   *
+   * @param cardinality","[{'comment': 'Would an accurate description for cardinality here be: the number of unique columns in the row?', 'commenter': 'estherbuchwalter'}, {'comment': 'I think so. ', 'commenter': 'cgivre'}, {'comment': 'Okay, thank you. Should I also add descriptors to any empty Javadoc return statements?', 'commenter': 'estherbuchwalter'}, {'comment': 'Yes please. ', 'commenter': 'cgivre'}]"
2310,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/VarLenColumnBulkInput.java,"@@ -75,7 +75,7 @@
    * CTOR.
    * @param parentInst parent object instance
    * @param recordsToRead number of records to read
-   * @param columnPrecInfo column precision information
+   * @param bulkReaderState
    * @throws IOException runtime exception in case of processing error
    */","[{'comment': 'This is a comment for lines 94, 290, and 366. There is a typo in the function name: loadPageIfNeeed(). I would like to fix it but do not want to mess up any other files as a result. If I change the name in all 3 cases, is it safe to fix?', 'commenter': 'estherbuchwalter'}, {'comment': 'Also, the DefinitionLevelReader link throws an error on line 684. Would `DefLevelReaderWrapper` be a proper replacement?', 'commenter': 'estherbuchwalter'}, {'comment': '> This is a comment for lines 94, 290, and 366. There is a typo in the function name: loadPageIfNeeed(). I would like to fix it but do not want to mess up any other files as a result. If I change the name in all 3 cases, is it safe to fix?\r\n\r\nYou should the `refactor->rename` feature in your IDE.  It will rename all the occurrences of that function name, even those in other files. ;-)', 'commenter': 'cgivre'}, {'comment': '> Also, the DefinitionLevelReader link throws an error on line 684. Would `DefLevelReaderWrapper` be a proper replacement?\r\n\r\nI think so.', 'commenter': 'cgivre'}, {'comment': 'Great, thank you!', 'commenter': 'estherbuchwalter'}]"
2310,exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/rm/QueryQueue.java,"@@ -42,8 +42,6 @@
     /**
      * Release a query lease obtained from {@link #queue(QueryId, double))}.
      * Should be called by the per-query resource manager.
-     *
-     * @param lease the lease to be released.
      */
 
     void release();","[{'comment': ""This comment is for line 128. The link '#release(QueueLease)' throws an error. Does anyone have an idea what the replacement should be?"", 'commenter': 'estherbuchwalter'}, {'comment': 'There are two methods related to this, but they both are private in `EmbeddedQueryQueue` and `DistributedQueryQueue`. I think it will be fine to use code instead of link there to avoid error.', 'commenter': 'vvysotskyi'}, {'comment': 'Thank you for checking into this. So, what would you recommend I replace the link with, or should I just delete the link?', 'commenter': 'estherbuchwalter'}, {'comment': 'Please replace `{@link` with `{@code`', 'commenter': 'vvysotskyi'}, {'comment': 'Only got here now.  Thanks @vvysotskyi for assisting.', 'commenter': 'jnturton'}]"
2311,exec/java-exec/src/main/resources/bootstrap-storage-plugins.json,"@@ -111,6 +111,65 @@
       },
       ""enabled"" : false
     },
+    ""os"" : {","[{'comment': ""@vdiravka \r\nThanks for this PR.  I'm concerned with calling this config `os` as that kind of implies `operating system` and there are similar tools that query operating systems.  In the example config, could we perhaps call it `oci` or `oracle_cloud` or something besides `os`?"", 'commenter': 'cgivre'}, {'comment': ""agree. I also named that `oci` when tested locally. The reason why I named it OS, because it corresponds to `S3` in Amazon:\r\n`aws` = `oci` and `s3` = `os`.\r\nBut since we don't expect any other Storage Plugin for Oracle Cloud Infrastructure, we can name it as `oci`."", 'commenter': 'vdiravka'}]"
2317,Dockerfile,"@@ -16,31 +16,53 @@
 # limitations under the License.
 #
 
-# This Dockerfile is used for automated builds in DockerHub. It adds project sources into the build image, builds
-# Drill and copies built binaries into the target image based on openjdk:8u232-jdk image.
+# This Dockerfile is used for automated builds in DockerHub. It adds
+# project sources into the build image, builds Drill and copies built
+# binaries into the target image based on the image name in BASE_NAME
+# env var which you should set when invoking docker build. 
+# Example BASE_NAME values: openjdk:8-jre, openjdk:11-jre, openjdk:latest
 
 # Uses intermediate image for building Drill to reduce target image size
-FROM maven:3.6.3-jdk-8 as build
+FROM maven:3.8.2-openjdk-11 as build","[{'comment': 'Are you sure that jars built using JDK 11 will be running fine on JDB 8?', 'commenter': 'vvysotskyi'}, {'comment': 'Oh I better check this, thanks', 'commenter': 'jnturton'}, {'comment': 'Reverted the build container to OpenJDK 8', 'commenter': 'jnturton'}]"
2317,Dockerfile,"@@ -16,31 +16,53 @@
 # limitations under the License.
 #
 
-# This Dockerfile is used for automated builds in DockerHub. It adds project sources into the build image, builds
-# Drill and copies built binaries into the target image based on openjdk:8u232-jdk image.
+# This Dockerfile is used for automated builds in DockerHub. It adds
+# project sources into the build image, builds Drill and copies built
+# binaries into the target image based on the image name in BASE_NAME
+# env var which you should set when invoking docker build. 
+# Example BASE_NAME values: openjdk:8-jre, openjdk:11-jre, openjdk:latest
 
 # Uses intermediate image for building Drill to reduce target image size
-FROM maven:3.6.3-jdk-8 as build
+FROM maven:3.8.2-openjdk-11 as build
+
+WORKDIR /src
 
 # Copy project sources into the container
-COPY . /src
+COPY . .
 
-WORKDIR /src
+# Optimisation: build a selection of Drill modules in advance.
+# It isn't necessary to build any modules independently but doing so divides
+# the downloading and compiling up over multiple cacheable layers, better
+# enabling reuse in the event of an isolated change and resume in the event
+# of a build error.  This paragraph can safely be commented out.
+RUN mvn -am -pl tools        -Dmaven.artifact.threads=5 -T1C install -DskipTests
+RUN mvn -am -pl protocol     -Dmaven.artifact.threads=5 -T1C install -DskipTests
+RUN mvn -am -pl common       -Dmaven.artifact.threads=5 -T1C install -DskipTests
+RUN mvn -am -pl logical      -Dmaven.artifact.threads=5 -T1C install -DskipTests
+RUN mvn -am -pl exec         -Dmaven.artifact.threads=5 -T1C install -DskipTests
+RUN mvn -am -pl drill-yarn   -Dmaven.artifact.threads=5 -T1C install -DskipTests
+RUN mvn -am -pl distribution -Dmaven.artifact.threads=5 -T1C install -DskipTests
 
 # Builds Drill
-RUN  mvn clean install -DskipTests -q
+RUN mvn -Dmaven.artifact.threads=5 -T1C install -DskipTests
 
 # Get project version and copy built binaries into /opt/drill directory
 RUN VERSION=$(mvn -q -Dexec.executable=echo -Dexec.args='${project.version}' --non-recursive exec:exec) \
  && mkdir /opt/drill \
  && mv distribution/target/apache-drill-${VERSION}/apache-drill-${VERSION}/* /opt/drill
 
 # Target image
-FROM openjdk:8u232-jdk
 
-RUN mkdir /opt/drill
+# Set the BASE_IMAGE env var when you invoke docker build.  
+FROM $BASE_IMAGE","[{'comment': 'Please assign a default value for `BASE_IMAGE` to preserve backward compatibility when building images without specifying env variables.', 'commenter': 'vvysotskyi'}]"
2317,Dockerfile,"@@ -16,31 +16,43 @@
 # limitations under the License.
 #
 
-# This Dockerfile is used for automated builds in DockerHub. It adds project sources into the build image, builds
-# Drill and copies built binaries into the target image based on openjdk:8u232-jdk image.
+# This Dockerfile is used for automated builds in DockerHub. It adds
+# project sources into the build image, builds Drill and copies built
+# binaries into the target image based on the image name in BASE_IMAGE
+# build arg which you should set when invoking docker build. 
+# Example syntax: docker build --build-arg BASE_IMAGE=""openjdk:8-jre""
 
-# Uses intermediate image for building Drill to reduce target image size
-FROM maven:3.6.3-jdk-8 as build
+ARG $BASE_IMAGE=openjdk:11-jre
 
-# Copy project sources into the container
-COPY . /src
+# Uses intermediate image for building Drill to reduce target image size
+# Build using OpenJDK 8 to maintain compatibility with all OpenJDK >= 8
+FROM maven:3.8.2-openjdk-8 as build","[{'comment': 'It can be made configurable too.', 'commenter': 'vvysotskyi'}]"
2317,Dockerfile,"@@ -16,31 +16,43 @@
 # limitations under the License.
 #
 
-# This Dockerfile is used for automated builds in DockerHub. It adds project sources into the build image, builds
-# Drill and copies built binaries into the target image based on openjdk:8u232-jdk image.
+# This Dockerfile is used for automated builds in DockerHub. It adds
+# project sources into the build image, builds Drill and copies built
+# binaries into the target image based on the image name in BASE_IMAGE
+# build arg which you should set when invoking docker build. 
+# Example syntax: docker build --build-arg BASE_IMAGE=""openjdk:8-jre""
 
-# Uses intermediate image for building Drill to reduce target image size
-FROM maven:3.6.3-jdk-8 as build
+ARG $BASE_IMAGE=openjdk:11-jre","[{'comment': 'Please use JDK 8 as a default one.', 'commenter': 'vvysotskyi'}]"
2317,hooks/build,"@@ -0,0 +1,4 @@
+docker build --build-arg BASE_IMAGE=openjdk:8-jre -t apache/drill:latest .
+docker build --build-arg BASE_IMAGE=openjdk:11-jre -t apache/drill:openjdk11 .
+docker build --build-arg BASE_IMAGE=openjdk:17 -t apache/drill:openjdk17 .","[{'comment': ""We don't support JDK 17 yet: https://github.com/apache/drill/blob/master/.github/workflows/ci.yml#L36"", 'commenter': 'vvysotskyi'}]"
2317,hooks/build,"@@ -0,0 +1,38 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+docker build \
+	--build-arg BUILD_BASE_IMAGE=maven:3.8.2-openjdk-8 \
+	--build-arg BASE_IMAGE=openjdk:8-jre \
+	-t apache/drill:openjdk-8 \
+	-t apache/drill:latest \","[{'comment': 'Just recalled the latest tag is not published for the master version. It is published for release only, so please delete it here. We use the `master` tag for builds from the master branch. The good news is that according to https://docs.docker.com/docker-hub/builds/advanced/#environment-variables-for-building-and-testing, we can use tag name from the dockerhub configuration: `DOCKER_TAG`, so please replace `latest` with that value, and add this variable to tag names with open-jdk, so we will have something like `$DOCKER_TAG-openjdk:8-jre` that will become `master-openjdk:8-jre` or `latest-openjdk:8-jre` or `1.20.0-openjdk:8-jre` for the release.', 'commenter': 'vvysotskyi'}]"
2317,hooks/push,"@@ -0,0 +1,21 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+docker push apache/drill:openjdk-8
+docker push apache/drill:openjdk-11
+docker push apache/drill:openjdk-14","[{'comment': 'Please also use `DOCKER_TAG` var here, and add one more line to push `apache/drill:$DOCKER_TAG` (tag without openjdk suffix)', 'commenter': 'vvysotskyi'}]"
2321,contrib/storage-hive/hive-exec-shade/pom.xml,"@@ -120,6 +120,10 @@
           <groupId>org.codehaus.jackson</groupId>
           <artifactId>jackson-xc</artifactId>
         </exclusion>
+        <exclusion>
+          <groupId>io.airlift</groupId>
+          <artifactId>aircompressor</artifactId>
+        </exclusion>","[{'comment': 'Is there any reason for excluding `aircompressor` here if you have relocated it below?', 'commenter': 'vvysotskyi'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/PageReader.java,"@@ -17,18 +17,24 @@
  */
 package org.apache.drill.exec.store.parquet.columnreaders;
 
+import static org.apache.parquet.column.Encoding.valueOf;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;","[{'comment': ""Please revert changes related to rearranging imports until we don't have determined the correct ordering for all the project."", 'commenter': 'vvysotskyi'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/AirliftBytesInputCompressor.java,"@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.compression;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Stack;
+
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.parquet.bytes.ByteBufferAllocator;
+import org.apache.parquet.bytes.BytesInput;
+import org.apache.parquet.compression.CompressionCodecFactory;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+
+import io.airlift.compress.Compressor;
+
+/**
+ * A shim making an aircompressor compressor available through the BytesInputCompressor
+ * interface.
+ */
+public class AirliftBytesInputCompressor implements CompressionCodecFactory.BytesInputCompressor {
+
+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AirliftBytesInputCompressor.class);","[{'comment': 'Please either use imports instead of specifying the whole pakage here or use Lombok.', 'commenter': 'vvysotskyi'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/AirliftBytesInputCompressor.java,"@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.compression;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Stack;
+
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.parquet.bytes.ByteBufferAllocator;
+import org.apache.parquet.bytes.BytesInput;
+import org.apache.parquet.compression.CompressionCodecFactory;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+
+import io.airlift.compress.Compressor;
+
+/**
+ * A shim making an aircompressor compressor available through the BytesInputCompressor
+ * interface.
+ */
+public class AirliftBytesInputCompressor implements CompressionCodecFactory.BytesInputCompressor {
+
+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AirliftBytesInputCompressor.class);
+
+  // the codec used by this compressor
+  private CompressionCodecName codecName;
+
+  // the relevant aircompressor compressor
+  private Compressor airComp = null;
+
+  // the direct memory allocator to be used during compression
+  private ByteBufferAllocator allocator;
+
+  // stack tracking all direct memory buffers we allocated, and must release
+  private Stack<ByteBuffer> ourAllocations;
+
+  public AirliftBytesInputCompressor(CompressionCodecName codecName, ByteBufferAllocator allocator) {
+    this.codecName = codecName;
+
+    switch (codecName) {
+    case LZ4:
+      airComp = new io.airlift.compress.lz4.Lz4Compressor();","[{'comment': 'Please use imports here and below too.', 'commenter': 'vvysotskyi'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/AirliftBytesInputCompressor.java,"@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.compression;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Stack;
+
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.parquet.bytes.ByteBufferAllocator;
+import org.apache.parquet.bytes.BytesInput;
+import org.apache.parquet.compression.CompressionCodecFactory;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+
+import io.airlift.compress.Compressor;
+
+/**
+ * A shim making an aircompressor compressor available through the BytesInputCompressor
+ * interface.
+ */
+public class AirliftBytesInputCompressor implements CompressionCodecFactory.BytesInputCompressor {
+
+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AirliftBytesInputCompressor.class);
+
+  // the codec used by this compressor
+  private CompressionCodecName codecName;
+
+  // the relevant aircompressor compressor
+  private Compressor airComp = null;
+
+  // the direct memory allocator to be used during compression
+  private ByteBufferAllocator allocator;
+
+  // stack tracking all direct memory buffers we allocated, and must release
+  private Stack<ByteBuffer> ourAllocations;","[{'comment': '```suggestion\r\n  private Stack<ByteBuffer> allocatedBuffers;\r\n```', 'commenter': 'vvysotskyi'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/DrillCompressionCodecFactory.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.compression;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.parquet.bytes.ByteBufferAllocator;
+import org.apache.parquet.compression.CompressionCodecFactory;
+import org.apache.parquet.hadoop.CodecFactory;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+
+/**
+ * A delegating compression codec factory that returns (de)compressors based on
+ * https://github.com/airlift/aircompressor when possible and falls back to
+ * parquet-mr otherwise.  The aircompressor lib was introduced into Drill
+ * because of difficulties encountered with the JNI-based implementations of
+ * lzo, lz4 and zstd in parquet-mr.
+ *
+ * By modifying the constant AIRCOMPRESSOR_CODECS it is possible to choose
+ * which codecs should be routed to which lib.  In addition, this class
+ * implements parquet-mr's CompressionCodecFactory interface meaning that
+ * swapping this factory for e.g. one in parquet-mr will have minimal impact
+ * on the calling code in Parquet reading and writing parts of the Drill code
+ * base.
+ *
+ */
+public class DrillCompressionCodecFactory implements CompressionCodecFactory {
+
+  // The set of codecs to be handled by aircompressor
+  private static final Set<CompressionCodecName> AIRCOMPRESSOR_CODECS = new HashSet<>(
+      Arrays.asList(new CompressionCodecName[] { CompressionCodecName.LZ4, CompressionCodecName.LZO,
+          CompressionCodecName.SNAPPY, CompressionCodecName.ZSTD }));
+
+  // pool of reused aircompressor compressors (parquet-mr's factory has its own)
+  private final Map<CompressionCodecName, BytesInputCompressor> compressors = new HashMap<>();
+
+  // pool of reused aircompressor decompressors (parquet-mr's factory has its own)
+  private final Map<CompressionCodecName, BytesInputDecompressor> decompressors = new HashMap<>();
+
+  // fallback parquet-mr compression codec factory
+  private CompressionCodecFactory parqCodecFactory;
+
+  // direct memory allocator to be used during (de)compression
+  private ByteBufferAllocator allocator;
+
+  // static builder method, solely to mimick the parquet-mr API as closely as possible
+  public static CompressionCodecFactory createDirectCodecFactory(Configuration config, ByteBufferAllocator allocator,
+      int pageSize) {
+    return new DrillCompressionCodecFactory(config, allocator, pageSize);
+  }
+
+  public DrillCompressionCodecFactory(Configuration config, ByteBufferAllocator allocator, int pageSize) {
+    this.allocator = allocator;
+    this.parqCodecFactory = CodecFactory.createDirectCodecFactory(config, allocator, pageSize);
+  }
+
+  @Override
+  public BytesInputCompressor getCompressor(CompressionCodecName codecName) {
+    if (AIRCOMPRESSOR_CODECS.contains(codecName)) {
+      BytesInputCompressor comp = compressors.get(codecName);
+      if (comp == null) {
+        comp = new AirliftBytesInputCompressor(codecName, allocator);
+        compressors.put(codecName, comp);
+      }
+      return comp;","[{'comment': 'Please use the `Map.computeIfAbsent()` method here and below.', 'commenter': 'vvysotskyi'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/DrillCompressionCodecFactory.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.compression;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.parquet.bytes.ByteBufferAllocator;
+import org.apache.parquet.compression.CompressionCodecFactory;
+import org.apache.parquet.hadoop.CodecFactory;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+
+/**
+ * A delegating compression codec factory that returns (de)compressors based on
+ * https://github.com/airlift/aircompressor when possible and falls back to
+ * parquet-mr otherwise.  The aircompressor lib was introduced into Drill
+ * because of difficulties encountered with the JNI-based implementations of
+ * lzo, lz4 and zstd in parquet-mr.
+ *
+ * By modifying the constant AIRCOMPRESSOR_CODECS it is possible to choose
+ * which codecs should be routed to which lib.  In addition, this class
+ * implements parquet-mr's CompressionCodecFactory interface meaning that
+ * swapping this factory for e.g. one in parquet-mr will have minimal impact
+ * on the calling code in Parquet reading and writing parts of the Drill code
+ * base.
+ *
+ */
+public class DrillCompressionCodecFactory implements CompressionCodecFactory {
+
+  // The set of codecs to be handled by aircompressor
+  private static final Set<CompressionCodecName> AIRCOMPRESSOR_CODECS = new HashSet<>(
+      Arrays.asList(new CompressionCodecName[] { CompressionCodecName.LZ4, CompressionCodecName.LZO,","[{'comment': '`new CompressionCodecName[] {}` is redundant here, please remove it and specify a list of codecs for `Arrays.asList()` method.', 'commenter': 'vvysotskyi'}]"
2321,exec/jdbc-all/pom.xml,"@@ -548,7 +548,7 @@
                   This is likely due to you adding new dependencies to a java-exec and not updating the excludes in this module. This is important as it minimizes the size of the dependency of Drill application users.
 
                   </message>
-                  <maxsize>46600000</maxsize>
+                  <maxsize>47200000</maxsize>","[{'comment': 'Are you sure that these newly added libraries should be included in the JDBC jar? I think it is better to exclude them from here...', 'commenter': 'vvysotskyi'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/AirliftBytesInputCompressor.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.compression;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Stack;
+
+import org.apache.parquet.bytes.ByteBufferAllocator;
+import org.apache.parquet.bytes.BytesInput;
+import org.apache.parquet.compression.CompressionCodecFactory;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import io.airlift.compress.Compressor;
+import io.airlift.compress.Decompressor;
+import io.airlift.compress.lz4.Lz4Compressor;
+import io.airlift.compress.lz4.Lz4Decompressor;
+import io.airlift.compress.lzo.LzoCompressor;
+import io.airlift.compress.lzo.LzoDecompressor;
+import io.airlift.compress.snappy.SnappyCompressor;
+import io.airlift.compress.snappy.SnappyDecompressor;
+import io.airlift.compress.zstd.ZstdCompressor;
+import io.airlift.compress.zstd.ZstdDecompressor;
+
+/**
+ * A shim making an aircompressor (de)compressor available through the BytesInputCompressor
+ * and BytesInputDecompressor interfaces.
+ */
+public class AirliftBytesInputCompressor implements CompressionCodecFactory.BytesInputCompressor, CompressionCodecFactory.BytesInputDecompressor {
+  private static final Logger logger = LoggerFactory.getLogger(AirliftBytesInputCompressor.class);
+
+  // name of the codec provided by this compressor
+  private CompressionCodecName codecName;
+
+  // backing aircompressor compressor
+  private Compressor airComp = null;","[{'comment': 'No need to initialize fields with null values.', 'commenter': 'vvysotskyi'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/AirliftBytesInputCompressor.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.compression;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Stack;
+
+import org.apache.parquet.bytes.ByteBufferAllocator;
+import org.apache.parquet.bytes.BytesInput;
+import org.apache.parquet.compression.CompressionCodecFactory;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import io.airlift.compress.Compressor;
+import io.airlift.compress.Decompressor;
+import io.airlift.compress.lz4.Lz4Compressor;
+import io.airlift.compress.lz4.Lz4Decompressor;
+import io.airlift.compress.lzo.LzoCompressor;
+import io.airlift.compress.lzo.LzoDecompressor;
+import io.airlift.compress.snappy.SnappyCompressor;
+import io.airlift.compress.snappy.SnappyDecompressor;
+import io.airlift.compress.zstd.ZstdCompressor;
+import io.airlift.compress.zstd.ZstdDecompressor;
+
+/**
+ * A shim making an aircompressor (de)compressor available through the BytesInputCompressor
+ * and BytesInputDecompressor interfaces.
+ */
+public class AirliftBytesInputCompressor implements CompressionCodecFactory.BytesInputCompressor, CompressionCodecFactory.BytesInputDecompressor {
+  private static final Logger logger = LoggerFactory.getLogger(AirliftBytesInputCompressor.class);
+
+  // name of the codec provided by this compressor
+  private CompressionCodecName codecName;
+
+  // backing aircompressor compressor
+  private Compressor airComp = null;
+
+  // backing aircompressor decompressor
+  private Decompressor airDecomp = null;
+
+  // the direct memory allocator to be used for (de)compression outputs
+  private ByteBufferAllocator allocator;
+
+  // all the direct memory buffers we've allocated, and must release
+  private Stack<ByteBuffer> allocatedBuffers;
+
+  public AirliftBytesInputCompressor(CompressionCodecName codecName, ByteBufferAllocator allocator) {
+    this.codecName = codecName;
+
+    switch (codecName) {
+    case LZ4:
+      airComp = new Lz4Compressor();
+      airDecomp = new Lz4Decompressor();
+      break;
+    case LZO:
+      airComp = new LzoCompressor();
+      airDecomp = new LzoDecompressor();
+      break;
+    case SNAPPY:
+      airComp = new SnappyCompressor();
+      airDecomp = new SnappyDecompressor();
+      break;
+    case ZSTD:
+      airComp = new ZstdCompressor();
+      airDecomp = new ZstdDecompressor();
+      break;
+    default:
+      throw new UnsupportedOperationException(""Parquet compression codec is not supported: "" + codecName);
+    }
+
+    this.allocator = allocator;
+    this.allocatedBuffers = new Stack<>();
+
+    logger.debug(String.format(
+        ""constructed a %s using a backing compressor of %s"",
+        getClass().getName(),
+        airComp.getClass().getName()
+    ));","[{'comment': 'Please use logger formatting instead of `String.format()`, so for the case when log level is above provided, the message wouldn\'t be constructed.\r\n```suggestion\r\n    logger.debug(\r\n        ""constructed a {} using a backing compressor of {}"",\r\n        getClass().getName(),\r\n        airComp.getClass().getName()\r\n    );\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Clever!', 'commenter': 'jnturton'}]"
2321,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/DrillCompressionCodecFactory.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet.compression;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.parquet.bytes.ByteBufferAllocator;
+import org.apache.parquet.compression.CompressionCodecFactory;
+import org.apache.parquet.hadoop.CodecFactory;
+import org.apache.parquet.hadoop.metadata.CompressionCodecName;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * A delegating compression codec factory that returns (de)compressors based on
+ * https://github.com/airlift/aircompressor when possible and falls back to
+ * parquet-mr otherwise.  The aircompressor lib was introduced into Drill
+ * because of difficulties encountered with the JNI-based implementations of
+ * lzo, lz4 and zstd in parquet-mr.
+ *
+ * By modifying the constant AIRCOMPRESSOR_CODECS it is possible to choose
+ * which codecs should be routed to which lib.  In addition, this class
+ * implements parquet-mr's CompressionCodecFactory interface meaning that
+ * swapping this factory for e.g. one in parquet-mr will have minimal impact
+ * on code in Drill relying on a CompressCodecFactory.
+ *
+ */
+public class DrillCompressionCodecFactory implements CompressionCodecFactory {
+  private static final Logger logger = LoggerFactory.getLogger(DrillCompressionCodecFactory.class);
+
+  // The set of codecs to be handled by aircompressor
+  private static final Set<CompressionCodecName> AIRCOMPRESSOR_CODECS = new HashSet<>(
+      Arrays.asList(CompressionCodecName.LZ4, CompressionCodecName.LZO,
+          CompressionCodecName.SNAPPY, CompressionCodecName.ZSTD));
+
+  // pool of reused aircompressor compressors (parquet-mr's factory has its own)
+  private final Map airCompressors = new HashMap();","[{'comment': 'Please use generics and remove unnecessary casts below:\r\n```suggestion\r\n  private final Map<CompressionCodecName, AirliftBytesInputCompressor> airCompressors = new HashMap<>();\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Just one cast remained necessary, because I have a single class implementing both BytesInputCompressor and BytesInputDecompressor ', 'commenter': 'jnturton'}, {'comment': 'If `AirliftBytesInputCompressor` is specified, no cast is required.', 'commenter': 'vvysotskyi'}, {'comment': 'Of course 🤦.', 'commenter': 'jnturton'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/utils/JdbcQueryBuilder.java,"@@ -0,0 +1,127 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.jdbc.utils;
+
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.store.jdbc.JdbcRecordWriter;
+import org.apache.parquet.Strings;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.sql.JDBCType;
+
+public class JdbcQueryBuilder {
+  private static final Logger logger = LoggerFactory.getLogger(JdbcQueryBuilder.class);
+  public static final int DEFAULT_VARCHAR_PRECISION = 100;
+
+  private static final String CREATE_TABLE_QUERY = ""CREATE TABLE %s ("";
+  private final StringBuilder createTableQuery;
+  private SqlDialect dialect;
+  private StringBuilder columns;
+
+  public JdbcQueryBuilder(String tableName, SqlDialect dialect) {
+    if (Strings.isNullOrEmpty(tableName)) {
+      throw new UnsupportedOperationException(""Table name cannot be empty"");
+    }
+    this.dialect = dialect;
+    createTableQuery = new StringBuilder();
+    createTableQuery.append(String.format(CREATE_TABLE_QUERY, tableName));
+    columns = new StringBuilder();
+  }
+
+  /**
+   * Adds a column to the CREATE TABLE statement
+   * @param colName The column to be added to the table
+   * @param type The Drill MinorType of the column
+   * @param nullable If the column is nullable or not.
+   * @param precision The precision, or overall length of a column
+   * @param scale The scale, or number of digits after the decimal
+   */
+  public void addColumn(String colName, MinorType type, boolean nullable, int precision, int scale) {
+    StringBuilder queryText = new StringBuilder();
+    String jdbcColType = """";
+    try {
+      jdbcColType = JDBCType.valueOf(JdbcRecordWriter.JDBC_TYPE_MAPPINGS.get(type)).getName();
+    } catch (NullPointerException e) {
+      // JDBC Does not support writing complex fields to databases
+      throw UserException.dataWriteError()
+        .message(""Drill does not support writing complex fields to JDBC data sources."")
+        .addContext(colName + "" is a complex type."")
+        .build(logger);
+    }
+
+    queryText.append(colName).append("" "").append(jdbcColType);
+
+    // Add precision or scale if applicable
+    if (jdbcColType.equals(""VARCHAR"")) {
+      int max_precision = Math.max(precision, DEFAULT_VARCHAR_PRECISION);
+      queryText.append(""("").append(max_precision).append("")"");
+    }
+
+    if (!nullable) {
+      queryText.append("" NOT NULL"");
+    }
+
+    if (! Strings.isNullOrEmpty(columns.toString())) {
+      columns.append("",\n"");
+    }
+
+    columns.append(queryText);
+  }
+
+  /**
+   * Generates the CREATE TABLE query.
+   * @return The create table query.
+   */
+  public String getCreateTableQuery() {
+    createTableQuery.append(columns);
+    createTableQuery.append(""\n)"");
+    return createTableQuery.toString();
+  }
+
+  @Override
+  public String toString() {
+    return getCreateTableQuery();
+  }
+
+  /**
+   * This function adds the appropriate catalog, schema and table for the FROM clauses for INSERT queries
+   * @param table The table
+   * @param catalog The database catalog
+   * @param schema The database schema
+   * @return The table with catalog and schema added, if present
+   */
+  public static String buildCompleteTableName(String table, String catalog, String schema) {","[{'comment': 'Quite often RDBMSes allow spaces, and possibly other tricky characters, to be used in table names.  Trouble is that they differ in how they want want such identifiers enclosed in those cases e.g. `[spaced out table]` vs ````spaced out table````.  Do we want to raise an error or warning here if e.g. a regexp sees some of these characters?  Otherwise I think later `INSERT` statements could break.', 'commenter': 'jnturton'}, {'comment': ""Thanks for the comment.  I'll add a unit test for this use case. "", 'commenter': 'cgivre'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/utils/JdbcDDLQueryUtils.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.jdbc.utils;
+
+import org.apache.calcite.config.Lex;
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.calcite.sql.SqlNode;
+import org.apache.calcite.sql.parser.SqlParseException;
+import org.apache.calcite.sql.parser.SqlParser;
+import org.apache.calcite.sql.validate.SqlConformanceEnum;
+import org.apache.calcite.sql.parser.ddl.SqlDdlParserImpl;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class JdbcDDLQueryUtils {
+
+  private static final Logger logger = LoggerFactory.getLogger(JdbcDDLQueryUtils.class);
+  /**
+   * Converts a given SQL query from the generic dialect to the destination system dialect.  Returns
+   * null if the original query is not valid.
+   *
+   * @param query An ANSI SQL statement
+   * @param dialect The destination system dialect
+   * @return A representation of the original query in the destination dialect
+   */
+  public static String cleanDDLQuery(String query, SqlDialect dialect) {
+    SqlParser.Config sqlParserConfig = SqlParser.configBuilder()
+      .setParserFactory(SqlDdlParserImpl.FACTORY)
+      .setConformance(SqlConformanceEnum.MYSQL_5)
+      .setCaseSensitive(true)
+      .setLex(Lex.MYSQL_ANSI)
+      .build();
+
+    try {
+      SqlNode node = SqlParser.create(query, sqlParserConfig).parseQuery();
+      String cleanSQL =  node.toSqlString(dialect).getSql();
+
+      // TODO Fix this hack
+      // HACK  See CALCITE-4820 (https://issues.apache.org/jira/browse/CALCITE-4820)","[{'comment': 'Did you see the response from the Calcite team on CALCITE-4820?', 'commenter': 'jnturton'}, {'comment': ""I didn't see the response.  \r\n@vvysotskyi would it be possible to merge https://github.com/apache/calcite/pull/1568 to the Drill calcite?"", 'commenter': 'cgivre'}, {'comment': 'I think yes, it is a quite small fix, so no conflicts should appear.', 'commenter': 'vvysotskyi'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/CapitalizingJdbcSchema.java,"@@ -93,6 +110,65 @@ void setHolder(SchemaPlus plusOfThis) {
     return inner.getTableNames();
   }
 
+
+  @Override
+  public CreateTableEntry createNewTable(String tableName, List<String> partitionColumns, StorageStrategy strategy) {
+    if (! plugin.getConfig().isWritable()) {
+      throw UserException
+        .dataWriteError()
+        .message(plugin.getName() + "" is not writable."")
+        .build(logger);
+    }
+
+    return new CreateTableEntry() {
+
+      @Override
+      public Writer getWriter(PhysicalOperator child) throws IOException {
+        String tableWithSchema = JdbcQueryBuilder.buildCompleteTableName(tableName, catalog, schema);
+        return new JdbcWriter(child, tableWithSchema, inner, plugin);
+      }
+
+      @Override
+      public List<String> getPartitionColumns() {
+        return Collections.emptyList();
+      }
+    };
+  }
+
+  @Override
+  public void dropTable(String tableName) {
+    String tableWithSchema = JdbcQueryBuilder.buildCompleteTableName(tableName, catalog, schema);
+    String dropTableQuery = String.format(""DROP TABLE %s"", tableWithSchema);
+    dropTableQuery = JdbcDDLQueryUtils.cleanDDLQuery(dropTableQuery, plugin.getDialect());
+
+    try {
+      Connection conn = inner.getDataSource().getConnection();","[{'comment': 'Do we need a `finally { conn.close() }` for this?', 'commenter': 'jnturton'}, {'comment': 'Fixed', 'commenter': 'cgivre'}, {'comment': 'Addressed in try-with-resources.', 'commenter': 'cgivre'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcCatalogSchema.java,"@@ -56,7 +56,7 @@
       while (set.next()) {
         final String catalogName = set.getString(1);
         CapitalizingJdbcSchema schema = new CapitalizingJdbcSchema(
-            getSchemaPath(), catalogName, source, dialect, convention, catalogName, null, caseSensitive);
+            getSchemaPath(), catalogName, source, dialect, convention, catalogName, null, caseSensitive, convention.getPlugin());","[{'comment': ""If CapitalizingJdbcSchema can only correctly use the JdbcStoragePlugin returned by `convention.getPlugin()` then I'd consider not adding to the constructor args here, but letting CapitalizingJdbcSchema do the lookup `convention.getPlugin()` itself, in the constructor."", 'commenter': 'jnturton'}]"
2327,contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithPostgres.java,"@@ -0,0 +1,316 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.jdbc;
+
+import org.apache.drill.categories.JdbcStorageTest;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.rowSet.DirectRowSet;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.rowSet.RowSetUtilities;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.testcontainers.containers.PostgreSQLContainer;
+import org.testcontainers.containers.JdbcDatabaseContainer;
+import org.testcontainers.utility.DockerImageName;
+
+import java.math.BigDecimal;
+import java.time.LocalDate;
+import java.util.TimeZone;
+
+import static org.junit.Assert.assertEquals;
+
+/**
+ * JDBC storage plugin tests against Postgres.
+ */
+@Category(JdbcStorageTest.class)
+public class TestJdbcPluginWithPostgres extends ClusterTest {","[{'comment': ""The contents of this class look like they're not writer-related and should have been there before...  was it just something we didn't have?"", 'commenter': 'jnturton'}, {'comment': ""Correct.  This probably should have been a separate PR.  I wanted to test the writer on a few different databases so I figured it made sense to duplicate the reader unit tests.   \r\n\r\nI'd like to add similar tests for MSSQL and Oracle, but getting the testcontainers to run was difficult.  I may do this later in a separate pull request."", 'commenter': 'cgivre'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcRecordWriter.java,"@@ -0,0 +1,859 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.jdbc;
+
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.expr.holders.BitHolder;
+import org.apache.drill.exec.expr.holders.DateHolder;
+import org.apache.drill.exec.expr.holders.Float4Holder;
+import org.apache.drill.exec.expr.holders.Float8Holder;
+import org.apache.drill.exec.expr.holders.IntHolder;
+import org.apache.drill.exec.expr.holders.NullableBigIntHolder;
+import org.apache.drill.exec.expr.holders.NullableBitHolder;
+import org.apache.drill.exec.expr.holders.NullableDateHolder;
+import org.apache.drill.exec.expr.holders.NullableFloat4Holder;
+import org.apache.drill.exec.expr.holders.NullableFloat8Holder;
+import org.apache.drill.exec.expr.holders.NullableIntHolder;
+import org.apache.drill.exec.expr.holders.NullableSmallIntHolder;
+import org.apache.drill.exec.expr.holders.NullableTimeHolder;
+import org.apache.drill.exec.expr.holders.NullableTimeStampHolder;
+import org.apache.drill.exec.expr.holders.NullableTinyIntHolder;
+import org.apache.drill.exec.expr.holders.NullableVarCharHolder;
+import org.apache.drill.exec.expr.holders.NullableVarDecimalHolder;
+import org.apache.drill.exec.expr.holders.SmallIntHolder;
+import org.apache.drill.exec.expr.holders.TimeHolder;
+import org.apache.drill.exec.expr.holders.TimeStampHolder;
+import org.apache.drill.exec.expr.holders.TinyIntHolder;
+import org.apache.drill.exec.expr.holders.VarCharHolder;
+import org.apache.drill.exec.expr.holders.VarDecimalHolder;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.record.VectorAccessible;
+import org.apache.drill.exec.store.AbstractRecordWriter;
+import org.apache.drill.exec.store.EventBasedRecordWriter.FieldConverter;
+import org.apache.drill.exec.store.jdbc.utils.JdbcDDLQueryUtils;
+import org.apache.drill.exec.store.jdbc.utils.JdbcQueryBuilder;
+import org.apache.drill.exec.util.DecimalUtility;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.sql.DataSource;
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.sql.Connection;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.text.Format;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+public class JdbcRecordWriter extends AbstractRecordWriter {
+
+  private static final Logger logger = LoggerFactory.getLogger(JdbcRecordWriter.class);
+  public static final ImmutableMap<MinorType, Integer> JDBC_TYPE_MAPPINGS;
+
+  private static final String INSERT_QUERY_TEMPLATE = ""INSERT INTO %s VALUES\n%s"";
+  private final String tableName;
+  private final Connection connection;
+  private final JdbcWriter config;
+  private final SqlDialect dialect;
+  private final List<Object> rowList;
+  private final List<String> insertRows;
+  private final List<JdbcWriterField> fields;
+  private StringBuilder rowString;
+
+  /*
+   * This map maps JDBC data types to their Drill equivalents.  The basic strategy is that if there
+   * is a Drill equivalent, then do the mapping as expected.
+   *
+   * All flavors of character fields are mapped to VARCHAR in Drill. All versions of binary fields are
+   * mapped to VARBINARY.
+   */
+  static {
+    JDBC_TYPE_MAPPINGS = ImmutableMap.<MinorType, Integer>builder()
+      .put(MinorType.FLOAT8, java.sql.Types.NUMERIC)
+      .put(MinorType.FLOAT4, java.sql.Types.NUMERIC)
+      .put(MinorType.TINYINT, java.sql.Types.TINYINT)
+      .put(MinorType.SMALLINT, java.sql.Types.SMALLINT)
+      .put(MinorType.INT, java.sql.Types.INTEGER)
+      .put(MinorType.BIGINT, java.sql.Types.BIGINT)
+      .put(MinorType.VARCHAR, java.sql.Types.VARCHAR)
+      .put(MinorType.VARBINARY, java.sql.Types.VARBINARY)
+      .put(MinorType.VARDECIMAL, java.sql.Types.DECIMAL)
+      .put(MinorType.DATE, java.sql.Types.DATE)
+      .put(MinorType.TIME, java.sql.Types.TIME)
+      .put(MinorType.TIMESTAMP, java.sql.Types.TIMESTAMP)
+      .put(MinorType.BIT, java.sql.Types.BOOLEAN)
+      .build();
+  }
+
+  public JdbcRecordWriter(DataSource source, OperatorContext context, String name, JdbcWriter config) {
+    this.tableName = name;
+    this.config = config;
+    rowList = new ArrayList<>();
+    insertRows = new ArrayList<>();
+    this.dialect = config.getPlugin().getDialect();
+
+    this.fields = new ArrayList<>();
+
+    try {
+      this.connection = source.getConnection();
+    } catch (SQLException e) {
+      throw UserException.connectionError()
+        .message(""Unable to open JDBC connection for writing."")
+        .addContext(e.getSQLState())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public void init(Map<String, String> writerOptions) {
+
+  }
+
+  @Override
+  public void updateSchema(VectorAccessible batch) {
+    BatchSchema schema = batch.getSchema();
+    String columnName;
+    MinorType type;
+    String sql;
+    Statement statement;
+    boolean nullable = false;
+    JdbcQueryBuilder queryBuilder = new JdbcQueryBuilder(tableName, dialect);
+
+    for (MaterializedField field : schema) {
+      columnName = field.getName();
+      type = field.getType().getMinorType();
+      logger.debug(""Adding column {} of type {}."", columnName, type);
+
+      if (field.getType().getMode() == DataMode.REPEATED) {
+        throw UserException.dataWriteError()
+          .message(""Drill does not yet support writing arrays to JDBC. "" + columnName + "" is an array."")
+          .build(logger);
+      }
+
+      if (field.getType().getMode() == DataMode.OPTIONAL) {
+        nullable = true;
+      }
+
+      int precision = field.getPrecision();
+      int scale = field.getScale();
+
+      queryBuilder.addColumn(columnName, field.getType().getMinorType(), nullable, precision, scale);
+    }
+
+    sql = queryBuilder.getCreateTableQuery();
+    sql = JdbcDDLQueryUtils.cleanDDLQuery(sql, dialect);
+    logger.debug(""Final query: {}"", sql);
+
+    // Execute the query to build the schema
+    try {
+      statement = connection.createStatement();
+      logger.debug(""Executing CREATE query: {}"", sql);
+      statement.execute(sql);
+      statement.close();
+    } catch (SQLException e) {
+      throw UserException.dataReadError(e)
+        .message(""The JDBC storage plugin failed while trying to create the schema. "")
+        .addContext(""Sql"", sql)
+        .build(logger);
+    }
+  }
+
+  @Override
+  public void startRecord() throws IOException {
+    rowString = new StringBuilder();
+    rowList.clear();
+    rowString.append(""("");
+    logger.debug(""Start record"");
+  }
+
+  @Override
+  public void endRecord() throws IOException {
+    logger.debug(""Ending record"");
+
+    // Add values to rowString
+    for (int i = 0; i < rowList.size(); i++) {
+      if (i > 0) {
+        rowString.append("", "");
+      }
+
+      // Add null value to rowstring
+      if (rowList.get(i) instanceof String && ((String) rowList.get(i)).equalsIgnoreCase(""null"")) {
+        rowString.append(""null"");
+        continue;
+      }
+
+      JdbcWriterField currentField = fields.get(i);
+      if (currentField.getDataType() == MinorType.VARCHAR) {
+        String value = null;
+        // Get the string value
+        if (currentField.getMode() == DataMode.REQUIRED) {
+          VarCharHolder varCharHolder = (VarCharHolder) rowList.get(i);
+          value = StringFunctionHelpers.getStringFromVarCharHolder(varCharHolder);
+          // Escape any naughty characters
+          value = JdbcDDLQueryUtils.sqlEscapeString(value);
+        } else {
+          try {
+            NullableVarCharHolder nullableVarCharHolder = (NullableVarCharHolder) rowList.get(i);
+            value = StringFunctionHelpers.getStringFromVarCharHolder(nullableVarCharHolder);
+            value = JdbcDDLQueryUtils.sqlEscapeString(value);
+          } catch (ClassCastException e) {
+            logger.error(""Unable to read field: {}"",  rowList.get(i));
+          }
+        }
+
+        // Add to value string
+        rowString.append(value);
+        //rowString.append(""'"").append(value).append(""'"");
+      } else if (currentField.getDataType() == MinorType.DATE) {
+        String dateString = formatDateForInsertQuery((Long) rowList.get(i));
+        rowString.append(""'"").append(dateString).append(""'"");
+      } else if (currentField.getDataType() == MinorType.TIME) {
+        String timeString = formatTimeForInsertQuery((Integer) rowList.get(i));
+        rowString.append(""'"").append(timeString).append(""'"");
+      } else if (currentField.getDataType() == MinorType.TIMESTAMP) {
+        String timeString = formatTimeStampForInsertQuery((Long) rowList.get(i));
+        rowString.append(""'"").append(timeString).append(""'"");
+      } else {
+        rowString.append(rowList.get(i));
+      }
+    }
+
+    rowString.append("")"");
+    rowList.clear();
+    insertRows.add(rowString.toString());
+    logger.debug(""End record: {}"", rowString.toString());
+  }
+
+  @Override
+  public void abort() throws IOException {
+    logger.debug(""Abort insert."");
+  }
+
+  @Override
+  public void cleanup() throws IOException {
+    logger.debug(""Cleanup record"");
+    // Execute query
+    String insertQuery = buildInsertQuery();
+
+    try {
+      logger.debug(""Executing insert query: {}"", insertQuery);
+      Statement stmt = connection.createStatement();
+      stmt.execute(insertQuery);
+      logger.debug(""Query complete"");
+      // Close connection
+      AutoCloseables.closeSilently(stmt, connection);
+    } catch (SQLException e) {
+      logger.error(""Error: {} "", e.getMessage());
+      throw new IOException();
+    }
+  }
+
+  private String buildInsertQuery() {
+    StringBuilder values = new StringBuilder();
+    for (int i = 0; i < insertRows.size(); i++) {
+      if (i > 0) {
+        values.append("",\n"");
+      }
+      values.append(insertRows.get(i));
+    }
+
+    String sql = String.format(INSERT_QUERY_TEMPLATE, tableName, values);
+    return JdbcDDLQueryUtils.cleanDDLQuery(sql, dialect);
+  }
+
+  private String formatDateForInsertQuery(Long dateVal) {
+    Date date=new Date(dateVal);
+    SimpleDateFormat df2 = new SimpleDateFormat(""yyyy-MM-dd"");
+    return df2.format(date);
+  }
+
+  private String formatTimeForInsertQuery(Integer millis) {
+    return String.format(""%02d:%02d:%02d"", TimeUnit.MILLISECONDS.toHours(millis),
+      TimeUnit.MILLISECONDS.toMinutes(millis) % TimeUnit.HOURS.toMinutes(1),
+      TimeUnit.MILLISECONDS.toSeconds(millis) % TimeUnit.MINUTES.toSeconds(1));
+  }
+
+  private String formatTimeStampForInsertQuery(Long time) {
+    Date date = new Date(time);
+    Format format = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"");
+    return format.format(date);
+  }
+
+  @Override
+  public FieldConverter getNewNullableIntConverter(int fieldId, String fieldName, FieldReader reader) {
+    return new NullableIntJDBCConverter(fieldId, fieldName, reader, fields);
+  }
+
+  public class NullableIntJDBCConverter extends FieldConverter {","[{'comment': ""I guess maybe we could do something with a Freemarker template for the converters but I'm not convinced it's worth it now that we already have these written."", 'commenter': 'jnturton'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcRecordWriter.java,"@@ -0,0 +1,859 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.jdbc;
+
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.expr.holders.BitHolder;
+import org.apache.drill.exec.expr.holders.DateHolder;
+import org.apache.drill.exec.expr.holders.Float4Holder;
+import org.apache.drill.exec.expr.holders.Float8Holder;
+import org.apache.drill.exec.expr.holders.IntHolder;
+import org.apache.drill.exec.expr.holders.NullableBigIntHolder;
+import org.apache.drill.exec.expr.holders.NullableBitHolder;
+import org.apache.drill.exec.expr.holders.NullableDateHolder;
+import org.apache.drill.exec.expr.holders.NullableFloat4Holder;
+import org.apache.drill.exec.expr.holders.NullableFloat8Holder;
+import org.apache.drill.exec.expr.holders.NullableIntHolder;
+import org.apache.drill.exec.expr.holders.NullableSmallIntHolder;
+import org.apache.drill.exec.expr.holders.NullableTimeHolder;
+import org.apache.drill.exec.expr.holders.NullableTimeStampHolder;
+import org.apache.drill.exec.expr.holders.NullableTinyIntHolder;
+import org.apache.drill.exec.expr.holders.NullableVarCharHolder;
+import org.apache.drill.exec.expr.holders.NullableVarDecimalHolder;
+import org.apache.drill.exec.expr.holders.SmallIntHolder;
+import org.apache.drill.exec.expr.holders.TimeHolder;
+import org.apache.drill.exec.expr.holders.TimeStampHolder;
+import org.apache.drill.exec.expr.holders.TinyIntHolder;
+import org.apache.drill.exec.expr.holders.VarCharHolder;
+import org.apache.drill.exec.expr.holders.VarDecimalHolder;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.record.VectorAccessible;
+import org.apache.drill.exec.store.AbstractRecordWriter;
+import org.apache.drill.exec.store.EventBasedRecordWriter.FieldConverter;
+import org.apache.drill.exec.store.jdbc.utils.JdbcDDLQueryUtils;
+import org.apache.drill.exec.store.jdbc.utils.JdbcQueryBuilder;
+import org.apache.drill.exec.util.DecimalUtility;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.sql.DataSource;
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.sql.Connection;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.text.Format;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+public class JdbcRecordWriter extends AbstractRecordWriter {
+
+  private static final Logger logger = LoggerFactory.getLogger(JdbcRecordWriter.class);
+  public static final ImmutableMap<MinorType, Integer> JDBC_TYPE_MAPPINGS;
+
+  private static final String INSERT_QUERY_TEMPLATE = ""INSERT INTO %s VALUES\n%s"";
+  private final String tableName;
+  private final Connection connection;
+  private final JdbcWriter config;
+  private final SqlDialect dialect;
+  private final List<Object> rowList;
+  private final List<String> insertRows;
+  private final List<JdbcWriterField> fields;
+  private StringBuilder rowString;
+
+  /*
+   * This map maps JDBC data types to their Drill equivalents.  The basic strategy is that if there
+   * is a Drill equivalent, then do the mapping as expected.
+   *
+   * All flavors of character fields are mapped to VARCHAR in Drill. All versions of binary fields are
+   * mapped to VARBINARY.
+   */
+  static {
+    JDBC_TYPE_MAPPINGS = ImmutableMap.<MinorType, Integer>builder()
+      .put(MinorType.FLOAT8, java.sql.Types.NUMERIC)
+      .put(MinorType.FLOAT4, java.sql.Types.NUMERIC)
+      .put(MinorType.TINYINT, java.sql.Types.TINYINT)
+      .put(MinorType.SMALLINT, java.sql.Types.SMALLINT)
+      .put(MinorType.INT, java.sql.Types.INTEGER)
+      .put(MinorType.BIGINT, java.sql.Types.BIGINT)
+      .put(MinorType.VARCHAR, java.sql.Types.VARCHAR)
+      .put(MinorType.VARBINARY, java.sql.Types.VARBINARY)
+      .put(MinorType.VARDECIMAL, java.sql.Types.DECIMAL)
+      .put(MinorType.DATE, java.sql.Types.DATE)
+      .put(MinorType.TIME, java.sql.Types.TIME)
+      .put(MinorType.TIMESTAMP, java.sql.Types.TIMESTAMP)
+      .put(MinorType.BIT, java.sql.Types.BOOLEAN)
+      .build();
+  }
+
+  public JdbcRecordWriter(DataSource source, OperatorContext context, String name, JdbcWriter config) {
+    this.tableName = name;
+    this.config = config;
+    rowList = new ArrayList<>();
+    insertRows = new ArrayList<>();
+    this.dialect = config.getPlugin().getDialect();
+
+    this.fields = new ArrayList<>();
+
+    try {
+      this.connection = source.getConnection();
+    } catch (SQLException e) {
+      throw UserException.connectionError()
+        .message(""Unable to open JDBC connection for writing."")
+        .addContext(e.getSQLState())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public void init(Map<String, String> writerOptions) {
+
+  }
+
+  @Override
+  public void updateSchema(VectorAccessible batch) {
+    BatchSchema schema = batch.getSchema();
+    String columnName;
+    MinorType type;
+    String sql;
+    Statement statement;
+    boolean nullable = false;
+    JdbcQueryBuilder queryBuilder = new JdbcQueryBuilder(tableName, dialect);
+
+    for (MaterializedField field : schema) {
+      columnName = field.getName();
+      type = field.getType().getMinorType();
+      logger.debug(""Adding column {} of type {}."", columnName, type);
+
+      if (field.getType().getMode() == DataMode.REPEATED) {
+        throw UserException.dataWriteError()
+          .message(""Drill does not yet support writing arrays to JDBC. "" + columnName + "" is an array."")
+          .build(logger);
+      }
+
+      if (field.getType().getMode() == DataMode.OPTIONAL) {
+        nullable = true;
+      }
+
+      int precision = field.getPrecision();
+      int scale = field.getScale();
+
+      queryBuilder.addColumn(columnName, field.getType().getMinorType(), nullable, precision, scale);
+    }
+
+    sql = queryBuilder.getCreateTableQuery();
+    sql = JdbcDDLQueryUtils.cleanDDLQuery(sql, dialect);
+    logger.debug(""Final query: {}"", sql);
+
+    // Execute the query to build the schema
+    try {
+      statement = connection.createStatement();
+      logger.debug(""Executing CREATE query: {}"", sql);
+      statement.execute(sql);
+      statement.close();
+    } catch (SQLException e) {
+      throw UserException.dataReadError(e)
+        .message(""The JDBC storage plugin failed while trying to create the schema. "")
+        .addContext(""Sql"", sql)
+        .build(logger);
+    }
+  }
+
+  @Override
+  public void startRecord() throws IOException {
+    rowString = new StringBuilder();
+    rowList.clear();
+    rowString.append(""("");
+    logger.debug(""Start record"");
+  }
+
+  @Override
+  public void endRecord() throws IOException {
+    logger.debug(""Ending record"");
+
+    // Add values to rowString
+    for (int i = 0; i < rowList.size(); i++) {
+      if (i > 0) {
+        rowString.append("", "");
+      }
+
+      // Add null value to rowstring
+      if (rowList.get(i) instanceof String && ((String) rowList.get(i)).equalsIgnoreCase(""null"")) {
+        rowString.append(""null"");
+        continue;
+      }
+
+      JdbcWriterField currentField = fields.get(i);
+      if (currentField.getDataType() == MinorType.VARCHAR) {
+        String value = null;
+        // Get the string value
+        if (currentField.getMode() == DataMode.REQUIRED) {
+          VarCharHolder varCharHolder = (VarCharHolder) rowList.get(i);
+          value = StringFunctionHelpers.getStringFromVarCharHolder(varCharHolder);
+          // Escape any naughty characters
+          value = JdbcDDLQueryUtils.sqlEscapeString(value);
+        } else {
+          try {
+            NullableVarCharHolder nullableVarCharHolder = (NullableVarCharHolder) rowList.get(i);
+            value = StringFunctionHelpers.getStringFromVarCharHolder(nullableVarCharHolder);
+            value = JdbcDDLQueryUtils.sqlEscapeString(value);
+          } catch (ClassCastException e) {
+            logger.error(""Unable to read field: {}"",  rowList.get(i));
+          }
+        }
+
+        // Add to value string
+        rowString.append(value);
+        //rowString.append(""'"").append(value).append(""'"");
+      } else if (currentField.getDataType() == MinorType.DATE) {
+        String dateString = formatDateForInsertQuery((Long) rowList.get(i));
+        rowString.append(""'"").append(dateString).append(""'"");
+      } else if (currentField.getDataType() == MinorType.TIME) {
+        String timeString = formatTimeForInsertQuery((Integer) rowList.get(i));
+        rowString.append(""'"").append(timeString).append(""'"");
+      } else if (currentField.getDataType() == MinorType.TIMESTAMP) {
+        String timeString = formatTimeStampForInsertQuery((Long) rowList.get(i));
+        rowString.append(""'"").append(timeString).append(""'"");
+      } else {
+        rowString.append(rowList.get(i));
+      }
+    }
+
+    rowString.append("")"");
+    rowList.clear();
+    insertRows.add(rowString.toString());
+    logger.debug(""End record: {}"", rowString.toString());
+  }
+
+  @Override
+  public void abort() throws IOException {
+    logger.debug(""Abort insert."");
+  }
+
+  @Override
+  public void cleanup() throws IOException {","[{'comment': 'It feels weird that we do the actual inserting in a method called `cleanup` - is this the right place?', 'commenter': 'jnturton'}, {'comment': 'Yeah...the methods really make more sense if you view them as being written for a file-based system.  The `cleanup` function is basically what gets executed at the end. ', 'commenter': 'cgivre'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcRecordWriter.java,"@@ -0,0 +1,859 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.jdbc;
+
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers;
+import org.apache.drill.exec.expr.holders.BigIntHolder;
+import org.apache.drill.exec.expr.holders.BitHolder;
+import org.apache.drill.exec.expr.holders.DateHolder;
+import org.apache.drill.exec.expr.holders.Float4Holder;
+import org.apache.drill.exec.expr.holders.Float8Holder;
+import org.apache.drill.exec.expr.holders.IntHolder;
+import org.apache.drill.exec.expr.holders.NullableBigIntHolder;
+import org.apache.drill.exec.expr.holders.NullableBitHolder;
+import org.apache.drill.exec.expr.holders.NullableDateHolder;
+import org.apache.drill.exec.expr.holders.NullableFloat4Holder;
+import org.apache.drill.exec.expr.holders.NullableFloat8Holder;
+import org.apache.drill.exec.expr.holders.NullableIntHolder;
+import org.apache.drill.exec.expr.holders.NullableSmallIntHolder;
+import org.apache.drill.exec.expr.holders.NullableTimeHolder;
+import org.apache.drill.exec.expr.holders.NullableTimeStampHolder;
+import org.apache.drill.exec.expr.holders.NullableTinyIntHolder;
+import org.apache.drill.exec.expr.holders.NullableVarCharHolder;
+import org.apache.drill.exec.expr.holders.NullableVarDecimalHolder;
+import org.apache.drill.exec.expr.holders.SmallIntHolder;
+import org.apache.drill.exec.expr.holders.TimeHolder;
+import org.apache.drill.exec.expr.holders.TimeStampHolder;
+import org.apache.drill.exec.expr.holders.TinyIntHolder;
+import org.apache.drill.exec.expr.holders.VarCharHolder;
+import org.apache.drill.exec.expr.holders.VarDecimalHolder;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.record.VectorAccessible;
+import org.apache.drill.exec.store.AbstractRecordWriter;
+import org.apache.drill.exec.store.EventBasedRecordWriter.FieldConverter;
+import org.apache.drill.exec.store.jdbc.utils.JdbcDDLQueryUtils;
+import org.apache.drill.exec.store.jdbc.utils.JdbcQueryBuilder;
+import org.apache.drill.exec.util.DecimalUtility;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.sql.DataSource;
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.sql.Connection;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.text.Format;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+public class JdbcRecordWriter extends AbstractRecordWriter {
+
+  private static final Logger logger = LoggerFactory.getLogger(JdbcRecordWriter.class);
+  public static final ImmutableMap<MinorType, Integer> JDBC_TYPE_MAPPINGS;
+
+  private static final String INSERT_QUERY_TEMPLATE = ""INSERT INTO %s VALUES\n%s"";
+  private final String tableName;
+  private final Connection connection;
+  private final JdbcWriter config;
+  private final SqlDialect dialect;
+  private final List<Object> rowList;
+  private final List<String> insertRows;
+  private final List<JdbcWriterField> fields;
+  private StringBuilder rowString;
+
+  /*
+   * This map maps JDBC data types to their Drill equivalents.  The basic strategy is that if there
+   * is a Drill equivalent, then do the mapping as expected.
+   *
+   * All flavors of character fields are mapped to VARCHAR in Drill. All versions of binary fields are
+   * mapped to VARBINARY.
+   */
+  static {
+    JDBC_TYPE_MAPPINGS = ImmutableMap.<MinorType, Integer>builder()
+      .put(MinorType.FLOAT8, java.sql.Types.NUMERIC)
+      .put(MinorType.FLOAT4, java.sql.Types.NUMERIC)
+      .put(MinorType.TINYINT, java.sql.Types.TINYINT)
+      .put(MinorType.SMALLINT, java.sql.Types.SMALLINT)
+      .put(MinorType.INT, java.sql.Types.INTEGER)
+      .put(MinorType.BIGINT, java.sql.Types.BIGINT)
+      .put(MinorType.VARCHAR, java.sql.Types.VARCHAR)
+      .put(MinorType.VARBINARY, java.sql.Types.VARBINARY)
+      .put(MinorType.VARDECIMAL, java.sql.Types.DECIMAL)
+      .put(MinorType.DATE, java.sql.Types.DATE)
+      .put(MinorType.TIME, java.sql.Types.TIME)
+      .put(MinorType.TIMESTAMP, java.sql.Types.TIMESTAMP)
+      .put(MinorType.BIT, java.sql.Types.BOOLEAN)
+      .build();
+  }
+
+  public JdbcRecordWriter(DataSource source, OperatorContext context, String name, JdbcWriter config) {
+    this.tableName = name;
+    this.config = config;
+    rowList = new ArrayList<>();
+    insertRows = new ArrayList<>();
+    this.dialect = config.getPlugin().getDialect();
+
+    this.fields = new ArrayList<>();
+
+    try {
+      this.connection = source.getConnection();
+    } catch (SQLException e) {
+      throw UserException.connectionError()
+        .message(""Unable to open JDBC connection for writing."")
+        .addContext(e.getSQLState())
+        .build(logger);
+    }
+  }
+
+  @Override
+  public void init(Map<String, String> writerOptions) {
+
+  }
+
+  @Override
+  public void updateSchema(VectorAccessible batch) {
+    BatchSchema schema = batch.getSchema();
+    String columnName;
+    MinorType type;
+    String sql;
+    Statement statement;
+    boolean nullable = false;
+    JdbcQueryBuilder queryBuilder = new JdbcQueryBuilder(tableName, dialect);
+
+    for (MaterializedField field : schema) {
+      columnName = field.getName();
+      type = field.getType().getMinorType();
+      logger.debug(""Adding column {} of type {}."", columnName, type);
+
+      if (field.getType().getMode() == DataMode.REPEATED) {
+        throw UserException.dataWriteError()
+          .message(""Drill does not yet support writing arrays to JDBC. "" + columnName + "" is an array."")
+          .build(logger);
+      }
+
+      if (field.getType().getMode() == DataMode.OPTIONAL) {
+        nullable = true;
+      }
+
+      int precision = field.getPrecision();
+      int scale = field.getScale();
+
+      queryBuilder.addColumn(columnName, field.getType().getMinorType(), nullable, precision, scale);
+    }
+
+    sql = queryBuilder.getCreateTableQuery();
+    sql = JdbcDDLQueryUtils.cleanDDLQuery(sql, dialect);
+    logger.debug(""Final query: {}"", sql);
+
+    // Execute the query to build the schema
+    try {
+      statement = connection.createStatement();
+      logger.debug(""Executing CREATE query: {}"", sql);
+      statement.execute(sql);
+      statement.close();
+    } catch (SQLException e) {
+      throw UserException.dataReadError(e)
+        .message(""The JDBC storage plugin failed while trying to create the schema. "")
+        .addContext(""Sql"", sql)
+        .build(logger);
+    }
+  }
+
+  @Override
+  public void startRecord() throws IOException {
+    rowString = new StringBuilder();
+    rowList.clear();
+    rowString.append(""("");
+    logger.debug(""Start record"");
+  }
+
+  @Override
+  public void endRecord() throws IOException {
+    logger.debug(""Ending record"");
+
+    // Add values to rowString
+    for (int i = 0; i < rowList.size(); i++) {
+      if (i > 0) {
+        rowString.append("", "");
+      }
+
+      // Add null value to rowstring
+      if (rowList.get(i) instanceof String && ((String) rowList.get(i)).equalsIgnoreCase(""null"")) {
+        rowString.append(""null"");
+        continue;
+      }
+
+      JdbcWriterField currentField = fields.get(i);
+      if (currentField.getDataType() == MinorType.VARCHAR) {
+        String value = null;
+        // Get the string value
+        if (currentField.getMode() == DataMode.REQUIRED) {
+          VarCharHolder varCharHolder = (VarCharHolder) rowList.get(i);
+          value = StringFunctionHelpers.getStringFromVarCharHolder(varCharHolder);
+          // Escape any naughty characters
+          value = JdbcDDLQueryUtils.sqlEscapeString(value);
+        } else {
+          try {
+            NullableVarCharHolder nullableVarCharHolder = (NullableVarCharHolder) rowList.get(i);
+            value = StringFunctionHelpers.getStringFromVarCharHolder(nullableVarCharHolder);
+            value = JdbcDDLQueryUtils.sqlEscapeString(value);
+          } catch (ClassCastException e) {
+            logger.error(""Unable to read field: {}"",  rowList.get(i));
+          }
+        }
+
+        // Add to value string
+        rowString.append(value);
+        //rowString.append(""'"").append(value).append(""'"");
+      } else if (currentField.getDataType() == MinorType.DATE) {
+        String dateString = formatDateForInsertQuery((Long) rowList.get(i));
+        rowString.append(""'"").append(dateString).append(""'"");
+      } else if (currentField.getDataType() == MinorType.TIME) {
+        String timeString = formatTimeForInsertQuery((Integer) rowList.get(i));
+        rowString.append(""'"").append(timeString).append(""'"");
+      } else if (currentField.getDataType() == MinorType.TIMESTAMP) {
+        String timeString = formatTimeStampForInsertQuery((Long) rowList.get(i));
+        rowString.append(""'"").append(timeString).append(""'"");
+      } else {
+        rowString.append(rowList.get(i));
+      }
+    }
+
+    rowString.append("")"");
+    rowList.clear();
+    insertRows.add(rowString.toString());
+    logger.debug(""End record: {}"", rowString.toString());
+  }
+
+  @Override
+  public void abort() throws IOException {
+    logger.debug(""Abort insert."");
+  }
+
+  @Override
+  public void cleanup() throws IOException {
+    logger.debug(""Cleanup record"");
+    // Execute query
+    String insertQuery = buildInsertQuery();
+
+    try {
+      logger.debug(""Executing insert query: {}"", insertQuery);
+      Statement stmt = connection.createStatement();
+      stmt.execute(insertQuery);
+      logger.debug(""Query complete"");
+      // Close connection
+      AutoCloseables.closeSilently(stmt, connection);
+    } catch (SQLException e) {
+      logger.error(""Error: {} "", e.getMessage());
+      throw new IOException();
+    }
+  }
+
+  private String buildInsertQuery() {","[{'comment': ""I think that the maximum number of records DBMSes allow in a `VALUES` expression is commonly order 1e3 to 1e4.  If Drill batch sizes can exceed that we're going to have a problem.  A possible solution is to always partition into conservative insert batches of, say 500 records.  The `PreparedStatement` and `executeBatch` JDBC API usage in this answer https://stackoverflow.com/a/3786127/1153953 might help to keep things as efficient as possible."", 'commenter': 'jnturton'}, {'comment': '@cgivre did you see this?  Have we tested CTAS statements with 10k, 100k, 1m records?', 'commenter': 'jnturton'}, {'comment': '@dzamo \r\nThis is a good question.  What is supposed to happen is that inserts actually happen in batches.   Any suggestions as to how to test?  Do you think I should just generate a CSV file with 1M records and see what happens?', 'commenter': 'cgivre'}, {'comment': ""I'm still learning about the writer API myself, so I'm figuring this out as we go, but I'm also not quite sure where you control the batch size.  I can see if I can figure that out. "", 'commenter': 'cgivre'}, {'comment': ""@cgivre I think generating a test file of 1m records is a good thing to do at least once.  I don't know much about Drill's batching but I think of it as unrelated to the size limitations of VALUES expressions in external dbs.  If it were me I'd assume Drill could send batches bigger than the target db's VALUES limit and I'd write a loop in JdbcRecordWriter that inserts no more than ~500 records at a time, as outlined in my first comment."", 'commenter': 'jnturton'}]"
2327,.gitignore,"@@ -27,3 +27,4 @@ exec/jdbc-all/dependency-reduced-pom.xml
 .*.html
 venv/
 tools/venv/
+contrib/storage-jdbc/src/test/resources/logback-test.xml","[{'comment': 'Please remove this line, it is not needed.', 'commenter': 'vvysotskyi'}, {'comment': 'Removed', 'commenter': 'cgivre'}]"
2327,contrib/storage-jdbc/pom.xml,"@@ -30,10 +30,18 @@
 
   <name>Drill : Contrib : Storage : JDBC</name>
 
+  <repositories>
+    <repository>
+      <id>jitpack.io</id>
+      <url>https://jitpack.io</url>
+    </repository>
+  </repositories>
+","[{'comment': 'It is declared in root pom, so no need to declare it here too.', 'commenter': 'vvysotskyi'}, {'comment': 'Removed.', 'commenter': 'cgivre'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/CapitalizingJdbcSchema.java,"@@ -93,6 +110,65 @@ void setHolder(SchemaPlus plusOfThis) {
     return inner.getTableNames();
   }
 
+
+  @Override
+  public CreateTableEntry createNewTable(String tableName, List<String> partitionColumns, StorageStrategy strategy) {
+    if (! plugin.getConfig().isWritable()) {
+      throw UserException
+        .dataWriteError()
+        .message(plugin.getName() + "" is not writable."")
+        .build(logger);
+    }
+
+    return new CreateTableEntry() {
+
+      @Override
+      public Writer getWriter(PhysicalOperator child) throws IOException {
+        String tableWithSchema = JdbcQueryBuilder.buildCompleteTableName(tableName, catalog, schema);
+        return new JdbcWriter(child, tableWithSchema, inner, plugin);
+      }
+
+      @Override
+      public List<String> getPartitionColumns() {
+        return Collections.emptyList();
+      }
+    };
+  }
+
+  @Override
+  public void dropTable(String tableName) {
+    String tableWithSchema = JdbcQueryBuilder.buildCompleteTableName(tableName, catalog, schema);
+    String dropTableQuery = String.format(""DROP TABLE %s"", tableWithSchema);
+    dropTableQuery = JdbcDDLQueryUtils.cleanDDLQuery(dropTableQuery, plugin.getDialect());
+
+    try {
+      Connection conn = inner.getDataSource().getConnection();
+      Statement stmt = conn.createStatement();","[{'comment': 'Please wrap it with a try-with-resources.', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.  Here and elsewhere.', 'commenter': 'cgivre'}]"
2327,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcStoragePlugin.java,"@@ -81,7 +80,12 @@ public boolean supportsRead() {
     return true;
   }
 
-  public DataSource getDataSource() {
+  @Override
+  public boolean supportsWrite() {
+    return config.isWritable();
+  }
+
+  public HikariDataSource getDataSource() {","[{'comment': 'Is there any reason for returning `HikariDataSource` here instead of `DataSource`? We should not depend on the specific implementation.', 'commenter': 'vvysotskyi'}, {'comment': 'Nope..  Fixed.', 'commenter': 'cgivre'}]"
2327,docs/dev/CreatingAWriter.md,"@@ -0,0 +1,2 @@
+# Creating a Writer for a Storage Plugin
+This tutorial explains the mostly undocumented features of how to create a writer for a Drill storage plugin.  ","[{'comment': 'Looks like it should be added.', 'commenter': 'vvysotskyi'}, {'comment': ""I'll write up docs for this once the PR is basically ready to go. "", 'commenter': 'cgivre'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixBatchReader.java,"@@ -0,0 +1,370 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.math.BigDecimal;
+import java.sql.Array;
+import java.sql.Connection;
+import java.sql.Date;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.ResultSetMetaData;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.sql.Types;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.impl.scan.framework.SchemaNegotiator;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
+import org.slf4j.LoggerFactory;
+
+public class PhoenixBatchReader implements ManagedReader<SchemaNegotiator> {
+
+  private static final org.slf4j.Logger logger = LoggerFactory.getLogger(PhoenixBatchReader.class);
+
+  private final PhoenixSubScan subScan;
+  private CustomErrorContext errorContext;
+  private PhoenixReader reader;
+  private Connection conn;
+  private PreparedStatement pstmt;
+  private ResultSet rs;
+  private ResultSetMetaData meta;
+  private ColumnDefn[] columns;
+  private Stopwatch watch;
+  private int count = 0;
+
+  public PhoenixBatchReader(PhoenixSubScan subScan) {
+    this.subScan = subScan;
+  }
+
+  @Override
+  public boolean open(SchemaNegotiator negotiator) {
+    try {
+      errorContext = negotiator.parentErrorContext();
+      conn = subScan.getPlugin().getDataSource().getConnection();
+      pstmt = conn.prepareStatement(subScan.getSql());
+      rs = pstmt.executeQuery();
+      meta = pstmt.getMetaData();
+    } catch (SQLException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to execute the phoenix sql query. "" + e.getMessage())
+              .build(logger);
+    }
+    try {
+      negotiator.tableSchema(defineMetadata(), true);
+      reader = new PhoenixReader(negotiator.build());
+      bindColumns(reader.getStorage());
+    } catch (SQLException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to get type of columns from metadata. "" + e.getMessage())
+              .build(logger);
+    }
+    watch = Stopwatch.createStarted();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    try {
+      while (rs.next()) {
+        { // TODO refactor this to PhoenixReader
+          reader.getStorage().start();
+          for (int index = 0; index < columns.length; index++) {
+            if (columns[index].getSqlType() == Types.ARRAY) {
+              Array result = rs.getArray(index + 1);
+              if (result != null) {
+                columns[index].load(result.getArray());
+              }
+            } else {
+              columns[index].load(rs.getObject(index + 1));
+            }
+          }
+          count++;
+          reader.getStorage().save();
+        }
+        if (reader.getStorage().isFull()) { // batch full but not reached the EOF
+          return true;
+        }
+      }
+    } catch (SQLException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to get the data from the result set. "" + e.getMessage())
+              .build(logger);
+    }
+    watch.stop();
+    logger.debug(""Phoenix fetch total record numbers : {}"", count);
+    return false; // the EOF is reached.
+  }
+
+  @Override
+  public void close() {
+    count = reader.getStorage().loader().batchCount();
+    logger.debug(""Phoenix fetch batch size : {}, took {} ms. "", count, watch.elapsed(TimeUnit.MILLISECONDS));
+    AutoCloseables.closeSilently(rs, pstmt, conn);
+  }
+
+  private TupleMetadata defineMetadata() throws SQLException {
+    List<SchemaPath> cols = subScan.getColumns();
+    columns = new ColumnDefn[cols.size()];
+    SchemaBuilder builder = new SchemaBuilder();
+    for (int index = 0; index < cols.size(); index++) {
+      int sqlType = meta.getColumnType(index + 1); // column the first column is 1
+      String columnName = cols.get(index).rootName();
+      columns[index] = makeColumn(columnName, sqlType, meta.getColumnTypeName(index + 1), index);
+      columns[index].define(builder);
+    }
+    return builder.buildSchema();
+  }
+
+  private ColumnDefn makeColumn(String name, int sqlType, String baseType, int index) {
+    if (sqlType == Types.ARRAY) {
+      return new ArrayDefn(name, sqlType, baseType, index);
+    }
+    return new GenericDefn(name, sqlType, index);
+  }
+
+  private void bindColumns(RowSetLoader loader) {
+    for (int i = 0; i < columns.length; i++) {
+      columns[i].bind(loader);
+    }
+  }
+
+  protected static final Map<Integer, MinorType> COLUMN_TYPE_MAP = Maps.newHashMap();
+
+  static {
+    // text
+    COLUMN_TYPE_MAP.put(Types.VARCHAR, MinorType.VARCHAR);
+    COLUMN_TYPE_MAP.put(Types.CHAR, MinorType.VARCHAR);
+    // numbers
+    COLUMN_TYPE_MAP.put(Types.BIGINT, MinorType.BIGINT);
+    COLUMN_TYPE_MAP.put(Types.INTEGER, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.SMALLINT, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.TINYINT, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.DOUBLE, MinorType.FLOAT8);
+    COLUMN_TYPE_MAP.put(Types.FLOAT, MinorType.FLOAT8);
+    COLUMN_TYPE_MAP.put(Types.DECIMAL, MinorType.VARDECIMAL);
+    // time
+    COLUMN_TYPE_MAP.put(Types.DATE, MinorType.DATE);
+    COLUMN_TYPE_MAP.put(Types.TIME, MinorType.TIME);
+    COLUMN_TYPE_MAP.put(Types.TIMESTAMP, MinorType.TIMESTAMP);
+    // binary
+    COLUMN_TYPE_MAP.put(Types.BINARY, MinorType.VARBINARY); // Raw fixed length byte array. Mapped to byte[].
+    COLUMN_TYPE_MAP.put(Types.VARBINARY, MinorType.VARBINARY); // Raw variable length byte array.
+    // boolean
+    COLUMN_TYPE_MAP.put(Types.BOOLEAN, MinorType.BIT);
+  }
+
+  public abstract static class ColumnDefn {
+
+    final String name;
+    final int index;
+    final int sqlType;
+    ColumnWriter writer;
+
+    public String getName() {
+      return name;
+    }
+
+    public int getIndex() {
+      return index;
+    }
+
+    public int getSqlType() {
+      return sqlType;
+    }
+
+    public ColumnDefn(String name, int sqlType, int index) {
+      this.name = name;
+      this.sqlType = sqlType;
+      this.index = index;
+    }
+
+    public void define(SchemaBuilder builder) {
+      builder.addNullable(getName(), COLUMN_TYPE_MAP.get(getSqlType()));
+    }
+
+    public void bind(RowSetLoader loader) {
+      writer = loader.scalar(getName());
+    }
+
+    public abstract void load(Object value);
+  }
+
+  public static class GenericDefn extends ColumnDefn {
+
+    public GenericDefn(String name, int sqlType, int index) {
+      super(name, sqlType, index);
+    }
+
+    @Override
+    public void load(Object value) { // TODO refactor this to AbstractScalarWriter
+      ScalarWriter scalarWriter = (ScalarWriter) writer;
+      switch (getSqlType()) {","[{'comment': ""Suggestion: we're in the innermost loop here: reading every column for every row. The preferred way to handle multiple types is with a class (or lambda) that we can jump to directly without the indirection of a switch. In existing code, we tend to create a class per type. Might be fun to try the more modern approach a lambda that implements `Consumer`. The result is that the virtual function call to `load()` gets you directly to the code that sets the value: no need for a per-column switch statement."", 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixBatchReader.java,"@@ -0,0 +1,370 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.math.BigDecimal;
+import java.sql.Array;
+import java.sql.Connection;
+import java.sql.Date;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.ResultSetMetaData;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.sql.Types;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.impl.scan.framework.SchemaNegotiator;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;
+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
+import org.slf4j.LoggerFactory;
+
+public class PhoenixBatchReader implements ManagedReader<SchemaNegotiator> {
+
+  private static final org.slf4j.Logger logger = LoggerFactory.getLogger(PhoenixBatchReader.class);
+
+  private final PhoenixSubScan subScan;
+  private CustomErrorContext errorContext;
+  private PhoenixReader reader;
+  private Connection conn;
+  private PreparedStatement pstmt;
+  private ResultSet rs;
+  private ResultSetMetaData meta;
+  private ColumnDefn[] columns;
+  private Stopwatch watch;
+  private int count = 0;
+
+  public PhoenixBatchReader(PhoenixSubScan subScan) {
+    this.subScan = subScan;
+  }
+
+  @Override
+  public boolean open(SchemaNegotiator negotiator) {
+    try {
+      errorContext = negotiator.parentErrorContext();
+      conn = subScan.getPlugin().getDataSource().getConnection();
+      pstmt = conn.prepareStatement(subScan.getSql());
+      rs = pstmt.executeQuery();
+      meta = pstmt.getMetaData();
+    } catch (SQLException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to execute the phoenix sql query. "" + e.getMessage())
+              .build(logger);
+    }
+    try {
+      negotiator.tableSchema(defineMetadata(), true);
+      reader = new PhoenixReader(negotiator.build());
+      bindColumns(reader.getStorage());
+    } catch (SQLException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to get type of columns from metadata. "" + e.getMessage())
+              .build(logger);
+    }
+    watch = Stopwatch.createStarted();
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    try {
+      while (rs.next()) {
+        { // TODO refactor this to PhoenixReader
+          reader.getStorage().start();
+          for (int index = 0; index < columns.length; index++) {
+            if (columns[index].getSqlType() == Types.ARRAY) {
+              Array result = rs.getArray(index + 1);
+              if (result != null) {
+                columns[index].load(result.getArray());
+              }
+            } else {
+              columns[index].load(rs.getObject(index + 1));
+            }
+          }
+          count++;
+          reader.getStorage().save();
+        }
+        if (reader.getStorage().isFull()) { // batch full but not reached the EOF
+          return true;
+        }
+      }
+    } catch (SQLException e) {
+      throw UserException
+              .dataReadError(e)
+              .message(""Failed to get the data from the result set. "" + e.getMessage())
+              .build(logger);
+    }
+    watch.stop();
+    logger.debug(""Phoenix fetch total record numbers : {}"", count);
+    return false; // the EOF is reached.
+  }
+
+  @Override
+  public void close() {
+    count = reader.getStorage().loader().batchCount();
+    logger.debug(""Phoenix fetch batch size : {}, took {} ms. "", count, watch.elapsed(TimeUnit.MILLISECONDS));
+    AutoCloseables.closeSilently(rs, pstmt, conn);
+  }
+
+  private TupleMetadata defineMetadata() throws SQLException {
+    List<SchemaPath> cols = subScan.getColumns();
+    columns = new ColumnDefn[cols.size()];
+    SchemaBuilder builder = new SchemaBuilder();
+    for (int index = 0; index < cols.size(); index++) {
+      int sqlType = meta.getColumnType(index + 1); // column the first column is 1
+      String columnName = cols.get(index).rootName();
+      columns[index] = makeColumn(columnName, sqlType, meta.getColumnTypeName(index + 1), index);
+      columns[index].define(builder);
+    }
+    return builder.buildSchema();
+  }
+
+  private ColumnDefn makeColumn(String name, int sqlType, String baseType, int index) {
+    if (sqlType == Types.ARRAY) {
+      return new ArrayDefn(name, sqlType, baseType, index);
+    }
+    return new GenericDefn(name, sqlType, index);
+  }
+
+  private void bindColumns(RowSetLoader loader) {
+    for (int i = 0; i < columns.length; i++) {
+      columns[i].bind(loader);
+    }
+  }
+
+  protected static final Map<Integer, MinorType> COLUMN_TYPE_MAP = Maps.newHashMap();
+
+  static {
+    // text
+    COLUMN_TYPE_MAP.put(Types.VARCHAR, MinorType.VARCHAR);
+    COLUMN_TYPE_MAP.put(Types.CHAR, MinorType.VARCHAR);
+    // numbers
+    COLUMN_TYPE_MAP.put(Types.BIGINT, MinorType.BIGINT);
+    COLUMN_TYPE_MAP.put(Types.INTEGER, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.SMALLINT, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.TINYINT, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.DOUBLE, MinorType.FLOAT8);
+    COLUMN_TYPE_MAP.put(Types.FLOAT, MinorType.FLOAT8);
+    COLUMN_TYPE_MAP.put(Types.DECIMAL, MinorType.VARDECIMAL);
+    // time
+    COLUMN_TYPE_MAP.put(Types.DATE, MinorType.DATE);
+    COLUMN_TYPE_MAP.put(Types.TIME, MinorType.TIME);
+    COLUMN_TYPE_MAP.put(Types.TIMESTAMP, MinorType.TIMESTAMP);
+    // binary
+    COLUMN_TYPE_MAP.put(Types.BINARY, MinorType.VARBINARY); // Raw fixed length byte array. Mapped to byte[].
+    COLUMN_TYPE_MAP.put(Types.VARBINARY, MinorType.VARBINARY); // Raw variable length byte array.
+    // boolean
+    COLUMN_TYPE_MAP.put(Types.BOOLEAN, MinorType.BIT);
+  }
+
+  public abstract static class ColumnDefn {
+
+    final String name;
+    final int index;
+    final int sqlType;
+    ColumnWriter writer;
+
+    public String getName() {
+      return name;
+    }
+
+    public int getIndex() {
+      return index;
+    }
+
+    public int getSqlType() {
+      return sqlType;
+    }
+
+    public ColumnDefn(String name, int sqlType, int index) {
+      this.name = name;
+      this.sqlType = sqlType;
+      this.index = index;
+    }
+
+    public void define(SchemaBuilder builder) {
+      builder.addNullable(getName(), COLUMN_TYPE_MAP.get(getSqlType()));
+    }
+
+    public void bind(RowSetLoader loader) {
+      writer = loader.scalar(getName());
+    }
+
+    public abstract void load(Object value);
+  }
+
+  public static class GenericDefn extends ColumnDefn {
+
+    public GenericDefn(String name, int sqlType, int index) {
+      super(name, sqlType, index);
+    }
+
+    @Override
+    public void load(Object value) { // TODO refactor this to AbstractScalarWriter
+      ScalarWriter scalarWriter = (ScalarWriter) writer;
+      switch (getSqlType()) {
+      case Types.VARCHAR:
+      case Types.CHAR:
+        scalarWriter.setString((String) value);
+        break;
+      case Types.BIGINT :
+        scalarWriter.setLong((Long) value);
+        break;
+      case Types.INTEGER :
+        scalarWriter.setInt((Integer) value);
+        break;
+      case Types.SMALLINT :
+        scalarWriter.setInt((Short) value);
+        break;
+      case Types.TINYINT :
+        scalarWriter.setInt((Byte) value);
+        break;
+      case Types.DOUBLE :
+      case Types.FLOAT :
+        scalarWriter.setDouble((Double) value);
+        break;
+      case Types.DECIMAL :
+        scalarWriter.setDecimal((BigDecimal) value);
+        break;
+      case Types.DATE :
+        scalarWriter.setDate(((Date) value).toLocalDate());
+        break;
+      case Types.TIME :
+        scalarWriter.setTime(((Time) value).toLocalTime());
+        break;
+      case Types.TIMESTAMP :
+        scalarWriter.setTimestamp(((Timestamp) value).toInstant());
+        break;
+      case Types.BINARY :
+      case Types.VARBINARY :
+        byte[] byteValue = (byte[]) value;
+        scalarWriter.setBytes(byteValue, byteValue.length);
+        break;
+      case Types.BOOLEAN :
+        scalarWriter.setBoolean((Boolean) value);
+        break;
+      default:
+        break;
+      }
+    }
+  }
+
+  public static class ArrayDefn extends ColumnDefn {
+
+    final String VARCHAR = ""VARCHAR ARRAY"";
+    final String CHAR = ""CHAR ARRAY"";
+    final String BIGINT = ""BIGINT ARRAY"";
+    final String INTEGER = ""INTEGER ARRAY"";
+    final String DOUBLE = ""DOUBLE ARRAY"";
+    final String FLOAT = ""FLOAT ARRAY"";
+    final String SMALLINT = ""SMALLINT ARRAY"";
+    final String TINYINT = ""TINYINT ARRAY"";
+    final String BOOLEAN = ""BOOLEAN ARRAY"";
+
+    final String baseType;
+
+    public ArrayDefn(String name, int sqlType, String baseType, int index) {
+      super(name, sqlType, index);
+      this.baseType = baseType;
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      switch (baseType) {
+      case VARCHAR:
+      case CHAR:
+        builder.addArray(getName(), MinorType.VARCHAR);
+        break;
+      case BIGINT:
+        builder.addArray(getName(), MinorType.BIGINT);
+        break;
+      case INTEGER:
+        builder.addArray(getName(), MinorType.INT);
+        break;
+      case DOUBLE:
+      case FLOAT:
+        builder.addArray(getName(), MinorType.FLOAT8);
+        break;
+      case SMALLINT:
+        builder.addArray(getName(), MinorType.SMALLINT);
+        break;
+      case TINYINT:
+        builder.addArray(getName(), MinorType.TINYINT);
+        break;
+      case BOOLEAN:
+        builder.addArray(getName(), MinorType.BIT);
+        break;
+      default:
+        break;
+      }
+    }
+
+    @Override
+    public void bind(RowSetLoader loader) {
+      writer = loader.array(getName());
+    }
+
+    @Override
+    public void load(Object value) {","[{'comment': 'As above: try to avoid a switch in the inner-most loop.', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixGroupScan.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.physical.PhysicalOperatorSetupException;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.physical.base.ScanStats.GroupScanProperty;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+@JsonTypeName(""phoenix-scan"")
+public class PhoenixGroupScan extends AbstractGroupScan {
+
+  private final String sql;
+  private final List<SchemaPath> columns;
+  private final PhoenixScanSpec scanSpec;
+  private final double rows;
+  private final ScanStats scanStats;
+  private final PhoenixStoragePlugin plugin;
+
+  private int hashCode;
+
+  @JsonCreator
+  public PhoenixGroupScan(
+      @JsonProperty(""sql"") String sql,
+      @JsonProperty(""columns"") List<SchemaPath> columns,
+      @JsonProperty(""scanSpec"") PhoenixScanSpec scanSpec,
+      @JsonProperty(""rows"") double rows,
+      @JsonProperty(""config"") PhoenixStoragePluginConfig config,
+      @JacksonInject StoragePluginRegistry plugins) {
+    super(""no-user"");
+    this.sql = sql;
+    this.columns = columns;
+    this.scanSpec = scanSpec;
+    this.rows = rows;
+    this.scanStats = computeScanStats();
+    this.plugin = plugins.resolve(config, PhoenixStoragePlugin.class);
+  }
+
+  public PhoenixGroupScan(PhoenixScanSpec scanSpec, PhoenixStoragePlugin plugin) {
+    super(""no-user"");
+    this.sql = scanSpec.getSql();
+    this.columns = ALL_COLUMNS;
+    this.scanSpec = scanSpec;
+    this.rows = 100;","[{'comment': ""The row count is used, I believe, to plan joins. Is 100 a good estimate? A number this lows suggests to Drill that it can ship the results to all nodes as part of a broadcast join. If the actual number of rows is 1M or 100M, that will have turned out to be a poor choice.\r\n\r\nNo good solution here: you actually don't know the number of rows..."", 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixGroupScan.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.physical.PhysicalOperatorSetupException;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.physical.base.ScanStats.GroupScanProperty;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+@JsonTypeName(""phoenix-scan"")
+public class PhoenixGroupScan extends AbstractGroupScan {
+
+  private final String sql;
+  private final List<SchemaPath> columns;
+  private final PhoenixScanSpec scanSpec;
+  private final double rows;
+  private final ScanStats scanStats;
+  private final PhoenixStoragePlugin plugin;
+
+  private int hashCode;
+
+  @JsonCreator
+  public PhoenixGroupScan(
+      @JsonProperty(""sql"") String sql,
+      @JsonProperty(""columns"") List<SchemaPath> columns,
+      @JsonProperty(""scanSpec"") PhoenixScanSpec scanSpec,
+      @JsonProperty(""rows"") double rows,
+      @JsonProperty(""config"") PhoenixStoragePluginConfig config,
+      @JacksonInject StoragePluginRegistry plugins) {
+    super(""no-user"");
+    this.sql = sql;
+    this.columns = columns;
+    this.scanSpec = scanSpec;
+    this.rows = rows;
+    this.scanStats = computeScanStats();
+    this.plugin = plugins.resolve(config, PhoenixStoragePlugin.class);
+  }
+
+  public PhoenixGroupScan(PhoenixScanSpec scanSpec, PhoenixStoragePlugin plugin) {
+    super(""no-user"");
+    this.sql = scanSpec.getSql();
+    this.columns = ALL_COLUMNS;
+    this.scanSpec = scanSpec;
+    this.rows = 100;
+    this.scanStats = computeScanStats();
+    this.plugin = plugin;
+  }
+
+  public PhoenixGroupScan(PhoenixGroupScan scan) {
+    super(scan);
+    this.sql = scan.sql;
+    this.columns = scan.columns;
+    this.scanSpec = scan.scanSpec;
+    this.rows = scan.rows;
+    this.scanStats = scan.scanStats;
+    this.plugin = scan.plugin;
+  }
+
+  public PhoenixGroupScan(PhoenixGroupScan scan, List<SchemaPath> columns) {
+    super(scan);
+    this.sql = scan.sql;
+    this.columns = columns;
+    this.scanSpec = scan.scanSpec;
+    this.rows = scan.rows;
+    this.scanStats = scan.scanStats;
+    this.plugin = scan.plugin;
+  }
+
+  public PhoenixGroupScan(String sql, List<SchemaPath> columns, PhoenixScanSpec scanSpec, double rows, PhoenixStoragePlugin plugin) {
+    super(""no-user"");
+    this.sql = sql;
+    this.columns = columns;
+    this.scanSpec = scanSpec;
+    this.rows = rows;
+    this.scanStats = computeScanStats();
+    this.plugin = plugin;
+  }
+
+  @JsonProperty(""sql"")
+  public String sql() {
+    return sql;
+  }
+
+  @JsonProperty(""columns"")
+  public List<SchemaPath> columns() {
+    return columns;
+  }
+
+  @JsonProperty(""scanSpec"")
+  public PhoenixScanSpec scanSpec() {
+    return scanSpec;
+  }
+
+  @JsonProperty(""rows"")
+  public double rows() {
+    return rows;
+  }
+
+  @JsonProperty(""scanStats"")
+  public ScanStats scanStats() {
+    return scanStats;
+  }
+
+  @JsonIgnore
+  public PhoenixStoragePlugin plugin() {
+    return plugin;
+  }
+
+  @JsonProperty(""config"")
+  public StoragePluginConfig config() {
+    return plugin.getConfig();
+  }
+
+  @Override
+  public void applyAssignments(List<DrillbitEndpoint> endpoints) throws PhysicalOperatorSetupException {  }
+
+  @Override
+  public SubScan getSpecificScan(int minorFragmentId) throws ExecutionSetupException {
+    return new PhoenixSubScan(sql, columns, scanSpec, plugin);
+  }
+
+  @Override
+  public int getMaxParallelizationWidth() {
+    return 1;
+  }
+
+  @Override
+  public String getDigest() {
+    return toString();
+  }
+
+  @Override
+  public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) throws ExecutionSetupException {
+    return new PhoenixGroupScan(this);
+  }
+
+  @Override
+  public ScanStats getScanStats() {
+    return scanStats;
+  }
+
+  @Override
+  public GroupScan clone(List<SchemaPath> columns) {
+    return new PhoenixGroupScan(this, columns);
+  }
+
+  @Override
+  public int hashCode() {
+    if (hashCode == 0) {
+      hashCode = Objects.hash(sql, columns, scanSpec, rows, plugin.getConfig());
+    }
+    return hashCode;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if(this == obj) {","[{'comment': 'Nit: insert space after `if` here and below.', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixScanBatchCreator.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.util.List;
+
+import org.apache.drill.common.exceptions.ChildErrorContext;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.ops.ExecutorFragmentContext;
+import org.apache.drill.exec.physical.impl.BatchCreator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedScanFramework.ReaderFactory;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedScanFramework.ScanFrameworkBuilder;
+import org.apache.drill.exec.physical.impl.scan.framework.SchemaNegotiator;
+import org.apache.drill.exec.record.CloseableRecordBatch;
+import org.apache.drill.exec.record.RecordBatch;
+import org.apache.drill.exec.server.options.OptionManager;
+
+public class PhoenixScanBatchCreator implements BatchCreator<PhoenixSubScan> {
+
+  @Override
+  public CloseableRecordBatch getBatch(ExecutorFragmentContext context, PhoenixSubScan subScan, List<RecordBatch> children) throws ExecutionSetupException {
+    try {
+      ScanFrameworkBuilder builder = createBuilder(context.getOptions(), subScan);
+      return builder.buildScanOperator(context, subScan);
+    } catch (UserException e) {
+      throw e;
+    } catch (Throwable e) {
+      throw new ExecutionSetupException(e);
+    }
+  }
+
+  private ScanFrameworkBuilder createBuilder(OptionManager options, PhoenixSubScan subScan) {
+    ScanFrameworkBuilder builder = new ScanFrameworkBuilder();
+    builder.projection(subScan.getColumns());
+    builder.setUserName(subScan.getUserName());
+    // Phoenix reader
+    ReaderFactory readerFactory = new PhoenixReaderFactory(subScan);
+    builder.setReaderFactory(readerFactory);
+    builder.nullType(Types.optional(MinorType.VARCHAR));
+    // Add custom error context
+    builder.errorContext(new ChildErrorContext(builder.errorContext()) {
+      @Override
+      public void addContext(UserException.Builder builder) {
+        builder.addContext(""Database : "", subScan.getScanSpec().getDbName());
+        builder.addContext(""Table : "", subScan.getScanSpec().getTableName());
+      }
+    });
+
+    return builder;
+  }
+
+  private static class PhoenixReaderFactory implements ReaderFactory {
+
+    private final PhoenixSubScan subScan;
+    private int count = 0;","[{'comment': 'Nit: initializer not needed: default is already 0.', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/rules/PhoenixPrel.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix.rules;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.calcite.adapter.java.JavaTypeFactory;
+import org.apache.calcite.adapter.jdbc.JdbcImplementor;
+import org.apache.calcite.plan.ConventionTraitDef;
+import org.apache.calcite.plan.RelOptCluster;
+import org.apache.calcite.plan.RelTraitSet;
+import org.apache.calcite.rel.AbstractRelNode;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.rel.RelWriter;
+import org.apache.calcite.rel.metadata.RelMetadataQuery;
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.planner.physical.PhysicalPlanCreator;
+import org.apache.drill.exec.planner.physical.Prel;
+import org.apache.drill.exec.planner.physical.visitor.PrelVisitor;
+import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
+import org.apache.drill.exec.store.SubsetRemover;
+import org.apache.drill.exec.store.phoenix.PhoenixGroupScan;
+
+public class PhoenixPrel extends AbstractRelNode implements Prel {
+
+  private final String sql;
+  private final double rows;
+  private final PhoenixConvention convention;
+
+  public PhoenixPrel(RelOptCluster cluster, RelTraitSet traitSet, PhoenixIntermediatePrel prel) {
+    super(cluster, traitSet);
+    final RelNode input = prel.getInput();
+    rows = input.estimateRowCount(cluster.getMetadataQuery());
+    convention = (PhoenixConvention) input.getTraitSet().getTrait(ConventionTraitDef.INSTANCE);
+    final SqlDialect dialect = convention.getPlugin().getDialect();
+    final JdbcImplementor jdbcImplementor = new PhoenixImplementor(dialect, (JavaTypeFactory) getCluster().getTypeFactory());
+    final JdbcImplementor.Result result = jdbcImplementor.visitChild(0, input.accept(SubsetRemover.INSTANCE));
+    sql = result.asStatement().toSqlString(dialect).getSql();
+    rowType = input.getRowType();
+  }
+
+  @Override
+  public Iterator<Prel> iterator() {
+    return Collections.emptyIterator();
+  }
+
+  @Override
+  public PhysicalOperator getPhysicalOperator(PhysicalPlanCreator creator)
+      throws IOException {
+    List<SchemaPath> columns = new ArrayList<SchemaPath>();
+    for (String col : rowType.getFieldNames()) {
+      columns.add(SchemaPath.getSimplePath(col));
+    }
+    PhoenixGroupScan output = new PhoenixGroupScan(sql, columns, null, rows, convention.getPlugin());
+    return creator.addMetadata(this, output);
+  }
+
+  @Override
+  public RelWriter explainTerms(RelWriter pw) {
+    return super.explainTerms(pw).item(""sql"", stripToOneLineSql(sql));
+  }
+
+  private String stripToOneLineSql(String sql) {
+    StringBuilder sbt = new StringBuilder(sql.length());
+    String[] sqlToken = sql.split(""\\n"");
+    for (String sqlText : sqlToken) {
+      if (!sqlText.trim().startsWith(""--"")) {","[{'comment': ""Sadly, the following is also legal (in most SQL):\r\n\r\n```\r\nSELECT a, -- first comment\r\n       b, -- second comment\r\nFROM ...\r\n```\r\n\r\nThe above code will produce:\r\n\r\n```sql\r\nSELECT a, --first comment b -- second comment FROM ...\r\n```\r\n\r\nThis leads to a question: why convert to a single line? Is this to convert SQL to Phoenix? Can't Phoenix parse newlines? (This would be very odd as the SQL standard says newlines are fine whitespace.)\r\n\r\nIf you do have to remove newlines, you may need to have a mini-parser to handle comments and newlines.\r\n\r\n[This page](https://phoenix.apache.org/language/index.html#comments) suggests that Phoenix can handle comments and newlines."", 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/resources/bootstrap-storage-plugins.json,"@@ -0,0 +1,14 @@
+{
+  ""storage"": {
+    ""phoenix"": {
+      ""type"": ""phoenix"",
+      ""jdbcURL"": ""jdbc:phoenix:thin:url=http://the.queryserver.hostname:8765;serialization=PROTOBUF"",
+      ""username"": ""drill"",
+      ""password"": ""drill"",","[{'comment': 'Is this the default out-of-the-box Phoenix password? Probably not. Maybe we should use the default one so a simple install & run works without fiddling?', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/resources/bootstrap-storage-plugins.json,"@@ -0,0 +1,14 @@
+{
+  ""storage"": {
+    ""phoenix"": {
+      ""type"": ""phoenix"",
+      ""jdbcURL"": ""jdbc:phoenix:thin:url=http://the.queryserver.hostname:8765;serialization=PROTOBUF"",","[{'comment': 'Should this point to localhost with the default port? To allow the simplest config to work out-of-the-box?', 'commenter': 'paul-rogers'}, {'comment': 'Just to clarify, can this be:\r\n\r\n```json\r\n      ""jdbcURL"": ""jdbc:phoenix:thin:url=http://localhost:8765;serialization=PROTOBUF"",\r\n```\r\n\r\nActually, let\'s think about this a bit more. You\'re using JDBC with the thin client. You require the `PROTOBUF` serialization. Looks like the config allows a host and port, and the reader code handles that case. So, can the JSON instead be:\r\n\r\n```json\r\n       ""host"": ""localhost"",\r\n       ""port"": 8765,\r\n```\r\n\r\nIt is much more likely people will get the above right: they only have to change the host field. No chance to accidentally change the wrong bits of the URL.\r\n\r\nLeave the URL only for an advanced case.', 'commenter': 'paul-rogers'}]"
2332,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/writer/ScalarArrayWriter.java,"@@ -174,6 +174,14 @@ public void setObject(Object array) {
         setLongObjectArray((Long[]) array);
       } else if (memberClassName.equals(Double.class.getName())) {
         setDoubleObjectArray((Double[]) array);
+      } else if (memberClassName.equals(Float.class.getName())) {
+        setFloatObjectArray((Float[]) array);
+      } else if (memberClassName.equals(Short.class.getName())) {
+        setShortObjectArray((Short[]) array);
+      } else if (memberClassName.equals(Byte.class.getName())) {
+        setByteObjectArray((Byte[]) array);
+      } else if (memberClassName.equals(Boolean.class.getName())) {
+        setBooleanObjectArray((Boolean[]) array);","[{'comment': 'Thanks for adding these missing types! Please add unit tests for these changes. I believe there are tests which exercise this code; you can just add new cases for the new types.', 'commenter': 'paul-rogers'}]"
2332,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/writer/ScalarArrayWriter.java,"@@ -201,12 +220,34 @@ public void setByteArray(byte[] value) {
     }
   }
 
+  public void setByteObjectArray(Byte[] value) {
+    for (int i = 0; i < value.length; i++) {
+      final Byte element = value[i];
+      if (element == null) {
+        elementWriter.setNull();
+      } else {
+        elementWriter.setInt(element);
+      }
+    }
+  }
+
   public void setShortArray(short[] value) {
     for (int i = 0; i < value.length; i++) {
       elementWriter.setInt(value[i]);
     }
   }
 
+  public void setShortObjectArray(Short[] value) {
+    for (int i = 0; i < value.length; i++) {
+      final Short element = value[i];
+      if (element == null) {
+        elementWriter.setNull();","[{'comment': 'Was this tested? As it turns out, Drill does not allow individual array elements to be null, so I believe this call will throw an exception. Drill only allows the entire array to be empty (which Drill treats equivalent to NULL, IIRC.)\r\n\r\nOne way to handle nulls is to replace them with a default value. Since that default value is likely to be datasource-specific, the conversion should be done by the caller.', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixReader.java,"@@ -0,0 +1,463 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.math.BigDecimal;
+import java.sql.Array;
+import java.sql.Date;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.sql.Types;
+import java.util.Arrays;
+import java.util.Map;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
+
+public class PhoenixReader {
+
+  private final RowSetLoader writer;
+  private final ColumnDefn[] columns;
+  private final ResultSet results;
+  private long count;
+
+  public PhoenixReader(ResultSetLoader loader, ColumnDefn[] columns, ResultSet results) {
+    this.writer = loader.writer();
+    this.columns = columns;
+    this.results = results;
+  }
+
+  public RowSetLoader getStorage() {
+    return writer;
+  }
+
+  public long getCount() {
+    return count;
+  }
+
+  /**
+   * Fetch and process one row.
+   * @return return true if one row is processed, return false if there is no next row.
+   * @throws SQLException
+   */
+  public boolean processRow() throws SQLException {
+    if (results.next()) {
+      writer.start();
+      for (int index = 0; index < columns.length; index++) {
+        if (columns[index].getSqlType() == Types.ARRAY) {
+          Array result = results.getArray(index + 1);
+          if (result != null) {
+            columns[index].load(result.getArray());
+          }
+        } else {
+          Object result = results.getObject(index + 1);
+          if (result != null) {
+            columns[index].load(result);
+          }
+        }
+      }
+      count++;
+      writer.save();
+      return true;
+    }
+    return false;
+  }
+
+  protected static final Map<Integer, MinorType> COLUMN_TYPE_MAP = Maps.newHashMap();
+
+  static {
+    // text
+    COLUMN_TYPE_MAP.put(Types.VARCHAR, MinorType.VARCHAR);
+    COLUMN_TYPE_MAP.put(Types.CHAR, MinorType.VARCHAR);
+    // numbers
+    COLUMN_TYPE_MAP.put(Types.BIGINT, MinorType.BIGINT);
+    COLUMN_TYPE_MAP.put(Types.INTEGER, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.SMALLINT, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.TINYINT, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.DOUBLE, MinorType.FLOAT8);
+    COLUMN_TYPE_MAP.put(Types.FLOAT, MinorType.FLOAT4);
+    COLUMN_TYPE_MAP.put(Types.DECIMAL, MinorType.VARDECIMAL);
+    // time
+    COLUMN_TYPE_MAP.put(Types.DATE, MinorType.DATE);
+    COLUMN_TYPE_MAP.put(Types.TIME, MinorType.TIME);
+    COLUMN_TYPE_MAP.put(Types.TIMESTAMP, MinorType.TIMESTAMP);
+    // binary
+    COLUMN_TYPE_MAP.put(Types.BINARY, MinorType.VARBINARY); // Raw fixed length byte array. Mapped to byte[].
+    COLUMN_TYPE_MAP.put(Types.VARBINARY, MinorType.VARBINARY); // Raw variable length byte array.
+    // boolean
+    COLUMN_TYPE_MAP.put(Types.BOOLEAN, MinorType.BIT);
+  }
+
+  protected abstract static class ColumnDefn {
+
+    final String name;
+    final int index;
+    final int sqlType;
+    ColumnWriter writer;
+
+    public String getName() {
+      return name;
+    }
+
+    public int getIndex() {
+      return index;
+    }
+
+    public int getSqlType() {
+      return sqlType;
+    }
+
+    public ColumnDefn(String name, int index, int sqlType) {
+      this.name = name;
+      this.index = index;
+      this.sqlType = sqlType;
+    }
+
+    public void define(SchemaBuilder builder) {
+      builder.addNullable(getName(), COLUMN_TYPE_MAP.get(getSqlType()));
+    }
+
+    public void bind(RowSetLoader loader) {
+      writer = loader.scalar(getName());
+    }
+
+    public abstract void load(Object value);
+  }
+
+  protected static abstract class GenericDefn extends ColumnDefn {
+
+    public GenericDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+  }
+
+  protected static class GenericVarcharDefn extends GenericDefn {
+
+    public GenericVarcharDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setString((String) value);
+    }
+  }
+
+  protected static class GenericBigintDefn extends GenericDefn {
+
+    public GenericBigintDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setLong((Long) value);
+    }
+  }
+
+  protected static class GenericIntegerDefn extends GenericDefn {
+
+    public GenericIntegerDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setInt((Integer) value);
+    }
+  }
+
+  protected static class GenericSmallintDefn extends GenericDefn {
+
+    public GenericSmallintDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setInt((Short) value);
+    }
+  }
+
+  protected static class GenericTinyintDefn extends GenericDefn {
+
+    public GenericTinyintDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setInt((Byte) value);
+    }
+  }
+
+  protected static class GenericDoubleDefn extends GenericDefn {
+
+    public GenericDoubleDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setDouble((Double) value);
+    }
+  }
+
+  protected static class GenericFloatDefn extends GenericDefn {
+
+    public GenericFloatDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setFloat((Float) value);
+    }
+  }
+
+  protected static class GenericDecimalDefn extends GenericDefn {
+
+    public GenericDecimalDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setDecimal((BigDecimal) value);
+    }
+  }
+
+  protected static class GenericDateDefn extends GenericDefn {
+
+    public GenericDateDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setDate(((Date) value).toLocalDate());
+    }
+  }
+
+  protected static class GenericTimeDefn extends GenericDefn {
+
+    public GenericTimeDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setTime(((Time) value).toLocalTime());
+    }
+  }
+
+  protected static class GenericTimestampDefn extends GenericDefn {
+
+    public GenericTimestampDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setTimestamp(((Timestamp) value).toInstant());
+    }
+  }
+
+  protected static class GenericBinaryDefn extends GenericDefn {
+
+    public GenericBinaryDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      byte[] byteValue = (byte[]) value;
+      ((ScalarWriter) writer).setBytes(byteValue, byteValue.length);
+    }
+  }
+
+  protected static class GenericBooleanDefn extends GenericDefn {
+
+    public GenericBooleanDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setBoolean((Boolean) value);
+    }
+  }
+
+  protected static abstract class ArrayDefn extends ColumnDefn {
+
+    static final String VARCHAR = ""VARCHAR ARRAY"";
+    static final String CHAR = ""CHAR ARRAY"";
+    static final String BIGINT = ""BIGINT ARRAY"";
+    static final String INTEGER = ""INTEGER ARRAY"";
+    static final String DOUBLE = ""DOUBLE ARRAY"";
+    static final String FLOAT = ""FLOAT ARRAY"";
+    static final String SMALLINT = ""SMALLINT ARRAY"";
+    static final String TINYINT = ""TINYINT ARRAY"";
+    static final String BOOLEAN = ""BOOLEAN ARRAY"";
+
+    final String baseType;
+
+    public ArrayDefn(String name, int index, int sqlType, String baseType) {
+      super(name, index, sqlType);
+      this.baseType = baseType;
+    }
+
+    @Override
+    public void bind(RowSetLoader loader) {
+      writer = loader.array(getName());
+    }
+  }
+
+  protected static class ArrayVarcharDefn extends ArrayDefn {
+
+    public ArrayVarcharDefn(String name, int index, int sqlType, String baseType) {
+      super(name, index, sqlType, baseType);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      builder.addArray(getName(), MinorType.VARCHAR);
+    }
+
+    @Override
+    public void load(Object value) {
+      Object[] values = (Object[]) value;
+      writer.setObject(Arrays.copyOf(values, values.length, String[].class));
+    }
+  }
+
+  protected static class ArrayBigintDefn extends ArrayDefn {
+
+    public ArrayBigintDefn(String name, int index, int sqlType, String baseType) {
+      super(name, index, sqlType, baseType);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      builder.addArray(getName(), MinorType.BIGINT);
+    }
+
+    @Override
+    public void load(Object value) {
+      Object[] values = (Object[]) value;
+      writer.setObject(Arrays.copyOf(values, values.length, Long[].class));
+    }
+  }
+
+  protected static class ArrayIntegerDefn extends ArrayDefn {
+
+    public ArrayIntegerDefn(String name, int index, int sqlType, String baseType) {
+      super(name, index, sqlType, baseType);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      builder.addArray(getName(), MinorType.INT);
+    }
+
+    @Override
+    public void load(Object value) {
+      Object[] values = (Object[]) value;
+      writer.setObject(Arrays.copyOf(values, values.length, Integer[].class));
+    }
+  }
+
+  protected static class ArraySmallintDefn extends ArrayDefn {
+
+    public ArraySmallintDefn(String name, int index, int sqlType, String baseType) {
+      super(name, index, sqlType, baseType);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      builder.addArray(getName(), MinorType.SMALLINT);
+    }
+
+    @Override
+    public void load(Object value) {
+      Object[] values = (Object[]) value;
+      writer.setObject(Arrays.copyOf(values, values.length, Short[].class));
+    }
+  }
+
+  protected static class ArrayTinyintDefn extends ArrayDefn {
+
+    public ArrayTinyintDefn(String name, int index, int sqlType, String baseType) {
+      super(name, index, sqlType, baseType);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      builder.addArray(getName(), MinorType.TINYINT);
+    }
+
+    @Override
+    public void load(Object value) {
+      Object[] values = (Object[]) value;
+      writer.setObject(Arrays.copyOf(values, values.length, Byte[].class));
+    }
+  }
+
+  protected static class ArrayDoubleDefn extends ArrayDefn {
+
+    public ArrayDoubleDefn(String name, int index, int sqlType, String baseType) {
+      super(name, index, sqlType, baseType);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      builder.addArray(getName(), MinorType.FLOAT8);
+    }
+
+    @Override
+    public void load(Object value) {
+      Object[] values = (Object[]) value;
+      writer.setObject(Arrays.copyOf(values, values.length, Double[].class));
+    }
+  }
+
+  protected static class ArrayBooleanDefn extends ArrayDefn {
+
+    public ArrayBooleanDefn(String name, int index, int sqlType, String baseType) {
+      super(name, index, sqlType, baseType);
+    }
+
+    @Override
+    public void define(SchemaBuilder builder) {
+      builder.addArray(getName(), MinorType.BIT);
+    }
+
+    @Override
+    public void load(Object value) {
+      Object[] values = (Object[]) value;
+      writer.setObject(Arrays.copyOf(values, values.length, Boolean[].class));","[{'comment': ""This is good. We can do better. `setObject()` for an array parses the array type, which is more-or-less the switch statement we wanted to avoid. In order for the parser to work, you had to copy the array, which is a nuisance. So, instead ensure that `writer` is of type `ScalarArrayWriter` and call `setBooleanObjectArray((Boolean[]) value)` instead. (But, it seems that we don't have this method, so it would need to be added...)\r\n\r\nNow, it might be that the cast won't work. So, the next change would be to add a `setObjectArray()` method to `ScalarArrayWriter` something like this:\r\n\r\n```java\r\n  public void setObjectArray(Object[] value) {\r\n    for (int i = 0; i < value.length; i++) {\r\n      final Object element = value[i];\r\n      if (element == null) {\r\n        elementWriter.setNull();\r\n      } else {\r\n        elementWriter.setObject(element);\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\nIn the above, the `setObject()` call will be implemented by the underlying Boolean writer, which will expect the object to be a `Boolean`.\r\n\r\nIf either of these works, apply the same change to the other array classes here."", 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixReader.java,"@@ -0,0 +1,463 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.math.BigDecimal;
+import java.sql.Array;
+import java.sql.Date;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.sql.Types;
+import java.util.Arrays;
+import java.util.Map;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
+
+public class PhoenixReader {
+
+  private final RowSetLoader writer;
+  private final ColumnDefn[] columns;
+  private final ResultSet results;
+  private long count;
+
+  public PhoenixReader(ResultSetLoader loader, ColumnDefn[] columns, ResultSet results) {
+    this.writer = loader.writer();
+    this.columns = columns;
+    this.results = results;
+  }
+
+  public RowSetLoader getStorage() {
+    return writer;
+  }
+
+  public long getCount() {
+    return count;
+  }
+
+  /**
+   * Fetch and process one row.
+   * @return return true if one row is processed, return false if there is no next row.
+   * @throws SQLException
+   */
+  public boolean processRow() throws SQLException {
+    if (results.next()) {
+      writer.start();
+      for (int index = 0; index < columns.length; index++) {
+        if (columns[index].getSqlType() == Types.ARRAY) {
+          Array result = results.getArray(index + 1);
+          if (result != null) {
+            columns[index].load(result.getArray());
+          }
+        } else {
+          Object result = results.getObject(index + 1);
+          if (result != null) {
+            columns[index].load(result);
+          }
+        }
+      }
+      count++;
+      writer.save();
+      return true;
+    }
+    return false;
+  }
+
+  protected static final Map<Integer, MinorType> COLUMN_TYPE_MAP = Maps.newHashMap();
+
+  static {
+    // text
+    COLUMN_TYPE_MAP.put(Types.VARCHAR, MinorType.VARCHAR);
+    COLUMN_TYPE_MAP.put(Types.CHAR, MinorType.VARCHAR);
+    // numbers
+    COLUMN_TYPE_MAP.put(Types.BIGINT, MinorType.BIGINT);
+    COLUMN_TYPE_MAP.put(Types.INTEGER, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.SMALLINT, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.TINYINT, MinorType.INT);
+    COLUMN_TYPE_MAP.put(Types.DOUBLE, MinorType.FLOAT8);
+    COLUMN_TYPE_MAP.put(Types.FLOAT, MinorType.FLOAT4);
+    COLUMN_TYPE_MAP.put(Types.DECIMAL, MinorType.VARDECIMAL);
+    // time
+    COLUMN_TYPE_MAP.put(Types.DATE, MinorType.DATE);
+    COLUMN_TYPE_MAP.put(Types.TIME, MinorType.TIME);
+    COLUMN_TYPE_MAP.put(Types.TIMESTAMP, MinorType.TIMESTAMP);
+    // binary
+    COLUMN_TYPE_MAP.put(Types.BINARY, MinorType.VARBINARY); // Raw fixed length byte array. Mapped to byte[].
+    COLUMN_TYPE_MAP.put(Types.VARBINARY, MinorType.VARBINARY); // Raw variable length byte array.
+    // boolean
+    COLUMN_TYPE_MAP.put(Types.BOOLEAN, MinorType.BIT);
+  }
+
+  protected abstract static class ColumnDefn {
+
+    final String name;
+    final int index;
+    final int sqlType;
+    ColumnWriter writer;
+
+    public String getName() {
+      return name;
+    }
+
+    public int getIndex() {
+      return index;
+    }
+
+    public int getSqlType() {
+      return sqlType;
+    }
+
+    public ColumnDefn(String name, int index, int sqlType) {
+      this.name = name;
+      this.index = index;
+      this.sqlType = sqlType;
+    }
+
+    public void define(SchemaBuilder builder) {
+      builder.addNullable(getName(), COLUMN_TYPE_MAP.get(getSqlType()));
+    }
+
+    public void bind(RowSetLoader loader) {
+      writer = loader.scalar(getName());
+    }
+
+    public abstract void load(Object value);
+  }
+
+  protected static abstract class GenericDefn extends ColumnDefn {
+
+    public GenericDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+  }
+
+  protected static class GenericVarcharDefn extends GenericDefn {
+
+    public GenericVarcharDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setString((String) value);
+    }
+  }
+
+  protected static class GenericBigintDefn extends GenericDefn {
+
+    public GenericBigintDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setLong((Long) value);
+    }
+  }
+
+  protected static class GenericIntegerDefn extends GenericDefn {
+
+    public GenericIntegerDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setInt((Integer) value);
+    }
+  }
+
+  protected static class GenericSmallintDefn extends GenericDefn {
+
+    public GenericSmallintDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setInt((Short) value);
+    }
+  }
+
+  protected static class GenericTinyintDefn extends GenericDefn {
+
+    public GenericTinyintDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setInt((Byte) value);
+    }
+  }
+
+  protected static class GenericDoubleDefn extends GenericDefn {
+
+    public GenericDoubleDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setDouble((Double) value);
+    }
+  }
+
+  protected static class GenericFloatDefn extends GenericDefn {
+
+    public GenericFloatDefn(String name, int index, int sqlType) {
+      super(name, index, sqlType);
+    }
+
+    @Override
+    public void load(Object value) {
+      ((ScalarWriter) writer).setFloat((Float) value);","[{'comment': 'Two minor suggestions. First, maybe have an abstract scalar class to avoid the repeated casts. The scalar abstract class holds the `ScalarWriter` while the array version holds the `ScalarArrayWriter`.\r\n\r\nSecond, for scalars, it is fine to call `setObject()` which will do the cast for you. The scalar writers don\'t do any of the ""object parsing"" logic that arrays do: they expect the object to be of the correct type.\r\n\r\nWhat that, you\'d have:\r\n\r\n```java\r\n    writer.setObject(value)\r\n```\r\n\r\nBecause of that, you can have one class for the types that don\'t need conversion (which seems to be Float, Double, String, etc.) since the implementation would be the same for all of them.', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixReader.java,"@@ -0,0 +1,463 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.math.BigDecimal;
+import java.sql.Array;
+import java.sql.Date;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.sql.Types;
+import java.util.Arrays;
+import java.util.Map;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
+
+public class PhoenixReader {
+
+  private final RowSetLoader writer;
+  private final ColumnDefn[] columns;
+  private final ResultSet results;
+  private long count;
+
+  public PhoenixReader(ResultSetLoader loader, ColumnDefn[] columns, ResultSet results) {
+    this.writer = loader.writer();
+    this.columns = columns;
+    this.results = results;
+  }
+
+  public RowSetLoader getStorage() {
+    return writer;
+  }
+
+  public long getCount() {
+    return count;
+  }
+
+  /**
+   * Fetch and process one row.
+   * @return return true if one row is processed, return false if there is no next row.
+   * @throws SQLException
+   */
+  public boolean processRow() throws SQLException {
+    if (results.next()) {
+      writer.start();
+      for (int index = 0; index < columns.length; index++) {
+        if (columns[index].getSqlType() == Types.ARRAY) {
+          Array result = results.getArray(index + 1);
+          if (result != null) {
+            columns[index].load(result.getArray());
+          }
+        } else {
+          Object result = results.getObject(index + 1);
+          if (result != null) {
+            columns[index].load(result);
+          }
+        }
+      }
+      count++;","[{'comment': 'The result set loader maintains row and batch counts for you, if you need them.', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixReader.java,"@@ -0,0 +1,463 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.math.BigDecimal;
+import java.sql.Array;
+import java.sql.Date;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.sql.Types;
+import java.util.Arrays;
+import java.util.Map;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.vector.accessor.ColumnWriter;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
+
+public class PhoenixReader {
+
+  private final RowSetLoader writer;
+  private final ColumnDefn[] columns;
+  private final ResultSet results;
+  private long count;
+
+  public PhoenixReader(ResultSetLoader loader, ColumnDefn[] columns, ResultSet results) {
+    this.writer = loader.writer();
+    this.columns = columns;
+    this.results = results;
+  }
+
+  public RowSetLoader getStorage() {
+    return writer;
+  }
+
+  public long getCount() {
+    return count;
+  }
+
+  /**
+   * Fetch and process one row.
+   * @return return true if one row is processed, return false if there is no next row.
+   * @throws SQLException
+   */
+  public boolean processRow() throws SQLException {
+    if (results.next()) {
+      writer.start();
+      for (int index = 0; index < columns.length; index++) {
+        if (columns[index].getSqlType() == Types.ARRAY) {","[{'comment': ""Looking better. Even this if-statement can be removed. Each column should know its index. Then, for the generic scalars (see comment below) you could write:\r\n\r\n```java\r\n    public void load(Something results) {\r\n      scalar.setObject(results.getObject(index))\r\n    }\r\n```\r\n\r\nSimilar for the array load.\r\n\r\nNote that the stored index would be for the `result` object to avoid the `+ 1` for every column.\r\n\r\nYour loop would then look like this:\r\n\r\n```java\r\n    for (Something column : columns) {\r\n      column.load(results);\r\n    }\r\n```\r\n\r\nCode can't get much simpler than that."", 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixStoragePluginConfig.java,"@@ -0,0 +1,141 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.Objects;
+
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.logical.AbstractSecuredStoragePluginConfig;
+import org.apache.drill.common.logical.security.CredentialsProvider;
+import org.apache.drill.exec.store.security.CredentialProviderUtils;
+import org.apache.drill.exec.store.security.UsernamePasswordCredentials;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+@JsonTypeName(PhoenixStoragePluginConfig.NAME)
+public class PhoenixStoragePluginConfig extends AbstractSecuredStoragePluginConfig {
+
+  public static final String NAME = ""phoenix"";
+  public static final String THIN_DRIVER_CLASS = ""org.apache.phoenix.queryserver.client.Driver"";
+  public static final String FAT_DRIVER_CLASS = ""org.apache.phoenix.jdbc.PhoenixDriver"";
+
+  private final String host;
+  private final int port;
+  private final String jdbcURL; // (options) Equal to host + port
+  private final Map<String, Object> props; // (options) See also http://phoenix.apache.org/tuning.html
+
+  @JsonCreator
+  public PhoenixStoragePluginConfig(
+      @JsonProperty(""host"") String host,
+      @JsonProperty(""port"") int port,
+      @JsonProperty(""username"") String username,","[{'comment': 'Nit: a quick search of the sources suggests the Drill convention here is `""userName""` (upper case ""N"").', 'commenter': 'paul-rogers'}, {'comment': 'As a side note, What is the difference between `opUserName` and `queryUserName`?\r\n```java\r\nopUserName = scan.getUserName();\r\nqueryUserName = negotiator.context().getFragmentContext().getQueryUserName();\r\n```', 'commenter': 'luocooong'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixStoragePluginConfig.java,"@@ -0,0 +1,141 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.Objects;
+
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.logical.AbstractSecuredStoragePluginConfig;
+import org.apache.drill.common.logical.security.CredentialsProvider;
+import org.apache.drill.exec.store.security.CredentialProviderUtils;
+import org.apache.drill.exec.store.security.UsernamePasswordCredentials;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+@JsonTypeName(PhoenixStoragePluginConfig.NAME)
+public class PhoenixStoragePluginConfig extends AbstractSecuredStoragePluginConfig {
+
+  public static final String NAME = ""phoenix"";
+  public static final String THIN_DRIVER_CLASS = ""org.apache.phoenix.queryserver.client.Driver"";
+  public static final String FAT_DRIVER_CLASS = ""org.apache.phoenix.jdbc.PhoenixDriver"";
+
+  private final String host;
+  private final int port;
+  private final String jdbcURL; // (options) Equal to host + port
+  private final Map<String, Object> props; // (options) See also http://phoenix.apache.org/tuning.html
+
+  @JsonCreator
+  public PhoenixStoragePluginConfig(
+      @JsonProperty(""host"") String host,
+      @JsonProperty(""port"") int port,
+      @JsonProperty(""username"") String username,
+      @JsonProperty(""password"") String password,
+      @JsonProperty(""jdbcURL"") String jdbcURL,
+      @JsonProperty(""credentialsProvider"") CredentialsProvider credentialsProvider,
+      @JsonProperty(""props"") Map<String, Object> props) {
+    super(CredentialProviderUtils.getCredentialsProvider(username, password, credentialsProvider), credentialsProvider == null);
+    this.host = host;
+    this.port = port == 0 ? 8765 : port;
+    this.jdbcURL = jdbcURL;
+    this.props = props == null ? Collections.emptyMap() : props;
+  }
+
+  @JsonIgnore
+  public UsernamePasswordCredentials getUsernamePasswordCredentials() {
+    return new UsernamePasswordCredentials(credentialsProvider);
+  }
+
+  @JsonProperty(""host"")
+  public String getHost() {
+    return host;
+  }
+
+  @JsonProperty(""port"")
+  public int getPort() {
+    return port;
+  }
+
+  @JsonProperty(""username"")
+  public String getUsername() {
+    if (directCredentials) {
+      return getUsernamePasswordCredentials().getUsername();
+    }
+    return null;
+  }
+
+  @JsonIgnore
+  @JsonProperty(""password"")
+  public String getPassword() {
+    if (directCredentials) {
+      return getUsernamePasswordCredentials().getPassword();","[{'comment': 'This can probably just be `getCredentials()` since there are no credentials here other than user name/password.', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixStoragePluginConfig.java,"@@ -0,0 +1,141 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.Objects;
+
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.logical.AbstractSecuredStoragePluginConfig;
+import org.apache.drill.common.logical.security.CredentialsProvider;
+import org.apache.drill.exec.store.security.CredentialProviderUtils;
+import org.apache.drill.exec.store.security.UsernamePasswordCredentials;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+@JsonTypeName(PhoenixStoragePluginConfig.NAME)
+public class PhoenixStoragePluginConfig extends AbstractSecuredStoragePluginConfig {
+
+  public static final String NAME = ""phoenix"";
+  public static final String THIN_DRIVER_CLASS = ""org.apache.phoenix.queryserver.client.Driver"";
+  public static final String FAT_DRIVER_CLASS = ""org.apache.phoenix.jdbc.PhoenixDriver"";
+
+  private final String host;
+  private final int port;
+  private final String jdbcURL; // (options) Equal to host + port
+  private final Map<String, Object> props; // (options) See also http://phoenix.apache.org/tuning.html
+
+  @JsonCreator
+  public PhoenixStoragePluginConfig(
+      @JsonProperty(""host"") String host,
+      @JsonProperty(""port"") int port,
+      @JsonProperty(""username"") String username,
+      @JsonProperty(""password"") String password,
+      @JsonProperty(""jdbcURL"") String jdbcURL,
+      @JsonProperty(""credentialsProvider"") CredentialsProvider credentialsProvider,
+      @JsonProperty(""props"") Map<String, Object> props) {
+    super(CredentialProviderUtils.getCredentialsProvider(username, password, credentialsProvider), credentialsProvider == null);
+    this.host = host;
+    this.port = port == 0 ? 8765 : port;
+    this.jdbcURL = jdbcURL;
+    this.props = props == null ? Collections.emptyMap() : props;
+  }
+
+  @JsonIgnore
+  public UsernamePasswordCredentials getUsernamePasswordCredentials() {
+    return new UsernamePasswordCredentials(credentialsProvider);
+  }
+
+  @JsonProperty(""host"")
+  public String getHost() {
+    return host;
+  }
+
+  @JsonProperty(""port"")
+  public int getPort() {
+    return port;
+  }
+
+  @JsonProperty(""username"")
+  public String getUsername() {
+    if (directCredentials) {
+      return getUsernamePasswordCredentials().getUsername();
+    }
+    return null;
+  }
+
+  @JsonIgnore
+  @JsonProperty(""password"")
+  public String getPassword() {
+    if (directCredentials) {
+      return getUsernamePasswordCredentials().getPassword();
+    }
+    return null;
+  }
+
+  @JsonProperty(""jdbcURL"")
+  public String getJdbcURL() {
+    return jdbcURL;
+  }
+
+  @JsonProperty(""props"")
+  public Map<String, Object> getProps() {
+    return props;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (o == this) {
+      return true;
+    }
+    if (o == null || !(o instanceof PhoenixStoragePluginConfig)) {
+      return false;
+    }
+    PhoenixStoragePluginConfig config = (PhoenixStoragePluginConfig) o;
+    // URL first
+    if (StringUtils.isNotBlank(config.getJdbcURL())) {
+      return Objects.equals(this.jdbcURL, config.getJdbcURL());
+    }
+    // Then the host and port
+    return Objects.equals(this.host, config.getHost()) && Objects.equals(this.port, config.getPort());
+  }
+
+  @Override
+  public int hashCode() {
+    if (StringUtils.isNotBlank(jdbcURL)) {
+     return Objects.hash(jdbcURL);
+    }
+    return Objects.hash(host, port);
+  }
+
+  @Override
+  public String toString() {
+    return new PlanStringBuilder(PhoenixStoragePluginConfig.NAME)
+        .field(""driverName"", THIN_DRIVER_CLASS)
+        .field(""host"", host)
+        .field(""port"", port)
+        .field(""jdbcURL"", jdbcURL)
+        .field(""props"", props)","[{'comment': 'Should this include the user name? And an obfuscated password? That is, show, say, ""*****"" if the password is set. (Show the same number of asterisks regardless of password length.)', 'commenter': 'paul-rogers'}, {'comment': ""Just as an FYSA, if you're using the `PlanStringBuilder`, there is the `maskedField` function which will do that for you. "", 'commenter': 'cgivre'}]"
2332,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixStoragePlugin.java,"@@ -0,0 +1,123 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.phoenix;
+
+import java.io.IOException;
+import java.util.Set;
+
+import javax.sql.DataSource;
+
+import org.apache.calcite.adapter.jdbc.JdbcSchema;
+import org.apache.calcite.plan.RelOptRule;
+import org.apache.calcite.schema.SchemaPlus;
+import org.apache.calcite.sql.SqlDialect;
+import org.apache.calcite.sql.SqlDialectFactoryImpl;
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.JSONOptions;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.ops.OptimizerRulesContext;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.store.AbstractStoragePlugin;
+import org.apache.drill.exec.store.SchemaConfig;
+import org.apache.drill.exec.store.phoenix.rules.PhoenixConvention;
+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
+
+import com.fasterxml.jackson.core.type.TypeReference;
+
+public class PhoenixStoragePlugin extends AbstractStoragePlugin {
+
+  private final PhoenixStoragePluginConfig config;
+  private final DataSource dataSource;
+  private final SqlDialect dialect;
+  private final PhoenixConvention convention;
+  private final PhoenixSchemaFactory schemaFactory;
+
+  public PhoenixStoragePlugin(PhoenixStoragePluginConfig config, DrillbitContext context, String name) {
+    super(context, name);
+    this.config = config;
+    this.dataSource = initNoPoolingDataSource(config);
+    this.dialect = JdbcSchema.createDialect(SqlDialectFactoryImpl.INSTANCE, dataSource);
+    this.convention = new PhoenixConvention(dialect, name, this);
+    this.schemaFactory = new PhoenixSchemaFactory(this);
+  }
+
+  @Override
+  public StoragePluginConfig getConfig() {
+    return config;
+  }
+
+  public DataSource getDataSource() {
+    return dataSource;
+  }
+
+  public SqlDialect getDialect() {
+    return dialect;
+  }
+
+  public PhoenixConvention getConvention() {
+    return convention;
+  }
+
+  @Override
+  public boolean supportsRead() {
+    return true;
+  }
+
+  @Override
+  public Set<? extends RelOptRule> getPhysicalOptimizerRules(OptimizerRulesContext optimizerRulesContext) {
+    return convention.getRules();
+  }
+
+  @Override
+  public void registerSchemas(SchemaConfig schemaConfig, SchemaPlus parent) throws IOException {
+    schemaFactory.registerSchemas(schemaConfig, parent);
+  }
+
+  @Override
+  public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection) throws IOException {
+    PhoenixScanSpec scanSpec = selection.getListWith(context.getLpPersistence().getMapper(), new TypeReference<PhoenixScanSpec>() {});
+    return new PhoenixGroupScan(scanSpec, this);
+  }
+
+  private static DataSource initNoPoolingDataSource(PhoenixStoragePluginConfig config) {
+    // Don't use the pool with the connection
+    PhoenixDataSource dataSource = null;
+    if (StringUtils.isNotBlank(config.getJdbcURL())) {
+      if (!config.getProps().isEmpty()) {
+        dataSource = new PhoenixDataSource(config.getJdbcURL(), config.getProps());
+      } else {
+        dataSource = new PhoenixDataSource(config.getJdbcURL());
+      }
+    } else {
+      if (!config.getProps().isEmpty()) {
+        dataSource = new PhoenixDataSource(config.getHost(), config.getPort(), config.getProps());
+      } else {
+        dataSource = new PhoenixDataSource(config.getHost(), config.getPort());
+      }
+    }","[{'comment': 'Suggestion:\r\n\r\n```java\r\n    Map<String, Object> = config.getProps();\r\n    if (props == null) {\r\n      props = new HashMap<>();\r\n    }\r\n```\r\n\r\nThen, you can eliminate the various props/no props variations, just always pass the (possibly empty) props. Even better, check if the props version accepts a null props value.', 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/src/main/resources/logback-test.xml.bak,"@@ -0,0 +1,49 @@
+<?xml version=""1.0"" encoding=""UTF-8"" ?>
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    ""License""); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+-->
+<configuration>","[{'comment': 'Did you check if this is actually needed? AFAIK, Logback will see all these files on the class path, and will load only one.\r\n\r\nAlso, `logback-test.xml` probably should not be in the `main/resources`, else Logback will load it even in production.\r\n\r\nIf this was copy-pasted from another extension plugin, then that one is probably also wrong.', 'commenter': 'paul-rogers'}, {'comment': 'The original plan was to delete it before merging the PR, but now I have deleted it.', 'commenter': 'luocooong'}]"
2332,contrib/storage-phoenix/src/test/resources/hbase-site.xml,"@@ -0,0 +1,31 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    ""License""); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<configuration>
+  <property>
+    <name>hbase.master.start.timeout.localHBaseCluster</name>
+    <value>60000</value>
+  </property>
+  <property>
+    <name>phoenix.schema.isNamespaceMappingEnabled</name>
+    <value>true</value>
+  </property>
+</configuration>","[{'comment': 'Nit: missing final newline.', 'commenter': 'paul-rogers'}]"
2332,exec/vector/src/main/java/org/apache/drill/exec/vector/accessor/writer/ScalarArrayWriter.java,"@@ -201,12 +218,30 @@ public void setByteArray(byte[] value) {
     }
   }
 
+  public void setByteObjectArray(Byte[] value) {
+    for (int i = 0; i < value.length; i++) {
+      final Byte element = value[i];
+      if (element != null) {
+        elementWriter.setInt(element);
+      }
+    }
+  }
+
   public void setShortArray(short[] value) {
     for (int i = 0; i < value.length; i++) {
       elementWriter.setInt(value[i]);
     }
   }
 
+  public void setShortObjectArray(Short[] value) {
+    for (int i = 0; i < value.length; i++) {
+      final Short element = value[i];
+      if (element != null) {
+        elementWriter.setInt(element);","[{'comment': ""Thanks for fixing this. While you're at it, can you fix the existing `setFooObjectArray()` methods that call `setNull()`? Not sure how they got into the code..."", 'commenter': 'paul-rogers'}]"
2332,contrib/storage-phoenix/README.md,"@@ -0,0 +1,98 @@
+# [DRILL-7863](https://issues.apache.org/jira/browse/DRILL-7863): Add Storage Plugin for Apache Phoenix
+
+## Description
+
+ Phoenix say : ""We put the SQL back in NoSQL"",<br/>
+ Drill call : ""We use the SQL to cross almost all the file systems and storage engines"",<br/>
+ ""Cheers !"", users said.
+
+## Documentation
+
+Features :
+
+ - Full support for Enhanced Vector Framework.
+ 
+ - Tested in phoenix 4.14 and 5.1.2.
+ 
+ - Support the array data type.
+ 
+ - Support the pushdown (Project, Limit, Filter, Aggregate, Join, CrossJoin, Join_Filter, GroupBy, Distinct and more).
+ 
+ - Use the PQS client (6.0).
+
+Related Information :
+
+ 1. PHOENIX-6398: Returns uniform SQL dialect in calcite for the PQS
+
+ 2. PHOENIX-6582: Bump default HBase version to 2.3.7 and 2.4.8
+
+ 3. PHOENIX-6605, PHOENIX-6606 and PHOENIX-6607.
+
+ 4. DRILL-8060, DRILL-8061 and DRILL-8062.
+
+ 5. [QueryServer 6.0.0-drill-r1](https://github.com/luocooong/phoenix-queryserver/releases/tag/6.0.0-drill-r1)
+
+## Testing
+
+ The test framework of phoenix queryserver required the Hadoop 3, but exist `PHOENIX-5993` and `HBASE-22394` :
+
+```
+"" The HBase PMC does not release multiple artifacts for both Hadoop2 and Hadoop3 support at the current time.
+Current HBase2 releases still compile against Hadoop2 by default, and using Hadoop 3 against HBase2
+requires a recompilation of HBase because of incompatible changes between Hadoop2 and Hadoop3. ""
+```
+
+### Recommended Practices
+
+ 1. Download HBase 2.4.2 sources and rebuild with Hadoop 3.
+
+ 2. Remove the `Ignore` annotation in `PhoenixTestSuite.java`.
+
+ 3. Go to the phoenix root folder and run test.
+
+### To Add Features
+
+ - Don't forget to add a test function to the test class.
+ 
+ - If a new test class is added, please declare it in the `PhoenixTestSuite` class.
+
+### Play in CLI
+
+```sql","[{'comment': 'Nit: `text`. All the tables and what-not will throw off the SQL formatter.', 'commenter': 'paul-rogers'}, {'comment': '@paul-rogers Thank you very much. Does it mean that the ""sql"" character can be removed?', 'commenter': 'luocooong'}, {'comment': 'The `sql` keyword id fine.  Using a lot of tables in the docs can throw things off. ', 'commenter': 'cgivre'}]"
2332,contrib/storage-phoenix/README.md,"@@ -0,0 +1,98 @@
+# [DRILL-7863](https://issues.apache.org/jira/browse/DRILL-7863): Add Storage Plugin for Apache Phoenix
+
+## Description
+
+ Phoenix say : ""We put the SQL back in NoSQL"",<br/>
+ Drill call : ""We use the SQL to cross almost all the file systems and storage engines"",<br/>
+ ""Cheers !"", users said.
+
+## Documentation
+","[{'comment': ""@luocooong \r\nThank you so much for this submission.  I don't think I could add much more than @paul-rogers did for the code review.  \r\nI do have some suggestions for the docs however.  Could you please add a section about how to configure Drill to connect to Phoenix and perhaps a demo config?\r\nThanks!"", 'commenter': 'cgivre'}]"
2338,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/AsyncPageReader.java,"@@ -78,209 +79,237 @@
  *
  */
 class AsyncPageReader extends PageReader {
-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AsyncPageReader.class);
+  static final Logger logger = LoggerFactory.getLogger(AsyncPageReader.class);
 
   private ExecutorService threadPool;
   private long queueSize;
   private LinkedBlockingQueue<ReadStatus> pageQueue;
   private ConcurrentLinkedQueue<Future<Void>> asyncPageRead;
   private long totalPageValuesRead = 0;
-  private Object pageQueueSyncronize = new Object(); // Object to use to synchronize access to the page Queue.
+  private final Object pageQueueSyncronize = new Object(); // Object to use to synchronize access to the page Queue.
                                                      // FindBugs complains if we synchronize on a Concurrent Queue
 
-  AsyncPageReader(ColumnReader<?> parentStatus, FileSystem fs, Path path,
-      ColumnChunkMetaData columnChunkMetaData) throws ExecutionSetupException {
-    super(parentStatus, fs, path, columnChunkMetaData);
+  AsyncPageReader(ColumnReader<?> parentStatus, FileSystem fs, Path path) throws ExecutionSetupException {
+    super(parentStatus, fs, path);
     threadPool = parentColumnReader.parentReader.getOperatorContext().getScanExecutor();
     queueSize = parentColumnReader.parentReader.readQueueSize;
     pageQueue = new LinkedBlockingQueue<>((int) queueSize);
     asyncPageRead = new ConcurrentLinkedQueue<>();
   }
 
   @Override
-  protected void loadDictionaryIfExists(final ColumnReader<?> parentStatus,
-      final ColumnChunkMetaData columnChunkMetaData, final DirectBufInputStream f) throws UserException {
-    if (columnChunkMetaData.getDictionaryPageOffset() > 0) {
-      try {
-        assert(columnChunkMetaData.getDictionaryPageOffset() >= dataReader.getPos() );
-        long bytesToSkip = columnChunkMetaData.getDictionaryPageOffset() - dataReader.getPos();
-        while (bytesToSkip > 0) {
-          long skipped = dataReader.skip(bytesToSkip);
-          if (skipped > 0) {
-            bytesToSkip -= skipped;
-          } else {
-            // no good way to handle this. Guava uses InputStream.available to check
-            // if EOF is reached and because available is not reliable,
-            // tries to read the rest of the data.
-            DrillBuf skipBuf = dataReader.getNext((int) bytesToSkip);
-            if (skipBuf != null) {
-              skipBuf.release();
-            } else {
-              throw new EOFException(""End of File reached."");
-            }
-          }
-        }
-      } catch (IOException e) {
-        handleAndThrowException(e, ""Error Reading dictionary page."");
-      }
-    }
-  }
-
-  @Override protected void init() throws IOException {
+  protected void init() throws IOException {
     super.init();
     //Avoid Init if a shutdown is already in progress even if init() is called once
     if (!parentColumnReader.isShuttingDown) {
       asyncPageRead.offer(ExecutorServiceUtil.submit(threadPool, new AsyncPageReaderTask(debugName, pageQueue)));
     }
   }
 
-  private DrillBuf getDecompressedPageData(ReadStatus readStatus) {
-    DrillBuf data;
-    boolean isDictionary = false;
-    synchronized (this) {
-      data = readStatus.getPageData();
-      readStatus.setPageData(null);
-      isDictionary = readStatus.isDictionaryPage;
-    }
-    if (parentColumnReader.columnChunkMetaData.getCodec() != CompressionCodecName.UNCOMPRESSED) {
-      DrillBuf compressedData = data;
-      data = decompress(readStatus.getPageHeader(), compressedData);
-      synchronized (this) {
-        readStatus.setPageData(null);
-      }
-      compressedData.release();
-    } else {
-      if (isDictionary) {
-        stats.totalDictPageReadBytes.addAndGet(readStatus.bytesRead);
-      } else {
-        stats.totalDataPageReadBytes.addAndGet(readStatus.bytesRead);
-      }
-    }
-    return data;
+  /**
+   * Reads and stores this column chunk's dictionary page.
+   * @throws IOException
+   */
+  protected void loadDictionary(ReadStatus readStatus) throws IOException {
+    assert readStatus.isDictionaryPage();
+    assert this.dictionary == null;
+
+    // dictData is not a local because we need to release it later.
+    this.dictData = codecName == CompressionCodecName.UNCOMPRESSED
+      ? readStatus.getPageData()
+      : decompressPageV1(readStatus);
+
+    DictionaryPage page = new DictionaryPage(
+      asBytesInput(dictData, 0, pageHeader.uncompressed_page_size),
+      pageHeader.uncompressed_page_size,
+      pageHeader.dictionary_page_header.num_values,
+      valueOf(pageHeader.dictionary_page_header.encoding.name())
+    );
+
+    this.dictionary = page.getEncoding().initDictionary(columnDescriptor, page);
   }
 
-  // Read and decode the dictionary data
-  private void readDictionaryPageData(final ReadStatus readStatus, final ColumnReader<?> parentStatus)
-      throws UserException {
+  /**
+   * Reads a compressed v1 data page or a dictionary page, both of which are compressed
+   * in their entirety.
+   * @return decompressed Parquet page data
+   * @throws IOException
+   */
+  protected DrillBuf decompressPageV1(ReadStatus readStatus) throws IOException {
+    Stopwatch timer = Stopwatch.createUnstarted();
+
+    PageHeader pageHeader = readStatus.getPageHeader();
+    int inputSize = pageHeader.getCompressed_page_size();
+    int outputSize = pageHeader.getUncompressed_page_size();
+    // TODO: does reporting this number have the same meaning in an async context?
+    long start = dataReader.getPos();
+    long timeToRead;
+
+    DrillBuf inputPageData = readStatus.getPageData();
+    DrillBuf outputPageData = this.allocator.buffer(outputSize);
+
     try {
-      pageHeader = readStatus.getPageHeader();
-      int uncompressedSize = pageHeader.getUncompressed_page_size();
-      final DrillBuf dictionaryData = getDecompressedPageData(readStatus);
-      Stopwatch timer = Stopwatch.createStarted();
-      allocatedDictionaryBuffers.add(dictionaryData);
-      DictionaryPage page = new DictionaryPage(asBytesInput(dictionaryData, 0, uncompressedSize),
-          pageHeader.uncompressed_page_size, pageHeader.dictionary_page_header.num_values,
-          valueOf(pageHeader.dictionary_page_header.encoding.name()));
-      this.dictionary = page.getEncoding().initDictionary(parentStatus.columnDescriptor, page);
-      long timeToDecode = timer.elapsed(TimeUnit.NANOSECONDS);
-      stats.timeDictPageDecode.addAndGet(timeToDecode);
-    } catch (Exception e) {
-      handleAndThrowException(e, ""Error decoding dictionary page."");
+      timer.start();
+      CompressionCodecName codecName = columnChunkMetaData.getCodec();
+      CompressionCodecFactory.BytesInputDecompressor decomp = codecFactory.getDecompressor(codecName);
+      ByteBuffer input = inputPageData.nioBuffer(0, inputSize);
+      ByteBuffer output = outputPageData.nioBuffer(0, outputSize);
+
+      decomp.decompress(input, inputSize, output, outputSize);
+      outputPageData.writerIndex(outputSize);
+      timeToRead = timer.elapsed(TimeUnit.NANOSECONDS);
+
+      logger.trace(
+        ""Col: {}  readPos: {}  Uncompressed_size: {}  pageData: {}"",
+        columnChunkMetaData.toString(),
+        dataReader.getPos(), // TODO: see comment on earlier call to getPos()
+        outputSize,
+        ByteBufUtil.hexDump(outputPageData)
+      );
+
+      this.updateStats(pageHeader, ""Decompress"", start, timeToRead, inputSize, outputSize);
+    } finally {
+      readStatus.setPageData(null);
+      if (inputPageData != null) {
+        inputPageData.release();
+      }
     }
-  }
 
-  private void handleAndThrowException(Exception e, String msg) throws UserException {
-    UserException ex = UserException.dataReadError(e).message(msg)
-        .pushContext(""Row Group Start: "", this.parentColumnReader.columnChunkMetaData.getStartingPos())
-        .pushContext(""Column: "", this.parentColumnReader.schemaElement.getName())
-        .pushContext(""File: "", this.fileName).build(logger);
-    throw ex;
+    return outputPageData;
   }
 
-  private DrillBuf decompress(PageHeader pageHeader, DrillBuf compressedData) {
-    DrillBuf pageDataBuf = null;
+  /**
+   * Reads a compressed v2 data page which excluded the repetition and definition level
+   * sections from compression.
+   * @return decompressed Parquet page data
+   * @throws IOException
+   */
+  protected DrillBuf decompressPageV2(ReadStatus readStatus) throws IOException {
     Stopwatch timer = Stopwatch.createUnstarted();
+
+    PageHeader pageHeader = readStatus.getPageHeader();
+    int inputSize = pageHeader.getCompressed_page_size();
+    int repLevelSize = pageHeader.data_page_header_v2.getRepetition_levels_byte_length();
+    int defLevelSize = pageHeader.data_page_header_v2.getDefinition_levels_byte_length();
+    int compDataOffset = repLevelSize + defLevelSize;
+    int outputSize = pageHeader.uncompressed_page_size;
+    // TODO: does reporting this number have the same meaning in an async context?
+    long start = dataReader.getPos();
     long timeToRead;
-    int compressedSize = pageHeader.getCompressed_page_size();
-    int uncompressedSize = pageHeader.getUncompressed_page_size();
-    pageDataBuf = allocateTemporaryBuffer(uncompressedSize);
+
+    DrillBuf inputPageData = readStatus.getPageData();
+    DrillBuf outputPageData = this.allocator.buffer(outputSize);
+
     try {
       timer.start();
+      // Write out the uncompressed section
+      // Note that the following setBytes call to read the repetition and definition level sections
+      // advances readerIndex in inputPageData but not writerIndex in outputPageData.
+      outputPageData.setBytes(0, inputPageData, compDataOffset);
+
+      // decompress from the start of compressed data to the end of the input buffer
+      CompressionCodecName codecName = columnChunkMetaData.getCodec();
+      CompressionCodecFactory.BytesInputDecompressor decomp = codecFactory.getDecompressor(codecName);
+      ByteBuffer input = inputPageData.nioBuffer(compDataOffset, inputSize - compDataOffset);
+      ByteBuffer output = outputPageData.nioBuffer(compDataOffset, outputSize - compDataOffset);
+      decomp.decompress(
+        input,
+        inputSize - compDataOffset,
+        output,
+        outputSize - compDataOffset
+      );
+      outputPageData.writerIndex(outputSize);
+      timeToRead = timer.elapsed(TimeUnit.NANOSECONDS);
 
-      CompressionCodecName codecName = parentColumnReader.columnChunkMetaData.getCodec();
-      BytesInputDecompressor decomp = codecFactory.getDecompressor(codecName);
-      ByteBuffer input = compressedData.nioBuffer(0, compressedSize);
-      ByteBuffer output = pageDataBuf.nioBuffer(0, uncompressedSize);
+      logger.trace(
+        ""Col: {}  readPos: {}  Uncompressed_size: {}  pageData: {}"",
+        columnChunkMetaData.toString(),
+        dataReader.getPos(), // TODO: see comment on earlier call to getPos()
+        outputSize,
+        ByteBufUtil.hexDump(outputPageData)","[{'comment': 'This method will be called regardless of the logging level...', 'commenter': 'vvysotskyi'}]"
2338,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ColumnReader.java,"@@ -29,14 +33,27 @@
 import org.apache.drill.exec.vector.BaseDataValueVector;
 import org.apache.drill.exec.vector.ValueVector;
 
+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
 import org.apache.parquet.column.ColumnDescriptor;
+import org.apache.parquet.column.Encoding;
 import org.apache.parquet.format.SchemaElement;
 import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;
 
 public abstract class ColumnReader<V extends ValueVector> {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ColumnReader.class);
+  public static final Set<Encoding> DICTIONARY_ENCODINGS = new HashSet<>(Arrays.asList(
+    Encoding.PLAIN_DICTIONARY,
+    Encoding.RLE_DICTIONARY
+  ));","[{'comment': 'You can use `ImmutableSet` here, since it is public:\r\n```suggestion\r\n  public static final Set<Encoding> DICTIONARY_ENCODINGS = ImmutableSet.of(\r\n    Encoding.PLAIN_DICTIONARY,\r\n    Encoding.RLE_DICTIONARY\r\n  );\r\n```', 'commenter': 'vvysotskyi'}]"
2338,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ColumnReader.java,"@@ -29,14 +33,27 @@
 import org.apache.drill.exec.vector.BaseDataValueVector;
 import org.apache.drill.exec.vector.ValueVector;
 
+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;
 import org.apache.parquet.column.ColumnDescriptor;
+import org.apache.parquet.column.Encoding;
 import org.apache.parquet.format.SchemaElement;
 import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;
 
 public abstract class ColumnReader<V extends ValueVector> {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ColumnReader.class);
+  public static final Set<Encoding> DICTIONARY_ENCODINGS = new HashSet<>(Arrays.asList(
+    Encoding.PLAIN_DICTIONARY,
+    Encoding.RLE_DICTIONARY
+  ));
+  public static final Set<Encoding> VALUE_ENCODINGS = Sets.union(
+    DICTIONARY_ENCODINGS,
+    new HashSet<>(Arrays.asList(
+      Encoding.DELTA_BINARY_PACKED,
+      Encoding.DELTA_BYTE_ARRAY,
+      Encoding.DELTA_LENGTH_BYTE_ARRAY
+  )));","[{'comment': 'And here the same:\r\n```suggestion\r\n  public static final Set<Encoding> VALUE_ENCODINGS = ImmutableSet.<Encoding>builder()\r\n    .addAll(DICTIONARY_ENCODINGS)\r\n    .add(Encoding.DELTA_BINARY_PACKED)\r\n    .add(Encoding.DELTA_BYTE_ARRAY)\r\n    .add(Encoding.DELTA_LENGTH_BYTE_ARRAY)\r\n    .build();\r\n```', 'commenter': 'vvysotskyi'}]"
2338,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/AsyncPageReader.java,"@@ -78,209 +79,237 @@
  *
  */
 class AsyncPageReader extends PageReader {
-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AsyncPageReader.class);
+  static final Logger logger = LoggerFactory.getLogger(AsyncPageReader.class);
 
   private ExecutorService threadPool;
   private long queueSize;
   private LinkedBlockingQueue<ReadStatus> pageQueue;
   private ConcurrentLinkedQueue<Future<Void>> asyncPageRead;
   private long totalPageValuesRead = 0;
-  private Object pageQueueSyncronize = new Object(); // Object to use to synchronize access to the page Queue.
+  private final Object pageQueueSyncronize = new Object(); // Object to use to synchronize access to the page Queue.
                                                      // FindBugs complains if we synchronize on a Concurrent Queue
 
-  AsyncPageReader(ColumnReader<?> parentStatus, FileSystem fs, Path path,
-      ColumnChunkMetaData columnChunkMetaData) throws ExecutionSetupException {
-    super(parentStatus, fs, path, columnChunkMetaData);
+  AsyncPageReader(ColumnReader<?> parentStatus, FileSystem fs, Path path) throws ExecutionSetupException {
+    super(parentStatus, fs, path);
     threadPool = parentColumnReader.parentReader.getOperatorContext().getScanExecutor();
     queueSize = parentColumnReader.parentReader.readQueueSize;
     pageQueue = new LinkedBlockingQueue<>((int) queueSize);
     asyncPageRead = new ConcurrentLinkedQueue<>();
   }
 
   @Override
-  protected void loadDictionaryIfExists(final ColumnReader<?> parentStatus,
-      final ColumnChunkMetaData columnChunkMetaData, final DirectBufInputStream f) throws UserException {
-    if (columnChunkMetaData.getDictionaryPageOffset() > 0) {
-      try {
-        assert(columnChunkMetaData.getDictionaryPageOffset() >= dataReader.getPos() );
-        long bytesToSkip = columnChunkMetaData.getDictionaryPageOffset() - dataReader.getPos();
-        while (bytesToSkip > 0) {
-          long skipped = dataReader.skip(bytesToSkip);
-          if (skipped > 0) {
-            bytesToSkip -= skipped;
-          } else {
-            // no good way to handle this. Guava uses InputStream.available to check
-            // if EOF is reached and because available is not reliable,
-            // tries to read the rest of the data.
-            DrillBuf skipBuf = dataReader.getNext((int) bytesToSkip);
-            if (skipBuf != null) {
-              skipBuf.release();
-            } else {
-              throw new EOFException(""End of File reached."");
-            }
-          }
-        }
-      } catch (IOException e) {
-        handleAndThrowException(e, ""Error Reading dictionary page."");
-      }
-    }
-  }
-
-  @Override protected void init() throws IOException {
+  protected void init() throws IOException {
     super.init();
     //Avoid Init if a shutdown is already in progress even if init() is called once
     if (!parentColumnReader.isShuttingDown) {
       asyncPageRead.offer(ExecutorServiceUtil.submit(threadPool, new AsyncPageReaderTask(debugName, pageQueue)));
     }
   }
 
-  private DrillBuf getDecompressedPageData(ReadStatus readStatus) {
-    DrillBuf data;
-    boolean isDictionary = false;
-    synchronized (this) {
-      data = readStatus.getPageData();
-      readStatus.setPageData(null);
-      isDictionary = readStatus.isDictionaryPage;
-    }
-    if (parentColumnReader.columnChunkMetaData.getCodec() != CompressionCodecName.UNCOMPRESSED) {
-      DrillBuf compressedData = data;
-      data = decompress(readStatus.getPageHeader(), compressedData);
-      synchronized (this) {
-        readStatus.setPageData(null);
-      }
-      compressedData.release();
-    } else {
-      if (isDictionary) {
-        stats.totalDictPageReadBytes.addAndGet(readStatus.bytesRead);
-      } else {
-        stats.totalDataPageReadBytes.addAndGet(readStatus.bytesRead);
-      }
-    }
-    return data;
+  /**
+   * Reads and stores this column chunk's dictionary page.
+   * @throws IOException
+   */
+  protected void loadDictionary(ReadStatus readStatus) throws IOException {
+    assert readStatus.isDictionaryPage();
+    assert this.dictionary == null;
+
+    // dictData is not a local because we need to release it later.
+    this.dictData = codecName == CompressionCodecName.UNCOMPRESSED
+      ? readStatus.getPageData()
+      : decompressPageV1(readStatus);
+
+    DictionaryPage page = new DictionaryPage(
+      asBytesInput(dictData, 0, pageHeader.uncompressed_page_size),
+      pageHeader.uncompressed_page_size,
+      pageHeader.dictionary_page_header.num_values,
+      valueOf(pageHeader.dictionary_page_header.encoding.name())
+    );
+
+    this.dictionary = page.getEncoding().initDictionary(columnDescriptor, page);
   }
 
-  // Read and decode the dictionary data
-  private void readDictionaryPageData(final ReadStatus readStatus, final ColumnReader<?> parentStatus)
-      throws UserException {
+  /**
+   * Reads a compressed v1 data page or a dictionary page, both of which are compressed
+   * in their entirety.
+   * @return decompressed Parquet page data
+   * @throws IOException
+   */
+  protected DrillBuf decompressPageV1(ReadStatus readStatus) throws IOException {
+    Stopwatch timer = Stopwatch.createUnstarted();
+
+    PageHeader pageHeader = readStatus.getPageHeader();
+    int inputSize = pageHeader.getCompressed_page_size();
+    int outputSize = pageHeader.getUncompressed_page_size();
+    // TODO: does reporting this number have the same meaning in an async context?
+    long start = dataReader.getPos();
+    long timeToRead;
+
+    DrillBuf inputPageData = readStatus.getPageData();
+    DrillBuf outputPageData = this.allocator.buffer(outputSize);
+
     try {
-      pageHeader = readStatus.getPageHeader();
-      int uncompressedSize = pageHeader.getUncompressed_page_size();
-      final DrillBuf dictionaryData = getDecompressedPageData(readStatus);
-      Stopwatch timer = Stopwatch.createStarted();
-      allocatedDictionaryBuffers.add(dictionaryData);
-      DictionaryPage page = new DictionaryPage(asBytesInput(dictionaryData, 0, uncompressedSize),
-          pageHeader.uncompressed_page_size, pageHeader.dictionary_page_header.num_values,
-          valueOf(pageHeader.dictionary_page_header.encoding.name()));
-      this.dictionary = page.getEncoding().initDictionary(parentStatus.columnDescriptor, page);
-      long timeToDecode = timer.elapsed(TimeUnit.NANOSECONDS);
-      stats.timeDictPageDecode.addAndGet(timeToDecode);
-    } catch (Exception e) {
-      handleAndThrowException(e, ""Error decoding dictionary page."");
+      timer.start();
+      CompressionCodecName codecName = columnChunkMetaData.getCodec();
+      CompressionCodecFactory.BytesInputDecompressor decomp = codecFactory.getDecompressor(codecName);
+      ByteBuffer input = inputPageData.nioBuffer(0, inputSize);
+      ByteBuffer output = outputPageData.nioBuffer(0, outputSize);
+
+      decomp.decompress(input, inputSize, output, outputSize);
+      outputPageData.writerIndex(outputSize);
+      timeToRead = timer.elapsed(TimeUnit.NANOSECONDS);
+
+      logger.trace(
+        ""Col: {}  readPos: {}  Uncompressed_size: {}  pageData: {}"",
+        columnChunkMetaData.toString(),
+        dataReader.getPos(), // TODO: see comment on earlier call to getPos()
+        outputSize,
+        ByteBufUtil.hexDump(outputPageData)","[{'comment': 'And this call too.', 'commenter': 'vvysotskyi'}]"
2338,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/AsyncPageReader.java,"@@ -78,209 +79,237 @@
  *
  */
 class AsyncPageReader extends PageReader {
-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AsyncPageReader.class);
+  static final Logger logger = LoggerFactory.getLogger(AsyncPageReader.class);
 
   private ExecutorService threadPool;
   private long queueSize;
   private LinkedBlockingQueue<ReadStatus> pageQueue;
   private ConcurrentLinkedQueue<Future<Void>> asyncPageRead;
   private long totalPageValuesRead = 0;
-  private Object pageQueueSyncronize = new Object(); // Object to use to synchronize access to the page Queue.
+  private final Object pageQueueSyncronize = new Object(); // Object to use to synchronize access to the page Queue.
                                                      // FindBugs complains if we synchronize on a Concurrent Queue
 
-  AsyncPageReader(ColumnReader<?> parentStatus, FileSystem fs, Path path,
-      ColumnChunkMetaData columnChunkMetaData) throws ExecutionSetupException {
-    super(parentStatus, fs, path, columnChunkMetaData);
+  AsyncPageReader(ColumnReader<?> parentStatus, FileSystem fs, Path path) throws ExecutionSetupException {
+    super(parentStatus, fs, path);
     threadPool = parentColumnReader.parentReader.getOperatorContext().getScanExecutor();
     queueSize = parentColumnReader.parentReader.readQueueSize;
     pageQueue = new LinkedBlockingQueue<>((int) queueSize);
     asyncPageRead = new ConcurrentLinkedQueue<>();
   }
 
   @Override
-  protected void loadDictionaryIfExists(final ColumnReader<?> parentStatus,
-      final ColumnChunkMetaData columnChunkMetaData, final DirectBufInputStream f) throws UserException {
-    if (columnChunkMetaData.getDictionaryPageOffset() > 0) {
-      try {
-        assert(columnChunkMetaData.getDictionaryPageOffset() >= dataReader.getPos() );
-        long bytesToSkip = columnChunkMetaData.getDictionaryPageOffset() - dataReader.getPos();
-        while (bytesToSkip > 0) {
-          long skipped = dataReader.skip(bytesToSkip);
-          if (skipped > 0) {
-            bytesToSkip -= skipped;
-          } else {
-            // no good way to handle this. Guava uses InputStream.available to check
-            // if EOF is reached and because available is not reliable,
-            // tries to read the rest of the data.
-            DrillBuf skipBuf = dataReader.getNext((int) bytesToSkip);
-            if (skipBuf != null) {
-              skipBuf.release();
-            } else {
-              throw new EOFException(""End of File reached."");
-            }
-          }
-        }
-      } catch (IOException e) {
-        handleAndThrowException(e, ""Error Reading dictionary page."");
-      }
-    }
-  }
-
-  @Override protected void init() throws IOException {
+  protected void init() throws IOException {
     super.init();
     //Avoid Init if a shutdown is already in progress even if init() is called once
     if (!parentColumnReader.isShuttingDown) {
       asyncPageRead.offer(ExecutorServiceUtil.submit(threadPool, new AsyncPageReaderTask(debugName, pageQueue)));
     }
   }
 
-  private DrillBuf getDecompressedPageData(ReadStatus readStatus) {
-    DrillBuf data;
-    boolean isDictionary = false;
-    synchronized (this) {
-      data = readStatus.getPageData();
-      readStatus.setPageData(null);
-      isDictionary = readStatus.isDictionaryPage;
-    }
-    if (parentColumnReader.columnChunkMetaData.getCodec() != CompressionCodecName.UNCOMPRESSED) {
-      DrillBuf compressedData = data;
-      data = decompress(readStatus.getPageHeader(), compressedData);
-      synchronized (this) {
-        readStatus.setPageData(null);
-      }
-      compressedData.release();
-    } else {
-      if (isDictionary) {
-        stats.totalDictPageReadBytes.addAndGet(readStatus.bytesRead);
-      } else {
-        stats.totalDataPageReadBytes.addAndGet(readStatus.bytesRead);
-      }
-    }
-    return data;
+  /**
+   * Reads and stores this column chunk's dictionary page.
+   * @throws IOException
+   */
+  protected void loadDictionary(ReadStatus readStatus) throws IOException {
+    assert readStatus.isDictionaryPage();
+    assert this.dictionary == null;
+
+    // dictData is not a local because we need to release it later.
+    this.dictData = codecName == CompressionCodecName.UNCOMPRESSED
+      ? readStatus.getPageData()
+      : decompressPageV1(readStatus);
+
+    DictionaryPage page = new DictionaryPage(
+      asBytesInput(dictData, 0, pageHeader.uncompressed_page_size),
+      pageHeader.uncompressed_page_size,
+      pageHeader.dictionary_page_header.num_values,
+      valueOf(pageHeader.dictionary_page_header.encoding.name())
+    );
+
+    this.dictionary = page.getEncoding().initDictionary(columnDescriptor, page);
   }
 
-  // Read and decode the dictionary data
-  private void readDictionaryPageData(final ReadStatus readStatus, final ColumnReader<?> parentStatus)
-      throws UserException {
+  /**
+   * Reads a compressed v1 data page or a dictionary page, both of which are compressed
+   * in their entirety.
+   * @return decompressed Parquet page data
+   * @throws IOException
+   */
+  protected DrillBuf decompressPageV1(ReadStatus readStatus) throws IOException {
+    Stopwatch timer = Stopwatch.createUnstarted();
+
+    PageHeader pageHeader = readStatus.getPageHeader();
+    int inputSize = pageHeader.getCompressed_page_size();
+    int outputSize = pageHeader.getUncompressed_page_size();
+    // TODO: does reporting this number have the same meaning in an async context?
+    long start = dataReader.getPos();
+    long timeToRead;
+
+    DrillBuf inputPageData = readStatus.getPageData();
+    DrillBuf outputPageData = this.allocator.buffer(outputSize);
+
     try {
-      pageHeader = readStatus.getPageHeader();
-      int uncompressedSize = pageHeader.getUncompressed_page_size();
-      final DrillBuf dictionaryData = getDecompressedPageData(readStatus);
-      Stopwatch timer = Stopwatch.createStarted();
-      allocatedDictionaryBuffers.add(dictionaryData);
-      DictionaryPage page = new DictionaryPage(asBytesInput(dictionaryData, 0, uncompressedSize),
-          pageHeader.uncompressed_page_size, pageHeader.dictionary_page_header.num_values,
-          valueOf(pageHeader.dictionary_page_header.encoding.name()));
-      this.dictionary = page.getEncoding().initDictionary(parentStatus.columnDescriptor, page);
-      long timeToDecode = timer.elapsed(TimeUnit.NANOSECONDS);
-      stats.timeDictPageDecode.addAndGet(timeToDecode);
-    } catch (Exception e) {
-      handleAndThrowException(e, ""Error decoding dictionary page."");
+      timer.start();
+      CompressionCodecName codecName = columnChunkMetaData.getCodec();
+      CompressionCodecFactory.BytesInputDecompressor decomp = codecFactory.getDecompressor(codecName);
+      ByteBuffer input = inputPageData.nioBuffer(0, inputSize);
+      ByteBuffer output = outputPageData.nioBuffer(0, outputSize);
+
+      decomp.decompress(input, inputSize, output, outputSize);
+      outputPageData.writerIndex(outputSize);
+      timeToRead = timer.elapsed(TimeUnit.NANOSECONDS);
+
+      logger.trace(
+        ""Col: {}  readPos: {}  Uncompressed_size: {}  pageData: {}"",
+        columnChunkMetaData.toString(),
+        dataReader.getPos(), // TODO: see comment on earlier call to getPos()
+        outputSize,
+        ByteBufUtil.hexDump(outputPageData)
+      );
+
+      this.updateStats(pageHeader, ""Decompress"", start, timeToRead, inputSize, outputSize);
+    } finally {
+      readStatus.setPageData(null);
+      if (inputPageData != null) {
+        inputPageData.release();
+      }
     }
-  }
 
-  private void handleAndThrowException(Exception e, String msg) throws UserException {
-    UserException ex = UserException.dataReadError(e).message(msg)
-        .pushContext(""Row Group Start: "", this.parentColumnReader.columnChunkMetaData.getStartingPos())
-        .pushContext(""Column: "", this.parentColumnReader.schemaElement.getName())
-        .pushContext(""File: "", this.fileName).build(logger);
-    throw ex;
+    return outputPageData;
   }
 
-  private DrillBuf decompress(PageHeader pageHeader, DrillBuf compressedData) {
-    DrillBuf pageDataBuf = null;
+  /**
+   * Reads a compressed v2 data page which excluded the repetition and definition level
+   * sections from compression.
+   * @return decompressed Parquet page data
+   * @throws IOException
+   */
+  protected DrillBuf decompressPageV2(ReadStatus readStatus) throws IOException {
     Stopwatch timer = Stopwatch.createUnstarted();
+
+    PageHeader pageHeader = readStatus.getPageHeader();
+    int inputSize = pageHeader.getCompressed_page_size();
+    int repLevelSize = pageHeader.data_page_header_v2.getRepetition_levels_byte_length();
+    int defLevelSize = pageHeader.data_page_header_v2.getDefinition_levels_byte_length();
+    int compDataOffset = repLevelSize + defLevelSize;
+    int outputSize = pageHeader.uncompressed_page_size;
+    // TODO: does reporting this number have the same meaning in an async context?
+    long start = dataReader.getPos();
     long timeToRead;
-    int compressedSize = pageHeader.getCompressed_page_size();
-    int uncompressedSize = pageHeader.getUncompressed_page_size();
-    pageDataBuf = allocateTemporaryBuffer(uncompressedSize);
+
+    DrillBuf inputPageData = readStatus.getPageData();
+    DrillBuf outputPageData = this.allocator.buffer(outputSize);
+
     try {
       timer.start();
+      // Write out the uncompressed section
+      // Note that the following setBytes call to read the repetition and definition level sections
+      // advances readerIndex in inputPageData but not writerIndex in outputPageData.
+      outputPageData.setBytes(0, inputPageData, compDataOffset);
+
+      // decompress from the start of compressed data to the end of the input buffer
+      CompressionCodecName codecName = columnChunkMetaData.getCodec();
+      CompressionCodecFactory.BytesInputDecompressor decomp = codecFactory.getDecompressor(codecName);
+      ByteBuffer input = inputPageData.nioBuffer(compDataOffset, inputSize - compDataOffset);
+      ByteBuffer output = outputPageData.nioBuffer(compDataOffset, outputSize - compDataOffset);
+      decomp.decompress(
+        input,
+        inputSize - compDataOffset,
+        output,
+        outputSize - compDataOffset
+      );
+      outputPageData.writerIndex(outputSize);
+      timeToRead = timer.elapsed(TimeUnit.NANOSECONDS);
 
-      CompressionCodecName codecName = parentColumnReader.columnChunkMetaData.getCodec();
-      BytesInputDecompressor decomp = codecFactory.getDecompressor(codecName);
-      ByteBuffer input = compressedData.nioBuffer(0, compressedSize);
-      ByteBuffer output = pageDataBuf.nioBuffer(0, uncompressedSize);
+      logger.trace(
+        ""Col: {}  readPos: {}  Uncompressed_size: {}  pageData: {}"",
+        columnChunkMetaData.toString(),
+        dataReader.getPos(), // TODO: see comment on earlier call to getPos()
+        outputSize,
+        ByteBufUtil.hexDump(outputPageData)
+      );
 
-      decomp.decompress(input, compressedSize, output, uncompressedSize);
-      pageDataBuf.writerIndex(uncompressedSize);
-      timeToRead = timer.elapsed(TimeUnit.NANOSECONDS);
-      this.updateStats(pageHeader, ""Decompress"", 0, timeToRead, compressedSize, uncompressedSize);
-    } catch (IOException e) {
-      handleAndThrowException(e, ""Error decompressing data."");
+      this.updateStats(pageHeader, ""Decompress"", start, timeToRead, inputSize, outputSize);
+    } finally {
+      readStatus.setPageData(null);
+      if (inputPageData != null) {
+        inputPageData.release();
+      }
     }
-    return pageDataBuf;
+
+    return outputPageData;
   }
 
-  @Override
-  protected void nextInternal() throws IOException {
-    ReadStatus readStatus = null;
+  private ReadStatus nextPageFromQueue() throws InterruptedException, ExecutionException {
+    ReadStatus readStatus;
+    Stopwatch timer = Stopwatch.createStarted();
+    parentColumnReader.parentReader.getOperatorContext().getStats().startWait();
     try {
-      Stopwatch timer = Stopwatch.createStarted();
-      parentColumnReader.parentReader.getOperatorContext().getStats().startWait();
-      try {
-        waitForExecutionResult(); // get the result of execution
-        synchronized (pageQueueSyncronize) {
-          boolean pageQueueFull = pageQueue.remainingCapacity() == 0;
-          readStatus = pageQueue.take(); // get the data if no exception has been thrown
-          if (readStatus.pageData == null || readStatus == ReadStatus.EMPTY) {
-            throw new DrillRuntimeException(""Unexpected end of data"");
-          }
-          //if the queue was full before we took a page out, then there would
-          // have been no new read tasks scheduled. In that case, schedule a new read.
-          if (!parentColumnReader.isShuttingDown && pageQueueFull) {
-            asyncPageRead.offer(ExecutorServiceUtil.submit(threadPool, new AsyncPageReaderTask(debugName, pageQueue)));
-          }
+      waitForExecutionResult(); // get the result of execution
+      synchronized (pageQueueSyncronize) {
+        boolean pageQueueFull = pageQueue.remainingCapacity() == 0;
+        readStatus = pageQueue.take(); // get the data if no exception has been thrown
+        if (readStatus.pageData == null || readStatus == ReadStatus.EMPTY) {
+          throw new DrillRuntimeException(""Unexpected end of data"");
+        }
+        //if the queue was full before we took a page out, then there would
+        // have been no new read tasks scheduled. In that case, schedule a new read.
+        if (!parentColumnReader.isShuttingDown && pageQueueFull) {
+          asyncPageRead.offer(ExecutorServiceUtil.submit(threadPool, new AsyncPageReaderTask(debugName, pageQueue)));
         }
-      } finally {
-        parentColumnReader.parentReader.getOperatorContext().getStats().stopWait();
-      }
-      long timeBlocked = timer.elapsed(TimeUnit.NANOSECONDS);
-      stats.timeDiskScanWait.addAndGet(timeBlocked);
-      stats.timeDiskScan.addAndGet(readStatus.getDiskScanTime());
-      if (readStatus.isDictionaryPage) {
-        stats.numDictPageLoads.incrementAndGet();
-        stats.timeDictPageLoads.addAndGet(timeBlocked + readStatus.getDiskScanTime());
-      } else {
-        stats.numDataPageLoads.incrementAndGet();
-        stats.timeDataPageLoads.addAndGet(timeBlocked + readStatus.getDiskScanTime());
       }
-      pageHeader = readStatus.getPageHeader();
+    } finally {
+      parentColumnReader.parentReader.getOperatorContext().getStats().stopWait();
+    }
 
-      // TODO - figure out if we need multiple dictionary pages, I believe it may be limited to one
-      // I think we are clobbering parts of the dictionary if there can be multiple pages of dictionary
-
-      do {
-        if (pageHeader.getType() == PageType.DICTIONARY_PAGE) {
-          readDictionaryPageData(readStatus, parentColumnReader);
-          waitForExecutionResult(); // get the result of execution
-          synchronized (pageQueueSyncronize) {
-            boolean pageQueueFull = pageQueue.remainingCapacity() == 0;
-            readStatus = pageQueue.take(); // get the data if no exception has been thrown
-            if (readStatus.pageData == null || readStatus == ReadStatus.EMPTY) {
-              break;
-            }
-            //if the queue was full before we took a page out, then there would
-            // have been no new read tasks scheduled. In that case, schedule a new read.
-            if (!parentColumnReader.isShuttingDown && pageQueueFull) {
-              asyncPageRead.offer(ExecutorServiceUtil.submit(threadPool, new AsyncPageReaderTask(debugName, pageQueue)));
-            }
-          }
-          pageHeader = readStatus.getPageHeader();
-        }
-      } while (pageHeader.getType() == PageType.DICTIONARY_PAGE);
+    long timeBlocked = timer.elapsed(TimeUnit.NANOSECONDS);
+    stats.timeDiskScanWait.addAndGet(timeBlocked);
+    stats.timeDiskScan.addAndGet(readStatus.getDiskScanTime());
+    if (readStatus.isDictionaryPage) {
+      stats.numDictPageLoads.incrementAndGet();
+      stats.timeDictPageLoads.addAndGet(timeBlocked + readStatus.getDiskScanTime());
+    } else {
+      stats.numDataPageLoads.incrementAndGet();
+      stats.timeDataPageLoads.addAndGet(timeBlocked + readStatus.getDiskScanTime());
+    }
+
+    return readStatus;
+  }
 
+  @Override
+  protected void nextInternal() throws IOException {
+    try {
+      ReadStatus readStatus = nextPageFromQueue();
       pageHeader = readStatus.getPageHeader();
-      pageData = getDecompressedPageData(readStatus);
-      assert (pageData != null);
+
+      if (pageHeader.getType() == PageType.DICTIONARY_PAGE) {","[{'comment': 'We can combine this if with the switch statement below.', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi I thought the same thing in an earlier version, but I now don't think it would work.  The switch expression evaluates `pageHeader.getType()`, and the DICTIONARY_PAGE case *modifies* pageHeader because after loading the dictionary it loads another page.  So if we fell through from DICTIONARY_PAGE we'd need the switch expression to reevaluate `pageHeader.getType()` and I don't think it will do that?  I.e. I'd think switches only evaluate their expression once..."", 'commenter': 'jnturton'}]"
2338,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/DataPageHeaderInfoProvider.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet;
+
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.parquet.format.DataPageHeaderV2;
+import org.apache.parquet.format.Encoding;
+import org.apache.parquet.format.PageHeader;
+import org.apache.parquet.format.Statistics;
+
+public interface DataPageHeaderInfoProvider {
+  int getNumValues();
+
+  Encoding getEncoding();
+
+  Encoding getDefinitionLevelEncoding();","[{'comment': 'Looks like this and `getRepetitionLevelEncoding` methods are used now for the v1 version, so no need in introducing them in this interface.', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi these two methods were already in the interface, I didn't add them.  But I agree that, unfortunately, they are not helpful because v2 rep and def level decoding requires special treatment anyway.  So should I remove them from the interface?"", 'commenter': 'jnturton'}]"
2342,contrib/storage-splunk/src/main/java/org/apache/drill/exec/store/splunk/SplunkSchemaFactory.java,"@@ -40,19 +38,11 @@
   private static final Logger logger = LoggerFactory.getLogger(SplunkSchemaFactory.class);
   private static final String SPL_TABLE_NAME = ""spl"";
   private final SplunkStoragePlugin plugin;
-  private final EntityCollection<Index> indexes;
+  // private final EntityCollection<Index> indexes;","[{'comment': 'Please remove commented out code. ', 'commenter': 'cgivre'}]"
2342,contrib/storage-jdbc/src/main/resources/bootstrap-storage-plugins.json,"@@ -8,6 +8,10 @@
       ""password"": ""xxx"",
       ""caseInsensitiveTableNames"": false,
       ""sourceParameters"" : {
+        ""idleTimeout"": 3600000,","[{'comment': ""I'm not sure if there exist an incompatible after the boot file is upgraded. Please check that :\r\n1. boot Drill (with standalone ZK) using old version\r\n2. create a JDBC storage\r\n3. shutdown and upgrade the Drill\r\n5. reboot the Drill & do check"", 'commenter': 'luocooong'}, {'comment': '@luocooong Oh interesting.  Is it okay if I test with Drill 1.18 for the old version?  And use storage configs in files stored by drill-embedded, instead of ZooKeeper?  The drill-embedded of the new Drill version will go and look for the config files in the same location...', 'commenter': 'jnturton'}, {'comment': 'The ""old"" does not included this patch. So, you can build code on the master (mark to old), then build the new version on your patch branch.\n\nActually, we add the parameter is in the Map data structure and there should be no problem, but it\'s better to test it.', 'commenter': 'luocooong'}, {'comment': '@luocooong Okay I followed your steps using a public pg database (details below, not sensitive).  The Drill version used to create the storage config was 1.18 (just because I have it lying around) then I started Drill built from this branch and the storage config was present and enabled.  I disabled and enabled it again without errors.\r\n\r\n```\r\n{\r\n  ""type"": ""jdbc"",\r\n  ""driver"": ""org.postgresql.Driver"",\r\n  ""url"": ""jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs"",\r\n  ""username"": ""reader"",\r\n  ""password"": ""NWDMCE5xdipIjRrp"",\r\n  ""caseInsensitiveTableNames"": false,\r\n  ""sourceParameters"": {\r\n    ""maximumPoolSize"": 1\r\n  },\r\n  ""enabled"": true\r\n}\r\n```', 'commenter': 'jnturton'}]"
2344,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/StoragePlugins.java,"@@ -17,12 +17,8 @@
  */
 package org.apache.drill.exec.planner.logical;
 
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
+import java.util.*;","[{'comment': 'Do not use imports with stars', 'commenter': 'vdiravka'}, {'comment': 'My IDE did that and I missed that... Fixed.', 'commenter': 'cgivre'}]"
2344,contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java,"@@ -53,6 +53,30 @@ public int hashCode() {
     return result;
   }
 
+  @Override
+  public boolean equals(Object obj) {","[{'comment': 'What about equals and hashcode annotation from Lombok?', 'commenter': 'vdiravka'}, {'comment': ""I'm hesitant to mess with this too much.  Lombok does both and this function already had a `hashcode` function. "", 'commenter': 'cgivre'}]"
2344,contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/streams/StreamsFormatPluginConfig.java,"@@ -31,6 +31,16 @@ public int hashCode() {
     return 47;
   }
 
+  @Override
+  public boolean equals (Object that) {","[{'comment': '```suggestion\r\n  public boolean equals(Object that) {\r\n```', 'commenter': 'vdiravka'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2344,contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/TableFormatPluginConfig.java,"@@ -31,5 +31,10 @@ public boolean equals(Object obj) {
     return impEquals(obj);
   }
 
+  @Override
+  public int hashCode() {
+    return 613;","[{'comment': 'Why constant?', 'commenter': 'vdiravka'}, {'comment': ""This class is abstract and doesn't have any properties.  I removed this and suppressed the LGTM alert."", 'commenter': 'cgivre'}]"
2344,contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/TableFormatPluginConfig.java,"@@ -20,7 +20,7 @@
 import org.apache.drill.common.logical.FormatPluginConfig;
 
 public abstract class TableFormatPluginConfig implements FormatPluginConfig {
-
+  //lgtm [java/inconsistent-equals-and-hashcode]","[{'comment': 'Is this eliminate this part of code from LGTM code analyzing? Or it is just a note/todo?', 'commenter': 'vdiravka'}, {'comment': 'This tells LGTM that to ignore this issue in this class.  ', 'commenter': 'cgivre'}, {'comment': 'Possibly we can configure it in `lgtm.yml` later', 'commenter': 'vdiravka'}]"
2344,logical/src/main/java/org/apache/drill/common/logical/data/LogicalOperatorBase.java,"@@ -40,6 +40,11 @@ public final int hashCode() {
     return super.hashCode();","[{'comment': 'Could you add the space please after `implements LogicalOperator` on the 32 line', 'commenter': 'vdiravka'}, {'comment': 'Sure!', 'commenter': 'cgivre'}]"
2348,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpJsonOptions.java,"@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
+import lombok.Builder;
+import lombok.EqualsAndHashCode;
+import lombok.Getter;
+import lombok.ToString;
+import lombok.experimental.Accessors;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.server.options.OptionSet;
+import org.apache.drill.exec.store.easy.json.loader.JsonLoaderOptions;
+
+@Slf4j
+@Builder
+@Getter
+@Accessors(fluent = true)
+@EqualsAndHashCode
+@ToString
+@JsonInclude(JsonInclude.Include.NON_DEFAULT)
+@JsonDeserialize(builder = HttpJsonOptions.HttpJsonOptionsBuilder.class)
+public class HttpJsonOptions {
+
+  @JsonInclude
+  private final boolean allowNanInf;
+
+  @JsonInclude
+  private final boolean allTextMode;
+
+  @JsonInclude
+  private final boolean readNumbersAsDouble;
+
+  @JsonInclude
+  private final boolean enableEscapeAnyChar;
+
+  /**
+   * Describes whether or not this reader can unwrap a single root array record
+   * and treat it like a set of distinct records.
+   */
+  @JsonInclude
+  private final boolean skipOuterList;  // Default should be true
+
+  @JsonIgnore
+  public JsonLoaderOptions getJsonOptions(OptionSet optionSet) {
+
+    JsonLoaderOptions options = new JsonLoaderOptions();
+
+    if (optionSet.getBoolean(ExecConstants.JSON_READER_NAN_INF_NUMBERS) != allowNanInf) {
+      options.allowNanInf = allowNanInf;
+    } else {
+      options.allowNanInf = optionSet.getBoolean(ExecConstants.JSON_READER_NAN_INF_NUMBERS);
+    }
+
+    if (optionSet.getBoolean(ExecConstants.JSON_ALL_TEXT_MODE) != allTextMode) {
+
+      options.allTextMode = allTextMode;
+    } else {
+      options.allTextMode = optionSet.getBoolean(ExecConstants.JSON_ALL_TEXT_MODE);","[{'comment': 'Since `optionSet.getBoolean(ExecConstants.JSON_ALL_TEXT_MODE)` == `allTextMode` here, we can put:\r\n`options.allTextMode = allTextMode`. \r\nTherefore looks like checking `optionSet` is redundant in `getJsonOptions` method.\r\n', 'commenter': 'vdiravka'}, {'comment': ""The issue I saw with that was that if `allTextMode` is not defined by the user then it will be `false` and we don't want that overwriting the system values."", 'commenter': 'cgivre'}, {'comment': '@vdiravka  Do you have any other questions/comments about this PR?  Thanks!', 'commenter': 'cgivre'}, {'comment': 'It is fine to set `options.allTextMode = allTextMode`. But looks like the above `if-else` is redundant and can be simplified to `options.allTextMode = allTextMode;`.  Same for other options', 'commenter': 'vdiravka'}, {'comment': '@vdiravka Fixed.  Thanks!', 'commenter': 'cgivre'}, {'comment': 'Just logically\r\n```\r\n    if (optionSet.getBoolean(ExecConstants.JSON_ALL_TEXT_MODE) != allTextMode) {\r\n      options.allTextMode = allTextMode;\r\n    } else {\r\n      options.allTextMode = optionSet.getBoolean(ExecConstants.JSON_ALL_TEXT_MODE);\r\n    }\r\n```\r\nfully equals to:\r\n```\r\n    options.allTextMode = allTextMode;\r\n```\r\n\r\nBut you are right about missing option in plugin configs and non default system option. In this case, this system option should be taken into account. To do this we need to understand, when the plugin config option is absent. It is not possible with primitive `boolean`, because `false` can mean missing plugin config value or user supplied false value. So to fix this need to use Boolean. Therefore by default (when option is not specified in plugin config) it will be null and for this case we can use system/session option here.', 'commenter': 'vdiravka'}, {'comment': ""@vdiravka  Thanks for the feedback.  I changed the variable type to `Boolean` rather than the primitive.  But I still see three cases:\r\n1.  User provided input does not match system settings.  In this case, we go with the user provided input.  \r\n2. User provided input does match system settings.  In this case it doesn't really matter.\r\n3. User did not provide input.  \r\n\r\nTo fix this, I refactored and used a `null` check for the input.\r\n "", 'commenter': 'cgivre'}, {'comment': 'Nice! I agree with the above cases', 'commenter': 'vdiravka'}]"
2348,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpJsonOptions.java,"@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
+import lombok.Builder;
+import lombok.EqualsAndHashCode;
+import lombok.Getter;
+import lombok.ToString;
+import lombok.experimental.Accessors;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.server.options.OptionSet;
+import org.apache.drill.exec.store.easy.json.loader.JsonLoaderOptions;
+
+@Slf4j
+@Builder
+@Getter
+@Accessors(fluent = true)
+@EqualsAndHashCode
+@ToString
+@JsonInclude(JsonInclude.Include.NON_DEFAULT)
+@JsonDeserialize(builder = HttpJsonOptions.HttpJsonOptionsBuilder.class)
+public class HttpJsonOptions {
+
+  @JsonInclude
+  private final boolean allowNanInf;
+
+  @JsonInclude
+  private final boolean allTextMode;
+
+  @JsonInclude
+  private final boolean readNumbersAsDouble;
+
+  @JsonInclude
+  private final boolean enableEscapeAnyChar;
+
+  /**
+   * Describes whether or not this reader can unwrap a single root array record
+   * and treat it like a set of distinct records.
+   */
+  @JsonInclude
+  private final boolean skipOuterList;  // Default should be true
+
+  @JsonIgnore
+  public JsonLoaderOptions getJsonOptions(OptionSet optionSet) {
+
+    JsonLoaderOptions options = new JsonLoaderOptions();","[{'comment': '```suggestion\r\n    JsonLoaderOptions options = new JsonLoaderOptions(optionSet);\r\n```\r\nNeed to pass `optionSet`, because it can have more options, than number of options are obtained from plugin config.', 'commenter': 'vdiravka'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2348,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpJsonOptions.java,"@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
+import lombok.Builder;
+import lombok.EqualsAndHashCode;
+import lombok.Getter;
+import lombok.ToString;
+import lombok.experimental.Accessors;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.server.options.OptionSet;
+import org.apache.drill.exec.store.easy.json.loader.JsonLoaderOptions;
+
+@Slf4j
+@Builder
+@Getter
+@Accessors(fluent = true)
+@EqualsAndHashCode
+@ToString
+@JsonInclude(JsonInclude.Include.NON_DEFAULT)
+@JsonDeserialize(builder = HttpJsonOptions.HttpJsonOptionsBuilder.class)
+public class HttpJsonOptions {
+
+  @JsonInclude
+  private final boolean allowNanInf;
+
+  @JsonInclude
+  private final boolean allTextMode;
+
+  @JsonInclude
+  private final boolean readNumbersAsDouble;
+
+  @JsonInclude
+  private final boolean enableEscapeAnyChar;
+
+  /**
+   * Describes whether or not this reader can unwrap a single root array record
+   * and treat it like a set of distinct records.
+   */
+  @JsonInclude
+  private final boolean skipOuterList;  // Default should be true","[{'comment': '> // Default should be true\r\n\r\nShould the initial value be specified in this case?\r\n`private final boolean skipOuterList = true`', 'commenter': 'vdiravka'}, {'comment': 'I removed that from the options that are user configurable.  The main ones I wanted to add are the `allTextMode` and `readNumbersAsDoubles`', 'commenter': 'cgivre'}, {'comment': 'So could it be marked as deprecated?\r\nI just was confused on `// Default should be true`. Where is it complies?', 'commenter': 'vdiravka'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetFormatPlugin.java,"@@ -167,6 +167,10 @@ public RecordWriter getRecordWriter(FragmentContext context, ParquetWriter write
     options.put(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS,
         context.getOptions().getOption(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS).bool_val.toString());
 
+    options.put(
+      ExecConstants.PARQUET_WRITER_FORMAT_VERSION,
+      context.getOptions().getOption(ExecConstants.PARQUET_WRITER_FORMAT_VERSION).string_val);","[{'comment': '```suggestion\r\ncontext.getOptions().getOption(ExecConstants.PARQUET_WRITER_FORMAT_VERSION).getValue());\r\n```\r\n', 'commenter': 'vdiravka'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java,"@@ -375,6 +375,12 @@ private ExecConstants() {
   public static final OptionValidator PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS_VALIDATOR = new EnumeratedStringValidator(PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS,
       new OptionDescription(""Parquet writer logical type for decimal; supported types \'fixed_len_byte_array\' and \'binary\'""),
       ""fixed_len_byte_array"", ""binary"");
+  public static final String PARQUET_WRITER_FORMAT_VERSION = ""store.parquet.writer.format_version"";
+  public static final OptionValidator PARQUET_WRITER_FORMAT_VERSION_VALIDATOR = new EnumeratedStringValidator(
+    PARQUET_WRITER_FORMAT_VERSION,
+    new OptionDescription(""Parquet format version used for storing Parquet output.  Allowed values: PARQUET_1_0, PARQUET_2_0""),
+    ""PARQUET_1_0"", ""PARQUET_2_0""","[{'comment': 'Optionally you can create String array of parquet versions (possibly in `ParquetFormatPlugin`) and use it for description and `EnumeratedStringValidator` values. Also (minor) consider adding short description.\r\n\r\n```\r\npublic static final String[] parquetVersions= {""PARQUET_1_0"", ""PARQUET_2_0""};\r\n```\r\n```\r\npublic static final OptionValidator PARQUET_WRITER_FORMAT_VERSION_VALIDATOR = new EnumeratedStringValidator(\r\n    PARQUET_WRITER_FORMAT_VERSION,\r\n    new OptionDescription(""Parquet format version used for storing Parquet output.  Allowed values:"" + Arrays.toString(parquetVersions)), ""Parquet format version"",\r\n    parquetVersions);\r\n```', 'commenter': 'vdiravka'}, {'comment': ""@vdiravka nice, thanks!  I'll do this...\r\n"", 'commenter': 'jnturton'}]"
2351,exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/writer/TestParquetWriter.java,"@@ -963,6 +963,17 @@ private void compareParquetInt96Converters(String selection, String table) throw
     }
   }
 
+  @Test
+  public void testTPCHReadWriteFormatV2() throws Exception {
+    try {
+      alterSession(ExecConstants.PARQUET_WRITER_FORMAT_VERSION, ""parquet_2_0"");","[{'comment': 'Do we need enabling it here, if the aim of the PR is enabling V2 by default?', 'commenter': 'vdiravka'}, {'comment': '@vdiravka I discussed the default format version with @vvysotskyi who said that v2 does not have wide adoption.  I then checked Apache Spark and found that, indeed, reading v2 will not work without a non-default setting there.  So it was conceded that v1 should remain the default, in spite of the title of the old Jira ticket.', 'commenter': 'jnturton'}]"
2351,exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetWriterConfig.java,"@@ -0,0 +1,67 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.parquet;
+
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterFixtureBuilder;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.nio.file.Paths;
+
+public class TestParquetWriterConfig extends ClusterTest {","[{'comment': 'I recommend introducing new test classes based on `JUnit5` framework. As an example you can check my recent update `TestDrillbitResilience` onto the JUnit5 (it is also extends `ClusterTest`)', 'commenter': 'vdiravka'}, {'comment': ""@vdiravka, thanks I will do this.  My first question here is whether I should even introduce a new test class here.  I don't think there is any other test class in Drill that only contains a single test of whether format config options are present in the query plan?"", 'commenter': 'jnturton'}, {'comment': 'Not sure about format config in query plan, but there are a lot of test cases with checking patterns in the plan.\r\nThey are based on the old `PlanTestBase#testPlanMatchingPatterns` method. For instance `TestPlanVerificationUtilities` has only one test case `testPlanVerifier`. You can change this one or add smth similar', 'commenter': 'vdiravka'}, {'comment': '@vdiravka Okay, I decided to upgrade `TestParquetWriter` to `ClusterTest` so that this test can live in there.', 'commenter': 'jnturton'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetFormatPlugin.java,"@@ -78,6 +80,8 @@
 
 public class ParquetFormatPlugin implements FormatPlugin {
 
+  public static final String[] PARQUET_VERSIONS = {""PARQUET_1_0"", ""PARQUET_2_0""};","[{'comment': 'I think we can simplify the values, the key already has `parquet` keyword - `PARQUET_VERSIONS`. Similar to `MetadataVersion.Constants` the array can be:\r\n`{""1.0"", ""2.0""}`.\r\n\r\nOr make it similar/corresponding to `ParquetProperties#WriterVersion` enum', 'commenter': 'vdiravka'}, {'comment': ""@vdiravka I followed `ParquetProperties#WriterVersion`, that's where `PARQUET_1_0` and `PARQUET_2_0` come from.  I agree it's clunky, but on the plus side I did not have to introduce any new version format strings or case statements.  Which do you think is preferable?"", 'commenter': 'jnturton'}, {'comment': 'You can use string values from `ParquetProperties#WriterVersion` enum for `PARQUET_VERSIONS`, so they will be tied up.', 'commenter': 'vdiravka'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetFormatPlugin.java,"@@ -141,40 +145,94 @@ public AbstractWriter getWriter(PhysicalOperator child, String location, List<St
   }
 
   public RecordWriter getRecordWriter(FragmentContext context, ParquetWriter writer) throws IOException, OutOfMemoryException {
-    Map<String, String> options = new HashMap<>();
+    Map<String, String> writerOpts = new HashMap<>();
+    OptionManager contextOpts = context.getOptions();
 
-    options.put(""location"", writer.getLocation());
+    writerOpts.put(""location"", writer.getLocation());
 
     FragmentHandle handle = context.getHandle();
     String fragmentId = String.format(""%d_%d"", handle.getMajorFragmentId(), handle.getMinorFragmentId());
-    options.put(""prefix"", fragmentId);
-
-    options.put(ExecConstants.PARQUET_BLOCK_SIZE, context.getOptions().getOption(ExecConstants.PARQUET_BLOCK_SIZE).num_val.toString());
-    options.put(ExecConstants.PARQUET_WRITER_USE_SINGLE_FS_BLOCK,
-      context.getOptions().getOption(ExecConstants.PARQUET_WRITER_USE_SINGLE_FS_BLOCK).bool_val.toString());
-    options.put(ExecConstants.PARQUET_PAGE_SIZE, context.getOptions().getOption(ExecConstants.PARQUET_PAGE_SIZE).num_val.toString());
-    options.put(ExecConstants.PARQUET_DICT_PAGE_SIZE, context.getOptions().getOption(ExecConstants.PARQUET_DICT_PAGE_SIZE).num_val.toString());
-
-    options.put(ExecConstants.PARQUET_WRITER_COMPRESSION_TYPE,
-        context.getOptions().getOption(ExecConstants.PARQUET_WRITER_COMPRESSION_TYPE).string_val);
-
-    options.put(ExecConstants.PARQUET_WRITER_ENABLE_DICTIONARY_ENCODING,
-        context.getOptions().getOption(ExecConstants.PARQUET_WRITER_ENABLE_DICTIONARY_ENCODING).bool_val.toString());
-
-    options.put(ExecConstants.PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS,
-        context.getOptions().getOption(ExecConstants.PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS).string_val);
-
-    options.put(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS,
-        context.getOptions().getOption(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS).bool_val.toString());
+    writerOpts.put(""prefix"", fragmentId);
+
+    // Many options which follow may be set as Drill config options or in the parquet format
+    // plugin config.  If there is a Drill option set at session scope or narrower it takes precendence.
+    OptionValue.OptionScope minScope = OptionValue.OptionScope.SESSION;
+
+    writerOpts.put(ExecConstants.PARQUET_BLOCK_SIZE,
+      ObjectUtils.firstNonNull(
+        contextOpts.getOption(ExecConstants.PARQUET_BLOCK_SIZE).getValueMinScope(minScope),
+        config.getBlockSize(),
+        contextOpts.getInt(ExecConstants.PARQUET_BLOCK_SIZE)
+      ).toString()
+    );
+
+    writerOpts.put(ExecConstants.PARQUET_WRITER_USE_SINGLE_FS_BLOCK,
+      ObjectUtils.firstNonNull(
+        contextOpts.getOption(ExecConstants.PARQUET_WRITER_USE_SINGLE_FS_BLOCK).getValueMinScope(minScope),
+        config.getUseSingleFSBlock(),
+        contextOpts.getBoolean(ExecConstants.PARQUET_WRITER_USE_SINGLE_FS_BLOCK)
+      ).toString()
+    );
+
+    writerOpts.put(ExecConstants.PARQUET_PAGE_SIZE,
+      ObjectUtils.firstNonNull(
+        contextOpts.getOption(ExecConstants.PARQUET_PAGE_SIZE).getValueMinScope(minScope),
+        config.getPageSize(),
+        contextOpts.getInt(ExecConstants.PARQUET_PAGE_SIZE)
+      ).toString()
+    );
+
+    // ""internal use"" so not settable in format config
+    writerOpts.put(ExecConstants.PARQUET_DICT_PAGE_SIZE,
+      contextOpts.getOption(ExecConstants.PARQUET_DICT_PAGE_SIZE).num_val.toString()
+    );
+
+    // ""internal use"" so not settable in format config
+    writerOpts.put(ExecConstants.PARQUET_WRITER_ENABLE_DICTIONARY_ENCODING,
+      contextOpts.getOption(ExecConstants.PARQUET_WRITER_ENABLE_DICTIONARY_ENCODING).bool_val.toString()
+    );
+
+    writerOpts.put(ExecConstants.PARQUET_WRITER_COMPRESSION_TYPE,
+      ObjectUtils.firstNonNull(
+        contextOpts.getOption(ExecConstants.PARQUET_WRITER_COMPRESSION_TYPE).getValueMinScope(minScope),
+        config.getWriterCompressionType(),
+        contextOpts.getString(ExecConstants.PARQUET_WRITER_COMPRESSION_TYPE)
+      ).toString()
+    );
+
+    writerOpts.put(ExecConstants.PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS,
+      ObjectUtils.firstNonNull(
+        contextOpts.getOption(ExecConstants.PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS).getValueMinScope(minScope),
+        config.getWriterLogicalTypeForDecimals(),
+        contextOpts.getString(ExecConstants.PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS)
+      ).toString()
+    );
+
+    writerOpts.put(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS,
+      ObjectUtils.firstNonNull(
+        contextOpts.getOption(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS).getValueMinScope(minScope),
+        config.getWriterUsePrimitivesForDecimals(),
+        contextOpts.getBoolean(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS)
+      ).toString()
+    );
+
+    writerOpts.put(ExecConstants.PARQUET_WRITER_FORMAT_VERSION,
+      ObjectUtils.firstNonNull(
+        contextOpts.getOption(ExecConstants.PARQUET_WRITER_FORMAT_VERSION).getValueMinScope(minScope),
+        config.getWriterFormatVersion(),
+        contextOpts.getString(ExecConstants.PARQUET_WRITER_FORMAT_VERSION)
+      ).toString()
+    );
 
     RecordWriter recordWriter = new ParquetRecordWriter(context, writer);
-    recordWriter.init(options);
+    recordWriter.init(writerOpts);
 
     return recordWriter;
   }
 
   public WriterRecordBatch getWriterBatch(FragmentContext context, RecordBatch incoming, ParquetWriter writer)
           throws ExecutionSetupException {
+    // getConfig().get","[{'comment': '```suggestion\r\n```', 'commenter': 'vdiravka'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderConfig.java,"@@ -186,11 +187,13 @@ public ParquetReaderConfig build() {
         readerConfig.enableTimeReadCounter = conf.getBoolean(ENABLE_TIME_READ_COUNTER, readerConfig.enableTimeReadCounter);
       }
 
-      // last assign values from session options, session options have higher priority than other configurations
+      // last assign values from session or query scoped options which have higher priority than other configurations
       if (options != null) {
-        String option = options.getOption(ExecConstants.PARQUET_READER_STRINGS_SIGNED_MIN_MAX_VALIDATOR);
-        if (!option.isEmpty()) {
-          readerConfig.enableStringsSignedMinMax = Boolean.valueOf(option);
+        String optVal  = (String) options.getOption(
+          ExecConstants.PARQUET_READER_STRINGS_SIGNED_MIN_MAX
+        ).getValueMinScope(OptionValue.OptionScope.SESSION);","[{'comment': 'What about `OptionScope.QUERY` (according to above comment)?', 'commenter': 'vdiravka'}, {'comment': '@vdiravka if you look at the new `getValueMinScope` method, it works returns values from all scopes from `SESSION` _and up_ under the ordering implied by the `OptionScope` enum, namely\r\n```\r\n  public enum OptionScope {\r\n    BOOT, SYSTEM, SESSION, QUERY\r\n  }\r\n```\r\n\r\nSo `SESSION` and `QUERY` would both override here.', 'commenter': 'jnturton'}, {'comment': 'I thought about other direction :) \r\nNot sure I understood the purpose for this method fully. If session and query scope options are absent, the system and boot scope options will not be set to `optVal`?', 'commenter': 'vdiravka'}, {'comment': '@vdiravka The idea for `getValueMinScope` is that it returns the value of an option only if it is defined at scope as least as narrow as the passed-in minimum.  If you call it with `SESSION` as the minimum while the identified option only has a value in `SYSTEM` or `BOOT` then null is returned.  This is useful for implementing the option priority QUERY > SESSION > FORMAT > SYSTEM', 'commenter': 'jnturton'}, {'comment': 'Sounds reasonable. Possibly this logic will be migrated to `EffectiveConfigResolver` eventually', 'commenter': 'vdiravka'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetFormatPlugin.java,"@@ -141,40 +145,94 @@ public AbstractWriter getWriter(PhysicalOperator child, String location, List<St
   }
 
   public RecordWriter getRecordWriter(FragmentContext context, ParquetWriter writer) throws IOException, OutOfMemoryException {
-    Map<String, String> options = new HashMap<>();
+    Map<String, String> writerOpts = new HashMap<>();
+    OptionManager contextOpts = context.getOptions();
 
-    options.put(""location"", writer.getLocation());
+    writerOpts.put(""location"", writer.getLocation());
 
     FragmentHandle handle = context.getHandle();
     String fragmentId = String.format(""%d_%d"", handle.getMajorFragmentId(), handle.getMinorFragmentId());
-    options.put(""prefix"", fragmentId);
-
-    options.put(ExecConstants.PARQUET_BLOCK_SIZE, context.getOptions().getOption(ExecConstants.PARQUET_BLOCK_SIZE).num_val.toString());
-    options.put(ExecConstants.PARQUET_WRITER_USE_SINGLE_FS_BLOCK,
-      context.getOptions().getOption(ExecConstants.PARQUET_WRITER_USE_SINGLE_FS_BLOCK).bool_val.toString());
-    options.put(ExecConstants.PARQUET_PAGE_SIZE, context.getOptions().getOption(ExecConstants.PARQUET_PAGE_SIZE).num_val.toString());
-    options.put(ExecConstants.PARQUET_DICT_PAGE_SIZE, context.getOptions().getOption(ExecConstants.PARQUET_DICT_PAGE_SIZE).num_val.toString());
-
-    options.put(ExecConstants.PARQUET_WRITER_COMPRESSION_TYPE,
-        context.getOptions().getOption(ExecConstants.PARQUET_WRITER_COMPRESSION_TYPE).string_val);
-
-    options.put(ExecConstants.PARQUET_WRITER_ENABLE_DICTIONARY_ENCODING,
-        context.getOptions().getOption(ExecConstants.PARQUET_WRITER_ENABLE_DICTIONARY_ENCODING).bool_val.toString());
-
-    options.put(ExecConstants.PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS,
-        context.getOptions().getOption(ExecConstants.PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS).string_val);
-
-    options.put(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS,
-        context.getOptions().getOption(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS).bool_val.toString());
+    writerOpts.put(""prefix"", fragmentId);
+
+    // Many options which follow may be set as Drill config options or in the parquet format
+    // plugin config.  If there is a Drill option set at session scope or narrower it takes precendence.
+    OptionValue.OptionScope minScope = OptionValue.OptionScope.SESSION;","[{'comment': 'As far as I understand SESSION and QUERY scopes had precedence over plugin config earlier too. The aim of these changes for giving higher precedence for plugin config over SYSTEM options, right?\r\nIt is fine but could we manage plugin configs within a new OptionManager? And this manager can be in the managers fallback chain, so then the priority of the options will be resolved automatically for all options and plugins. Just consider it as an idea for enhancement (this PR or the new one)', 'commenter': 'vdiravka'}, {'comment': ""@vdiravka yes I thought exactly the same except I made up the name `EffectiveConfigResolver`.  Plugin developers should not have to learn and rewrite the config option priority logic every time.  I'm not exactly sure where the most natural home for this animal would be - which package..."", 'commenter': 'jnturton'}, {'comment': ""@vdiravka Okay let's discuss a design for this for a new PR.  I guess it would impact all the existing format plugins, and how they  access format configs."", 'commenter': 'jnturton'}, {'comment': '@vdiravka maybe we should put something about a new options manager which completely handles the different priorities into the Drill v2 ideas wiki page?', 'commenter': 'jnturton'}, {'comment': 'Sounds good. In Drill V2 we can define the motivation for using SESSION, QUERY and FORMAT options. And we can discuss whether we need to manage format config options via SESSION options.\r\nAt least we are fully free with choosing the best behavior for Drill users in spite of backward compatibility', 'commenter': 'vdiravka'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetFormatConfig.java,"@@ -18,81 +18,95 @@
 package org.apache.drill.exec.store.parquet;
 
 import com.fasterxml.jackson.annotation.JsonCreator;
-import com.fasterxml.jackson.annotation.JsonIgnore;
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
 
-import java.util.Objects;
+import lombok.Builder;
+import lombok.EqualsAndHashCode;
+import lombok.Getter;
 
 import org.apache.drill.common.PlanStringBuilder;
 import org.apache.drill.common.logical.FormatPluginConfig;
 
 import com.fasterxml.jackson.annotation.JsonTypeName;
 
+@EqualsAndHashCode
 @JsonTypeName(""parquet"") @JsonInclude(JsonInclude.Include.NON_DEFAULT)
 public class ParquetFormatConfig implements FormatPluginConfig {
 
-  private final boolean autoCorrectCorruptDates;
-  private final boolean enableStringsSignedMinMax;
-
-  public ParquetFormatConfig() {
-    this(true, false);
-  }
-
-  @JsonCreator
-  public ParquetFormatConfig(@JsonProperty(""autoCorrectCorruptDates"") Boolean autoCorrectCorruptDates,
-      @JsonProperty(""enableStringsSignedMinMax"") boolean enableStringsSignedMinMax) {
-    this.autoCorrectCorruptDates = autoCorrectCorruptDates == null ? true : autoCorrectCorruptDates;
-    this.enableStringsSignedMinMax = enableStringsSignedMinMax;
-  }
+  @Getter private final boolean autoCorrectCorruptDates;","[{'comment': '`enableStringsSignedMinMax` property has a good javadoc.\r\nBut `autoCorrectCorruptDates` has a good javadoc in `TestCorruptParquetDateCorrection`. Could you borrow some info from there or add a link to that javadoc?', 'commenter': 'vdiravka'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java,"@@ -263,20 +269,29 @@ private void newSchema() throws IOException {
     // We don't want this number to be too small either. Ideally, slightly bigger than the page size,
     // but not bigger than the block buffer
     int initialPageBufferSize = max(MINIMUM_BUFFER_SIZE, min(pageSize + pageSize / 10, initialBlockBufferSize));
+    ValuesWriterFactory valWriterFactory = writerVersion == WriterVersion.PARQUET_1_0
+      ? new DefaultV1ValuesWriterFactory()
+      : new DefaultV2ValuesWriterFactory();
+
     ParquetProperties parquetProperties = ParquetProperties.builder()
         .withPageSize(pageSize)
         .withDictionaryEncoding(enableDictionary)
         .withDictionaryPageSize(initialPageBufferSize)
-        .withWriterVersion(writerVersion)
         .withAllocator(new ParquetDirectByteBufferAllocator(oContext))
-        .withValuesWriterFactory(new DefaultV1ValuesWriterFactory())
+        .withValuesWriterFactory(valWriterFactory)
+        .withWriterVersion(writerVersion)
         .build();
+
     // TODO: Replace ParquetColumnChunkPageWriteStore with ColumnChunkPageWriteStore from parquet library
     //   once DRILL-7906 (PARQUET-1006) will be resolved
     pageStore = new ParquetColumnChunkPageWriteStore(codecFactory.getCompressor(codec), schema,
             parquetProperties.getInitialSlabSize(), pageSize, parquetProperties.getAllocator(),
             parquetProperties.getColumnIndexTruncateLength(), parquetProperties.getPageWriteChecksumEnabled());
-    store = new ColumnWriteStoreV1(pageStore, parquetProperties);
+
+    store = writerVersion == WriterVersion.PARQUET_1_0","[{'comment': 'switch-case?', 'commenter': 'vdiravka'}, {'comment': '@vdiravka do you mean a switch statement could be easier to read than this new ternary conditional?', 'commenter': 'jnturton'}, {'comment': 'oh yes, but starting from `java14`. Sorry. \r\n```\r\nstore = switch(writerVersion) {\r\n      case WriterVersion.PARQUET_1_0 -> new ColumnWriteStoreV1(schema, pageStore, parquetProperties);\r\n      case WriterVersion.PARQUET_2_0 -> new ColumnWriteStoreV2(schema, pageStore, parquetProperties);\r\n    };\r\n```', 'commenter': 'vdiravka'}]"
2351,exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/writer/TestParquetWriter.java,"@@ -137,14 +140,14 @@ public static void setupTestFiles() {
   public int repeat = 1;
 
   @BeforeClass","[{'comment': 'For sure we can still use JUnit4 here, but interested have you tries JUnit5? \r\nI suppose just updating annotations is required for that, for instance `@BeforeClass` -> `@BeforeAll`..', 'commenter': 'vdiravka'}, {'comment': '@vdiravka I did try to upgrade this class to JUnit5 but `@RunWith(Parameterized.class)` does not seem to have a simple direct translation (there are different translations and I do no have experience with any) so I rolled it back.', 'commenter': 'jnturton'}]"
2351,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetFormatPlugin.java,"@@ -78,6 +81,9 @@
 
 public class ParquetFormatPlugin implements FormatPlugin {
 
+  // {@link org.apache.parquet.column.ParquetProperties#WriterVersion}","[{'comment': 'It can be added via javadoc so `@link` will allow to use reference as a hyperlink in IDE', 'commenter': 'vdiravka'}]"
2352,contrib/storage-mongo/src/main/java/org/apache/drill/exec/store/mongo/MongoStoragePlugin.java,"@@ -195,6 +199,18 @@ public MongoClient getClient() {
     return getClient(addresses);
   }
 
+  public synchronized MongoClient getClientWithSrvProtocol() {
+    MongoClientSettings settings = MongoClientSettings.builder()
+        .applyConnectionString(clientURI)
+        .build();
+    MongoClient client = MongoClients.create(settings);
+    ServerAddress serverAddress = new ServerAddress(clientURI.getHosts().get(0));
+    String userName = clientURI.getCredential().getUserName();","[{'comment': '@luocooong \r\nThanks for the rapid response on this.  I have a minor suggestion.  We should include support for credentials coming from a credential provider, not just hard coded creds in the URL.  I think you can reuse some code below. \r\nThanks again!', 'commenter': 'cgivre'}, {'comment': 'Good practices. But we have done it. The credentials in the `clientURI` variable were processed during the construction period, and the `clientURI` is only available in the memory.  See that :\r\nhttps://github.com/apache/drill/blob/b6da35ece5a278a5ca72ecc33bafc98ba8f861f6/contrib/storage-mongo/src/main/java/org/apache/drill/exec/store/mongo/MongoStoragePlugin.java#L80-L81', 'commenter': 'luocooong'}, {'comment': 'In that case LGTM +1', 'commenter': 'cgivre'}]"
2359,distribution/src/assemble/component.xml,"@@ -49,7 +49,8 @@
         <include>org.apache.drill.contrib:drill-format-pcapng:jar</include>
         <include>org.apache.drill.contrib:drill-format-hdf5:jar</include>
         <include>org.apache.drill.contrib:drill-format-ltsv:jar</include>
-        <include>org.apache.drill.contrib:drill-format-httpd:jar</include>
+        <include>org.apache.drill.contrib:drill-format-https:jar</include>","[{'comment': 'the httpd to https change looks like an unintentional change', 'commenter': 'pjfanning'}, {'comment': 'Good catch!', 'commenter': 'cgivre'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfFormatConfig.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.annotation.JsonInclude.Include;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+@JsonTypeName(PdfFormatPlugin.DEFAULT_NAME)
+public class PdfFormatConfig implements FormatPluginConfig {
+
+  private final List<String> extensions;
+  private final boolean extractHeaders;
+  private final String extractionAlgorithm;
+  private final boolean combinePages;
+  private final int defaultTableIndex;
+
+  @JsonCreator
+  public PdfFormatConfig(@JsonProperty(""extensions"") List<String> extensions,
+                         @JsonProperty(""extractHeaders"") boolean extractHeaders,
+                         @JsonProperty(""extractionAlgorithm"") String extractionAlgorithm,
+                         @JsonProperty(""combinePages"") boolean combinePages,
+                         @JsonProperty(""defaultTableIndex"") int defaultTableIndex) {
+    this.extensions = extensions == null
+      ? Collections.singletonList(""pdf"")","[{'comment': 'Can also use the `PdfFormatPlugin.DEFAULT_NAME` instead of `""pdf""`.', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfFormatConfig.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.annotation.JsonInclude.Include;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+@JsonTypeName(PdfFormatPlugin.DEFAULT_NAME)
+public class PdfFormatConfig implements FormatPluginConfig {
+
+  private final List<String> extensions;
+  private final boolean extractHeaders;
+  private final String extractionAlgorithm;
+  private final boolean combinePages;
+  private final int defaultTableIndex;
+
+  @JsonCreator
+  public PdfFormatConfig(@JsonProperty(""extensions"") List<String> extensions,
+                         @JsonProperty(""extractHeaders"") boolean extractHeaders,
+                         @JsonProperty(""extractionAlgorithm"") String extractionAlgorithm,
+                         @JsonProperty(""combinePages"") boolean combinePages,
+                         @JsonProperty(""defaultTableIndex"") int defaultTableIndex) {
+    this.extensions = extensions == null
+      ? Collections.singletonList(""pdf"")
+      : ImmutableList.copyOf(extensions);
+    this.extractHeaders = extractHeaders;
+    this.extractionAlgorithm = extractionAlgorithm;
+    this.combinePages = combinePages;
+    this.defaultTableIndex = defaultTableIndex;
+  }
+
+  public PdfBatchReader.PdfReaderConfig getReaderConfig(PdfFormatPlugin plugin) {
+    return new PdfBatchReader.PdfReaderConfig(plugin);
+  }
+
+  @JsonInclude(Include.NON_DEFAULT)
+  @JsonProperty(""extensions"")
+  public List<String> getExtensions() {
+    return extensions;
+  }
+
+  @JsonInclude(Include.NON_DEFAULT)
+  @JsonProperty(""combinePages"")
+  public boolean getCombinePages() { return combinePages; }","[{'comment': 'Keep the code format of getter as consistent as possible.\r\n```java\r\npublic boolean getX() {\r\n  return x;\r\n}\r\n```\r\nor\r\n```java\r\npublic boolean getX() { return x; }\r\n```', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/Utils.java,"@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.ObjectExtractor;
+import technology.tabula.Page;
+import technology.tabula.PageIterator;
+import technology.tabula.Rectangle;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+import technology.tabula.detectors.NurminenDetectionAlgorithm;
+import technology.tabula.extractors.BasicExtractionAlgorithm;
+import technology.tabula.extractors.ExtractionAlgorithm;
+import technology.tabula.extractors.SpreadsheetExtractionAlgorithm;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class Utils {
+
+  public static final ExtractionAlgorithm DEFAULT_ALGORITHM = new BasicExtractionAlgorithm();
+  private static final Logger logger = LoggerFactory.getLogger(Utils.class);
+
+  /**
+   * Returns a list of tables found in a given PDF document.  There are several extraction algorithms
+   * available and this function uses the default Basic Extraction Algorithm.
+   * @param document The input PDF document to search for tables
+   * @return A list of tables found in the document.
+   */
+  public static List<Table> extractTablesFromPDF(PDDocument document) {
+    return extractTablesFromPDF(document, DEFAULT_ALGORITHM);
+  }
+
+  public static List<Table> extractTablesFromPDF(PDDocument document, ExtractionAlgorithm algorithm) {
+    System.setProperty(""java.awt.headless"", ""true"");
+
+    NurminenDetectionAlgorithm detectionAlgorithm = new NurminenDetectionAlgorithm();
+
+    ExtractionAlgorithm algExtractor;
+
+    SpreadsheetExtractionAlgorithm extractor = new SpreadsheetExtractionAlgorithm();
+
+    ObjectExtractor objectExtractor = new ObjectExtractor(document);
+    PageIterator pages = objectExtractor.extract();
+    List<Table> tables= new ArrayList<>();
+    while (pages.hasNext()) {
+      Page page = pages.next();
+
+      algExtractor = algorithm;
+      List<Rectangle> tablesOnPage = detectionAlgorithm.detect(page);
+
+      for (Rectangle guessRect : tablesOnPage) {
+        Page guess = page.getArea(guessRect);
+        tables.addAll(algExtractor.extract(guess));
+      }
+    }
+
+    try {
+      objectExtractor.close();
+    } catch (Exception e) {
+      logger.debug(""Error closing Object extractor."");
+    }
+
+    return tables;
+  }
+
+  /**
+   * Returns the values contained in a PDF Table row
+   * @param table The source table
+   * @return A list of the header rows
+   */
+  public static List<String> extractRowValues(Table table) {
+    List<RectangularTextContainer> firstRow = table.getRows().get(0);","[{'comment': 'Is it possible to `table.getRows()` is null?', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/main/resources/drill-module.conf,"@@ -0,0 +1,23 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+#  This file tells Drill to consider this module when class path scanning.
+#  This file can also include any supplementary configuration information.
+#  This file is in HOCON format, see https://github.com/typesafehub/config/blob/master/HOCON.md for more information.
+
+drill.classpath.scanning.packages += ""org.apache.drill.exec.store.pdf""","[{'comment': 'No newfile at end of file.', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/test/java/org/apache/drill/exec/store/pdf/TestPdfFormat.java,"@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.categories.RowSetTests;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.rpc.RpcException;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryBuilder;
+import org.apache.drill.test.QueryBuilder.QuerySummary;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.nio.file.Paths;
+import java.time.LocalDate;
+
+import static org.apache.drill.test.QueryTestUtil.generateCompressedFile;
+import static org.junit.Assert.assertEquals;
+
+@Category(RowSetTests.class)
+public class TestPdfFormat extends ClusterTest {
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));
+
+    // Needed for compressed file unit test
+    dirTestWatcher.copyResourceToRoot(Paths.get(""pdf/""));
+  }
+
+  @Test
+  public void testStarQuery() throws RpcException {
+    String sql = ""SELECT * FROM cp.`pdf/argentina_diputados_voting_record.pdf` WHERE `Provincia` = 'Rio Negro'"";
+
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""Apellido y Nombre"", MinorType.VARCHAR)
+      .addNullable(""Bloque político"", MinorType.VARCHAR)
+      .addNullable(""Provincia"", MinorType.VARCHAR)
+      .addNullable(""field_0"", MinorType.VARCHAR)
+      .buildSchema();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), expectedSchema)
+      .addRow(""ALBRIEU, Oscar Edmundo Nicolas"", ""Frente para la Victoria - PJ"", ""Rio Negro"", ""AFIRMATIVO"")
+      .addRow(""AVOSCAN, Herman Horacio"", ""Frente para la Victoria - PJ"", ""Rio Negro"", ""AFIRMATIVO"")
+      .addRow(""CEJAS, Jorge Alberto"", ""Frente para la Victoria - PJ"", ""Rio Negro"", ""AFIRMATIVO"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
+  }
+
+  @Test
+  public void testExplicitQuery() throws RpcException {
+    String sql = ""SELECT `Apellido y Nombre`, `Bloque político`, `Provincia`, `field_0` "" +
+      ""FROM cp.`pdf/argentina_diputados_voting_record.pdf` WHERE `Provincia` = 'Rio Negro'"";
+
+    QueryBuilder q = client.queryBuilder().sql(sql);
+    RowSet results = q.rowSet();
+
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""Apellido y Nombre"", MinorType.VARCHAR)
+      .addNullable(""Bloque político"", MinorType.VARCHAR)
+      .addNullable(""Provincia"", MinorType.VARCHAR)
+      .addNullable(""field_0"", MinorType.VARCHAR)","[{'comment': 'A good solution to the blank header.', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/test/java/org/apache/drill/exec/store/pdf/testPDFUtils.java,"@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.junit.Test;
+import technology.tabula.Table;
+import java.io.File;
+import java.util.List;
+
+import static org.junit.Assert.assertEquals;
+
+
+public class testPDFUtils {","[{'comment': 'Use the `TestPDFUtils`.', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/test/java/org/apache/drill/exec/store/pdf/testPDFUtils.java,"@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.junit.Test;
+import technology.tabula.Table;
+import java.io.File;
+import java.util.List;
+
+import static org.junit.Assert.assertEquals;
+
+
+public class testPDFUtils {
+
+  private static final String DATA_PATH = ""src/test/resources/pdf/"";
+
+  @Test
+  public void testTableExtractor() throws Exception {
+    PDDocument document = getDocument(""argentina_diputados_voting_record.pdf"");
+    List<Table> tableList = Utils.extractTablesFromPDF(document);
+    document.close();
+    assertEquals(tableList.size(), 1);
+
+    PDDocument document2 = getDocument(""twotables.pdf"");
+    List<Table> tableList2 = Utils.extractTablesFromPDF(document2);
+    document2.close();
+    assertEquals(tableList2.size(), 2);
+  }
+
+  @Test
+  public void testTableExtractorWithNoBoundingFrame() throws Exception {
+    PDDocument document = getDocument(""spreadsheet_no_bounding_frame.pdf"");
+    List<Table> tableList = Utils.extractTablesFromPDF(document);
+    document.close();
+    assertEquals(tableList.size(), 1);
+  }
+
+  @Test
+  public void testTableExtractorWitMultipage() throws Exception {
+    PDDocument document = getDocument(""us-020.pdf"");
+    List<Table> tableList = Utils.extractTablesFromPDF(document);
+    document.close();
+    assertEquals(tableList.size(), 4);
+  }
+
+  @Test
+  public void testFirstRowExtractor() throws Exception {
+    PDDocument document = getDocument(""schools.pdf"");
+    List<Table> tableList = Utils.extractTablesFromPDF(document);
+    document.close();
+
+    List<String> values = Utils.extractRowValues(tableList.get(0));
+    assertEquals(values.size(), 11);
+  }
+
+","[{'comment': 'Too many blank lines.', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/test/resources/logback-test.xml,"@@ -0,0 +1,57 @@
+<?xml version=""1.0"" encoding=""UTF-8"" ?>","[{'comment': 'Please remove this file or move to `/dev-support` folder.', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");","[{'comment': 'This property has been set many times, maybe it can be done like this :\r\n```java\r\nif (System.getProperty(""java.awt.headless"") == null) {\r\n  System.setProperty(""java.awt.headless"", ""true"");\r\n}\r\n```\r\nAnd remove the `Utils.java`, line 60. `PdfBatchReader.java`, line 164?', 'commenter': 'luocooong'}, {'comment': ""Oh, my. Drill can run embedded. Suppose I'm running Drill embedded in my AWT application. Will this break things?\r\n\r\nDrill is multi-threaded: we an have a dozen readers all setting this same system property concurrently with other code reading it. Is this a safe thing to do?\r\n\r\nMore fundamentally, should we be mucking about with shared system properties in a reader? Probably not."", 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");
+    this.negotiator = negotiator;
+
+    split = negotiator.split();
+    errorContext = negotiator.parentErrorContext();
+    builder = new SchemaBuilder();
+
+    openFile();
+
+    // Get the tables
+    tables = Utils.extractTablesFromPDF(document);
+    populateMetadata();
+    logger.debug(""Found {} tables"", tables.size());
+
+    // Support provided schema
+    TupleMetadata schema = null;
+    if (this.negotiator.hasProvidedSchema()) {
+      schema = this.negotiator.providedSchema();
+      this.negotiator.tableSchema(schema, false);
+    } else {
+      this.negotiator.tableSchema(buildSchema(), false);
+    }
+    ResultSetLoader loader = this.negotiator.build();
+    rowWriter = loader.writer();
+
+    if (negotiator.hasProvidedSchema()) {
+      buildWriterListFromProvidedSchema(schema);
+    } else {
+      buildWriterList();
+    }
+    addImplicitColumnsToSchema();
+
+    // Prepare for reading
+    currentRowIndex = 1;  // Skip the first line if there are headers
+    currentTable = tables.get(startingTableIndex);","[{'comment': 'What happens if `tables.size() < startingTableIndex`?', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");
+    this.negotiator = negotiator;
+
+    split = negotiator.split();
+    errorContext = negotiator.parentErrorContext();
+    builder = new SchemaBuilder();
+
+    openFile();
+
+    // Get the tables
+    tables = Utils.extractTablesFromPDF(document);
+    populateMetadata();
+    logger.debug(""Found {} tables"", tables.size());
+
+    // Support provided schema
+    TupleMetadata schema = null;
+    if (this.negotiator.hasProvidedSchema()) {
+      schema = this.negotiator.providedSchema();
+      this.negotiator.tableSchema(schema, false);
+    } else {
+      this.negotiator.tableSchema(buildSchema(), false);
+    }
+    ResultSetLoader loader = this.negotiator.build();
+    rowWriter = loader.writer();
+
+    if (negotiator.hasProvidedSchema()) {
+      buildWriterListFromProvidedSchema(schema);
+    } else {
+      buildWriterList();
+    }
+    addImplicitColumnsToSchema();
+
+    // Prepare for reading
+    currentRowIndex = 1;  // Skip the first line if there are headers
+    currentTable = tables.get(startingTableIndex);
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    System.setProperty(""java.awt.headless"", ""true"");
+
+    while(!rowWriter.isFull()) {
+      // Check to see if the limit has been reached
+      if (rowWriter.limitReached(maxRecords)) {
+        return false;
+      } else if (currentRowIndex >= currentTable.getRows().size() &&","[{'comment': 'Is this the logic to handle a table that spans multiple pages?', 'commenter': 'luocooong'}, {'comment': ""If so, I'm a bit confused by the comparison of row count and table count. Imagine each table has 100K rows. It would have to split across Drill batches. And, if each table has 3 rows, all tables should be combined in a single Drill batch."", 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");
+    this.negotiator = negotiator;
+
+    split = negotiator.split();
+    errorContext = negotiator.parentErrorContext();
+    builder = new SchemaBuilder();
+
+    openFile();
+
+    // Get the tables
+    tables = Utils.extractTablesFromPDF(document);
+    populateMetadata();
+    logger.debug(""Found {} tables"", tables.size());
+
+    // Support provided schema
+    TupleMetadata schema = null;
+    if (this.negotiator.hasProvidedSchema()) {
+      schema = this.negotiator.providedSchema();
+      this.negotiator.tableSchema(schema, false);
+    } else {
+      this.negotiator.tableSchema(buildSchema(), false);
+    }
+    ResultSetLoader loader = this.negotiator.build();
+    rowWriter = loader.writer();
+
+    if (negotiator.hasProvidedSchema()) {
+      buildWriterListFromProvidedSchema(schema);
+    } else {
+      buildWriterList();
+    }
+    addImplicitColumnsToSchema();
+
+    // Prepare for reading
+    currentRowIndex = 1;  // Skip the first line if there are headers
+    currentTable = tables.get(startingTableIndex);
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    System.setProperty(""java.awt.headless"", ""true"");
+
+    while(!rowWriter.isFull()) {
+      // Check to see if the limit has been reached
+      if (rowWriter.limitReached(maxRecords)) {
+        return false;
+      } else if (currentRowIndex >= currentTable.getRows().size() &&
+                  currentTableIndex < tables.size() &&
+                  config.plugin.getConfig().getCombinePages()) {
+        currentRowIndex = 0;
+        currentTable = tables.get(currentTableIndex++);
+      } else if (currentRowIndex >= currentTable.getRows().size()) {
+        return false;
+      }
+
+      // Process the row
+      processRow(currentTable.getRows().get(currentRowIndex));
+      currentRowIndex++;
+    }
+    return true;
+  }
+
+  private void processRow(List<RectangularTextContainer> row) {
+    if (row == null || row.size() == 0) {
+      return;
+    }
+
+    String value;
+    rowWriter.start();
+    for (int i = 0; i < row.size(); i++) {
+      value = row.get(i).getText();
+
+      if (Strings.isNullOrEmpty(value)) {
+        continue;
+      }
+      writers.get(i).load(row.get(i));
+    }
+    writeMetadata();
+    rowWriter.save();
+  }
+
+  @Override
+  public void close() {
+    if (fsStream != null) {
+      AutoCloseables.closeSilently(fsStream);
+      fsStream = null;
+    }
+
+    if (document != null) {
+      AutoCloseables.closeSilently(document.getDocument());
+      AutoCloseables.closeSilently(document);
+      document = null;
+    }
+  }
+
+  /**
+   * This method opens the PDF file, and finds the tables
+   */
+  private void openFile() {
+    try {
+      fsStream = negotiator.fileSystem().openPossiblyCompressedStream(split.getPath());
+      document = PDDocument.load(fsStream);
+    } catch (Exception e) {
+      throw UserException
+        .dataReadError(e)
+        .message(""Failed to open open input file: %s"", split.getPath().toString())
+        .addContext(e.getMessage())
+        .addContext(errorContext)
+        .build(logger);
+    }
+  }
+
+  /**
+   * Metadata fields are calculated once when the file is opened.  This function populates
+   * the metadata fields so that these are only calculated once.
+   */
+  private void populateMetadata() {
+    PDDocumentInformation info = document.getDocumentInformation();
+    pageCount = document.getNumberOfPages();
+    title = info.getTitle();
+    author = info.getAuthor();
+    subject = info.getSubject();
+    keywords = info.getKeywords();
+    creator = info.getCreator();
+    producer = info.getProducer();
+    creationDate = info.getCreationDate();
+    modificationDate = info.getModificationDate();
+    trapped = info.getTrapped();
+    tableCount = tables.size();
+  }
+
+  private void addImplicitColumnsToSchema() {
+    metadataIndex = columns;
+    // Add to schema
+    addMetadataColumnToSchema(""_page_count"", MinorType.INT);
+    addMetadataColumnToSchema(""_title"", MinorType.VARCHAR);
+    addMetadataColumnToSchema(""_author"", MinorType.VARCHAR);
+    addMetadataColumnToSchema(""_subject"", MinorType.VARCHAR);
+    addMetadataColumnToSchema(""_keywords"", MinorType.VARCHAR);
+    addMetadataColumnToSchema(""_creator"", MinorType.VARCHAR);
+    addMetadataColumnToSchema(""_producer"", MinorType.VARCHAR);
+    addMetadataColumnToSchema(""_creation_date"", MinorType.TIMESTAMP);
+    addMetadataColumnToSchema(""_modification_date"", MinorType.TIMESTAMP);
+    addMetadataColumnToSchema(""_trapped"", MinorType.VARCHAR);
+    addMetadataColumnToSchema(""_table_count"", MinorType.INT);
+  }
+
+  private void addMetadataColumnToSchema(String columnName, MinorType dataType) {
+    int index = rowWriter.tupleSchema().index(columnName);
+    if (index == -1) {
+      ColumnMetadata colSchema = MetadataUtils.newScalar(columnName, dataType, DataMode.OPTIONAL);
+
+      // Exclude from wildcard queries
+      colSchema.setBooleanProperty(ColumnMetadata.EXCLUDE_FROM_WILDCARD, true);
+      metadataIndex++;
+
+      index = rowWriter.addColumn(colSchema);
+    }
+
+    writers.add(new StringPdfColumnWriter(index, columnName, rowWriter));
+  }
+
+  private void writeMetadata() {
+    int startingIndex = columnHeaders.size();
+    writers.get(startingIndex).getWriter().setInt(pageCount);
+    writeStringMetadataField(title, startingIndex+1);
+    writeStringMetadataField(author, startingIndex+2);
+    writeStringMetadataField(subject, startingIndex+3);
+    writeStringMetadataField(keywords, startingIndex+4);
+    writeStringMetadataField(creator, startingIndex+5);
+    writeStringMetadataField(producer, startingIndex+6);
+    writeTimestampMetadataField(creationDate, startingIndex+7);
+    writeTimestampMetadataField(modificationDate, startingIndex+8);
+    writeStringMetadataField(trapped, startingIndex+9);
+    writers.get(startingIndex+10).getWriter().setInt(tableCount);
+  }
+
+  private void writeStringMetadataField(String value, int index) {
+    if (value == null) {
+      return;
+    }
+    writers.get(index).getWriter().setString(value);
+  }
+
+  private void writeTimestampMetadataField(Calendar dateValue, int index) {
+    if (dateValue == null) {
+      return;
+    }
+
+    writers.get(index).getWriter().setTimestamp(Instant.ofEpochMilli(dateValue.getTimeInMillis()));
+  }
+
+  private void addUnknownColumnToSchemaAndCreateWriter (TupleWriter rowWriter, String name) {","[{'comment': 'This method is never used locally, the `metadataIndex` is only used in this method.\r\nIn this class, at line 95 and 322, the value of field is not used.', 'commenter': 'luocooong'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  ","[{'comment': 'Nit: extra leading space.', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  
+
+## Data Model
+Since PDF files generally are not intended to be queried or read by machines, mapping the data to tables and rows is not a perfect process.  The PDF reader does support 
+provided schema. ","[{'comment': ""Probably want to link to the reference for provided schema. I suspect it's not a well-known feature. Also, explain what the provided schema provides (fields within the table?)"", 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  
+
+## Data Model
+Since PDF files generally are not intended to be queried or read by machines, mapping the data to tables and rows is not a perfect process.  The PDF reader does support 
+provided schema. 
+
+### Merging Pages
+The PDF reader reads tables from PDF files on each page.  If your PDF file has tables that span multiple pages, you can set the `combinePages` parameter to `true` and Drill 
+will merge all the tables in the PDF file.  You can also do this at query time with the `table()` function.","[{'comment': 'Suppose my file has three, mutually incompatible tables, and I only want to read the first. Can I? If so, how?\r\n\r\nOr, do I read all of them (sales of apples by state, shelf life of various apple kinds, list of largest apple growers) into a big messy, combined table, then use a `WHERE` clause to try to keep just the shelf life info?', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  
+
+## Data Model
+Since PDF files generally are not intended to be queried or read by machines, mapping the data to tables and rows is not a perfect process.  The PDF reader does support 
+provided schema. 
+
+### Merging Pages
+The PDF reader reads tables from PDF files on each page.  If your PDF file has tables that span multiple pages, you can set the `combinePages` parameter to `true` and Drill 
+will merge all the tables in the PDF file.  You can also do this at query time with the `table()` function.
+
+## Configuration
+To configure the PDF reader, simply add the information below to the `formats` section of a file base storage plugin. ","[{'comment': 'What is a ""file base storage plugin""? Maybe add ""such as dfs"".', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  
+
+## Data Model
+Since PDF files generally are not intended to be queried or read by machines, mapping the data to tables and rows is not a perfect process.  The PDF reader does support 
+provided schema. 
+
+### Merging Pages
+The PDF reader reads tables from PDF files on each page.  If your PDF file has tables that span multiple pages, you can set the `combinePages` parameter to `true` and Drill 
+will merge all the tables in the PDF file.  You can also do this at query time with the `table()` function.
+
+## Configuration
+To configure the PDF reader, simply add the information below to the `formats` section of a file base storage plugin. 
+
+```json
+""pdf"": {
+  ""type"": ""pdf"",
+  ""extensions"": [
+    ""pdf""
+  ],
+  ""extractHeaders"": true,
+  ""combinePages"": false
+}
+```
+The avaialable options are:
+* `extractHeaders`: Extracts the first row of any tables as the header row.  If set to false, Drill will assign column names of `field_0`, `field_1` to each column.","[{'comment': 'Explain a bit more. I have two tables with titles ""a | b"", and ""c | d | e"". Do I get a union row with ""a, b, c, d, e"" as fields? Or, just ""a, b"" as fields? When I read the second table, do I then get ""a, b, e""?', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  
+
+## Data Model
+Since PDF files generally are not intended to be queried or read by machines, mapping the data to tables and rows is not a perfect process.  The PDF reader does support 
+provided schema. 
+
+### Merging Pages
+The PDF reader reads tables from PDF files on each page.  If your PDF file has tables that span multiple pages, you can set the `combinePages` parameter to `true` and Drill 
+will merge all the tables in the PDF file.  You can also do this at query time with the `table()` function.
+
+## Configuration
+To configure the PDF reader, simply add the information below to the `formats` section of a file base storage plugin. 
+
+```json
+""pdf"": {
+  ""type"": ""pdf"",
+  ""extensions"": [
+    ""pdf""
+  ],
+  ""extractHeaders"": true,
+  ""combinePages"": false
+}
+```
+The avaialable options are:
+* `extractHeaders`: Extracts the first row of any tables as the header row.  If set to false, Drill will assign column names of `field_0`, `field_1` to each column.
+* `combinePages`: Merges multipage tables together.
+* `defaultTableIndex`:  Allows you to query different tables within the PDF file. Index starts at `0`. ","[{'comment': 'It should start with 1: makes it easier for humans to count.', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  
+
+## Data Model
+Since PDF files generally are not intended to be queried or read by machines, mapping the data to tables and rows is not a perfect process.  The PDF reader does support 
+provided schema. 
+
+### Merging Pages
+The PDF reader reads tables from PDF files on each page.  If your PDF file has tables that span multiple pages, you can set the `combinePages` parameter to `true` and Drill 
+will merge all the tables in the PDF file.  You can also do this at query time with the `table()` function.
+
+## Configuration
+To configure the PDF reader, simply add the information below to the `formats` section of a file base storage plugin. 
+
+```json
+""pdf"": {
+  ""type"": ""pdf"",
+  ""extensions"": [
+    ""pdf""
+  ],
+  ""extractHeaders"": true,
+  ""combinePages"": false
+}
+```
+The avaialable options are:
+* `extractHeaders`: Extracts the first row of any tables as the header row.  If set to false, Drill will assign column names of `field_0`, `field_1` to each column.
+* `combinePages`: Merges multipage tables together.
+* `defaultTableIndex`:  Allows you to query different tables within the PDF file. Index starts at `0`. 
+
+
+## Accessing Document Metadata Fields
+PDF files have a considerable amount of metadata which can be useful for analysis.  Drill will extract the following fields from every PDF file.  Note that these fields are not
+ projected in star queries and must be selected explicitly.  The document's creator populates these fields and some or all may be empty. With the exception of `_page_count","[{'comment': 'Nit: leading space.', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  
+
+## Data Model
+Since PDF files generally are not intended to be queried or read by machines, mapping the data to tables and rows is not a perfect process.  The PDF reader does support 
+provided schema. 
+
+### Merging Pages
+The PDF reader reads tables from PDF files on each page.  If your PDF file has tables that span multiple pages, you can set the `combinePages` parameter to `true` and Drill 
+will merge all the tables in the PDF file.  You can also do this at query time with the `table()` function.
+
+## Configuration
+To configure the PDF reader, simply add the information below to the `formats` section of a file base storage plugin. 
+
+```json
+""pdf"": {
+  ""type"": ""pdf"",
+  ""extensions"": [
+    ""pdf""
+  ],
+  ""extractHeaders"": true,
+  ""combinePages"": false
+}
+```
+The avaialable options are:
+* `extractHeaders`: Extracts the first row of any tables as the header row.  If set to false, Drill will assign column names of `field_0`, `field_1` to each column.
+* `combinePages`: Merges multipage tables together.
+* `defaultTableIndex`:  Allows you to query different tables within the PDF file. Index starts at `0`. 
+
+
+## Accessing Document Metadata Fields
+PDF files have a considerable amount of metadata which can be useful for analysis.  Drill will extract the following fields from every PDF file.  Note that these fields are not
+ projected in star queries and must be selected explicitly.  The document's creator populates these fields and some or all may be empty. With the exception of `_page_count
+ ` which is an `INT` and the two date fields, all the other fields are `VARCHAR` fields.","[{'comment': 'Nit: I think the newline gets included in the quoted bit.', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/README.md,"@@ -0,0 +1,67 @@
+# Format Plugin for PDF Table Reader
+One of the most annoying tasks is when you are working on a data science project and you get data that is in a PDF file. This plugin endeavours to enable you to query data in
+ PDF tables using Drill's SQL interface.  
+
+## Data Model
+Since PDF files generally are not intended to be queried or read by machines, mapping the data to tables and rows is not a perfect process.  The PDF reader does support 
+provided schema. 
+
+### Merging Pages
+The PDF reader reads tables from PDF files on each page.  If your PDF file has tables that span multiple pages, you can set the `combinePages` parameter to `true` and Drill 
+will merge all the tables in the PDF file.  You can also do this at query time with the `table()` function.
+
+## Configuration
+To configure the PDF reader, simply add the information below to the `formats` section of a file base storage plugin. 
+
+```json
+""pdf"": {
+  ""type"": ""pdf"",
+  ""extensions"": [
+    ""pdf""
+  ],
+  ""extractHeaders"": true,
+  ""combinePages"": false
+}
+```
+The avaialable options are:
+* `extractHeaders`: Extracts the first row of any tables as the header row.  If set to false, Drill will assign column names of `field_0`, `field_1` to each column.
+* `combinePages`: Merges multipage tables together.
+* `defaultTableIndex`:  Allows you to query different tables within the PDF file. Index starts at `0`. 
+
+
+## Accessing Document Metadata Fields
+PDF files have a considerable amount of metadata which can be useful for analysis.  Drill will extract the following fields from every PDF file.  Note that these fields are not
+ projected in star queries and must be selected explicitly.  The document's creator populates these fields and some or all may be empty. With the exception of `_page_count
+ ` which is an `INT` and the two date fields, all the other fields are `VARCHAR` fields.
+ 
+ The fields are:
+ * `_page_count`
+ * `_author`
+ * `_title`
+ * `_keywords`
+ * `_creator`
+ * `_producer`
+ * `_creation_date`
+ * `_modification_date`
+ * `_trapped`
+ * `_table_count`","[{'comment': 'Here, I assume `_author` will repeat for every table row? That `_page_count` will increase and gives the page number? Or, is constant and will always be, say, 25 if that\'s the number of pages?\r\n\r\nDoes `_table_count` stay fixed at the number of tables (5, say), or does it count tables (1, 2, 3, 4, 5)?\r\n\r\nWhat is `_trapped`? Is there some PDF standard we could point to where this stuff is defined?\r\n\r\nActually, there is a larger question. If I\'m exploring a pile of PDFs, I may want to extract metadata, such as the above. If I\'m extracting tables, I don\'t need the metadata repeated per table row.\r\n\r\nShould the plugin allow two query types? Either metadata or tables? Can this be done with some kind of Calcite trickery? `FROM ""foo.pdf""` means the tables, `FROM ""foo.pdf.metadata""` means the metadata?\r\n\r\nOr, we check and if we see only metadata columns, we return one row per file rather than one row per table row?\r\n\r\nWouldn\'t something like this be useful to extract metadata from a PDF with no tables?', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");
+    this.negotiator = negotiator;
+
+    split = negotiator.split();
+    errorContext = negotiator.parentErrorContext();
+    builder = new SchemaBuilder();
+
+    openFile();
+
+    // Get the tables
+    tables = Utils.extractTablesFromPDF(document);
+    populateMetadata();
+    logger.debug(""Found {} tables"", tables.size());","[{'comment': 'Think concurrency. Found {} tables in which file?', 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");
+    this.negotiator = negotiator;
+
+    split = negotiator.split();
+    errorContext = negotiator.parentErrorContext();
+    builder = new SchemaBuilder();
+
+    openFile();
+
+    // Get the tables
+    tables = Utils.extractTablesFromPDF(document);
+    populateMetadata();
+    logger.debug(""Found {} tables"", tables.size());
+
+    // Support provided schema
+    TupleMetadata schema = null;
+    if (this.negotiator.hasProvidedSchema()) {
+      schema = this.negotiator.providedSchema();
+      this.negotiator.tableSchema(schema, false);
+    } else {
+      this.negotiator.tableSchema(buildSchema(), false);
+    }
+    ResultSetLoader loader = this.negotiator.build();
+    rowWriter = loader.writer();
+
+    if (negotiator.hasProvidedSchema()) {
+      buildWriterListFromProvidedSchema(schema);
+    } else {
+      buildWriterList();
+    }
+    addImplicitColumnsToSchema();","[{'comment': ""I'd think you'd want to do this before creating the writer. Add columns to the schema before creating the vectors so that we get a simple picture before we start grinding the vector details."", 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");
+    this.negotiator = negotiator;
+
+    split = negotiator.split();
+    errorContext = negotiator.parentErrorContext();
+    builder = new SchemaBuilder();
+
+    openFile();
+
+    // Get the tables
+    tables = Utils.extractTablesFromPDF(document);","[{'comment': ""Does this scale? Get all data on open, cached in memory? In each of a dozen or more PDF readers in the same process? (This is why having this kind of plugin in the core is problematic: won't scale for PDFs with large numbers of tables.)"", 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");
+    this.negotiator = negotiator;
+
+    split = negotiator.split();
+    errorContext = negotiator.parentErrorContext();
+    builder = new SchemaBuilder();
+
+    openFile();
+
+    // Get the tables
+    tables = Utils.extractTablesFromPDF(document);
+    populateMetadata();
+    logger.debug(""Found {} tables"", tables.size());
+
+    // Support provided schema
+    TupleMetadata schema = null;
+    if (this.negotiator.hasProvidedSchema()) {
+      schema = this.negotiator.providedSchema();
+      this.negotiator.tableSchema(schema, false);
+    } else {
+      this.negotiator.tableSchema(buildSchema(), false);
+    }
+    ResultSetLoader loader = this.negotiator.build();
+    rowWriter = loader.writer();
+
+    if (negotiator.hasProvidedSchema()) {
+      buildWriterListFromProvidedSchema(schema);
+    } else {
+      buildWriterList();
+    }
+    addImplicitColumnsToSchema();
+
+    // Prepare for reading
+    currentRowIndex = 1;  // Skip the first line if there are headers
+    currentTable = tables.get(startingTableIndex);
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    System.setProperty(""java.awt.headless"", ""true"");","[{'comment': ""See comment above. We shouldn't be doing this!"", 'commenter': 'paul-rogers'}]"
2359,contrib/format-pdf/src/main/java/org/apache/drill/exec/store/pdf/PdfBatchReader.java,"@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.pdf;
+
+import org.apache.drill.common.types.TypeProtos.DataMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.pdfbox.pdmodel.PDDocument;
+import org.apache.pdfbox.pdmodel.PDDocumentInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import technology.tabula.RectangularTextContainer;
+import technology.tabula.Table;
+
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+
+public class PdfBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+
+  private static final Logger logger = LoggerFactory.getLogger(PdfBatchReader.class);
+  private static final String NEW_FIELD_PREFIX = ""field_"";
+  private final int maxRecords;
+
+  private final List<PdfColumnWriter> writers;
+  private FileSplit split;
+  private CustomErrorContext errorContext;
+  private RowSetLoader rowWriter;
+  private InputStream fsStream;
+  private PDDocument document;
+  private PdfReaderConfig config;
+
+  private SchemaBuilder builder;
+  private List<String> columnHeaders;
+  private int currentRowIndex;
+  private Table currentTable;
+  private int currentTableIndex;
+  private int startingTableIndex;
+  private FileScanFramework.FileSchemaNegotiator negotiator;
+
+  // Document Metadata Fields
+  private int pageCount;
+  private String title;
+  private String author;
+  private String subject;
+  private String keywords;
+  private String creator;
+  private String producer;
+  private Calendar creationDate;
+  private Calendar modificationDate;
+  private String trapped;
+  private int unregisteredColumnCount;
+  private int columns;
+  private int tableCount;
+  private int rows;
+  private int metadataIndex;
+
+
+  // Tables
+  private List<Table> tables;
+
+
+  static class PdfReaderConfig {
+    final PdfFormatPlugin plugin;
+
+    PdfReaderConfig(PdfFormatPlugin plugin) {
+      this.plugin = plugin;
+    }
+  }
+
+  public PdfBatchReader(PdfReaderConfig readerConfig, int maxRecords) {
+    this.maxRecords = maxRecords;
+    this.unregisteredColumnCount = 0;
+    this.writers = new ArrayList<>();
+    this.config = readerConfig;
+    this.startingTableIndex = readerConfig.plugin.getConfig().getDefaultTableIndex() < 0 ? 0 : readerConfig.plugin.getConfig().getDefaultTableIndex();
+    this.currentTableIndex = this.startingTableIndex;
+    this.columnHeaders = new ArrayList<>();
+  }
+
+  @Override
+  public boolean open(FileScanFramework.FileSchemaNegotiator negotiator) {
+    System.setProperty(""java.awt.headless"", ""true"");
+    this.negotiator = negotiator;
+
+    split = negotiator.split();
+    errorContext = negotiator.parentErrorContext();
+    builder = new SchemaBuilder();
+
+    openFile();
+
+    // Get the tables
+    tables = Utils.extractTablesFromPDF(document);
+    populateMetadata();
+    logger.debug(""Found {} tables"", tables.size());
+
+    // Support provided schema
+    TupleMetadata schema = null;
+    if (this.negotiator.hasProvidedSchema()) {
+      schema = this.negotiator.providedSchema();
+      this.negotiator.tableSchema(schema, false);
+    } else {
+      this.negotiator.tableSchema(buildSchema(), false);
+    }
+    ResultSetLoader loader = this.negotiator.build();
+    rowWriter = loader.writer();
+
+    if (negotiator.hasProvidedSchema()) {
+      buildWriterListFromProvidedSchema(schema);
+    } else {
+      buildWriterList();
+    }
+    addImplicitColumnsToSchema();
+
+    // Prepare for reading
+    currentRowIndex = 1;  // Skip the first line if there are headers
+    currentTable = tables.get(startingTableIndex);
+
+    return true;
+  }
+
+  @Override
+  public boolean next() {
+    System.setProperty(""java.awt.headless"", ""true"");
+
+    while(!rowWriter.isFull()) {
+      // Check to see if the limit has been reached
+      if (rowWriter.limitReached(maxRecords)) {","[{'comment': ""Is this for `LIMIT` pushdown? If so, I'd suggest we add it to the RSL itself as a new parameter. That way, the RSL knows to declare EOF and stop reading from the input once the limit is reached. It can also ignore any rows past the limit."", 'commenter': 'paul-rogers'}]"
2364,exec/java-exec/src/main/java/org/apache/drill/exec/physical/resultSet/impl/SingleVectorState.java,"@@ -221,13 +231,18 @@ protected void copyOverflow(int sourceStartIndex, int sourceEndIndex) {
         destMutator.set(newIndex, sourceAccessor.get(src) - offset);
       }
 
+      // Adjust offset vector length
+      int offsetLength = writer.rowStartIndex() + 1;
+      sourceVector.getMutator().setValueCount(offsetLength );
+
       // Getting offsets right was a pain. If you modify this code,
       // you'll likely relive that experience. Enabling the next two","[{'comment': 'next *three* lines', 'commenter': 'paul-rogers'}]"
2364,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/JSONFormatPlugin.java,"@@ -19,64 +19,82 @@
 
 import java.io.IOException;
 import java.io.OutputStream;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
-import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.exceptions.ExecutionSetupException;
 import org.apache.drill.common.expression.SchemaPath;
-import org.apache.drill.common.logical.FormatPluginConfig;
 import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.types.Types;
 import org.apache.drill.exec.ExecConstants;
 import org.apache.drill.exec.ops.FragmentContext;
 import org.apache.drill.exec.ops.QueryContext.SqlStatementType;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileReaderFactory;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileScanBuilder;
+import org.apache.drill.exec.physical.impl.scan.file.FileScanFramework.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;
 import org.apache.drill.exec.planner.common.DrillStatsTable;
 import org.apache.drill.exec.planner.common.DrillStatsTable.TableStatistics;
 import org.apache.drill.exec.proto.ExecProtos.FragmentHandle;
 import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.server.options.OptionSet;
 import org.apache.drill.exec.store.RecordReader;
 import org.apache.drill.exec.store.RecordWriter;
 import org.apache.drill.exec.store.StatisticsRecordWriter;
 import org.apache.drill.exec.store.dfs.DrillFileSystem;
 import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;
+import org.apache.drill.exec.store.dfs.easy.EasySubScan;
 import org.apache.drill.exec.store.dfs.easy.EasyWriter;
 import org.apache.drill.exec.store.dfs.easy.FileWork;
-import org.apache.drill.exec.store.easy.json.JSONFormatPlugin.JSONFormatConfig;
-import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;
-import org.apache.drill.shaded.guava.com.google.common.collect.Maps;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import com.fasterxml.jackson.annotation.JsonCreator;
-import com.fasterxml.jackson.annotation.JsonInclude;
-import com.fasterxml.jackson.annotation.JsonProperty;
-import com.fasterxml.jackson.annotation.JsonTypeName;
 import com.fasterxml.jackson.core.JsonFactory;
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.databind.ObjectMapper;
 
+import static org.apache.drill.exec.store.easy.json.JSONFormatConfig.PLUGIN_NAME;
+
 public class JSONFormatPlugin extends EasyFormatPlugin<JSONFormatConfig> {
 
   private static final Logger logger = LoggerFactory.getLogger(JSONFormatPlugin.class);
-  public static final String DEFAULT_NAME = ""json"";
-
   private static final boolean IS_COMPRESSIBLE = true;
 
-  public static final String OPERATOR_TYPE = ""JSON_SUB_SCAN"";
+  public static final String READER_OPERATOR_TYPE = ""JSON_SUB_SCAN"";
+  public static final String WRITER_OPERATOR_TYPE = ""JSON_WRITER"";
 
   public JSONFormatPlugin(String name, DrillbitContext context,
       Configuration fsConf, StoragePluginConfig storageConfig) {
     this(name, context, fsConf, storageConfig, new JSONFormatConfig(null));
   }
 
-  public JSONFormatPlugin(String name, DrillbitContext context,
-      Configuration fsConf, StoragePluginConfig config, JSONFormatConfig formatPluginConfig) {
-    super(name, context, fsConf, config, formatPluginConfig, true,
-          false, false, IS_COMPRESSIBLE, formatPluginConfig.getExtensions(), DEFAULT_NAME);
+  public JSONFormatPlugin(String name, DrillbitContext context, Configuration fsConf,
+      StoragePluginConfig config, JSONFormatConfig formatPluginConfig) {
+    super(name, easyConfig(fsConf, formatPluginConfig), context, config, formatPluginConfig);
+  }
+
+  private static EasyFormatConfig easyConfig(Configuration fsConf, JSONFormatConfig pluginConfig) {
+    return EasyFormatConfig.builder()
+      .readable(true)
+      .writable(true)
+      .blockSplittable(false)
+      .compressible(IS_COMPRESSIBLE)
+      .supportsProjectPushdown(true)
+      .extensions(pluginConfig.getExtensions())
+      .fsConf(fsConf)
+      .defaultName(PLUGIN_NAME)
+      .readerOperatorType(READER_OPERATOR_TYPE)
+      .writerOperatorType(WRITER_OPERATOR_TYPE)","[{'comment': 'Add the following:\r\n\r\n```java\r\n   // Temporary until V2 is the default.\r\n```', 'commenter': 'paul-rogers'}, {'comment': 'It is enabled by default. So what is temporary here?', 'commenter': 'vdiravka'}, {'comment': 'Sorry, I saw this in the PR description:\r\n\r\n> The new ""V2"" JSON scan is controlled by a new option:\r\n> store.json.enable_v2_reader, which is false by default in this PR.\r\n\r\nWhich I thought meant that the V2 reader is not enabled by default, hence the suggested comment.', 'commenter': 'paul-rogers'}]"
2364,exec/java-exec/src/main/resources/drill-module.conf,"@@ -692,6 +692,7 @@ drill.exec.options: {
     # Property name and value should be separated by =.
     # Properties should be separated by new line (\n).
     store.hive.conf.properties: """",
+    store.json.enable_v2_reader: true,","[{'comment': 'Do we want to use the V2 reader by default? I think this should be `false` for now.', 'commenter': 'paul-rogers'}, {'comment': 'yes, we want by default', 'commenter': 'vdiravka'}, {'comment': 'If so, then please change the PR description quoted above.', 'commenter': 'paul-rogers'}]"
2364,exec/java-exec/src/test/java/org/apache/drill/exec/store/json/TestJsonRecordReader.java,"@@ -231,21 +331,23 @@ public void testCountingQueryNotSkippingInvalidJSONRecords() throws Exception {
   /* Test for JSONReader */
   public void testNotCountingQuerySkippingInvalidJSONRecords() throws Exception {
     try {
-
       String set = ""alter session set `""
-          + ExecConstants.JSON_READER_SKIP_INVALID_RECORDS_FLAG + ""` = true"";
+        + ExecConstants.JSON_READER_SKIP_INVALID_RECORDS_FLAG + ""` = true"";
       String set1 = ""alter session set `""
-          + ExecConstants.JSON_READER_PRINT_INVALID_RECORDS_LINE_NOS_FLAG
-          + ""` = true"";
+        + ExecConstants.JSON_READER_PRINT_INVALID_RECORDS_LINE_NOS_FLAG
+        + ""` = true"";
       String query = ""select sum(balance) from cp.`jsoninput/drill4653/file.json`"";
       testNoResult(set);
       testNoResult(set1);
-      testBuilder().unOrdered().sqlQuery(query).sqlBaselineQuery(query).build()
-          .run();
+      testBuilder()
+        .unOrdered()
+        .sqlQuery(query)
+        .sqlBaselineQuery(query)
+        .go();
     }
     finally {
       String set = ""alter session set `""
-          + ExecConstants.JSON_READER_SKIP_INVALID_RECORDS_FLAG + ""` = false"";
+        + ExecConstants.JSON_READER_SKIP_INVALID_RECORDS_FLAG + ""` = false"";","[{'comment': 'There are some changes here and above that need to be copied over.', 'commenter': 'paul-rogers'}, {'comment': 'What changes?', 'commenter': 'vdiravka'}, {'comment': 'The code uses a method to do the alter session rather than building up a string. See [this file](https://github.com/paul-rogers/drill/blob/DRILL-6953-rev2/exec/java-exec/src/test/java/org/apache/drill/exec/store/json/TestJsonRecordReader.java#L283).', 'commenter': 'paul-rogers'}]"
2364,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestExtendedTypes.java,"@@ -26,11 +26,11 @@
 import org.apache.drill.test.BaseTestQuery;
 import org.apache.drill.exec.ExecConstants;
 import org.apache.drill.exec.rpc.user.QueryDataBatch;
-import org.junit.Assert;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
 public class TestExtendedTypes extends BaseTestQuery {
+","[{'comment': 'There is some cleanup in `TestComplexTypeWriter.java` to be copied across.', 'commenter': 'paul-rogers'}, {'comment': 'Spaces? Could you point me what cleanup do you mean?', 'commenter': 'vdiravka'}, {'comment': 'Interesting. Seems OK now. Maybe I read the diff backwards...', 'commenter': 'paul-rogers'}]"
2364,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestExtendedTypes.java,"@@ -78,10 +78,10 @@ public void testMongoExtendedTypes() throws Exception {
       List<QueryDataBatch> resultList = testSqlWithResults(String.format(""select * from dfs.`%s`"", originalFile));
       String actual = getResultString(resultList, "","");
       String expected = ""drill_timestamp_millies,bin,bin1\n2015-07-07 03:59:43.488,drill,drill\n"";
-      Assert.assertEquals(expected, actual);
+      assertEquals(expected, actual);","[{'comment': 'This file has some extended type tests to be copied over.', 'commenter': 'paul-rogers'}, {'comment': 'Do you mean to add test cases for every extended type from the file?', 'commenter': 'vdiravka'}, {'comment': 'Seems fine now.', 'commenter': 'paul-rogers'}]"
2364,exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonEscapeAnyChar.java,"@@ -80,6 +112,14 @@ private void resetJsonReaderEscapeAnyChar() {
     client.alterSession(ExecConstants.JSON_READER_ESCAPE_ANY_CHAR, false);
   }
 
+  private void enableV2Reader(boolean enable) {
+    client.alterSession(ExecConstants.ENABLE_V2_JSON_READER_KEY, enable);
+  }
+
+  private void resetV2Reader() {
+    client.resetSession(ExecConstants.ENABLE_V2_JSON_READER_KEY);
+  }
+","[{'comment': 'Changes in `TestJsonNanInf.java` need to be copied over.', 'commenter': 'paul-rogers'}, {'comment': 'What changes?', 'commenter': 'vdiravka'}]"
2387,contrib/storage-druid/README.md,"@@ -4,13 +4,13 @@ Drill druid storage plugin allows you to perform SQL queries against Druid datas
 This storage plugin is part of [Apache Drill](https://github.com/apache/drill)
 
 ### Tested with Druid version
-[0.16.0-incubating](https://github.com/apache/incubator-druid/releases/tag/druid-0.16.0-incubating)
+[0.22.0](https://github.com/apache/druid/releases/tag/druid-0.22.0)","[{'comment': ""Are there any versions of Druid that this will NOT work with?   If so, maybe say that this will work with Druid > n or whatever.... I'm not very knowledgable of Druid, so I don't really know if that is even relevant."", 'commenter': 'cgivre'}, {'comment': 'Druid 0.17.x to 0.19.x does not have the ""offset"" parametser in the scan query api, so paging is not possible. I thought it would be better to avoid all Druid Versions < 0.20.x for scan api instead of sacrifice pagination and reading all rows at once.\r\n\r\nThis means:\r\n\r\n- Drill < 1.20 works well with Druid <= 0.16 (select api)\r\n- Drill >= 1.20 will work with Druid >= 0.20 (scan api)\r\n- Druid 0.17 to 0.19 will not be supported by Drill because of missing offset parameter in scan api\r\n\r\nI mentioned this in the docs https://github.com/apache/drill-site/pull/18/files#diff-c47da4191a5cd392dabb624042159b63be990b216242e3da3491896378025e24R45 ', 'commenter': 'Z0ltrix'}, {'comment': 'Perfect.  Thx!', 'commenter': 'cgivre'}]"
2387,contrib/storage-druid/src/test/java/org/apache/drill/exec/store/druid/rest/DruidQueryClientTest.java,"@@ -90,23 +90,23 @@ public void executeQueryCalledNoResponsesFoundReturnsEmptyEventList()
         new ByteArrayInputStream(""[]"".getBytes(StandardCharsets.UTF_8.name()));
     when(httpEntity.getContent()).thenReturn(inputStream);
 
-    DruidSelectResponse response = druidQueryClient.executeQuery(QUERY);
+    DruidScanResponse response = druidQueryClient.executeQuery(QUERY);
     assertThat(response.getEvents()).isEmpty();
-    assertThat(response.getPagingIdentifiers()).isEmpty();
+    //assertThat(response.getPagingIdentifiers()).isEmpty();","[{'comment': 'Please remove commented out code. ', 'commenter': 'cgivre'}, {'comment': 'oh... yes definitely... missed that during coding', 'commenter': 'Z0ltrix'}, {'comment': 'Thx!', 'commenter': 'cgivre'}]"
2389,contrib/format-iceberg/src/main/java/org/apache/drill/exec/store/iceberg/plan/IcebergPluginImplementor.java,"@@ -99,6 +99,9 @@ public boolean canImplement(Filter filter) {
       try {
         TableScan tableScan = DrillRelOptUtil.findScan(filter.accept(SubsetRemover.INSTANCE));
         DrillTable drillTable = DrillRelOptUtil.getDrillTable(tableScan);","[{'comment': 'Please also add a null check for `tableScan` to fix https://issues.apache.org/jira/browse/DRILL-8058.', 'commenter': 'vvysotskyi'}, {'comment': 'But please use `Optional` to simplify the code with multiple consequent null checks.', 'commenter': 'vvysotskyi'}, {'comment': ""Good point. I'll give it a try."", 'commenter': 'luocooong'}]"
2389,contrib/format-iceberg/src/main/java/org/apache/drill/exec/store/iceberg/plan/IcebergPluginImplementor.java,"@@ -97,8 +98,13 @@ public boolean canImplement(Filter filter) {
 
     if (expression != null) {
       try {
-        TableScan tableScan = DrillRelOptUtil.findScan(filter.accept(SubsetRemover.INSTANCE));
-        DrillTable drillTable = DrillRelOptUtil.getDrillTable(tableScan);
+        TableScan tableScan;
+        DrillTable drillTable;
+        if (!Optional.ofNullable(tableScan = DrillRelOptUtil
+             .findScan(filter.accept(SubsetRemover.INSTANCE))).isPresent()
+            || !Optional.ofNullable(drillTable = DrillRelOptUtil.getDrillTable(tableScan)).isPresent()) {
+         return false;
+        }
         GroupScan scan = drillTable.getGroupScan();","[{'comment': 'I meant something like this...\r\n```suggestion\r\n        CheckedFunction<DrillTable, GroupScan, IOException> groupScanFunction = DrillTable::getGroupScan;\r\n        GroupScan scan = Optional.ofNullable(tableScan)\r\n          .map(DrillRelOptUtil::getDrillTable)\r\n          .map(groupScanFunction)\r\n          .orElse(null);\r\n```', 'commenter': 'vvysotskyi'}]"
2389,contrib/format-iceberg/src/main/java/org/apache/drill/exec/store/iceberg/plan/IcebergPluginImplementor.java,"@@ -144,7 +155,23 @@ public boolean splitProject(Project project) {
 
   @Override
   public boolean canImplement(Project project) {
-    return true;
+    RelTraitSet traitSet = project.getInput().getTraitSet();
+    if (!traitSet.isEmpty()) {
+      RelTrait trait = traitSet.get(0);
+      if (trait instanceof Convention) {
+        Convention c = (Convention) trait;
+        if (StringUtils.contains(c.getName(), IcebergFormatPlugin.ICEBERG_CONVENTION_PREFIX)) {
+          return true; // accept the iceberg queries
+        }
+      }
+    }
+    List<String> fields = project.getInput().getRowType().getFieldNames();
+    if (!fields.isEmpty()) {
+      if(StringUtils.equals(SchemaPath.DYNAMIC_STAR, fields.get(0))) {
+        return true; // accept the plan queries of iceberg
+      }
+    }
+    return false; // does not affect other format and storage","[{'comment': ""I don't think that we should rely on the convention of input rel node. It could have any convention, but the rule will be responsible for the conversion of input. Please see this line as an example: https://github.com/apache/drill/blob/ec76ad05680612b84147993d66312f040430cac0/exec/java-exec/src/main/java/org/apache/drill/exec/store/plan/rule/PluginProjectRule.java#L56. If no conversion of input happened, the planner will accept another plan where all children were converted."", 'commenter': 'vvysotskyi'}]"
2389,contrib/format-iceberg/src/test/resources/lateraljoin/multipleFiles/cust_order_10_1.json,"@@ -0,0 +1,2191 @@
+{","[{'comment': 'Please use files present in the java-exec module instead of introducing their copies...', 'commenter': 'vvysotskyi'}]"
2389,contrib/format-iceberg/src/test/java/org/apache/drill/exec/store/iceberg/IcebergIssuesTest.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.iceberg;
+
+import java.nio.file.Paths;
+
+import org.apache.drill.exec.planner.physical.PlannerSettings;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterFixtureBuilder;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class IcebergIssuesTest extends ClusterTest {
+
+  private static final String regularTestFile_1 = ""cust_order_10_1.json"";
+  private static final String regularTestFile_2 = ""cust_order_10_2.json"";
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    dirTestWatcher.copyResourceToRoot(Paths.get(""lateraljoin"", ""multipleFiles"", regularTestFile_1));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""lateraljoin"", ""multipleFiles"", regularTestFile_2));
+
+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher)
+        .sessionOption(PlannerSettings.ENABLE_UNNEST_LATERAL_KEY, true)
+        .maxParallelization(1);
+
+    startCluster(builder);
+  }
+
+  @Test
+  public void testDrill8058() throws Exception {","[{'comment': 'Please use the name that states what test checks instead of using ticket number...', 'commenter': 'vvysotskyi'}]"
2389,contrib/format-iceberg/src/test/java/org/apache/drill/exec/store/iceberg/IcebergIssuesTest.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.iceberg;
+
+import java.nio.file.Paths;
+
+import org.apache.drill.exec.planner.physical.PlannerSettings;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterFixtureBuilder;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class IcebergIssuesTest extends ClusterTest {
+
+  private static final String regularTestFile_1 = ""cust_order_10_1.json"";
+  private static final String regularTestFile_2 = ""cust_order_10_2.json"";
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    dirTestWatcher.copyResourceToRoot(Paths.get(""lateraljoin"", ""multipleFiles"", regularTestFile_1));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""lateraljoin"", ""multipleFiles"", regularTestFile_2));
+
+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher)
+        .sessionOption(PlannerSettings.ENABLE_UNNEST_LATERAL_KEY, true)
+        .maxParallelization(1);
+
+    startCluster(builder);
+  }
+
+  @Test
+  public void testDrill8058() throws Exception {
+    String sql = ""SELECT customer.c_name, avg(orders.o_totalprice) AS avgPrice "" +
+        ""FROM dfs.`lateraljoin/multipleFiles` customer, LATERAL "" +
+        ""(SELECT t.ord.o_totalprice as o_totalprice FROM UNNEST(customer.c_orders) t(ord) WHERE t.ord.o_totalprice > 100000 LIMIT 2) "" +
+        ""orders GROUP BY customer.c_name"";
+    runAndLog(sql);","[{'comment': 'Please add an actual check for results.', 'commenter': 'vvysotskyi'}]"
2401,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/oauth/AccessTokenAuthenticator.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.oauth;
+
+import lombok.NonNull;
+import okhttp3.Authenticator;
+import okhttp3.Request;
+import okhttp3.Response;
+import okhttp3.Route;
+import org.jetbrains.annotations.NotNull;","[{'comment': 'Is `NotNull` something different to `NonNull` that came in intentionally, or did it creep in from a typo?  There are occurrences in other files too.', 'commenter': 'jnturton'}]"
2401,exec/java-exec/src/main/resources/rest/storage/update.ftl,"@@ -133,6 +138,71 @@
       }
     });
 
+    $(""#getOauth"").click(function() {","[{'comment': 'I guess this JS should also be inside a `<#if model.getType() == ""HttpStoragePluginConfig"" >` block if it accesses `#getOauth`?', 'commenter': 'jnturton'}]"
2401,exec/jdbc-all/pom.xml,"@@ -562,7 +562,7 @@
                   This is likely due to you adding new dependencies to a java-exec and not updating the excludes in this module. This is important as it minimizes the size of the dependency of Drill application users.
 
                   </message>
-                  <maxsize>46600000</maxsize>
+                  <maxsize>50000000</maxsize>","[{'comment': ""I still don't completely understand this bit, but should the new dependencies rather be excluded from the jdbc-all module?"", 'commenter': 'jnturton'}, {'comment': ""Is there any reason for updating this limit? The functionality is only for HTTP plugin, so it shouldn't affect jdbc driver size."", 'commenter': 'vvysotskyi'}, {'comment': ""I'm not sure why this happened.  I did bump the version of `okhttp3` to 4.9.1 or whatever the latest version is, but there are no new dependencies. "", 'commenter': 'cgivre'}, {'comment': ""Before your change, okhttp3 was present only for test scope (and used only in some unit tests), and therefore it wasn't included in the JDBC driver. But you have changed its scope from the test to compile, so it is added to the driver. But the question here: is it required in JDBC?"", 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi Thanks for looking at this.  It actually is used in one of the files for Drill's internal rest API.  (See below). Does that need to be included in the JDBC driver?\r\n\r\nhttps://github.com/apache/drill/blob/d6c54936129f8e2a8eda83be7b1ab69998e893f8/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java#L206-L210"", 'commenter': 'cgivre'}, {'comment': ""I don't think it should be included, so please exclude it from the driver."", 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi Fixed!', 'commenter': 'cgivre'}, {'comment': '@vvysotskyi Unfortunately I tried excluding it from the JDBC and it does need to be there.', 'commenter': 'cgivre'}, {'comment': ""Unfortunately, I had to include them because there is an `OAuthUtils` class which uses OkHttp which is part of java-exec.  If you have some advice as to where else I could put that, I'm all ears.\r\n\r\nThe issue is that both the rest server and the http storage plugin needs to be able to access the same body of code for obtaining and refreshing tokens."", 'commenter': 'cgivre'}, {'comment': ""@cgivre I _think_ you include in java-exec's pom and also explcitly _exclude_ in the java-exec dependency section in jdbc-all's pom."", 'commenter': 'jnturton'}]"
2401,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -180,6 +193,48 @@ public Response enablePlugin(@PathParam(""name"") String name, @PathParam(""val"") B
     }
   }
 
+  @GET
+  @Path(""/storage/{name}/update_oath2_authtoken"")
+  @Produces(MediaType.TEXT_HTML)
+  public Response updateAuthToken(@PathParam(""name"") String name, @QueryParam(""code"") String code) {
+    try {
+      if (storage.getPlugin(name).getConfig().getClass().getSimpleName().equalsIgnoreCase(""HttpStoragePluginConfig"")) {
+        HttpStoragePluginConfig config = (HttpStoragePluginConfig)storage.getPlugin(name).getConfig();","[{'comment': ""I don't think adding http plugin related code to the exec module is a good idea."", 'commenter': 'vvysotskyi'}, {'comment': 'Can we do all these actions when creating the HTTP plugin?', 'commenter': 'vvysotskyi'}, {'comment': 'I refactored this so that it uses the `credentialProvider` to store the credentials. ', 'commenter': 'cgivre'}]"
2401,exec/java-exec/src/main/java/org/apache/drill/exec/store/http/HttpOAuthConfig.java,"@@ -0,0 +1,173 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;
+import lombok.Builder;
+import lombok.EqualsAndHashCode;
+import lombok.Getter;
+import lombok.Setter;
+import lombok.experimental.Accessors;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.drill.common.PlanStringBuilder;
+
+import java.util.Map;
+
+@Slf4j
+@Builder
+@Getter
+@Setter
+@Accessors(fluent = true)
+@EqualsAndHashCode
+@JsonInclude(JsonInclude.Include.NON_DEFAULT)
+@JsonDeserialize(builder = HttpOAuthConfig.HttpOAuthConfigBuilder.class)
+public class HttpOAuthConfig {
+
+  @JsonProperty(""baseURL"")
+  private final String baseURL;
+
+  @JsonProperty(""clientID"")
+  private final String clientID;
+
+  @JsonProperty(""clientSecret"")
+  private final String clientSecret;
+
+  @JsonProperty(""callbackURL"")
+  private final String callbackURL;
+
+  @JsonProperty(""authorizationURL"")
+  private final String authorizationURL;
+
+  @JsonProperty(""authorizationPath"")
+  private final String authorizationPath;
+
+  @JsonProperty(""authorizationParams"")
+  private final Map<String, String> authorizationParams;
+
+  @JsonProperty(""accessTokenPath"")
+  private final String accessTokenPath;
+
+  @JsonProperty(""generateCSRFToken"")
+  private final boolean generateCSRFToken;
+
+  @JsonProperty(""scope"")
+  private final String scope;
+
+  @JsonProperty(""tokens"")
+  private final Map<String, String> tokens;
+
+  /**
+   * Clone constructor used for updating tokens
+   * @param that The original oAuth configs
+   * @param tokens The updated tokens
+   */
+  public HttpOAuthConfig(HttpOAuthConfig that, Map<String, String> tokens) {
+    this.baseURL = that.baseURL;
+    this.clientID = that.clientID;
+    this.clientSecret = that.clientSecret;
+    this.callbackURL = that.callbackURL;
+    this.authorizationURL = that.authorizationURL;
+    this.authorizationPath = that.authorizationPath;
+    this.authorizationParams = that.authorizationParams;
+    this.accessTokenPath = that.accessTokenPath;
+    this.generateCSRFToken = that.generateCSRFToken;
+    this.scope = that.scope;
+    this.tokens = tokens;
+  }
+
+  private HttpOAuthConfig(HttpOAuthConfig.HttpOAuthConfigBuilder builder) {
+    this.baseURL = builder.baseURL;
+    this.clientID = builder.clientID;
+    this.clientSecret = builder.clientSecret;
+    this.callbackURL = builder.callbackURL;
+    this.authorizationURL = builder.authorizationURL;
+    this.authorizationPath = builder.authorizationPath;
+    this.authorizationParams = builder.authorizationParams;
+    this.accessTokenPath = builder.accessTokenPath;
+    this.generateCSRFToken = builder.generateCSRFToken;
+    this.scope = builder.scope;
+    this.tokens = builder.tokens;
+  }
+
+  @Override
+  public String toString() {
+    return new PlanStringBuilder(this)
+      .field(""baseURL"", baseURL)
+      .field(""clientID"", clientID)
+      .maskedField(""clientSecret"", clientSecret)
+      .field(""callbackURL"", callbackURL)
+      .field(""authorizationURL"", authorizationURL)
+      .field(""authorizationParams"", authorizationParams)
+      .field(""authorizationPath"", authorizationPath)
+      .field(""accessTokenPath"", accessTokenPath)
+      .field(""generateCSRFToken"", generateCSRFToken)
+      .field(""tokens"", tokens.keySet())
+      .toString();
+  }
+
+  @JsonPOJOBuilder(withPrefix = """")
+  public static class HttpOAuthConfigBuilder {
+    @Getter
+    @Setter","[{'comment': 'Instead of adding annotations for every field, you can add them to the class and they will be applied to every field.', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2401,contrib/storage-http/src/test/java/org/apache/drill/exec/store/http/oauth/TestOAuthAccessTokenRepository.java,"@@ -0,0 +1,168 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.http.oauth;
+
+import okhttp3.mockwebserver.MockResponse;
+import okhttp3.mockwebserver.MockWebServer;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.util.DrillFileUtils;
+import org.apache.drill.exec.store.http.HttpOAuthConfig;
+import org.apache.drill.exec.store.http.TestHttpPlugin;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.drill.shaded.guava.com.google.common.io.Files;
+
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.fail;
+
+public class TestOAuthAccessTokenRepository {","[{'comment': 'Please extend the test class from `BaseTest`.', 'commenter': 'vvysotskyi'}, {'comment': 'I actually removed this entire class.  Added different tests that better test the whole process. ', 'commenter': 'cgivre'}]"
2401,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/oauth/AccessTokenAuthenticator.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.oauth;
+
+import lombok.NonNull;
+import okhttp3.Authenticator;
+import okhttp3.Request;
+import okhttp3.Response;
+import okhttp3.Route;
+import org.jetbrains.annotations.NotNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class AccessTokenAuthenticator implements Authenticator {
+  private final static Logger logger = LoggerFactory.getLogger(AccessTokenAuthenticator.class);
+
+  private final AccessTokenRepository accessTokenRepository;
+
+  public AccessTokenAuthenticator(AccessTokenRepository accessTokenRepository) {
+    this.accessTokenRepository = accessTokenRepository;
+  }
+
+  @Override
+  public Request authenticate(Route route, @NotNull Response response) {","[{'comment': 'According to this logic, the access token will be regenerated for every `authenticate` call. So do we actually need to store it in the storage config and persist it to the zookeeper instead of holding it for a specific HTTP client?', 'commenter': 'vvysotskyi'}, {'comment': 'The way it should work is that it first tries the request with the access token that is stored in Drill.  If that request fails, it executes additional requests to obtain an updated token, and then re-executes the request.', 'commenter': 'cgivre'}]"
2401,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/oauth/AccessTokenRepository.java,"@@ -0,0 +1,149 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.oauth;
+
+import okhttp3.OkHttpClient.Builder;
+import okhttp3.OkHttpClient;
+import okhttp3.Request;
+
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.http.HttpOAuthConfig;
+import org.apache.drill.exec.store.http.HttpStoragePluginConfig;
+import org.apache.drill.exec.store.http.util.HttpProxyConfig;
+import org.apache.drill.exec.store.http.util.SimpleHttp;
+import org.apache.parquet.Strings;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Map;
+
+
+public class AccessTokenRepository {
+
+  private static final Logger logger = LoggerFactory.getLogger(AccessTokenRepository.class);
+  private String accessToken;
+  private final OkHttpClient client;
+  private final HttpOAuthConfig oAuthConfig;
+  private final StoragePluginRegistry registry;
+  private final HttpStoragePluginConfig pluginConfig;
+
+  public AccessTokenRepository(HttpOAuthConfig oAuthConfig,
+                               HttpProxyConfig proxyConfig,
+                               HttpStoragePluginConfig pluginConfig,
+                               StoragePluginRegistry registry) {
+    Builder builder = new OkHttpClient.Builder();
+    this.oAuthConfig = oAuthConfig;
+    this.registry = registry;
+    this.pluginConfig = pluginConfig;
+
+    if (oAuthConfig.tokens() != null && oAuthConfig.tokens().containsKey(""accessToken"")) {
+      accessToken = oAuthConfig.tokens().get(""accessToken"");
+    }
+
+    // Add proxy info
+    SimpleHttp.addProxyInfo(builder, proxyConfig);
+    client = builder.build();
+  }
+
+  /**
+   * Returns the current access token.  Does not perform an HTTP request.
+   * @return The current access token.
+   */
+  public String getAccessToken() {
+    logger.debug(""Getting Access token"");
+    if (accessToken == null) {
+      return refreshAccessToken();
+    }
+    return accessToken;
+  }
+
+  /**
+   * Refreshes the access token using the code and other information from the HTTP OAuthConfig.
+   * This executes a POST request.  This method will throw exceptions if any of the required fields
+   * are empty.  This plugin also updates the configuration in the storage plugin registry.
+   *
+   * In the event that a user submits a request and the access token is expired, the API will
+   * return a 401 non-authorized response.  In the event of a 401 response, the AccessTokenAuthenticator will
+   * create additional calls to obtain an updated token. This process should be transparent to the user.
+   *
+   * @return String of the new access token.
+   */
+  public String refreshAccessToken() {
+    Request request;
+    logger.debug(""Refreshing Access Token."");
+    validateKeys();
+
+    // If the refresh token is present process with that
+    if (oAuthConfig.tokens().containsKey(""refreshToken"") &&
+      StringUtils.isNotEmpty(oAuthConfig.tokens().get(""refreshToken""))) {
+      request = OAuthUtils.getAccessTokenRequestFromRefreshToken(oAuthConfig);
+    } else {
+      request = OAuthUtils.getAccessTokenRequest(oAuthConfig);
+    }
+
+    // Update/Refresh the tokens
+    Map<String, String> tokens = OAuthUtils.getOAuthTokens(client, request);
+    HttpOAuthConfig updatedConfig = new HttpOAuthConfig(oAuthConfig, tokens);
+
+    if (tokens.containsKey(""accessToken"")) {
+      accessToken = tokens.get(""accessToken"");
+    }
+
+    // This null check is here for testing only.  In actual Drill, the registry will not be null.
+    if (registry != null) {
+      OAuthUtils.updateOAuthTokens(registry, updatedConfig, pluginConfig);","[{'comment': 'Please see the previous comment regarding storing access tokens in ZK...', 'commenter': 'vvysotskyi'}, {'comment': 'This is completely refactored to store (and update) the creds in the `credentialProvider`', 'commenter': 'cgivre'}]"
2401,contrib/storage-http/OAuth.md,"@@ -0,0 +1,122 @@
+# OAuth2.0 Authentication in Drill
+Many APIs use OAuth2.0 as a means of authentication. Drill can connect to APIs that use OAuth2 for authentication but OAuth2 is significantly more complex than simple 
+username/password authentication.
+
+The good news, and bottom line here is that you can configure Drill to handle all this automagically, but you do have to understand how to configure it so that it works. First, 
+let's get a high level understanding of how OAuth works.  Click here to [skip to the section on configuring Drill](#configure-drill).
+
+### Understanding the OAuth2 Process
+There are many tutorials as to how OAuth works which we will not repeat here.  There are some slight variations but this is a good enough high level overview so you will understand the process works. 
+Thus, we will summarize the process as four steps:
+
+#### Step 1:  Obtain an Authorization Code
+For the first step, a user will have to log into the API's front end, and authorize the application to access the API.  The API will issue you a `clientID` and a 
+`client_secret`.  We will use these tokens in later stages.  
+
+You will also need to provide the API with a `callbackURL`.  This URL is how the API sends your application the `authorizationCode` which we will use in step 2.  
+Once you have the `clientID` and the `callbackURL`, your application will make a `GET` request to the API to obtain the `authorizationCode`. 
+
+#### Step 2:  Swap the Authorization Code for an Access Token
+At this point, we need to obtain the `accessToken`.  We do so by sending a `POST` request to the API with the `clientID`, the `clientSecret` and the `authorizationCode` we 
+obtained in step 1.  Note that the `authorizationCode` is a short lived token, but the `accessToken` lasts for a longer period.  When the access token expires, you may need to 
+either re-authorize the application or use a refresh token to obtain a new one.
+
+#### Step 3:  Call the Protected Resource with the Access Token
+Once you have the `accessToken` you are ready to make authenticated API calls. All you have to do here is add the `accessToken` to the API header and you can make API calls 
+just like any other. 
+
+#### Step 4: (Optional) Obtain a new Access Token using the Refresh Token
+Sometimes, the `accessToken` will expire.  When this happens, the API will respond with a `401` not authorized response. When this happens, the application will make a `POST` 
+request containing the `clientSecret`, the `clientID` and the `refreshToken` and will obtain new tokens.
+
+## The Artifacts
+Unlike simple username/password authentication, there are about 5 artifacts that you will need to authenticate using OAuth and it is also helpful to understand where they come 
+from and to whom they ""belong"".  Let's start with the artifacts that you will need to manually obtain from the API when you register your application:  (These are not the Drill 
+config variables, but the names are similar.  More on that later.)
+* `clientID`:  A token to uniquely identify your application with the API.
+* `clientSecret`:  A sort of password token which will be used to obtain additional tokens.
+* `callbackURL`:  The URL to which access and refresh tokens will be sent. You have to provide this URL when you register your application with the API.  If this does not match 
+  what you provide the API, the calls will fail.
+* `scope`:  An optional parameter which defines the scope of access request for the given access token. The API will provide examples, but you have to pick what accesses you 
+  are requesting.
+
+You will need to find two URLs in the API documentation:
+
+* `authorizationURL`:  This is the URL from which you will request the `authorizationCode`.  You should find this in the API documentation.
+* `tokenURL`: The URL from which you can request the `accessToken`. 
+
+There are two other artifacts that you will need, but these artifacts are generated by the API.  One thing to note is that while all the other artifacts are owned by the 
+application, these two are unique (and ""owned by"") the user.  These artifacts are:
+* `accessToken`: The token which is used to grant access to a protected resource
+* `refreshToken`: The token used to obtain a new `accessToken` without having to re-authorize the application.
+
+Currently, Drill does not allow per-user credentials.  However, it is possible to configure Drill to use the Vault Credential Provider so that each individual user has their ","[{'comment': ""I don't think this is the case..."", 'commenter': 'jnturton'}, {'comment': 'You are correct... :-)', 'commenter': 'cgivre'}]"
2401,logical/src/main/java/org/apache/drill/common/logical/security/CredentialsProvider.java,"@@ -33,6 +37,16 @@
    * Returns map with authentication credentials. Key is the credential name, for example {@code ""username""}
    * and map value is corresponding credential value.
    */
+  static final Logger logger = LoggerFactory.getLogger(CredentialsProvider.class);
+
   @JsonIgnore
   Map<String, String> getCredentials();
+
+  @JsonIgnore
+  default void updateCredentials(String key, String value) throws UserException {
+    throw UserException.internalError()
+      .message(""Update credential function not implemented for "" + Id.NAME)
+      .build(logger);
+  }","[{'comment': '```suggestion\r\n  /**\r\n    * Set an ephemeral credential.  Implementations are not expected to write this\r\n    * value to persistent storage.\r\n    */\r\n  @JsonIgnore\r\n  void setCredential(String name, String value);\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2401,exec/java-exec/src/main/java/org/apache/drill/exec/store/security/CredentialProviderUtils.java,"@@ -43,4 +43,59 @@ public static CredentialsProvider getCredentialsProvider(
     }
     return new PlainCredentialsProvider(mapBuilder.build());
   }
+
+  /**
+   * Constructor for OAuth based authentication.  Allows for tokens to be stored in whatever vault","[{'comment': 'I think we can avoid having all of this OAuth-specific code show up here like this, by a universally-supported `setCredential()`', 'commenter': 'jnturton'}]"
2401,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -180,6 +194,47 @@ public Response enablePlugin(@PathParam(""name"") String name, @PathParam(""val"") B
     }
   }
 
+  @GET
+  @Path(""/storage/{name}/update_oath2_authtoken"")
+  @Produces(MediaType.TEXT_HTML)
+  public Response updateAuthToken(@PathParam(""name"") String name, @QueryParam(""code"") String code) {
+    try {
+      if (storage.getPlugin(name).getConfig().getClass().getSimpleName().equalsIgnoreCase(""HttpStoragePluginConfig"")) {","[{'comment': ""Let's remove this check for `HttpStoragePluginConfig`, and add only check that plugin config is an instance of `AbstractSecuredStoragePluginConfig`. Theoretically, there might be other plugins that might want to use this functionality."", 'commenter': 'vvysotskyi'}]"
2401,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -62,6 +72,10 @@
 @RolesAllowed(ADMIN_ROLE)
 public class StorageResources {
   private static final Logger logger = LoggerFactory.getLogger(StorageResources.class);
+  private static final String OAUTH_SUCCESS = ""<html>\n"" + ""<head>\n"" + ""  <title>Success!</title>\n"" + ""  <link rel=\""shortcut icon\"" href=\""/static/img/drill.ico\"">\n"" + ""  "" +","[{'comment': 'Please move to the file and use ftl template as it is done for all other UI stuff.', 'commenter': 'vvysotskyi'}]"
2406,distribution/src/main/resources/drill-config.sh,"@@ -213,6 +213,9 @@ fi
 # Set Drill-provided defaults here. Do not put Drill defaults
 # in the distribution or user environment config files.
 
+# Prefer IPv4 over IPv6
+export DRILL_JAVA_OPTS=${$DRILL_JAVA_OPTS:-"" -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv6Addresses=false""}","[{'comment': 'Please use the `DRILLBIT_OPTS` variable below instead of `DRILL_JAVA_OPTS` and move it to the part where some other specific properties are set, like `ReservedCodeCacheSize` and so on.', 'commenter': 'vvysotskyi'}, {'comment': 'Isnt this important for both... drillbit and sqlline? \r\n\r\nAnd i thought this has to be done before including drill-env.sh ?', 'commenter': 'Z0ltrix'}, {'comment': 'ah, i see... there is also fine but this will make it impossible for the user to overwrite it in drill-env.sh ', 'commenter': 'Z0ltrix'}]"
2414,contrib/storage-http/README.md,"@@ -4,7 +4,7 @@ The HTTP storage plugin lets you query APIs over HTTP/REST. The plugin
 expects JSON responses.
 
 The HTTP plugin is new in Drill 1.18 and is an Alpha feature. It works well, and we","[{'comment': 'Should we drop this disclaimer now?  Drill 1.18 was a long time ago...', 'commenter': 'jnturton'}, {'comment': 'Dropped.', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/Pagination.md,"@@ -0,0 +1,38 @@
+# Auto Pagination in Drill
+Remote APIs frequently implement some sort of pagination as a way of limiting results.  However, if you are performing bulk data analysis, it is necessary to reassemble the 
+data into one larger dataset.  Drill's auto-pagination features allow this to happen in the background, so that the user will get clean data back.
+
+To use a paginator, you simply have to configure the paginator in the connection for the particular API.  
+
+## Offset Pagination
+Offset Pagination uses commands similar to SQL which has a `LIMIT` and an `OFFSET`.  With an offset paginator, let's say you want 200 records and the maximum page size is 50 
+records, the offset paginator will break up your query into 4 requests as shown below:
+
+* myapi.com?limit=50&offset=0
+* myapi.com?limit=50?offset=50
+* myapi.com?limit=50&offset=100
+* myapi.com?limit=50&offset=150
+
+### Configuring Offset Pagination
+To configure an offset paginator, simply add the following to the configuration for your connection. 
+
+```json
+""paginator"": {
+   ""limitField"": ""<limit>"",","[{'comment': 'I found it confusing that the limit/offset/page config vars are called fooField, while they refer to query parameters.  For me, fooParm or even fooQueryParm would be a lot clearer because we commonly use the word field to refer to a column of data.', 'commenter': 'jnturton'}, {'comment': 'I changed all the `fields` to `fooParam` here and elsewhere.  I also changed `maxPageSize` to `pageSize. ', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/README.md,"@@ -278,6 +278,21 @@ If the `authType` is set to `basic`, `username` and `password` must be set in th
 
 `password`: The password for basic authentication.
 
+#### Limiting Results
+Some APIs contain a field which is used to limit the number of results returned by the API.  If such a field is known, you can set the `limitField` parameter to this field name","[{'comment': '```suggestion\r\nSome APIs support a query parameter which is used to limit the number of results returned by the API.  In this case you can set the `limitQueryParm` config variable to the query parameter name\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/README.md,"@@ -278,6 +278,21 @@ If the `authType` is set to `basic`, `username` and `password` must be set in th
 
 `password`: The password for basic authentication.
 
+#### Limiting Results
+Some APIs contain a field which is used to limit the number of results returned by the API.  If such a field is known, you can set the `limitField` parameter to this field name
+ and Drill will automatically include this in your query.  For instance, if you have an API which has a limit field called `maxRecords`, if you set this option, and execute the","[{'comment': '```suggestion\r\n and Drill will automatically include this in your query.  For instance, if you have an API which supports a limit query parameter called `maxRecords` and you set the abovementioned config variable then execute the\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/README.md,"@@ -278,6 +278,21 @@ If the `authType` is set to `basic`, `username` and `password` must be set in th
 
 `password`: The password for basic authentication.
 
+#### Limiting Results
+Some APIs contain a field which is used to limit the number of results returned by the API.  If such a field is known, you can set the `limitField` parameter to this field name
+ and Drill will automatically include this in your query.  For instance, if you have an API which has a limit field called `maxRecords`, if you set this option, and execute the
+  following query:
+  
+```sql
+SELECT <fields>
+FROM api.limitedApi
+LIMIT 10 
+```  
+Drill will send the following query to your API:","[{'comment': '```suggestion\r\nDrill will send the following request to your API:\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpApiConfig.java,"@@ -101,6 +100,8 @@
   @JsonProperty
   private final int xmlDataLevel;
   @JsonProperty
+  private final String limitField;","[{'comment': '```suggestion\r\n  private final String limitQueryParm;\r\n```', 'commenter': 'jnturton'}, {'comment': ""(Etc. I won't comment on any more occurrences of this.)"", 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpBatchReader.java,"@@ -72,6 +88,7 @@ public boolean open(SchemaNegotiator negotiator) {
         .getString(ExecConstants.DRILL_TMP_DIR);
 
     HttpUrl url = buildUrl();
+    logger.debug(""Final URL: {}"", url.toString());","[{'comment': '(Micro optimisation) Can we drop the toString() here?  It will be invoked needlessly when the logging threshold is set higher than DEBUG.', 'commenter': 'jnturton'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpBatchReader.java,"@@ -216,13 +246,15 @@ protected HttpProxyConfig proxySettings(Config drillConfig, HttpUrl url) {
 
   @Override
   public boolean next() {
-    recordCount++;
+    boolean result = jsonLoader.readBatch();
 
-    // Stop after the limit has been reached
-    if (maxRecords >= 1 && recordCount > maxRecords) {
-      return false;
+    // Allows limitless pagination.
+    if (paginator != null &&
+      maxRecords < 0 && (resultSetLoader.totalRowCount()) < paginator.getMaxPageSize()) {
+      logger.debug(""Ending pagination: Pages too small"");","[{'comment': '```suggestion\r\n      logger.debug(""Partially filled page received, ending pagination"");\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/OffsetPaginator.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.apache.drill.common.exceptions.UserException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class OffsetPaginator extends Paginator {
+
+  private static final Logger logger = LoggerFactory.getLogger(OffsetPaginator.class);
+
+  private final int limit;
+  private final String limitField;
+  private final String offsetField;
+  private int offset;
+
+  /**
+   * This class implements the idea of an Offset Paginator. See here for complete explanation:
+   * https://nordicapis.com/everything-you-need-to-know-about-api-pagination/
+   * <p>
+   *
+   * @param builder     The okhttp3 URL builder which has the API root URL
+   * @param limit       The limit clause from the sql query
+   * @param maxPageSize The maximum page size the API documentation","[{'comment': '```suggestion\r\n   * @param pageSize The number of results in a single page returned by the API\r\n```\r\nTo me this is just a ""page size"", rather than a ""max page size"".  The final page of results from a call need not be full, but that doesn\'t change its fixed size.', 'commenter': 'jnturton'}, {'comment': 'I renamed this everywhere to `pageSize`. ', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/OffsetPaginator.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.apache.drill.common.exceptions.UserException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class OffsetPaginator extends Paginator {
+
+  private static final Logger logger = LoggerFactory.getLogger(OffsetPaginator.class);
+
+  private final int limit;
+  private final String limitField;
+  private final String offsetField;
+  private int offset;
+
+  /**
+   * This class implements the idea of an Offset Paginator. See here for complete explanation:
+   * https://nordicapis.com/everything-you-need-to-know-about-api-pagination/
+   * <p>
+   *
+   * @param builder     The okhttp3 URL builder which has the API root URL
+   * @param limit       The limit clause from the sql query
+   * @param maxPageSize The maximum page size the API documentation
+   * @param limitField  The field name which corresponds to the limit field from the API
+   * @param offsetField The field name which corresponds to the offset field from the API
+   */
+  public OffsetPaginator(Builder builder, int limit, int maxPageSize, String limitField, String offsetField) {
+    super(builder, paginationMode.OFFSET, maxPageSize, limit > 0);
+    this.limit = limit;
+    this.limitField = limitField;
+    this.offsetField = offsetField;
+    this.paginatedUrls = buildPaginatedURLs();
+    this.offset = 0;
+
+    // Page size must be greater than zero
+    if (maxPageSize <= 0) {
+      throw UserException
+              .validationError()
+              .message(""API limit cannot be zero"")
+              .build(logger);
+    }
+  }
+
+  public int getLimit() {
+    return limit;
+  }
+
+  @Override
+  public String next() {
+    if (hasLimit) {
+      return super.next();
+    } else {
+      return generateNextUrl();
+    }
+  }
+
+  @Override
+  public String generateNextUrl() {
+    builder.removeAllEncodedQueryParameters(offsetField);
+    builder.removeAllEncodedQueryParameters(limitField);
+
+    builder.addQueryParameter(offsetField, String.valueOf(offset));
+    builder.addQueryParameter(limitField, String.valueOf(maxPageSize));
+    offset += maxPageSize;
+
+    return builder.build().url().toString();
+  }
+
+
+  /**
+   * Build the paginated URLs.  If the parameters are invalid, return a list with the original URL.
+   *
+   * @return List of paginated URLs
+   */
+  @Override
+  public List<HttpUrl> buildPaginatedURLs() {
+    this.paginatedUrls = new ArrayList<>();
+    // If user wants 1000 records, and the max page size is 100, we need to send 10 requests","[{'comment': '```suggestion\r\n    // If user wants 1000 records, and the page size is 100, we need to send 10 requests\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/OffsetPaginator.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.apache.drill.common.exceptions.UserException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class OffsetPaginator extends Paginator {
+
+  private static final Logger logger = LoggerFactory.getLogger(OffsetPaginator.class);
+
+  private final int limit;
+  private final String limitField;
+  private final String offsetField;
+  private int offset;
+
+  /**
+   * This class implements the idea of an Offset Paginator. See here for complete explanation:
+   * https://nordicapis.com/everything-you-need-to-know-about-api-pagination/
+   * <p>
+   *
+   * @param builder     The okhttp3 URL builder which has the API root URL
+   * @param limit       The limit clause from the sql query
+   * @param maxPageSize The maximum page size the API documentation
+   * @param limitField  The field name which corresponds to the limit field from the API
+   * @param offsetField The field name which corresponds to the offset field from the API
+   */
+  public OffsetPaginator(Builder builder, int limit, int maxPageSize, String limitField, String offsetField) {
+    super(builder, paginationMode.OFFSET, maxPageSize, limit > 0);
+    this.limit = limit;
+    this.limitField = limitField;
+    this.offsetField = offsetField;
+    this.paginatedUrls = buildPaginatedURLs();
+    this.offset = 0;
+
+    // Page size must be greater than zero
+    if (maxPageSize <= 0) {
+      throw UserException
+              .validationError()
+              .message(""API limit cannot be zero"")","[{'comment': '```suggestion\r\n              .message(""API page size must be positive"")\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/PagePaginator.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.apache.drill.common.exceptions.UserException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class PagePaginator extends Paginator {
+
+  private static final Logger logger = LoggerFactory.getLogger(OffsetPaginator.class);
+
+  private final int limit;
+  private final String pageField;
+  private final String pageSizeField;
+  private int currentPage;
+
+
+  /**
+   * The Page Paginator works similarly to the offset paginator.  It requires the user to supply a page number,
+   * a page size variable, and a maximum page size.
+   * @param builder
+   * @param limit
+   * @param maxPageSize
+   * @param pageField
+   * @param pageSizeField
+   */
+  public PagePaginator(Builder builder, int limit, int maxPageSize, String pageField, String pageSizeField) {","[{'comment': '```suggestion\r\n  public PagePaginator(Builder builder, int limit, int pageSize, String pageNumQueryParm, String pageSizeQueryParm) {\r\n```', 'commenter': 'jnturton'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/PagePaginator.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.apache.drill.common.exceptions.UserException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class PagePaginator extends Paginator {
+
+  private static final Logger logger = LoggerFactory.getLogger(OffsetPaginator.class);
+
+  private final int limit;
+  private final String pageField;
+  private final String pageSizeField;
+  private int currentPage;
+
+
+  /**
+   * The Page Paginator works similarly to the offset paginator.  It requires the user to supply a page number,
+   * a page size variable, and a maximum page size.","[{'comment': '```suggestion\r\n   * a page number query parameter, and a page size.\r\n```', 'commenter': 'jnturton'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/Paginator.java,"@@ -0,0 +1,131 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.List;
+
+/**
+ * This class is the abstraction for the Paginator class.  There are
+ * different pagination methods, however, all essentially require the query
+ * engine to generate URLs to retrieve the next batch of data and also
+ * to determine whether the URL has more data.
+ *
+ * The Offset and Page paginators work either with a limit or without, but function
+ * slightly differently.  If a limit is specified and pushed down, the paginator will
+ * generate a list of URLs with the appropriate pagination parameters.  In the future
+ * this could be parallelized, however in the V1 all these requests are run in series.
+ */
+public abstract class Paginator {
+
+  private static final Logger logger = LoggerFactory.getLogger(Paginator.class);
+  private static final int MAX_ATTEMPTS = 100;
+  protected final int maxPageSize;","[{'comment': '```suggestion\r\n  protected final int pageSize;\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed. Here and elsewhere. ', 'commenter': 'cgivre'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/Paginator.java,"@@ -0,0 +1,131 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.List;
+
+/**
+ * This class is the abstraction for the Paginator class.  There are
+ * different pagination methods, however, all essentially require the query
+ * engine to generate URLs to retrieve the next batch of data and also
+ * to determine whether the URL has more data.
+ *
+ * The Offset and Page paginators work either with a limit or without, but function
+ * slightly differently.  If a limit is specified and pushed down, the paginator will
+ * generate a list of URLs with the appropriate pagination parameters.  In the future
+ * this could be parallelized, however in the V1 all these requests are run in series.
+ */
+public abstract class Paginator {
+
+  private static final Logger logger = LoggerFactory.getLogger(Paginator.class);
+  private static final int MAX_ATTEMPTS = 100;
+  protected final int maxPageSize;
+  private boolean hasMore;
+
+  public enum paginationMode {
+    OFFSET,
+    PAGE
+  }
+
+  protected final boolean hasLimit;
+  protected final paginationMode MODE;
+  protected int index;
+  protected Builder builder;
+  protected List<HttpUrl> paginatedUrls;
+
+
+  public Paginator(Builder builder, paginationMode mode, int maxPageSize, boolean hasLimit) {
+    this.MODE = mode;
+    this.builder = builder;
+    this.maxPageSize = maxPageSize;
+    this.hasLimit = hasLimit;
+    hasMore = true;
+    index = 0;
+  }
+
+  public void setBuilder(Builder builder) {
+    this.builder = builder;
+  }
+
+  public abstract List<HttpUrl> buildPaginatedURLs();
+
+  /**
+   * This method is to be used when the user does not include a limit in the query
+   * In each paginator, the paginator tracks whether there is more data.  If there is
+   * more data, the paginator marks the hasMore variable false.
+   * @return
+   */
+  public abstract String generateNextUrl();
+
+  public List<HttpUrl> getPaginatedUrls() {
+    return this.paginatedUrls;
+  }
+
+  /**
+   * This method is used in pagination queries when no limit is present.  The intended
+   * flow is that if no limit is present, if the batch reader encounters a page which has
+   * less data than the max page size, the batch reader should call this method to stop","[{'comment': '```suggestion\r\n   * less data than the page size, the batch reader should call this method to stop\r\n```', 'commenter': 'jnturton'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/OffsetPaginator.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.apache.drill.common.exceptions.UserException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class OffsetPaginator extends Paginator {
+
+  private static final Logger logger = LoggerFactory.getLogger(OffsetPaginator.class);
+
+  private final int limit;
+  private final String limitField;
+  private final String offsetField;
+  private int offset;
+
+  /**
+   * This class implements the idea of an Offset Paginator. See here for complete explanation:
+   * https://nordicapis.com/everything-you-need-to-know-about-api-pagination/
+   * <p>
+   *
+   * @param builder     The okhttp3 URL builder which has the API root URL
+   * @param limit       The limit clause from the sql query
+   * @param maxPageSize The maximum page size the API documentation
+   * @param limitField  The field name which corresponds to the limit field from the API
+   * @param offsetField The field name which corresponds to the offset field from the API
+   */
+  public OffsetPaginator(Builder builder, int limit, int maxPageSize, String limitField, String offsetField) {
+    super(builder, paginationMode.OFFSET, maxPageSize, limit > 0);
+    this.limit = limit;
+    this.limitField = limitField;
+    this.offsetField = offsetField;
+    this.paginatedUrls = buildPaginatedURLs();
+    this.offset = 0;
+
+    // Page size must be greater than zero
+    if (maxPageSize <= 0) {
+      throw UserException
+              .validationError()
+              .message(""API limit cannot be zero"")
+              .build(logger);
+    }
+  }
+
+  public int getLimit() {
+    return limit;
+  }
+
+  @Override
+  public String next() {
+    if (hasLimit) {
+      return super.next();
+    } else {
+      return generateNextUrl();
+    }
+  }
+
+  @Override
+  public String generateNextUrl() {
+    builder.removeAllEncodedQueryParameters(offsetField);
+    builder.removeAllEncodedQueryParameters(limitField);
+
+    builder.addQueryParameter(offsetField, String.valueOf(offset));
+    builder.addQueryParameter(limitField, String.valueOf(maxPageSize));
+    offset += maxPageSize;
+
+    return builder.build().url().toString();
+  }
+
+
+  /**
+   * Build the paginated URLs.  If the parameters are invalid, return a list with the original URL.
+   *
+   * @return List of paginated URLs
+   */
+  @Override
+  public List<HttpUrl> buildPaginatedURLs() {
+    this.paginatedUrls = new ArrayList<>();","[{'comment': 'I think this should be a method-local variable, not the object member variable which gets assigned to the result of this method', 'commenter': 'jnturton'}]"
2414,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/paginator/Paginator.java,"@@ -0,0 +1,131 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.paginator;
+
+import okhttp3.HttpUrl;
+import okhttp3.HttpUrl.Builder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.List;
+
+/**
+ * This class is the abstraction for the Paginator class.  There are
+ * different pagination methods, however, all essentially require the query
+ * engine to generate URLs to retrieve the next batch of data and also
+ * to determine whether the URL has more data.
+ *
+ * The Offset and Page paginators work either with a limit or without, but function
+ * slightly differently.  If a limit is specified and pushed down, the paginator will
+ * generate a list of URLs with the appropriate pagination parameters.  In the future
+ * this could be parallelized, however in the V1 all these requests are run in series.
+ */
+public abstract class Paginator {","[{'comment': 'Did you consider `class Paginator implements java.lang.Iterable`?  Looks like it might fit...', 'commenter': 'jnturton'}, {'comment': 'I did try this, but it forces the downstream paginators to implement the `next` and `hasMore` whereas without the `Iterable` those methods can be contained in the `Pagintaor` class.  Does that make sense?', 'commenter': 'cgivre'}]"
2416,exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/impl/StringFunctions.java,"@@ -416,16 +417,25 @@ public void setup() {
 
     @Override
     public void eval() {
-      if (index.value < 1) {
+      if (index.value == 0) {","[{'comment': 'Sigh, 1-based indexing.  I think the logic you chose in this PR for non-positive index values is the best we can do, considering.', 'commenter': 'jnturton'}, {'comment': '> Sigh, 1-based indexing. I think the logic you chose in this PR for non-positive index values is the best we can do, considering.\r\n@jnturton Hi James, anything wrong here?', 'commenter': 'Leon-WTF'}]"
2416,exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/impl/StringFunctions.java,"@@ -416,16 +417,25 @@ public void setup() {
 
     @Override
     public void eval() {
-      if (index.value < 1) {
+      if (index.value == 0) {
         throw org.apache.drill.common.exceptions.UserException.functionError()
-          .message(""Index in split_part must be positive, value provided was ""
-            + index.value).build();
+          .message(""Index in split_part can not be zero"").build();
       }
       String inputString = org.apache.drill.exec.expr.fn.impl.
         StringFunctionHelpers.getStringFromVarCharHolder(in);
-      int arrayIndex = index.value - 1;
-      String result =
-              (String) com.google.common.collect.Iterables.get(splitter.split(inputString), arrayIndex, """");
+      String result = """";
+      if (index.value < 0) {
+        java.util.List<String> splits = splitter.splitToList(inputString);
+        int size = splits.size();
+        int arrayIndex = size + index.value;
+        if (arrayIndex >= 0) {","[{'comment': 'What happens when arrayIndex < 0 here?  An informative error would be better than uninitialised result data', 'commenter': 'jnturton'}, {'comment': '> What happens when arrayIndex < 0 here? An informative error would be better than uninitialised result data\r\n\r\nI think arrayIndex < 0 is not an error, it will return the empty string, see the case below in UT.\r\n    testBuilder()\r\n      .sqlQuery(""select split_part(\'a,b,c\', \',\', -4) res1 from (values(1))"")\r\n      .ordered()\r\n      .baselineColumns(""res1"")\r\n      .baselineValues("""")\r\n      .go();\r\nSame as:\r\n    testBuilder()\r\n        .sqlQuery(""select split_part(\'a,b,c\', \',\', 4) res1 from (values(1))"")\r\n        .ordered()\r\n        .baselineColumns(""res1"")\r\n        .baselineValues("""")\r\n        .go();', 'commenter': 'Leon-WTF'}, {'comment': 'Okay that does actually appear to be consistent with what happens when arrayIndex is past the end of the array.', 'commenter': 'jnturton'}, {'comment': 'For performance we try to avoid branching inside function `eval` methods whenever possible.  Can any of these `if` statements be removed?  E.g. by using modular arithmetic to calculate the wrapped array index.', 'commenter': 'jnturton'}, {'comment': 'Perhaps in the context of this sort of string processing, branching penalities are not very significant 🤔', 'commenter': 'jnturton'}]"
2429,exec/jdbc-all/pom.xml,"@@ -874,6 +874,42 @@
         </plugins>
       </build>
     </profile>
+    <profile>
+      <id>hadoop-2</id>
+      <build>
+      <plugins>
+        <plugin>
+          <groupId>org.apache.maven.plugins</groupId>
+          <artifactId>maven-enforcer-plugin</artifactId>
+          <executions>
+            <execution>
+              <id>enforce-jdbc-jar-compactness</id>
+              <goals>
+                <goal>enforce</goal>
+              </goals>
+              <phase>verify</phase>
+              <configuration>
+                <rules>
+                  <requireFilesSize>
+                    <message>
+                      The file drill-jdbc-all-${project.version}.jar is outside the expected size range.
+                      This is likely due to you adding new dependencies to a java-exec and not updating the excludes in this module. This is important as it minimizes the size of the dependency of Drill application users.
+                    </message>
+                    <maxsize>47600000</maxsize>","[{'comment': 'If plugin configs have only different `maxsize`, you can define it as a property and set that property value in the profile instead of providing plugin configs.', 'commenter': 'vvysotskyi'}, {'comment': 'Sounds like something that we should make a property anyway.', 'commenter': 'jnturton'}, {'comment': 'Could we add something to the developer docs as well?  ', 'commenter': 'cgivre'}, {'comment': 'Agree with all. Property is added and `## drill-jdbc-all JAR maxsize` doc header is added in `docs/dev/Maven.md`', 'commenter': 'vdiravka'}]"
2431,contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/writers/primitive/HiveTimestampWriter.java,"@@ -33,10 +34,14 @@ public HiveTimestampWriter(PrimitiveObjectInspector inspector, TimeStampWriter w
 
   @Override
   public void write(Object value) {
-    String timestampString = PrimitiveObjectInspectorUtils.getString(value, inspector);
-    long timestampMillis = new DateTime(Timestamp.valueOf(timestampString).getTime())
-        .withZoneRetainFields(DateTimeZone.UTC).getMillis();
-    writer.writeTimeStamp(timestampMillis);
+    if (value instanceof LongWritable) {
+      writer.writeTimeStamp(((LongWritable) value).get() / 1000);","[{'comment': '```suggestion\r\n      writer.writeTimeStamp(((LongWritable) value).get() / DateTimeConstants.MILLIS_PER_SECOND);\r\n```', 'commenter': 'vdiravka'}, {'comment': '@vdiravka \r\nThank you for your suggestion ~', 'commenter': 'cdmikechen'}]"
2432,pom.xml,"@@ -104,7 +104,7 @@
     <license.skip>true</license.skip>
     <docker.repository>apache/drill</docker.repository>
     <antlr.version>4.8-1</antlr.version>
-    <maven.version>3.6.3</maven.version>
+    <maven.version>3.8.4</maven.version>","[{'comment': 'Could you try to build the project with this version? Is that used for current CI builds?', 'commenter': 'vdiravka'}, {'comment': '@vdiravka we have set GitHub ci to use `ubuntu-latest` which at the moment means Ubuntu 20.04 which has a Maven package version of 3.6.3-1.', 'commenter': 'jnturton'}, {'comment': '@vdiravka, it builds fine for me in Maven 3.8.4 too.', 'commenter': 'jnturton'}, {'comment': 'Could you update the correspondent maven.md doc?', 'commenter': 'vdiravka'}, {'comment': 'You can update `Envirnment.md` with links to recommended and minimum Maven versions :\r\n```\r\n...\r\nTo build, you need to have the following software installed on your system to successfully complete a build. \r\n  * Java 8\r\n  * Maven 3.6.3 or greater\r\n```', 'commenter': 'vdiravka'}]"
2432,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpScanBatchCreator.java,"@@ -161,8 +161,8 @@ public void bind(ManagedScanFramework framework) { }
         }
       } else if (paginatorConfig != null) {
         /*
-        * If the paginator is not null and generated a list of URLs, we create
-        * a new batch reader for each URL.  In the future, this could be parallelized in
+        * If the paginator is not null we create a new batch reader for each
+        * URL that it generates.  In the future, this could be parallelized in","[{'comment': '```suggestion\r\n        * URL that it generates. In the future, this could be parallelized in\r\n```', 'commenter': 'vdiravka'}, {'comment': ""@vdiravka I didn't realise that two spaces after a period has gone out of fashion, but it seems it started dying shortly after the typewriter :)"", 'commenter': 'jnturton'}]"
2432,distribution/pom.xml,"@@ -199,28 +199,30 @@
     <dependency>
       <groupId>io.netty</groupId>
       <artifactId>netty-tcnative</artifactId>
-      <version>2.0.1.Final</version>
+      <version>2.0.47.Final</version>
       <classifier>linux-x86_64</classifier>
       <scope>test</scope>
     </dependency>
     <dependency>
       <groupId>io.netty</groupId>
       <artifactId>netty-tcnative</artifactId>
-      <version>2.0.1.Final</version>
+      <version>2.0.47.Final</version>
       <classifier>linux-x86_64-fedora</classifier>
       <scope>test</scope>
     </dependency>
     <dependency>
       <groupId>io.netty</groupId>
       <artifactId>netty-tcnative</artifactId>
-      <version>2.0.1.Final</version>
+      <version>2.0.47.Final</version>
       <classifier>osx-x86_64</classifier>
       <scope>test</scope>
     </dependency>
     <dependency>
       <groupId>io.netty</groupId>
       <artifactId>netty-tcnative</artifactId>
-      <version>2.0.1.Final</version>
+      <!-- bump warning: windows-x86_64 jars apparently stopped being published
+      after 2.0.36, see https://repo1.maven.org/maven2/io/netty/netty-tcnative/ -->
+      <version>2.0.36.Final</version>","[{'comment': 'Does this mean that Drill will not work on Windows?', 'commenter': 'cgivre'}, {'comment': ""I don't _think_ so, just that Windows users are stuck on 2.0.36.Final."", 'commenter': 'jnturton'}, {'comment': 'What about 2.0.48.Final ? https://mvnrepository.com/artifact/io.netty/netty-tcnative/2.0.48.Final', 'commenter': 'vdiravka'}, {'comment': '@vdiravka I only see binaries for linux and osx in versions after 2.0.36, e.g.\r\n\r\nhttps://repo1.maven.org/maven2/io/netty/netty-tcnative/2.0.48.Final/\r\n\r\nShould I look at other repos?', 'commenter': 'jnturton'}, {'comment': ""I see all versions pom file has different sections related to windows, but you are right specific `windows-x86_64` jar is absent (only general, without OS name).\r\nLet's leave `2.0.36.Final` for now, since it is not very important - `<scope>test</scope>`."", 'commenter': 'vdiravka'}]"
2432,exec/jdbc-all/pom.xml,"@@ -33,7 +33,7 @@
        ""package.namespace.prefix"" equals to ""oadd."". It can be overridden if necessary within any profile -->
   <properties>
     <package.namespace.prefix>oadd.</package.namespace.prefix>
-    <jdbc-all-jar.maxsize>46700000</jdbc-all-jar.maxsize>
+    <jdbc-all-jar.maxsize>44100000</jdbc-all-jar.maxsize>","[{'comment': 'gj', 'commenter': 'vdiravka'}, {'comment': '@vdiravka I did not shrink jdbc-all by that anything like that much in the end.  I did not realise that I had removed some things that are real runtime dependencies.', 'commenter': 'jnturton'}]"
2432,pom.xml,"@@ -1802,7 +1803,7 @@
       <dependency>
         <groupId>io.netty</groupId>
         <artifactId>netty-tcnative</artifactId>
-        <version>2.0.39.Final</version>
+        <version>2.0.47.Final</version>","[{'comment': '48 is available now', 'commenter': 'vdiravka'}]"
2432,exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java,"@@ -492,7 +492,7 @@ public void testHistogramWithDataTypes1() throws Exception {
           .match();
 
       query = ""select 1 from dfs.tmp.employee1 where store_id < 15"";
-      String[] expectedPlan2 = {""Filter\\(condition.*\\).*rowcount = 676.*,.*"",
+      String[] expectedPlan2 = {""Filter\\(condition.*\\).*rowcount = 699.*,.*"",
               ""Scan.*columns=\\[`store_id`\\].*rowcount = 1128.0.*""};","[{'comment': 'Do you know why rowcount is changed?', 'commenter': 'vdiravka'}, {'comment': '@vdiravka yes, please see the comments on this PR from @tdunning.  The t-digest data structure changed a bit in 3.3 and estimates like this rowcount = 676 are expected to be a bit more accurate.', 'commenter': 'jnturton'}]"
2432,pom.xml,"@@ -98,8 +98,8 @@
     <javax.validation.api>2.0.1.Final</javax.validation.api>
     <asm.version>9.2</asm.version>
     <excludedGroups />
-    <memoryMb>1800</memoryMb>
-    <directMemoryMb>3000</directMemoryMb>
+    <memoryMb>2000</memoryMb>
+    <directMemoryMb>2500</directMemoryMb>","[{'comment': 'Not sure 2500 is enough. Drill can use more memory. If you will run several times tests with this configs sucesfully - it is fine. Otherwise try to decrease slightly - 100-200Mb', 'commenter': 'vdiravka'}, {'comment': '@vdiravka do you mean _increase_ slightly?  So heap = 2000, direct = 2700?', 'commenter': 'jnturton'}]"
2432,contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcWriterWithH2.java,"@@ -142,14 +142,14 @@ public void testBasicCTASWithDataTypes() throws Exception {
     DirectRowSet results = queryBuilder().sql(testQuery).rowSet();
 
     TupleMetadata expectedSchema = new SchemaBuilder()
-      .addNullable(""int_field"", MinorType.INT, 10)
-      .addNullable(""bigint_field"", MinorType.BIGINT, 19)
-      .addNullable(""float4_field"", MinorType.VARDECIMAL, 38, 37)
-      .addNullable(""float8_field"", MinorType.VARDECIMAL, 38, 37)
+      .addNullable(""int_field"", MinorType.INT, 32)
+      .addNullable(""bigint_field"", MinorType.BIGINT, 38)
+      .addNullable(""float4_field"", MinorType.FLOAT4, 38)
+      .addNullable(""float8_field"", MinorType.FLOAT8, 38)","[{'comment': 'Does it mean we started supporting float with new h2 version?', 'commenter': 'vdiravka'}, {'comment': '@vdiravka no - this change is not specific to H2.  I noticed while debugging H2 test failures that the new JDBC writer code maps Drill\'s FLOAT4 AND FLOAT8 to the JDBC data type NUMERIC (a vardecimal type).  Apparently the idea was to try to avoid inconsistent float support across databases, but I don\'t think this is a good idea.  Vardecimal columns have a fixed precision and scale, making them fixed point rather floating point, and introducing the potential for data conversion problems.\r\n\r\nFor example, in `TestJdbcWriterWithH2#testBasicCTASWithDataTypes` if you just change a test value by one decimal place the unit test fails:\r\n```\r\n--- ""CAST(3.0 AS FLOAT) AS float4_field,"" +\r\n+++ ""CAST(30.0 AS FLOAT) AS float4_field,"" +\r\n```\r\nbecause it has tried to convert a FLOAT4 to a DECIMAL(38,37) which can only represents numbers smaller than 10.  Since I found this while trying to upgrade H2, I decided to try to fix it too.\r\n\r\n', 'commenter': 'jnturton'}, {'comment': '@vdiravka I must still go and make the same test change in the other JDBC writer tests...', 'commenter': 'jnturton'}]"
2432,pom.xml,"@@ -92,20 +92,21 @@
     <javassist.version>3.28.0-GA</javassist.version>
     <msgpack.version>0.6.6</msgpack.version>
     <reflections.version>0.9.10</reflections.version>
-    <avro.version>1.9.1</avro.version>
+    <avro.version>1.11.0</avro.version>
     <metrics.version>4.0.2</metrics.version>
-    <jetty.version>9.4.41.v20210516</jetty.version>
+    <jetty.version>9.4.44.v20210927</jetty.version>
     <jersey.version>2.34</jersey.version>
     <javax.validation.api>2.0.1.Final</javax.validation.api>
     <asm.version>9.2</asm.version>
     <excludedGroups />
-    <memoryMb>1800</memoryMb>
-    <directMemoryMb>3000</directMemoryMb>
+    <memoryMb>2000</memoryMb>
+    <directMemoryMb>2700</directMemoryMb>
     <rat.skip>true</rat.skip>
     <license.skip>true</license.skip>
     <docker.repository>apache/drill</docker.repository>
     <antlr.version>4.8-1</antlr.version>
-    <maven.version>3.6.3</maven.version>
+    <maven.version>3.8.4</maven.version>
+    <maven.version.min>3.6.3</maven.version.min>","[{'comment': 'What about:\r\n```suggestion\r\n    <maven.min.version>3.6.3</maven.min.version>\r\n```', 'commenter': 'vdiravka'}, {'comment': 'I am ok with either of variants. Just want to confirm this was not resolved accidentally', 'commenter': 'vdiravka'}]"
2445,exec/java-exec/src/main/java/org/apache/drill/exec/store/StoragePlugin.java,"@@ -62,6 +63,14 @@
   */
   StoragePluginConfig getConfig();
 
+  /**
+   * Sets the user session.
+   * @param session The user session for the query.
+   */
+  void setUserSession(UserSession session);","[{'comment': ""I don't think it is safe to set `UserSession` in this way. For the case when we have concurrent queries, `UserSession` from one user could be set, and before it is used, another user can override it..."", 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi Thanks for the response.  I'm trying to get the `UserSession` into the constructor, but it's a little tricky with all the reflection.  Would that be better?"", 'commenter': 'cgivre'}, {'comment': 'One thing to note is that method gets called immediately after the plugin is constructed.', 'commenter': 'cgivre'}, {'comment': ""In changes from this PR, it is set only but isn't obtained"", 'commenter': 'vvysotskyi'}, {'comment': ""For this PR, I'm not going to use the UserSession for anything.... Just get it to the StoragePlugin.  Future work I'll use it for per-user credentials."", 'commenter': 'cgivre'}, {'comment': '`storages.getPlugin(schemaName, getSession());` might not create a plugin, but obtain it from the cache, so we have a single instance of the same plugin that will be attempted to hold different user sessions.', 'commenter': 'vvysotskyi'}, {'comment': 'If we pass the session to the constructor, there is no need for this setter.', 'commenter': 'vvysotskyi'}, {'comment': ""> `storages.getPlugin(schemaName, getSession());` might not create a plugin, but obtain it from the cache, so we have a single instance of the same plugin that will be attempted to hold different user sessions.\r\n\r\nDo you think the right approach here would be to update the `UserSession` here?   If so, we'd still need the setter.  "", 'commenter': 'cgivre'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2445,exec/java-exec/src/main/java/org/apache/calcite/jdbc/DynamicSchema.java,"@@ -32,23 +33,34 @@
  */
 public class DynamicSchema extends SimpleCalciteSchema implements AutoCloseable {
 
-  public DynamicSchema(CalciteSchema parent, Schema schema, String name) {
+  private UserSession session;
+
+  public DynamicSchema(CalciteSchema parent, Schema schema, String name, UserSession session) {
     super(parent, schema, name);
+    this.session = session;
   }
 
   @Override
   protected CalciteSchema getImplicitSubSchema(String schemaName,
                                                boolean caseSensitive) {
     Schema s = schema.getSubSchema(schemaName);
     if (s != null) {
-      return new DynamicSchema(this, s, schemaName);
+      return new DynamicSchema(this, s, schemaName, session);
     }
     return getSubSchemaMap().get(schemaName);
   }
 
+  public UserSession getSession() {
+    return session;
+  }
+
+  public void setSession(UserSession session) {","[{'comment': ""I think we shouldn't set a session here also, just use the one from the constructor."", 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.  `UserSession` is now passed only in constructors.', 'commenter': 'cgivre'}, {'comment': 'Fixed... the `UserSession` is only updated in the constructor.  There is now a `clone()` method which also updates the  UserSession and an additional constructor. ', 'commenter': 'cgivre'}]"
2445,exec/java-exec/src/main/java/org/apache/drill/exec/store/PluginHandle.java,"@@ -30,7 +31,7 @@
  * long Drillbit start times. Plugin creation is synchronized as is closing.
  * <p>
  * A handle has a type used to determine which operations are allowed on
- * the handle. For example, inrinsic (system) plugins cannot be deleted or
+ * the handle. For example, intrinsic (system) plugins cannot be deleted or
  * disabled.
  *
  * <h4>Caveats</h4>","[{'comment': ""Please check javadoc for`PluginHandle`.\r\nThe plugin instances are not created separately for every user. Usually the instances are reused by different users (see an example for user1, user2, user3 in javadoc).\r\nTherefore UserSession can't be kept with current plugins.\r\nThere is a solution In #2251 to have separate `StoragePluginRegistryImpl` for every session. That can be used for  per-user credentials then."", 'commenter': 'vdiravka'}, {'comment': ""The plan here is to actually use the credential provider to store different user's credentials.  I got this to work in a separate branch, but I was blocked because I didn't have the active user name.  This work here represents what is necessary just to get the active user name.  "", 'commenter': 'cgivre'}]"
2447,contrib/storage-phoenix/pom.xml,"@@ -33,7 +33,7 @@
     <phoenix.version>5.1.2</phoenix.version>
     <!-- Keep the 2.4.2 to reduce dependency conflict -->
     <hbase.minicluster.version>2.4.2</hbase.minicluster.version>
-    <jetty.test.version>9.4.31.v20200723</jetty.test.version>
+    <jetty.test.version>9.4.44.v20210927</jetty.test.version>","[{'comment': ""I've checked and `SecuredPhoenixTestSuite` works fine with this jetty version (dependencies in `drill-storage-phoenix` are still needed for that).\r\n\r\nSo please remove `jetty.test.version` property and it's usage"", 'commenter': 'vdiravka'}]"
2447,.travis.yml,"@@ -50,41 +50,43 @@ before_install:
   - git fetch --unshallow
   # Install libraries required for protobuf generation
   - |
-    if [ $PHASE = ""build_checkstyle_protobuf"" ]; then \
-      sudo apt-get install -y libboost-all-dev libzookeeper-mt-dev libsasl2-dev cmake libcppunit-dev checkinstall && \
-      pushd .. && \
-      if [ -f $HOME/protobuf/protobuf_3.16.1* ]; then \
-        sudo dpkg -i $HOME/protobuf/protobuf_3.16.1*; \
-      else \
-        wget https://github.com/protocolbuffers/protobuf/releases/download/v3.16.1/protobuf-java-3.16.1.zip && \
-        unzip protobuf-java-3.16.1.zip && pushd protobuf-3.16.1 && \
-        ./configure && sudo make && sudo checkinstall -y && \
-        if [ ! -d $HOME/protobuf ]; then \
-          mkdir -p $HOME/protobuf; \
-        fi && \
-        mv protobuf_3.16.1* $HOME/protobuf/ && popd; \
-      fi && \
-      sudo ldconfig && popd; \
+    if [ $PHASE = ""build_checkstyle_protobuf"" ]; then
+      sudo apt-get install -y libboost-all-dev libzookeeper-mt-dev libsasl2-dev cmake libcppunit-dev checkinstall &&
+      pushd .. &&
+      if [ -f $HOME/protobuf/protobuf_3.16.1* ]; then
+        sudo dpkg -i $HOME/protobuf/protobuf_3.16.1*;
+      else
+        wget https://github.com/protocolbuffers/protobuf/releases/download/v3.16.1/protobuf-java-3.16.1.zip &&
+        unzip protobuf-java-3.16.1.zip && pushd protobuf-3.16.1 &&
+        ./configure && sudo make && sudo checkinstall -y &&
+        if [ ! -d $HOME/protobuf ]; then
+          mkdir -p $HOME/protobuf;
+        fi &&
+        mv protobuf_3.16.1* $HOME/protobuf/ && popd;
+      fi &&
+      sudo ldconfig && popd;
     fi
 install:
   # For tests phase runs unit tests
   # For protobuf phase: builds Drill project, performs license checkstyle goal and regenerates Java and C++ Protobuf files
   - |
-    if [ $PHASE = ""tests"" ]; then \
-      mvn install --batch-mode --no-transfer-progress \
-        -DexcludedGroups=""org.apache.drill.categories.SlowTest,org.apache.drill.categories.UnlikelyTest,org.apache.drill.categories.SecurityTest""; \
-    elif [ $PHASE = ""build_checkstyle_protobuf"" ]; then \
-      MAVEN_OPTS=""-Xms1G -Xmx1G"" mvn install --no-transfer-progress -Drat.skip=false -Dlicense.skip=false --batch-mode -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn -DskipTests=true -Dmaven.javadoc.skip=true -Dmaven.source.skip=true && \
-      pushd protocol && mvn process-sources -P proto-compile && popd && \
-      mkdir contrib/native/client/build && pushd contrib/native/client/build && /usr/bin/cmake -G ""Unix Makefiles"" .. && make cpProtobufs && popd; \
+    if [ $PHASE = ""tests"" ]; then
+      echo Selecting parallel GC to minimise peak mem usage.","[{'comment': 'Please avoid adding such comments in the ci output, they might confuse people that trying to figure out why their tests failing.', 'commenter': 'vvysotskyi'}]"
2447,.travis.yml,"@@ -50,41 +50,43 @@ before_install:
   - git fetch --unshallow
   # Install libraries required for protobuf generation
   - |
-    if [ $PHASE = ""build_checkstyle_protobuf"" ]; then \
-      sudo apt-get install -y libboost-all-dev libzookeeper-mt-dev libsasl2-dev cmake libcppunit-dev checkinstall && \
-      pushd .. && \
-      if [ -f $HOME/protobuf/protobuf_3.16.1* ]; then \
-        sudo dpkg -i $HOME/protobuf/protobuf_3.16.1*; \
-      else \
-        wget https://github.com/protocolbuffers/protobuf/releases/download/v3.16.1/protobuf-java-3.16.1.zip && \
-        unzip protobuf-java-3.16.1.zip && pushd protobuf-3.16.1 && \
-        ./configure && sudo make && sudo checkinstall -y && \
-        if [ ! -d $HOME/protobuf ]; then \
-          mkdir -p $HOME/protobuf; \
-        fi && \
-        mv protobuf_3.16.1* $HOME/protobuf/ && popd; \
-      fi && \
-      sudo ldconfig && popd; \
+    if [ $PHASE = ""build_checkstyle_protobuf"" ]; then
+      sudo apt-get install -y libboost-all-dev libzookeeper-mt-dev libsasl2-dev cmake libcppunit-dev checkinstall &&
+      pushd .. &&
+      if [ -f $HOME/protobuf/protobuf_3.16.1* ]; then
+        sudo dpkg -i $HOME/protobuf/protobuf_3.16.1*;
+      else
+        wget https://github.com/protocolbuffers/protobuf/releases/download/v3.16.1/protobuf-java-3.16.1.zip &&
+        unzip protobuf-java-3.16.1.zip && pushd protobuf-3.16.1 &&
+        ./configure && sudo make && sudo checkinstall -y &&
+        if [ ! -d $HOME/protobuf ]; then
+          mkdir -p $HOME/protobuf;
+        fi &&
+        mv protobuf_3.16.1* $HOME/protobuf/ && popd;
+      fi &&
+      sudo ldconfig && popd;
     fi
 install:
   # For tests phase runs unit tests
   # For protobuf phase: builds Drill project, performs license checkstyle goal and regenerates Java and C++ Protobuf files
   - |
-    if [ $PHASE = ""tests"" ]; then \
-      mvn install --batch-mode --no-transfer-progress \
-        -DexcludedGroups=""org.apache.drill.categories.SlowTest,org.apache.drill.categories.UnlikelyTest,org.apache.drill.categories.SecurityTest""; \
-    elif [ $PHASE = ""build_checkstyle_protobuf"" ]; then \
-      MAVEN_OPTS=""-Xms1G -Xmx1G"" mvn install --no-transfer-progress -Drat.skip=false -Dlicense.skip=false --batch-mode -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn -DskipTests=true -Dmaven.javadoc.skip=true -Dmaven.source.skip=true && \
-      pushd protocol && mvn process-sources -P proto-compile && popd && \
-      mkdir contrib/native/client/build && pushd contrib/native/client/build && /usr/bin/cmake -G ""Unix Makefiles"" .. && make cpProtobufs && popd; \
+    if [ $PHASE = ""tests"" ]; then
+      echo Selecting parallel GC to minimise peak mem usage.
+      MAVEN_OPTS=""-XX:+UseParallelGC""","[{'comment': ""Can we use the same GC as for regular Drill runs, so tests will be closer to the real envs? If not, why don't use UseParNewGC?"", 'commenter': 'vvysotskyi'}, {'comment': '@vdiravka it was selected for an extremely memory constrained environment and regular Drill users with such memory constraints are well-advised to make the same selection.  Will revert for now...', 'commenter': 'jnturton'}, {'comment': '@jnturton Could you create a task to tune the application to use less memory and back to G1 as default for GitHub CI?', 'commenter': 'vdiravka'}]"
2455,exec/java-exec/pom.xml,"@@ -652,6 +652,12 @@
       <version>${testcontainers.version}</version>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.drill.contrib</groupId>
+      <artifactId>drill-iceberg-format</artifactId>","[{'comment': 'Here we have a cycle... any ideas how i could solve this without moving everything to java-exec ?', 'commenter': 'Z0ltrix'}]"
2456,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java,"@@ -227,6 +229,33 @@ private void init() {
     }
   }
 
+
+  /** Workaround for Kafka > 2.0 version due to <a href=""https://cwiki.apache.org/confluence/display/KAFKA/KIP-505%3A+Add+new+public+method+to+only+update+assignment+metadata+in+consumer"">KIP-505</a>.","[{'comment': '```suggestion\r\n  /**\r\n  Workaround for Kafka > 2.0 version due to KIP-505. It can be replaced with Kafka implementation once it will be introduced.\r\n```', 'commenter': 'luocooong'}, {'comment': ""Why do you prefer to remove the direct link? I think it's a very convenient thing and it's fast to check whether it was resolved."", 'commenter': 'rymarm'}, {'comment': '@rymarm Thanks for the question.\nIn general, we can record any information of limitations into the README.md of plugins, such as `contrib/format-xml/README.md`. Then, we can keep the comment as brief as possible, and here we focus on providing an alternative function. So the KIP-505 keyword is enough.', 'commenter': 'luocooong'}, {'comment': '@luocooong Thank you for explaining! Followed your suggestion. ', 'commenter': 'rymarm'}]"
2456,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java,"@@ -227,6 +229,33 @@ private void init() {
     }
   }
 
+
+  /** Workaround for Kafka > 2.0 version due to <a href=""https://cwiki.apache.org/confluence/display/KAFKA/KIP-505%3A+Add+new+public+method+to+only+update+assignment+metadata+in+consumer"">KIP-505</a>.
+   * It can be replaced with Kafka implementation once it will be introduced.
+   * @param consumer Kafka consumer whom need to get assignments
+   * @return
+   * @throws InterruptedException
+   */
+  private Set<TopicPartition> waitForConsumerAssignment(Consumer consumer) throws InterruptedException {
+    Set<TopicPartition> assignments = consumer.assignment();
+
+    long waitingForAssigmentTimeout = kafkaStoragePlugin.getContext().getOptionManager().getLong(ExecConstants.KAFKA_POLL_TIMEOUT);
+    long timeout = 0;
+
+    while (assignments.isEmpty() && timeout < waitingForAssigmentTimeout) {
+      Thread.sleep(500);
+      timeout += 500;
+      assignments = consumer.assignment();
+    }
+
+    if (timeout >= waitingForAssigmentTimeout) {
+      logger.error(""Consumer assignment wasn't completed within the timeout {}"", waitingForAssigmentTimeout);
+      throw UserException.dataReadError().build(logger);","[{'comment': '```\r\nthrow UserException.dataReadError()\r\n  .message(""Consumer assignment wasn\'t completed within the timeout %s"", waitingForAssigmentTimeout)\r\n  .build(logger);\r\n```', 'commenter': 'luocooong'}]"
2456,contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java,"@@ -156,6 +157,29 @@ public void testInformationSchema() throws Exception {
     }
   }
 
+  private Set<TopicPartition> waitForConsumerAssignment(Consumer consumer)  {
+    Set<TopicPartition> assignments = consumer.assignment();
+
+    long waitingForAssigmentTimeout = 5000;
+    long timeout = 0;
+
+    while (assignments.isEmpty() && timeout < waitingForAssigmentTimeout) {
+      try {
+        Thread.sleep(500);
+      } catch (InterruptedException e) {
+        e.printStackTrace();","[{'comment': 'Printing stack information is not recommended.', 'commenter': 'luocooong'}]"
2463,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/AirliftBytesInputCompressor.java,"@@ -159,13 +160,13 @@ public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output,
 
   @Override
   public void release() {
-    logger.debug(
-        ""will release {} allocated buffers."",
-        this.allocatedBuffers.size()
-    );
+    int bufCount  = allocatedBuffers.size();
 
-    while (!this.allocatedBuffers.isEmpty()) {
-      this.allocator.release(allocatedBuffers.pop());
+    // LIFO release order to try to reduce memory fragmentation.
+    while (!allocatedBuffers.isEmpty()) {
+      allocator.release(allocatedBuffers.pop());
     }
+
+    logger.debug(""released {} allocated buffers"", bufCount);","[{'comment': 'We possibly can assert or log the actual `allocatedBuffers.size()` to make sure allocated buffers are released', 'commenter': 'vdiravka'}]"
2463,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/DrillCompressionCodecFactory.java,"@@ -50,17 +52,31 @@
 
   // The set of codecs to be handled by aircompressor
   private static final Set<CompressionCodecName> AIRCOMPRESSOR_CODECS = new HashSet<>(
-      Arrays.asList(CompressionCodecName.LZ4, CompressionCodecName.LZO,
-          CompressionCodecName.SNAPPY, CompressionCodecName.ZSTD));
+      Arrays.asList(
+        CompressionCodecName.LZ4,
+        CompressionCodecName.LZO,
+        CompressionCodecName.SNAPPY,
+        CompressionCodecName.ZSTD
+      )
+  );
 
-  // pool of reused aircompressor compressors (parquet-mr's factory has its own)
+  // pool of reusable thread-safe aircompressor compressors (parquet-mr's factory","[{'comment': ""```suggestion\r\n  // pool of reusable thread-safe aircompressor compressors (parquet-mr's factory has its own)\r\n```\r\n"", 'commenter': 'vdiravka'}]"
2463,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/DrillCompressionCodecFactory.java,"@@ -50,17 +52,31 @@
 
   // The set of codecs to be handled by aircompressor
   private static final Set<CompressionCodecName> AIRCOMPRESSOR_CODECS = new HashSet<>(
-      Arrays.asList(CompressionCodecName.LZ4, CompressionCodecName.LZO,
-          CompressionCodecName.SNAPPY, CompressionCodecName.ZSTD));
+      Arrays.asList(
+        CompressionCodecName.LZ4,
+        CompressionCodecName.LZO,
+        CompressionCodecName.SNAPPY,
+        CompressionCodecName.ZSTD
+      )
+  );
 
-  // pool of reused aircompressor compressors (parquet-mr's factory has its own)
+  // pool of reusable thread-safe aircompressor compressors (parquet-mr's factory
+  // has its own)
   private final Map<CompressionCodecName, AirliftBytesInputCompressor> airCompressors = new HashMap<>();
 
   // fallback parquet-mr compression codec factory
-  private CompressionCodecFactory parqCodecFactory;
+  private final CompressionCodecFactory parqCodecFactory;
 
   // direct memory allocator to be used during (de)compression
-  private ByteBufferAllocator allocator;
+  private final ByteBufferAllocator allocator;
+
+  // Start: members for working around a gzip concurrency bug c.f. DRILL-8139
+  private final Deque<CompressionCodecFactory> singleUseFactories;
+","[{'comment': '```suggestion\r\n```', 'commenter': 'vdiravka'}]"
2463,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/DrillCompressionCodecFactory.java,"@@ -50,17 +52,31 @@
 
   // The set of codecs to be handled by aircompressor
   private static final Set<CompressionCodecName> AIRCOMPRESSOR_CODECS = new HashSet<>(
-      Arrays.asList(CompressionCodecName.LZ4, CompressionCodecName.LZO,
-          CompressionCodecName.SNAPPY, CompressionCodecName.ZSTD));
+      Arrays.asList(
+        CompressionCodecName.LZ4,
+        CompressionCodecName.LZO,
+        CompressionCodecName.SNAPPY,
+        CompressionCodecName.ZSTD
+      )
+  );
 
-  // pool of reused aircompressor compressors (parquet-mr's factory has its own)
+  // pool of reusable thread-safe aircompressor compressors (parquet-mr's factory
+  // has its own)
   private final Map<CompressionCodecName, AirliftBytesInputCompressor> airCompressors = new HashMap<>();
 
   // fallback parquet-mr compression codec factory
-  private CompressionCodecFactory parqCodecFactory;
+  private final CompressionCodecFactory parqCodecFactory;
 
   // direct memory allocator to be used during (de)compression
-  private ByteBufferAllocator allocator;
+  private final ByteBufferAllocator allocator;
+
+  // Start: members for working around a gzip concurrency bug c.f. DRILL-8139
+  private final Deque<CompressionCodecFactory> singleUseFactories;
+
+  private final Configuration config;
+","[{'comment': '```suggestion\r\n```', 'commenter': 'vdiravka'}]"
2463,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/DrillCompressionCodecFactory.java,"@@ -80,38 +99,56 @@ public DrillCompressionCodecFactory(Configuration config, ByteBufferAllocator al
   }
 
   @Override
-  public BytesInputCompressor getCompressor(CompressionCodecName codecName) {
+  public synchronized BytesInputCompressor getCompressor(CompressionCodecName codecName) {
     if (AIRCOMPRESSOR_CODECS.contains(codecName)) {
       return airCompressors.computeIfAbsent(
           codecName,
           c -> new AirliftBytesInputCompressor(codecName, allocator)
       );
+    } else if (codecName != CompressionCodecName.SNAPPY) {
+      // Work around PARQUET-2126: construct a new codec factory every time to
+      // avoid a concurrrency bug c.f. DRILL-8139. Remove once PARQUET-2126 is","[{'comment': '```suggestion\r\n     // avoid a concurrrency bug c.f. DRILL-8139. TODO: Remove once PARQUET-2126 is\r\nWrite Preview\r\n```', 'commenter': 'vdiravka'}]"
2463,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/DrillCompressionCodecFactory.java,"@@ -80,38 +99,56 @@ public DrillCompressionCodecFactory(Configuration config, ByteBufferAllocator al
   }
 
   @Override
-  public BytesInputCompressor getCompressor(CompressionCodecName codecName) {
+  public synchronized BytesInputCompressor getCompressor(CompressionCodecName codecName) {
     if (AIRCOMPRESSOR_CODECS.contains(codecName)) {
       return airCompressors.computeIfAbsent(
           codecName,
           c -> new AirliftBytesInputCompressor(codecName, allocator)
       );
+    } else if (codecName != CompressionCodecName.SNAPPY) {
+      // Work around PARQUET-2126: construct a new codec factory every time to
+      // avoid a concurrrency bug c.f. DRILL-8139. Remove once PARQUET-2126 is
+      // fixed.  Snappy is immune because of the thread safety in the Xerial lib.
+      CompressionCodecFactory ccf = CodecFactory.createDirectCodecFactory(config, allocator, pageSize);
+      // hold onto a reference for later release()
+      singleUseFactories.add(ccf);
+      return ccf.getCompressor(codecName);
     } else {
       return parqCodecFactory.getCompressor(codecName);
     }
   }
 
   @Override
-  public BytesInputDecompressor getDecompressor(CompressionCodecName codecName) {
+  public synchronized BytesInputDecompressor getDecompressor(CompressionCodecName codecName) {
     if (AIRCOMPRESSOR_CODECS.contains(codecName)) {
       return airCompressors.computeIfAbsent(
           codecName,
           c -> new AirliftBytesInputCompressor(codecName, allocator)
       );
+    } else if (codecName != CompressionCodecName.SNAPPY) {","[{'comment': 'Looks like existed `else` block starts to be never reached:\r\nWhen `codecName` is SNAPPY or other from `AIRCOMPRESSOR_CODECS` we will pass through the first condition \r\nand for all other cases we will pass through the new `else if` block. If that by design the logic can be simplified: `else if` -> `else`. `return parqCodecFactory.getDecompressor(codecName);` possibly can be commented.\r\n\r\nThe same for `#getCompressor` method', 'commenter': 'vdiravka'}, {'comment': '`AIRCOMPRESSOR_CODECS` contains `CompressionCodecName.SNAPPY`, so this condition will be always `true`.', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi oh of course, snappy is in aircompressor now.', 'commenter': 'jnturton'}]"
2463,exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/compression/AirliftBytesInputCompressor.java,"@@ -159,13 +160,16 @@ public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output,
 
   @Override
   public void release() {
-    logger.debug(
-        ""will release {} allocated buffers."",
-        this.allocatedBuffers.size()
-    );
+    int bufCount  = allocatedBuffers.size();
 
-    while (!this.allocatedBuffers.isEmpty()) {
-      this.allocator.release(allocatedBuffers.pop());
+    // LIFO release order to try to reduce memory fragmentation.
+    int i = 0;
+    while (!allocatedBuffers.isEmpty()) {
+      allocator.release(allocatedBuffers.pop());
+      i++;
     }
+    assert bufCount == i;","[{'comment': 'It is better to write assertion message always. It helps to understand the error in StackTrace better', 'commenter': 'vdiravka'}]"
2477,contrib/storage-elasticsearch/src/test/java/org/apache/drill/exec/store/elasticsearch/ElasticSearchQueryTest.java,"@@ -95,6 +96,14 @@ private static void prepareData() throws IOException {
     builder.field(""marital_status"", ""S"");
     builder.field(""gender"", ""F"");
     builder.field(""management_role"", ""Senior Management"");
+    builder.field(""binary_filed"", ""Senior Management"".getBytes());","[{'comment': '```suggestion\r\n    builder.field(""binary_field"", ""Senior Management"".getBytes());\r\n```', 'commenter': 'jnturton'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}]"
2477,contrib/storage-elasticsearch/src/test/java/org/apache/drill/exec/store/elasticsearch/TestElasticsearchSuit.java,"@@ -0,0 +1,79 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.elasticsearch;
+
+import org.apache.drill.categories.SlowTest;
+import org.apache.drill.test.BaseTest;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.experimental.categories.Category;
+import org.junit.runner.RunWith;
+import org.junit.runners.Suite;
+import org.testcontainers.elasticsearch.ElasticsearchContainer;
+import org.testcontainers.utility.DockerImageName;
+
+import java.time.Duration;
+import java.util.concurrent.atomic.AtomicInteger;
+
+@Category(SlowTest.class)
+@RunWith(Suite.class)
+@Suite.SuiteClasses({ElasticComplexTypesTest.class, ElasticInfoSchemaTest.class, ElasticSearchPlanTest.class, ElasticSearchQueryTest.class})
+public class TestElasticsearchSuit extends BaseTest {","[{'comment': '```suggestion\r\npublic class TestElasticsearchSuite extends BaseTest {\r\n```', 'commenter': 'jnturton'}, {'comment': 'Done.', 'commenter': 'vvysotskyi'}]"
2477,contrib/storage-elasticsearch/pom.xml,"@@ -88,36 +97,17 @@
         <artifactId>maven-surefire-plugin</artifactId>
         <configuration>
           <forkCount combine.self=""override"">1</forkCount>
+          <includes>
+            <include>**/TestElasticsearchSuit.class</include>","[{'comment': '```suggestion\r\n            <include>**/TestElasticsearchSuite.class</include>\r\n```', 'commenter': 'jnturton'}, {'comment': 'Thanks, done.', 'commenter': 'vvysotskyi'}]"
2477,contrib/storage-cassandra/src/test/java/org/apache/drill/exec/store/cassandra/CassandraQueryTest.java,"@@ -327,9 +349,18 @@ public void testWithProvidedSchema() throws Exception {
         .ordered()
         .baselineColumns(""employee_id"", ""full_name"", ""first_name"", ""last_name"", ""position_id"",
             ""position_title"", ""store_id"", ""department_id"", ""birth_date"", ""hire_date"", ""salary"",
-            ""supervisor_id"", ""education_level"", ""marital_status"", ""gender"", ""management_role"")
+            ""supervisor_id"", ""education_level"", ""marital_status"", ""gender"", ""management_role"",
+            ""ascii_field"", ""blob_field"", ""boolean_field"", ""date_field"", ""decimal_field"", ""double_field"",
+            ""duration_field"", ""inet_field"", ""time_field"", ""timestamp_field"", ""timeuuid_field"",
+            ""uuid_field"", ""varchar_field"", ""varint_field"")
         .baselineValues(1L, ""Sheri Nowmer"", ""Sheri"", ""Nowmer"", 1, ""President"", 0, 1, LocalDate.parse(""1961-08-26""),
-            ""1994-12-01 00:00:00.0"", new BigDecimal(""80000.00""), 0, ""Graduate Degree"", ""S"", ""F"", ""Senior Management"")
+            ""1994-12-01 00:00:00.0"", new BigDecimal(""80000.00""), 0, ""Graduate Degree"", ""S"", ""F"", ""Senior Management"",
+            ""abc"", ""0000000000000003"", true, 15008L, BigDecimal.valueOf(123), 321.123,
+            new Period(0, 0, 0, 3, 0, 0, 0, 320688000),
+            InetAddress.getByName(""8.8.8.8"").getAddress(), 14700000000000L, 1296705900000L,
+            ByteBuffer.wrap(new byte[16]).order(ByteOrder.BIG_ENDIAN).putLong(5788618031596507621L).putLong(-5528732594182759265L).array(),","[{'comment': 'These 64-bit int literals are a bit tricky to read and think about.  Would some code that converts a UUID from a hex string literal into a byte[16] work here?', 'commenter': 'jnturton'}, {'comment': 'Thanks, done', 'commenter': 'vvysotskyi'}]"
2477,contrib/storage-elasticsearch/pom.xml,"@@ -88,36 +97,17 @@
         <artifactId>maven-surefire-plugin</artifactId>
         <configuration>
           <forkCount combine.self=""override"">1</forkCount>
+          <includes>
+            <include>**/TestElasticsearchSuit.class</include>
+          </includes>
+          <excludes>
+            <exclude>**/ElasticComplexTypesTest.java</exclude>","[{'comment': 'Just checking, do we exclude these tests because they rely on testcontainers?', 'commenter': 'jnturton'}, {'comment': 'It is excluded because now tests will be running as a test suite.', 'commenter': 'vvysotskyi'}]"
2477,contrib/storage-cassandra/src/main/java/org/apache/drill/exec/store/cassandra/CassandraColumnConverterFactory.java,"@@ -0,0 +1,144 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.cassandra;
+
+import com.datastax.driver.core.Duration;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.record.ColumnConverter;
+import org.apache.drill.exec.record.ColumnConverterFactory;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.TupleWriter;
+import org.apache.drill.exec.vector.accessor.ValueWriter;
+import org.joda.time.Period;
+import org.joda.time.format.PeriodFormatter;
+import org.joda.time.format.PeriodFormatterBuilder;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.net.Inet4Address;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.util.Map;
+import java.util.UUID;
+import java.util.stream.Collectors;
+import java.util.stream.StreamSupport;
+
+public class CassandraColumnConverterFactory extends ColumnConverterFactory {
+
+  private static final PeriodFormatter FORMATTER = new PeriodFormatterBuilder()
+    .appendYears()
+    .appendSuffix(""Y"")
+    .appendMonths()
+    .appendSuffix(""M"")
+    .appendWeeks()
+    .appendSuffix(""W"")
+    .appendDays()
+    .appendSuffix(""D"")
+    .appendHours()
+    .appendSuffix(""H"")
+    .appendMinutes()
+    .appendSuffix(""M"")
+    .appendSecondsWithOptionalMillis()
+    .appendSuffix(""S"")
+    .toFormatter();
+
+  public CassandraColumnConverterFactory(TupleMetadata providedSchema) {
+    super(providedSchema);
+  }
+
+  @Override
+  public ColumnConverter.ScalarColumnConverter buildScalar(ColumnMetadata readerSchema, ValueWriter writer) {
+    switch (readerSchema.type()) {
+      case INTERVAL:
+        return new ColumnConverter.ScalarColumnConverter(value -> {
+          Duration duration = (Duration) value;
+          writer.setPeriod(Period.parse(duration.toString(), FORMATTER));
+        });
+      case BIGINT:
+        return new ColumnConverter.ScalarColumnConverter(value -> {
+          long longValue;
+          if (value instanceof BigInteger) {
+            longValue = ((BigInteger) value).longValue();
+          } else {
+            longValue = (Long) value;
+          }
+          writer.setLong(longValue);
+        });
+      case VARCHAR:
+        return new ColumnConverter.ScalarColumnConverter(value -> writer.setString(value.toString()));
+      case VARDECIMAL:
+        return new ColumnConverter.ScalarColumnConverter(value -> writer.setDecimal((BigDecimal) value));
+      case VARBINARY:
+        return new ColumnConverter.ScalarColumnConverter(value -> {
+          byte[] bytes;
+          if (value instanceof Inet4Address) {
+            bytes = ((Inet4Address) value).getAddress();","[{'comment': 'Does this do anything different to just doing `bytes = (byte[]) value;`?', 'commenter': 'jnturton'}, {'comment': 'It is a different object, so it cannot be cast to `byte[]`.', 'commenter': 'vvysotskyi'}]"
2477,exec/java-exec/src/main/java/org/apache/drill/exec/store/enumerable/ColumnConverterFactoryProvider.java,"@@ -0,0 +1,28 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.enumerable;
+
+import com.fasterxml.jackson.annotation.JsonTypeInfo;
+import org.apache.drill.exec.record.ColumnConverterFactory;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+
+@JsonTypeInfo(use = JsonTypeInfo.Id.NAME)
+public interface ColumnConverterFactoryProvider {","[{'comment': ""We've got this new interface which seems like a factory of factories and which only ever has very simple implementations that instantiate and return a new object.  So I just want to check if its really needed and carrying its weight."", 'commenter': 'jnturton'}, {'comment': ""Factory requires schema which can be available only at execution time, so we can't create a specific instance of converter factory in group scan. Therefore I'm using one more interface which will be also serialized/deserialized using its name."", 'commenter': 'vvysotskyi'}]"
2484,pom.xml,"@@ -548,67 +547,6 @@
           </execution>
         </executions>
       </plugin>
-      <plugin>
-        <groupId>pl.project13.maven</groupId>
-        <artifactId>git-commit-id-plugin</artifactId>","[{'comment': 'This plugin generates a `.git` file that contains the exact commit id, message, and other info that helps to find out the information about a specific build, especially when it was done for the master. Instead of removing the file at all, you can configure the plugin to include only entries that will be the same for different environments.', 'commenter': 'vvysotskyi'}, {'comment': 'See https://github.com/git-commit-id/git-commit-id-maven-plugin/blob/master/docs/using-the-plugin.md for detailed plugin usage.', 'commenter': 'vvysotskyi'}, {'comment': ""@hboutemy Do you think you'll be able to address @vvysotskyi's comment?"", 'commenter': 'cgivre'}, {'comment': ""I have one question: how do you handle official Apache release building done from source tarball instead of Git?\r\n\r\nI imagine this plugin can be useful for SNAPSHOTs, where you don't know precisely which commit is built"", 'commenter': 'hboutemy'}, {'comment': ""> I have one question: how do you handle official Apache release building done from source tarball instead of Git?\r\n> \r\n> I imagine this plugin can be useful for SNAPSHOTs, where you don't know precisely which commit is built\r\n\r\n@jnturton Maybe you can take this one?"", 'commenter': 'cgivre'}, {'comment': '@hboutemy our official Apache builds are created from a Git commit using Maven release plugins, not from a source tarball (although this is an output of the process)...', 'commenter': 'jnturton'}, {'comment': ""yes, that's expected to be done like this while releasing: no choice, the beginning of the release process is a source control. It has been cvs, then svn, now git, perhaps something else in the future.\r\nthe fact is that when voting, the source tarball is what is registered at Apache level, with distribution area, because it's the same format for years and for foreseeable future\r\nand sadly, if you have such dependency on Git, you can't rebuild from that official source tarball and get the same output\r\n\r\nthat's why I propose to get the best of the 2 worlds:\r\n- when building a SNAPSHOT, yes, use that Git binding because it gives you useful details\r\n- when building a release, just don't use that Git binding because it causes that issue"", 'commenter': 'hboutemy'}, {'comment': ""@hboutemy would it be sufficient, as @vvysotskyi proposed, for us to remove from the generated git.properties file those of the plugin's outputs that are not are not reproducible.  I.e. kill lines such as these?\r\n\r\n\r\n```\r\n│ -git.build.host=jtpc\r\n│ -git.build.time=22.02.2022 @ 20\\:20\\:27 SAST\r\n│ +git.build.host=c23e344f8445\r\n│ +git.build.time=26.02.2022 @ 17\\:58\\:09 UTC\r\n│  git.build.user.email=\r\n│  git.build.user.name=\r\n```\r\n\r\nIn general, I'm a fan of the idea of being to build from only a source tarball with no dependency on the VCS _de jour_ but on the other hand, adjusting this git-commit-id plugin looks less disruptive?"", 'commenter': 'jnturton'}, {'comment': ""ok, let's do like this :)"", 'commenter': 'hboutemy'}]"
2489,exec/java-exec/pom.xml,"@@ -889,11 +914,11 @@
               <lookAhead>2</lookAhead>
               <isStatic>false</isStatic>
               <outputDirectory>${project.build.directory}/generated-sources/</outputDirectory>
-<!--","[{'comment': ""Do you know why we have this commented out code in this `pom.xml` file?   I'd imagine it is here for a reason, and if so, we could indicate that.   If you have no idea, it's fine and we can just leave it. "", 'commenter': 'cgivre'}, {'comment': ""No, I don't know what this code is for."", 'commenter': 'estherbuchwalter'}]"
2489,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java,"@@ -73,6 +78,11 @@
 import java.util.ArrayList;
 import java.util.List;
 
+@OpenAPIDefinition(info = @Info(title = ""Apache Drill REST API"", description = ""OpenAPI Specification"", license = @License(name = ""Apache Software Foundation (ASF)"", url = ""http://www.apache"" + "".org/licenses/LICENSE-2.0""), contact = @Contact(name = ""Apache Drill"", url = ""https://drill.apache.org/"")))/*,
+tags = {
+  @Tag(name = ""My Tag 1"", description = ""Tag 1's Description"", externalDocs = @ExternalDocumentation(description = ""docs description1"")),
+  @Tag(name = ""My Tag 2"", description = ""Tag 2's Description"", externalDocs = @ExternalDocumentation(description = ""docs description2""))})*/","[{'comment': 'Please remove commented out code, here and elsewhere.   The line above, please break that up a bit so that it fits w/o horizontal scroll. ', 'commenter': 'cgivre'}]"
2489,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java,"@@ -188,20 +198,13 @@ public WebUserConnection provide() {
       }
 
       // User is logged in, get/set the WebSessionResources attribute
-      WebSessionResources webSessionResources =
-              (WebSessionResources) session.getAttribute(WebSessionResources.class.getSimpleName());
+      WebSessionResources webSessionResources = (WebSessionResources) session.getAttribute(WebSessionResources.class.getSimpleName());
 
       if (webSessionResources == null) {
         // User is login in for the first time
         final DrillbitContext drillbitContext = workManager.getContext();
         final DrillConfig config = drillbitContext.getConfig();
-        final UserSession drillUserSession = UserSession.Builder.newBuilder()
-                .withCredentials(UserBitShared.UserCredentials.newBuilder()
-                        .setUserName(sessionUserPrincipal.getName())
-                        .build())
-                .withOptionManager(drillbitContext.getOptionManager())
-                .setSupportComplexTypes(config.getBoolean(ExecConstants.CLIENT_SUPPORT_COMPLEX_TYPES))
-                .build();
+        final UserSession drillUserSession = UserSession.Builder.newBuilder().withCredentials(UserBitShared.UserCredentials.newBuilder().setUserName(sessionUserPrincipal.getName()).build()).withOptionManager(drillbitContext.getOptionManager()).setSupportComplexTypes(config.getBoolean(ExecConstants.CLIENT_SUPPORT_COMPLEX_TYPES)).build();","[{'comment': 'nit:  Please revert the line breaks to the original pattern for readability, here and elsewhere.', 'commenter': 'cgivre'}, {'comment': 'Ok, sure. I think this is from reformatting the code.', 'commenter': 'estherbuchwalter'}]"
2489,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRoot.java,"@@ -103,6 +109,8 @@ public Response getDrillbitStatus() {
   @GET
   @Path(""/gracePeriod"")
   @Produces(MediaType.APPLICATION_JSON)
+  @Operation(externalDocs = @ExternalDocumentation(description = ""Apache Drill REST API documentation:"", url = ""  https://drill.apache.org/docs/stopping-drill/#:~:text=draining%2C%20and%20offline.%0A%20%20%20%20%20--,grace,-%3A%20A%20period%20in""))","[{'comment': ""Please remove extra spaces after the `url` tag.  Here and elsewhere.  Also, is it necessary to URL encode the links, or will Drill figure that out by itself?\r\n\r\nIf it is not necessary, I'd recommend rendering them normally, w/o the URL encoding. "", 'commenter': 'cgivre'}, {'comment': 'The URL encoding came from copying the link to specific text on the page. Would you rather me remove the encoding and have the URLs point to the general page instead?', 'commenter': 'estherbuchwalter'}, {'comment': ""I think a link the general page is fine.  If there is an internal reference IE:\r\nhttps://somepage.com#part2\r\nDefinitely include that.  But I'd remove the URL encoding if it isn't necessary.  "", 'commenter': 'cgivre'}]"
2489,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRoot.java,"@@ -230,278 +240,300 @@ public ClusterInfo getClusterInfoJSON() {
    * @return true if drillbit are the same
    */
   private boolean isDrillbitsTheSame(DrillbitEndpoint endpoint1, DrillbitEndpoint endpoint2) {
-    return endpoint1.getAddress().equals(endpoint2.getAddress()) &&
-        endpoint1.getControlPort() == endpoint2.getControlPort() &&
-        endpoint1.getDataPort() == endpoint2.getDataPort() &&
-        endpoint1.getUserPort() == endpoint2.getUserPort();
+    return endpoint1.getAddress().equals(endpoint2.getAddress()) && endpoint1.getControlPort() == endpoint2.getControlPort() && endpoint1.getDataPort() == endpoint2.getDataPort() && endpoint1.getUserPort() == endpoint2.getUserPort();
   }
 
   private Response setResponse(Map<String, ?> entity) {
-    return Response.ok()
-            .entity(entity)
-            .header(""Access-Control-Allow-Origin"", ""*"")
-            .header(""Access-Control-Allow-Methods"", ""GET, POST, DELETE, PUT"")
-            .header(""Access-Control-Allow-Credentials"",""true"")
-            .allow(""OPTIONS"").build();
+    return Response.ok().entity(entity).header(""Access-Control-Allow-Origin"", ""*"").header(""Access-Control-Allow-Methods"", ""GET, POST, DELETE, PUT"").header(""Access-Control-Allow-Credentials"", ""true"").allow(""OPTIONS"").build();
   }
 
   private Response shutdown(String resp) throws Exception {
     Map<String, String> shutdownInfo = new HashMap<String, String>();
     new Thread(new Runnable() {
-        @Override
-        public void run() {
-          try {
-            drillbit.close();
-          } catch (Exception e) {
-            logger.error(""Request to shutdown drillbit failed"", e);
-          }
+      @Override
+      public void run() {
+        try {
+          drillbit.close();
+        } catch (Exception e) {
+          logger.error(""Request to shutdown drillbit failed"", e);
         }
-      }).start();
-    shutdownInfo.put(""response"",resp);
+      }
+    }).start();
+    shutdownInfo.put(""response"", resp);
     return setResponse(shutdownInfo);
   }
 
-/**
- * Pretty-printing wrapper class around the ZK-based queue summary.
- */
+  /**
+   * Pretty-printing wrapper class around the ZK-based queue summary.
+   */
 
-@XmlRootElement
-public static class QueueInfo {
-  private final ZKQueueInfo zkQueueInfo;
+  @XmlRootElement
+  public static class QueueInfo {
+    private final ZKQueueInfo zkQueueInfo;
 
-  public static QueueInfo build(ResourceManager rm) {
+    public static QueueInfo build(ResourceManager rm) {
 
-    // Consider queues enabled only if the ZK-based queues are in use.
+      // Consider queues enabled only if the ZK-based queues are in use.
 
-    ThrottledResourceManager throttledRM = null;
-    if (rm != null && rm instanceof DynamicResourceManager) {
-      DynamicResourceManager dynamicRM = (DynamicResourceManager) rm;
-      rm = dynamicRM.activeRM();
+      ThrottledResourceManager throttledRM = null;
+      if (rm != null && rm instanceof DynamicResourceManager) {","[{'comment': 'Why are these changes here?  This looks like this came from another branch or PR.  Can you please rebase on current master and push again?\r\n\r\n```\r\n> git checkout master\r\n> git fetch upstream\r\n> git rebase upstream/master\r\n> git checkout <your_branch>\r\n> git rebase master\r\n> git push -f\r\n```\r\n', 'commenter': 'cgivre'}]"
2489,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRoot.java,"@@ -181,19 +194,15 @@ public ClusterInfo getClusterInfoJSON() {
     final String currentVersion = currentDrillbit.getVersion();
 
     final DrillConfig config = dbContext.getConfig();
-    final boolean userEncryptionEnabled =
-            config.getBoolean(ExecConstants.USER_ENCRYPTION_SASL_ENABLED) ||
-                    config .getBoolean(ExecConstants.USER_SSL_ENABLED);
+    final boolean userEncryptionEnabled = config.getBoolean(ExecConstants.USER_ENCRYPTION_SASL_ENABLED) || config.getBoolean(ExecConstants.USER_SSL_ENABLED);
     final boolean bitEncryptionEnabled = config.getBoolean(ExecConstants.BIT_ENCRYPTION_SASL_ENABLED);
 
     OptionManager optionManager = work.getContext().getOptionManager();
     final boolean isUserLoggedIn = AuthDynamicFeature.isUserLoggedIn(sc);
-    final boolean shouldShowAdminInfo = isUserLoggedIn && ((DrillUserPrincipal)sc.getUserPrincipal()).isAdminUser();
+    final boolean shouldShowAdminInfo = isUserLoggedIn && ((DrillUserPrincipal) sc.getUserPrincipal()).isAdminUser();","[{'comment': 'nit:  remove space.', 'commenter': 'cgivre'}]"
2489,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRoot.java,"@@ -181,19 +194,15 @@ public ClusterInfo getClusterInfoJSON() {
     final String currentVersion = currentDrillbit.getVersion();
 
     final DrillConfig config = dbContext.getConfig();
-    final boolean userEncryptionEnabled =
-            config.getBoolean(ExecConstants.USER_ENCRYPTION_SASL_ENABLED) ||
-                    config .getBoolean(ExecConstants.USER_SSL_ENABLED);
+    final boolean userEncryptionEnabled = config.getBoolean(ExecConstants.USER_ENCRYPTION_SASL_ENABLED) || config.getBoolean(ExecConstants.USER_SSL_ENABLED);
     final boolean bitEncryptionEnabled = config.getBoolean(ExecConstants.BIT_ENCRYPTION_SASL_ENABLED);
 
     OptionManager optionManager = work.getContext().getOptionManager();
     final boolean isUserLoggedIn = AuthDynamicFeature.isUserLoggedIn(sc);
-    final boolean shouldShowAdminInfo = isUserLoggedIn && ((DrillUserPrincipal)sc.getUserPrincipal()).isAdminUser();
+    final boolean shouldShowAdminInfo = isUserLoggedIn && ((DrillUserPrincipal) sc.getUserPrincipal()).isAdminUser();
 
     for (DrillbitEndpoint endpoint : work.getContext().getAvailableBits()) {
-      final DrillbitInfo drillbit = new DrillbitInfo(endpoint,
-              isDrillbitsTheSame(currentDrillbit, endpoint),
-              currentVersion.equals(endpoint.getVersion()));
+      final DrillbitInfo drillbit = new DrillbitInfo(endpoint, isDrillbitsTheSame(currentDrillbit, endpoint), currentVersion.equals(endpoint.getVersion()));","[{'comment': 'Nit:  Please revert spacing changes. Here and elsewhere.', 'commenter': 'cgivre'}]"
2489,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRoot.java,"@@ -208,18 +217,12 @@ public ClusterInfo getClusterInfoJSON() {
       String adminUsers = ExecConstants.ADMIN_USERS_VALIDATOR.getAdminUsers(optionManager);
       String adminUserGroups = ExecConstants.ADMIN_USER_GROUPS_VALIDATOR.getAdminUserGroups(optionManager);
 
-      logger.debug(""Admin info: user: ""  + adminUsers +  "" user group: "" + adminUserGroups +
-          "" userLoggedIn ""  + isUserLoggedIn + "" shouldShowAdminInfo: "" + shouldShowAdminInfo);
+      logger.debug(""Admin info: user: "" + adminUsers + "" user group: "" + adminUserGroups + "" userLoggedIn "" + isUserLoggedIn + "" shouldShowAdminInfo: "" + shouldShowAdminInfo);","[{'comment': 'Please format log messages as shown below:  (I didn\'t do the whole line but you get the idea.)\r\n\r\n```\r\nlogger.debug(""Admin info: {} user group: {}"", adminUsers, adminUserGroups);\r\n```', 'commenter': 'cgivre'}]"
2489,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRoot.java,"@@ -208,18 +217,12 @@ public ClusterInfo getClusterInfoJSON() {
       String adminUsers = ExecConstants.ADMIN_USERS_VALIDATOR.getAdminUsers(optionManager);
       String adminUserGroups = ExecConstants.ADMIN_USER_GROUPS_VALIDATOR.getAdminUserGroups(optionManager);
 
-      logger.debug(""Admin info: user: ""  + adminUsers +  "" user group: "" + adminUserGroups +
-          "" userLoggedIn ""  + isUserLoggedIn + "" shouldShowAdminInfo: "" + shouldShowAdminInfo);
+      logger.debug(""Admin info: user: "" + adminUsers + "" user group: "" + adminUserGroups + "" userLoggedIn "" + isUserLoggedIn + "" shouldShowAdminInfo: "" + shouldShowAdminInfo);
 
-      return new ClusterInfo(drillbits, currentVersion, mismatchedVersions,
-          userEncryptionEnabled, bitEncryptionEnabled, shouldShowAdminInfo,
-          QueueInfo.build(dbContext.getResourceManager()),
-          processUser, processUserGroups, adminUsers, adminUserGroups, authEnabled.get());
+      return new ClusterInfo(drillbits, currentVersion, mismatchedVersions, userEncryptionEnabled, bitEncryptionEnabled, shouldShowAdminInfo, QueueInfo.build(dbContext.getResourceManager()), processUser, processUserGroups, adminUsers, adminUserGroups, authEnabled.get());","[{'comment': 'Please revert spacing changes, here and elsewheree.', 'commenter': 'cgivre'}]"
2491,Dockerfile,"@@ -56,17 +56,26 @@ RUN VERSION=$(mvn -q -Dexec.executable=echo -Dexec.args='${project.version}' --n
 # Set the BASE_IMAGE build arg when you invoke docker build.  
 FROM $BASE_IMAGE
 
-ENV DRILL_HOME=/opt/drill DRILL_USER=drilluser
+ENV DRILL_HOME=/opt/drill
+ENV DRILL_USER=drilluser
+ENV DRILL_USER_HOME=/var/lib/drill
+ENV DRILL_LOG_DIR=$DRILL_USER_HOME/log
+ENV DATA_VOL=/data
 
-RUN mkdir $DRILL_HOME
+RUN mkdir $DRILL_HOME $DATA_VOL
+
+COPY --from=build /opt/drill $DRILL_HOME","[{'comment': 'With these changes, we will have a much larger size of the docker image, since base layers have a larger size. Please move the copy command as close to the end as possible.', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi I don't think Docker is meant to duplicate data across layers this way.   I think that each layer is supposed to be stored as a delta from the previous layer (even though it may be reported as having the cumulative size of the layers up to that point).  So the layer ordering should not affect the size of the final image.  Neverthess I have moved everything that I could above the COPY in the Dockerfile and I do still worry about a size blowup because when I list images I see 1.47GB for the image from this Dockerfile, while pulling apache/drill:1.20.0-openjdk-8 gives me an image smaller than 1GB.  \r\n\r\n```\r\napache/drill               snapshot-openjdk-8   57306e5337db   3 minutes ago    1.47GB\r\napache/drill               1.20.0-openjdk-8     7479402ba1b3   6 days ago       983MB\r\n```"", 'commenter': 'jnturton'}, {'comment': '@vvysotskyi in the final commit I found and fixed the size blowup.  The `RUN chmod` command was responsible for duplicating the entire Drill installation just to set file attributes, pretty lame CoW system that Docker has there.  Anyway, now the `RUN chmod` is done in the intermediate container.', 'commenter': 'jnturton'}]"
2491,Dockerfile,"@@ -56,17 +56,26 @@ RUN VERSION=$(mvn -q -Dexec.executable=echo -Dexec.args='${project.version}' --n
 # Set the BASE_IMAGE build arg when you invoke docker build.  
 FROM $BASE_IMAGE
 
-ENV DRILL_HOME=/opt/drill DRILL_USER=drilluser
+ENV DRILL_HOME=/opt/drill
+ENV DRILL_USER=drilluser
+ENV DRILL_USER_HOME=/var/lib/drill
+ENV DRILL_LOG_DIR=$DRILL_USER_HOME/log
+ENV DATA_VOL=/data
 
-RUN mkdir $DRILL_HOME
+RUN mkdir $DRILL_HOME $DATA_VOL
+
+COPY --from=build /opt/drill $DRILL_HOME
 
 RUN groupadd -g 999 $DRILL_USER \
- && useradd -r -u 999 -g $DRILL_USER $DRILL_USER -m -d /var/lib/drill \
- && chown -R $DRILL_USER: $DRILL_HOME
+ && useradd -r -u 999 -g $DRILL_USER $DRILL_USER -m -d $DRILL_USER_HOME \
+ && chown $DRILL_USER: $DATA_VOL \
+ && chmod -R +r $DRILL_HOME","[{'comment': 'Instead of doing it here, you can use the `--chmod` flag when doing the COPY command.', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi it introduces a dependency on something called BuildKit, output from my attempt to use this flag:\r\n\r\n```\r\nCOPY --from=build --chmod=0755 /opt/drill $DRILL_HOME\r\nthe --chmod option requires BuildKit. Refer to https://docs.docker.com/go/buildkit/ to learn how to build images with BuildKit enabled\r\n```\r\n\r\nDo you still think it's worth it?"", 'commenter': 'jnturton'}]"
2491,Dockerfile,"@@ -49,25 +49,33 @@ RUN mvn -Dmaven.artifact.threads=5 -T1C clean install -DskipTests
 # Get project version and copy built binaries into /opt/drill directory
 RUN VERSION=$(mvn -q -Dexec.executable=echo -Dexec.args='${project.version}' --non-recursive exec:exec) \
  && mkdir /opt/drill \
- && mv distribution/target/apache-drill-${VERSION}/apache-drill-${VERSION}/* /opt/drill
+ && mv distribution/target/apache-drill-${VERSION}/apache-drill-${VERSION}/* /opt/drill \
+ && chmod -R +r /opt/drill
 
 # Target image
 
 # Set the BASE_IMAGE build arg when you invoke docker build.  
 FROM $BASE_IMAGE
 
-ENV DRILL_HOME=/opt/drill DRILL_USER=drilluser
+# Starts Drill in embedded mode and connects to Sqlline
+ENTRYPOINT $DRILL_HOME/bin/drill-embedded
 
-RUN mkdir $DRILL_HOME
+ENV DRILL_HOME=/opt/drill
+ENV DRILL_USER=drilluser
+ENV DRILL_USER_HOME=/var/lib/drill
+ENV DRILL_LOG_DIR=$DRILL_USER_HOME/log
+ENV DATA_VOL=/data
 
-RUN groupadd -g 999 $DRILL_USER \
- && useradd -r -u 999 -g $DRILL_USER $DRILL_USER -m -d /var/lib/drill \
- && chown -R $DRILL_USER: $DRILL_HOME
+RUN mkdir $DRILL_HOME $DATA_VOL
 
-USER $DRILL_USER
+RUN groupadd -g 999 $DRILL_USER \","[{'comment': 'Could these two commands be combined?', 'commenter': 'vvysotskyi'}, {'comment': 'Yes, thanks.', 'commenter': 'jnturton'}]"
2491,Dockerfile,"@@ -49,25 +49,33 @@ RUN mvn -Dmaven.artifact.threads=5 -T1C clean install -DskipTests
 # Get project version and copy built binaries into /opt/drill directory
 RUN VERSION=$(mvn -q -Dexec.executable=echo -Dexec.args='${project.version}' --non-recursive exec:exec) \
  && mkdir /opt/drill \
- && mv distribution/target/apache-drill-${VERSION}/apache-drill-${VERSION}/* /opt/drill
+ && mv distribution/target/apache-drill-${VERSION}/apache-drill-${VERSION}/* /opt/drill \
+ && chmod -R +r /opt/drill","[{'comment': ""It's also fine if this could help instead of making chown in the target image."", 'commenter': 'vvysotskyi'}]"
2491,Dockerfile,"@@ -49,25 +49,33 @@ RUN mvn -Dmaven.artifact.threads=5 -T1C clean install -DskipTests
 # Get project version and copy built binaries into /opt/drill directory
 RUN VERSION=$(mvn -q -Dexec.executable=echo -Dexec.args='${project.version}' --non-recursive exec:exec) \
  && mkdir /opt/drill \
- && mv distribution/target/apache-drill-${VERSION}/apache-drill-${VERSION}/* /opt/drill
+ && mv distribution/target/apache-drill-${VERSION}/apache-drill-${VERSION}/* /opt/drill \
+ && chmod -R +r /opt/drill
 
 # Target image
 
 # Set the BASE_IMAGE build arg when you invoke docker build.  
 FROM $BASE_IMAGE
 
-ENV DRILL_HOME=/opt/drill DRILL_USER=drilluser
+# Starts Drill in embedded mode and connects to Sqlline
+ENTRYPOINT $DRILL_HOME/bin/drill-embedded
 
-RUN mkdir $DRILL_HOME
+ENV DRILL_HOME=/opt/drill
+ENV DRILL_USER=drilluser
+ENV DRILL_USER_HOME=/var/lib/drill
+ENV DRILL_LOG_DIR=$DRILL_USER_HOME/log
+ENV DATA_VOL=/data
 
-RUN groupadd -g 999 $DRILL_USER \
- && useradd -r -u 999 -g $DRILL_USER $DRILL_USER -m -d /var/lib/drill \
- && chown -R $DRILL_USER: $DRILL_HOME
+RUN mkdir $DRILL_HOME $DATA_VOL
 
-USER $DRILL_USER
+RUN groupadd -g 999 $DRILL_USER \
+ && useradd -r -u 999 -g $DRILL_USER $DRILL_USER -m -d $DRILL_USER_HOME \
+ && chown $DRILL_USER: $DATA_VOL
 
-COPY --from=build --chown=$DRILL_USER /opt/drill $DRILL_HOME
+# A Docker volume where users may store persistent data, e.g. persistent Drill
+# config by specifying a Drill BOOT option of sys.store.provider.local.path: ""/data"".
+VOLUME $DATA_VOL
 
-# Starts Drill in embedded mode and connects to Sqlline
-ENTRYPOINT $DRILL_HOME/bin/drill-embedded","[{'comment': 'Looks like it was deleted...', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi no it was just one of the things that it was possible to move above the COPY :)  I don't think that this particular move helped with image size at all so if we prefer ENTRYPOINT at the end then I can move it back.  Containers launched from this image do still correctly start up drill-embedded even with the ENTRYPOINT higher up."", 'commenter': 'jnturton'}]"
2494,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/JSONRecordReader.java,"@@ -124,19 +139,76 @@ private JSONRecordReader(FragmentContext fragmentContext, Path inputPath, JsonNo
       this.embeddedContent = embeddedContent;
     }
 
+    // If the config is null, create a temporary one with the global options.
+    if (config == null) {
+      this.config = new JSONFormatConfig(null,
+        embeddedContent == null && fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_ALL_TEXT_MODE_VALIDATOR),
+        embeddedContent == null && fragmentContext.getOptions().getOption(ExecConstants.JSON_READ_NUMBERS_AS_DOUBLE_VALIDATOR),
+        fragmentContext.getOptions().getOption(ExecConstants.JSON_SKIP_MALFORMED_RECORDS_VALIDATOR),
+        fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_ESCAPE_ANY_CHAR_VALIDATOR),
+        fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_NAN_INF_NUMBERS_VALIDATOR));
+    } else {
+      this.config = config;
+    }
+
     this.fileSystem = fileSystem;
     this.fragmentContext = fragmentContext;
-    // only enable all text mode if we aren't using embedded content mode.
-    this.enableAllTextMode = embeddedContent == null && fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_ALL_TEXT_MODE_VALIDATOR);
-    this.enableNanInf = fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_NAN_INF_NUMBERS_VALIDATOR);
-    this.enableEscapeAnyChar = fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_ESCAPE_ANY_CHAR_VALIDATOR);
-    this.readNumbersAsDouble = embeddedContent == null && fragmentContext.getOptions().getOption(ExecConstants.JSON_READ_NUMBERS_AS_DOUBLE_VALIDATOR);
+
+    this.enableAllTextMode = allTextMode();
+    this.enableNanInf = nanInf();
+    this.enableEscapeAnyChar = escapeAnyChar();
+    this.readNumbersAsDouble = readNumbersAsDouble();
     this.unionEnabled = embeddedContent == null && fragmentContext.getOptions().getBoolean(ExecConstants.ENABLE_UNION_TYPE_KEY);
-    this.skipMalformedJSONRecords = fragmentContext.getOptions().getOption(ExecConstants.JSON_SKIP_MALFORMED_RECORDS_VALIDATOR);
+    this.skipMalformedJSONRecords = skipMalformedJSONRecords();
     this.printSkippedMalformedJSONRecordLineNumber = fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_PRINT_INVALID_RECORDS_LINE_NOS_FLAG_VALIDATOR);
     setColumns(columns);
   }
 
+  /**
+   * Returns the value of the all text mode.  Values set in the format config will override global values.
+   * @return The value of allTextMode
+   */
+  private boolean allTextMode() {
+    // only enable all text mode if we aren't using embedded content mode.
+    if (config.getAllTextMode() == null) {
+      return embeddedContent == null && fragmentContext.getOptions().getOption(ExecConstants.JSON_READER_ALL_TEXT_MODE_VALIDATOR);","[{'comment': 'Please ensure that we preserve the existing option scope precedence: SYSTEM < FORMAT < SESSION < QUERY.', 'commenter': 'jnturton'}, {'comment': ""@jnturton \r\nThanks for the comments.  I followed the pattern you suggested and now the precedence is SYSTEM < FORMAT < SESSION.  One thing to note is that I don't think you can override the session scope.  Drill doesn't seem to have the distinction between format and query.  But... this follows the pattern from parquet so at least it is consistent."", 'commenter': 'cgivre'}]"
2496,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperFunctions.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.udfs;
+
+import io.netty.buffer.DrillBuf;
+import org.apache.drill.exec.expr.DrillSimpleFunc;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate;
+import org.apache.drill.exec.expr.annotations.Output;
+import org.apache.drill.exec.expr.annotations.Param;
+import org.apache.drill.exec.expr.annotations.Workspace;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.server.options.OptionManager;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter.ComplexWriter;
+
+import javax.inject.Inject;
+
+public class HttpHelperFunctions {
+
+  @FunctionTemplate(names = {""http_get_url"", ""httpGetUrl""},
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    isVarArg = true)
+  public static class HttpGetFunction implements DrillSimpleFunc {
+
+    @Param
+    FieldReader[] inputReaders;
+
+    @Output
+    ComplexWriter writer;
+
+    @Inject
+    OptionManager options;
+
+    @Inject
+    DrillBuf buffer;
+
+    @Workspace
+    org.apache.drill.exec.vector.complex.fn.JsonReader jsonReader;
+
+    @Override
+    public void setup() {
+      jsonReader = new org.apache.drill.exec.vector.complex.fn.JsonReader.Builder(buffer)
+        .defaultSchemaPathColumns()
+        .readNumbersAsDouble(options.getOption(org.apache.drill.exec.ExecConstants.JSON_READ_NUMBERS_AS_DOUBLE).bool_val)
+        .allTextMode(options.getOption(org.apache.drill.exec.ExecConstants.JSON_ALL_TEXT_MODE).bool_val)
+        .enableNanInf(options.getOption(org.apache.drill.exec.ExecConstants.JSON_READER_NAN_INF_NUMBERS).bool_val)
+        .build();
+    }
+
+    @Override
+    public void eval() {
+      if (inputReaders.length > 0) {
+        // Get the URL
+        FieldReader urlReader = inputReaders[0];
+        String url = urlReader.readObject().toString();
+
+        // Process Positional Arguments
+        java.util.List args = org.apache.drill.exec.store.http.udfs.HttpHelperUtils.buildParameterList(inputReaders);
+        String finalUrl = org.apache.drill.exec.util.HttpUtils.mapPositionalParameters(url, args);
+
+        // Make the API call
+        String results = org.apache.drill.exec.util.HttpUtils.makeSimpleGetRequest(finalUrl);
+
+        // If the result string is null or empty, return an empty map
+        if (results == null || results.length() == 0) {
+          // Return empty map
+          org.apache.drill.exec.vector.complex.writer.BaseWriter.MapWriter mapWriter = writer.rootAsMap();
+          mapWriter.start();
+          mapWriter.end();
+          return;
+        }
+
+        try {
+          jsonReader.setSource(results);
+          jsonReader.setIgnoreJSONParseErrors(true);  // Reduce number of errors
+          jsonReader.write(writer);
+          buffer = jsonReader.getWorkBuf();
+        } catch (Exception e) {
+          throw new org.apache.drill.common.exceptions.DrillRuntimeException(""Error while converting from JSON. "", e);
+        }
+      }
+    }
+  }
+
+
+  @FunctionTemplate(names = {""http_get"", ""httpGet""},","[{'comment': 'If this UDF is not only for for GET requests, but for whatever request type is configured in the named storage config then can we name it differently?  `http_req_storage` or something?', 'commenter': 'jnturton'}, {'comment': ""sure!  Do you think I should rename the other one 'http_get' because that is only a simple http get?"", 'commenter': 'cgivre'}, {'comment': ""Yes I think the other one can become `http_get`.  Maybe later it will be joined by an `http_post` which has a parameter called `content_type` that allows values of 'application/x-www-form-urlencoded' and 'application/json' or something."", 'commenter': 'jnturton'}, {'comment': 'Done!  I called them `http_get` for the simple get request, and `http_request` for the request from storage.', 'commenter': 'cgivre'}]"
2496,exec/java-exec/src/main/java/org/apache/drill/exec/util/HttpUtils.java,"@@ -0,0 +1,168 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.util;","[{'comment': ""This class looks like it belongs in contrib/storage-http.  Is there a reason why it's here?"", 'commenter': 'jnturton'}, {'comment': 'So... originally, I started work with this in the `udfs` package.  I thought it would be a good idea to make a Drill utils class that can be accessed anywhere that has all the associated methods for making http requests.   Then, I ended up moving the UDF to `storage-http` because it needed the http storage config and never relocated it.  \r\n\r\nIf you feel strongly about it, I can move all those methods to `SimpleHttp` and eliminate the class.\r\n', 'commenter': 'cgivre'}, {'comment': ""Thank you, I think it's better to move or absorb it into something in storage-http.  To preserve layering and to eventually decouple our plugins successfully, we will want them not to have introduced plugin-specific code into exec/.  These utils are specific to storage-http in places e.g. `getDefaultParameterValue(...)`"", 'commenter': 'jnturton'}, {'comment': 'Done!', 'commenter': 'cgivre'}]"
2496,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperFunctions.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.udfs;
+
+import io.netty.buffer.DrillBuf;
+import org.apache.drill.exec.expr.DrillSimpleFunc;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate;
+import org.apache.drill.exec.expr.annotations.Output;
+import org.apache.drill.exec.expr.annotations.Param;
+import org.apache.drill.exec.expr.annotations.Workspace;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.server.options.OptionManager;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter.ComplexWriter;
+
+import javax.inject.Inject;
+
+public class HttpHelperFunctions {
+
+  @FunctionTemplate(names = {""http_get_url"", ""httpGetUrl""},
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    isVarArg = true)
+  public static class HttpGetFunction implements DrillSimpleFunc {
+
+    @Param
+    FieldReader[] inputReaders;
+
+    @Output
+    ComplexWriter writer;
+
+    @Inject
+    OptionManager options;
+
+    @Inject
+    DrillBuf buffer;
+
+    @Workspace
+    org.apache.drill.exec.vector.complex.fn.JsonReader jsonReader;
+
+    @Override
+    public void setup() {
+      jsonReader = new org.apache.drill.exec.vector.complex.fn.JsonReader.Builder(buffer)
+        .defaultSchemaPathColumns()
+        .readNumbersAsDouble(options.getOption(org.apache.drill.exec.ExecConstants.JSON_READ_NUMBERS_AS_DOUBLE).bool_val)
+        .allTextMode(options.getOption(org.apache.drill.exec.ExecConstants.JSON_ALL_TEXT_MODE).bool_val)
+        .enableNanInf(options.getOption(org.apache.drill.exec.ExecConstants.JSON_READER_NAN_INF_NUMBERS).bool_val)
+        .build();
+    }
+
+    @Override
+    public void eval() {
+      if (inputReaders.length > 0) {
+        // Get the URL
+        FieldReader urlReader = inputReaders[0];","[{'comment': ""If the first argument is the URL string, is it makes sense to handle it separately instead of the list? As a bonus, this UDF wouldn't match if the URL wasn't specified."", 'commenter': 'vvysotskyi'}, {'comment': 'Done!', 'commenter': 'cgivre'}]"
2496,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperFunctions.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.udfs;
+
+import io.netty.buffer.DrillBuf;
+import org.apache.drill.exec.expr.DrillSimpleFunc;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate;
+import org.apache.drill.exec.expr.annotations.Output;
+import org.apache.drill.exec.expr.annotations.Param;
+import org.apache.drill.exec.expr.annotations.Workspace;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.server.options.OptionManager;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter.ComplexWriter;
+
+import javax.inject.Inject;
+
+public class HttpHelperFunctions {
+
+  @FunctionTemplate(names = {""http_get_url"", ""httpGetUrl""},
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    isVarArg = true)
+  public static class HttpGetFunction implements DrillSimpleFunc {
+
+    @Param
+    FieldReader[] inputReaders;
+
+    @Output
+    ComplexWriter writer;
+
+    @Inject
+    OptionManager options;
+
+    @Inject
+    DrillBuf buffer;
+
+    @Workspace
+    org.apache.drill.exec.vector.complex.fn.JsonReader jsonReader;
+
+    @Override
+    public void setup() {
+      jsonReader = new org.apache.drill.exec.vector.complex.fn.JsonReader.Builder(buffer)
+        .defaultSchemaPathColumns()
+        .readNumbersAsDouble(options.getOption(org.apache.drill.exec.ExecConstants.JSON_READ_NUMBERS_AS_DOUBLE).bool_val)
+        .allTextMode(options.getOption(org.apache.drill.exec.ExecConstants.JSON_ALL_TEXT_MODE).bool_val)
+        .enableNanInf(options.getOption(org.apache.drill.exec.ExecConstants.JSON_READER_NAN_INF_NUMBERS).bool_val)
+        .build();
+    }
+
+    @Override
+    public void eval() {
+      if (inputReaders.length > 0) {
+        // Get the URL
+        FieldReader urlReader = inputReaders[0];
+        String url = urlReader.readObject().toString();","[{'comment': 'If it is a String and we assume that it should be constant, why not declare the input with `@Param(constant = true)` and use `VarCharHolder`?', 'commenter': 'vvysotskyi'}, {'comment': 'I did this for the UDF that accepts a storage plugin as the first argument, but not the first one.  The reason being that the there are some APIs which return URLs and I could imagine a use case where a user might want to generate a list of URLs then call them. ', 'commenter': 'cgivre'}]"
2496,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperUtils.java,"@@ -0,0 +1,178 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.udfs;
+
+import okhttp3.HttpUrl;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.store.StoragePlugin;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.StoragePluginRegistry.PluginException;
+import org.apache.drill.exec.store.http.HttpApiConfig;
+import org.apache.drill.exec.store.http.HttpStoragePlugin;
+import org.apache.drill.exec.store.http.HttpStoragePluginConfig;
+import org.apache.drill.exec.store.http.util.HttpProxyConfig;
+import org.apache.drill.exec.store.http.util.SimpleHttp;
+import org.apache.drill.exec.store.http.util.SimpleHttp.SimpleHttpBuilder;
+import org.apache.drill.exec.util.HttpUtils;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.List;
+
+public class HttpHelperUtils {
+
+  private static Logger logger = LoggerFactory.getLogger(HttpHelperUtils.class);
+  /**
+   * Accepts a list of input readers and converts that into an ArrayList of Strings
+   * @param inputReaders The array of FieldReaders
+   * @return A List of Strings containing the values from the FieldReaders.
+   */
+  public static List<String> buildParameterList(FieldReader[] inputReaders) {
+    List<String> inputArguments = new ArrayList<>();
+
+    // Skip the first argument because that is the input URL
+    for (int i = 1; i < inputReaders.length; i++) {
+      inputArguments.add(inputReaders[i].readObject().toString());
+    }
+
+    return inputArguments;
+  }
+
+  public static HttpStoragePluginConfig getPluginConfig(String name, DrillbitContext context) throws PluginException {
+    HttpStoragePlugin httpStoragePlugin = getStoragePlugin(context, name);
+    return httpStoragePlugin.getConfig();
+  }
+
+  public static HttpApiConfig getEndpointConfig(String name, DrillbitContext context) {
+    // Get the plugin name and endpoint name
+    String[] parts = name.split(""\\."");
+    if (parts.length < 2) {
+      throw UserException.functionError()
+        .message(""You must call this function with a connection name and endpoint."")
+        .build(logger);
+    }
+    String plugin = parts[0];
+    String endpoint = parts[1];
+
+    HttpStoragePlugin httpStoragePlugin = getStoragePlugin(context, plugin);
+    HttpStoragePluginConfig config = httpStoragePlugin.getConfig();
+
+    HttpApiConfig endpointConfig = config.getConnection(endpoint);
+    if (endpointConfig == null) {
+      throw UserException.functionError()
+        .message(""You must call this function with a valid endpoint name."")
+        .build(logger);
+    } else if (endpointConfig.inputType() != ""json"") {
+      throw UserException.functionError()
+        .message(""Http_get only supports API endpoints which return json."")
+        .build(logger);
+    }
+
+    return endpointConfig;
+  }
+
+  private static HttpStoragePlugin getStoragePlugin(DrillbitContext context, String pluginName) {
+    StoragePluginRegistry storage = context.getStorage();
+    try {
+      StoragePlugin pluginInstance = storage.getPlugin(pluginName);
+      if (pluginInstance == null) {
+        throw UserException.functionError()
+          .message(pluginName + "" is not a valid plugin."")
+          .build(logger);
+      }
+
+      if (!(pluginInstance instanceof HttpStoragePlugin)) {
+        throw UserException.functionError()
+          .message(""You can only include HTTP plugins in this function."")
+          .build(logger);
+      }
+      return (HttpStoragePlugin) pluginInstance;
+    } catch (PluginException e) {
+      throw UserException.functionError()
+        .message(""Could not access plugin "" + pluginName)
+        .build(logger);
+    }
+  }
+
+
+  /**
+   * This function makes an API call and returns a string of the parsed results. It is used in the http_get() UDF
+   * and retrieves all the configuration parameters contained in the storage plugin and endpoint configuration. The exception
+   * is pagination.  This does not support pagination.
+   * @param schemaPath The path of storage_plugin.endpoint from which the data will be retrieved
+   * @param context {@link DrillbitContext} The context from the current query
+   * @param args An optional list of parameter arguments which will be included in the URL
+   * @return A String of the results.
+   */
+  public static String makeAPICall(String schemaPath, DrillbitContext context, List<String> args) {
+    HttpStoragePluginConfig pluginConfig;
+    HttpApiConfig endpointConfig;
+
+    // Get the plugin name and endpoint name
+    String[] parts = schemaPath.split(""\\."");
+    if (parts.length < 2) {
+      throw UserException.functionError()
+        .message(""You must call this function with a connection name and endpoint."")
+        .build(logger);
+    }
+    String pluginName = parts[0];","[{'comment': ""In this case, we should document that a fully qualified schema should be used, i.e. `use http;` before running the query wouldn't affect anything."", 'commenter': 'vvysotskyi'}, {'comment': 'Thanks @vvysotskyi.  In this case, the URL is first validated in the `mapPositionalArguments` function.   I added this to the documentation as well. ', 'commenter': 'cgivre'}]"
2496,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperUtils.java,"@@ -0,0 +1,178 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.udfs;
+
+import okhttp3.HttpUrl;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.store.StoragePlugin;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.StoragePluginRegistry.PluginException;
+import org.apache.drill.exec.store.http.HttpApiConfig;
+import org.apache.drill.exec.store.http.HttpStoragePlugin;
+import org.apache.drill.exec.store.http.HttpStoragePluginConfig;
+import org.apache.drill.exec.store.http.util.HttpProxyConfig;
+import org.apache.drill.exec.store.http.util.SimpleHttp;
+import org.apache.drill.exec.store.http.util.SimpleHttp.SimpleHttpBuilder;
+import org.apache.drill.exec.util.HttpUtils;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.List;
+
+public class HttpHelperUtils {
+
+  private static Logger logger = LoggerFactory.getLogger(HttpHelperUtils.class);
+  /**
+   * Accepts a list of input readers and converts that into an ArrayList of Strings
+   * @param inputReaders The array of FieldReaders
+   * @return A List of Strings containing the values from the FieldReaders.
+   */
+  public static List<String> buildParameterList(FieldReader[] inputReaders) {
+    List<String> inputArguments = new ArrayList<>();
+
+    // Skip the first argument because that is the input URL
+    for (int i = 1; i < inputReaders.length; i++) {
+      inputArguments.add(inputReaders[i].readObject().toString());
+    }
+
+    return inputArguments;
+  }
+
+  public static HttpStoragePluginConfig getPluginConfig(String name, DrillbitContext context) throws PluginException {
+    HttpStoragePlugin httpStoragePlugin = getStoragePlugin(context, name);
+    return httpStoragePlugin.getConfig();
+  }
+
+  public static HttpApiConfig getEndpointConfig(String name, DrillbitContext context) {
+    // Get the plugin name and endpoint name
+    String[] parts = name.split(""\\."");
+    if (parts.length < 2) {
+      throw UserException.functionError()
+        .message(""You must call this function with a connection name and endpoint."")
+        .build(logger);
+    }
+    String plugin = parts[0];
+    String endpoint = parts[1];
+
+    HttpStoragePlugin httpStoragePlugin = getStoragePlugin(context, plugin);
+    HttpStoragePluginConfig config = httpStoragePlugin.getConfig();
+
+    HttpApiConfig endpointConfig = config.getConnection(endpoint);
+    if (endpointConfig == null) {
+      throw UserException.functionError()
+        .message(""You must call this function with a valid endpoint name."")
+        .build(logger);
+    } else if (endpointConfig.inputType() != ""json"") {
+      throw UserException.functionError()
+        .message(""Http_get only supports API endpoints which return json."")
+        .build(logger);
+    }
+
+    return endpointConfig;
+  }
+
+  private static HttpStoragePlugin getStoragePlugin(DrillbitContext context, String pluginName) {
+    StoragePluginRegistry storage = context.getStorage();
+    try {
+      StoragePlugin pluginInstance = storage.getPlugin(pluginName);
+      if (pluginInstance == null) {
+        throw UserException.functionError()
+          .message(pluginName + "" is not a valid plugin."")
+          .build(logger);
+      }
+
+      if (!(pluginInstance instanceof HttpStoragePlugin)) {
+        throw UserException.functionError()
+          .message(""You can only include HTTP plugins in this function."")
+          .build(logger);
+      }
+      return (HttpStoragePlugin) pluginInstance;
+    } catch (PluginException e) {
+      throw UserException.functionError()
+        .message(""Could not access plugin "" + pluginName)
+        .build(logger);
+    }
+  }
+
+
+  /**
+   * This function makes an API call and returns a string of the parsed results. It is used in the http_get() UDF
+   * and retrieves all the configuration parameters contained in the storage plugin and endpoint configuration. The exception
+   * is pagination.  This does not support pagination.
+   * @param schemaPath The path of storage_plugin.endpoint from which the data will be retrieved
+   * @param context {@link DrillbitContext} The context from the current query
+   * @param args An optional list of parameter arguments which will be included in the URL
+   * @return A String of the results.
+   */
+  public static String makeAPICall(String schemaPath, DrillbitContext context, List<String> args) {
+    HttpStoragePluginConfig pluginConfig;
+    HttpApiConfig endpointConfig;
+
+    // Get the plugin name and endpoint name
+    String[] parts = schemaPath.split(""\\."");
+    if (parts.length < 2) {
+      throw UserException.functionError()
+        .message(""You must call this function with a connection name and endpoint."")
+        .build(logger);
+    }
+    String pluginName = parts[0];
+
+    HttpStoragePlugin plugin = getStoragePlugin(context, pluginName);
+
+    try {
+      pluginConfig = getPluginConfig(pluginName, context);
+      endpointConfig = getEndpointConfig(schemaPath, context);","[{'comment': 'Instead of obtaining `pluginConfig` here and in `getEndpointConfig`, you could change `getEndpointConfig` to accept obtained `pluginConfig` earlier.', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed!', 'commenter': 'cgivre'}]"
2496,exec/java-exec/src/main/java/org/apache/drill/exec/util/HttpUtils.java,"@@ -0,0 +1,168 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.util;
+
+import okhttp3.HttpUrl;
+import okhttp3.OkHttpClient;
+import okhttp3.Request;
+import okhttp3.Response;
+import org.apache.drill.common.exceptions.UserException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.io.UnsupportedEncodingException;
+import java.net.URLDecoder;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+public class HttpUtils {","[{'comment': 'This class could be moved to http plugin instead of exec.', 'commenter': 'vvysotskyi'}, {'comment': 'Done!  Removed this class entirely. ', 'commenter': 'cgivre'}]"
2496,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperUtils.java,"@@ -0,0 +1,178 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.udfs;
+
+import okhttp3.HttpUrl;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.store.StoragePlugin;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.StoragePluginRegistry.PluginException;
+import org.apache.drill.exec.store.http.HttpApiConfig;
+import org.apache.drill.exec.store.http.HttpStoragePlugin;
+import org.apache.drill.exec.store.http.HttpStoragePluginConfig;
+import org.apache.drill.exec.store.http.util.HttpProxyConfig;
+import org.apache.drill.exec.store.http.util.SimpleHttp;
+import org.apache.drill.exec.store.http.util.SimpleHttp.SimpleHttpBuilder;
+import org.apache.drill.exec.util.HttpUtils;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.List;
+
+public class HttpHelperUtils {
+
+  private static Logger logger = LoggerFactory.getLogger(HttpHelperUtils.class);
+  /**
+   * Accepts a list of input readers and converts that into an ArrayList of Strings
+   * @param inputReaders The array of FieldReaders
+   * @return A List of Strings containing the values from the FieldReaders.
+   */
+  public static List<String> buildParameterList(FieldReader[] inputReaders) {
+    List<String> inputArguments = new ArrayList<>();
+
+    // Skip the first argument because that is the input URL
+    for (int i = 1; i < inputReaders.length; i++) {
+      inputArguments.add(inputReaders[i].readObject().toString());
+    }
+
+    return inputArguments;
+  }
+
+  public static HttpStoragePluginConfig getPluginConfig(String name, DrillbitContext context) throws PluginException {
+    HttpStoragePlugin httpStoragePlugin = getStoragePlugin(context, name);
+    return httpStoragePlugin.getConfig();
+  }
+
+  public static HttpApiConfig getEndpointConfig(String name, DrillbitContext context) {
+    // Get the plugin name and endpoint name
+    String[] parts = name.split(""\\."");
+    if (parts.length < 2) {
+      throw UserException.functionError()
+        .message(""You must call this function with a connection name and endpoint."")
+        .build(logger);
+    }
+    String plugin = parts[0];
+    String endpoint = parts[1];
+
+    HttpStoragePlugin httpStoragePlugin = getStoragePlugin(context, plugin);
+    HttpStoragePluginConfig config = httpStoragePlugin.getConfig();
+
+    HttpApiConfig endpointConfig = config.getConnection(endpoint);
+    if (endpointConfig == null) {
+      throw UserException.functionError()
+        .message(""You must call this function with a valid endpoint name."")
+        .build(logger);
+    } else if (endpointConfig.inputType() != ""json"") {
+      throw UserException.functionError()
+        .message(""Http_get only supports API endpoints which return json."")
+        .build(logger);
+    }
+
+    return endpointConfig;
+  }
+
+  private static HttpStoragePlugin getStoragePlugin(DrillbitContext context, String pluginName) {
+    StoragePluginRegistry storage = context.getStorage();
+    try {
+      StoragePlugin pluginInstance = storage.getPlugin(pluginName);
+      if (pluginInstance == null) {
+        throw UserException.functionError()
+          .message(pluginName + "" is not a valid plugin."")
+          .build(logger);
+      }
+
+      if (!(pluginInstance instanceof HttpStoragePlugin)) {
+        throw UserException.functionError()
+          .message(""You can only include HTTP plugins in this function."")
+          .build(logger);
+      }
+      return (HttpStoragePlugin) pluginInstance;
+    } catch (PluginException e) {
+      throw UserException.functionError()
+        .message(""Could not access plugin "" + pluginName)
+        .build(logger);
+    }
+  }
+
+
+  /**
+   * This function makes an API call and returns a string of the parsed results. It is used in the http_get() UDF
+   * and retrieves all the configuration parameters contained in the storage plugin and endpoint configuration. The exception
+   * is pagination.  This does not support pagination.
+   * @param schemaPath The path of storage_plugin.endpoint from which the data will be retrieved
+   * @param context {@link DrillbitContext} The context from the current query
+   * @param args An optional list of parameter arguments which will be included in the URL
+   * @return A String of the results.
+   */
+  public static String makeAPICall(String schemaPath, DrillbitContext context, List<String> args) {
+    HttpStoragePluginConfig pluginConfig;
+    HttpApiConfig endpointConfig;
+
+    // Get the plugin name and endpoint name
+    String[] parts = schemaPath.split(""\\."");
+    if (parts.length < 2) {
+      throw UserException.functionError()
+        .message(""You must call this function with a connection name and endpoint."")
+        .build(logger);
+    }
+    String pluginName = parts[0];
+
+    HttpStoragePlugin plugin = getStoragePlugin(context, pluginName);
+
+    try {
+      pluginConfig = getPluginConfig(pluginName, context);
+      endpointConfig = getEndpointConfig(schemaPath, context);
+    } catch (PluginException e) {
+      throw UserException.functionError()
+        .message(""Could not access plugin "" + pluginName)
+        .build(logger);
+    }
+
+    // Get proxy settings
+    HttpProxyConfig proxyConfig = SimpleHttp.getProxySettings(pluginConfig, context.getConfig(), endpointConfig.getHttpUrl());
+
+    // For this use case, we will replace the URL parameters here, rather than doing it in the SimpleHttp client
+    // because we are using positional mapping rather than k/v pairs for this.
+    String finalUrl;","[{'comment': 'could some of this logic be combined with the code from `HttpBatchReader`, so we will have a single place with the logic related to handling it?', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.  Moved everything to `SimpleHttp`.', 'commenter': 'cgivre'}]"
2496,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperFunctions.java,"@@ -0,0 +1,167 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.udfs;
+
+import io.netty.buffer.DrillBuf;
+import org.apache.drill.exec.expr.DrillSimpleFunc;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate;
+import org.apache.drill.exec.expr.annotations.Output;
+import org.apache.drill.exec.expr.annotations.Param;
+import org.apache.drill.exec.expr.annotations.Workspace;
+import org.apache.drill.exec.expr.holders.VarCharHolder;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.server.options.OptionManager;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+import org.apache.drill.exec.vector.complex.writer.BaseWriter.ComplexWriter;
+
+import javax.inject.Inject;
+
+public class HttpHelperFunctions {
+
+  @FunctionTemplate(names = {""http_get"", ""httpGet""},
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    isVarArg = true)
+  public static class HttpGetFunction implements DrillSimpleFunc {
+
+    @Param
+    VarCharHolder rawInput;
+
+    @Param
+    FieldReader[] inputReaders;","[{'comment': 'This one could also be `VarCharHolder[]`.', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi One thing to think about is that the input args may not be strings.  If I make this a `VarCharHolder` would that break things if the user tries to use an int field?', 'commenter': 'cgivre'}, {'comment': ""I'm not sure regarding current implementation, whether it would work with arguments of different types."", 'commenter': 'vvysotskyi'}, {'comment': 'Thanks @vvysotskyi.  I changed to `VarCharHolder[]` for both functions.', 'commenter': 'cgivre'}]"
2499,exec/java-exec/src/test/java/org/apache/drill/exec/impersonation/TestInboundImpersonation.java,"@@ -156,22 +159,25 @@ public void unauthorizedTarget() throws Exception {
 
   @Test
   public void invalidPolicy() throws Exception {
-    thrownException.expect(new UserExceptionMatcher(UserBitShared.DrillPBError.ErrorType.VALIDATION,
-        ""Invalid impersonation policies.""));
+    String query = ""ALTER SYSTEM SET `%s`='%s'"";","[{'comment': 'We have methods that do this work: it is very common. Maybe the required clients are not available in this particular test?', 'commenter': 'paul-rogers'}, {'comment': 'Hi Paul,\r\nGreetings to you!\r\nI did this because the UT code in Drill used `ExpectedException thrown = ExpectedException.none();`\r\nfor ExpectedException instantiation.\r\nHowever the method `ExpectedException.none()` has been deprecated, so I replaced `ExpectedException.none()` by `Assert.assertThrows` as the [deprecation notice](https://junit.org/junit4/javadoc/latest/org/junit/rules/ExpectedException.html#none()) suggested.\r\nCould you please give some suggestions will be really appreciated.\r\nBest,\r\nJingchuan', 'commenter': 'kingswanwho'}, {'comment': ""Just picking one test file at random, here's an example of the idea: https://github.com/apache/drill/blob/master/exec/java-exec/src/test/java/org/apache/drill/exec/compile/TestLargeFileCompilation.java#L273\r\n\r\nIt is not about the exception handling, which is fine. It's just about the busy-work of formatting the `ALTER SYSTEM SET` commands over and over.\r\n\r\nAgain, without looking closely, I don't know if this test happens to use one of the two clients that have these methods. If not, then you have to do things the tedious way."", 'commenter': 'paul-rogers'}, {'comment': 'Hi Paul, it does really help! I will replace all those hard-code in my PR by client  thanks!', 'commenter': 'kingswanwho'}, {'comment': '@paul-rogers Hi Paul, I am sorry that I dragged this PR out too long. I replaced hard-code to FixtureClient, but UT failed due to the Error:  \r\n`java.lang.AssertionError: unexpected exception type thrown; expected:<org.apache.drill.common.exceptions.UserException> but was:<java.lang.IllegalStateException>\r\n\tat org.apache.drill.exec.impersonation.TestInboundImpersonation.invalidProxy(TestInboundImpersonation.java:186)\r\nCaused by: java.lang.IllegalStateException: \r\norg.apache.drill.common.exceptions.UserRemoteException: PERMISSION ERROR: Cannot change option exec.impersonation.inbound_policies in scope SESSION`  \r\nI think the two clients cannot exist at the same time, so I changed back to the hard-code way.', 'commenter': 'kingswanwho'}, {'comment': 'Did you try the following here?\r\n```\r\nclient.alterSystem(...);\r\ntry {\r\n  // run test\r\n} finally {\r\n  client.resetSystem(...);\r\n}\r\n```', 'commenter': 'jnturton'}, {'comment': ""@jnturton Hi James, the **_client_** here inherits from BaseTestQuery, which is DrillClient but not ClientFixture in TestLargeFileCompilation as Paul suggested. DrillClient doesn't have method like client.alterSystem(). BaseTestQuery is deprecated actually, but not marked as deprecated because it is still widely used. The best choice here I think is change base test class from BaseTestQuery to ClusterTest so that we could use client.alterSystem() for clean and compact code style. There are multiple Inheritance levels here, which are TestInboundImpersonation -> BaseTestImpersonation -> PlanTestBase -> BaseTestQuery. Due to PlanTestBase involves many classes. Maybe change BaseTestImpersonation inheritance from PlanTestBase to ClusterTest directly is an easier way. However, this PR mainly focus on the deprecated clean up, but induces more modifications on UT. Is this an appropriate change here? "", 'commenter': 'kingswanwho'}, {'comment': ""Ah, okay. In that case I think we can go two ways here, since upgrading this test, or family of tests, to ClusterTest could be a significant expansion of PR's scope.\r\n\r\n1. We can merge this as it is and leave the ClusterTest work for another PR or\r\n2. we can expand the scope here and you can convert BaseTestImpersonation from PlanTestBase to ClusterTest. I believe that ClusterTest offers the plan string matching features in PlanTestBase.\r\n\r\nI leave the choice for you to make based on which way you prefer to work."", 'commenter': 'jnturton'}, {'comment': ""Hi James, I would like to apply the second way, since this PR has been existing for a while, it doesn't matter to wait for a bit longer : )"", 'commenter': 'kingswanwho'}, {'comment': ""Okay great! Let's switch this PR to Draft mode to show that you're still busy."", 'commenter': 'jnturton'}]"
2499,exec/java-exec/src/test/java/org/apache/drill/test/ClusterTest.java,"@@ -125,4 +132,56 @@ public static void run(String query, Object... args) throws Exception {
   public QueryBuilder queryBuilder( ) {
     return client.queryBuilder();
   }
+
+  /**
+   * Utility method which tests given query produces a {@link UserException} and the exception message contains
+   * the given message.
+   * @param testSqlQuery Test query
+   * @param expectedErrorMsg Expected error message.
+   */
+  protected static void errorMsgTestHelper(String testSqlQuery, String expectedErrorMsg) throws Exception {
+    try {
+      run(testSqlQuery);
+      fail(""Expected a UserException when running "" + testSqlQuery);
+    } catch (UserException actualException) {
+      try {
+        assertThat(""message of UserException when running "" + testSqlQuery, actualException.getMessage(), containsString(expectedErrorMsg));
+      } catch (AssertionError e) {
+        e.addSuppressed(actualException);
+        throw e;
+      }
+    }
+  }
+
+  protected static void updateClient(Properties properties) {
+    if (client != null) {
+      client.close();
+      client = null;
+    }
+    ClientFixture.ClientBuilder clientBuilder = cluster.clientBuilder();
+    if (properties != null) {
+      for (final String key : properties.stringPropertyNames()) {
+        final String lowerCaseKey = key.toLowerCase();
+        clientBuilder.property(lowerCaseKey, properties.getProperty(key));
+      }
+    }
+    client = clientBuilder.build();
+  }
+
+  protected static void updateClient(final String user) {
+    updateClient(user, null);
+  }
+
+  protected static void updateClient(final String user, final String password) {
+    if (client != null) {
+      client.close();
+      client = null;
+    }
+    final Properties properties = new Properties();
+    properties.setProperty(DrillProperties.USER, user);
+    if (password != null) {
+      properties.setProperty(DrillProperties.PASSWORD, password);
+    }
+    updateClient(properties);
+  }","[{'comment': ""Let's move these additions to ClientFixture. It will mean that they're no longer drop-in replacements but they'll be more at home there."", 'commenter': 'jnturton'}, {'comment': ""Hi James, ClientFixture doesn't have a client variable there, so that we could't close client at first and we need to change updateClient signature from void to return ClientFixture, besides due to cluster variable here is none static, we also need to change updateClient from static method to none static method, and those modifications involve lots of code change. Furthermore, there are still lots of test cases that we could just update client without restart cluster, and for those cases that do need restart cluster, we could use startCluster method to config new cluster and client at same time without using updateClient. So maybe we could still keep those code here?"", 'commenter': 'kingswanwho'}, {'comment': ""Ah, yes you're right. These methods still look a bit out of place. Can errorMsgTestHelper move to ClientFixture? And can the new updateClient methods be replaced by usage of cluster.addClientFixture(Properties p) and cluster.client(int number)?"", 'commenter': 'jnturton'}, {'comment': ""Let me test out a couple of things out locally. If they work I'll send a PR to you, otherwise we'll be ready to merge as is."", 'commenter': 'jnturton'}, {'comment': ""Thanks James! I have tried cluster.addClientFixture(), due to this method doesn't copy all the client properties from ClusterFixture, so after create a new ClientFixture, it still has problem to connect with ClusterFixture. And I also tried to modify updateClient method signature to return a ClientFixture, however due to the ClusterFixture and ClientFixture started by startCluster() are hold by two static variables, and updateClient should be a non-static method to refactor into ClientFixture, seems a non-static method return a value to static variable made some errors here, and I am still figuring it out. I think the feasible way should be started ClientFixture and ClusterFixture separately in each test case, so that those two variables are both non-static. The drawback here is the code change should be a little too much, but I can submit a commit to see whether it works\r\n"", 'commenter': 'kingswanwho'}, {'comment': 'Hi James, I pushed a new commit which refactor updateClient from ClusterTest to ClientFixture.', 'commenter': 'kingswanwho'}]"
2511,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -122,60 +112,35 @@ public String toString() {
     } catch (IOException e) {
       logger.trace(""Unable to obtain Avro reader position: {}"", e.getMessage(), e);
     }
-    return ""AvroBatchReader[File="" + filePath
-      + "", Position="" + currentPosition
-      + ""]"";
+    return String.format(""AvroBatchReader [ File = %s, Position = %d ]"", filePath, currentPosition);","[{'comment': 'Could we use the `PlanStringBuilder` here?', 'commenter': 'cgivre'}, {'comment': 'Done, thanks.', 'commenter': 'luocooong'}]"
2511,exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroBatchReader.java,"@@ -38,59 +45,52 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-
-public class AvroBatchReader implements ManagedReader<FileScanFramework.FileSchemaNegotiator> {
+public class AvroBatchReader implements ManagedReader {
   private static final Logger logger = LoggerFactory.getLogger(AvroBatchReader.class);
 
-  private Path filePath;
-  private long endPosition;
+  private final Path filePath;
+  private final long endPosition;
   private DataFileReader<GenericRecord> reader;","[{'comment': 'Can `reader` be final as well?', 'commenter': 'paul-rogers'}, {'comment': 'Done, thanks.', 'commenter': 'luocooong'}]"
2526,contrib/storage-http/JSON_Options.md,"@@ -0,0 +1,125 @@
+# JSON Options and Configuration 
+
+Drill has a collection of JSON configuration options to allow you to configure how Drill interprets JSON files.  These are set at the global level, however the HTTP plugin
+allows you to configure these options individually per connection and override the Drill defaults.  The options are:
+
+* `allowNanInf`:  Configures the connection to interpret `NaN` and `Inf` values
+* `allTextMode`:  By default, Drill attempts to infer data types from JSON data. If the data is malformed, Drill may throw schema change exceptions. If your data is
+  inconsistent, you can enable `allTextMode` which when true, Drill will read all JSON values as strings, rather than try to infer the data type.
+* `readNumbersAsDouble`:  By default Drill will attempt to interpret integers, floating point number types and strings.  One challenge is when data is consistent, Drill may
+  throw schema change exceptions. In addition to `allTextMode`, you can make Drill less sensitive by setting the `readNumbersAsDouble` to `true` which causes Drill to read all
+  numeric fields in JSON data as `double` data type rather than trying to distinguish between ints and doubles.
+* `enableEscapeAnyChar`:  Allows a user to escape any character with a \
+* `skipMalformedRecords`:  Allows Drill to skip malformed records and recover without throwing exceptions.
+* `skipMalformedDocument`:  Allows Drill to skip entire malformed documents without throwing errors.
+
+All of these can be set by adding the `jsonOptions` to your connection configuration as shown below:
+
+```json
+
+""jsonOptions"": {
+  ""allTextMode"": true, 
+  ""readNumbersAsDouble"": true
+}
+
+```
+
+## Schema Provisioning
+One of the challenges of querying APIs is inconsistent data.  Drill allows you to provide a schema for individual endpoints.  You can do this in one of three ways: 
+
+1. By providing a schema inline [See: Specifying Schema as Table Function Parameter](https://drill.apache.org/docs/plugin-configuration-basics/#specifying-the-schema-as-table-function-parameter)
+2. By providing a schema in the configuration for the endpoint.
+3. By providing a serialized TupleMetadata of the desired schema.  This is an advanced functionality and should only be used by advanced Drill users.
+
+The schema provisioning currently supports complex types of Arrays and Maps at any nesting level.
+
+### Example Schema Provisioning:
+```json
+""jsonOptions"": {
+  ""providedSchema"": [","[{'comment': 'Can we use fields, schema serialization, and deserialization logic from TupleMetadata? It has a good human-readable string representation that is much more compact than JSON', 'commenter': 'vvysotskyi'}, {'comment': 'You can sort of do that now.  The issue I ran into with the `TupleMetadata` representation is that it gets very complicated when you have nested fields.  The current implementation allows you to either use the simplified implementation or you can pass a serialized `TupleMetadata` object.', 'commenter': 'cgivre'}, {'comment': 'One more thing to mention.  The way I implemented it only supports the data types that are supported by the JSON reader. ', 'commenter': 'cgivre'}, {'comment': 'It is not necessary to have only JSON-supported types because Drill should be able to do the auto conversion from one type to another one. `TupleMetadata` has also JSON representation (see https://drill.apache.org/docs/create-or-replace-schema/#creating-a-schema), so at least this format can be used instead of the custom one.\r\n', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2544,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java,"@@ -212,61 +195,15 @@ public Response enablePlugin(@PathParam(""name"") String name, @PathParam(""val"") B
   @Consumes(MediaType.APPLICATION_JSON)
   @Produces(MediaType.APPLICATION_JSON)
   public Response updateRefreshToken(@PathParam(""name"") String name, OAuthTokenContainer tokens) {","[{'comment': 'Are these endpoints different in any way from their counterparts in CredentialResources? Could we do everything we need to without the ones here?', 'commenter': 'jnturton'}, {'comment': ""I had debated removing the endpoints in `StorageResources`.  The only reason I left them was in case someone was using these endpoints, then we'd have a breaking change when they upgraded.   What if I mark them as deprecated?\r\n\r\nAs a point of clarification, the reason I moved them to `CredentialResources` was because the credentials page allows a user to update their creds.  The `StorageResources` requires admin access, whereas the credentials page only requires an authorized user."", 'commenter': 'cgivre'}, {'comment': 'Thanks, perhaps deprecation is better even though the next version is 2.0', 'commenter': 'jnturton'}, {'comment': 'Done!  ', 'commenter': 'cgivre'}]"
2544,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/OAuthRequests.java,"@@ -0,0 +1,230 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.server.rest;
+
+import okhttp3.OkHttpClient;
+import okhttp3.OkHttpClient.Builder;
+import okhttp3.Request;
+import org.apache.drill.common.logical.CredentialedStoragePluginConfig;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.common.logical.StoragePluginConfig.AuthMode;
+import org.apache.drill.common.logical.security.CredentialsProvider;
+import org.apache.drill.exec.oauth.OAuthTokenProvider;
+import org.apache.drill.exec.oauth.PersistentTokenTable;
+import org.apache.drill.exec.oauth.TokenRegistry;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.server.rest.DrillRestServer.UserAuthEnabled;
+import org.apache.drill.exec.server.rest.StorageResources.JsonResult;
+import org.apache.drill.exec.store.AbstractStoragePlugin;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.StoragePluginRegistry.PluginException;
+import org.apache.drill.exec.store.http.oauth.OAuthUtils;
+import org.apache.drill.exec.store.security.oauth.OAuthTokenCredentials;
+import org.eclipse.jetty.util.resource.Resource;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.servlet.http.HttpServletRequest;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.Response.Status;
+import javax.ws.rs.core.SecurityContext;
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class OAuthRequests {
+
+  private static final Logger logger = LoggerFactory.getLogger(OAuthRequests.class);
+  private static final String OAUTH_SUCCESS_PAGE = ""/rest/storage/success.html"";
+
+  public static Response updateAccessToken(String name,
+                                           OAuthTokenContainer tokens,
+                                           StoragePluginRegistry storage,
+                                           UserAuthEnabled authEnabled,
+                                           SecurityContext sc) {
+    try {
+      if (storage.getPlugin(name).getConfig() instanceof CredentialedStoragePluginConfig) {
+        DrillbitContext context = ((AbstractStoragePlugin) storage.getPlugin(name)).getContext();
+        OAuthTokenProvider tokenProvider = context.getoAuthTokenProvider();
+        PersistentTokenTable tokenTable = tokenProvider.getOauthTokenRegistry(getActiveUser(storage.getPlugin(name).getConfig(), authEnabled, sc)).getTokenTable(name);
+
+        // Set the access token
+        tokenTable.setAccessToken(tokens.getAccessToken());
+
+        return Response.status(Status.OK)
+          .entity(""Access tokens have been updated."")
+          .build();
+      } else {
+        logger.error(""{} does not support OAuth2.0.  You can only add tokens to OAuth enabled plugins."", name);
+        return Response.status(Status.INTERNAL_SERVER_ERROR)
+          .entity(message(""Unable to add tokens: %s"", name))
+          .build();
+      }
+    } catch (PluginException e) {
+      logger.error(""Error when adding tokens to {}"", name);
+      return Response.status(Status.INTERNAL_SERVER_ERROR)
+        .entity(message(""Unable to add tokens: %s"", e.getMessage()))
+        .build();
+    }
+  }
+
+  public static Response updateRefreshToken(String name, OAuthTokenContainer tokens,
+                                            StoragePluginRegistry storage, UserAuthEnabled authEnabled,
+                                            SecurityContext sc) {
+    try {
+      if (storage.getPlugin(name).getConfig() instanceof CredentialedStoragePluginConfig) {
+        DrillbitContext context = ((AbstractStoragePlugin) storage.getPlugin(name)).getContext();
+        OAuthTokenProvider tokenProvider = context.getoAuthTokenProvider();
+        PersistentTokenTable tokenTable = tokenProvider.getOauthTokenRegistry(
+          getActiveUser(storage.getPlugin(name).getConfig(), authEnabled, sc)).getTokenTable(name);
+
+        // Set the access token
+        tokenTable.setRefreshToken(tokens.getRefreshToken());
+
+        return Response.status(Status.OK)
+          .entity(""Refresh token have been updated."")
+          .build();
+      } else {
+        logger.error(""{} is not a HTTP plugin. You can only add access tokens to HTTP plugins."", name);
+        return Response.status(Status.INTERNAL_SERVER_ERROR)
+          .entity(message(""Unable to add tokens: %s"", name))
+          .build();
+      }
+    } catch (PluginException e) {
+      logger.error(""Error when adding tokens to {}"", name);
+      return Response.status(Status.INTERNAL_SERVER_ERROR)
+        .entity(message(""Unable to add tokens: %s"", e.getMessage()))
+        .build();
+    }
+  }
+
+  public static Response updateOAuthTokens(String name, OAuthTokenContainer tokenContainer, StoragePluginRegistry storage,
+                                           UserAuthEnabled authEnabled, SecurityContext sc) {
+    try {
+      if (storage.getPlugin(name).getConfig() instanceof CredentialedStoragePluginConfig) {
+        DrillbitContext context = ((AbstractStoragePlugin) storage.getPlugin(name)).getContext();
+        OAuthTokenProvider tokenProvider = context.getoAuthTokenProvider();
+        PersistentTokenTable tokenTable = tokenProvider
+          .getOauthTokenRegistry(getActiveUser(storage.getPlugin(name).getConfig(), authEnabled, sc))
+          .getTokenTable(name);
+
+        // Set the access and refresh token
+        tokenTable.setAccessToken(tokenContainer.getAccessToken());
+        tokenTable.setRefreshToken(tokenContainer.getRefreshToken());
+
+        return Response.status(Status.OK)
+          .entity(""Access tokens have been updated."")
+          .build();
+      } else {
+        logger.error(""{} is not a HTTP plugin. You can only add access tokens to HTTP plugins."", name);
+        return Response.status(Status.INTERNAL_SERVER_ERROR)
+          .entity(message(""Unable to add tokens: %s"", name))
+          .build();
+      }
+    } catch (PluginException e) {
+      logger.error(""Error when adding tokens to {}"", name);
+      return Response.status(Status.INTERNAL_SERVER_ERROR)
+        .entity(message(""Unable to add tokens: %s"", e.getMessage()))
+        .build();
+    }
+  }
+
+  public static Response updateAuthToken(String name, String code, HttpServletRequest request,
+                                         StoragePluginRegistry storage, UserAuthEnabled authEnabled,
+                                         SecurityContext sc) {
+    try {
+      if (storage.getPlugin(name).getConfig() instanceof CredentialedStoragePluginConfig) {
+        CredentialedStoragePluginConfig securedStoragePluginConfig = (CredentialedStoragePluginConfig) storage.getPlugin(name).getConfig();
+        CredentialsProvider credentialsProvider = securedStoragePluginConfig.getCredentialsProvider();
+        String callbackURL = request.getRequestURL().toString();
+
+        // Now exchange the authorization token for an access token
+        Builder builder = new OkHttpClient.Builder();
+        OkHttpClient client = builder.build();
+
+        Request accessTokenRequest = OAuthUtils.getAccessTokenRequest(credentialsProvider, code, callbackURL);
+        Map<String, String> updatedTokens = OAuthUtils.getOAuthTokens(client, accessTokenRequest);
+
+        // Add to token registry
+        // If USER_TRANSLATION is enabled, Drill will create a token table for each user.
+        TokenRegistry tokenRegistry = ((AbstractStoragePlugin) storage.getPlugin(name))
+          .getContext()
+          .getoAuthTokenProvider()
+          .getOauthTokenRegistry(getActiveUser(storage.getPlugin(name).getConfig(), authEnabled, sc));
+
+        // Add a token registry table if none exists
+        tokenRegistry.createTokenTable(name);
+        PersistentTokenTable tokenTable = tokenRegistry.getTokenTable(name);
+
+        // Add tokens to persistent storage
+        tokenTable.setAccessToken(updatedTokens.get(OAuthTokenCredentials.ACCESS_TOKEN));
+        tokenTable.setRefreshToken(updatedTokens.get(OAuthTokenCredentials.REFRESH_TOKEN));
+
+        // Get success page
+        String successPage = null;
+        try (InputStream inputStream = Resource.newClassPathResource(OAUTH_SUCCESS_PAGE).getInputStream()) {
+          InputStreamReader reader = new InputStreamReader(inputStream, StandardCharsets.UTF_8);
+          BufferedReader bufferedReader = new BufferedReader(reader);
+          successPage = bufferedReader.lines()
+            .collect(Collectors.joining(""\n""));
+          bufferedReader.close();
+          reader.close();
+        } catch (IOException e) {
+          return Response.status(Status.OK).entity(""You may close this window."").build();
+        }
+
+        return Response.status(Status.OK).entity(successPage).build();
+      } else {
+        logger.error(""{} is not a HTTP plugin. You can only add auth code to HTTP plugins."", name);
+        return Response.status(Status.INTERNAL_SERVER_ERROR)
+          .entity(message(""Unable to add authorization code: %s"", name))
+          .build();
+      }
+    } catch (PluginException e) {
+      logger.error(""Error when adding auth token to {}"", name);
+      return Response.status(Status.INTERNAL_SERVER_ERROR)
+        .entity(message(""Unable to add authorization code: %s"", e.getMessage()))
+        .build();
+    }
+  }
+
+  private static JsonResult message(String message, Object... args) {
+    return new JsonResult(String.format(message, args));  // lgtm [java/tainted-format-string]
+  }
+
+  /**
+   * This function checks to see if a given storage plugin is using USER_TRANSLATION mode and if user
+   * authentication is enabled.  If so, it will return the active user name.  If not it will return null.
+   * @param config {@link StoragePluginConfig} The current plugin configuration
+   * @return If USER_TRANSLATION is enabled, returns the active user.  If not, returns null.
+   */
+  private static String getActiveUser(StoragePluginConfig config,","[{'comment': 'Nit, can we standardise teminolgy here to make our own lives easier and refer to the ""query user"" as already happens in some other places? I suppose there is no query running in this context, which makes things less obvious, but it is still eventually the query user that we\'re managing managing tokens for here.', 'commenter': 'jnturton'}, {'comment': 'Renamed function to `getQueryUser()`.', 'commenter': 'cgivre'}]"
2544,exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/PluginConfigWrapper.java,"@@ -114,4 +125,75 @@ public boolean isOauth() {
 
     return tokenCredentials.map(OAuthTokenCredentials::getClientID).orElse(null) != null;
   }
+
+  @JsonIgnore
+  public String getClientID() {
+    CredentialedStoragePluginConfig securedStoragePluginConfig = (CredentialedStoragePluginConfig) config;
+    CredentialsProvider credentialsProvider = securedStoragePluginConfig.getCredentialsProvider();
+
+    return credentialsProvider.getCredentials().getOrDefault(""clientID"", """");
+  }
+
+  /**
+   * This function generates the authorization URI for use when a non-admin user is authorizing
+   * OAuth2.0 access for a storage plugin.  This function is necessary as we do not wish to expose
+   * any plugin configuration information to the user.
+   *
+   * If the plugin is not OAuth, or is missing components, the function will return an empty string.
+   * @return The authorization URI for an OAuth enabled plugin.
+   */
+  @JsonIgnore
+  public String getAuthorizationURIWithParams() {","[{'comment': ""Just a note that I'll try to move this code, probably into OAuthTokenCredentials or the storage config, in DRILL-8200."", 'commenter': 'jnturton'}, {'comment': 'Sounds good.  Just be aware that this code is used in the `list.ftl` for the Credentials page.', 'commenter': 'cgivre'}]"
2553,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperFunctions.java,"@@ -65,6 +65,9 @@ public void setup() {
         .allTextMode(options.getOption(org.apache.drill.exec.ExecConstants.JSON_ALL_TEXT_MODE).bool_val)
         .enableNanInf(options.getOption(org.apache.drill.exec.ExecConstants.JSON_READER_NAN_INF_NUMBERS).bool_val)
         .build();
+
+      jsonReader.setIgnoreJSONParseErrors(
+        options.getOption(org.apache.drill.exec.ExecConstants.JSON_READER_SKIP_INVALID_RECORDS_FLAG).bool_val);","[{'comment': '```suggestion\r\noptions.getBoolean(org.apache.drill.exec.ExecConstants.JSON_READER_SKIP_INVALID_RECORDS_FLAG);\r\n```', 'commenter': 'vdiravka'}, {'comment': 'Fixed!  Thanks!', 'commenter': 'cgivre'}]"
2553,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperFunctions.java,"@@ -151,6 +154,9 @@ public void setup() {
         .enableNanInf(options.getOption(org.apache.drill.exec.ExecConstants.JSON_READER_NAN_INF_NUMBERS).bool_val)
         .build();
 
+      jsonReader.setIgnoreJSONParseErrors(
+        options.getOption(org.apache.drill.exec.ExecConstants.JSON_READER_SKIP_INVALID_RECORDS_FLAG).bool_val);","[{'comment': 'same here', 'commenter': 'vdiravka'}, {'comment': 'Fixed here as well. ', 'commenter': 'cgivre'}]"
2571,contrib/storage-http/src/test/java/org/apache/drill/exec/store/http/TestHttpUDFFunctions.java,"@@ -101,6 +123,30 @@ public static void setup() throws Exception {
     cluster.defineStoragePlugin(""local"", mockStorageConfigWithWorkspace);
   }
 
+  @Test
+  public void testProvidedSchema() throws Exception {","[{'comment': 'Thanks for adding a test to exercise a provided schema.', 'commenter': 'jnturton'}]"
2571,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpUdfUtils.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http.udfs;
+
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.server.options.OptionManager;
+import org.apache.drill.exec.store.easy.json.loader.JsonLoaderImpl.JsonLoaderBuilder;
+import org.apache.drill.exec.store.easy.json.loader.JsonLoaderOptions;
+import org.apache.drill.exec.store.http.HttpApiConfig;
+import org.apache.drill.exec.store.http.HttpJsonOptions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HttpUdfUtils {
+
+  private static final Logger logger = LoggerFactory.getLogger(HttpUdfUtils.class);
+
+  public static JsonLoaderBuilder setupJsonBuilder(HttpApiConfig endpointConfig, ResultSetLoader loader, OptionManager options) {
+    logger.debug(String.valueOf(options));","[{'comment': 'It will print classname and this object hash for all `OptionManager` implementation. Is it useful? If yes, it makes sence to add some context info here.', 'commenter': 'vdiravka'}, {'comment': ""Good call... I'll remove that. "", 'commenter': 'cgivre'}]"
2571,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperFunctions.java,"@@ -153,10 +149,15 @@ public void setup() {
         drillbitContext,
         pluginName
       );
+
       endpointConfig = org.apache.drill.exec.store.http.util.SimpleHttp.getEndpointConfig(
         endpointName,
         plugin.getConfig()
       );
+
+      // Add JSON configuration from Storage plugin, if present.
+      jsonLoaderBuilder = org.apache.drill.exec.store.http.udfs.HttpUdfUtils.setupJsonBuilder(endpointConfig, loader, options);","[{'comment': 'Looks like the same should be applied for the `http_get` and `httpGet`: (62 row)\r\n', 'commenter': 'vdiravka'}, {'comment': ""It's different here because in teh `http_response`, the configs are being pulled from a Drill storage plugin.  Whereas the `http_get` function just does a basic http GET request."", 'commenter': 'cgivre'}, {'comment': 'There was a bunch of logic that was getting difficult to do with all the full paths, and it was really hard to debug.', 'commenter': 'cgivre'}, {'comment': '> There was a bunch of logic that was getting difficult to do with all the full paths, and it was really hard to debug.\r\n\r\nDo you mean full path of classes (without imports)?', 'commenter': 'vdiravka'}, {'comment': 'Yes.  Plus also if the logic is not in the UDF it is easier to add breakpoints. ', 'commenter': 'cgivre'}, {'comment': 'It can be easly debug after uncommenting this [saveCode](https://github.com/apache/drill/blob/570ce63088ab4e2b479b53a9b41696adbb0c9278/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java#L303) line of code in `ProjectRecordBatch`.\r\nYou can find more info in [GeneratedCode.md](https://github.com/apache/drill/blob/dc8d0107cb6a4a31e1beaa9c98d6e51cf7486a67/docs/dev/GeneratedCode.md#instructions-for-intellij) doc', 'commenter': 'vdiravka'}]"
2573,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/loader/SingleElementIterator.java,"@@ -0,0 +1,28 @@
+package org.apache.drill.exec.store.easy.json.loader;","[{'comment': 'Need a license header. ', 'commenter': 'cgivre'}, {'comment': 'Thanks. Fixed', 'commenter': 'vdiravka'}]"
2573,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java,"@@ -110,7 +110,7 @@ protected IterOutcome doWork() {
     memoryManager.update();
 
     if (first && incomingRecordCount == 0) {
-      if (complexWriters != null || rsLoader != null ) {
+      if ((complexWriters != null && !complexWriters.isEmpty()) || rsLoader != null ) {","[{'comment': '`(complexWriters != null && !complexWriters.isEmpty())` -> `!CollectionUtils.isEmpty(complexWriters)`', 'commenter': 'vvysotskyi'}]"
2573,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/loader/SingleElementIterator.java,"@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.easy.json.loader;
+
+import java.util.Iterator;
+
+/**
+ * It allows setting current value in the iterator and cane be used once after {@link #next} call","[{'comment': '```suggestion\r\n * It allows setting the current value in the iterator and can be used once after {@link #next} call\r\n```', 'commenter': 'vvysotskyi'}]"
2580,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java,"@@ -359,7 +382,7 @@ public KafkaStoragePlugin getStoragePlugin() {
 
   @Override
   public String toString() {
-    return String.format(""KafkaGroupScan [KafkaScanSpec=%s, columns=%s]"", kafkaScanSpec, columns);
+    return String.format(""KafkaGroupScan [KafkaScanSpec=%s, columns=%s, records=%d]"", kafkaScanSpec, columns, records);","[{'comment': 'Could we use the `PlanStringBuilder` here?', 'commenter': 'cgivre'}, {'comment': 'Good idea, thanks!', 'commenter': 'luocooong'}]"
2580,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java,"@@ -262,6 +269,17 @@ public void applyAssignments(List<DrillbitEndpoint> incomingEndpoints) {
     assignments = AssignmentCreator.getMappings(incomingEndpoints, Lists.newArrayList(partitionWorkMap.values()));
   }
 
+  @Override
+  public GroupScan applyLimit(int maxRecords) {
+    records = maxRecords; // Just apply the limit value into sub-scan
+    return super.applyLimit(maxRecords);
+  }
+","[{'comment': ""I'm fairly certain this isn't actually working.  The super method always returns `null`. \r\n\r\nhttps://github.com/apache/drill/blob/53e6f2697b31cb76a38e75fe95283512715d28c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractGroupScan.java#L174-L178\r\n\r\nWhat I think you need to do here is:\r\n1.  Create a clone constructor in the `KafkaGroupScan`\r\n2. Call that and return a new `KafkaGroupScan` with the limit applied.\r\n"", 'commenter': 'cgivre'}, {'comment': ""Yes, I'm just call this parent interface that returns null (As long as the default behavior is not broken), because I just need to get the limit value from the execution plan."", 'commenter': 'luocooong'}, {'comment': ""I wouldn't recommend that.  My understanding of how all this works is that Calcite/Drill create multiple copies of the GroupScan to try to find the optimal query plan.   The GroupScans have to be immutable otherwise strange things will happen when you execute more complex queries, and in all likelihood, the limit won't be pushed down. "", 'commenter': 'cgivre'}, {'comment': 'Follow your suggestion, create the immutable groupscan always.', 'commenter': 'luocooong'}]"
2580,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java,"@@ -77,6 +77,7 @@ public class KafkaGroupScan extends AbstractGroupScan {
   private final KafkaScanSpec kafkaScanSpec;
 
   private List<SchemaPath> columns;
+  private int records;","[{'comment': 'These variables should all be `final`.', 'commenter': 'cgivre'}, {'comment': 'Done.', 'commenter': 'luocooong'}]"
2580,contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java,"@@ -271,8 +292,11 @@ public void applyAssignments(List<DrillbitEndpoint> incomingEndpoints) {
 
   @Override
   public GroupScan applyLimit(int maxRecords) {
-    records = maxRecords; // Just apply the limit value into sub-scan
-    return super.applyLimit(maxRecords);
+    if (maxRecords > records) { // pass the limit value into sub-scan
+      return new KafkaGroupScan(this, maxRecords);
+    } else { // stop the transform
+      return super.applyLimit(maxRecords);","[{'comment': 'Do we still need to call the `super` method here?  Could we just return `null`?', 'commenter': 'cgivre'}, {'comment': 'Can be returned null directly.', 'commenter': 'luocooong'}]"
2585,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/utils/GoogleSheetsUtils.java,"@@ -0,0 +1,534 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets.utils;
+
+import com.google.api.client.auth.oauth2.AuthorizationCodeFlow;
+import com.google.api.client.auth.oauth2.Credential;
+import com.google.api.client.auth.oauth2.DataStoreCredentialRefreshListener;
+import com.google.api.client.auth.oauth2.StoredCredential;
+import com.google.api.client.googleapis.auth.oauth2.GoogleAuthorizationCodeFlow;
+import com.google.api.client.googleapis.auth.oauth2.GoogleClientSecrets;
+import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport;
+import com.google.api.client.json.JsonFactory;
+import com.google.api.client.json.gson.GsonFactory;
+import com.google.api.client.util.store.DataStore;
+import com.google.api.services.sheets.v4.Sheets;
+import com.google.api.services.sheets.v4.Sheets.Spreadsheets.Values.BatchGet;
+import com.google.api.services.sheets.v4.SheetsScopes;
+import com.google.api.services.sheets.v4.model.AddSheetRequest;
+import com.google.api.services.sheets.v4.model.BatchUpdateSpreadsheetRequest;
+import com.google.api.services.sheets.v4.model.Request;
+import com.google.api.services.sheets.v4.model.Sheet;
+import com.google.api.services.sheets.v4.model.SheetProperties;
+import com.google.api.services.sheets.v4.model.Spreadsheet;
+import com.google.api.services.sheets.v4.model.SpreadsheetProperties;
+import com.google.api.services.sheets.v4.model.UpdateValuesResponse;
+import com.google.api.services.sheets.v4.model.ValueRange;
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.oauth.OAuthTokenProvider;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.googlesheets.DrillDataStore;
+import org.apache.drill.exec.store.googlesheets.GoogleSheetsColumn;
+import org.apache.drill.exec.store.googlesheets.GoogleSheetsStoragePluginConfig;
+import org.apache.drill.exec.store.googlesheets.columns.GoogleSheetsColumnRange;
+import org.apache.drill.exec.util.Utilities;
+import org.apache.parquet.Strings;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.security.GeneralSecurityException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import static com.google.api.client.util.Strings.isNullOrEmpty;
+
+
+public class GoogleSheetsUtils {
+
+  private static final Logger logger = LoggerFactory.getLogger(GoogleSheetsUtils.class);
+  private static final int SAMPLE_SIZE = 5;
+  private static final JsonFactory JSON_FACTORY = GsonFactory.getDefaultInstance();
+  private static final String UNKNOWN_HEADER = ""field_"";
+
+  /**
+   * Represents the possible data types found in a GoogleSheets document
+   */
+  public enum DATA_TYPES {
+    /**
+     * Represents a field before the datatype is known
+     */
+    UNKNOWN,
+    /**
+     * A numeric data type, either a float or an int.  These are all
+     * converted to Doubles when projected.
+     */
+    NUMERIC,
+    /**
+     * A string data type
+     */
+    VARCHAR,
+    /**
+     * A field containing a date
+     */
+    DATE,
+    /**
+     * A field containing a time
+     */
+    TIME,
+    /**
+     * A field containing timestamps.
+     */
+    TIMESTAMP
+  }
+
+  /**
+   * Returns a Google Credential Object which shall be used to authenticate calls to the Google Sheets service
+   * @param config The Drill GoogleSheets config
+   * @return An authorized Credential
+   * @throws IOException in the event of network or other connectivity issues, throws an IOException
+   * @throws GeneralSecurityException In the event the credentials are incorrect, throws a Security Exception
+   */
+  public static Credential authorize(GoogleSheetsStoragePluginConfig config,
+                                     DataStore<StoredCredential> dataStore,
+                                     OAuthTokenProvider tokenProvider,
+                                     String pluginName,
+                                     String queryUser) throws IOException, GeneralSecurityException {
+    GoogleClientSecrets clientSecrets = config.getSecrets();
+    GoogleAuthorizationCodeFlow flow;
+    List<String> scopes = Collections.singletonList(SheetsScopes.SPREADSHEETS);
+
+    if (dataStore == null) {
+      logger.debug(""Datastore is null"");
+    } else if (dataStore.getDataStoreFactory() == null) {
+      logger.debug(""Datastore factory is null"");
+    }
+
+    flow = new GoogleAuthorizationCodeFlow.Builder
+      (GoogleNetHttpTransport.newTrustedTransport(), JSON_FACTORY, clientSecrets, scopes)
+        .setDataStoreFactory(dataStore.getDataStoreFactory())
+        .setAccessType(""offline"")
+        .build();
+
+    return loadCredential(queryUser, flow, dataStore);
+  }
+
+  public static Credential loadCredential(String userId, GoogleAuthorizationCodeFlow flow, DataStore<StoredCredential> credentialDataStore) throws IOException {
+
+    // No requests need to be performed when userId is not specified.
+    if (isNullOrEmpty(userId)) {
+      return null;
+    }
+
+    if (credentialDataStore == null) {
+      return null;
+    }
+    Credential credential = newCredential(userId, flow, credentialDataStore);
+    StoredCredential stored = ((DrillDataStore<StoredCredential>)credentialDataStore).getStoredCredential(userId);
+    if (stored == null) {
+      return null;
+    }
+    credential.setAccessToken(stored.getAccessToken());
+    credential.setRefreshToken(stored.getRefreshToken());
+    credential.setExpirationTimeMilliseconds(stored.getExpirationTimeMilliseconds());
+
+    return credential;
+  }
+
+  /**
+   * Returns a new credential instance based on the given user ID.
+   *
+   * @param userId user ID or {@code null} if not using a persisted credential store
+   */
+  private static Credential newCredential(String userId, AuthorizationCodeFlow flow, DataStore<StoredCredential> credentialDataStore) {
+    Credential.Builder builder =
+      new Credential.Builder(flow.getMethod())
+        .setTransport(flow.getTransport())
+        .setJsonFactory(flow.getJsonFactory())
+        .setTokenServerEncodedUrl(flow.getTokenServerEncodedUrl())
+        .setClientAuthentication(flow.getClientAuthentication())
+        .setRequestInitializer(flow.getRequestInitializer())
+        .setClock(flow.getClock());
+
+    if (credentialDataStore != null) {
+      builder.addRefreshListener(
+        new DataStoreCredentialRefreshListener(userId, credentialDataStore));
+    }
+    builder.getRefreshListeners().addAll(flow.getRefreshListeners());
+    return builder.build();
+  }
+
+
+
+  public static Sheets getSheetsService(GoogleSheetsStoragePluginConfig config,
+                                        DataStore<StoredCredential> dataStore,
+                                        OAuthTokenProvider tokenProvider,
+                                        String pluginName,
+                                        String queryUser)
+    throws IOException, GeneralSecurityException {
+    Credential credential = GoogleSheetsUtils.authorize(config, dataStore, tokenProvider, pluginName, queryUser);
+    return new Sheets.Builder(
+      GoogleNetHttpTransport.newTrustedTransport(), GsonFactory.getDefaultInstance(), credential)
+      .setApplicationName(""Drill"")
+      .build();
+  }
+
+  /**
+   * Returns the title of the GoogleSheet corresponding with the sheetID.  This is the human readable
+   * name of the actual GoogleSheet document.
+   * @param service An authenticated GoogleSheet service
+   * @param sheetID The sheetID.  This can be obtained from the sheet URL
+   * @return The title of the sheet document.
+   * @throws IOException
+   */
+  public static String getSheetTitle (Sheets service, String sheetID) throws IOException {
+    Spreadsheet spreadsheet = service.spreadsheets().get(sheetID).execute();
+    SpreadsheetProperties properties = spreadsheet.getProperties();
+    return properties.getTitle();
+  }
+
+  /**
+   * Returns a list of the titles of the available spreadsheets within a given Google sheet.
+   * @param service The Google Sheets service
+   * @param sheetID The sheetID for the Google sheet.  This can be obtained from the URL of your Google sheet
+   * @return A list of spreadsheet names within a given Google Sheet
+   * @throws IOException If the Google sheet is unreachable or invalid.
+   */
+  public static List<Sheet> getSheetList(Sheets service, String sheetID) throws IOException {
+    Spreadsheet spreadsheet = service.spreadsheets().get(sheetID).execute();
+    return spreadsheet.getSheets();
+  }
+
+  /**
+   * Converts a column index to A1 notation. Google sheets has a limitation of approx 18k
+   * columns, but that is not enforced here. The column index must be greater than zero or
+   * the function will return null.
+   *
+   * References code found here:
+   * <a href=""https://stackoverflow.com/questions/21229180/convert-column-index-into-corresponding-column-letter"">Stack Overflow Article</a>
+   * @param column The column index for the desired column. Must be greater than zero
+   * @return The A1 representation of the column index.
+   */
+  public static String columnToLetter(int column) {
+    if (column <= 0) {
+      return null;
+    }
+
+    int temp = 0;
+    StringBuilder letter = new StringBuilder();
+    while (column > 0) {
+      temp = (column - 1) % 26;
+      letter.insert(0, (char) (temp + 65));
+      column = (column - temp - 1) / 26;
+    }
+    return letter.toString();
+  }
+
+  /**
+   * Given a column reference in A1 notation, this function will
+   * return the column numeric index. GoogleSheets has a limit of approx
+   * 18k columns, but that is not enforced here.
+   *
+   * References code found here:
+   * <a href=""https://stackoverflow.com/questions/21229180/convert-column-index-into-corresponding-column-letter"">Stack Overflow Article</a>
+   * @param letter The desired column in A1 notation
+   * @return The index of the supplied column
+   */
+  public static int letterToColumnIndex(String letter) {
+    // Make sure the letters are all upper case.
+    letter = letter.toUpperCase();
+    int column = 0;
+    int length = letter.length();
+    for (int i = 0; i < length; i++) {
+      column += (Character.codePointAt(letter, i) - 64) * Math.pow(26, length - i - 1);","[{'comment': '## Implicit narrowing conversion in compound assignment\n\nImplicit cast of source type double to narrower destination type int.\n\n[Show more details](https://github.com/apache/drill/security/code-scanning/33)', 'commenter': 'github-advanced-security[bot]'}]"
2585,.gitignore,"@@ -28,3 +28,6 @@ exec/jdbc-all/dependency-reduced-pom.xml
 .*.html
 venv/
 tools/venv/
+
+# Directory to store oauth tokens for testing Googlesheets Storage plugin","[{'comment': ""Let's start keeping plugins' .gitignore rules in their own new .gitignore files. E.g. this rule would go into a new contrib/storage-googlesheets/.gitignore file and the ignored path would become src/test/resources/tokens/*."", 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2585,contrib/native/client/patches/zookeeper-3.4.6-x64.patch,"@@ -1,163 +0,0 @@
-From 64697ddd8a90f29d1693658f04e975e435e3c869 Mon Sep 17 00:00:00 2001","[{'comment': ""An accidental deletion? I have to admit that I'm not sure what the history of this patch is or why we carry it..."", 'commenter': 'jnturton'}, {'comment': ""I don't know.  Patch files are supposed to be excluded by the .gitignore.  What happened was I reapplied Drill's `.gitignore` and it deleted that patch file.   I would assume this file should not be included. "", 'commenter': 'cgivre'}, {'comment': ""It's very old:\r\n```\r\ncommit d944918ff8a1f76d9b4c400512a130bff7fb2c5f\r\nAuthor: Parth Chandra <pchandra@maprtech.com>\r\nDate:   Wed Jun 11 18:10:34 2014 -0700\r\n\r\n    DRILL-1021: Windows build\r\n```\r\n@vdiravka any thoughts about this patch file?"", 'commenter': 'jnturton'}]"
2585,logical/src/main/java/org/apache/drill/common/logical/StoragePluginConfig.java,"@@ -33,6 +34,7 @@
 
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = ""type"")
 @JsonInclude(JsonInclude.Include.NON_DEFAULT)
+@JsonFormat(with = JsonFormat.Feature.ACCEPT_CASE_INSENSITIVE_PROPERTIES)","[{'comment': ""I think I'm probably in favour of case insensitive config properties but only if we can do it consistently. So have we also got case insensitivity for format config properties? Does that carry through to the table function format config override syntax?"", 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/README.md,"@@ -0,0 +1,156 @@
+# Google Sheets Connector for Apache Drill
+This connector enables you to query and write to Google Sheets.  
+
+### Usage Notes:
+This feature should be considered experimental as Google's API for Sheets is quite complex and amazingly 
+poorly documented.
+
+## Setup Step 1:  Obtain Credential Information from Google
+Ok... this is a pain.  GoogleSheets uses OAuth2.0 (may it be quickly deprecated) for authorization. In order to query GoogleSheets, you will first need to obtain three artifacts:
+
+* Your `clientID`:  This is an identifier which uniquely identifies your application to Google
+* Your `client_secret`: You can think of this as your password for your application to access GoogleSheets
+* Your redirect URL:  This is the URL which Google will send the various access tokens which you will need later.  For a local installation of Drill, it will be: ","[{'comment': '```suggestion\r\n* Your redirect URL:  This is the URL to which Google will send the various access tokens and which you will need later.  For a local installation of Drill, it will be: \r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2585,contrib/storage-googlesheets/README.md,"@@ -0,0 +1,156 @@
+# Google Sheets Connector for Apache Drill
+This connector enables you to query and write to Google Sheets.  
+
+### Usage Notes:
+This feature should be considered experimental as Google's API for Sheets is quite complex and amazingly 
+poorly documented.
+
+## Setup Step 1:  Obtain Credential Information from Google
+Ok... this is a pain.  GoogleSheets uses OAuth2.0 (may it be quickly deprecated) for authorization. In order to query GoogleSheets, you will first need to obtain three artifacts:
+
+* Your `clientID`:  This is an identifier which uniquely identifies your application to Google
+* Your `client_secret`: You can think of this as your password for your application to access GoogleSheets
+* Your redirect URL:  This is the URL which Google will send the various access tokens which you will need later.  For a local installation of Drill, it will be: 
+  `http://localhost:8047/credentials/<plugin name>/update_oauth2_authtoken`.
+
+1. To obtain the `clientID` and `client_secret` you will need to obtain the Google keys, open the [Google Sheets API](https://console.cloud.google.com/apis/library/sheets.googleapis.com) and click on the `Enable` button. 
+2. Once you've enabled teh API, you will be taken to the API Manager.  Either select a pre-existing project or create a new one.","[{'comment': ""```suggestion\r\n2. Once you've enabled the API you will be taken to the API Manager.  Either select an existing project or create a new one.\r\n```"", 'commenter': 'jnturton'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2585,contrib/storage-googlesheets/README.md,"@@ -0,0 +1,156 @@
+# Google Sheets Connector for Apache Drill
+This connector enables you to query and write to Google Sheets.  
+
+### Usage Notes:
+This feature should be considered experimental as Google's API for Sheets is quite complex and amazingly 
+poorly documented.
+
+## Setup Step 1:  Obtain Credential Information from Google
+Ok... this is a pain.  GoogleSheets uses OAuth2.0 (may it be quickly deprecated) for authorization. In order to query GoogleSheets, you will first need to obtain three artifacts:
+
+* Your `clientID`:  This is an identifier which uniquely identifies your application to Google
+* Your `client_secret`: You can think of this as your password for your application to access GoogleSheets
+* Your redirect URL:  This is the URL which Google will send the various access tokens which you will need later.  For a local installation of Drill, it will be: 
+  `http://localhost:8047/credentials/<plugin name>/update_oauth2_authtoken`.
+
+1. To obtain the `clientID` and `client_secret` you will need to obtain the Google keys, open the [Google Sheets API](https://console.cloud.google.com/apis/library/sheets.googleapis.com) and click on the `Enable` button. 
+2. Once you've enabled teh API, you will be taken to the API Manager.  Either select a pre-existing project or create a new one.
+3. Next, navigate to the `Credentials` in the left panel.
+4. Click on `+Create Credentials` at the top of the page.  Select `OAuth client ID` and select `Web Application` or `Desktop` as the type.  Follow the instructions and download 
+   the JSON file that Google provides.
+
+Drill does not use the JSON file, but you will be cutting and pasting values from the JSON file into the Drill configuration.
+
+## Setup Step 2:  Configure Drill
+Create a storage plugin following the normal procedure for doing so.  You can use the example below as a template.  Cut and paste the `clientID` and `client_secret` from the 
+JSON file into your Drill configuration as shown below.  Once you've done that, save the configuration.
+
+```json
+{
+  ""type"": ""googlesheets"",
+  ""allTextMode"": true,
+  ""extractHeaders"": true,
+  ""oAuthConfig"": {
+    ""callbackURL"": ""http://localhost:8047/credentials/googlesheets/update_oauth2_authtoken"",
+    ""authorizationURL"": ""https://accounts.google.com/o/oauth2/auth"",
+    ""authorizationParams"": {
+      ""response_type"": ""code"",
+      ""scope"": ""https://www.googleapis.com/auth/spreadsheets""
+    }
+  },
+  ""credentialsProvider"": {
+    ""credentialsProviderType"": ""PlainCredentialsProvider"",
+    ""credentials"": {
+      ""clientID"": ""<YOUR CLIENT ID>"",
+      ""clientSecret"": ""<YOUR CLIENT SECRET>"",
+      ""tokenURI"": ""https://oauth2.googleapis.com/token""
+    },
+    ""userCredentials"": {}
+  },
+  ""enabled"": true,
+  ""authMode"": ""SHARED_USER""
+}
+```
+
+With the exception of the clientID, client_secret and redirects, you should not have to modify any of the other parameters in the configuration. ","[{'comment': '```suggestion\r\nWith the exception of the clientID, clientSecret and redirects, you should not have to modify any of the other parameters in the configuration. \r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2585,contrib/storage-googlesheets/README.md,"@@ -0,0 +1,156 @@
+# Google Sheets Connector for Apache Drill
+This connector enables you to query and write to Google Sheets.  
+
+### Usage Notes:
+This feature should be considered experimental as Google's API for Sheets is quite complex and amazingly 
+poorly documented.
+
+## Setup Step 1:  Obtain Credential Information from Google
+Ok... this is a pain.  GoogleSheets uses OAuth2.0 (may it be quickly deprecated) for authorization. In order to query GoogleSheets, you will first need to obtain three artifacts:
+
+* Your `clientID`:  This is an identifier which uniquely identifies your application to Google
+* Your `client_secret`: You can think of this as your password for your application to access GoogleSheets
+* Your redirect URL:  This is the URL which Google will send the various access tokens which you will need later.  For a local installation of Drill, it will be: 
+  `http://localhost:8047/credentials/<plugin name>/update_oauth2_authtoken`.
+
+1. To obtain the `clientID` and `client_secret` you will need to obtain the Google keys, open the [Google Sheets API](https://console.cloud.google.com/apis/library/sheets.googleapis.com) and click on the `Enable` button. 
+2. Once you've enabled teh API, you will be taken to the API Manager.  Either select a pre-existing project or create a new one.
+3. Next, navigate to the `Credentials` in the left panel.
+4. Click on `+Create Credentials` at the top of the page.  Select `OAuth client ID` and select `Web Application` or `Desktop` as the type.  Follow the instructions and download 
+   the JSON file that Google provides.
+
+Drill does not use the JSON file, but you will be cutting and pasting values from the JSON file into the Drill configuration.
+
+## Setup Step 2:  Configure Drill
+Create a storage plugin following the normal procedure for doing so.  You can use the example below as a template.  Cut and paste the `clientID` and `client_secret` from the 
+JSON file into your Drill configuration as shown below.  Once you've done that, save the configuration.
+
+```json
+{
+  ""type"": ""googlesheets"",
+  ""allTextMode"": true,
+  ""extractHeaders"": true,
+  ""oAuthConfig"": {
+    ""callbackURL"": ""http://localhost:8047/credentials/googlesheets/update_oauth2_authtoken"",
+    ""authorizationURL"": ""https://accounts.google.com/o/oauth2/auth"",
+    ""authorizationParams"": {
+      ""response_type"": ""code"",
+      ""scope"": ""https://www.googleapis.com/auth/spreadsheets""
+    }
+  },
+  ""credentialsProvider"": {
+    ""credentialsProviderType"": ""PlainCredentialsProvider"",
+    ""credentials"": {
+      ""clientID"": ""<YOUR CLIENT ID>"",
+      ""clientSecret"": ""<YOUR CLIENT SECRET>"",
+      ""tokenURI"": ""https://oauth2.googleapis.com/token""
+    },
+    ""userCredentials"": {}
+  },
+  ""enabled"": true,
+  ""authMode"": ""SHARED_USER""
+}
+```
+
+With the exception of the clientID, client_secret and redirects, you should not have to modify any of the other parameters in the configuration. 
+
+### Other Configuration Parameters
+
+There are two configuration parameters which you may want to adjust:
+* `allTextMode`:  This parameter when `true` disables Drill's data type inferencing for your files.  If your data has inconsistent data types, set this to `true`.  Default is 
+  `true`. 
+* `extractHeaders`:  When `true`, Drill will treat the first row of your data as headers.  When `false` Drill will assign column names like `field_n` for each column.
+
+### Authenticating with Google
+Once you have configured Drill to query GoogleSheets, there is one final step before you can access data.  You must authenticate the application (Drill) with GoogleSheets.  After you have saved your GoogleSheets configuration, navigate back to the configuration screen for your plugin and click on `Authorize`. A new window should appear which will prompt you to authenticate with Google services.  Once you have done that, you should be able to query GoogleSheets!  See, that wasn't so hard!","[{'comment': ""```suggestion\r\nOnce you have configured Drill to query Google Sheets, there is one final step before you can access data.  You must authenticate the application (Drill) with Google Sheets.  After you have saved your Google Sheets configuration, navigate back to the configuration screen for your plugin and click on `Authorize`. A new window should appear which will prompt you to authenticate with Google services.  Once you have done that, you should be able to query Google Sheets!  See, that wasn't so hard!\r\n```"", 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/README.md,"@@ -0,0 +1,156 @@
+# Google Sheets Connector for Apache Drill
+This connector enables you to query and write to Google Sheets.  
+
+### Usage Notes:
+This feature should be considered experimental as Google's API for Sheets is quite complex and amazingly 
+poorly documented.
+
+## Setup Step 1:  Obtain Credential Information from Google
+Ok... this is a pain.  GoogleSheets uses OAuth2.0 (may it be quickly deprecated) for authorization. In order to query GoogleSheets, you will first need to obtain three artifacts:
+
+* Your `clientID`:  This is an identifier which uniquely identifies your application to Google
+* Your `client_secret`: You can think of this as your password for your application to access GoogleSheets
+* Your redirect URL:  This is the URL which Google will send the various access tokens which you will need later.  For a local installation of Drill, it will be: 
+  `http://localhost:8047/credentials/<plugin name>/update_oauth2_authtoken`.
+
+1. To obtain the `clientID` and `client_secret` you will need to obtain the Google keys, open the [Google Sheets API](https://console.cloud.google.com/apis/library/sheets.googleapis.com) and click on the `Enable` button. 
+2. Once you've enabled teh API, you will be taken to the API Manager.  Either select a pre-existing project or create a new one.
+3. Next, navigate to the `Credentials` in the left panel.
+4. Click on `+Create Credentials` at the top of the page.  Select `OAuth client ID` and select `Web Application` or `Desktop` as the type.  Follow the instructions and download 
+   the JSON file that Google provides.
+
+Drill does not use the JSON file, but you will be cutting and pasting values from the JSON file into the Drill configuration.
+
+## Setup Step 2:  Configure Drill
+Create a storage plugin following the normal procedure for doing so.  You can use the example below as a template.  Cut and paste the `clientID` and `client_secret` from the 
+JSON file into your Drill configuration as shown below.  Once you've done that, save the configuration.
+
+```json
+{
+  ""type"": ""googlesheets"",
+  ""allTextMode"": true,
+  ""extractHeaders"": true,
+  ""oAuthConfig"": {
+    ""callbackURL"": ""http://localhost:8047/credentials/googlesheets/update_oauth2_authtoken"",
+    ""authorizationURL"": ""https://accounts.google.com/o/oauth2/auth"",
+    ""authorizationParams"": {
+      ""response_type"": ""code"",
+      ""scope"": ""https://www.googleapis.com/auth/spreadsheets""
+    }
+  },
+  ""credentialsProvider"": {
+    ""credentialsProviderType"": ""PlainCredentialsProvider"",
+    ""credentials"": {
+      ""clientID"": ""<YOUR CLIENT ID>"",
+      ""clientSecret"": ""<YOUR CLIENT SECRET>"",
+      ""tokenURI"": ""https://oauth2.googleapis.com/token""
+    },
+    ""userCredentials"": {}
+  },
+  ""enabled"": true,
+  ""authMode"": ""SHARED_USER""
+}
+```
+
+With the exception of the clientID, client_secret and redirects, you should not have to modify any of the other parameters in the configuration. 
+
+### Other Configuration Parameters
+
+There are two configuration parameters which you may want to adjust:
+* `allTextMode`:  This parameter when `true` disables Drill's data type inferencing for your files.  If your data has inconsistent data types, set this to `true`.  Default is 
+  `true`. 
+* `extractHeaders`:  When `true`, Drill will treat the first row of your data as headers.  When `false` Drill will assign column names like `field_n` for each column.
+
+### Authenticating with Google
+Once you have configured Drill to query GoogleSheets, there is one final step before you can access data.  You must authenticate the application (Drill) with GoogleSheets.  After you have saved your GoogleSheets configuration, navigate back to the configuration screen for your plugin and click on `Authorize`. A new window should appear which will prompt you to authenticate with Google services.  Once you have done that, you should be able to query GoogleSheets!  See, that wasn't so hard!
+
+### Authentication Modes:
+The GoogleSheets plugin supports the `SHARED_USER` and `USER_TRANSLATION` authentication modes. `SHARED_USER` is as the name implies, one user for everyone. `USER_TRANSLATION` ","[{'comment': '```suggestion\r\nThe Google Sheets plugin supports the `SHARED_USER` and `USER_TRANSLATION` authentication modes. `SHARED_USER` is as the name implies, one user for everyone. `USER_TRANSLATION` \r\n```', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/README.md,"@@ -0,0 +1,156 @@
+# Google Sheets Connector for Apache Drill
+This connector enables you to query and write to Google Sheets.  
+
+### Usage Notes:
+This feature should be considered experimental as Google's API for Sheets is quite complex and amazingly 
+poorly documented.
+
+## Setup Step 1:  Obtain Credential Information from Google
+Ok... this is a pain.  GoogleSheets uses OAuth2.0 (may it be quickly deprecated) for authorization. In order to query GoogleSheets, you will first need to obtain three artifacts:
+
+* Your `clientID`:  This is an identifier which uniquely identifies your application to Google
+* Your `client_secret`: You can think of this as your password for your application to access GoogleSheets
+* Your redirect URL:  This is the URL which Google will send the various access tokens which you will need later.  For a local installation of Drill, it will be: 
+  `http://localhost:8047/credentials/<plugin name>/update_oauth2_authtoken`.
+
+1. To obtain the `clientID` and `client_secret` you will need to obtain the Google keys, open the [Google Sheets API](https://console.cloud.google.com/apis/library/sheets.googleapis.com) and click on the `Enable` button. 
+2. Once you've enabled teh API, you will be taken to the API Manager.  Either select a pre-existing project or create a new one.
+3. Next, navigate to the `Credentials` in the left panel.
+4. Click on `+Create Credentials` at the top of the page.  Select `OAuth client ID` and select `Web Application` or `Desktop` as the type.  Follow the instructions and download 
+   the JSON file that Google provides.
+
+Drill does not use the JSON file, but you will be cutting and pasting values from the JSON file into the Drill configuration.
+
+## Setup Step 2:  Configure Drill
+Create a storage plugin following the normal procedure for doing so.  You can use the example below as a template.  Cut and paste the `clientID` and `client_secret` from the 
+JSON file into your Drill configuration as shown below.  Once you've done that, save the configuration.
+
+```json
+{
+  ""type"": ""googlesheets"",
+  ""allTextMode"": true,
+  ""extractHeaders"": true,
+  ""oAuthConfig"": {
+    ""callbackURL"": ""http://localhost:8047/credentials/googlesheets/update_oauth2_authtoken"",
+    ""authorizationURL"": ""https://accounts.google.com/o/oauth2/auth"",
+    ""authorizationParams"": {
+      ""response_type"": ""code"",
+      ""scope"": ""https://www.googleapis.com/auth/spreadsheets""
+    }
+  },
+  ""credentialsProvider"": {
+    ""credentialsProviderType"": ""PlainCredentialsProvider"",
+    ""credentials"": {
+      ""clientID"": ""<YOUR CLIENT ID>"",
+      ""clientSecret"": ""<YOUR CLIENT SECRET>"",
+      ""tokenURI"": ""https://oauth2.googleapis.com/token""
+    },
+    ""userCredentials"": {}
+  },
+  ""enabled"": true,
+  ""authMode"": ""SHARED_USER""
+}
+```
+
+With the exception of the clientID, client_secret and redirects, you should not have to modify any of the other parameters in the configuration. 
+
+### Other Configuration Parameters
+
+There are two configuration parameters which you may want to adjust:
+* `allTextMode`:  This parameter when `true` disables Drill's data type inferencing for your files.  If your data has inconsistent data types, set this to `true`.  Default is 
+  `true`. 
+* `extractHeaders`:  When `true`, Drill will treat the first row of your data as headers.  When `false` Drill will assign column names like `field_n` for each column.
+
+### Authenticating with Google
+Once you have configured Drill to query GoogleSheets, there is one final step before you can access data.  You must authenticate the application (Drill) with GoogleSheets.  After you have saved your GoogleSheets configuration, navigate back to the configuration screen for your plugin and click on `Authorize`. A new window should appear which will prompt you to authenticate with Google services.  Once you have done that, you should be able to query GoogleSheets!  See, that wasn't so hard!
+
+### Authentication Modes:
+The GoogleSheets plugin supports the `SHARED_USER` and `USER_TRANSLATION` authentication modes. `SHARED_USER` is as the name implies, one user for everyone. `USER_TRANSLATION` 
+uses different credentials for each individual user.  In this case, the credentials are the OAuth2.0 access tokens.  
+
+At the time of writing, we have not yet documented `USER_TRANSLATION` fully, however we will update this readme once that is complete.
+
+## Querying Data
+Once you have configured Drill to query ","[{'comment': 'Truncated sentence?', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/README.md,"@@ -0,0 +1,156 @@
+# Google Sheets Connector for Apache Drill
+This connector enables you to query and write to Google Sheets.  
+
+### Usage Notes:
+This feature should be considered experimental as Google's API for Sheets is quite complex and amazingly 
+poorly documented.
+
+## Setup Step 1:  Obtain Credential Information from Google
+Ok... this is a pain.  GoogleSheets uses OAuth2.0 (may it be quickly deprecated) for authorization. In order to query GoogleSheets, you will first need to obtain three artifacts:
+
+* Your `clientID`:  This is an identifier which uniquely identifies your application to Google
+* Your `client_secret`: You can think of this as your password for your application to access GoogleSheets
+* Your redirect URL:  This is the URL which Google will send the various access tokens which you will need later.  For a local installation of Drill, it will be: 
+  `http://localhost:8047/credentials/<plugin name>/update_oauth2_authtoken`.
+
+1. To obtain the `clientID` and `client_secret` you will need to obtain the Google keys, open the [Google Sheets API](https://console.cloud.google.com/apis/library/sheets.googleapis.com) and click on the `Enable` button. 
+2. Once you've enabled teh API, you will be taken to the API Manager.  Either select a pre-existing project or create a new one.
+3. Next, navigate to the `Credentials` in the left panel.
+4. Click on `+Create Credentials` at the top of the page.  Select `OAuth client ID` and select `Web Application` or `Desktop` as the type.  Follow the instructions and download 
+   the JSON file that Google provides.
+
+Drill does not use the JSON file, but you will be cutting and pasting values from the JSON file into the Drill configuration.
+
+## Setup Step 2:  Configure Drill
+Create a storage plugin following the normal procedure for doing so.  You can use the example below as a template.  Cut and paste the `clientID` and `client_secret` from the 
+JSON file into your Drill configuration as shown below.  Once you've done that, save the configuration.
+
+```json
+{
+  ""type"": ""googlesheets"",
+  ""allTextMode"": true,
+  ""extractHeaders"": true,
+  ""oAuthConfig"": {
+    ""callbackURL"": ""http://localhost:8047/credentials/googlesheets/update_oauth2_authtoken"",
+    ""authorizationURL"": ""https://accounts.google.com/o/oauth2/auth"",
+    ""authorizationParams"": {
+      ""response_type"": ""code"",
+      ""scope"": ""https://www.googleapis.com/auth/spreadsheets""
+    }
+  },
+  ""credentialsProvider"": {
+    ""credentialsProviderType"": ""PlainCredentialsProvider"",
+    ""credentials"": {
+      ""clientID"": ""<YOUR CLIENT ID>"",
+      ""clientSecret"": ""<YOUR CLIENT SECRET>"",
+      ""tokenURI"": ""https://oauth2.googleapis.com/token""
+    },
+    ""userCredentials"": {}
+  },
+  ""enabled"": true,
+  ""authMode"": ""SHARED_USER""
+}
+```
+
+With the exception of the clientID, client_secret and redirects, you should not have to modify any of the other parameters in the configuration. 
+
+### Other Configuration Parameters
+
+There are two configuration parameters which you may want to adjust:
+* `allTextMode`:  This parameter when `true` disables Drill's data type inferencing for your files.  If your data has inconsistent data types, set this to `true`.  Default is 
+  `true`. 
+* `extractHeaders`:  When `true`, Drill will treat the first row of your data as headers.  When `false` Drill will assign column names like `field_n` for each column.
+
+### Authenticating with Google
+Once you have configured Drill to query GoogleSheets, there is one final step before you can access data.  You must authenticate the application (Drill) with GoogleSheets.  After you have saved your GoogleSheets configuration, navigate back to the configuration screen for your plugin and click on `Authorize`. A new window should appear which will prompt you to authenticate with Google services.  Once you have done that, you should be able to query GoogleSheets!  See, that wasn't so hard!
+
+### Authentication Modes:
+The GoogleSheets plugin supports the `SHARED_USER` and `USER_TRANSLATION` authentication modes. `SHARED_USER` is as the name implies, one user for everyone. `USER_TRANSLATION` 
+uses different credentials for each individual user.  In this case, the credentials are the OAuth2.0 access tokens.  
+
+At the time of writing, we have not yet documented `USER_TRANSLATION` fully, however we will update this readme once that is complete.
+
+## Querying Data
+Once you have configured Drill to query 
+
+### Obtaining the SpreadsheetID
+The URL below is a public spreadsheet hosted on GoogleSheets:
+[https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/](https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/)
+
+In this URL, the portion `1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms` is the spreadsheetID. Thus, 
+if you wanted to query this sheet in Drill, after configuring Drill, you could do so with the following
+query:
+
+```sql
+SELECT * 
+FROM googlesheets.`1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms`.`Class Data`
+```
+
+The format for the `FROM` clause for GoogleSheets is:
+```sql
+FROM <plugin name>.<sheet ID>.<tab name>
+```
+Note that you must specify the tab name to successfully query GoogleSheets.
+
+### Using Aliases","[{'comment': ""Nice application of @vvysotskyi's aliases :)"", 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/README.md,"@@ -0,0 +1,156 @@
+# Google Sheets Connector for Apache Drill
+This connector enables you to query and write to Google Sheets.  
+
+### Usage Notes:
+This feature should be considered experimental as Google's API for Sheets is quite complex and amazingly 
+poorly documented.
+
+## Setup Step 1:  Obtain Credential Information from Google
+Ok... this is a pain.  GoogleSheets uses OAuth2.0 (may it be quickly deprecated) for authorization. In order to query GoogleSheets, you will first need to obtain three artifacts:
+
+* Your `clientID`:  This is an identifier which uniquely identifies your application to Google
+* Your `client_secret`: You can think of this as your password for your application to access GoogleSheets
+* Your redirect URL:  This is the URL which Google will send the various access tokens which you will need later.  For a local installation of Drill, it will be: 
+  `http://localhost:8047/credentials/<plugin name>/update_oauth2_authtoken`.
+
+1. To obtain the `clientID` and `client_secret` you will need to obtain the Google keys, open the [Google Sheets API](https://console.cloud.google.com/apis/library/sheets.googleapis.com) and click on the `Enable` button. 
+2. Once you've enabled teh API, you will be taken to the API Manager.  Either select a pre-existing project or create a new one.
+3. Next, navigate to the `Credentials` in the left panel.
+4. Click on `+Create Credentials` at the top of the page.  Select `OAuth client ID` and select `Web Application` or `Desktop` as the type.  Follow the instructions and download 
+   the JSON file that Google provides.
+
+Drill does not use the JSON file, but you will be cutting and pasting values from the JSON file into the Drill configuration.
+
+## Setup Step 2:  Configure Drill
+Create a storage plugin following the normal procedure for doing so.  You can use the example below as a template.  Cut and paste the `clientID` and `client_secret` from the 
+JSON file into your Drill configuration as shown below.  Once you've done that, save the configuration.
+
+```json
+{
+  ""type"": ""googlesheets"",
+  ""allTextMode"": true,
+  ""extractHeaders"": true,
+  ""oAuthConfig"": {
+    ""callbackURL"": ""http://localhost:8047/credentials/googlesheets/update_oauth2_authtoken"",
+    ""authorizationURL"": ""https://accounts.google.com/o/oauth2/auth"",
+    ""authorizationParams"": {
+      ""response_type"": ""code"",
+      ""scope"": ""https://www.googleapis.com/auth/spreadsheets""
+    }
+  },
+  ""credentialsProvider"": {
+    ""credentialsProviderType"": ""PlainCredentialsProvider"",
+    ""credentials"": {
+      ""clientID"": ""<YOUR CLIENT ID>"",
+      ""clientSecret"": ""<YOUR CLIENT SECRET>"",
+      ""tokenURI"": ""https://oauth2.googleapis.com/token""
+    },
+    ""userCredentials"": {}
+  },
+  ""enabled"": true,
+  ""authMode"": ""SHARED_USER""
+}
+```
+
+With the exception of the clientID, client_secret and redirects, you should not have to modify any of the other parameters in the configuration. 
+
+### Other Configuration Parameters
+
+There are two configuration parameters which you may want to adjust:
+* `allTextMode`:  This parameter when `true` disables Drill's data type inferencing for your files.  If your data has inconsistent data types, set this to `true`.  Default is 
+  `true`. 
+* `extractHeaders`:  When `true`, Drill will treat the first row of your data as headers.  When `false` Drill will assign column names like `field_n` for each column.
+
+### Authenticating with Google
+Once you have configured Drill to query GoogleSheets, there is one final step before you can access data.  You must authenticate the application (Drill) with GoogleSheets.  After you have saved your GoogleSheets configuration, navigate back to the configuration screen for your plugin and click on `Authorize`. A new window should appear which will prompt you to authenticate with Google services.  Once you have done that, you should be able to query GoogleSheets!  See, that wasn't so hard!
+
+### Authentication Modes:
+The GoogleSheets plugin supports the `SHARED_USER` and `USER_TRANSLATION` authentication modes. `SHARED_USER` is as the name implies, one user for everyone. `USER_TRANSLATION` 
+uses different credentials for each individual user.  In this case, the credentials are the OAuth2.0 access tokens.  
+
+At the time of writing, we have not yet documented `USER_TRANSLATION` fully, however we will update this readme once that is complete.
+
+## Querying Data
+Once you have configured Drill to query 
+
+### Obtaining the SpreadsheetID
+The URL below is a public spreadsheet hosted on GoogleSheets:
+[https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/](https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/)
+
+In this URL, the portion `1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms` is the spreadsheetID. Thus, 
+if you wanted to query this sheet in Drill, after configuring Drill, you could do so with the following
+query:
+
+```sql
+SELECT * 
+FROM googlesheets.`1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms`.`Class Data`
+```
+
+The format for the `FROM` clause for GoogleSheets is:
+```sql
+FROM <plugin name>.<sheet ID>.<tab name>
+```
+Note that you must specify the tab name to successfully query GoogleSheets.
+
+### Using Aliases
+Since the sheet IDs from Google are not human readable, one way to make your life easier is to use Drill's aliasing features to provide a better name for the actual sheet name. 
+
+### Data Types
+Drill's Google Sheets reader will attempt to infer the data types of the incoming data.  As with other connectors, this is an imperfect process since GoogleSheets does not 
+supply a schema or other information to allow Drill to identify the data types of a column.  At present, here is how Drill will map your data:
+* Numbers:  All numeric columns will be mapped to `DOUBLE` data types
+* Boolean:  Columns containing `true/false` will be mapped to the `BOOLEAN` type
+* Time, Date, Timestamp:  Temporal fields will be mapped to the correct type.  You can disable able temporal fields by setting the config option `XXX` to `false`.","[{'comment': 'What is the config option `XXX` here?', 'commenter': 'jnturton'}]"
2585,exec/java-exec/src/main/java/org/apache/drill/exec/store/security/OAuthTokenCredentials.java,"@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.security;
+
+import org.apache.drill.common.logical.security.CredentialsProvider;
+
+import java.util.Map;
+
+public class OAuthTokenCredentials {","[{'comment': 'We do already have org.apache.drill.exec.store.security.oauth.OAuthTokenCredentials, possible duplicate?', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/test/java/org/apache/drill/exec/store/googlesheets/TestGoogleSheetsLimitPushdown.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.drill.common.util.DrillFileUtils;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.oauth.PersistentTokenTable;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.StoragePluginRegistry.PluginException;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.drill.shaded.guava.com.google.common.io.Files;
+import org.apache.drill.test.ClusterFixtureBuilder;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Ignore;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+
+import static org.junit.Assert.fail;
+
+@Ignore(""This test requires a live connection to GoogleSheets.  Please run tests manually."")","[{'comment': '```suggestion\r\n@Ignore(""This test requires a live connection to Google Sheets.  Please run tests manually."")\r\n```', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/GoogleSheetsGroupScan.java,"@@ -0,0 +1,369 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets;
+
+import com.fasterxml.jackson.annotation.JacksonInject;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.PlanStringBuilder;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.metastore.MetadataProviderManager;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.ScanStats;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.exec.planner.logical.DrillScanRel;
+import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.base.filter.ExprNode;
+import org.apache.drill.exec.util.Utilities;
+import org.apache.drill.metastore.metadata.TableMetadata;
+import org.apache.drill.metastore.metadata.TableMetadataProvider;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+
+@JsonTypeName(""googlesheets-group-scan"")
+public class GoogleSheetsGroupScan extends AbstractGroupScan {
+
+  private final GoogleSheetsScanSpec scanSpec;
+  private final GoogleSheetsStoragePluginConfig config;
+  private final List<SchemaPath> columns;
+  private final String pluginName;
+  private final Map<String, ExprNode.ColRelOpConstNode> filters;
+  private final ScanStats scanStats;
+  private final double filterSelectivity;
+  private final int maxRecords;
+  private final GoogleSheetsStoragePlugin plugin;
+  private int hashCode;
+  private MetadataProviderManager metadataProviderManager;
+
+  // Initial Constructor
+  public GoogleSheetsGroupScan(String userName,
+                               GoogleSheetsScanSpec scanSpec,
+                               GoogleSheetsStoragePlugin plugin,
+                               MetadataProviderManager metadataProviderManager) {
+    super(userName);
+    this.scanSpec = scanSpec;
+    this.config = scanSpec.getConfig();
+    this.columns = ALL_COLUMNS;
+    this.pluginName = plugin.getName();
+    this.filters = null;
+    this.filterSelectivity = 0.0;
+    this.maxRecords = -1;
+    this.scanStats = computeScanStats();
+    this.plugin = plugin;
+    this.metadataProviderManager = metadataProviderManager;
+  }
+
+  // Copy Constructor
+  public GoogleSheetsGroupScan(GoogleSheetsGroupScan that) {
+    super(that);
+    this.scanSpec = that.scanSpec;
+    this.config = that.config;
+    this.columns = that.columns;
+    this.filters = that.filters;
+    this.pluginName = that.pluginName;
+    this.filterSelectivity = that.filterSelectivity;
+    this.scanStats = that.scanStats;
+    this.maxRecords = that.maxRecords;
+    this.plugin = that.plugin;
+    this.metadataProviderManager = that.metadataProviderManager;
+    this.hashCode = hashCode();
+  }
+
+  /**
+   * Constructor for applying a limit.
+   * @param that The previous group scan without the limit.
+   * @param maxRecords  The desired limit, pushed down from Calcite
+   */
+  public GoogleSheetsGroupScan(GoogleSheetsGroupScan that, int maxRecords) {
+    super(that);
+    this.scanSpec = that.scanSpec;
+    this.config = that.config;
+    this.columns = that.columns;
+    this.pluginName = that.pluginName;
+    this.filters = that.filters;
+    this.filterSelectivity = that.filterSelectivity;
+    this.maxRecords = maxRecords;
+    this.plugin = that.plugin;
+    this.metadataProviderManager = that.metadataProviderManager;
+    this.scanStats = computeScanStats();
+  }
+
+  /**
+   * Constructor for applying columns (Projection pushdown).
+   * @param that The previous GroupScan, without the columns
+   * @param columns The list of columns to push down
+   */
+  public GoogleSheetsGroupScan(GoogleSheetsGroupScan that, List<SchemaPath> columns) {
+    super(that);
+    this.scanSpec = that.scanSpec;
+    this.config = scanSpec.getConfig();
+    this.columns = columns;
+    this.filters = that.filters;
+    this.pluginName = that.pluginName;
+    this.filterSelectivity = that.filterSelectivity;
+    this.maxRecords = that.maxRecords;
+    this.plugin = that.plugin;
+    this.metadataProviderManager = that.metadataProviderManager;
+    this.scanStats = computeScanStats();
+  }
+
+  /**
+   * Constructor for applying a filter
+   * @param that Previous group scan w/o filters
+   * @param filters The list of filters
+   * @param filterSelectivity  The filter selectivity
+   */
+  public GoogleSheetsGroupScan(GoogleSheetsGroupScan that,
+                               Map<String, ExprNode.ColRelOpConstNode> filters,
+                               double filterSelectivity) {
+    super(that);
+    this.scanSpec = that.scanSpec;
+    this.config = that.config;
+    this.columns = that.columns;
+    this.filters = filters;
+    this.pluginName = that.pluginName;
+    this.filterSelectivity = filterSelectivity;
+    this.maxRecords = that.maxRecords;
+    this.plugin = that.plugin;
+    this.metadataProviderManager = that.metadataProviderManager;
+    this.scanStats = computeScanStats();
+  }
+
+  @JsonCreator
+  public GoogleSheetsGroupScan(
+    @JsonProperty(""userName"") String userName,
+    @JsonProperty(""scanSpec"") GoogleSheetsScanSpec scanSpec,
+    @JsonProperty(""columns"") List<SchemaPath> columns,
+    @JsonProperty(""filters"") Map<String, ExprNode.ColRelOpConstNode> filters,
+    @JsonProperty(""filterSelectivity"") double selectivity,
+    @JsonProperty(""maxRecords"") int maxRecords,
+    @JacksonInject StoragePluginRegistry plugins
+  ) {
+    super(userName);
+    this.scanSpec = scanSpec;
+    this.config = scanSpec.getConfig();
+    this.columns = columns;
+    this.filters = filters;
+    this.filterSelectivity = selectivity;
+    this.maxRecords = maxRecords;
+    this.scanStats = computeScanStats();
+    this.plugin = plugins.resolve(config, GoogleSheetsStoragePlugin.class);
+    this.pluginName = plugin.getName();
+  }
+
+  @JsonProperty(""scanSpec"")
+  public GoogleSheetsScanSpec scanSpec() {
+    return scanSpec;
+  }
+
+  @JsonProperty(""config"")
+  public GoogleSheetsStoragePluginConfig config() {
+    return config;
+  }
+
+  @JsonProperty(""columns"")
+  public List<SchemaPath> columns() {
+    return columns;
+  }
+
+  @JsonProperty(""filters"")
+  public Map<String, ExprNode.ColRelOpConstNode> filters() {
+    return filters;
+  }
+
+  @JsonProperty(""maxRecords"")
+  public int maxRecords() {
+    return maxRecords;
+  }
+
+  @Override
+  public void applyAssignments(List<DrillbitEndpoint> endpoints) {
+
+  }
+
+  public TupleMetadata getSchema() {
+    if (metadataProviderManager == null) {
+      return null;
+    }
+    try {
+      return metadataProviderManager.getSchemaProvider().read().getSchema();
+    } catch (IOException | NullPointerException e) {
+      return null;
+    }
+  }
+
+  @Override
+  public TableMetadata getTableMetadata() {
+    if (getMetadataProvider() == null) {
+      return null;
+    }
+    return getMetadataProvider().getTableMetadata();
+  }
+
+  @Override
+  public TableMetadataProvider getMetadataProvider() {
+    if (metadataProviderManager == null) {
+      return null;
+    }
+    return metadataProviderManager.getTableMetadataProvider();
+  }
+
+  @Override
+  @JsonIgnore
+  public boolean canPushdownProjects(List<SchemaPath> columns) {
+    return true;
+  }
+
+  @JsonIgnore
+  public boolean allowsFilters() {
+    return true;
+  }
+
+  @Override
+  public SubScan getSpecificScan(int minorFragmentId) {
+    return new GoogleSheetsSubScan(userName, config, scanSpec, columns, filters, maxRecords, getSchema());
+  }
+
+  @Override
+  public int getMaxParallelizationWidth() {
+    return 1;
+  }
+
+  @Override
+  public GroupScan clone(List<SchemaPath> columns) {
+    return new GoogleSheetsGroupScan(this, columns);
+  }
+
+  @Override
+  public boolean supportsLimitPushdown() {
+    return true;
+  }
+
+  @Override
+  public GroupScan applyLimit(int maxRecords) {
+    if (maxRecords == this.maxRecords) {
+      return null;
+    }
+    return new GoogleSheetsGroupScan(this, maxRecords);
+  }
+
+  @Override
+  public String getDigest() {
+    return toString();
+  }
+
+  @Override
+  public ScanStats getScanStats() {
+
+    // Since this class is immutable, compute stats once and cache
+    // them. If the scan changes (adding columns, adding filters), we
+    // get a new scan without cached stats.
+    return scanStats;
+  }
+
+  private ScanStats computeScanStats() {
+
+    // If this config allows filters, then make the default
+    // cost very high to force the planner to choose the version
+    // with filters.
+    if (!hasFilters()) {
+      return new ScanStats(ScanStats.GroupScanProperty.ESTIMATED_TOTAL_COST,
+        1E9, 1E112, 1E12);
+    }
+
+    // No good estimates at all, just make up something.
+    double estRowCount = 10_000;","[{'comment': 'It might be possible to get [this information from the Sheets API](https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets/sheets#GridProperties) but how much benefit that would confer I do not know. Another option would be to leave a `// TODO: consider setting estRowCount using the sheet row count metadata from the Sheets API`.\r\n\r\n', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/columns/GoogleSheetsDateColumnWriter.java,"@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets.columns;
+
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+
+import java.time.LocalDate;
+
+public class GoogleSheetsDateColumnWriter extends GoogleSheetsColumnWriter {","[{'comment': 'Consider converting this (and siblings) to a static inner class GoogleSheetsColumnWriter.DateWriter to reduce the proliferation of small source files.', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/test/java/org/apache/drill/exec/store/googlesheets/TestGoogleSheetsWriter.java,"@@ -0,0 +1,135 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.drill.categories.RowSetTest;
+import org.apache.drill.common.util.DrillFileUtils;
+import org.apache.drill.exec.oauth.PersistentTokenTable;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.StoragePluginRegistry.PluginException;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.drill.shaded.guava.com.google.common.io.Files;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.QueryBuilder.QuerySummary;
+import org.junit.BeforeClass;
+import org.junit.Ignore;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.nio.file.Paths;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+@Category(RowSetTest.class)
+@Ignore(""These tests require a live Google Sheets connection.  Please run manually."")
+public class TestGoogleSheetsWriter extends ClusterTest {
+  private static final String AUTH_URI = ""https://accounts.google.com/o/oauth2/auth"";
+  private static final String TOKEN_URI = ""https://oauth2.googleapis.com/token"";
+  private static final List<String> REDIRECT_URI = new ArrayList<>(Arrays.asList(""urn:ietf:wg:oauth:2.0:oob"", ""http://localhost""));
+
+  private static StoragePluginRegistry pluginRegistry;
+  private static String accessToken;
+  private static String refreshToken;
+
+  @BeforeClass
+  public static void init() throws Exception {
+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""""));
+
+    String oauthJson = Files.asCharSource(DrillFileUtils.getResourceAsFile(""/tokens/oauth_tokens.json""), Charsets.UTF_8).read();
+
+    ObjectMapper mapper = new ObjectMapper();
+    Map<String,String> tokenMap = mapper.readValue(oauthJson, Map.class);
+
+    String clientID = tokenMap.get(""client_id"");
+    String clientSecret = tokenMap.get(""client_secret"");
+    accessToken = tokenMap.get(""access_token"");
+    refreshToken = tokenMap.get(""refresh_token"");
+
+    pluginRegistry = cluster.drillbit().getContext().getStorage();
+    GoogleSheetsStoragePluginConfig config = GoogleSheetsStoragePluginConfig.builder()
+      .clientID(clientID)
+      .clientSecret(clientSecret)
+      .redirectUris(REDIRECT_URI)
+      .authUri(AUTH_URI)
+      .tokenUri(TOKEN_URI)
+      .build();
+
+    config.setEnabled(true);
+    pluginRegistry.validatedPut(""googlesheets"", config);
+  }
+
+  @Test
+  public void testBasicCTAS() throws Exception {
+    try {
+      initializeTokens();
+    } catch (PluginException e) {
+      fail(e.getMessage());
+    }
+
+    String query = ""CREATE TABLE googlesheets.`test_sheet`.`test_table` (ID, NAME) AS "" +
+      ""SELECT * FROM (VALUES(1,2), (3,4))"";
+    // Create the table and insert the values
+    QuerySummary insertResults = queryBuilder().sql(query).run();
+    assertTrue(insertResults.succeeded());
+  }
+
+  @Test
+  public void testCTASFromFile() throws Exception {
+    try {
+      initializeTokens();
+    } catch (PluginException e) {
+      fail(e.getMessage());
+    }
+
+    /*String query = ""CREATE TABLE googlesheets.`test_sheet`.`test_table` (ID, NAME) AS "" +
+      ""SELECT * FROM (VALUES(1,2), (3,4))"";*/
+   String sql = ""SELECT * FROM table(cp.`data/Drill_Test_Data.xlsx` (type => 'excel', sheetName => 'MixedSheet'))"";","[{'comment': 'Is this actually testing a CTAS at the moment? It also looks like some commented temporary code can be cleaned up.', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/GoogleSheetsPushDownListener.java,"@@ -0,0 +1,149 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets;
+
+import org.apache.calcite.rex.RexNode;
+import org.apache.calcite.util.Pair;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.map.CaseInsensitiveMap;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.ops.OptimizerRulesContext;
+import org.apache.drill.exec.physical.base.GroupScan;
+import org.apache.drill.exec.store.StoragePluginOptimizerRule;
+import org.apache.drill.exec.store.base.filter.ExprNode;
+import org.apache.drill.exec.store.base.filter.ExprNode.AndNode;
+import org.apache.drill.exec.store.base.filter.ExprNode.ColRelOpConstNode;
+import org.apache.drill.exec.store.base.filter.ExprNode.OrNode;
+import org.apache.drill.exec.store.base.filter.FilterPushDownListener;
+import org.apache.drill.exec.store.base.filter.FilterPushDownStrategy;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * The GoogleSheets storage plugin accepts filters which are:
+ * <ul>
+ * <li>A single column = value expression </li>
+ * <li>An AND'ed set of such expressions,</li>
+ * <li>If the value is one with an unambiguous conversion to
+ * a string. (That is, not dates, binary, maps, etc.)</li>
+ * </ul>
+ */
+public class GoogleSheetsPushDownListener implements FilterPushDownListener {
+
+  public static Set<StoragePluginOptimizerRule> rulesFor(OptimizerRulesContext optimizerRulesContext) {
+    return FilterPushDownStrategy.rulesFor(new GoogleSheetsPushDownListener());
+  }
+
+  @Override
+  public String prefix() {
+    return ""GoogleSheets"";
+  }
+
+  @Override
+  public boolean isTargetScan(GroupScan groupScan) {
+    return groupScan instanceof GoogleSheetsGroupScan;
+  }
+
+  @Override
+  public ScanPushDownListener builderFor(GroupScan groupScan) {
+    GoogleSheetsGroupScan gsScan = (GoogleSheetsGroupScan) groupScan;
+    if (gsScan.hasFilters() || !gsScan.allowsFilters()) {
+      return null;
+    } else {
+      return new GoogleSheetsScanPushDownListener(gsScan);
+    }
+  }
+
+  private static class GoogleSheetsScanPushDownListener implements ScanPushDownListener {
+
+    private final GoogleSheetsGroupScan groupScan;
+    private final Map<String, String> filterParams = CaseInsensitiveMap.newHashMap();
+
+    GoogleSheetsScanPushDownListener(GoogleSheetsGroupScan groupScan) {
+      this.groupScan = groupScan;
+      for (SchemaPath field : groupScan.columns()) {
+        filterParams.put(field.getAsUnescapedPath(), field.getAsUnescapedPath());
+      }
+    }
+
+    @Override
+    public ExprNode accept(ExprNode node) {
+      if (node instanceof OrNode) {
+        return null;
+      } else if (node instanceof ColRelOpConstNode) {
+        return null;
+        //return acceptRelOp((ColRelOpConstNode) node);
+      } else {
+        return null;
+      }","[{'comment': 'This conditional reduces to `return null`, is that what is intended?', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/GoogleSheetsStoragePlugin.java,"@@ -0,0 +1,219 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets;
+
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.google.api.client.auth.oauth2.StoredCredential;
+import com.google.api.client.util.store.DataStore;
+import com.google.api.services.sheets.v4.Sheets;
+import org.apache.calcite.plan.RelOptRule;
+import org.apache.calcite.schema.SchemaPlus;
+import org.apache.drill.common.JSONOptions;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.common.logical.StoragePluginConfig.AuthMode;
+import org.apache.drill.exec.metastore.MetadataProviderManager;
+import org.apache.drill.exec.oauth.OAuthTokenProvider;
+import org.apache.drill.exec.oauth.PersistentTokenTable;
+import org.apache.drill.exec.oauth.TokenRegistry;
+import org.apache.drill.exec.ops.OptimizerRulesContext;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.planner.PlannerPhase;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.server.options.SessionOptionManager;
+import org.apache.drill.exec.store.AbstractStoragePlugin;
+import org.apache.drill.exec.store.SchemaConfig;
+import org.apache.drill.exec.store.base.filter.FilterPushDownUtils;
+import org.apache.drill.exec.store.googlesheets.schema.GoogleSheetsSchemaFactory;
+import org.apache.drill.exec.store.googlesheets.utils.GoogleSheetsUtils;
+import org.apache.drill.shaded.guava.com.google.common.annotations.VisibleForTesting;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableSet;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.security.GeneralSecurityException;
+import java.util.List;
+import java.util.Set;
+
+public class GoogleSheetsStoragePlugin extends AbstractStoragePlugin {
+  private final static Logger logger = LoggerFactory.getLogger(GoogleSheetsStoragePlugin.class);
+  private final static String SHARED_USERNAME = ""anonymous"";
+  private final GoogleSheetsStoragePluginConfig config;
+  private final GoogleSheetsSchemaFactory schemaFactory;
+  private final OAuthTokenProvider tokenProvider;
+  private DataStore<StoredCredential> dataStore;
+  private Sheets service;
+  private TokenRegistry tokenRegistry;
+  private String username;
+
+
+  public GoogleSheetsStoragePlugin(GoogleSheetsStoragePluginConfig configuration, DrillbitContext context, String name) {
+    super(context, name);
+    this.config = configuration;
+    this.tokenProvider = context.getoAuthTokenProvider();
+    this.schemaFactory = new GoogleSheetsSchemaFactory(this);
+  }
+
+  public void initializeOauthTokenTable(SchemaConfig schemaConfig) {
+    // A word about how GoogleSheets (GS) handles authorization and authentication.
+    // GS uses OAuth 2.0 for authorization.
+    // The GS Sheets object is the client which interacts with the actual data, however
+    // it does not provide a straightforward way of passing credentials into this object.
+    // GS has three objects:  the credential, storedCredential, and the credential dataStore.
+    //
+    // The Credential Object
+    // The credential really should be called the applicationCredential or something like that, as
+    // it stores the OAuth credentials for the application such as the clientID, clientSecret
+    //
+    // The Stored Credential Object
+    // This object has no relation to the Credential object, and it stores the user's credentials,
+    // specifically the access and refresh tokens.
+    //
+    // The DataStore Object is a synchronized store of storedCredential objects.
+    // The approach we take here is to use Drill's existing OAuth infrastructure
+    // to store the tokens in PersistentTokenStores, just like the HTTP plugin. When
+    // the plugin is loaded, we read the tokens from the persistent store into a GS dataStore.
+    // This happens when the plugin is registered.
+
+    if (config.getAuthMode() == AuthMode.USER_TRANSLATION) {
+      this.username = schemaConfig.getUserName();
+      tokenRegistry = tokenProvider.getOauthTokenRegistry(this.username);
+    } else {
+      this.username = SHARED_USERNAME;
+      tokenRegistry = tokenProvider.getOauthTokenRegistry(null);
+    }
+    tokenRegistry.createTokenTable(getName());
+    this.dataStore = new DrillDataStoreFactory(tokenProvider, getName()).createDataStore(this.username);
+  }
+
+  public DataStore<StoredCredential> getDataStore(String username) {
+    if (this.dataStore == null) {
+      this.dataStore = new DrillDataStoreFactory(tokenProvider, getName()).createDataStore(username);
+    }
+    return dataStore;
+  }
+
+
+  @Override
+  public void registerSchemas(SchemaConfig schemaConfig, SchemaPlus parent) {
+    initializeOauthTokenTable(schemaConfig);
+    schemaFactory.registerSchemas(schemaConfig, parent);
+  }
+
+  public PersistentTokenTable getTokenTable() {
+    return tokenRegistry.getTokenTable(getName());
+  }
+
+  @Override
+  public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection,
+                                           SessionOptionManager options) throws IOException {
+    return getPhysicalScan(userName, selection, AbstractGroupScan.ALL_COLUMNS,
+      options, null);
+  }
+
+  @Override
+  public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection,
+                                           SessionOptionManager options, MetadataProviderManager metadataProviderManager) throws IOException {
+    return getPhysicalScan(userName, selection, AbstractGroupScan.ALL_COLUMNS,
+      options, metadataProviderManager);
+  }
+
+  @Override
+  public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection,
+                                           List<SchemaPath> columns) throws IOException {
+    return getPhysicalScan(userName, selection, columns, null, null);
+  }
+
+  @Override
+  public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection) throws IOException {
+    return getPhysicalScan(userName, selection, AbstractGroupScan.ALL_COLUMNS, null);
+  }
+
+  @Override
+  public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection, List<SchemaPath> columns, SessionOptionManager options,
+                                           MetadataProviderManager metadataProviderManager) throws IOException {
+    GoogleSheetsScanSpec scanSpec = selection.getListWith(context.getLpPersistence().getMapper(), new TypeReference<GoogleSheetsScanSpec>() {});
+    return new GoogleSheetsGroupScan(this.username, scanSpec, this, metadataProviderManager);
+  }
+
+  @Override
+  public Set<? extends RelOptRule> getOptimizerRules(OptimizerRulesContext optimizerContext, PlannerPhase phase) {
+
+    // Push-down planning is done at the logical phase so it can
+    // influence parallelization in the physical phase. Note that many
+    // existing plugins perform filter push-down at the physical
+    // phase, which also works fine if push-down is independent of
+    // parallelization.
+    if (FilterPushDownUtils.isFilterPushDownPhase(phase) || phase == PlannerPhase.LOGICAL) {","[{'comment': '```suggestion\r\n    if (FilterPushDownUtils.isFilterPushDownPhase(phase) {\r\n```\r\nDoes this method not already match the wanted logical planning phases?', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/utils/GoogleSheetsTypifier.java,"@@ -0,0 +1,346 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets.utils;
+
+import java.nio.CharBuffer;
+
+import java.time.LocalDate;
+import java.time.format.DateTimeParseException;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Locale;
+import java.util.Map.Entry;
+import java.util.AbstractMap.SimpleEntry;
+
+import java.time.LocalDateTime;
+import java.time.format.DateTimeFormatter;
+
+/**
+ * This class attempts to infer the data type of an unknown data type. It is somewhat
+ * configurable.  This was sourced from <a href=""https://gist.github.com/awwsmm/56b8164410c89c719ebfca7b3d85870b"">this code on github</a>.","[{'comment': ""I went to check license compatibility but I see no copyright notice or license at the source Gist. I'm not sure if we need to confirm this?"", 'commenter': 'jnturton'}, {'comment': 'Charles brought this to my attention and I added an MIT license there. Enjoy!', 'commenter': 'awwsmm'}]"
2585,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/utils/GoogleSheetsTypifier.java,"@@ -0,0 +1,346 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets.utils;
+
+import java.nio.CharBuffer;
+
+import java.time.LocalDate;
+import java.time.format.DateTimeParseException;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Locale;
+import java.util.Map.Entry;
+import java.util.AbstractMap.SimpleEntry;
+
+import java.time.LocalDateTime;
+import java.time.format.DateTimeFormatter;
+
+/**
+ * This class attempts to infer the data type of an unknown data type. It is somewhat
+ * configurable.  This was sourced from <a href=""https://gist.github.com/awwsmm/56b8164410c89c719ebfca7b3d85870b"">this code on github</a>.
+ */
+public class GoogleSheetsTypifier {","[{'comment': 'Should we consider a system wide type guessing util class? I.e. move this to somewhere like common/ and make all our character-based data format readers decide a Drill type using the same logic?', 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/test/java/org/apache/drill/exec/store/googlesheets/TestGoogleSheetsQueries.java,"@@ -0,0 +1,317 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.util.DrillFileUtils;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.oauth.PersistentTokenTable;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.rpc.user.QueryDataBatch;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.StoragePluginRegistry.PluginException;
+import org.apache.drill.shaded.guava.com.google.common.base.Charsets;
+import org.apache.drill.shaded.guava.com.google.common.io.Files;
+import org.apache.drill.test.ClusterFixtureBuilder;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.junit.BeforeClass;
+import org.junit.Ignore;
+import org.junit.Test;
+
+import java.time.LocalDate;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+/**
+ * This class tests the Google Sheets plugin. Since GoogleSheets is essentially an API, these tests
+ * must be run with a live internet connection.  These tests use test data which can be found in the
+ * resources directory.
+ */
+@Ignore(""Requires live connection to GoogleSheets.  Please run tests manually."")
+public class TestGoogleSheetsQueries extends ClusterTest {
+
+  private static final String AUTH_URI = ""https://accounts.google.com/o/oauth2/auth"";
+  private static final String TOKEN_URI = ""https://oauth2.googleapis.com/token"";
+  private static final List<String> REDIRECT_URI = new ArrayList<>(Arrays.asList(""urn:ietf:wg:oauth:2.0:oob"", ""http://localhost""));
+
+  private static StoragePluginRegistry pluginRegistry;
+  private static String accessToken;
+  private static String refreshToken;
+  private static String sheetID;
+  private static String clientID;
+  private static String clientSecret;
+
+  @BeforeClass
+  public static void init() throws Exception {
+
+    String oauthJson = Files.asCharSource(DrillFileUtils.getResourceAsFile(""/tokens/oauth_tokens.json""), Charsets.UTF_8).read();
+
+    ObjectMapper mapper = new ObjectMapper();
+    Map<String,String> tokenMap = mapper.readValue(oauthJson, Map.class);
+
+    clientID = tokenMap.get(""client_id"");
+    clientSecret = tokenMap.get(""client_secret"");
+    accessToken = tokenMap.get(""access_token"");
+    refreshToken = tokenMap.get(""refresh_token"");
+    sheetID = tokenMap.get(""sheet_id"");
+
+    ClusterFixtureBuilder builder = new ClusterFixtureBuilder(dirTestWatcher)
+      .configProperty(ExecConstants.HTTP_ENABLE, true)
+      .configProperty(ExecConstants.HTTP_PORT_HUNT, true)
+      .configProperty(ExecConstants.IMPERSONATION_ENABLED, true);
+
+    startCluster(builder);
+
+    pluginRegistry = cluster.drillbit().getContext().getStorage();
+    GoogleSheetsStoragePluginConfig config = GoogleSheetsStoragePluginConfig.builder()
+      .clientID(clientID)
+      .clientSecret(clientSecret)
+      .redirectUris(REDIRECT_URI)
+      .authUri(AUTH_URI)
+      .tokenUri(TOKEN_URI)
+      .allTextMode(false)
+      .extractHeaders(true)
+      .build();
+    config.setEnabled(true);
+    pluginRegistry.validatedPut(""googlesheets"", config);
+  }
+
+  @Test
+  public void testStarQuery() throws Exception {
+    try {
+      initializeTokens(""googlesheets"");
+    } catch (PluginException e) {
+      fail(e.getMessage());
+    }
+
+    String sql = String.format(""SELECT * FROM googlesheets.`%s`.`MixedSheet` WHERE `Col2` < 6.0"", sheetID);
+    RowSet results = queryBuilder().sql(sql).rowSet();
+
+    TupleMetadata expectedSchema = new SchemaBuilder()
+      .addNullable(""Col1"", MinorType.VARCHAR)
+      .addNullable(""Col2"", MinorType.FLOAT8)
+      .addNullable(""Col3"", MinorType.DATE)
+      .buildSchema();
+
+   RowSet expected = client.rowSetBuilder(expectedSchema)
+      .addRow(""Rosaline  Thales"", 1.0, null)
+      .addRow(""Abdolhossein  Detlev"", 2.0001, LocalDate.parse(""2020-04-30""))
+      .addRow(null, 4.0, LocalDate.parse(""2020-06-30""))
+      .addRow(""Yunus  Elena"", 3.5, LocalDate.parse(""2021-01-15""))
+      .addRow(""Swaran  Ohiyesa"", -63.8, LocalDate.parse(""2021-04-08""))
+      .addRow(""Kalani  Godabert"", 0.0, LocalDate.parse(""2021-06-28""))
+      .addRow(""Caishen  Origenes"", 5.0E-7, LocalDate.parse(""2021-07-09""))
+      .addRow(""Toufik  Gurgen"", 2.0, LocalDate.parse(""2021-11-05""))
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(results);
+  }
+
+  @Test
+  public void testProjectPushdown() throws Exception {","[{'comment': ""Isn't it necessary to look in the plan string for the projected columns to test the pushdown part? I would expect this test to pass even if there was no projection pushdown..."", 'commenter': 'jnturton'}]"
2585,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/GoogleSheetsBatchReader.java,"@@ -0,0 +1,301 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.googlesheets;
+
+import com.google.api.services.sheets.v4.Sheets;
+import com.google.api.services.sheets.v4.model.Sheet;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.framework.ManagedReader;","[{'comment': '```suggestion\r\nimport org.apache.drill.exec.physical.impl.scan.v3.ManagedReader;\r\n```\r\nCan we use the EVF2 API here?', 'commenter': 'jnturton'}]"
2599,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java,"@@ -188,10 +192,13 @@ public HashAggBatch(HashAggregate popConfig, RecordBatch incoming, FragmentConte
     wasKilled = false;
 
     final int numGrpByExprs = popConfig.getGroupByExprs().size();
-    comparators = Lists.newArrayListWithExpectedSize(numGrpByExprs);
-    for (int i=0; i<numGrpByExprs; i++) {
-      // nulls are equal in group by case
-      comparators.add(Comparator.IS_NOT_DISTINCT_FROM);
+    isDynamicColumns = numGrpByExprs == 1 && SchemaPath.DYNAMIC_STAR.equals(popConfig.getGroupByExprs().get(0).getRef().getRootSegment().getPath());","[{'comment': 'It is possible to have both dynamic star and explicit columns projection, like\r\n```sql\r\nSELECT *, col1, col2 from tab1\r\n```', 'commenter': 'vvysotskyi'}, {'comment': ""Let's use the same approach as for union all - allow using dynamic star only if the datasource has a defined schema, otherwise, throw an error. In this case, star expansion will be done during planning, and there is no need to handle star columns here."", 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/common/HashTable.java,"@@ -98,6 +98,10 @@ void setup(HashTableConfig htConfig, BufferAllocator allocator, VectorContainer
    */
   int probeForKey(int incomingRowIdx, int hashCode) throws SchemaChangeException;
 
+  int getNum(int currentIndex);","[{'comment': 'Please rename this method to clarify that it holds the count of records for a specific key and add JavaDoc.', 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillSetOpRel.java,"@@ -0,0 +1,79 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.logical;
+
+import org.apache.calcite.linq4j.Ord;
+import org.apache.calcite.plan.RelOptCluster;
+import org.apache.calcite.plan.RelTraitSet;
+import org.apache.calcite.rel.InvalidRelException;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.sql.SqlKind;
+import org.apache.drill.common.logical.data.LogicalOperator;
+import org.apache.drill.common.logical.data.Union;
+import org.apache.drill.exec.planner.common.DrillSetOpRelBase;
+
+import java.util.List;
+
+/**
+ * SetOp implemented in Drill.
+ */
+public class DrillSetOpRel extends DrillSetOpRelBase implements DrillRel {
+  private boolean isAggAdded;
+
+  public DrillSetOpRel(RelOptCluster cluster, RelTraitSet traits,
+                       List<RelNode> inputs, SqlKind kind, boolean all, boolean checkCompatibility, boolean isAggAdded) throws InvalidRelException {
+    super(cluster, traits, inputs, kind, all, checkCompatibility);
+    this.isAggAdded = isAggAdded;
+  }
+
+  public DrillSetOpRel(RelOptCluster cluster, RelTraitSet traits,
+                       List<RelNode> inputs, SqlKind kind, boolean all, boolean checkCompatibility) throws InvalidRelException {
+    super(cluster, traits, inputs, kind, all, checkCompatibility);
+    this.isAggAdded = false;
+  }
+
+  public boolean isAggAdded() {
+    return isAggAdded;
+  }
+
+  public void setAggAdded(boolean aggAdded) {
+    isAggAdded = aggAdded;","[{'comment': ""Rel nodes should be effectively immutable; rules shouldn't modify rel nodes but create new ones."", 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/test/java/org/apache/drill/TestSetOp.java,"@@ -0,0 +1,1093 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill;
+
+import org.apache.drill.exec.planner.physical.PlannerSettings;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.BatchSchemaBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.apache.commons.lang3.tuple.Pair;
+import org.apache.drill.categories.OperatorTest;
+import org.apache.drill.categories.SqlTest;
+import org.apache.drill.categories.UnlikelyTest;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.nio.file.Paths;
+import java.util.List;
+
+@Category({SqlTest.class, OperatorTest.class})
+public class TestSetOp extends ClusterTest {
+  private static final String EMPTY_DIR_NAME = ""empty_directory"";
+  private static final String SLICE_TARGET_DEFAULT = ""alter session reset `planner.slice_target`"";
+
+  @BeforeClass
+  public static void setupTestFiles() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""multilevel"", ""parquet""));
+    dirTestWatcher.makeTestTmpSubDir(Paths.get(EMPTY_DIR_NAME));
+  }
+
+  @Test
+  public void testIntersect() throws Exception {
+    String query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) intersect select * from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""a"", ""b"")
+      .baselineValues(2, 2)
+      .baselineValues(1, 1)
+      .build().run();
+
+    query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) intersect all select * from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""a"", ""b"")
+      .baselineValues(2, 2)
+      .baselineValues(1, 1)
+      .baselineValues(1, 1)
+      .build().run();
+  }
+
+  @Test
+  public void testExcept() throws Exception {
+    String query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) except select * from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+    String aggAbovePattern = "".*Screen.*Agg.*SetOp.*"";
+    String aggBelowPattern = "".*SetOp.*Agg.*Values.*"";
+
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .include(aggAbovePattern)
+      .exclude(aggBelowPattern)
+      .match(true);
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""a"", ""b"")
+      .baselineValues(4, 4)
+      .baselineValues(3, 4)
+      .build().run();
+
+    try {
+      client.alterSession(ExecConstants.EXCEPT_ADD_AGG_BELOW_KEY, true);
+      query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) except select a, b from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+      queryBuilder()
+        .sql(query)
+        .planMatcher()
+        .include(aggBelowPattern)
+        .exclude(aggAbovePattern)
+        .match(true);
+
+      testBuilder()
+        .sqlQuery(query)
+        .unOrdered()
+        .baselineColumns(""a"", ""b"")
+        .baselineValues(4, 4)
+        .baselineValues(3, 4)
+        .build().run();
+    } finally {
+      client.resetSession(ExecConstants.EXCEPT_ADD_AGG_BELOW_KEY);
+    }
+
+    query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) except all select * from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""a"", ""b"")
+      .baselineValues(4, 4)
+      .baselineValues(4, 4)
+      .baselineValues(3, 4)
+      .baselineValues(2, 2)
+      .build().run();
+  }
+
+
+  @Test
+  public void testOverJoin() throws Exception {
+    String query =
+      ""select n1.n_nationkey from cp.`tpch/nation.parquet` n1 inner join cp.`tpch/region.parquet` r1 on n1.n_regionkey = r1.r_regionkey where n1.n_nationkey in (1, 2, 3, 4) "" +
+      ""except "" +
+      ""select n2.n_nationkey from cp.`tpch/nation.parquet` n2 inner join cp.`tpch/region.parquet` r2 on n2.n_regionkey = r2.r_regionkey where n2.n_nationkey in (3, 4)"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"")
+      .baselineValues(1)
+      .baselineValues(2)
+      .build().run();
+  }
+
+  @Test
+  public void testOverAgg() throws Exception {
+    String query = ""select n1.n_regionkey from cp.`tpch/nation.parquet` n1 group by n1.n_regionkey except "" +
+      ""select r1.r_regionkey from cp.`tpch/region.parquet` r1 where r1.r_regionkey in (0, 1) group by r1.r_regionkey"";
+
+    String excludePattern = ""Screen.*Agg.*SetOp"";
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .exclude(excludePattern)
+      .match(true);
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_regionkey"")
+      .baselineValues(2)
+      .baselineValues(3)
+      .baselineValues(4)
+      .build().run();
+  }
+
+  @Test
+  public void testChain() throws Exception {
+    String query = ""select n_regionkey from cp.`tpch/nation.parquet` intersect "" +
+      ""select r_regionkey from cp.`tpch/region.parquet` intersect "" +
+      ""select n_nationkey from cp.`tpch/nation.parquet` where n_nationkey in (1,2) intersect "" +
+      ""select c_custkey from cp.`tpch/customer.parquet` where c_custkey < 5"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_regionkey"")
+      .baselineValues(1)
+      .baselineValues(2)
+      .build().run();
+  }
+
+  @Test
+  public void testSameColumn() throws Exception {
+    String query = ""select n_nationkey, n_regionkey from cp.`tpch/nation.parquet` where n_regionkey = 1 intersect all select r_regionkey, r_regionkey from cp.`tpch/region.parquet` where r_regionkey = 1"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"", ""n_regionkey"")
+      .baselineValues(1, 1)
+      .build().run();
+
+    query = ""select n_regionkey, n_regionkey from cp.`tpch/nation.parquet` where n_regionkey = 1 except all select r_regionkey, r_regionkey from cp.`tpch/region.parquet` where r_regionkey = 1"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_regionkey"", ""n_regionkey0"")
+      .baselineValues(1, 1)
+      .baselineValues(1, 1)
+      .baselineValues(1, 1)
+      .baselineValues(1, 1)
+      .build().run();
+  }
+
+  @Test
+  public void testTwoStringColumns() throws Exception {
+    String query = ""select r_comment, r_regionkey from cp.`tpch/region.parquet` except select n_name, n_nationkey from cp.`tpch/nation.parquet`"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""r_comment"", ""r_regionkey"")
+      .baselineValues(""lar deposits. blithely final packages cajole. regular waters are final requests. regular accounts are according to "", 0)
+      .baselineValues(""hs use ironic, even requests. s"", 1)
+      .baselineValues(""ges. thinly even pinto beans ca"", 2)
+      .baselineValues(""ly final courts cajole furiously final excuse"", 3)
+      .baselineValues(""uickly special accounts cajole carefully blithely close requests. carefully final asymptotes haggle furiousl"", 4)
+      .build().run();
+  }
+
+
+  @Test
+  public void testConstantLiterals() throws Exception {
+    String query = ""(select 'CONST' as LiteralConstant, 1 as NumberConstant, n_nationkey from cp.`tpch/nation.parquet`) "" +
+      ""intersect "" +
+      ""(select 'CONST', 1, r_regionkey from cp.`tpch/region.parquet`)"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""LiteralConstant"", ""NumberConstant"", ""n_nationkey"")
+      .baselineValues(""CONST"", 1, 0)
+      .baselineValues(""CONST"", 1, 1)
+      .baselineValues(""CONST"", 1, 2)
+      .baselineValues(""CONST"", 1, 3)
+      .baselineValues(""CONST"", 1, 4)
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testViewExpandableStar() throws Exception {
+    try {
+      run(""use dfs.tmp"");
+      run(""create view nation_view as select n_nationkey, n_name from (values(4,'4'), (2,'2'), (4,'4'), (1,'1'), (3,'4'), (2,'2'), (1,'1')) t(n_nationkey, n_name)"");
+      run(""create view region_view as select r_regionkey, r_name from (values(1,'1'), (1,'1'), (2,'2'), (3,'3')) t(r_regionkey, r_name)"");
+
+      String query1 = ""(select * from dfs.tmp.`nation_view`) "" +
+        ""except "" +
+        ""(select * from dfs.tmp.`region_view`) "";
+
+      String query2 =  ""(select r_regionkey, r_name from (values(1,'1'), (1,'1'), (2,'2'), (3,'3')) t(r_regionkey, r_name)) "" +
+        ""intersect "" +
+        ""(select * from dfs.tmp.`nation_view`)"";
+
+      testBuilder()
+        .sqlQuery(query1)
+        .unOrdered()
+        .baselineColumns(""n_nationkey"", ""n_name"")
+        .baselineValues(4, ""4"")
+        .baselineValues(3, ""4"")
+        .build().run();
+
+      testBuilder()
+        .sqlQuery(query2)
+        .unOrdered()
+        .baselineColumns(""r_regionkey"", ""r_name"")
+        .baselineValues(1, ""1"")
+        .baselineValues(2, ""2"")
+        .build().run();
+    } finally {
+      run(""drop view if exists nation_view"");
+      run(""drop view if exists region_view"");
+    }
+  }
+
+  @Test
+  public void testDiffDataTypesAndModes() throws Exception {
+    try {
+      run(""use dfs.tmp"");
+      run(""create view nation_view as select n_nationkey, n_name from (values(4,'4'), (2,'2'), (4,'4'), (1,'1'), (3,'4'), (2,'2'), (1,'1')) t(n_nationkey, n_name)"");
+      run(""create view region_view as select r_regionkey, r_name from (values(1,'1'), (1,'1'), (2,'2'), (3,'3')) t(r_regionkey, r_name)"");
+
+
+      String t1 = ""(select r_regionkey, r_name from (values(1,'1'), (1,'1'), (2,'2'), (3,'3')) t(r_regionkey, r_name))"";
+      String t2 = ""(select * from nation_view)"";
+      String t3 = ""(select * from region_view)"";
+      String t4 = ""(select store_id, full_name from cp.`employee.json` limit 5)"";
+
+      String query1 = t1 + "" intersect all "" + t2 + "" intersect all "" + t3 + "" except all "" + t4;
+
+      testBuilder()
+        .sqlQuery(query1)
+        .unOrdered()
+        .baselineColumns(""r_regionkey"", ""r_name"")
+        .baselineValues(1, ""1"")
+        .baselineValues(1, ""1"")
+        .baselineValues(2, ""2"")
+        .build().run();
+    } finally {
+      run(""drop view if exists nation_view"");
+      run(""drop view if exists region_view"");
+    }
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testDistinctOverUnionAllwithFullyQualifiedColumnNames() throws Exception {
+    String query = ""select distinct sq.x1 "" +
+      ""from "" +
+      ""((select n_regionkey as a1 from cp.`tpch/nation.parquet`) "" +
+      ""intersect all "" +
+      ""(select r_regionkey as a2 from cp.`tpch/region.parquet`)) as sq(x1)"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""x1"")
+      .baselineValues(0)
+      .baselineValues(1)
+      .baselineValues(2)
+      .baselineValues(3)
+      .baselineValues(4)
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testContainsColumnAndNumericConstant() throws Exception {
+    String query = ""(select n_nationkey, n_regionkey, n_name from cp.`tpch/nation.parquet`) "" +
+      ""intersect "" +
+      ""(select 1, n_regionkey, 'ARGENTINA' from cp.`tpch/nation.parquet`)"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"", ""n_regionkey"", ""n_name"")
+      .baselineValues(1, 1, ""ARGENTINA"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testEmptySides() throws Exception {
+    String query1 = ""(select n_nationkey, n_regionkey, n_name from cp.`tpch/nation.parquet` limit 0) "" +
+      ""intersect "" +
+      ""(select 1, n_regionkey, 'ARGENTINA' from cp.`tpch/nation.parquet`)"";
+
+    String query2 = ""(select n_nationkey, n_regionkey, n_name from cp.`tpch/nation.parquet` where n_nationkey = 1) "" +
+      ""except "" +
+      ""(select 1, n_regionkey, 'ARGENTINA' from cp.`tpch/nation.parquet` limit 0)"";
+
+    testBuilder()
+      .sqlQuery(query1)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"", ""n_regionkey"", ""n_name"")
+      .expectsEmptyResultSet()
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(query2)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"", ""n_regionkey"", ""n_name"")
+      .baselineValues(1, 1, ""ARGENTINA"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testAggregationOnIntersectOperator() throws Exception {
+    String root = ""/store/text/data/t.json"";
+
+    testBuilder()
+      .sqlQuery(""(select calc1, max(b1) as `max`, min(b1) as `min`, count(c1) as `count` "" +
+        ""from (select a1 + 10 as calc1, b1, c1 from cp.`%s` "" +
+        ""intersect all select a1 + 10 as diff1, b1 as diff2, c1 as diff3 from cp.`%s`) "" +
+        ""group by calc1 order by calc1)"", root, root)
+      .ordered()
+      .baselineColumns(""calc1"", ""max"", ""min"", ""count"")
+      .baselineValues(10L, 2L, 1L, 5L)
+      .baselineValues(20L, 5L, 3L, 5L)
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""(select calc1, min(b1) as `min`, max(b1) as `max`, count(c1) as `count` "" +
+        ""from (select a1 + 10 as calc1, b1, c1 from cp.`%s` "" +
+        ""intersect all select a1 + 10 as diff1, b1 as diff2, c1 as diff3 from cp.`%s`) "" +
+        ""group by calc1 order by calc1)"", root, root)
+      .ordered()
+      .baselineColumns(""calc1"", ""min"", ""max"", ""count"")
+      .baselineValues(10L, 1L, 2L, 5L)
+      .baselineValues(20L, 3L, 5L, 5L)
+      .build().run();
+  }
+
+  @Test(expected = UserException.class)
+  public void testImplicitCastingFailure() throws Exception {
+    String rootInt = ""/store/json/intData.json"";
+    String rootBoolean = ""/store/json/booleanData.json"";
+
+    run(""(select key from cp.`%s` "" +
+      ""intersect all "" +
+      ""select key from cp.`%s` )"", rootInt, rootBoolean);
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testDateAndTimestampJson() throws Exception {
+    String rootDate = ""/store/json/dateData.json"";
+    String rootTimpStmp = ""/store/json/timeStmpData.json"";
+
+    testBuilder()
+      .sqlQuery(""(select max(key) as key from cp.`%s` "" +
+        ""except all select key from cp.`%s`)"", rootDate, rootTimpStmp)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .baselineValues(""2011-07-26"")
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+        ""except select max(key) as key from cp.`%s`"", rootTimpStmp, rootDate)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .baselineValues(""2015-03-26 19:04:55.542"")
+      .baselineValues(""2015-03-26 19:04:55.543"")
+      .baselineValues(""2015-03-26 19:04:55.544"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testOneInputContainsAggFunction() throws Exception {
+    String root = ""/multilevel/csv/1994/Q1/orders_94_q1.csv"";
+
+    testBuilder()
+      .sqlQuery(""select * from ((select max(c1) as ct from (select columns[0] c1 from cp.`%s`)) \n"" +
+        ""intersect all (select columns[0] c2 from cp.`%s`)) order by ct limit 3"", root, root)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues(""99"")
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select * from ((select columns[0] ct from cp.`%s`)\n"" +
+        ""intersect all (select max(c1) as c2 from (select columns[0] c1 from cp.`%s`))) order by ct limit 3"", root, root)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues(""99"")
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select * from ((select max(c1) as ct from (select columns[0] c1 from cp.`%s`))\n"" +
+        ""intersect all (select max(c1) as c2 from (select columns[0] c1 from cp.`%s`))) order by ct"", root, root)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues(""99"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testUnionInputsGroupByOnCSV() throws Exception {
+    String root = ""/multilevel/csv/1994/Q1/orders_94_q1.csv"";
+
+    testBuilder()
+      .sqlQuery(""select * from \n"" +
+          ""((select columns[0] as col0 from cp.`%s` t1 \n"" +
+          ""where t1.columns[0] = 66) \n"" +
+          ""intersect all \n"" +
+          ""(select columns[0] c2 from cp.`%s` t2 \n"" +
+          ""where t2.columns[0] is not null \n"" +
+          ""group by columns[0])) \n"" +
+          ""group by col0"",
+        root, root)
+      .unOrdered()
+      .baselineColumns(""col0"")
+      .baselineValues(""66"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testDiffTypesAtPlanning() throws Exception {
+    testBuilder()
+      .sqlQuery(""select count(c1) as ct from (select cast(r_regionkey as int) c1 from cp.`tpch/region.parquet`) "" +
+        ""intersect (select cast(r_regionkey as int) + 1 c2 from cp.`tpch/region.parquet`)"")
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues((long) 5)
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testRightEmptyJson() throws Exception {
+    String rootEmpty = ""/project/pushdown/empty.json"";
+    String rootSimple = ""/store/json/booleanData.json"";
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+          ""intersect all "" +
+          ""select key from cp.`%s`"",
+        rootSimple,
+        rootEmpty)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .expectsEmptyResultSet()
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+          ""except all "" +
+          ""select key from cp.`%s`"",
+        rootSimple,
+        rootEmpty)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .baselineValues(true)
+      .baselineValues(false)
+      .build().run();
+  }
+
+  @Test
+  public void testUnionAllLeftEmptyJson() throws Exception {","[{'comment': 'Intersect', 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/test/java/org/apache/drill/TestSetOp.java,"@@ -0,0 +1,1093 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill;
+
+import org.apache.drill.exec.planner.physical.PlannerSettings;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.BatchSchemaBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.apache.commons.lang3.tuple.Pair;
+import org.apache.drill.categories.OperatorTest;
+import org.apache.drill.categories.SqlTest;
+import org.apache.drill.categories.UnlikelyTest;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.nio.file.Paths;
+import java.util.List;
+
+@Category({SqlTest.class, OperatorTest.class})
+public class TestSetOp extends ClusterTest {
+  private static final String EMPTY_DIR_NAME = ""empty_directory"";
+  private static final String SLICE_TARGET_DEFAULT = ""alter session reset `planner.slice_target`"";
+
+  @BeforeClass
+  public static void setupTestFiles() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""multilevel"", ""parquet""));
+    dirTestWatcher.makeTestTmpSubDir(Paths.get(EMPTY_DIR_NAME));
+  }
+
+  @Test
+  public void testIntersect() throws Exception {
+    String query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) intersect select * from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""a"", ""b"")
+      .baselineValues(2, 2)
+      .baselineValues(1, 1)
+      .build().run();
+
+    query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) intersect all select * from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""a"", ""b"")
+      .baselineValues(2, 2)
+      .baselineValues(1, 1)
+      .baselineValues(1, 1)
+      .build().run();
+  }
+
+  @Test
+  public void testExcept() throws Exception {
+    String query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) except select * from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+    String aggAbovePattern = "".*Screen.*Agg.*SetOp.*"";
+    String aggBelowPattern = "".*SetOp.*Agg.*Values.*"";
+
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .include(aggAbovePattern)
+      .exclude(aggBelowPattern)
+      .match(true);
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""a"", ""b"")
+      .baselineValues(4, 4)
+      .baselineValues(3, 4)
+      .build().run();
+
+    try {
+      client.alterSession(ExecConstants.EXCEPT_ADD_AGG_BELOW_KEY, true);
+      query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) except select a, b from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+      queryBuilder()
+        .sql(query)
+        .planMatcher()
+        .include(aggBelowPattern)
+        .exclude(aggAbovePattern)
+        .match(true);
+
+      testBuilder()
+        .sqlQuery(query)
+        .unOrdered()
+        .baselineColumns(""a"", ""b"")
+        .baselineValues(4, 4)
+        .baselineValues(3, 4)
+        .build().run();
+    } finally {
+      client.resetSession(ExecConstants.EXCEPT_ADD_AGG_BELOW_KEY);
+    }
+
+    query = ""select * from (values(4,4), (2,2), (4,4), (1,1), (3,4), (2,2), (1,1)) t(a,b) except all select * from (values(1,1), (1,1), (2,2), (3,3)) t(a,b)"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""a"", ""b"")
+      .baselineValues(4, 4)
+      .baselineValues(4, 4)
+      .baselineValues(3, 4)
+      .baselineValues(2, 2)
+      .build().run();
+  }
+
+
+  @Test
+  public void testOverJoin() throws Exception {
+    String query =
+      ""select n1.n_nationkey from cp.`tpch/nation.parquet` n1 inner join cp.`tpch/region.parquet` r1 on n1.n_regionkey = r1.r_regionkey where n1.n_nationkey in (1, 2, 3, 4) "" +
+      ""except "" +
+      ""select n2.n_nationkey from cp.`tpch/nation.parquet` n2 inner join cp.`tpch/region.parquet` r2 on n2.n_regionkey = r2.r_regionkey where n2.n_nationkey in (3, 4)"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"")
+      .baselineValues(1)
+      .baselineValues(2)
+      .build().run();
+  }
+
+  @Test
+  public void testOverAgg() throws Exception {
+    String query = ""select n1.n_regionkey from cp.`tpch/nation.parquet` n1 group by n1.n_regionkey except "" +
+      ""select r1.r_regionkey from cp.`tpch/region.parquet` r1 where r1.r_regionkey in (0, 1) group by r1.r_regionkey"";
+
+    String excludePattern = ""Screen.*Agg.*SetOp"";
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .exclude(excludePattern)
+      .match(true);
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_regionkey"")
+      .baselineValues(2)
+      .baselineValues(3)
+      .baselineValues(4)
+      .build().run();
+  }
+
+  @Test
+  public void testChain() throws Exception {
+    String query = ""select n_regionkey from cp.`tpch/nation.parquet` intersect "" +
+      ""select r_regionkey from cp.`tpch/region.parquet` intersect "" +
+      ""select n_nationkey from cp.`tpch/nation.parquet` where n_nationkey in (1,2) intersect "" +
+      ""select c_custkey from cp.`tpch/customer.parquet` where c_custkey < 5"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_regionkey"")
+      .baselineValues(1)
+      .baselineValues(2)
+      .build().run();
+  }
+
+  @Test
+  public void testSameColumn() throws Exception {
+    String query = ""select n_nationkey, n_regionkey from cp.`tpch/nation.parquet` where n_regionkey = 1 intersect all select r_regionkey, r_regionkey from cp.`tpch/region.parquet` where r_regionkey = 1"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"", ""n_regionkey"")
+      .baselineValues(1, 1)
+      .build().run();
+
+    query = ""select n_regionkey, n_regionkey from cp.`tpch/nation.parquet` where n_regionkey = 1 except all select r_regionkey, r_regionkey from cp.`tpch/region.parquet` where r_regionkey = 1"";
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_regionkey"", ""n_regionkey0"")
+      .baselineValues(1, 1)
+      .baselineValues(1, 1)
+      .baselineValues(1, 1)
+      .baselineValues(1, 1)
+      .build().run();
+  }
+
+  @Test
+  public void testTwoStringColumns() throws Exception {
+    String query = ""select r_comment, r_regionkey from cp.`tpch/region.parquet` except select n_name, n_nationkey from cp.`tpch/nation.parquet`"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""r_comment"", ""r_regionkey"")
+      .baselineValues(""lar deposits. blithely final packages cajole. regular waters are final requests. regular accounts are according to "", 0)
+      .baselineValues(""hs use ironic, even requests. s"", 1)
+      .baselineValues(""ges. thinly even pinto beans ca"", 2)
+      .baselineValues(""ly final courts cajole furiously final excuse"", 3)
+      .baselineValues(""uickly special accounts cajole carefully blithely close requests. carefully final asymptotes haggle furiousl"", 4)
+      .build().run();
+  }
+
+
+  @Test
+  public void testConstantLiterals() throws Exception {
+    String query = ""(select 'CONST' as LiteralConstant, 1 as NumberConstant, n_nationkey from cp.`tpch/nation.parquet`) "" +
+      ""intersect "" +
+      ""(select 'CONST', 1, r_regionkey from cp.`tpch/region.parquet`)"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""LiteralConstant"", ""NumberConstant"", ""n_nationkey"")
+      .baselineValues(""CONST"", 1, 0)
+      .baselineValues(""CONST"", 1, 1)
+      .baselineValues(""CONST"", 1, 2)
+      .baselineValues(""CONST"", 1, 3)
+      .baselineValues(""CONST"", 1, 4)
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testViewExpandableStar() throws Exception {
+    try {
+      run(""use dfs.tmp"");
+      run(""create view nation_view as select n_nationkey, n_name from (values(4,'4'), (2,'2'), (4,'4'), (1,'1'), (3,'4'), (2,'2'), (1,'1')) t(n_nationkey, n_name)"");
+      run(""create view region_view as select r_regionkey, r_name from (values(1,'1'), (1,'1'), (2,'2'), (3,'3')) t(r_regionkey, r_name)"");
+
+      String query1 = ""(select * from dfs.tmp.`nation_view`) "" +
+        ""except "" +
+        ""(select * from dfs.tmp.`region_view`) "";
+
+      String query2 =  ""(select r_regionkey, r_name from (values(1,'1'), (1,'1'), (2,'2'), (3,'3')) t(r_regionkey, r_name)) "" +
+        ""intersect "" +
+        ""(select * from dfs.tmp.`nation_view`)"";
+
+      testBuilder()
+        .sqlQuery(query1)
+        .unOrdered()
+        .baselineColumns(""n_nationkey"", ""n_name"")
+        .baselineValues(4, ""4"")
+        .baselineValues(3, ""4"")
+        .build().run();
+
+      testBuilder()
+        .sqlQuery(query2)
+        .unOrdered()
+        .baselineColumns(""r_regionkey"", ""r_name"")
+        .baselineValues(1, ""1"")
+        .baselineValues(2, ""2"")
+        .build().run();
+    } finally {
+      run(""drop view if exists nation_view"");
+      run(""drop view if exists region_view"");
+    }
+  }
+
+  @Test
+  public void testDiffDataTypesAndModes() throws Exception {
+    try {
+      run(""use dfs.tmp"");
+      run(""create view nation_view as select n_nationkey, n_name from (values(4,'4'), (2,'2'), (4,'4'), (1,'1'), (3,'4'), (2,'2'), (1,'1')) t(n_nationkey, n_name)"");
+      run(""create view region_view as select r_regionkey, r_name from (values(1,'1'), (1,'1'), (2,'2'), (3,'3')) t(r_regionkey, r_name)"");
+
+
+      String t1 = ""(select r_regionkey, r_name from (values(1,'1'), (1,'1'), (2,'2'), (3,'3')) t(r_regionkey, r_name))"";
+      String t2 = ""(select * from nation_view)"";
+      String t3 = ""(select * from region_view)"";
+      String t4 = ""(select store_id, full_name from cp.`employee.json` limit 5)"";
+
+      String query1 = t1 + "" intersect all "" + t2 + "" intersect all "" + t3 + "" except all "" + t4;
+
+      testBuilder()
+        .sqlQuery(query1)
+        .unOrdered()
+        .baselineColumns(""r_regionkey"", ""r_name"")
+        .baselineValues(1, ""1"")
+        .baselineValues(1, ""1"")
+        .baselineValues(2, ""2"")
+        .build().run();
+    } finally {
+      run(""drop view if exists nation_view"");
+      run(""drop view if exists region_view"");
+    }
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testDistinctOverUnionAllwithFullyQualifiedColumnNames() throws Exception {
+    String query = ""select distinct sq.x1 "" +
+      ""from "" +
+      ""((select n_regionkey as a1 from cp.`tpch/nation.parquet`) "" +
+      ""intersect all "" +
+      ""(select r_regionkey as a2 from cp.`tpch/region.parquet`)) as sq(x1)"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""x1"")
+      .baselineValues(0)
+      .baselineValues(1)
+      .baselineValues(2)
+      .baselineValues(3)
+      .baselineValues(4)
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testContainsColumnAndNumericConstant() throws Exception {
+    String query = ""(select n_nationkey, n_regionkey, n_name from cp.`tpch/nation.parquet`) "" +
+      ""intersect "" +
+      ""(select 1, n_regionkey, 'ARGENTINA' from cp.`tpch/nation.parquet`)"";
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"", ""n_regionkey"", ""n_name"")
+      .baselineValues(1, 1, ""ARGENTINA"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testEmptySides() throws Exception {
+    String query1 = ""(select n_nationkey, n_regionkey, n_name from cp.`tpch/nation.parquet` limit 0) "" +
+      ""intersect "" +
+      ""(select 1, n_regionkey, 'ARGENTINA' from cp.`tpch/nation.parquet`)"";
+
+    String query2 = ""(select n_nationkey, n_regionkey, n_name from cp.`tpch/nation.parquet` where n_nationkey = 1) "" +
+      ""except "" +
+      ""(select 1, n_regionkey, 'ARGENTINA' from cp.`tpch/nation.parquet` limit 0)"";
+
+    testBuilder()
+      .sqlQuery(query1)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"", ""n_regionkey"", ""n_name"")
+      .expectsEmptyResultSet()
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(query2)
+      .unOrdered()
+      .baselineColumns(""n_nationkey"", ""n_regionkey"", ""n_name"")
+      .baselineValues(1, 1, ""ARGENTINA"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testAggregationOnIntersectOperator() throws Exception {
+    String root = ""/store/text/data/t.json"";
+
+    testBuilder()
+      .sqlQuery(""(select calc1, max(b1) as `max`, min(b1) as `min`, count(c1) as `count` "" +
+        ""from (select a1 + 10 as calc1, b1, c1 from cp.`%s` "" +
+        ""intersect all select a1 + 10 as diff1, b1 as diff2, c1 as diff3 from cp.`%s`) "" +
+        ""group by calc1 order by calc1)"", root, root)
+      .ordered()
+      .baselineColumns(""calc1"", ""max"", ""min"", ""count"")
+      .baselineValues(10L, 2L, 1L, 5L)
+      .baselineValues(20L, 5L, 3L, 5L)
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""(select calc1, min(b1) as `min`, max(b1) as `max`, count(c1) as `count` "" +
+        ""from (select a1 + 10 as calc1, b1, c1 from cp.`%s` "" +
+        ""intersect all select a1 + 10 as diff1, b1 as diff2, c1 as diff3 from cp.`%s`) "" +
+        ""group by calc1 order by calc1)"", root, root)
+      .ordered()
+      .baselineColumns(""calc1"", ""min"", ""max"", ""count"")
+      .baselineValues(10L, 1L, 2L, 5L)
+      .baselineValues(20L, 3L, 5L, 5L)
+      .build().run();
+  }
+
+  @Test(expected = UserException.class)
+  public void testImplicitCastingFailure() throws Exception {
+    String rootInt = ""/store/json/intData.json"";
+    String rootBoolean = ""/store/json/booleanData.json"";
+
+    run(""(select key from cp.`%s` "" +
+      ""intersect all "" +
+      ""select key from cp.`%s` )"", rootInt, rootBoolean);
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testDateAndTimestampJson() throws Exception {
+    String rootDate = ""/store/json/dateData.json"";
+    String rootTimpStmp = ""/store/json/timeStmpData.json"";
+
+    testBuilder()
+      .sqlQuery(""(select max(key) as key from cp.`%s` "" +
+        ""except all select key from cp.`%s`)"", rootDate, rootTimpStmp)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .baselineValues(""2011-07-26"")
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+        ""except select max(key) as key from cp.`%s`"", rootTimpStmp, rootDate)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .baselineValues(""2015-03-26 19:04:55.542"")
+      .baselineValues(""2015-03-26 19:04:55.543"")
+      .baselineValues(""2015-03-26 19:04:55.544"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testOneInputContainsAggFunction() throws Exception {
+    String root = ""/multilevel/csv/1994/Q1/orders_94_q1.csv"";
+
+    testBuilder()
+      .sqlQuery(""select * from ((select max(c1) as ct from (select columns[0] c1 from cp.`%s`)) \n"" +
+        ""intersect all (select columns[0] c2 from cp.`%s`)) order by ct limit 3"", root, root)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues(""99"")
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select * from ((select columns[0] ct from cp.`%s`)\n"" +
+        ""intersect all (select max(c1) as c2 from (select columns[0] c1 from cp.`%s`))) order by ct limit 3"", root, root)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues(""99"")
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select * from ((select max(c1) as ct from (select columns[0] c1 from cp.`%s`))\n"" +
+        ""intersect all (select max(c1) as c2 from (select columns[0] c1 from cp.`%s`))) order by ct"", root, root)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues(""99"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testUnionInputsGroupByOnCSV() throws Exception {
+    String root = ""/multilevel/csv/1994/Q1/orders_94_q1.csv"";
+
+    testBuilder()
+      .sqlQuery(""select * from \n"" +
+          ""((select columns[0] as col0 from cp.`%s` t1 \n"" +
+          ""where t1.columns[0] = 66) \n"" +
+          ""intersect all \n"" +
+          ""(select columns[0] c2 from cp.`%s` t2 \n"" +
+          ""where t2.columns[0] is not null \n"" +
+          ""group by columns[0])) \n"" +
+          ""group by col0"",
+        root, root)
+      .unOrdered()
+      .baselineColumns(""col0"")
+      .baselineValues(""66"")
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testDiffTypesAtPlanning() throws Exception {
+    testBuilder()
+      .sqlQuery(""select count(c1) as ct from (select cast(r_regionkey as int) c1 from cp.`tpch/region.parquet`) "" +
+        ""intersect (select cast(r_regionkey as int) + 1 c2 from cp.`tpch/region.parquet`)"")
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues((long) 5)
+      .build().run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testRightEmptyJson() throws Exception {
+    String rootEmpty = ""/project/pushdown/empty.json"";
+    String rootSimple = ""/store/json/booleanData.json"";
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+          ""intersect all "" +
+          ""select key from cp.`%s`"",
+        rootSimple,
+        rootEmpty)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .expectsEmptyResultSet()
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+          ""except all "" +
+          ""select key from cp.`%s`"",
+        rootSimple,
+        rootEmpty)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .baselineValues(true)
+      .baselineValues(false)
+      .build().run();
+  }
+
+  @Test
+  public void testUnionAllLeftEmptyJson() throws Exception {
+    final String rootEmpty = ""/project/pushdown/empty.json"";
+    final String rootSimple = ""/store/json/booleanData.json"";
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+          ""intersect all "" +
+          ""select key from cp.`%s`"",
+        rootEmpty,
+        rootSimple)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .expectsEmptyResultSet()
+      .build().run();
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+          ""except all "" +
+          ""select key from cp.`%s`"",
+        rootEmpty,
+        rootSimple)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .expectsEmptyResultSet()
+      .build().run();
+  }
+
+  @Test
+  public void testUnionAllBothEmptyJson() throws Exception {
+    final String rootEmpty = ""/project/pushdown/empty.json"";
+
+    final List<Pair<SchemaPath, TypeProtos.MajorType>> expectedSchema = Lists.newArrayList();
+    final TypeProtos.MajorType majorType = TypeProtos.MajorType.newBuilder()
+      .setMinorType(TypeProtos.MinorType.INT)
+      .setMode(TypeProtos.DataMode.OPTIONAL)
+      .build();
+    expectedSchema.add(Pair.of(SchemaPath.getSimplePath(""key""), majorType));
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+          ""intersect all "" +
+          ""select key from cp.`%s`"",
+        rootEmpty,
+        rootEmpty)
+      .schemaBaseLine(expectedSchema)
+      .build()
+      .run();
+  }
+
+  @Test
+  public void testRightEmptyDataBatch() throws Exception {
+    String rootSimple = ""/store/json/booleanData.json"";
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` "" +
+          ""except all "" +
+          ""select key from cp.`%s` where 1 = 0"",
+        rootSimple,
+        rootSimple)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .baselineValues(true)
+      .baselineValues(false)
+      .build().run();
+  }
+
+  @Test
+  public void testLeftEmptyDataBatch() throws Exception {
+    String rootSimple = ""/store/json/booleanData.json"";
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` where 1 = 0 "" +
+          ""except all "" +
+          ""select key from cp.`%s`"",
+        rootSimple,
+        rootSimple)
+      .unOrdered()
+      .baselineColumns(""key"")
+      .expectsEmptyResultSet()
+      .build()
+      .run();
+  }
+
+  @Test
+  public void testBothEmptyDataBatch() throws Exception {
+    String rootSimple = ""/store/json/booleanData.json"";
+
+    final List<Pair<SchemaPath, TypeProtos.MajorType>> expectedSchema = Lists.newArrayList();
+    final TypeProtos.MajorType majorType = TypeProtos.MajorType.newBuilder()
+      .setMinorType(TypeProtos.MinorType.BIT) // field ""key"" is boolean type
+      .setMode(TypeProtos.DataMode.OPTIONAL)
+      .build();
+    expectedSchema.add(Pair.of(SchemaPath.getSimplePath(""key""), majorType));
+
+    testBuilder()
+      .sqlQuery(""select key from cp.`%s` where 1 = 0 "" +
+          ""intersect all "" +
+          ""select key from cp.`%s` where 1 = 0"",
+        rootSimple,
+        rootSimple)
+      .schemaBaseLine(expectedSchema)
+      .build()
+      .run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testInListOnIntersect() throws Exception {
+    String query = ""select n_nationkey \n"" +
+      ""from (select n1.n_nationkey from cp.`tpch/nation.parquet` n1 inner join cp.`tpch/region.parquet` r1 on n1.n_regionkey = r1.r_regionkey \n"" +
+      ""intersect \n"" +
+      ""select n2.n_nationkey from cp.`tpch/nation.parquet` n2 inner join cp.`tpch/region.parquet` r2 on n2.n_regionkey = r2.r_regionkey) \n"" +
+      ""where n_nationkey in (1, 2)"";
+
+    // Validate the plan
+    final String[] expectedPlan = {""Project.*\n"" +
+      "".*SetOp\\(all=\\[false\\], kind=\\[INTERSECT\\]\\).*\n"" +
+      "".*Project.*\n"" +
+      "".*HashJoin.*\n"" +
+      "".*SelectionVectorRemover.*\n"" +
+      "".*Filter.*\n"" +
+      "".*Scan.*columns=\\[`n_regionkey`, `n_nationkey`\\].*\n"" +
+      "".*Scan.*columns=\\[`r_regionkey`\\].*\n"" +
+      "".*Project.*\n"" +
+      "".*HashJoin.*\n"" +
+      "".*SelectionVectorRemover.*\n"" +
+      "".*Filter.*\n"" +
+      "".*Scan.*columns=\\[`n_regionkey`, `n_nationkey`\\].*\n"" +
+      "".*Scan.*columns=\\[`r_regionkey`\\].*""};
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .include(expectedPlan)
+      .match();
+
+    // Validate the result
+    testBuilder()
+      .sqlQuery(query)
+      .ordered()
+      .baselineColumns(""n_nationkey"")
+      .baselineValues(1)
+      .baselineValues(2)
+      .build()
+      .run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testIntersectWith() throws Exception {
+    final String query1 = ""WITH year_total \n"" +
+      ""     AS (SELECT c.r_regionkey    customer_id,\n"" +
+      ""                1 year_total\n"" +
+      ""         FROM   cp.`tpch/region.parquet` c\n"" +
+      ""         Intersect ALL \n"" +
+      ""         SELECT c.r_regionkey    customer_id, \n"" +
+      ""                1 year_total\n"" +
+      ""         FROM   cp.`tpch/region.parquet` c) \n"" +
+      ""SELECT count(t_s_secyear.customer_id) as ct \n"" +
+      ""FROM   year_total t_s_firstyear, \n"" +
+      ""       year_total t_s_secyear, \n"" +
+      ""       year_total t_w_firstyear, \n"" +
+      ""       year_total t_w_secyear \n"" +
+      ""WHERE  t_s_secyear.customer_id = t_s_firstyear.customer_id \n"" +
+      ""       AND t_s_firstyear.customer_id = t_w_secyear.customer_id \n"" +
+      ""       AND t_s_firstyear.customer_id = t_w_firstyear.customer_id \n"" +
+      ""       AND CASE \n"" +
+      ""             WHEN t_w_firstyear.year_total > 0 THEN t_w_secyear.year_total \n"" +
+      ""             ELSE NULL \n"" +
+      ""           END > -1"";
+
+    final String query2 = ""WITH year_total \n"" +
+      ""     AS (SELECT c.r_regionkey    customer_id,\n"" +
+      ""                1 year_total\n"" +
+      ""         FROM   cp.`tpch/region.parquet` c\n"" +
+      ""         Intersect ALL \n"" +
+      ""         SELECT c.r_regionkey    customer_id, \n"" +
+      ""                1 year_total\n"" +
+      ""         FROM   cp.`tpch/region.parquet` c) \n"" +
+      ""SELECT count(t_w_firstyear.customer_id) as ct \n"" +
+      ""FROM   year_total t_w_firstyear, \n"" +
+      ""       year_total t_w_secyear \n"" +
+      ""WHERE  t_w_firstyear.year_total = t_w_secyear.year_total \n"" +
+      "" AND t_w_firstyear.year_total > 0 and t_w_secyear.year_total > 0"";
+
+    final String query3 = ""WITH year_total_1\n"" +
+      ""             AS (SELECT c.r_regionkey    customer_id,\n"" +
+      ""                        1 year_total\n"" +
+      ""                 FROM   cp.`tpch/region.parquet` c\n"" +
+      ""                 Intersect ALL \n"" +
+      ""                 SELECT c.r_regionkey    customer_id, \n"" +
+      ""                        1 year_total\n"" +
+      ""                 FROM   cp.`tpch/region.parquet` c) \n"" +
+      ""             , year_total_2\n"" +
+      ""             AS (SELECT c.n_nationkey    customer_id,\n"" +
+      ""                        1 year_total\n"" +
+      ""                 FROM   cp.`tpch/nation.parquet` c\n"" +
+      ""                 Intersect ALL \n"" +
+      ""                 SELECT c.n_nationkey    customer_id, \n"" +
+      ""                        1 year_total\n"" +
+      ""                 FROM   cp.`tpch/nation.parquet` c) \n"" +
+      ""        SELECT count(t_w_firstyear.customer_id) as ct\n"" +
+      ""        FROM   year_total_1 t_w_firstyear,\n"" +
+      ""               year_total_2 t_w_secyear\n"" +
+      ""        WHERE  t_w_firstyear.year_total = t_w_secyear.year_total\n"" +
+      ""           AND t_w_firstyear.year_total > 0 and t_w_secyear.year_total > 0"";
+
+    final String query4 = ""WITH year_total_1\n"" +
+      ""             AS (SELECT c.n_regionkey    customer_id,\n"" +
+      ""                        1 year_total\n"" +
+      ""                 FROM   cp.`tpch/nation.parquet` c\n"" +
+      ""                 Intersect ALL \n"" +
+      ""                 SELECT c.r_regionkey    customer_id, \n"" +
+      ""                        1 year_total\n"" +
+      ""                 FROM   cp.`tpch/region.parquet` c), \n"" +
+      ""             year_total_2\n"" +
+      ""             AS (SELECT c.n_regionkey    customer_id,\n"" +
+      ""                        1 year_total\n"" +
+      ""                 FROM   cp.`tpch/nation.parquet` c\n"" +
+      ""                 Intersect ALL \n"" +
+      ""                 SELECT c.r_regionkey    customer_id, \n"" +
+      ""                        1 year_total\n"" +
+      ""                 FROM   cp.`tpch/region.parquet` c) \n"" +
+      ""        SELECT count(t_w_firstyear.customer_id) as ct \n"" +
+      ""        FROM   year_total_1 t_w_firstyear,\n"" +
+      ""               year_total_2 t_w_secyear\n"" +
+      ""        WHERE  t_w_firstyear.year_total = t_w_secyear.year_total\n"" +
+      ""         AND t_w_firstyear.year_total > 0 and t_w_secyear.year_total > 0"";
+
+    testBuilder()
+      .sqlQuery(query1)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues((long) 5)
+      .build()
+      .run();
+
+    testBuilder()
+      .sqlQuery(query2)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues((long) 25)
+      .build()
+      .run();
+
+    testBuilder()
+      .sqlQuery(query3)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues((long) 125)
+      .build()
+      .run();
+
+    testBuilder()
+      .sqlQuery(query4)
+      .ordered()
+      .baselineColumns(""ct"")
+      .baselineValues((long) 25)
+      .build()
+      .run();
+  }
+
+  @Test
+  @Category(UnlikelyTest.class)
+  public void testDrill4147_1() throws Exception {
+    final String l = ""/multilevel/parquet/1994"";
+    final String r = ""/multilevel/parquet/1995"";
+
+    final String query = String.format(""SELECT o_custkey FROM dfs.`%s` \n"" +
+      ""Except All SELECT o_custkey FROM dfs.`%s`"", l, r);
+
+    // Validate the plan
+    final String[] expectedPlan = {""UnionExchange.*\n"",
+      "".*SetOp""};
+
+    try {
+      client.alterSession(ExecConstants.SLICE_TARGET, 1);
+      queryBuilder()
+        .sql(query)
+        .planMatcher()
+        .include(expectedPlan)
+        .match();
+
+      testBuilder()
+        .optionSettingQueriesForBaseline(SLICE_TARGET_DEFAULT)
+        .unOrdered()
+        .sqlQuery(query)
+        .sqlBaselineQuery(query)
+        .build()
+        .run();
+    } finally {
+      client.resetSession(ExecConstants.SLICE_TARGET);
+    }
+  }
+
+  @Test// group-by on top of set op
+  public void testDrill4147_2() throws Exception {","[{'comment': 'Please fix test name', 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/test/java/org/apache/drill/TestSetOp.java,"@@ -0,0 +1,1093 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill;
+
+import org.apache.drill.exec.planner.physical.PlannerSettings;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.BatchSchemaBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.apache.commons.lang3.tuple.Pair;
+import org.apache.drill.categories.OperatorTest;
+import org.apache.drill.categories.SqlTest;
+import org.apache.drill.categories.UnlikelyTest;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.nio.file.Paths;
+import java.util.List;
+
+@Category({SqlTest.class, OperatorTest.class})
+public class TestSetOp extends ClusterTest {","[{'comment': 'Could you please add more tests that check several batches? It could be done using the `UNION ALL` operator. Also, it would be interesting to see cases when the first batch of one side is empty, and so on.\r\nOne more scenario is to check how it behaves with complex types. It is fine if not supported, but we should be sure that we have the correct error message and error handling.', 'commenter': 'vvysotskyi'}, {'comment': 'first empty batch will be skipped in sniffNonEmptyBatch', 'commenter': 'Leon-WTF'}, {'comment': 'It is good that it is handled in the code, but it could also be fine to have a test that verifies it works as expected, so we will be sure that no future changes will break it.', 'commenter': 'vvysotskyi'}, {'comment': 'Sure, I have added that.', 'commenter': 'Leon-WTF'}]"
2599,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillSetOpRel.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.logical;
+
+import org.apache.calcite.linq4j.Ord;
+import org.apache.calcite.plan.RelOptCluster;
+import org.apache.calcite.plan.RelTraitSet;
+import org.apache.calcite.rel.InvalidRelException;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.sql.SqlKind;
+import org.apache.drill.common.logical.data.LogicalOperator;
+import org.apache.drill.common.logical.data.Union;
+import org.apache.drill.exec.planner.common.DrillSetOpRelBase;
+
+import java.util.List;
+
+/**
+ * SetOp implemented in Drill.
+ */
+public class DrillSetOpRel extends DrillSetOpRelBase implements DrillRel {","[{'comment': 'I think instead of having a common class for the intersect and except, it would be better to differ them as it is done in Calcite and extend their implementations, so it will help to remove defining custom `estimateRowCount` methods and allow using more optimizations designed for these operators, like push down to JDBC and so on.', 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/setop/HashSetOpProbeTemplate.java,"@@ -0,0 +1,354 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.impl.setop;
+
+import org.apache.calcite.sql.SqlKind;
+import org.apache.drill.exec.exception.SchemaChangeException;
+import org.apache.drill.exec.physical.impl.common.HashPartition;
+import org.apache.drill.exec.physical.impl.join.HashJoinHelper;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.RecordBatch;
+import org.apache.drill.exec.record.RecordBatch.IterOutcome;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.VectorWrapper;
+import org.apache.drill.exec.vector.IntVector;
+import org.apache.drill.exec.vector.ValueVector;
+
+import java.util.ArrayList;
+
+import static org.apache.drill.exec.record.JoinBatchMemoryManager.LEFT_INDEX;
+
+public class HashSetOpProbeTemplate implements HashSetOpProbe {","[{'comment': 'It looks like it has a lot of code copied from HashAggTemplate. Is it possible to move out common code to avoid duplicating it?', 'commenter': 'vvysotskyi'}, {'comment': ""It's more like HashJoinProbeTemplate, I have refactored it."", 'commenter': 'Leon-WTF'}]"
2599,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillAddAggForExceptRule.java,"@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.logical;
+
+import org.apache.calcite.plan.RelOptRule;
+import org.apache.calcite.plan.RelOptRuleCall;
+import org.apache.calcite.plan.RelOptRuleOperand;
+import org.apache.calcite.plan.hep.HepRelVertex;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.util.ImmutableBitSet;
+import org.apache.calcite.util.trace.CalciteTrace;
+import org.apache.curator.shaded.com.google.common.collect.ImmutableList;
+import org.apache.drill.exec.planner.physical.PrelUtil;
+import org.slf4j.Logger;
+
+import static org.apache.drill.exec.ExecConstants.EXCEPT_ADD_AGG_BELOW;
+
+/**
+ * Rule that try to add agg for Except set op.
+ */
+public class DrillAddAggForExceptRule extends RelOptRule {
+  public static final RelOptRule INSTANCE = new DrillAddAggForExceptRule(RelOptHelper.any(DrillExceptRel.class), ""DrillAddAggForExceptRule"");
+  protected static final Logger tracer = CalciteTrace.getPlannerTracer();
+
+  public DrillAddAggForExceptRule(RelOptRuleOperand operand, String description) {
+    super(operand, description);
+  }
+
+  @Override
+  public boolean matches(RelOptRuleCall call) {
+    DrillExceptRel drillExceptRel = call.rel(0);
+    return !drillExceptRel.all && !drillExceptRel.isAggAdded() && !findAggRel(drillExceptRel.getInput(0));","[{'comment': ""I'm not sure whether this check would work properly in some cases. For example, the volcano planner will use RelSet to wrap nodes, and perhaps there are some other cases. Instead, I propose using `RelMetadataQuery.getUniqueKeys()` to ensure that input columns have unique values, and if it is so, do not add aggregate. It calls methods from `org.apache.calcite.rel.metadata.RelMdUniqueKeys` for specific node types and should handle more cases than existing checks. In this case, the `isAggAdded` field wouldn't be required."", 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillAddAggForExceptRule.java,"@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.logical;
+
+import org.apache.calcite.plan.RelOptRule;
+import org.apache.calcite.plan.RelOptRuleCall;
+import org.apache.calcite.plan.RelOptRuleOperand;
+import org.apache.calcite.plan.hep.HepRelVertex;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.util.ImmutableBitSet;
+import org.apache.calcite.util.trace.CalciteTrace;
+import org.apache.curator.shaded.com.google.common.collect.ImmutableList;
+import org.apache.drill.exec.planner.physical.PrelUtil;
+import org.slf4j.Logger;
+
+import static org.apache.drill.exec.ExecConstants.EXCEPT_ADD_AGG_BELOW;
+
+/**
+ * Rule that try to add agg for Except set op.
+ */
+public class DrillAddAggForExceptRule extends RelOptRule {
+  public static final RelOptRule INSTANCE = new DrillAddAggForExceptRule(RelOptHelper.any(DrillExceptRel.class), ""DrillAddAggForExceptRule"");
+  protected static final Logger tracer = CalciteTrace.getPlannerTracer();
+
+  public DrillAddAggForExceptRule(RelOptRuleOperand operand, String description) {
+    super(operand, description);
+  }
+
+  @Override
+  public boolean matches(RelOptRuleCall call) {
+    DrillExceptRel drillExceptRel = call.rel(0);
+    return !drillExceptRel.all && !drillExceptRel.isAggAdded() && !findAggRel(drillExceptRel.getInput(0));
+  }
+
+  private boolean findAggRel(RelNode relNode) {
+    if (relNode instanceof HepRelVertex) {
+      return findAggRel(((HepRelVertex) relNode).getCurrentRel());
+    }
+    if (relNode instanceof DrillAggregateRel) {
+      return true;
+    }
+    if (relNode.getInputs().size() == 1 && relNode.getInput(0) != null) {
+      return findAggRel(relNode.getInput(0));
+    }
+    return false;
+  }
+
+  @Override
+  public void onMatch(RelOptRuleCall call) {
+    final DrillExceptRel drillExceptRel = call.rel(0);
+    boolean addAggBelow = PrelUtil.getPlannerSettings(call.getPlanner()).getOptions().getOption(EXCEPT_ADD_AGG_BELOW);
+    if (addAggBelow) {
+      RelNode aggNode = new DrillAggregateRel(drillExceptRel.getCluster(), drillExceptRel.getTraitSet(), drillExceptRel.getInput(0),
+        ImmutableBitSet.range(0, drillExceptRel.getInput(0).getRowType().getFieldList().size()), ImmutableList.of(), ImmutableList.of());
+      call.transformTo(drillExceptRel.copy(ImmutableList.of(aggNode, drillExceptRel.getInput(1)), true));
+    } else {
+      call.transformTo(new DrillAggregateRel(drillExceptRel.getCluster(), drillExceptRel.getTraitSet(), drillExceptRel.copy(true),","[{'comment': 'Do we need to add aggregate on top of except?', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi It's for performance, if the data cardinality is high, aggregate before except may not reduce many data, if the data after except left are few, aggregate after except will only handle few data which is faster than before except. This may be choosen by statistics info + CBO automaticlly in the future."", 'commenter': 'Leon-WTF'}, {'comment': 'But output values should be already distinct after the execution of except operator, so the aggregate will do nothing.', 'commenter': 'vvysotskyi'}, {'comment': ""It's not distinct for left table after except operator. I choosed to reuse an aggregate operator to do the distinct."", 'commenter': 'Leon-WTF'}, {'comment': 'Oh, ok, if it is specific to our implementation of except operator, aggregation added here possibly could be removed by other Calcite rules which assume that results would be already distinct.\r\n\r\nI think it would be better to add an aggregation when converting it to physical rel nodes.', 'commenter': 'vvysotskyi'}, {'comment': 'oh, yes, I will refactor that.', 'commenter': 'Leon-WTF'}, {'comment': '@vvysotskyi One more question about moving converting rule to physical phase, I need to add physical agg rel node, so needs to add both hash(distribute by all keys/single key) and stream agg, right?', 'commenter': 'Leon-WTF'}, {'comment': ""Drill doesn't use streaming aggregate for distinct calls, so only hash agg should be enough."", 'commenter': 'vvysotskyi'}, {'comment': '> Drill doesn\'t use streaming aggregate for distinct calls, so only hash agg should be enough.\r\n\r\n@vvysotskyi I see it checks aggregate.containsDistinctCall() in StreamAggPrule, but It will generate steam agg for sql like ""select a,b,c from foo group by a,b,c"".', 'commenter': 'Leon-WTF'}]"
2599,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillSetOpRule.java,"@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.logical;
+
+import org.apache.calcite.plan.Convention;
+import org.apache.calcite.plan.RelOptRule;
+import org.apache.calcite.plan.RelOptRuleCall;
+import org.apache.calcite.plan.RelOptRuleOperand;
+import org.apache.calcite.plan.RelTraitSet;
+import org.apache.calcite.rel.InvalidRelException;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.rel.core.SetOp;
+import org.apache.calcite.rel.logical.LogicalIntersect;
+import org.apache.calcite.rel.logical.LogicalMinus;
+import org.apache.calcite.util.trace.CalciteTrace;
+import org.slf4j.Logger;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+/**
+ * Rule that converts {@link LogicalIntersect} or {@link LogicalMinus} to
+ * {@link DrillIntersectRel} or {@link DrillExceptRel}.
+ */
+public class DrillSetOpRule extends RelOptRule {
+  public static final List<RelOptRule> INSTANCES = Arrays.asList(
+      new DrillSetOpRule(RelOptHelper.any(LogicalIntersect.class, Convention.NONE), ""DrillIntersectRelRule""),
+      new DrillSetOpRule(RelOptHelper.any(LogicalMinus.class, Convention.NONE), ""DrillExceptRelRule"")
+  );
+  protected static final Logger tracer = CalciteTrace.getPlannerTracer();
+
+  public DrillSetOpRule(RelOptRuleOperand operand, String description) {
+    super(operand, description);
+  }
+
+  @Override
+  public void onMatch(RelOptRuleCall call) {
+    final SetOp setOp = call.rel(0);
+    final RelTraitSet traits = setOp.getTraitSet().plus(DrillRel.DRILL_LOGICAL);
+    final List<RelNode> convertedInputs = new ArrayList<>();
+    for (RelNode input : setOp.getInputs()) {
+      RelNode convertedInput = convert(input, input.getTraitSet().plus(DrillRel.DRILL_LOGICAL).simplify());
+      convertedInputs.add(convertedInput);
+    }
+    try {
+      if (setOp instanceof LogicalMinus) {","[{'comment': 'We could create and pass a specific rel factory to the place where the rule is created and use it here instead of checking which type of node is.', 'commenter': 'vvysotskyi'}]"
2599,logical/src/main/java/org/apache/drill/common/logical/data/visitors/LogicalVisitor.java,"@@ -62,7 +64,10 @@
     public RETURN visitRunningAggregate(RunningAggregate runningAggregate, EXTRA value) throws EXCEP;
     public RETURN visitTransform(Transform transform, EXTRA value) throws EXCEP;
     public RETURN visitUnion(Union union, EXTRA value) throws EXCEP;
-    public RETURN visitWindow(Window window, EXTRA value) throws EXCEP;
+    public RETURN visitExcept(Except except, EXTRA value) throws EXCEP;
+    public RETURN visitIntersect(Intersect intersect, EXTRA value) throws EXCEP;
+
+  public RETURN visitWindow(Window window, EXTRA value) throws EXCEP;","[{'comment': 'Please fix the indentation here.', 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/test/java/org/apache/drill/TestSetOp.java,"@@ -0,0 +1,1183 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill;
+
+import org.apache.drill.exec.planner.physical.PlannerSettings;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.BatchSchemaBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.apache.commons.lang3.tuple.Pair;
+import org.apache.drill.categories.OperatorTest;
+import org.apache.drill.categories.SqlTest;
+import org.apache.drill.categories.UnlikelyTest;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.nio.file.Paths;
+import java.util.List;
+
+import static org.junit.Assert.assertTrue;
+
+@Category({SqlTest.class, OperatorTest.class})
+public class TestSetOp extends ClusterTest {
+  private static final String EMPTY_DIR_NAME = ""empty_directory"";
+  private static final String SLICE_TARGET_DEFAULT = ""alter session reset `planner.slice_target`"";
+
+  @BeforeClass
+  public static void setupTestFiles() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""multilevel"", ""parquet""));
+    dirTestWatcher.makeTestTmpSubDir(Paths.get(EMPTY_DIR_NAME));
+  }
+
+  @Test
+  public void TestExceptionWithSchemaLessDataSource() {
+    boolean exceptionEncountered = true;
+    String root = ""/multilevel/csv/1994/Q1/orders_94_q1.csv"";
+    try {
+      testBuilder()
+        .sqlQuery(""select * from cp.`%s` intersect select * from cp.`%s`"", root, root)
+        .unOrdered()
+        .baselineColumns(""a"", ""b"")
+        .baselineValues(1, 1)
+        .go();
+      exceptionEncountered = false;","[{'comment': 'instead of the flag please use the `Assert.fail(""reason"")` method here.', 'commenter': 'vvysotskyi'}]"
2599,exec/java-exec/src/test/java/org/apache/drill/TestSetOp.java,"@@ -0,0 +1,1183 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill;
+
+import org.apache.drill.exec.planner.physical.PlannerSettings;
+import org.apache.drill.exec.record.BatchSchema;
+import org.apache.drill.exec.record.BatchSchemaBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;
+import org.apache.commons.lang3.tuple.Pair;
+import org.apache.drill.categories.OperatorTest;
+import org.apache.drill.categories.SqlTest;
+import org.apache.drill.categories.UnlikelyTest;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.nio.file.Paths;
+import java.util.List;
+
+import static org.junit.Assert.assertTrue;
+
+@Category({SqlTest.class, OperatorTest.class})
+public class TestSetOp extends ClusterTest {
+  private static final String EMPTY_DIR_NAME = ""empty_directory"";
+  private static final String SLICE_TARGET_DEFAULT = ""alter session reset `planner.slice_target`"";
+
+  @BeforeClass
+  public static void setupTestFiles() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""multilevel"", ""parquet""));
+    dirTestWatcher.makeTestTmpSubDir(Paths.get(EMPTY_DIR_NAME));
+  }
+
+  @Test
+  public void TestExceptionWithSchemaLessDataSource() {
+    boolean exceptionEncountered = true;
+    String root = ""/multilevel/csv/1994/Q1/orders_94_q1.csv"";
+    try {
+      testBuilder()
+        .sqlQuery(""select * from cp.`%s` intersect select * from cp.`%s`"", root, root)
+        .unOrdered()
+        .baselineColumns(""a"", ""b"")
+        .baselineValues(1, 1)
+        .go();
+      exceptionEncountered = false;
+    } catch (Exception ex) {
+      assertTrue(ex.getMessage(),","[{'comment': '`assertThat` + `containsString` will show more informative messages in the case of failures.', 'commenter': 'vvysotskyi'}]"
2609,pom.xml,"@@ -1984,17 +1983,6 @@
         <artifactId>xercesImpl</artifactId>
         <version>${xerces.version}</version>
       </dependency>
-      <dependency>","[{'comment': 'Note that what we\'re doing here is removing the _management_ of a dependency from the dependencyManagement element, not removing any dependency itself. If I look at a Drill installation then I see that we don\'t ship commons-configuration, only commons-configuration2.\r\n```\r\n➜  ~ ls /opt/apache-drill-1.20.1/jars/3rdparty/commons-conf*                    \r\n/opt/apache-drill-1.20.1/jars/3rdparty/commons-configuration2-2.1.1.jar\r\n```\r\nIf I then look at `mvn dependency:tree` I see that the Phoenix storage plugin is the one place where we depend on commons-configration via org.apache.phoenix:phoenix-core\r\n```\r\n[INFO] org.apache.drill.contrib:drill-storage-phoenix:jar:2.0.0-SNAPSHOT\r\n[INFO] \\- org.apache.phoenix:phoenix-core:jar:tests:5.1.2:test\r\n[INFO]    \\- org.apache.omid:omid-transaction-client:jar:1.0.2:test\r\n[INFO]       \\- commons-configuration:commons-configuration:jar:1.10:test\r\n```\r\nbut the scope of dependency is _test_ which explains why commons-configuration is not to be found in a Drill installation.\r\n\r\nBottom line: I don\'t think we do need to manage this dependency any more so I\'m in favour of this change even though it has no impact, because it simplifies our beast of a pom. But the Jira and the PR descriptions should be adjusted to reflect what\'s happening: ""Remove unneeded management of commons-configuration which only appears in test scope for storage-phoenix"" or something.\r\n\r\nLet\'s also check in with @vdiravka about this change...', 'commenter': 'jnturton'}, {'comment': 'This management is for two purposes:\r\n1. Per commit DRILL-7713 I understand the dependency was added to remove vulnerability from the transitive dependencies.\r\n2. To avoid using `commons-logging` as dependency. https://github.com/apache/commons-configuration/blob/master/pom.xml#L301\r\n\r\nIn case we are sure `commons-configuration` dependency is [1.10](https://github.com/apache/phoenix-omid/blob/ba43c8e1d73543fafa102c57af79516c4dc88860/pom.xml#L175) or newer version in Drill after removing this management and `commons-logging` is not used (successful mvn build is enough for this, because [commons-logging](https://github.com/apache/drill/blob/master/pom.xml#L663) is banned in Drill), we can remove this management.\r\nThe other question do we really need it?! It is possible in future the new dependency will have `commons-configuration` as a transitive dependency and we will face with the same issues, but now they are solved.', 'commenter': 'vdiravka'}, {'comment': ""Okay @pjfanning, based on @vdiravka's comments I think this dependency management is helping (to keep commons-logging out) and not hurting, so we should probably just leave it as it is."", 'commenter': 'jnturton'}]"
2618,contrib/storage-splunk/src/test/java/org/apache/drill/exec/store/splunk/TestSplunkUserTranslation.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.splunk;
+
+import org.apache.drill.categories.SlowTest;
+import org.apache.drill.common.config.DrillProperties;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.test.ClientFixture;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+
+import static org.apache.drill.exec.rpc.user.security.testing.UserAuthenticatorTestImpl.ADMIN_USER;
+import static org.apache.drill.exec.rpc.user.security.testing.UserAuthenticatorTestImpl.ADMIN_USER_PASSWORD;
+import static org.apache.drill.exec.rpc.user.security.testing.UserAuthenticatorTestImpl.TEST_USER_1;
+import static org.apache.drill.exec.rpc.user.security.testing.UserAuthenticatorTestImpl.TEST_USER_1_PASSWORD;
+import static org.junit.Assert.assertEquals;
+
+@Category({SlowTest.class})
+public class TestSplunkUserTranslation extends SplunkBaseTest {
+
+  @Test
+  public void testQueryWithMissingCredentials() throws Exception {
+    // This test validates that the correct credentials are sent down to Splunk.
+    // This user should not see the ut_splunk because they do not have valid credentials
+    ClientFixture client = cluster
+      .clientBuilder()
+      .property(DrillProperties.USER, ADMIN_USER)
+      .property(DrillProperties.PASSWORD, ADMIN_USER_PASSWORD)
+      .build();
+
+    String sql = ""SHOW DATABASES"";
+
+    RowSet results = client.queryBuilder().sql(sql).rowSet();
+    assertEquals(8, results.rowCount());","[{'comment': 'This magic number of 8 is a bit brittle. Can we replace it with a short search through the results to check for the absence of the Splunk plugin?\r\n\r\nAfterthought: ever read any Terry Pratchett? I think 8 _is_ the magic number in the Discworld :-)', 'commenter': 'jnturton'}, {'comment': ""I guess something like this would take care of it: `show databases where table_schema like 'ut_splunk.%'` and `assertEquals(0, results.rowCount());`"", 'commenter': 'jnturton'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2618,contrib/storage-splunk/src/main/java/org/apache/drill/exec/store/splunk/SplunkGroupScan.java,"@@ -50,17 +54,20 @@ public class SplunkGroupScan extends AbstractGroupScan {
 
   private int hashCode;
 
+  private MetadataProviderManager metadataProviderManager;","[{'comment': 'Can you elaborate on the role played by the metastore in this PR a bit, please? ', 'commenter': 'jnturton'}, {'comment': ""This actually isn't related to the metastore.  It is some boilerplate code to allow for provided schema.  You have to have  the MetadataProviderManager to access the inline schemata, which you then add to the schema negotiator in the ScanBatchCreator class.  This also is why I added all of the `getPhysicalScan()` functions to the `SplunkStoragePlugin` class.   With that said, this PR does not go all the way and add provided schema support.  I will do that as a separate PR.\r\n\r\nFor future storage plugins, we should make that a standard, as it really isn't difficult to add. "", 'commenter': 'cgivre'}]"
2618,contrib/storage-splunk/src/test/java/org/apache/drill/exec/store/splunk/TestSplunkUserTranslation.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.splunk;
+
+import org.apache.drill.categories.SlowTest;
+import org.apache.drill.common.config.DrillProperties;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.test.ClientFixture;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+
+import static org.apache.drill.exec.rpc.user.security.testing.UserAuthenticatorTestImpl.ADMIN_USER;
+import static org.apache.drill.exec.rpc.user.security.testing.UserAuthenticatorTestImpl.ADMIN_USER_PASSWORD;
+import static org.apache.drill.exec.rpc.user.security.testing.UserAuthenticatorTestImpl.TEST_USER_1;
+import static org.apache.drill.exec.rpc.user.security.testing.UserAuthenticatorTestImpl.TEST_USER_1_PASSWORD;
+import static org.junit.Assert.assertEquals;
+
+@Category({SlowTest.class})
+public class TestSplunkUserTranslation extends SplunkBaseTest {
+
+  @Test
+  public void testQueryWithMissingCredentials() throws Exception {
+    // This test validates that the correct credentials are sent down to Splunk.
+    // This user should not see the ut_splunk because they do not have valid credentials
+    ClientFixture client = cluster
+      .clientBuilder()
+      .property(DrillProperties.USER, ADMIN_USER)
+      .property(DrillProperties.PASSWORD, ADMIN_USER_PASSWORD)
+      .build();
+
+    String sql = ""SHOW DATABASES"";
+
+    RowSet results = client.queryBuilder().sql(sql).rowSet();
+    assertEquals(8, results.rowCount());
+  }
+
+  @Test
+  public void testQueryWithValidCredentials() throws Exception {
+    ClientFixture client = cluster
+      .clientBuilder()
+      .property(DrillProperties.USER, TEST_USER_1)
+      .property(DrillProperties.PASSWORD, TEST_USER_1_PASSWORD)
+      .build();
+
+    String sql = ""SHOW DATABASES"";
+
+    RowSet results = client.queryBuilder().sql(sql).rowSet();
+    assertEquals(9, results.rowCount());","[{'comment': 'Restrict this count to schemas registered by _this_ plugin using a LIKE filter in the SHOW DATABASES, or similar.', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2622,contrib/storage-phoenix/README.md,"@@ -83,11 +83,11 @@ Tips :
 Configurations :
 1. Enable [Drill User Impersonation](https://drill.apache.org/docs/configuring-user-impersonation/)
 2. Enable [PQS Impersonation](https://phoenix.apache.org/server.html#Impersonation)
-3. PQS URL:
-  1. Provide `host` and `port` and Drill will generate the PQS URL with a doAs parameter of current session user
-  2. Provide the `jdbcURL` with a `doAs` url param and `$user` placeholder as a value, for instance:
-     `jdbc:phoenix:thin:url=http://localhost:8765?doAs=$user`. In case Drill Impersonation is enabled, but `doAs=$user`
-     is missing the User Exception is thrown.
+3. Phoenix URL:
+  1. Provide `zkQUorum` and `port` and Drill will create a connection to Phoenix with a doAs of current ","[{'comment': '```suggestion\r\n  1. Provide `zkQuorum` and `port` and Drill will create a connection to Phoenix with a doAs of current \r\n```', 'commenter': 'jnturton'}]"
2622,contrib/storage-phoenix/pom.xml,"@@ -353,6 +381,9 @@
   <profiles>
     <profile>
       <id>hadoop-2</id>
+      <properties>
+        <phoenix.skip.tests>true</phoenix.skip.tests>","[{'comment': 'Is the Phoenix thick driver only compatible with Hadoop 3?', 'commenter': 'jnturton'}, {'comment': 'See here: https://phoenix.apache.org/faq.html\r\n\r\n```\r\nPhoenix 4.x supports HBase 1.x running on Hadoop 2\r\n\r\nPhoenix 5.x supports HBase 2.x running on Hadoop 3\r\n```', 'commenter': 'cgivre'}, {'comment': 'For some reason CI tests fail with hadoop-2 profile, but pass fine locally. Seems like some issue with MiniKdc.', 'commenter': 'vvysotskyi'}]"
2622,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixDataSource.java,"@@ -138,30 +137,37 @@ public boolean isWrapperFor(Class<?> iface) {
 
   @Override
   public Connection getConnection() throws SQLException {
-    useDriverClass();
+    loadDriverClass();
     return getConnection(this.user, null);
   }
 
   @Override
   public Connection getConnection(String userName, String password) throws SQLException {
-    useDriverClass();
+    loadDriverClass();
     logger.debug(""Drill/Phoenix connection url: {}"", url);
-    return DriverManager.getConnection(url, useConfProperties());
+    CheckedSupplier<Connection, SQLException> action =
+      () -> DriverManager.getConnection(url, useConfProperties());
+    if (impersonationEnabled) {
+      return doAsRemoteUser(userName, action);
+    }
+    return action.getAndThrow();
+  }
+
+  private <T> T doAsRemoteUser(String remoteUserName, final Supplier<T> action) {
+    try {
+      UserGroupInformation proxyUser = ImpersonationUtil.createProxyUgi(remoteUserName);
+      return proxyUser.doAs((PrivilegedExceptionAction<T>) action::get);
+    } catch (Exception e) {
+      throw new DrillRuntimeException(e);
+    }
   }
 
   /**
-   * The thin-client is lightweight and better compatibility.
-   * Only thin-client is currently supported.
-   *
-   * @throws SQLException
+   * Only thick-client is currently supported doe to shaded Avatica issues with thin client.","[{'comment': '```suggestion\r\n   * Only thick-client is currently supported due to a shaded Avatica conflict created by the thin client.\r\n```', 'commenter': 'jnturton'}]"
2622,contrib/storage-phoenix/src/main/java/org/apache/drill/exec/store/phoenix/PhoenixDataSource.java,"@@ -18,40 +18,42 @@
 package org.apache.drill.exec.store.phoenix;
 
 import java.io.PrintWriter;
+import java.security.PrivilegedExceptionAction;
 import java.sql.Connection;
 import java.sql.DriverManager;
 import java.sql.SQLException;
 import java.util.Map;
+import java.util.Optional;
 import java.util.Properties;
+import java.util.function.Supplier;
 import java.util.logging.Logger;
 
 import javax.sql.DataSource;
 
-import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.drill.common.util.function.CheckedSupplier;
+import org.apache.drill.exec.util.ImpersonationUtil;
 import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+import org.apache.hadoop.security.UserGroupInformation;
 import org.slf4j.LoggerFactory;
 
 /**
  * Phoenix’s Connection objects are different from most other JDBC Connections
  * due to the underlying HBase connection. The Phoenix Connection object
  * is designed to be a thin object that is inexpensive to create.
- *
+ * <p>
  * If Phoenix Connections are reused, it is possible that the underlying HBase connection
  * is not always left in a healthy state by the previous user. It is better to
  * create new Phoenix Connections to ensure that you avoid any potential issues.","[{'comment': ""Reviewer's note: I checked the Phoenix FAQ and believe that these comments remain true of the thick driver."", 'commenter': 'jnturton'}]"
2633,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpPaginatorConfig.java,"@@ -137,21 +162,28 @@ public String toString() {
       .field(""pageSize"", pageSize)
       .field(""maxRecords"", maxRecords)
       .field(""method"", method)
+      .field(""indexParam"", indexParam)
+      .field(""hasMoreParam"", hasMoreParam)
+      .field(""nextPageParam"", nextPageParam)
       .toString();
   }
 
   public enum PaginatorMethod {
     OFFSET,
-    PAGE
+    PAGE,
+    INDEX
   }
 
-  private HttpPaginatorConfig(HttpPaginatorConfig.HttpPaginatorBuilder builder) {
+  /*private HttpPaginatorConfig(HttpPaginatorConfig.HttpPaginatorConfigBuilder builder) {","[{'comment': 'Is this commented out code meant to be included?', 'commenter': 'jnturton'}, {'comment': 'Oops... Fixed.', 'commenter': 'cgivre'}]"
2633,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/parser/SimpleMessageParser.java,"@@ -66,11 +68,13 @@
 public class SimpleMessageParser implements MessageParser {
 
   private final String[] path;
+  private final Map<String, Object> paginationFields;
 
-  public SimpleMessageParser(String dataPath) {
+  public SimpleMessageParser(String dataPath, Map<String, Object> paginationFields) {","[{'comment': 'Can we rename ""pagination"" here too?', 'commenter': 'jnturton'}, {'comment': 'Done!', 'commenter': 'cgivre'}]"
2633,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/values/ScalarListener.java,"@@ -76,4 +79,33 @@ protected void setArrayNull() {
   protected UserException typeConversionError(String jsonType) {
     return loader.typeConversionError(schema(), jsonType);
   }
+
+  /**
+   * Adds a field's most recent value to the pagination map.  This is necessary for the HTTP plugin
+   * for index or keyset pagination where the API transmits values in the results that are used to
+   * generate the next page.
+   *
+   * This data is only stored if the pagination map is defined, and has keys.","[{'comment': 'Can this be rewritten in terms of generic column listeners rather than pagination and the HTTP plugin?', 'commenter': 'jnturton'}, {'comment': 'Done!', 'commenter': 'cgivre'}]"
2633,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/loader/TupleParser.java,"@@ -127,10 +127,19 @@ public TupleParser(JsonLoaderImpl loader, TupleWriter tupleWriter, TupleMetadata
 
   @Override
   public ElementParser onField(String key, TokenIterator tokenizer) {
-    if (!tupleWriter.isProjected(key)) {
+    if (projectField(key)) {
+      return fieldParserFor(key, tokenizer);
+    } else {
       return fieldFactory().ignoredFieldParser();
+    }
+  }
+
+  private boolean projectField(String key) {
+    // This method makes sure that fields necessary for pagination are read.","[{'comment': '```suggestion\r\n    // This method makes sure that fields necessary for column listeners are read.\r\n```', 'commenter': 'jnturton'}]"
2633,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/values/ScalarListener.java,"@@ -76,4 +79,30 @@ protected void setArrayNull() {
   protected UserException typeConversionError(String jsonType) {
     return loader.typeConversionError(schema(), jsonType);
   }
+
+  /**
+   * Adds a field's most recent value to the column listener map.
+   * This data is only stored if the listener column map is defined, and has keys.
+   * @param key The key of the pagination field
+   * @param value The value of to be retained
+   */
+  protected void addValueToListenerMap(String key, String value) {
+    Map<String,Object> listenerColumnMap = loader.listenerColumnMap();
+
+    if (listenerColumnMap == null || listenerColumnMap.isEmpty()) {
+      return;
+    } else if (listenerColumnMap.containsKey(key) && StringUtils.isNotEmpty(value)) {
+      listenerColumnMap.put(key, value);
+    }
+  }
+
+  protected void addValueToListenerMap(String key, Object value) {
+    Map<String, Object> paginationMap = loader.listenerColumnMap();","[{'comment': '```suggestion\r\n    Map<String, Object> listenerMap = loader.listenerColumnMap();\r\n```', 'commenter': 'jnturton'}]"
2633,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/parser/SimpleMessageParser.java,"@@ -129,6 +135,44 @@ private boolean parseInnerLevel(TokenIterator tokenizer, int level) throws Messa
     return parseToElement(tokenizer, level + 1);
   }
 
+  /**
+   * This function is called when a storage plugin needs to retrieve values which have been read.  This logic
+   * enables use of the data path in these situations.  Normally, when the datapath is defined, the JSON reader
+   * will ""free-wheel"" over unprojected columns or columns outside of the datapath.  However, in this case, often
+   * the values which are being read, are outside the dataPath.  This logic offers a way to capture these values
+   * without creating a ValueVector for them.
+   *
+   * @param tokenizer A {@link TokenIterator} of the parsed JSON data.
+   * @param fieldName A {@link String} of the pagination field name.","[{'comment': '```suggestion\r\n   * @param fieldName A {@link String} of the listener column name.\r\n```', 'commenter': 'jnturton'}]"
2633,exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/json/values/ScalarListener.java,"@@ -76,4 +79,30 @@ protected void setArrayNull() {
   protected UserException typeConversionError(String jsonType) {
     return loader.typeConversionError(schema(), jsonType);
   }
+
+  /**
+   * Adds a field's most recent value to the column listener map.
+   * This data is only stored if the listener column map is defined, and has keys.
+   * @param key The key of the pagination field","[{'comment': '```suggestion\r\n   * @param key The key of the listener field\r\n```', 'commenter': 'jnturton'}]"
2636,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java,"@@ -645,10 +646,18 @@ public Void visitOp(PhysicalOperator op, Collection<PhysicalOperator> collection
       }
       return null;
     }
-
   }
 
   protected Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException, RelConversionException, ForemanSetupException {
+    if (context.getOptions().getOption(ExecConstants.FILE_LISTING_LIMIT0_OPT)) {
+      // Check for a LIMIT 0 in the root portion of the query before validation
+      // because validation of the query's FROM clauses will already trigger
+      // the recursive listing files to which FILE_LISTING_LIMIT0_OPT is meant
+      // to apply.
+      boolean rootSelectLimit0 = FindLimit0SqlVisitor.containsLimit0(sqlNode);
+      context.getPlannerSettings().setRootSelectLimit0(rootSelectLimit0);","[{'comment': 'Perhaps with the query option would be simpler to pass the value rather than extend the planner setting interface and pass it through. And schema config already has access to query options.', 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi I've made use of a query option, how do things look now?"", 'commenter': 'jnturton'}]"
2646,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlWorker.java,"@@ -219,11 +220,14 @@ private static PhysicalPlan getQueryPlan(QueryContext context, String sql, Point
           handler = new SetOptionHandler(context);","[{'comment': ""It looks like this case could be merged with default case on line 227. But then the code would be less explicit so I'm not requesting a change, just making a note."", 'commenter': 'jnturton'}, {'comment': 'Thanks, fixed it.', 'commenter': 'vvysotskyi'}]"
2646,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/InsertHandler.java,"@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.planner.sql.handlers;
+
+import org.apache.calcite.sql.SqlNode;
+import org.apache.calcite.tools.RelConversionException;
+import org.apache.calcite.tools.ValidationException;
+import org.apache.drill.common.exceptions.DrillRuntimeException;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.exec.planner.sql.SchemaUtilites;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.util.Pointer;
+import org.apache.drill.exec.work.foreman.ForemanSetupException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Constructs plan to be executed for collecting metadata and storing it to the Metastore.","[{'comment': ""I don't think this comment belongs here."", 'commenter': 'jnturton'}, {'comment': 'Oh, thanks, fixed.', 'commenter': 'vvysotskyi'}]"
2646,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/utils/CreateTableStmtBuilder.java,"@@ -18,126 +18,130 @@
 
 package org.apache.drill.exec.store.jdbc.utils;
 
+import org.apache.calcite.schema.ColumnStrategy;
+import org.apache.calcite.sql.SqlBasicTypeNameSpec;
+import org.apache.calcite.sql.SqlDataTypeSpec;
 import org.apache.calcite.sql.SqlDialect;
+import org.apache.calcite.sql.SqlIdentifier;
+import org.apache.calcite.sql.SqlNode;
+import org.apache.calcite.sql.SqlNodeList;
+import org.apache.calcite.sql.ddl.SqlDdlNodes;
 import org.apache.calcite.sql.dialect.PostgresqlSqlDialect;
+import org.apache.calcite.sql.parser.SqlParserPos;
+import org.apache.calcite.sql.type.SqlTypeName;
+import org.apache.commons.collections4.CollectionUtils;
 import org.apache.drill.common.exceptions.UserException;
-import org.apache.drill.common.types.TypeProtos.MinorType;
-import org.apache.drill.exec.store.jdbc.JdbcRecordWriter;
-import org.apache.parquet.Strings;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.Types;
+import org.apache.drill.exec.record.MaterializedField;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.sql.JDBCType;
+import java.util.List;
 
 public class CreateTableStmtBuilder {","[{'comment': 'Thanks for all the refactoring and clean up in this area.', 'commenter': 'jnturton'}]"
2646,exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/TableModify.java,"@@ -0,0 +1,56 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.physical.config;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.exec.physical.base.AbstractSingle;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.record.BatchSchema;
+
+@JsonTypeName(""table_modify"")
+public class TableModify extends AbstractSingle {","[{'comment': 'Is this operator called table_modify rather than, say, table_insert because it might be applied for other operations e.g. an upsert? ', 'commenter': 'jnturton'}, {'comment': 'Oh I see, the name table_modify comes from Calcite.', 'commenter': 'jnturton'}]"
2646,exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/generators/NonCoveringIndexPlanGenerator.java,"@@ -188,7 +188,7 @@ public RelNode convertChild(final RelNode topRel, final RelNode input) throws In
     if (indexDesc.getCollation() != null &&
          !settings.isIndexForceSortNonCovering()) {
       collation = IndexPlanUtils.buildCollationNonCoveringIndexScan(indexDesc, indexScanRowType, dbscanRowType, indexContext);
-      if (restrictedScanTraitSet.contains(RelCollationTraitDef.INSTANCE)) { // replace existing trait
+      if (restrictedScanTraitSet.getTrait(RelCollationTraitDef.INSTANCE) != null) { // replace existing trait","[{'comment': ""I'm curious about the benefit of rewriting this test this way..."", 'commenter': 'jnturton'}, {'comment': ""The existing code wouldn't work as expected, trait set doesn't contain RelCollationTraitDef instances."", 'commenter': 'vvysotskyi'}]"
2646,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/DrillJdbcConvention.java,"@@ -98,4 +98,31 @@ public Set<RelOptRule> getRules() {
   public JdbcStoragePlugin getPlugin() {
     return plugin;
   }
+
+  private static List<RelOptRule> rules(
+    RelTrait inputTrait, JdbcConvention out) {
+    ImmutableList.Builder<RelOptRule> b = ImmutableList.builder();
+    foreachRule(out, r ->
+      b.add(r.config
+        .as(ConverterRule.Config.class)
+        .withConversion(r.getOperand().getMatchedClass(), inputTrait, out, r.config.description())
+        .withRelBuilderFactory(DrillRelFactories.LOGICAL_BUILDER)
+        .toRule()));
+    return b.build();
+  }
+
+  private static void foreachRule(JdbcConvention out,","[{'comment': ""Could you give a short explanation of how this change relates to adding INSERT support? I can't trace it."", 'commenter': 'jnturton'}, {'comment': 'It is related not only to INSERT but to general JDBC functionality. With this change, rules will match not only to the `NONE` convention but also to the `DRILL_LOGICAL` one, so as a result, Drill will be able to push down more operators for complex queries, where possible.', 'commenter': 'vvysotskyi'}]"
2655,exec/java-exec/src/main/java/org/apache/calcite/jdbc/DynamicRootSchema.java,"@@ -91,22 +91,69 @@ private CalciteSchema getSchema(String schemaName, boolean caseSensitive) {
 
   public SchemaPath resolveTableAlias(String alias) {
     return Optional.ofNullable(aliasRegistryProvider.getTableAliasesRegistry()
-        .getUserAliases(schemaConfig.getUserName()).get(alias))
+      .getUserAliases(schemaConfig.getUserName()).get(alias))
       .map(SchemaPath::parseFromString)
       .orElse(null);
   }
 
+  private void attemptToRegisterSchemas(StoragePlugin plugin) throws Exception {
+    long maxAttempts = schemaConfig
+      .getOption(ExecConstants.STORAGE_PLUGIN_ACCESS_ATTEMPTS)
+      .num_val;
+    long attemptDelayMs = schemaConfig
+      .getOption(ExecConstants.STORAGE_PLUGIN_ATTEMPT_DELAY)
+      .num_val;
+    int attempt=0;
+    Exception lastAttemptEx = null;
+
+    while (attempt++ < maxAttempts) {","[{'comment': 'Could we either move retry-related logic to a utility method or use some library that does that, for example, https://github.com/failsafe-lib/failsafe?', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi let me know if you still prefer the use of a library for the retry logic.', 'commenter': 'jnturton'}]"
2655,exec/java-exec/src/main/java/org/apache/calcite/jdbc/DynamicRootSchema.java,"@@ -91,22 +91,69 @@ private CalciteSchema getSchema(String schemaName, boolean caseSensitive) {
 
   public SchemaPath resolveTableAlias(String alias) {
     return Optional.ofNullable(aliasRegistryProvider.getTableAliasesRegistry()
-        .getUserAliases(schemaConfig.getUserName()).get(alias))
+      .getUserAliases(schemaConfig.getUserName()).get(alias))
       .map(SchemaPath::parseFromString)
       .orElse(null);
   }
 
+  private void attemptToRegisterSchemas(StoragePlugin plugin) throws Exception {","[{'comment': 'Please rename the method to something like register with retry.', 'commenter': 'vvysotskyi'}]"
2655,exec/java-exec/src/main/java/org/apache/calcite/jdbc/DynamicRootSchema.java,"@@ -91,22 +91,69 @@ private CalciteSchema getSchema(String schemaName, boolean caseSensitive) {
 
   public SchemaPath resolveTableAlias(String alias) {
     return Optional.ofNullable(aliasRegistryProvider.getTableAliasesRegistry()
-        .getUserAliases(schemaConfig.getUserName()).get(alias))
+      .getUserAliases(schemaConfig.getUserName()).get(alias))
       .map(SchemaPath::parseFromString)
       .orElse(null);
   }
 
+  private void attemptToRegisterSchemas(StoragePlugin plugin) throws Exception {
+    long maxAttempts = schemaConfig
+      .getOption(ExecConstants.STORAGE_PLUGIN_ACCESS_ATTEMPTS)
+      .num_val;
+    long attemptDelayMs = schemaConfig
+      .getOption(ExecConstants.STORAGE_PLUGIN_ATTEMPT_DELAY)
+      .num_val;
+    int attempt=0;
+    Exception lastAttemptEx = null;
+
+    while (attempt++ < maxAttempts) {
+      try {
+        plugin.registerSchemas(schemaConfig, plus());
+        return;
+      } catch (Exception ex) {
+        lastAttemptEx = ex;
+        logger.warn(
+          ""Attempt {} of {} to register schemas for plugin {} failed."",
+          attempt, maxAttempts, plugin,
+          ex
+        );
+
+        if (attempt < maxAttempts) {
+          logger.info(
+            ""Next attempt to register schemas for plugin {} will be made in {}ms."",
+            plugin,
+            attemptDelayMs
+          );
+          try {
+            Thread.sleep(attemptDelayMs);
+          } catch (InterruptedException intEx) {
+            logger.warn(
+              ""Interrupted while waiting to make another attempt to register "" +
+              ""chemas for plugin {}."",
+              plugin,
+              intEx
+            );
+          }
+        }
+      }
+    }
+
+    throw lastAttemptEx;
+  }
+
   /**
    * Loads schema factory(storage plugin) for specified {@code schemaName}
    * @param schemaName the name of the schema
    * @param caseSensitive whether matching for the schema name is case sensitive
    */
   private void loadSchemaFactory(String schemaName, boolean caseSensitive) {
+    StoragePlugin plugin = null;
     try {
       SchemaPlus schemaPlus = this.plus();
-      StoragePlugin plugin = storages.getPlugin(schemaName);
+      plugin = storages.getPlugin(schemaName);
       if (plugin != null) {
-        plugin.registerSchemas(schemaConfig, schemaPlus);
+        attemptToRegisterSchemas(plugin);
+        // plugin.registerSchemas(schemaConfig, schemaPlus);","[{'comment': 'Please delete the commented line.', 'commenter': 'vvysotskyi'}]"
2655,exec/java-exec/src/main/java/org/apache/calcite/jdbc/DynamicRootSchema.java,"@@ -122,7 +169,8 @@ private void loadSchemaFactory(String schemaName, boolean caseSensitive) {
         SchemaPlus firstLevelSchema = schemaPlus.getSubSchema(paths.get(0));
         if (firstLevelSchema == null) {
           // register schema for this storage plugin to 'this'.
-          plugin.registerSchemas(schemaConfig, schemaPlus);
+          attemptToRegisterSchemas(plugin);
+          //plugin.registerSchemas(schemaConfig, schemaPlus);","[{'comment': 'And here also.', 'commenter': 'vvysotskyi'}]"
2655,exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java,"@@ -1094,6 +1095,35 @@ public static String bootDefaultFor(String name) {
       new OptionDescription(""Enables recursive files listing when querying the `INFORMATION_SCHEMA.FILES` table or executing the SHOW FILES command. "" +
         ""Default is false. (Drill 1.15+)""));
 
+  public static final String STORAGE_PLUGIN_ACCESS_ATTEMPTS = ""storage.plugin_access_attempts"";
+  public static final PositiveLongValidator STORAGE_PLUGIN_ACCESS_ATTEMPTS_VALIDATOR = new PositiveLongValidator(
+    STORAGE_PLUGIN_ACCESS_ATTEMPTS,
+    10,
+    new OptionDescription(
+      ""The maximum number of attempts that will be made to request metadata "" +
+      ""needed for query planning from a storage plugin.""
+    )
+  );
+  public static final String STORAGE_PLUGIN_ATTEMPT_DELAY = ""storage.plugin_access_attempt_delay"";
+  public static final NonNegativeLongValidator STORAGE_PLUGIN_ATTEMPT_DELAY_VALIDATOR = new NonNegativeLongValidator(
+    STORAGE_PLUGIN_ATTEMPT_DELAY,
+    5*60*1000,","[{'comment': '5 minutes is too long a delay. Please set it to 3 or 5 seconds, since the user will be waiting for its execution. Also please format the code properly.', 'commenter': 'vvysotskyi'}, {'comment': 'Done. I asked IDEA to reformat the code and it indented the subsequent lines in the string additions by one level. Let me know if you meant a bigger reformatting.', 'commenter': 'jnturton'}, {'comment': 'I meant to add missing spaces.', 'commenter': 'vvysotskyi'}]"
2655,exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java,"@@ -1094,6 +1095,35 @@ public static String bootDefaultFor(String name) {
       new OptionDescription(""Enables recursive files listing when querying the `INFORMATION_SCHEMA.FILES` table or executing the SHOW FILES command. "" +
         ""Default is false. (Drill 1.15+)""));
 
+  public static final String STORAGE_PLUGIN_ACCESS_ATTEMPTS = ""storage.plugin_access_attempts"";
+  public static final PositiveLongValidator STORAGE_PLUGIN_ACCESS_ATTEMPTS_VALIDATOR = new PositiveLongValidator(
+    STORAGE_PLUGIN_ACCESS_ATTEMPTS,
+    10,
+    new OptionDescription(
+      ""The maximum number of attempts that will be made to request metadata "" +
+      ""needed for query planning from a storage plugin.""
+    )
+  );
+  public static final String STORAGE_PLUGIN_ATTEMPT_DELAY = ""storage.plugin_access_attempt_delay"";
+  public static final NonNegativeLongValidator STORAGE_PLUGIN_ATTEMPT_DELAY_VALIDATOR = new NonNegativeLongValidator(
+    STORAGE_PLUGIN_ATTEMPT_DELAY,
+    5*60*1000,
+    new OptionDescription(
+      ""The delay in milliseconds between repeated attempts to request metadata "" +","[{'comment': 'Please mention here and in the option above that they will be used only when `storage.plugin_auto_disable` is set to true.', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi I did implement this that way initially but if you look I ended up making the two features - storage plugin retry and storage plugin auto disable - independent of each other here. Users could choose to have retry and no auto disable, or to have auto disable and no retry the way it is currently.', 'commenter': 'jnturton'}]"
2655,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlWorker.java,"@@ -130,11 +132,18 @@ private static PhysicalPlan convertPlan(QueryContext context, String sql, Pointe
       logger.trace(""There was an error during conversion into physical plan. "" +
           ""Will sync remote and local function registries if needed and retry "" +
           ""in case if issue was due to missing function implementation."", e);
-      // it is prohibited to retry query planning for ANALYZE statement since it changes
-      // query-level option values and will fail when rerunning with updated values
-      if (context.getFunctionRegistry().syncWithRemoteRegistry(
-              context.getDrillOperatorTable().getFunctionRegistryVersion())
-        && context.getSQLStatementType() != SqlStatementType.ANALYZE) {
+
+      int funcRegVer = context.getDrillOperatorTable().getFunctionRegistryVersion();
+      // We do not retry conversion if the error is a UserException of type RESOURCE
+      boolean isResourceErr = e instanceof UserException && ((UserException) e).getErrorType() == RESOURCE;","[{'comment': ""I don't think that it wouldn't break the initial intention for retrying the query after updating the function registry. It is possible that some part of the code wraps exceptions caused by absent functions as `UserException` with `RESOURCE` type."", 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi when plugin auto disabling is switched on then this second attempt to convert the query involving a broken plugin fails with an error of ""schema not found"" because the plugin has been disabled. This error isn\'t as informative to the user as the original error raised when attempting to access the plugin so I didn\'t want this to be the error that they get back...', 'commenter': 'jnturton'}, {'comment': ""Maybe after plugin auto disabling has happened I should set a flag somewhere like QueryContext to record that it has happened. Then this code doesn't need to try to tell what happened from the exception type, it can look at the flag and see that it should not try to sync the function registry and convert the query again because a needed plugin was broken and is now disabled?"", 'commenter': 'jnturton'}, {'comment': 'Yes, it would be a more reliable approach. Initially, I was looking for a way how to pass it properly, one of the options is to use the query option, but it will be tricky to set it in the place where plugins could be disabled with this feature.', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi do you think that some new variable in UserException or UserExceptionContext which indicates that a plugin was disabled as a result of the underlying error would be preferable? This exception is visible in all of the needed places and seems like a pretty natural home for this information...', 'commenter': 'jnturton'}]"
2655,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerConfig.java,"@@ -52,12 +59,81 @@ public QueryContext getContext() {
     return context;
   }
 
+  private RuleSet attemptToGetRules(PlannerPhase phase, Collection<StoragePlugin> plugins) throws PluginException{","[{'comment': ""I don't think that we should add retry logic here. Plugins will be checked during previous stages, at least during query validation."", 'commenter': 'vvysotskyi'}, {'comment': ""@vvysotskyi so do we keep auto disabling but drop retry here? We've seen JDBC plugins that point to an unresponsive remote DB fail with the first exception coming from rule collection (because, unlike most plugins, JDBC already tries to contact the remote DB for this operation). And of course before DRILL-8234, JDBC plugins not even involved in a query could cause it to fail during rule collection. "", 'commenter': 'jnturton'}, {'comment': 'I meant to drop both retry and auto-disabling logic. I have checked JDBC plugin, and it has several places where the data source is created. One of them is `getOptimizerRules` which is used here, and another one is `registerSchemas`, which is called during validation, before this place. So if we have something broken, it should fail before calling this method.', 'commenter': 'vvysotskyi'}]"
2655,exec/java-exec/src/main/java/org/apache/drill/exec/store/StoragePluginRegistry.java,"@@ -34,12 +34,22 @@ public interface StoragePluginRegistry extends Iterable<Map.Entry<String, Storag
 
   @SuppressWarnings(""serial"")
   public static class PluginException extends Exception {
+
+    public final StoragePlugin plugin;","[{'comment': 'Is there any particular reason for having a plugin in the exception class?', 'commenter': 'vvysotskyi'}, {'comment': '@vvysotskyi I added a reference to the plugin related to the exception so that when an exception is thrown from PlannerPhase#getStorageRules the calling code has a way to find _which_ plugin from the collection that it passed in was the one that failed. It needs to know that if it is to go on to disable that plugin. But there must be other ways of sharing that data with the caller if you think using the exception object is not a good choice...', 'commenter': 'jnturton'}, {'comment': 'Looks like it is not used now', 'commenter': 'vvysotskyi'}]"
2655,contrib/storage-jdbc/src/main/java/org/apache/drill/exec/store/jdbc/JdbcStoragePlugin.java,"@@ -108,19 +109,22 @@ public Optional<DataSource> getDataSource(UserCredentials userCredentials) {
     ));
   }
 
-  public SqlDialect getDialect(DataSource dataSource) {
-    return JdbcSchema.createDialect(
-      SqlDialectFactoryImpl.INSTANCE,
-      dataSource
-    );
+  public synchronized SqlDialect getDialect(UserCredentials userCredentials) {
+    if (sqlDialect == null) {","[{'comment': 'Please use double-check to avoid extra overhead caused by synchronization for most of the calls when the  `sqlDialect` is already created.', 'commenter': 'vvysotskyi'}]"
2660,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpBatchReader.java,"@@ -66,7 +67,7 @@ public class HttpBatchReader implements ManagedReader<SchemaNegotiator> {
   private final int maxRecords;
   protected final Paginator paginator;
   protected String baseUrl;
-  private JsonLoader jsonLoader;
+  private JsonLoaderImpl jsonLoader;","[{'comment': ""I hadn't noticed that we'd already started depending on JsonLoaderImpl. Is something preventing working with the JsonLoader interface?"", 'commenter': 'jnturton'}]"
2667,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/GoogleSheetsBatchReader.java,"@@ -90,7 +90,7 @@ public GoogleSheetsBatchReader(GoogleSheetsStoragePluginConfig config, GoogleShe
     this.sheetID = subScan.getScanSpec().getSheetID();
     this.sheetNames = new ArrayList<>();
     try {
-      List<Sheet> sheetList = GoogleSheetsUtils.getSheetList(service, sheetID);
+      List<Sheet> sheetList = GoogleSheetsUtils.getTabList(service, sheetID);","[{'comment': '```suggestion\r\n      List<Sheet> tabList = GoogleSheetsUtils.getTabList(service, sheetID);\r\n```\r\nand elsewhere.', 'commenter': 'jnturton'}]"
2667,contrib/storage-googlesheets/src/test/java/org/apache/drill/exec/store/googlesheets/TestGoogleSheetsQueries.java,"@@ -133,6 +134,34 @@ public void testStarQuery() throws Exception {
     new RowSetComparison(expected).verifyAndClearAll(results);
   }
 
+  @Test
+  public void testSchemataInformationSchema() throws Exception {
+    try {
+      initializeTokens(""googlesheets"");
+    } catch (PluginException e) {
+      fail(e.getMessage());
+    }
+    // Makes sure that the root level plugin shows up in the information schema
+    String sql = ""SELECT * FROM `INFORMATION_SCHEMA`.`SCHEMATA` WHERE SCHEMA_NAME LIKE 'googlesheets%'"";","[{'comment': '```suggestion\r\n    String sql = ""SELECT * FROM `INFORMATION_SCHEMA`.`SCHEMATA` WHERE SCHEMA_NAME LIKE \'googlesheets.%\'"";\r\n```\r\nand elsewhere. There is no longer a bug preventing the use of a period character to make these SQL LIKE patterns unambiguous.', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2667,contrib/storage-googlesheets/src/main/java/org/apache/drill/exec/store/googlesheets/GoogleSheetsStoragePlugin.java,"@@ -61,6 +62,7 @@ public class GoogleSheetsStoragePlugin extends AbstractStoragePlugin {
   private final OAuthTokenProvider tokenProvider;
   private DataStore<StoredCredential> dataStore;
   private Sheets service;
+  private Drive driveService;","[{'comment': 'Do these need to be closed at some point?', 'commenter': 'jnturton'}, {'comment': ""If you're happy that we're closing all the stuff we need to be then I think we're done here..."", 'commenter': 'jnturton'}, {'comment': ""@jnturton I don't think they need to be closed.  These objects are really more like containers for the various bits that go into calling the Google APIs.  They don't actually open any input streams, at least I don't think they do.  You can see that when we actually pull data from Google, there is some convoluted call which ends in `execute`.  That makes the actual HTTP call. "", 'commenter': 'cgivre'}, {'comment': 'Let me verify that before we merge this.', 'commenter': 'cgivre'}, {'comment': ""Confirmed.  The objects in question don't have a `close` method."", 'commenter': 'cgivre'}]"
2668,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/util/SimpleHttp.java,"@@ -870,6 +928,22 @@ public static HttpStoragePlugin getStoragePlugin(DrillbitContext context, String
     }
   }
 
+  public static String addBackTicksToAliasName(String plugin) {","[{'comment': 'The identifier quoting character in Drill is configurable and not necessarily a backtick. See  https://drill.apache.org/docs/lexical-structure/#identifier-quotes. Is there any existing code for unwrapping and wrapping identifiers like this that could be reused?', 'commenter': 'jnturton'}, {'comment': 'I fixed this to use the planner option. ', 'commenter': 'cgivre'}]"
2668,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/util/SimpleHttp.java,"@@ -832,8 +835,36 @@ public static List<String> buildParameterList(NullableVarCharHolder[] inputReade
     return inputArguments;
   }
 
-  public static HttpApiConfig getEndpointConfig(String endpoint, HttpStoragePluginConfig pluginConfig) {
-    HttpApiConfig endpointConfig = pluginConfig.getConnection(endpoint);
+  /**
+   * This function is used to obtain the configuration information for a given API in the HTTP UDF.
+   * If aliasing is enabled, this function will resolve aliases for connections.
+   * @param endpoint The name of the endpoint.  Should be a {@link String}
+   * @param context The {@link DrillbitContext} from the current query.
+   * @param info {@link ContextInformation} from the current query.
+   * @param pluginConfig The {@link HttpStoragePluginConfig} the configuration from the plugin
+   * @return The {@link HttpApiConfig} corresponding with the endpoint.
+   */
+  public static HttpApiConfig getEndpointConfig(String endpoint,
+                                                DrillbitContext context,
+                                                ContextInformation info,
+                                                HttpStoragePluginConfig pluginConfig) {
+    String queryUser = info.getQueryUser();
+    AliasRegistryProvider aliasRegistryProvider = context.getAliasRegistryProvider();","[{'comment': ""It would be nice to get @vvysotskyi's feedback here but maybe it would be nicer to create one or more new UDFs along the lines of `resolve_storage_alias(VARCHAR alias)` instead of rewriting this alias resolution code in UDFs that have other, specific purposes. I guess doing that would mean `select http_request('some_storage_alias')` would have to become `select http_request(resolve_storage_alias('some_storage_alias'))`. On the other hand, perhaps so few UDFs will ever require alias resolution that the present approach is better."", 'commenter': 'jnturton'}, {'comment': 'Good point regarding splitting the logic. I would propose to move it instead of UDF to another place, before the `http_request` is evaluated, for example to `RexToDrill.getDrillFunctionFromOptiqCall` (or another location), so it could have a consistent behavior for regular aliases and aliases for this function when query plans contain actual storage name instead of the aliases.', 'commenter': 'vvysotskyi'}, {'comment': ""> \r\n\r\nTo my knowledge, this is the only UDF that accesses the storage layer.  I was thinking the same thing however, but if the user doesn't know that aliasing is being used, they'd get a lot of strange errors and not necessarily know what to do."", 'commenter': 'cgivre'}, {'comment': '> Good point regarding splitting the logic. I would propose to move it instead of UDF to another place, before the `http_request` is evaluated, for example to `RexToDrill.getDrillFunctionFromOptiqCall` (or another location), so it could have a consistent behavior for regular aliases and aliases for this function when query plans contain actual storage name instead of the aliases.\r\n\r\nI like the idea.  I was almost thinking of doing it in the `StoragePluginRegistry`.  What do you think?   Could we make that a separate JIRA as I imagine that is considerably more complex?\r\n\r\nI should mention that this will still work if there are no aliases.  The order is first check to see if there is a user alias, if not, then check to see if there is a public alias, and finally if neither, use the actual text.\r\n\r\n', 'commenter': 'cgivre'}]"
2668,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/util/SimpleHttp.java,"@@ -870,6 +888,22 @@ public static HttpStoragePlugin getStoragePlugin(DrillbitContext context, String
     }
   }
 
+  public static String addBackTicksToAliasName(String plugin, String identifier) {
+    plugin = plugin.trim();
+    if (! plugin.startsWith(identifier)) {
+      plugin = identifier + plugin;
+    }
+    if (! plugin.endsWith(identifier)) {
+      plugin = plugin + identifier;
+    }
+    return plugin;
+  }
+
+  public static String removeBackTicksFromPluginName(String plugin, String identifier) {
+    plugin = plugin.trim();
+    plugin = plugin.replaceAll(identifier, """");
+    return plugin;
+  }","[{'comment': ""Looks like these two methods aren't used and should be removed."", 'commenter': 'vvysotskyi'}, {'comment': 'Removed.', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -713,11 +715,89 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {
         case ""date_trunc"": {
           return handleDateTruncFunction(args);
         }
+        case ""httprequest"":
+        case ""http_request"": {
+          final String QUOTING_IDENTIFIER = context.getPlannerSettings().getOptions().getOption(PlannerSettings.QUOTING_IDENTIFIERS_KEY).string_val;
+
+          // This code resolves aliases in the http_request function.
+          String completeRawPluginName = ((QuotedString) args.get(0)).value;
+          String username = context.getPlannerSettings().getQueryContext().getQueryUserName();
+          AliasRegistryProvider aliasRegistryProvider = context.getPlannerSettings().getQueryContext().getAliasRegistryProvider();
+          AliasRegistry storageAliasRegistry = aliasRegistryProvider.getStorageAliasesRegistry();
+          AliasRegistry tableAliasRegistry = aliasRegistryProvider.getTableAliasesRegistry();
+
+          // Split into plugin and endpoint
+          String[] parts = completeRawPluginName.split(""\\."");
+          if (parts.length < 2) {
+            throw new org.apache.drill.common.exceptions.DrillRuntimeException(
+              ""You must call this function with a connection name and endpoint.""
+            );
+          }
+
+          String rawPluginName = addBackTicksToAliasName(parts[0], QUOTING_IDENTIFIER);","[{'comment': 'Please change to here and below:\r\n```suggestion\r\n          String rawPluginName = SchemaPath.getSimplePath(parts[0]).toExpr();\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -713,11 +715,89 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {
         case ""date_trunc"": {
           return handleDateTruncFunction(args);
         }
+        case ""httprequest"":
+        case ""http_request"": {
+          final String QUOTING_IDENTIFIER = context.getPlannerSettings().getOptions().getOption(PlannerSettings.QUOTING_IDENTIFIERS_KEY).string_val;","[{'comment': 'There is no need to use QUOTING_IDENTIFIER here.', 'commenter': 'vvysotskyi'}, {'comment': 'Removed.', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -713,11 +715,89 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {
         case ""date_trunc"": {
           return handleDateTruncFunction(args);
         }
+        case ""httprequest"":
+        case ""http_request"": {
+          final String QUOTING_IDENTIFIER = context.getPlannerSettings().getOptions().getOption(PlannerSettings.QUOTING_IDENTIFIERS_KEY).string_val;
+
+          // This code resolves aliases in the http_request function.
+          String completeRawPluginName = ((QuotedString) args.get(0)).value;
+          String username = context.getPlannerSettings().getQueryContext().getQueryUserName();
+          AliasRegistryProvider aliasRegistryProvider = context.getPlannerSettings().getQueryContext().getAliasRegistryProvider();
+          AliasRegistry storageAliasRegistry = aliasRegistryProvider.getStorageAliasesRegistry();
+          AliasRegistry tableAliasRegistry = aliasRegistryProvider.getTableAliasesRegistry();
+
+          // Split into plugin and endpoint
+          String[] parts = completeRawPluginName.split(""\\."");
+          if (parts.length < 2) {
+            throw new org.apache.drill.common.exceptions.DrillRuntimeException(
+              ""You must call this function with a connection name and endpoint.""
+            );
+          }
+
+          String rawPluginName = addBackTicksToAliasName(parts[0], QUOTING_IDENTIFIER);
+          String rawEndpoint = addBackTicksToAliasName(parts[1], QUOTING_IDENTIFIER);
+
+
+          // Now resolve plugin name
+          String actualPluginName = storageAliasRegistry.getUserAliases(username).get(rawPluginName);
+          if (StringUtils.isEmpty(actualPluginName)) {
+            // Now check if there is a public alias for the plugin
+            actualPluginName = storageAliasRegistry.getPublicAliases().get(rawPluginName);
+            // If it is still empty, assign it the original name,
+            if (StringUtils.isEmpty(actualPluginName)) {
+              actualPluginName = rawPluginName;
+            }
+          }
+
+          // Finally remove backticks
+          actualPluginName = removeBackTicksFromPluginName(actualPluginName, QUOTING_IDENTIFIER);","[{'comment': 'Please change it to here and below:\r\n```suggestion\r\n          actualPluginName = SchemaPath.parseFromString(actualPluginName).getRootSegmentPath();\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -713,11 +715,89 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {
         case ""date_trunc"": {
           return handleDateTruncFunction(args);
         }
+        case ""httprequest"":
+        case ""http_request"": {
+          final String QUOTING_IDENTIFIER = context.getPlannerSettings().getOptions().getOption(PlannerSettings.QUOTING_IDENTIFIERS_KEY).string_val;
+
+          // This code resolves aliases in the http_request function.
+          String completeRawPluginName = ((QuotedString) args.get(0)).value;
+          String username = context.getPlannerSettings().getQueryContext().getQueryUserName();
+          AliasRegistryProvider aliasRegistryProvider = context.getPlannerSettings().getQueryContext().getAliasRegistryProvider();
+          AliasRegistry storageAliasRegistry = aliasRegistryProvider.getStorageAliasesRegistry();
+          AliasRegistry tableAliasRegistry = aliasRegistryProvider.getTableAliasesRegistry();
+
+          // Split into plugin and endpoint
+          String[] parts = completeRawPluginName.split(""\\."");
+          if (parts.length < 2) {
+            throw new org.apache.drill.common.exceptions.DrillRuntimeException(
+              ""You must call this function with a connection name and endpoint.""
+            );
+          }
+
+          String rawPluginName = addBackTicksToAliasName(parts[0], QUOTING_IDENTIFIER);
+          String rawEndpoint = addBackTicksToAliasName(parts[1], QUOTING_IDENTIFIER);
+
+
+          // Now resolve plugin name
+          String actualPluginName = storageAliasRegistry.getUserAliases(username).get(rawPluginName);
+          if (StringUtils.isEmpty(actualPluginName)) {
+            // Now check if there is a public alias for the plugin
+            actualPluginName = storageAliasRegistry.getPublicAliases().get(rawPluginName);
+            // If it is still empty, assign it the original name,
+            if (StringUtils.isEmpty(actualPluginName)) {
+              actualPluginName = rawPluginName;
+            }
+          }
+
+          // Finally remove backticks
+          actualPluginName = removeBackTicksFromPluginName(actualPluginName, QUOTING_IDENTIFIER);
+
+          // Now do the same for the endpoint name
+          String actualEndpointName = tableAliasRegistry.getUserAliases(username).get(rawEndpoint);
+          if (StringUtils.isEmpty(actualEndpointName)) {
+            // Now check if there is a public alias for the plugin
+            actualEndpointName = tableAliasRegistry.getPublicAliases().get(rawEndpoint);","[{'comment': 'There is no need to look up it manually. Public aliases will be used automatically if no user alias is found.', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -713,11 +715,89 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {
         case ""date_trunc"": {
           return handleDateTruncFunction(args);
         }
+        case ""httprequest"":
+        case ""http_request"": {
+          final String QUOTING_IDENTIFIER = context.getPlannerSettings().getOptions().getOption(PlannerSettings.QUOTING_IDENTIFIERS_KEY).string_val;
+
+          // This code resolves aliases in the http_request function.
+          String completeRawPluginName = ((QuotedString) args.get(0)).value;
+          String username = context.getPlannerSettings().getQueryContext().getQueryUserName();
+          AliasRegistryProvider aliasRegistryProvider = context.getPlannerSettings().getQueryContext().getAliasRegistryProvider();
+          AliasRegistry storageAliasRegistry = aliasRegistryProvider.getStorageAliasesRegistry();
+          AliasRegistry tableAliasRegistry = aliasRegistryProvider.getTableAliasesRegistry();
+
+          // Split into plugin and endpoint
+          String[] parts = completeRawPluginName.split(""\\."");","[{'comment': 'No need to split it manually, please use the `SchemaPath.parseFromString` method.', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -713,11 +715,89 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {
         case ""date_trunc"": {
           return handleDateTruncFunction(args);
         }
+        case ""httprequest"":
+        case ""http_request"": {
+          final String QUOTING_IDENTIFIER = context.getPlannerSettings().getOptions().getOption(PlannerSettings.QUOTING_IDENTIFIERS_KEY).string_val;
+
+          // This code resolves aliases in the http_request function.
+          String completeRawPluginName = ((QuotedString) args.get(0)).value;
+          String username = context.getPlannerSettings().getQueryContext().getQueryUserName();
+          AliasRegistryProvider aliasRegistryProvider = context.getPlannerSettings().getQueryContext().getAliasRegistryProvider();
+          AliasRegistry storageAliasRegistry = aliasRegistryProvider.getStorageAliasesRegistry();
+          AliasRegistry tableAliasRegistry = aliasRegistryProvider.getTableAliasesRegistry();
+
+          // Split into plugin and endpoint
+          String[] parts = completeRawPluginName.split(""\\."");
+          if (parts.length < 2) {
+            throw new org.apache.drill.common.exceptions.DrillRuntimeException(
+              ""You must call this function with a connection name and endpoint.""
+            );
+          }
+
+          String rawPluginName = addBackTicksToAliasName(parts[0], QUOTING_IDENTIFIER);
+          String rawEndpoint = addBackTicksToAliasName(parts[1], QUOTING_IDENTIFIER);
+
+
+          // Now resolve plugin name
+          String actualPluginName = storageAliasRegistry.getUserAliases(username).get(rawPluginName);
+          if (StringUtils.isEmpty(actualPluginName)) {
+            // Now check if there is a public alias for the plugin
+            actualPluginName = storageAliasRegistry.getPublicAliases().get(rawPluginName);
+            // If it is still empty, assign it the original name,
+            if (StringUtils.isEmpty(actualPluginName)) {
+              actualPluginName = rawPluginName;
+            }
+          }
+
+          // Finally remove backticks
+          actualPluginName = removeBackTicksFromPluginName(actualPluginName, QUOTING_IDENTIFIER);
+
+          // Now do the same for the endpoint name
+          String actualEndpointName = tableAliasRegistry.getUserAliases(username).get(rawEndpoint);
+          if (StringUtils.isEmpty(actualEndpointName)) {
+            // Now check if there is a public alias for the plugin
+            actualEndpointName = tableAliasRegistry.getPublicAliases().get(rawEndpoint);
+            // If it is still empty, assign it the original name,
+            if (StringUtils.isEmpty(actualEndpointName)) {
+              actualEndpointName = rawEndpoint;
+            }
+          }
+
+          // Now remove backticks
+          actualEndpointName = removeBackTicksFromPluginName(actualEndpointName, QUOTING_IDENTIFIER);
+
+          String finalPluginName = actualPluginName + ""."" + actualEndpointName;","[{'comment': 'Please use the schema path:\r\n```suggestion\r\n          String finalPluginName = SchemaPath.getCompoundPath(actualPluginName, actualEndpointName).getAsUnescapedPath();\r\n```', 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PlannerSettings.java,"@@ -242,10 +243,17 @@ public class PlannerSettings implements Context{
 
   public OptionManager options = null;
   public FunctionImplementationRegistry functionImplementationRegistry = null;
+  public QueryContext queryContext = null;
 
-  public PlannerSettings(OptionManager options, FunctionImplementationRegistry functionImplementationRegistry){
+  @Override
+  public boolean equals(Object obj) {","[{'comment': 'Looks like this equals method is not needed.', 'commenter': 'vvysotskyi'}, {'comment': 'fixed.   Not sure how that appeared.', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PlannerSettings.java,"@@ -242,10 +243,17 @@ public class PlannerSettings implements Context{
 
   public OptionManager options = null;
   public FunctionImplementationRegistry functionImplementationRegistry = null;
+  public QueryContext queryContext = null;","[{'comment': 'Please use the proper modifier.', 'commenter': 'vvysotskyi'}, {'comment': 'Made `private final`', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PlannerSettings.java,"@@ -242,10 +243,17 @@ public class PlannerSettings implements Context{
 
   public OptionManager options = null;
   public FunctionImplementationRegistry functionImplementationRegistry = null;
+  public QueryContext queryContext = null;
 
-  public PlannerSettings(OptionManager options, FunctionImplementationRegistry functionImplementationRegistry){
+  @Override
+  public boolean equals(Object obj) {
+    return super.equals(obj);
+  }
+
+  public PlannerSettings(OptionManager options, FunctionImplementationRegistry functionImplementationRegistry, QueryContext queryContext){","[{'comment': ""Please pass only fields that are required. query context has a lot of things that aren't required in planner settings."", 'commenter': 'vvysotskyi'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/work/metadata/ServerMetaProvider.java,"@@ -121,7 +121,7 @@ public void run() {
       final GetServerMetaResp.Builder respBuilder = GetServerMetaResp.newBuilder();
       try {
         final ServerMeta.Builder metaBuilder = ServerMeta.newBuilder(DEFAULT);
-        PlannerSettings plannerSettings = new PlannerSettings(session.getOptions(), context.getFunctionImplementationRegistry());
+        PlannerSettings plannerSettings = new PlannerSettings(session.getOptions(), context.getFunctionImplementationRegistry(), null);","[{'comment': ""After passing only the required things, please don't forget to pass them here."", 'commenter': 'vvysotskyi'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2668,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/udfs/HttpHelperFunctions.java,"@@ -125,6 +125,9 @@ public static class HttpGetFromStoragePluginFunction implements DrillSimpleFunc
     @Inject
     DrillbitContext drillbitContext;
 
+    @Inject
+    org.apache.drill.exec.ops.ContextInformation contextInformation;","[{'comment': 'Looks like this field could be removed.', 'commenter': 'vvysotskyi'}]"
2668,exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java,"@@ -713,11 +715,58 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {
         case ""date_trunc"": {
           return handleDateTruncFunction(args);
         }
+        case ""httprequest"":
+        case ""http_request"": {
+          // This code resolves aliases in the http_request function.
+          String completeRawPluginName = ((QuotedString) args.get(0)).value;
+          String username = context.getPlannerSettings().getQueryUser();
+
+          AliasRegistryProvider aliasRegistryProvider = context.getPlannerSettings().getAliasRegistryProvider();
+          AliasRegistry storageAliasRegistry = aliasRegistryProvider.getStorageAliasesRegistry();
+          AliasRegistry tableAliasRegistry = aliasRegistryProvider.getTableAliasesRegistry();
+
+          // Split into plugin and endpoint
+          SchemaPath schemaPath = SchemaPath.parseFromString(completeRawPluginName);
+          String rawPluginName = SchemaPath.getSimplePath(schemaPath.rootName()).toExpr();
+          String rawEndpoint = SchemaPath.getSimplePath(schemaPath.getLastSegment().getNameSegment().getPath()).toExpr();
+
+          // Now resolve plugin name
+          String actualPluginName = storageAliasRegistry.getUserAliases(username).get(rawPluginName);
+          if (StringUtils.isEmpty(actualPluginName)) {
+            // If it is empty, assign it the original name,
+            actualPluginName = rawPluginName;
+          }
+
+          // Finally remove backticks
+          actualPluginName = SchemaPath.parseFromString(actualPluginName).getRootSegmentPath();
+
+          // Now do the same for the endpoint name
+          String actualEndpointName = tableAliasRegistry.getUserAliases(username).get(rawEndpoint);
+          if (StringUtils.isEmpty(actualEndpointName)) {
+            // If it is empty, assign it the original name,
+            actualEndpointName = rawEndpoint;
+          }
+
+          // Now remove backticks
+          actualEndpointName = SchemaPath.parseFromString(actualEndpointName).getRootSegmentPath();
+
+          String finalPluginName = SchemaPath
+            .getCompoundPath(actualPluginName, actualEndpointName)
+            .getAsUnescapedPath();
+
+          QuotedString q = new QuotedString(finalPluginName, finalPluginName.length(), ExpressionPosition.UNKNOWN);
+
+          // Add args to new arg lists
+          List<LogicalExpression> requestArgs = new ArrayList<>();
+          requestArgs.add(q);
+          requestArgs.addAll(args.subList(1, args.size()));
+
+          return FunctionCallFactory.createExpression(functionName, requestArgs);
+        }
       }
 
       return FunctionCallFactory.createExpression(functionName, args);
     }
-","[{'comment': 'Please revert change in this line', 'commenter': 'vvysotskyi'}]"
2673,pom.xml,"@@ -17,9 +17,7 @@
     See the License for the specific language governing permissions and
     limitations under the License.
 
--->
-<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
-         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+--><project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">","[{'comment': 'Nit:  Please revert formatting changes, here and at the end of the file. ', 'commenter': 'cgivre'}]"
2689,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/DateFunctions.java,"@@ -140,8 +143,77 @@ public void eval() {
       java.time.format.DateTimeFormatter formatter = java.time.format.DateTimeFormatter.ofPattern(format);
       java.time.LocalDateTime dateTime = java.time.LocalDateTime.parse(inputDate, formatter);
 
-      java.time.LocalDateTime td = org.apache.drill.exec.udfs.NearestDateUtils.getDate(dateTime, intervalString);
+      java.time.LocalDateTime td = DateConversionUtils.getDate(dateTime, intervalString);
       out.value = td.atZone(java.time.ZoneId.of(""UTC"")).toInstant().toEpochMilli();
     }
   }
+
+  @FunctionTemplate(names = {""yearweek"",""year_week""},
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class YearWeekFunction implements DrillSimpleFunc {
+    @Param
+    VarCharHolder inputHolder;","[{'comment': ""If Drill's implicit casting is working the way it should be then we should not need to duplicate this function for varchar inputs."", 'commenter': 'jnturton'}, {'comment': 'Question here... My thinking in having the String input was that I wanted this function to be durable.  IE:  If a user provides strangely formatted dates, it still should work.  However, do you think it might make sense to remove the version which accepts a date as input?', 'commenter': 'cgivre'}, {'comment': 'See below, I think that in that case we want this function defined for DATE only using the convertlet stuff to convert it to `100*year()+week()` and a new\r\n```\r\ncreate function INT year_week_auto_parse(date_str VARCHAR) as 100*year(to_timestamp(date_str, \'auto\')) + week(to_timestamp(date_str, \'auto\'));\r\n```\r\n(assuming we can do these pure SQL UDFs). So we\'re composing general purpose functions to get to the specific ""What I need right now is a durable year_week function"" instead of hardcoding different behaviour into a new `year_week` to what we offer for e.g. `year`, `month`, or `week`. The latter would leave users frustrated by date extraction functions that arbitrarily (to them) work on mixed format strings in some cases and not in others.', 'commenter': 'jnturton'}, {'comment': ""@jnturton I removed the extra UDF.  I haven't messed with the convertlet logic before, but my initial digging seemed to imply that it was only for SQL keywords as defined by Calcite.  Do you know where in Drill I might add this?"", 'commenter': 'cgivre'}, {'comment': '@vvysotskyi suggested DrillConvertletTable...', 'commenter': 'jnturton'}]"
2689,contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestDateUtils.java,"@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.udfs;
+
+import org.junit.Test;
+
+import java.time.LocalDate;
+import java.time.LocalDateTime;
+
+import static org.junit.Assert.assertEquals;
+
+public class TestDateUtils {
+
+  @Test
+  public void testDateFromString() {
+    LocalDate testDate = LocalDate.of(2022, 3,14);
+    LocalDate badDate = LocalDate.of(1970, 1, 1);
+    assertEquals(testDate, DateUtilFunctions.getDateFromString(""2022-03-14""));
+    assertEquals(testDate, DateUtilFunctions.getDateFromString(""3/14/2022""));
+    assertEquals(testDate, DateUtilFunctions.getDateFromString(""14/03/2022"", true));
+    assertEquals(testDate, DateUtilFunctions.getDateFromString(""2022/3/14""));
+
+    // Test bad dates
+    assertEquals(badDate, DateUtilFunctions.getDateFromString(null));
+    assertEquals(badDate, DateUtilFunctions.getDateFromString(""1975-13-56""));
+    assertEquals(badDate, DateUtilFunctions.getDateFromString(""1975-1s""));","[{'comment': ""Is there a reason we aren't using null to represent invalid and missing dates? This seems like a perfect fit for it to me."", 'commenter': 'jnturton'}, {'comment': 'My thinking with that is that `null` tends to break secondary functions.  Returning a default value would prevent that.  This is a debate that Paul Rogers and I have had many times.  Is it better to return null, a default or error out. ', 'commenter': 'cgivre'}, {'comment': ""One possibility is to recast the question from a debate to asking what the industry norm is. In my experience other SQL engines don't usually return default values for invalid data and I'd bet that the reason is that the engineers recognised what lies down that road. Remember the proposal for numerical functions to return float values of NaN, or a special finite number, when they encountered invalid data and how that breaks null elimination in aggregates making things like SUM and AVG silently return useless or bogus answers rather than helpfully skip invalid data. \r\n\r\nConsidering the dates here, validity propagation breaks since `null + interval '1' day` remains null while `date '1970-01-01' + interval '1' day` is 1970-01-02, a new value which cannot even be checked against 1970-01-01 to test for validity.\r\n\r\nIf foo_func is an existing INT-valued function of an INT and we want to provide users with a short hand for `coalesce(foo_func(int_col), a_default_int)` then I propose that we \r\n\r\n- look for a general mechanism rather having just one or two functions work this way and\r\n- force the user to think about what they're doing and what Drill is doing so they think about the potential impact on their results\r\n- at the same time make sure we're doing things like null-if-null everywhere we should be so that secondary functions don't break in the first place, but sensibly return null themselves.\r\n\r\nOne way might be a facility to for users to define aliases for SQL expressions, or perhaps a better notion is the dynamic definition of pure SQL functions, something like\r\n```\r\ncreate alias foo_with_default(INT int_col) as coalesce(foo_func(int_col), a_default_int)\r\n```\r\nor\r\n```\r\ncreate function INT foo_with_default(INT int_col) as coalesce(foo_func(int_col), a_default_int)\r\n```\r\n\r\nMaybe we can chat with @vvysotskyi about whether a feature like this would work (and his thoughts on default-for-invalid logic)."", 'commenter': 'jnturton'}, {'comment': ""CC @paul-rogers in case he's also got any thoughts to add to this thread."", 'commenter': 'jnturton'}, {'comment': 'Footnote, since this case comes up quite often. Those of us who\'ve done data analytics with Pandas _did_ get used to floating point NaN being used as a ""sentinel"" for missing or invalid data but we should recognise this for what it was: a performance hack that entered Pandas from its Numpy foundation. It resulted in pain: automatic casting of ints to floats even though precision loss could happen because hardware and C integer types have no NaN value, the subsequent proliferation of different flavours of null - np.nan, Python None, and pd.NaT, special code to implement `skipna` for making NaNs behave like nulls rather than according to the IEEE float rules, etc.\r\n\r\nA one-line Drill query reveals the square peg and round hole relationship between IEEE 754 NaN and ANSI SQL null.\r\n```\r\napache drill> select cast(\'NaN\' as float) = cast(\'NaN\' as float), null = null;\r\nEXPR$0  true\r\nEXPR$1  null\r\n```\r\n\r\nAnd indeed, [here is Wes McKinney talking about moving away from that approach](https://wesmckinney.com/blog/bitmaps-vs-sentinel-values/)[1]. Here\'s a relevant excerpt for us, one hop over in the SQL world where null is a standardised first class citizen.\r\n\r\n> From the perspective of databases and data warehousing, reserving certain values to mark a null (or NA) is widely considered unacceptable. NaN is valid data, as is INT32_MIN and other common values used as sentinels.\r\n\r\n[1] Also see https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#missing-data-na', 'commenter': 'jnturton'}, {'comment': ""@jnturton I'm a little stuck here.  I did a bunch of experimentation, and right now, if the input is `null`, the function will return `null`.   However, if the input is unparsable, then the output is Jan 1, 1970 because the value of the `TimestampHolder` is `0`.   My goal is to make it so that if the input is not parsable, it will also return `null`, however I can't seem to get it to do that.\r\n\r\nI've tried using a `NullableTimestampHolder` as output and simply setting the value to `null` or not setting it at all. but When I do that, the function does not work at all.   Meaning that in the unit tests, ALL results come back as `null`.  \r\n\r\n@vvysotskyi Any suggestions?"", 'commenter': 'cgivre'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2689,contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNearestDateFunctions.java,"@@ -149,7 +149,7 @@ public void testReadException() throws Exception {
       run(query);
       fail();
     } catch (DrillRuntimeException e) {
-      assertTrue(e.getMessage().contains(""[BAD_DATE] is not a valid time statement. Expecting: "" + Arrays.asList(NearestDateUtils.TimeInterval.values())));
+      assertTrue(e.getMessage().contains(""[BAD_DATE] is not a valid time statement. Expecting: "" + Arrays.asList(DateConversionUtils.TimeInterval.values())));","[{'comment': 'Is TestNearestDateFunctions still a good name for this class given the renamings made in this PR?', 'commenter': 'jnturton'}, {'comment': ""I was trying to consolidate all the non-core date/time UDFs and their tests into fewer classes.  I'll refactor so that's the case."", 'commenter': 'cgivre'}, {'comment': 'Renamed...', 'commenter': 'cgivre'}]"
2689,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/DateFunctions.java,"@@ -140,8 +143,77 @@ public void eval() {
       java.time.format.DateTimeFormatter formatter = java.time.format.DateTimeFormatter.ofPattern(format);
       java.time.LocalDateTime dateTime = java.time.LocalDateTime.parse(inputDate, formatter);
 
-      java.time.LocalDateTime td = org.apache.drill.exec.udfs.NearestDateUtils.getDate(dateTime, intervalString);
+      java.time.LocalDateTime td = DateConversionUtils.getDate(dateTime, intervalString);
       out.value = td.atZone(java.time.ZoneId.of(""UTC"")).toInstant().toEpochMilli();
     }
   }
+
+  @FunctionTemplate(names = {""yearweek"",""year_week""},
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class YearWeekFunction implements DrillSimpleFunc {
+    @Param
+    VarCharHolder inputHolder;
+
+    @Output
+    IntHolder out;
+
+    @Override
+    public void setup() {
+      // noop
+    }
+
+    @Override
+    public void eval() {
+      String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputHolder.start, inputHolder.end, inputHolder.buffer);
+      java.time.LocalDateTime dt = org.apache.drill.exec.udfs.DateUtilFunctions.getTimestampFromString(input);
+      int week = dt.get(java.time.temporal.IsoFields.WEEK_OF_WEEK_BASED_YEAR);
+      int year = dt.getYear();
+      out.value = (year * 100) + week;
+    }
+  }
+
+  @FunctionTemplate(names = {""yearweek"",""year_week""},","[{'comment': 'I admit that I don\'t know all of the detail but I understand that because year_week(x) = 100*year(x) + week(x) we have the opportunity to provide ""no code"" implementation of this function in terms of existing functions by making an addition to DrillConvertletTable.', 'commenter': 'jnturton'}, {'comment': ""@jnturton We don't have `year()` and `week()` functions unfortunately.  I do plan on adding those as well as `quarter()` and a few others.  It is possible now to do some of that but the syntax is really non-intuitive."", 'commenter': 'cgivre'}, {'comment': ""Basically, what I was hoping to do was follow this as a guide: https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html.\r\n\r\nMany of these functions have Drill equivalents so I'm not going to bother with that.  But a lot don't..."", 'commenter': 'cgivre'}, {'comment': ""> @jnturton We don't have `year()` and `week()` functions unfortunately. I do plan on adding those as well as `quarter()` and a few others. It is possible now to do some of that but the syntax is really non-intuitive.\r\n\r\nThey work for me...\r\n\r\n```\r\napache drill> select 100*year(current_date)+week(current_date);\r\nEXPR$0  202243\r\n\r\n1 row selected (0.562 seconds)\r\n```"", 'commenter': 'jnturton'}, {'comment': ""Wow... I hate it when good functionality is not documented...  Thanks for letting me know about this and I've since updated the Drill docs. (https://github.com/apache/drill-site/pull/34) "", 'commenter': 'cgivre'}]"
2689,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/DateFunctions.java,"@@ -140,8 +143,77 @@ public void eval() {
       java.time.format.DateTimeFormatter formatter = java.time.format.DateTimeFormatter.ofPattern(format);
       java.time.LocalDateTime dateTime = java.time.LocalDateTime.parse(inputDate, formatter);
 
-      java.time.LocalDateTime td = org.apache.drill.exec.udfs.NearestDateUtils.getDate(dateTime, intervalString);
+      java.time.LocalDateTime td = DateConversionUtils.getDate(dateTime, intervalString);
       out.value = td.atZone(java.time.ZoneId.of(""UTC"")).toInstant().toEpochMilli();
     }
   }
+
+  @FunctionTemplate(names = {""yearweek"",""year_week""},
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class YearWeekFunction implements DrillSimpleFunc {
+    @Param
+    VarCharHolder inputHolder;
+
+    @Output
+    IntHolder out;
+
+    @Override
+    public void setup() {
+      // noop
+    }
+
+    @Override
+    public void eval() {
+      String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputHolder.start, inputHolder.end, inputHolder.buffer);
+      java.time.LocalDateTime dt = org.apache.drill.exec.udfs.DateUtilFunctions.getTimestampFromString(input);
+      int week = dt.get(java.time.temporal.IsoFields.WEEK_OF_WEEK_BASED_YEAR);
+      int year = dt.getYear();
+      out.value = (year * 100) + week;
+    }
+  }
+
+  @FunctionTemplate(names = {""yearweek"",""year_week""},
+    scope = FunctionTemplate.FunctionScope.SIMPLE,
+    nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)
+  public static class YearWeekFromDateFunction implements DrillSimpleFunc {
+    @Param
+    DateHolder inputHolder;
+
+    @Output
+    IntHolder out;
+
+    @Override
+    public void setup() {
+      // noop
+    }
+
+    @Override
+    public void eval() {
+      out.value = org.apache.drill.exec.udfs.DateUtilFunctions.getYearWeek(inputHolder.value);
+    }
+  }
+
+  @FunctionTemplate(names = {""time_stamp"", ""timestamp""},","[{'comment': ""Timestamp is one word everywhere else in Drill. I vote for sticking to that and for a more descriptive name like parse_timestamp or to_timestamp_auto (continuing our existing to_timestamp) for this function. I guess extending to_timestamp is also an option and clutters the namespace less e.g. `to_timestamp('1970-01-01', 'auto')` could apply the format detection logic here."", 'commenter': 'jnturton'}, {'comment': ""@jnturton I thought about that.  I'm totally open to options for this function.  Some background... I have data with inconsistent timestamp formatting... Yes in the ideal world the data would be well formed, but it isn't.  \r\n\r\nI was modeling the function names after MySQL which has a `timestamp` and `date` functions which take a string as input and return the respective data converted into a temporal format.  This way a user of MySQL wouldn't have to look up new function names.  \r\n\r\nI do like the idea of doing the auto option for the existing `TO_TIMESTAMP` function.  I'll take a look at the logic for `TO_TIMESTAMP` and see if that is a major headache to extend."", 'commenter': 'cgivre'}, {'comment': ""> I was modeling the function names after MySQL which has a timestamp and date functions which take a string as input and return the respective data converted into a temporal format. This way a user of MySQL wouldn't have to look up new function names.\r\n\r\nYes I do see, and support, the intent but only up to the point that the names chosen by the system inspiring the functions plug harmoniously into what we have. `time_stamp` is a spelling that is foreign to Drill (and to ANSI SQL) and Drill already does something else with `timestamp` today:\r\n\r\n```\r\napache drill> select `timestamp`('1970-01-01');\r\nEXPR$0  1970-01-01 00:00:00.0\r\n\r\n1 row selected (1.969 seconds)\r\n```\r\n\r\nLike you say, if `to_timestamp(date_str, 'auto')` doesn't turn that implementation into a monstrosity then maybe that's going to be the nicest option here. Hopefully your Util class could be used to prevent the eval method from growing very long..."", 'commenter': 'jnturton'}, {'comment': '@jnturton I renamed the function `TO_TIMESTAMP()`.  Now, if that function is called with one argument, it automatically attempts to parse the input into a timestamp.  If you have two arguments, the second is a format string.', 'commenter': 'cgivre'}]"
2689,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/DateFunctions.java,"@@ -140,8 +142,36 @@ public void eval() {
       java.time.format.DateTimeFormatter formatter = java.time.format.DateTimeFormatter.ofPattern(format);
       java.time.LocalDateTime dateTime = java.time.LocalDateTime.parse(inputDate, formatter);
 
-      java.time.LocalDateTime td = org.apache.drill.exec.udfs.NearestDateUtils.getDate(dateTime, intervalString);
+      java.time.LocalDateTime td = DateConversionUtils.getDate(dateTime, intervalString);
       out.value = td.atZone(java.time.ZoneId.of(""UTC"")).toInstant().toEpochMilli();
     }
   }
+
+  @FunctionTemplate(names = {""to_timestamp""},","[{'comment': 'Please add a comment explaining that this function differs from the other `to_timestamp` functions in that it takes no timestamp format argument and will try to determine one automatically.', 'commenter': 'jnturton'}]"
2702,contrib/format-deltalake/README.md,"@@ -0,0 +1,36 @@
+# Delta Lake format plugin
+
+This format plugin enabled Drill to query Delta Lake tables.","[{'comment': '```suggestion\r\nThis format plugin enables Drill to query Delta Lake tables.\r\n```', 'commenter': 'jnturton'}]"
2702,contrib/format-deltalake/README.md,"@@ -0,0 +1,36 @@
+# Delta Lake format plugin
+
+This format plugin enabled Drill to query Delta Lake tables.
+
+## Supported optimizations and features
+
+### Project pushdown
+
+This format plugin supports project and filter pushdown optimizations.
+
+For the case of project pushdown, only columns specified in the query will be read, even they are nested columns.","[{'comment': '```suggestion\r\nFor the case of project pushdown, only columns specified in the query will be read, even when they are nested columns.\r\n```', 'commenter': 'jnturton'}]"
2702,contrib/format-deltalake/README.md,"@@ -0,0 +1,36 @@
+# Delta Lake format plugin
+
+This format plugin enabled Drill to query Delta Lake tables.
+
+## Supported optimizations and features
+
+### Project pushdown
+
+This format plugin supports project and filter pushdown optimizations.
+
+For the case of project pushdown, only columns specified in the query will be read, even they are nested columns.
+
+### Filter pushdown
+
+For the case of filter pushdown, all expressions supported by Delta Lake API will be pushed down, so only data that
+matches the filter expression will be read. Additionally, filtering logic for parquet files is enabled
+to allow pruning of parquet files that do not match the filter expression.
+
+## Configuration
+
+Format plugin has the following configuration options:","[{'comment': '```suggestion\r\nThe format plugin has the following configuration options:\r\n```', 'commenter': 'jnturton'}]"
2702,exec/java-exec/src/main/java/org/apache/drill/exec/store/plan/PluginImplementor.java,"@@ -93,4 +93,12 @@ default void visitChild(RelNode input) throws IOException {
    * to ensure returning the correct rows number.
    */
   boolean artificialLimit();
+
+  /**
+   * If the plugin doesn't support native filter pushdown,
+   * but the reader can prune the number of rows to read.
+   * In this case filter operator on top of the scan should be preserved
+   * to ensure returning the correct rows number.","[{'comment': '```suggestion\r\n   * but the reader can prune the set of rows to read.\r\n   * In this case filter operator on top of the scan should be preserved\r\n   * to ensure returning the correct subset of rows.\r\n```', 'commenter': 'jnturton'}]"
2702,contrib/format-deltalake/src/main/java/org/apache/drill/exec/store/delta/format/DeltaFormatMatcher.java,"@@ -0,0 +1,68 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.delta.format;
+
+import io.delta.standalone.DeltaLog;
+import org.apache.drill.exec.planner.logical.DrillTable;
+import org.apache.drill.exec.store.SchemaConfig;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.dfs.FileSelection;
+import org.apache.drill.exec.store.dfs.FileSystemPlugin;
+import org.apache.drill.exec.store.dfs.FormatMatcher;
+import org.apache.drill.exec.store.dfs.FormatPlugin;
+import org.apache.drill.exec.store.dfs.FormatSelection;
+import org.apache.drill.exec.store.plan.rel.PluginDrillTable;
+import org.apache.hadoop.fs.FileStatus;
+
+public class DeltaFormatMatcher extends FormatMatcher {
+
+  private final DeltaFormatPlugin formatPlugin;
+
+  public DeltaFormatMatcher(DeltaFormatPlugin formatPlugin) {
+    this.formatPlugin = formatPlugin;
+  }
+
+  @Override
+  public boolean supportDirectoryReads() {","[{'comment': 'Is this something that was introduced by the Iceberg format plugin?', 'commenter': 'jnturton'}, {'comment': 'No, it was before. It means that the Drill plugin is able to handle queries where the directory is specified, not the file.', 'commenter': 'vvysotskyi'}]"
2702,contrib/format-deltalake/src/test/java/org/apache/drill/exec/store/delta/DeltaQueriesTest.java,"@@ -0,0 +1,195 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.delta;
+
+import org.apache.drill.common.logical.FormatPluginConfig;
+import org.apache.drill.common.logical.security.PlainCredentialsProvider;
+import org.apache.drill.exec.store.StoragePluginRegistry;
+import org.apache.drill.exec.store.delta.format.DeltaFormatPluginConfig;
+import org.apache.drill.exec.store.dfs.FileSystemConfig;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.math.BigDecimal;
+import java.nio.file.Paths;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.apache.drill.exec.util.StoragePluginTestUtils.DFS_PLUGIN_NAME;
+import static org.junit.Assert.assertEquals;
+
+public class DeltaQueriesTest extends ClusterTest {
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+
+    StoragePluginRegistry pluginRegistry = cluster.drillbit().getContext().getStorage();
+    FileSystemConfig pluginConfig = (FileSystemConfig) pluginRegistry.getPlugin(DFS_PLUGIN_NAME).getConfig();
+    Map<String, FormatPluginConfig> formats = new HashMap<>(pluginConfig.getFormats());
+    formats.put(""delta"", new DeltaFormatPluginConfig());
+    FileSystemConfig newPluginConfig = new FileSystemConfig(
+      pluginConfig.getConnection(),
+      pluginConfig.getConfig(),
+      pluginConfig.getWorkspaces(),
+      formats,
+      PlainCredentialsProvider.EMPTY_CREDENTIALS_PROVIDER);
+    newPluginConfig.setEnabled(pluginConfig.isEnabled());
+    pluginRegistry.put(DFS_PLUGIN_NAME, newPluginConfig);
+
+    dirTestWatcher.copyResourceToRoot(Paths.get(""data-reader-primitives""));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""data-reader-partition-values""));
+    dirTestWatcher.copyResourceToRoot(Paths.get(""data-reader-nested-struct""));
+  }
+
+  @Test
+  public void testSerDe() throws Exception {
+    String plan = queryBuilder().sql(""select * from dfs.`data-reader-partition-values`"").explainJson();
+    long count = queryBuilder().physical(plan).run().recordCount();
+    assertEquals(3, count);
+  }
+
+  @Test
+  public void testAllPrimitives() throws Exception {
+    testBuilder()
+      .sqlQuery(""select * from dfs.`data-reader-primitives`"")
+      .ordered()
+      .baselineColumns(""as_int"", ""as_long"", ""as_byte"", ""as_short"", ""as_boolean"", ""as_float"",
+        ""as_double"", ""as_string"", ""as_binary"", ""as_big_decimal"")
+      .baselineValues(null, null, null, null, null, null, null, null, null, null)
+      .baselineValues(0, 0L, 0, 0, true, 0.0f, 0.0, ""0"", new byte[]{0, 0}, BigDecimal.valueOf(0))
+      .baselineValues(1, 1L, 1, 1, false, 1.0f, 1.0, ""1"", new byte[]{1, 1}, BigDecimal.valueOf(1))
+      .baselineValues(2, 2L, 2, 2, true, 2.0f, 2.0, ""2"", new byte[]{2, 2}, BigDecimal.valueOf(2))
+      .baselineValues(3, 3L, 3, 3, false, 3.0f, 3.0, ""3"", new byte[]{3, 3}, BigDecimal.valueOf(3))
+      .baselineValues(4, 4L, 4, 4, true, 4.0f, 4.0, ""4"", new byte[]{4, 4}, BigDecimal.valueOf(4))
+      .baselineValues(5, 5L, 5, 5, false, 5.0f, 5.0, ""5"", new byte[]{5, 5}, BigDecimal.valueOf(5))
+      .baselineValues(6, 6L, 6, 6, true, 6.0f, 6.0, ""6"", new byte[]{6, 6}, BigDecimal.valueOf(6))
+      .baselineValues(7, 7L, 7, 7, false, 7.0f, 7.0, ""7"", new byte[]{7, 7}, BigDecimal.valueOf(7))
+      .baselineValues(8, 8L, 8, 8, true, 8.0f, 8.0, ""8"", new byte[]{8, 8}, BigDecimal.valueOf(8))
+      .baselineValues(9, 9L, 9, 9, false, 9.0f, 9.0, ""9"", new byte[]{9, 9}, BigDecimal.valueOf(9))
+      .go();
+  }
+
+  @Test
+  public void testProjectingColumns() throws Exception {
+
+    String query = ""select as_int, as_string from dfs.`data-reader-primitives`"";
+
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .include(""columns=\\[`as_int`, `as_string`\\]"")
+      .match();
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""as_int"", ""as_string"")
+      .baselineValues(null, null)
+      .baselineValues(0, ""0"")
+      .baselineValues(1, ""1"")
+      .baselineValues(2, ""2"")
+      .baselineValues(3, ""3"")
+      .baselineValues(4, ""4"")
+      .baselineValues(5, ""5"")
+      .baselineValues(6, ""6"")
+      .baselineValues(7, ""7"")
+      .baselineValues(8, ""8"")
+      .baselineValues(9, ""9"")
+      .go();
+  }
+
+  @Test
+  public void testProjectNestedColumn() throws Exception {
+    String query = ""select t.a.ac.acb as acb, b from dfs.`data-reader-nested-struct` t"";
+
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .include(""columns=\\[`a`.`ac`.`acb`, `b`\\]"")
+      .match();
+
+    testBuilder()
+      .sqlQuery(query)
+      .unOrdered()
+      .baselineColumns(""acb"", ""b"")
+      .baselineValues(0L, 0)
+      .baselineValues(1L, 1)
+      .baselineValues(2L, 2)
+      .baselineValues(3L, 3)
+      .baselineValues(4L, 4)
+      .baselineValues(5L, 5)
+      .baselineValues(6L, 6)
+      .baselineValues(7L, 7)
+      .baselineValues(8L, 8)
+      .baselineValues(9L, 9)
+      .go();
+  }
+
+  @Test
+  public void testPartitionPruning() throws Exception {
+    String query = ""select as_int, as_string from dfs.`data-reader-partition-values` where as_long = 1"";
+
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .include(""numFiles\\=1"")
+      .match();
+
+    testBuilder()
+      .sqlQuery(query)
+      .ordered()
+      .baselineColumns(""as_int"", ""as_string"")
+      .baselineValues(""1"", ""1"")
+      .go();
+  }
+
+  @Test
+  public void testEmptyResults() throws Exception {
+    String query = ""select as_int, as_string from dfs.`data-reader-partition-values` where as_long = 101"";
+
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .include(""numFiles\\=1"")
+      .match();
+
+    testBuilder()
+      .sqlQuery(query)
+      .ordered()
+      .expectsEmptyResultSet()
+      .go();
+  }
+
+  @Test
+  public void testLimit() throws Exception {
+    String query = ""select as_int, as_string from dfs.`data-reader-partition-values` limit 1"";
+
+    queryBuilder()
+      .sql(query)
+      .planMatcher()
+      .include(""Limit\\(fetch\\=\\[1\\]\\)"")","[{'comment': '```suggestion\r\n      // Note that both of the following two limits are expected because this format plugin supports an ""artificial"" limit.\r\n      .include(""Limit\\\\(fetch\\\\=\\\\[1\\\\]\\\\)"")\r\n```', 'commenter': 'jnturton'}]"
2702,contrib/format-deltalake/src/main/java/org/apache/drill/exec/store/delta/plan/DrillExprToDeltaTranslator.java,"@@ -0,0 +1,246 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.delta.plan;
+
+import io.delta.standalone.expressions.And;
+import io.delta.standalone.expressions.EqualTo;
+import io.delta.standalone.expressions.Expression;
+import io.delta.standalone.expressions.GreaterThan;
+import io.delta.standalone.expressions.GreaterThanOrEqual;
+import io.delta.standalone.expressions.IsNotNull;
+import io.delta.standalone.expressions.IsNull;
+import io.delta.standalone.expressions.LessThan;
+import io.delta.standalone.expressions.LessThanOrEqual;
+import io.delta.standalone.expressions.Literal;
+import io.delta.standalone.expressions.Not;
+import io.delta.standalone.expressions.Or;
+import io.delta.standalone.expressions.Predicate;
+import io.delta.standalone.types.StructType;
+import org.apache.drill.common.FunctionNames;
+import org.apache.drill.common.expression.FunctionCall;
+import org.apache.drill.common.expression.LogicalExpression;
+import org.apache.drill.common.expression.PathSegment;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.common.expression.ValueExpressions;
+import org.apache.drill.common.expression.visitors.AbstractExprVisitor;
+
+public class DrillExprToDeltaTranslator extends AbstractExprVisitor<Expression, Void, RuntimeException> {
+
+  private final StructType structType;
+
+  public DrillExprToDeltaTranslator(StructType structType) {
+    this.structType = structType;
+  }
+
+  @Override
+  public Expression visitFunctionCall(FunctionCall call, Void value) {
+    try {
+      return visitFunctionCall(call);
+    } catch (Exception e) {
+      return null;
+    }
+  }
+
+  private Predicate visitFunctionCall(FunctionCall call) {
+    switch (call.getName()) {
+      case FunctionNames.AND: {
+        Expression left = call.arg(0).accept(this, null);
+        Expression right = call.arg(1).accept(this, null);
+        if (left != null && right != null) {
+          return new And(left, right);
+        }
+        return null;
+      }
+      case FunctionNames.OR: {
+        Expression left = call.arg(0).accept(this, null);
+        Expression right = call.arg(1).accept(this, null);
+        if (left != null && right != null) {
+          return new Or(left, right);
+        }
+        return null;
+      }
+      case FunctionNames.NOT: {
+        Expression expression = call.arg(0).accept(this, null);
+        if (expression != null) {
+          return new Not(expression);
+        }
+        return null;
+      }
+      case FunctionNames.IS_NULL: {
+        LogicalExpression arg = call.arg(0);
+        if (arg instanceof SchemaPath) {
+          String name = getPath((SchemaPath) arg);
+          return new IsNull(structType.column(name));
+        }
+        return null;
+      }
+      case FunctionNames.IS_NOT_NULL: {
+        LogicalExpression arg = call.arg(0);
+        if (arg instanceof SchemaPath) {
+          String name = getPath((SchemaPath) arg);
+          return new IsNotNull(structType.column(name));
+        }
+        return null;
+      }
+      case FunctionNames.LT: {
+        LogicalExpression nameRef = call.arg(0);
+        Expression expression = call.arg(1).accept(this, null);
+        if (nameRef instanceof SchemaPath) {
+          String name = getPath((SchemaPath) nameRef);
+          return new LessThan(structType.column(name), expression);
+        }
+        return null;
+      }
+      case FunctionNames.LE: {
+        LogicalExpression nameRef = call.arg(0);
+        Expression expression = call.arg(1).accept(this, null);
+        if (nameRef instanceof SchemaPath) {
+          String name = getPath((SchemaPath) nameRef);
+          return new LessThanOrEqual(structType.column(name), expression);
+        }
+        return null;
+      }
+      case FunctionNames.GT: {
+        LogicalExpression nameRef = call.args().get(0);
+        Expression expression = call.args().get(1).accept(this, null);
+        if (nameRef instanceof SchemaPath) {
+          String name = getPath((SchemaPath) nameRef);
+          return new GreaterThan(structType.column(name), expression);
+        }
+        return null;
+      }
+      case FunctionNames.GE: {
+        LogicalExpression nameRef = call.args().get(0);
+        Expression expression = call.args().get(0).accept(this, null);
+        if (nameRef instanceof SchemaPath) {
+          String name = getPath((SchemaPath) nameRef);
+          return new GreaterThanOrEqual(structType.column(name), expression);
+        }
+        return null;
+      }
+      case FunctionNames.EQ: {
+        LogicalExpression nameRef = call.args().get(0);
+        Expression expression = call.args().get(1).accept(this, null);
+        if (nameRef instanceof SchemaPath) {
+          String name = getPath((SchemaPath) nameRef);
+          return new EqualTo(structType.column(name), expression);
+        }
+        return null;
+      }
+      case FunctionNames.NE: {
+        LogicalExpression nameRef = call.args().get(0);
+        Expression expression = call.args().get(1).accept(this, null);
+        if (nameRef instanceof SchemaPath) {
+          String name = getPath((SchemaPath) nameRef);
+          return new Not(new EqualTo(structType.column(name), expression));
+        }
+        return null;
+      }
+    }
+    return null;
+  }
+
+  @Override
+  public Expression visitFloatConstant(ValueExpressions.FloatExpression fExpr, Void value) {
+    return Literal.of(fExpr.getFloat());
+  }
+
+  @Override
+  public Expression visitIntConstant(ValueExpressions.IntExpression intExpr, Void value) {
+    return Literal.of(intExpr.getInt());
+  }
+
+  @Override
+  public Expression visitLongConstant(ValueExpressions.LongExpression longExpr, Void value) {
+    return Literal.of(longExpr.getLong());
+  }
+
+  @Override
+  public Expression visitDecimal9Constant(ValueExpressions.Decimal9Expression decExpr, Void value) {
+    return Literal.of(decExpr.getIntFromDecimal());
+  }
+
+  @Override
+  public Expression visitDecimal18Constant(ValueExpressions.Decimal18Expression decExpr, Void value) {
+    return Literal.of(decExpr.getLongFromDecimal());
+  }
+
+  @Override
+  public Expression visitDecimal28Constant(ValueExpressions.Decimal28Expression decExpr, Void value) {
+    return Literal.of(decExpr.getBigDecimal());
+  }
+
+  @Override
+  public Expression visitDecimal38Constant(ValueExpressions.Decimal38Expression decExpr, Void value) {
+    return Literal.of(decExpr.getBigDecimal());
+  }
+
+  @Override
+  public Expression visitVarDecimalConstant(ValueExpressions.VarDecimalExpression decExpr, Void value) {
+    return Literal.of(decExpr.getBigDecimal());
+  }
+
+  @Override
+  public Expression visitDateConstant(ValueExpressions.DateExpression dateExpr, Void value) {
+    return Literal.of(dateExpr.getDate());
+  }
+
+  @Override
+  public Expression visitTimeConstant(ValueExpressions.TimeExpression timeExpr, Void value) {
+    return Literal.of(timeExpr.getTime());
+  }
+
+  @Override
+  public Expression visitTimeStampConstant(ValueExpressions.TimeStampExpression timestampExpr, Void value) {
+    return Literal.of(timestampExpr.getTimeStamp());
+  }
+
+  @Override
+  public Expression visitDoubleConstant(ValueExpressions.DoubleExpression dExpr, Void value) {
+    return Literal.of(dExpr.getDouble());
+  }
+
+  @Override
+  public Expression visitBooleanConstant(ValueExpressions.BooleanExpression e, Void value) {
+    return Literal.of(e.getBoolean());
+  }
+
+  @Override
+  public Expression visitQuotedStringConstant(ValueExpressions.QuotedString e, Void value) {
+    return Literal.of(e.getString());
+  }
+
+  @Override
+  public Expression visitUnknown(LogicalExpression e, Void value) {
+    return null;
+  }
+
+  private static String getPath(SchemaPath schemaPath) {","[{'comment': 'Is any of this generic enough to go to a schema utils class, or is the ""element"" keyword specific to Delta Lake?', 'commenter': 'jnturton'}, {'comment': 'Yes, I think `element` keyword usage is not common.', 'commenter': 'vvysotskyi'}]"
2702,contrib/format-deltalake/src/main/java/org/apache/drill/exec/store/delta/read/DeltaScanBatchCreator.java,"@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.delta.read;
+
+import org.apache.drill.common.exceptions.ExecutionSetupException;
+import org.apache.drill.exec.ExecConstants;
+import org.apache.drill.exec.ops.ExecutorFragmentContext;
+import org.apache.drill.exec.ops.OperatorContext;
+import org.apache.drill.exec.physical.impl.BatchCreator;
+import org.apache.drill.exec.record.CloseableRecordBatch;
+import org.apache.drill.exec.record.RecordBatch;
+import org.apache.drill.exec.server.options.OptionManager;
+import org.apache.drill.exec.store.ColumnExplorer;
+import org.apache.drill.exec.store.delta.DeltaRowGroupScan;
+import org.apache.drill.exec.store.dfs.DrillFileSystem;
+import org.apache.drill.exec.store.parquet.AbstractParquetRowGroupScan;
+import org.apache.drill.exec.store.parquet.AbstractParquetScanBatchCreator;
+import org.apache.drill.exec.store.parquet.ParquetScanBatchCreator;
+import org.apache.drill.exec.store.parquet.RowGroupReadEntry;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.util.List;
+import java.util.Map;
+
+@SuppressWarnings(""unused"")
+public class DeltaScanBatchCreator extends AbstractParquetScanBatchCreator","[{'comment': ""So because you've inherited from AbstractParquetScanBatchCreator here, the various options that control the Parquet reader like `store.parquet.use_new_reader` will be applied for Delta Lake tables?"", 'commenter': 'jnturton'}, {'comment': 'Yes, this and other options are applicable to the delta plugin.', 'commenter': 'vvysotskyi'}]"
2702,contrib/format-deltalake/src/main/java/org/apache/drill/exec/store/delta/format/DeltaFormatPlugin.java,"@@ -0,0 +1,217 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.delta.format;
+
+import org.apache.calcite.plan.Convention;
+import org.apache.calcite.plan.RelOptRule;
+import org.apache.drill.common.expression.SchemaPath;
+import org.apache.drill.exec.metastore.MetadataProviderManager;
+import org.apache.drill.exec.physical.base.AbstractGroupScan;
+import org.apache.drill.exec.physical.base.AbstractWriter;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.planner.PlannerPhase;
+import org.apache.drill.exec.planner.common.DrillStatsTable;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.record.metadata.schema.SchemaProvider;
+import org.apache.drill.exec.server.DrillbitContext;
+import org.apache.drill.exec.store.PluginRulesProviderImpl;
+import org.apache.drill.exec.store.StoragePluginRulesSupplier;
+import org.apache.drill.exec.store.dfs.FileSelection;
+import org.apache.drill.exec.store.dfs.FileSystemConfig;
+import org.apache.drill.exec.store.dfs.FormatMatcher;
+import org.apache.drill.exec.store.dfs.FormatPlugin;
+import org.apache.drill.exec.store.delta.DeltaGroupScan;
+import org.apache.drill.exec.store.delta.plan.DeltaPluginImplementor;
+import org.apache.drill.exec.store.parquet.ParquetReaderConfig;
+import org.apache.drill.exec.store.plan.rel.PluginRel;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicInteger;
+
+public class DeltaFormatPlugin implements FormatPlugin {
+
+  private static final String DELTA_CONVENTION_PREFIX = ""DELTA."";
+
+  /**
+   * Generator for format id values. Formats with the same name may be defined
+   * in multiple storage plugins, so using the unique id within the convention name
+   * to ensure the rule names will be unique for different plugin instances.
+   */
+  private static final AtomicInteger NEXT_ID = new AtomicInteger(0);
+
+  private final FileSystemConfig storageConfig;
+
+  private final DeltaFormatPluginConfig config;
+
+  private final Configuration fsConf;
+
+  private final DrillbitContext context;
+
+  private final String name;
+
+  private final DeltaFormatMatcher matcher;
+
+  private final StoragePluginRulesSupplier storagePluginRulesSupplier;
+
+  public DeltaFormatPlugin(
+    String name,
+    DrillbitContext context,
+    Configuration fsConf,
+    FileSystemConfig storageConfig,
+    DeltaFormatPluginConfig config) {
+    this.storageConfig = storageConfig;
+    this.config = config;
+    this.fsConf = fsConf;
+    this.context = context;
+    this.name = name;
+    this.matcher = new DeltaFormatMatcher(this);
+    this.storagePluginRulesSupplier = storagePluginRulesSupplier(name + NEXT_ID.getAndIncrement());
+  }
+
+  private static StoragePluginRulesSupplier storagePluginRulesSupplier(String name) {
+    Convention convention = new Convention.Impl(DELTA_CONVENTION_PREFIX + name, PluginRel.class);
+    return StoragePluginRulesSupplier.builder()
+      .rulesProvider(new PluginRulesProviderImpl(convention, DeltaPluginImplementor::new))
+      .supportsFilterPushdown(true)
+      .supportsProjectPushdown(true)
+      .supportsLimitPushdown(true)
+      .convention(convention)
+      .build();
+  }
+
+  @Override
+  public boolean supportsRead() {
+    return true;
+  }
+
+  @Override
+  public boolean supportsWrite() {
+    return false;
+  }
+
+  @Override
+  public boolean supportsAutoPartitioning() {
+    return false;
+  }
+
+  @Override
+  public FormatMatcher getMatcher() {
+    return matcher;
+  }
+
+  @Override
+  public AbstractWriter getWriter(PhysicalOperator child, String location, List<String> partitionColumns) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public Set<? extends RelOptRule> getOptimizerRules(PlannerPhase phase) {
+    switch (phase) {
+      case PHYSICAL:
+      case LOGICAL:
+        return storagePluginRulesSupplier.getOptimizerRules();
+      case LOGICAL_PRUNE_AND_JOIN:
+      case LOGICAL_PRUNE:
+      case PARTITION_PRUNING:
+      case JOIN_PLANNING:
+      default:
+        return Collections.emptySet();
+    }
+  }
+
+  @Override
+  public AbstractGroupScan getGroupScan(String userName, FileSelection selection, List<SchemaPath> columns) throws IOException {
+    return DeltaGroupScan.builder()
+      .userName(userName)
+      .formatPlugin(this)
+      .readerConfig(ParquetReaderConfig.builder().withConf(fsConf).build())
+      .path(selection.selectionRoot.toUri().getPath())
+      .columns(columns)
+      .limit(-1)
+      .build();
+  }
+
+  @Override
+  public AbstractGroupScan getGroupScan(String userName, FileSelection selection,
+    List<SchemaPath> columns, MetadataProviderManager metadataProviderManager) throws IOException {
+    SchemaProvider schemaProvider = metadataProviderManager.getSchemaProvider();
+    TupleMetadata schema = schemaProvider != null
+      ? schemaProvider.read().getSchema()
+      : null;
+    return DeltaGroupScan.builder()
+      .userName(userName)
+      .formatPlugin(this)
+      .readerConfig(ParquetReaderConfig.builder().withConf(fsConf).build())
+      .schema(schema)
+      .path(selection.selectionRoot.toUri().getPath())
+      .columns(columns)
+      .limit(-1)
+      .build();
+  }
+
+  @Override
+  public boolean supportsStatistics() {
+    return false;
+  }
+
+  @Override
+  public DrillStatsTable.TableStatistics readStatistics(FileSystem fs, Path statsTablePath) {
+    throw new UnsupportedOperationException(""unimplemented"");","[{'comment': 'Could this in principle be implemented for Delta Lake?', 'commenter': 'jnturton'}, {'comment': ""It uses stats files that Drill creates (currently, only parquet format is supported), so for the case of the Delta plugin, I'm not sure whether it is a good idea to create extra files there."", 'commenter': 'vvysotskyi'}]"
2703,exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/impl/IsEmptyUtils.java,"@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.expr.fn.impl;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.vector.complex.impl.SingleMapReaderImpl;
+import org.apache.drill.exec.vector.complex.reader.FieldReader;
+
+public class IsEmptyUtils {
+
+  /**
+   * This function recursively traverses a Drill map to determine whether the map is empty or not.
+   * @param reader A {@link FieldReader} containing the field in question
+   * @return True if the field contains no data, false if it does.
+   */
+  public static boolean mapIsEmpty(FieldReader reader) {
+
+    if (reader.getType().getMinorType() == MinorType.MAP) {
+      SingleMapReaderImpl mapReader = (SingleMapReaderImpl) reader;
+
+      // If the map reader has no fields returns nothing return true","[{'comment': '```suggestion\r\n      // If the map reader has no fields return true\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2703,exec/java-exec/src/test/resources/jsoninput/is_empty_tests.json,"@@ -0,0 +1,27 @@
+[
+  {
+    ""numeric_col"": 1.3,
+    ""text_col"": ""text"",
+    ""list_col"": [
+      ""v1"", ""v2""
+    ],
+   ""map_column"": {
+    ""field1"": ""value1"",
+    ""field2"": ""value2"",
+    ""nested_map"": {
+      ""nested_field1"": ""nested_value1"",
+      ""nested_field2"" : ""nested_value2""
+      }
+    }
+  },{
+    ""numeric_col"": 2.3,
+    ""text_col"": """",
+    ""list_col"": [],
+    ""map_column"": {}
+  },{
+    ""numeric_col"": null,
+    ""text_col"": null,
+    ""list_col"": [],
+    ""map_column"": {}
+  }","[{'comment': '```suggestion\r\n  }, {\r\n    ""numeric_col"": 1\r\n  }\r\n```\r\nCan we add an object that is outright missing some queried properties and check that the is_empty unit tests pick those up as being empty?', 'commenter': 'jnturton'}, {'comment': '@jnturton Done!  I addressed your review comments.  Thanks for the quick review!', 'commenter': 'cgivre'}]"
2709,contrib/storage-drill/src/main/java/org/apache/drill/exec/store/drill/plugin/DrillSubScan.java,"@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.drill.plugin;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.exec.physical.base.AbstractBase;
+import org.apache.drill.exec.physical.base.PhysicalOperator;
+import org.apache.drill.exec.physical.base.PhysicalVisitor;
+import org.apache.drill.exec.physical.base.SubScan;
+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;
+
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+
+@JsonTypeName(""drill-read"")
+public class DrillSubScan extends AbstractBase implements SubScan {
+
+  public static final String OPERATOR_TYPE = ""DRILL_SUB_SCAN"";
+
+  private final String query;
+
+  @JsonProperty
+  private final DrillStoragePluginConfig pluginConfig;
+
+  @JsonCreator
+  public DrillSubScan(
+      @JsonProperty(""userName"") String userName,
+      @JsonProperty(""mongoPluginConfig"") StoragePluginConfig pluginConfig,","[{'comment': 'Is this supposed to be `mongoPluginConfig`?', 'commenter': 'cgivre'}, {'comment': ""No, it isn't, thanks, fixed it."", 'commenter': 'vvysotskyi'}]"
2709,contrib/storage-drill/src/main/java/org/apache/drill/exec/store/drill/plugin/package-info.java,"@@ -15,24 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.drill.jdbc;
-
-import java.sql.SQLTimeoutException;
-
 /**
- * Indicates that an operation timed out. This is not an error; you can
- * retry the operation.
+ * Drill storage plugin.
+ * <p>
+ * Enables querying Drill as a data store.
  */
-public class SqlTimeoutException extends SQLTimeoutException {
-  private static final long serialVersionUID = 2017_04_03L;
-
-  SqlTimeoutException() {
-    // SQLException(reason, SQLState, vendorCode)
-    // REVIEW mb 19-Jul-05 Is there a standard SQLState?
-    super(""timeout"", null, 0);
-  }
-
-  public SqlTimeoutException(long timeoutValueInSeconds) {
-    super(""Query timed out in ""+ timeoutValueInSeconds + "" seconds"");
-  }
-}
+package org.apache.drill.exec.store.drill.plugin;","[{'comment': 'So Git decided that this was the renaming of a file 😏', 'commenter': 'jnturton'}]"
2709,contrib/storage-drill/src/main/java/org/apache/drill/exec/store/drill/plugin/DrillStoragePluginConfig.java,"@@ -0,0 +1,126 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.drill.plugin;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeName;
+import org.apache.calcite.avatica.ConnectStringParser;
+import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.common.config.DrillProperties;
+import org.apache.drill.common.logical.StoragePluginConfig;
+import org.apache.drill.common.logical.security.CredentialsProvider;
+import org.apache.drill.common.logical.security.PlainCredentialsProvider;
+import org.apache.drill.exec.client.DrillClient;
+import org.apache.drill.exec.memory.BufferAllocator;
+import org.apache.drill.exec.rpc.RpcException;
+
+import java.sql.SQLException;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Properties;
+
+@JsonTypeName(DrillStoragePluginConfig.NAME)
+public class DrillStoragePluginConfig extends StoragePluginConfig {
+  public static final String NAME = ""drill"";
+  public static final String CONNECTION_STRING_PREFIX = ""jdbc:drill:"";
+
+  private static final String DEFAULT_QUOTING_IDENTIFIER = ""`"";
+
+  private final String connection;
+  private final Properties properties;
+
+  @JsonCreator
+  public DrillStoragePluginConfig(
+      @JsonProperty(""connection"") String connection,
+      @JsonProperty(""properties"") Properties properties,
+      @JsonProperty(""credentialsProvider"") CredentialsProvider credentialsProvider) {
+    super(getCredentialsProvider(credentialsProvider), credentialsProvider == null);
+    this.connection = connection;
+    this.properties = Optional.ofNullable(properties).orElse(new Properties());
+  }
+
+  private DrillStoragePluginConfig(DrillStoragePluginConfig that,
+    CredentialsProvider credentialsProvider) {
+    super(getCredentialsProvider(credentialsProvider),
+      credentialsProvider == null, that.authMode);
+    this.connection = that.connection;
+    this.properties = that.properties;
+  }
+
+  @JsonProperty(""connection"")
+  public String getConnection() {
+    return connection;
+  }
+
+  @JsonProperty(""properties"")
+  public Properties getProperties() {
+    return properties;
+  }
+
+  private static CredentialsProvider getCredentialsProvider(CredentialsProvider credentialsProvider) {
+    return credentialsProvider != null ? credentialsProvider : PlainCredentialsProvider.EMPTY_CREDENTIALS_PROVIDER;
+  }
+
+  @JsonIgnore
+  public String getIdentifierQuoteString() {
+    return properties.getProperty(DrillProperties.QUOTING_IDENTIFIERS, DEFAULT_QUOTING_IDENTIFIER);
+  }
+
+  @Override
+  public DrillStoragePluginConfig updateCredentialProvider(CredentialsProvider credentialsProvider) {
+    return new DrillStoragePluginConfig(this, credentialsProvider);
+  }
+
+  @JsonIgnore
+  public DrillClient getDrillClient(String userName, BufferAllocator allocator) {
+    try {
+      String urlSuffix = connection.substring(CONNECTION_STRING_PREFIX.length());
+      Properties props = ConnectStringParser.parse(urlSuffix, properties);
+      props.putAll(credentialsProvider.getUserCredentials(userName));","[{'comment': 'This getUserCredentials(String username) method is meant to fetch per-query-user credentials for plugins that are in user translation auth mode while the nullary method getUserCredentials() is meant for shared credentials. Only the plain and Vault providers currently support per-user credentials. You can see some logic for deciding which to call (via UsernamePasswordCredentials objects) in JdbcStorageConfig on line 142.\r\n\r\nThose APIs wound up being a little ugly :/', 'commenter': 'jnturton'}, {'comment': 'Thanks, fixed.', 'commenter': 'vvysotskyi'}]"
2709,contrib/storage-drill/src/test/java/org/apache/drill/exec/store/drill/plugin/DrillPluginQueriesTest.java,"@@ -0,0 +1,312 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store.drill.plugin;
+
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.physical.rowSet.RowSet;
+import org.apache.drill.exec.physical.rowSet.RowSetBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.test.ClientFixture;
+import org.apache.drill.test.ClusterFixture;
+import org.apache.drill.test.ClusterTest;
+import org.apache.drill.test.rowSet.RowSetComparison;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.util.Properties;
+
+import static org.junit.Assert.assertEquals;
+
+public class DrillPluginQueriesTest extends ClusterTest {
+
+  private static final String TABLE_NAME = ""dfs.tmp.test_table"";
+
+  private static ClusterFixture drill;
+  private static ClientFixture drillClient;
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    initPlugin();
+  }
+
+  @AfterClass
+  public static void shutdown() throws Exception {
+    AutoCloseables.close(drill, drillClient);
+  }
+
+  private static void initPlugin() throws Exception {
+    startCluster(ClusterFixture.builder(dirTestWatcher));
+    drill = ClusterFixture.builder(dirTestWatcher).build();
+
+    DrillStoragePluginConfig config = new DrillStoragePluginConfig(
+      ""jdbc:drill:drillbit=localhost:"" + drill.drillbit().getUserPort(),
+      new Properties(), null);
+    config.setEnabled(true);
+    cluster.defineStoragePlugin(""drill"", config);
+    cluster.defineStoragePlugin(""drill2"", config);
+    drillClient = drill.clientFixture();
+
+    drillClient.queryBuilder()
+      .sql(""create table %s as select * from cp.`tpch/nation.parquet`"", TABLE_NAME)
+      .run();
+  }
+
+  @Test
+  public void testSerDe() throws Exception {
+    String plan = queryBuilder().sql(""select * from drill.%s"", TABLE_NAME).explainJson();
+    long count = queryBuilder().physical(plan).run().recordCount();
+    assertEquals(25, count);
+  }
+
+  @Test
+  public void testShowDatabases() throws Exception {
+    testBuilder()
+      .sqlQuery(""show databases where SCHEMA_NAME='drill.dfs.tmp'"")
+      .unOrdered()
+      .baselineColumns(""SCHEMA_NAME"")
+      .baselineValues(""drill.dfs.tmp"")
+      .go();
+  }
+
+  @Test
+  public void testShowTables() throws Exception {
+    testBuilder()
+      .sqlQuery(""show tables IN drill.INFORMATION_SCHEMA"")
+      .unOrdered()
+      .baselineColumns(""TABLE_SCHEMA"", ""TABLE_NAME"")
+      .baselineValues(""drill.information_schema"", ""VIEWS"")
+      .baselineValues(""drill.information_schema"", ""CATALOGS"")
+      .baselineValues(""drill.information_schema"", ""COLUMNS"")
+      .baselineValues(""drill.information_schema"", ""PARTITIONS"")
+      .baselineValues(""drill.information_schema"", ""FILES"")
+      .baselineValues(""drill.information_schema"", ""SCHEMATA"")
+      .baselineValues(""drill.information_schema"", ""TABLES"")
+      .go();
+  }
+
+  @Test
+  public void testProjectPushDown() throws Exception {
+    String query = ""select n_nationkey, n_regionkey, n_name from drill.%s"";
+
+    queryBuilder()
+        .sql(query, TABLE_NAME)
+        .planMatcher()
+        .include(""query=\""SELECT `n_nationkey`, `n_regionkey`, `n_name`"")
+        .exclude(""\\*"")
+        .match();
+
+    RowSet sets = queryBuilder()
+      .sql(query, TABLE_NAME)
+      .rowSet();
+
+    TupleMetadata schema = new SchemaBuilder()
+      .add(""n_nationkey"", TypeProtos.MinorType.INT)
+      .add(""n_regionkey"", TypeProtos.MinorType.INT)
+      .add(""n_name"", TypeProtos.MinorType.VARCHAR)
+      .build();
+
+    RowSet expected = new RowSetBuilder(client.allocator(), schema)
+      .addRow(0, 0, ""ALGERIA"")
+      .addRow(1, 1, ""ARGENTINA"")
+      .addRow(2, 1, ""BRAZIL"")
+      .addRow(3, 1, ""CANADA"")
+      .addRow(4, 4, ""EGYPT"")
+      .addRow(5, 0, ""ETHIOPIA"")
+      .addRow(6, 3, ""FRANCE"")
+      .addRow(7, 3, ""GERMANY"")
+      .addRow(8, 2, ""INDIA"")
+      .addRow(9, 2, ""INDONESIA"")
+      .addRow(10, 4, ""IRAN"")
+      .addRow(11, 4, ""IRAQ"")
+      .addRow(12, 2, ""JAPAN"")
+      .addRow(13, 4, ""JORDAN"")
+      .addRow(14, 0, ""KENYA"")
+      .addRow(15, 0, ""MOROCCO"")
+      .addRow(16, 0, ""MOZAMBIQUE"")
+      .addRow(17, 1, ""PERU"")
+      .addRow(18, 2, ""CHINA"")
+      .addRow(19, 3, ""ROMANIA"")
+      .addRow(20, 4, ""SAUDI ARABIA"")
+      .addRow(21, 2, ""VIETNAM"")
+      .addRow(22, 3, ""RUSSIA"")
+      .addRow(23, 3, ""UNITED KINGDOM"")
+      .addRow(24, 1, ""UNITED STATES"")
+      .build();
+
+    new RowSetComparison(expected).verifyAndClearAll(sets);
+  }
+
+  @Test
+  public void testFilterPushDown() throws Exception {
+    String query = ""select n_name, n_nationkey from drill.%s where n_nationkey = 0"";
+    queryBuilder()
+        .sql(query, TABLE_NAME)
+        .planMatcher()
+        .include(""WHERE"")
+        .exclude(""Filter"")
+        .match();
+
+    testBuilder()
+      .unOrdered()
+      .sqlQuery(query, TABLE_NAME)
+      .baselineColumns(""n_name"", ""n_nationkey"")
+      .baselineValues(""ALGERIA"", 0)
+      .go();
+  }
+
+  @Test
+  public void testFilterPushDownWithJoin() throws Exception {
+    String query = ""select * from drill.%s e\n"" +
+        ""join drill.%s s on e.n_nationkey = s.n_nationkey where e.n_name = 'BRAZIL'"";
+
+    queryBuilder()
+        .sql(query, TABLE_NAME, TABLE_NAME)
+        .planMatcher()
+        .include(""INNER JOIN"")
+        .match();
+
+    testBuilder()
+      .ordered()
+      .sqlQuery(query, TABLE_NAME, TABLE_NAME)
+      .baselineColumns(""n_nationkey"", ""n_name"", ""n_regionkey"", ""n_comment"", ""n_nationkey0"",
+        ""n_name0"", ""n_regionkey0"", ""n_comment0"")
+      .baselineValues(2, ""BRAZIL"", 1, ""y alongside of the pending deposits. carefully special "" +
+        ""packages are about the ironic forges. slyly special "", 2, ""BRAZIL"", 1, ""y alongside of "" +
+        ""the pending deposits. carefully special packages are about the ironic forges. slyly special "")
+      .go();
+  }
+
+  @Test
+  public void testJoinDifferentDrillPlugins() throws Exception {
+    String query = ""select * from drill.%s e\n"" +
+      ""join drill2.cp.`tpch/nation.parquet` s on e.n_nationkey = s.n_nationkey where e.n_name = 'BRAZIL'"";
+
+    queryBuilder()
+      .sql(query, TABLE_NAME, TABLE_NAME)
+      .planMatcher()
+      .include(""HashJoin"")
+      .exclude(""INNER JOIN"")
+      .match();
+
+    testBuilder()
+      .unOrdered()
+      .sqlQuery(query, TABLE_NAME, TABLE_NAME)
+      .baselineColumns(""n_nationkey"", ""n_name"", ""n_regionkey"", ""n_comment"", ""n_nationkey0"",
+        ""n_name0"", ""n_regionkey0"", ""n_comment0"")
+      .baselineValues(2, ""BRAZIL"", 1, ""y alongside of the pending deposits. carefully special "" +
+        ""packages are about the ironic forges. slyly special "", 2, ""BRAZIL"", 1, ""y alongside of "" +
+        ""the pending deposits. carefully special packages are about the ironic forges. slyly special "")
+      .go();
+  }
+
+  @Test
+  public void testAggregationPushDown() throws Exception {
+    String query = ""select count(*) c from drill.%s"";
+    queryBuilder()
+        .sql(query, TABLE_NAME)
+        .planMatcher()
+        .include(""query=\""SELECT COUNT\\(\\*\\)"")
+        .match();
+
+    testBuilder()
+      .unOrdered()
+      .sqlQuery(query, TABLE_NAME)
+      .baselineColumns(""c"")
+      .baselineValues(25L)
+      .go();
+  }
+
+  @Test
+  public void testLimitPushDown() throws Exception {
+    String query = ""select n_name from drill.%s FETCH NEXT 1 ROWS ONLY"";
+    queryBuilder()
+        .sql(query, TABLE_NAME)
+        .planMatcher()
+        .include(""FETCH NEXT 1 ROWS ONLY"")
+        .match();
+
+    testBuilder()
+      .unOrdered()
+      .sqlQuery(query, TABLE_NAME)
+      .baselineColumns(""n_name"")
+      .baselineValues(""ALGERIA"")
+      .go();
+  }
+
+  @Test
+  public void testLimitWithSortPushDown() throws Exception {
+    String query = ""select n_nationkey from drill.%s order by n_name limit 3"";
+    queryBuilder()
+        .sql(query, TABLE_NAME)
+        .planMatcher()
+        .include(""ORDER BY `n_name`"", ""FETCH NEXT 3 ROWS ONLY"")
+        .match();
+
+    testBuilder()
+      .unOrdered()
+      .sqlQuery(query, TABLE_NAME)
+      .baselineColumns(""n_nationkey"")
+      .baselineValues(0)
+      .baselineValues(1)
+      .baselineValues(2)
+      .go();
+  }
+
+  @Test
+  public void testAggregationWithGroupByPushDown() throws Exception {
+    String query = ""select sum(n_nationkey) s from drill.%s group by n_regionkey"";
+    queryBuilder()
+        .sql(query, TABLE_NAME)
+        .planMatcher()
+        .include(""query=\""SELECT SUM\\(`n_nationkey`\\)"", ""GROUP BY `n_regionkey`"")
+        .match();
+
+    testBuilder()
+      .unOrdered()
+      .sqlQuery(query, TABLE_NAME)
+      .baselineColumns(""s"")
+      .baselineValues(47L)
+      .baselineValues(50L)
+      .baselineValues(58L)
+      .baselineValues(68L)
+      .baselineValues(77L)
+      .go();
+  }
+
+  @Test
+  public void testUnionAllPushDown() throws Exception {
+    String query = ""select col1, col2 from drill.%s "" +
+      ""union all "" +
+      ""select col1, col2 from drill.%s"";
+    queryBuilder()
+      .sql(query, TABLE_NAME, TABLE_NAME)
+      .planMatcher()
+      .include(""UNION ALL"")
+      .match();
+
+    long recordCount = queryBuilder()
+      .sql(query, TABLE_NAME, TABLE_NAME)
+      .run()
+      .recordCount();
+
+    assertEquals(50L, recordCount);
+  }","[{'comment': 'Can we have a test of a schema path that descends through directories in a filesystem plugin on the remote Drill cluster? E.g.\r\n```\r\nselect * from drill.`dfs.tmp`.`/path/to/foo.parquet`\r\n```', 'commenter': 'jnturton'}, {'comment': ""Yes, I'll add a unit test for it in one of the future pull requests."", 'commenter': 'vvysotskyi'}]"
2710,contrib/format-xml/src/main/java/org/apache/drill/exec/store/xml/XMLReader.java,"@@ -428,8 +435,67 @@ private void writeFieldData(String fieldName, String fieldValue, TupleWriter wri
       index = writer.addColumn(colSchema);
     }
     ScalarWriter colWriter = writer.scalar(index);
+    ColumnMetadata columnMetadata = writer.tupleSchema().metadata(index);
+    MinorType dataType = columnMetadata.schema().getType().getMinorType();
+    String dateFormat;
+
+    // Write the values depending on their data type.  This only applies to scalar fields.
     if (fieldValue != null && (currentState != xmlState.ROW_ENDED && currentState != xmlState.FIELD_ENDED)) {
-      colWriter.setString(fieldValue);
+      switch (dataType) {
+        case BIT:
+          colWriter.setBoolean(Boolean.parseBoolean(fieldValue));
+          break;
+        case TINYINT:
+        case SMALLINT:
+        case INT:
+          colWriter.setInt(Integer.parseInt(fieldValue));
+          break;
+        case BIGINT:
+          colWriter.setLong(Long.parseLong(fieldValue));
+          break;
+        case FLOAT4:
+        case FLOAT8:
+          colWriter.setDouble(Double.parseDouble(fieldValue));
+          break;
+        case DATE:
+          dateFormat = columnMetadata.property(""drill.format"");
+          LocalDate localDate;
+          if (Strings.isNullOrEmpty(dateFormat)) {
+            localDate = LocalDate.parse(fieldValue);
+          } else {
+            localDate = LocalDate.parse(fieldValue, DateTimeFormatter.ofPattern(dateFormat));
+          }
+          colWriter.setDate(localDate);
+          break;
+        case TIME:
+          dateFormat = columnMetadata.property(""drill.format"");
+          LocalTime localTime;
+          if (Strings.isNullOrEmpty(dateFormat)) {
+            localTime = LocalTime.parse(fieldValue);
+          } else {
+            localTime = LocalTime.parse(fieldValue, DateTimeFormatter.ofPattern(dateFormat));
+          }
+          colWriter.setTime(localTime);
+          break;
+        case TIMESTAMP:
+          dateFormat = columnMetadata.property(""drill.format"");
+          Instant timestamp = null;
+          if (Strings.isNullOrEmpty(dateFormat)) {
+            timestamp = Instant.parse(fieldValue);
+          } else {
+            try {
+              SimpleDateFormat simpleDateFormat = new SimpleDateFormat(dateFormat);
+              Date parsedDate = simpleDateFormat.parse(fieldValue);
+              timestamp = Instant.ofEpochMilli(parsedDate.getTime());
+            } catch (ParseException e) {","[{'comment': ""I think we should operate in one of two definite modes controlled by an option that we try to standardise across plugins.\r\n\r\n1. Invalid data fails hard and fast. Your results will not be silently distorted.\r\n2. Invalid data is swallowed and null is emitted. You know what you're doing and you'll provide appropriate logic for the nulls later.\r\n\r\nUsers that want a default substituted have the simple `ifnull(x,-1)` to combine with (2). My opinion is that if we don't have a `nullInvalidData: true` option set then we should continue to fail the query on the spot, as we do elsewhere."", 'commenter': 'jnturton'}, {'comment': ""Maybe this option is better as a global that is overridable at the session level come to think of it. But until we have such an option I think the default behaviour so far is that we fail on invalid data and we shouldn't muddy things up."", 'commenter': 'jnturton'}, {'comment': '@jnturton In principle I agree.  The code I used for date parsing was borrowed from elsewhere in Drill, so at least the behavior is consistent if not ideal.  Should I create another JIRA to introduce a global/session option for behavior on date/time casts?', 'commenter': 'cgivre'}, {'comment': ""Not only dates and times but numerics and strings when invalid UTF-8 sequences are encountered! I created [DRILL-8361](https://issues.apache.org/jira/browse/DRILL-8361) for this.\r\n\r\nI see we slipped up in the PDF plugin and we have two places where invalid data swallowing is present. Let's make both of them (XML and PDF) consistent with the rest of Drill (do not swallow invalid data) in this PR, and do the nulling of invalid data properly in 8361?"", 'commenter': 'jnturton'}, {'comment': '@jnturton Thanks for the review comments.  I fixed this here and also in the PDF reader.', 'commenter': 'cgivre'}]"
2714,exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BoxFileSystem.java,"@@ -0,0 +1,459 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.dfs;
+
+import com.box.sdk.BoxAPIConnection;
+import com.box.sdk.BoxFile;
+import com.box.sdk.BoxFolder;
+import com.box.sdk.BoxFolder.Info;
+import com.box.sdk.BoxItem;
+import com.box.sdk.BoxSearch;
+import com.box.sdk.BoxSearchParameters;
+import com.box.sdk.PartialCollection;
+import org.apache.commons.io.FilenameUtils;
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.logical.security.CredentialsProvider;
+import org.apache.drill.exec.oauth.PersistentTokenTable;
+import org.apache.drill.exec.store.security.oauth.OAuthTokenCredentials;
+import org.apache.drill.exec.store.security.oauth.OAuthTokenCredentials.Builder;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.util.Progressable;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.ByteArrayOutputStream;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+
+public class BoxFileSystem extends OAuthEnabledFileSystem {
+
+  private static final Logger logger = LoggerFactory.getLogger(BoxFileSystem.class);
+  private static final String TIMEOUT_DEFAULT = ""5000"";
+  private static final List<String> SEARCH_CONTENT_TYPES = new ArrayList<>(Collections.singletonList(""name""));
+  private Path workingDirectory;
+  private BoxAPIConnection client;
+  private String workingDirectoryID;
+  private BoxFolder rootFolder;
+  private boolean usesDeveloperToken;
+  private final List<String> ancestorFolderIDs = new ArrayList<>();
+  private final Map<Path, BoxItem> itemCache = new HashMap<>();
+
+  /**
+   * Returns a URI which identifies this FileSystem.
+   *
+   * @return the URI of this filesystem.
+   */
+  @Override
+  public URI getUri() {
+    try {
+      return new URI(""box:///"");
+    } catch (URISyntaxException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  /**
+   * Opens an FSDataInputStream at the indicated Path.
+   *
+   * @param inputPath the file name to open
+   * @param bufferSize the size of the buffer to be used.
+   * @throws IOException IO failure
+   */
+  @Override
+  public FSDataInputStream open(Path inputPath, int bufferSize) throws IOException {
+    client = getClient();
+    ByteArrayOutputStream out = new ByteArrayOutputStream();
+
+    BoxItem item = getItem(inputPath);
+    if (item instanceof BoxFile) {
+      BoxFile file = (BoxFile) getItem(inputPath);
+      updateTokens();
+
+      file.download(out);
+      updateTokens();
+
+      FSDataInputStream fsDataInputStream = new FSDataInputStream(new SeekableByteArrayInputStream(out.toByteArray()));","[{'comment': ""We're buffering query data into heap memory here, something we don't want to do, but I've just created DRILL-8367 so that we work through all of the places where this is done in a separate exercise."", 'commenter': 'jnturton'}]"
2714,exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BoxFileSystem.java,"@@ -0,0 +1,459 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.dfs;
+
+import com.box.sdk.BoxAPIConnection;
+import com.box.sdk.BoxFile;
+import com.box.sdk.BoxFolder;
+import com.box.sdk.BoxFolder.Info;
+import com.box.sdk.BoxItem;
+import com.box.sdk.BoxSearch;
+import com.box.sdk.BoxSearchParameters;
+import com.box.sdk.PartialCollection;
+import org.apache.commons.io.FilenameUtils;
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.logical.security.CredentialsProvider;
+import org.apache.drill.exec.oauth.PersistentTokenTable;
+import org.apache.drill.exec.store.security.oauth.OAuthTokenCredentials;
+import org.apache.drill.exec.store.security.oauth.OAuthTokenCredentials.Builder;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.util.Progressable;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.ByteArrayOutputStream;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+
+public class BoxFileSystem extends OAuthEnabledFileSystem {
+
+  private static final Logger logger = LoggerFactory.getLogger(BoxFileSystem.class);
+  private static final String TIMEOUT_DEFAULT = ""5000"";
+  private static final List<String> SEARCH_CONTENT_TYPES = new ArrayList<>(Collections.singletonList(""name""));
+  private Path workingDirectory;
+  private BoxAPIConnection client;
+  private String workingDirectoryID;
+  private BoxFolder rootFolder;
+  private boolean usesDeveloperToken;
+  private final List<String> ancestorFolderIDs = new ArrayList<>();
+  private final Map<Path, BoxItem> itemCache = new HashMap<>();
+
+  /**
+   * Returns a URI which identifies this FileSystem.
+   *
+   * @return the URI of this filesystem.
+   */
+  @Override
+  public URI getUri() {
+    try {
+      return new URI(""box:///"");
+    } catch (URISyntaxException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  /**
+   * Opens an FSDataInputStream at the indicated Path.
+   *
+   * @param inputPath the file name to open
+   * @param bufferSize the size of the buffer to be used.
+   * @throws IOException IO failure
+   */
+  @Override
+  public FSDataInputStream open(Path inputPath, int bufferSize) throws IOException {
+    client = getClient();
+    ByteArrayOutputStream out = new ByteArrayOutputStream();
+
+    BoxItem item = getItem(inputPath);
+    if (item instanceof BoxFile) {
+      BoxFile file = (BoxFile) getItem(inputPath);","[{'comment': '```suggestion\r\n      BoxFile file = (BoxFile) item;\r\n```', 'commenter': 'jnturton'}, {'comment': 'Fixed', 'commenter': 'cgivre'}]"
2714,exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/SeekableByteArrayInputStream.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.dfs;
+
+import org.apache.hadoop.fs.PositionedReadable;
+import org.apache.hadoop.fs.Seekable;
+
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+
+public class SeekableByteArrayInputStream extends ByteArrayInputStream implements Seekable, PositionedReadable {","[{'comment': ""There's already a SeekableBAIS in the codebase which I think will do the same thing without adding a class. In Drill-8367 we can remove the use of that."", 'commenter': 'jnturton'}, {'comment': '@jnturton \r\nI was able to replace this in the Dropbox reader, however the Box reader did not work.   Since there is additional work planned in Drill-8367, is it ok to leave this as is and we will fix it in the context of Drill-8367?', 'commenter': 'cgivre'}, {'comment': ""Okay let's do it that way."", 'commenter': 'jnturton'}]"
2714,exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemSchemaFactory.java,"@@ -82,13 +83,21 @@ public class FileSystemSchema extends AbstractSchema {
     public FileSystemSchema(String name, SchemaConfig schemaConfig) throws IOException {
       super(Collections.emptyList(), name);
       final DrillFileSystem fs = ImpersonationUtil.createFileSystem(schemaConfig.getUserName(), plugin.getFsConf());
+      // Set OAuth Information
+      OAuthConfig oAuthConfig = plugin.getConfig().oAuthConfig();
+      if (oAuthConfig != null) {
+        OAuthEnabledFileSystem underlyingFileSystem = (OAuthEnabledFileSystem) fs.getUnderlyingFs();","[{'comment': 'Last question from me - would it work out cleaner to make OAuthEnabledFileSystem inherit from DrillFileSystem? In particular, could that elimnate this new getUnderlyingFs method()? Or it would cause trouble elsewhere?', 'commenter': 'jnturton'}, {'comment': ""@jnturton Good question.  I think that may be possible but with a lot of refactoring.  I don't fully understand the file system creation process, but in following the flow, I do think that would involve a lot of refactoring. \r\n\r\nOn an unrelated note, Hadoop seems to ship with other classes which extend `FileSystem` such as FTP, SFTP and a few others. It may be possible for Drill to query those by simply adding a few import statements."", 'commenter': 'cgivre'}]"
2725,contrib/format-ltsv/pom.xml,"@@ -36,6 +36,11 @@
       <artifactId>drill-java-exec</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <dependency>
+      <groupId>com.github.lonely-lockley</groupId>
+      <artifactId>ltsv-parser</artifactId>","[{'comment': ""Reviewer's note: this library is under the Apache 2.0 license"", 'commenter': 'jnturton'}]"
2725,contrib/format-ltsv/src/main/java/org/apache/drill/exec/store/ltsv/LTSVBatchReader.java,"@@ -0,0 +1,264 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.ltsv;
+
+import com.github.lolo.ltsv.LtsvParser;
+import com.github.lolo.ltsv.LtsvParser.Builder;
+import org.apache.commons.lang3.StringUtils;
+import org.apache.drill.common.AutoCloseables;
+import org.apache.drill.common.exceptions.CustomErrorContext;
+import org.apache.drill.common.exceptions.UserException;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.physical.impl.scan.v3.ManagedReader;
+import org.apache.drill.exec.physical.impl.scan.v3.file.FileDescrip;
+import org.apache.drill.exec.physical.impl.scan.v3.file.FileSchemaNegotiator;
+import org.apache.drill.exec.physical.resultSet.ResultSetLoader;
+import org.apache.drill.exec.physical.resultSet.RowSetLoader;
+import org.apache.drill.exec.record.metadata.ColumnMetadata;
+import org.apache.drill.exec.record.metadata.MetadataUtils;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.exec.vector.accessor.ScalarWriter;
+import org.apache.drill.shaded.guava.com.google.common.base.Strings;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.format.DateTimeFormatter;
+import java.util.Date;
+import java.util.Iterator;
+import java.util.Map;
+
+public class LTSVBatchReader implements ManagedReader {
+
+  private static final Logger logger = LoggerFactory.getLogger(LTSVBatchReader.class);
+  private final LTSVFormatPluginConfig config;
+  private final FileDescrip file;
+  private final CustomErrorContext errorContext;
+  private final LtsvParser ltsvParser;
+  private final RowSetLoader rowWriter;
+  private final FileSchemaNegotiator negotiator;
+  private InputStream fsStream;
+  private Iterator<Map<String, String>> rowIterator;
+
+
+  public LTSVBatchReader(LTSVFormatPluginConfig config, FileSchemaNegotiator negotiator) {
+    this.config = config;
+    this.negotiator = negotiator;
+    file = negotiator.file();
+    errorContext = negotiator.parentErrorContext();
+    ltsvParser = buildParser();
+
+    openFile();
+
+    // If there is a provided schema, import it
+    if (negotiator.providedSchema() != null) {
+      TupleMetadata schema = negotiator.providedSchema();
+      negotiator.tableSchema(schema, false);
+    }
+    ResultSetLoader loader = negotiator.build();
+    rowWriter = loader.writer();
+
+  }
+
+  private void openFile() {
+    try {
+      fsStream = file.fileSystem().openPossiblyCompressedStream(file.split().getPath());
+    } catch (IOException e) {
+      throw UserException
+          .dataReadError(e)
+          .message(""Unable to open LTSV File %s"", file.split().getPath() + "" "" + e.getMessage())
+          .addContext(errorContext)
+          .build(logger);
+    }
+    rowIterator = ltsvParser.parse(fsStream);
+  }
+
+  @Override
+  public boolean next() {
+    while (!rowWriter.isFull()) {
+      if (!processNextRow()) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  private LtsvParser buildParser() {
+    Builder builder = LtsvParser.builder();
+    builder.trimKeys();
+    builder.trimValues();
+    builder.skipNullValues();
+
+    if (config.getParseMode().contentEquals(""strict"")) {
+      builder.strict();
+    } else {
+      builder.lenient();
+    }
+
+    if (StringUtils.isNotEmpty(config.getEscapeCharacter())) {
+      builder.withEscapeChar(config.getEscapeCharacter().charAt(0));
+    }
+
+    if (StringUtils.isNotEmpty(config.getKvDelimiter())) {
+      builder.withKvDelimiter(config.getKvDelimiter().charAt(0));
+    }
+
+    if (StringUtils.isNotEmpty(config.getEntryDelimiter())) {
+      builder.withEntryDelimiter(config.getEntryDelimiter().charAt(0));
+    }
+
+    if (StringUtils.isNotEmpty(config.getLineEnding())) {
+      builder.withLineEnding(config.getLineEnding().charAt(0));
+    }
+
+    if (StringUtils.isNotEmpty(config.getQuoteChar())) {
+      builder.withQuoteChar(config.getQuoteChar().charAt(0));
+    }
+
+    return builder.build();
+  }
+
+  private boolean processNextRow() {
+    if (!rowIterator.hasNext()) {
+      return false;
+    }
+    // Start the row
+    String key;
+    String value;
+    int columnIndex;
+    ScalarWriter columnWriter;
+    Map<String, String> row = rowIterator.next();
+
+    // Skip empty lines
+    if (row.isEmpty()) {
+      return true;
+    }
+    rowWriter.start();
+    for (Map.Entry<String,String> field: row.entrySet()) {
+      key = field.getKey();
+      value = field.getValue();
+      columnIndex = getColumnIndex(key);
+      columnWriter = getColumnWriter(key);
+
+
+      if (negotiator.providedSchema() != null) {
+        // Check the type. LTSV will only read other data types if a schema is provided.
+        ColumnMetadata columnMetadata = rowWriter.tupleSchema().metadata(columnIndex);
+        MinorType dataType = columnMetadata.type();
+        LocalTime localTime;
+        LocalDate localDate;
+
+        switch (dataType) {
+          case BIT:
+            columnWriter.setBoolean(Boolean.parseBoolean(value));
+            break;
+          case INT:
+          case SMALLINT:
+          case TINYINT:
+            columnWriter.setInt(Integer.parseInt(value));
+            break;
+          case BIGINT:
+            columnWriter.setLong(Long.parseLong(value));
+            break;
+          case FLOAT8:
+          case FLOAT4:
+            columnWriter.setDouble(Double.parseDouble(value));
+            break;
+          case TIME:
+            columnMetadata = rowWriter.tupleSchema().metadata(key);
+            String dateFormat = columnMetadata.property(""drill.format"");
+
+            if (Strings.isNullOrEmpty(dateFormat)) {
+              localTime = LocalTime.parse(value);
+            } else {
+              localTime = LocalTime.parse(value, DateTimeFormatter.ofPattern(dateFormat));
+            }
+            columnWriter.setTime(localTime);
+            break;
+          case DATE:
+            dateFormat = columnMetadata.property(""drill.format"");
+
+            if (Strings.isNullOrEmpty(dateFormat)) {
+              localDate = LocalDate.parse(value);
+            } else {
+              localDate = LocalDate.parse(value, DateTimeFormatter.ofPattern(dateFormat));
+            }
+            columnWriter.setDate(localDate);
+            break;
+          case TIMESTAMP:
+            dateFormat = columnMetadata.property(""drill.format"");
+            Instant timestamp;
+            if (Strings.isNullOrEmpty(dateFormat)) {
+              timestamp = Instant.parse(value);
+            } else {
+              try {
+                SimpleDateFormat simpleDateFormat = new SimpleDateFormat(dateFormat);
+                Date parsedDate = simpleDateFormat.parse(value);
+                timestamp = Instant.ofEpochMilli(parsedDate.getTime());
+              } catch (ParseException e) {
+                throw UserException.parseError(e)
+                    .message(""Cannot parse "" + value + "" as a timestamp. You can specify a format string in the provided schema to correct this."")
+                    .addContext(errorContext)
+                    .build(logger);
+              }
+            }
+            columnWriter.setTimestamp(timestamp);
+            break;
+          default:
+            columnWriter.setString(value);
+        }
+      } else {
+        columnWriter.setString(value);
+      }
+    }
+    // Finish the row
+    rowWriter.save();
+    return true;
+  }
+
+  @Override
+  public void close() {
+    AutoCloseables.closeSilently(fsStream);","[{'comment': ""Can I propose that we add a log output line at the debug level because we're freeing an operating resource here? We don't always log that, but I think we should because it can help someone pouring through logs while chasing a resource consumption problem."", 'commenter': 'jnturton'}, {'comment': 'Good idea.  Done.', 'commenter': 'cgivre'}]"
2725,contrib/format-ltsv/src/test/java/org/apache/drill/exec/store/ltsv/TestLTSVRecordReader.java,"@@ -37,34 +42,77 @@ public static void setup() throws Exception {
 
   @Test
   public void testWildcard() throws Exception {
-    testBuilder()
-      .sqlQuery(""SELECT * FROM cp.`simple.ltsv`"")
-      .unOrdered()
-      .baselineColumns(""host"", ""forwardedfor"", ""req"", ""status"", ""size"", ""referer"", ""ua"", ""reqtime"", ""apptime"", ""vhost"")
-      .baselineValues(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/xxx HTTP/1.1"", ""200"", ""4968"", ""-"", ""Java/1.8.0_131"", ""2.532"", ""2.532"", ""api.example.com"")
-      .baselineValues(""xxx.xxx.xxx.xxx"", ""-"", ""GET /v1/yyy HTTP/1.1"", ""200"", ""412"", ""-"", ""Java/1.8.0_201"", ""3.580"", ""3.580"", ""api.example.com"")
-      .go();
+    String sql = ""SELECT * FROM cp.`simple.ltsv`"";","[{'comment': ""Let's rename this class TestLTSVQueries or similar now that LTSVRecordReader is gone?"", 'commenter': 'jnturton'}, {'comment': 'Done', 'commenter': 'cgivre'}]"
2729,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/DistributionFunctions.java,"@@ -0,0 +1,335 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.udfs;
+
+import org.apache.drill.exec.expr.DrillAggFunc;
+import org.apache.drill.exec.expr.DrillSimpleFunc;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate.FunctionScope;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate.NullHandling;
+import org.apache.drill.exec.expr.annotations.Output;
+import org.apache.drill.exec.expr.annotations.Param;
+import org.apache.drill.exec.expr.annotations.Workspace;
+import org.apache.drill.exec.expr.holders.Float8Holder;
+import org.apache.drill.exec.expr.holders.IntHolder;
+
+public class DistributionFunctions {
+
+  @FunctionTemplate(names = {""width_bucket"", ""widthBucket""},
+      scope = FunctionScope.SIMPLE,
+      nulls = NullHandling.NULL_IF_NULL)
+  public static class WidthBucketFunction implements DrillSimpleFunc {
+
+    @Param
+    Float8Holder inputValue;
+
+    @Param
+    Float8Holder MinRangeValueHolder;
+
+    @Param
+    Float8Holder MaxRangeValueHolder;
+
+    @Param
+    IntHolder bucketCountHolder;
+
+    @Workspace
+    double binWidth;
+
+    @Output
+    IntHolder bucket;
+
+    @Override
+    public void setup() {
+      double max = MaxRangeValueHolder.value;
+      double min = MinRangeValueHolder.value;
+      int bucketCount = bucketCountHolder.value;
+      binWidth = (max - min) / bucketCount;
+    }
+
+    @Override
+    public void eval() {
+      // There is probably a more elegant way of doing this...
+      double binFloor = MinRangeValueHolder.value;
+      double binCeiling = binFloor + binWidth;
+
+      for (int i = 1; i <= bucketCountHolder.value; i++) {
+        if (inputValue.value <= binCeiling && inputValue.value > binFloor) {
+           bucket.value = i;
+           break;
+        } else {
+          binFloor = binCeiling;
+          binCeiling = binWidth * (i + 1);
+        }
+      }","[{'comment': '```suggestion\r\n  // We define the buckets as the partition of the range by right half open intervals [ x, x + width )\r\n  bucket.value = 1 + (inputValue.value - MinRangeValueHolder.value) / binWidth\r\n  // TODO: decide on behaviour if the result above is less than 1 or more than bucketCount. Return it as is? Error? Null?\r\n```', 'commenter': 'jnturton'}, {'comment': '@jnturton I looked at the docs for PostgreSQL (which is where this function is modeled after), and saw that in PostgreSQL, if the result is less than the bucket count, it goes into bucket `0` and if it is larger than the range, it goes into bucket `n+1`. \r\n\r\n', 'commenter': 'cgivre'}]"
2729,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/DistributionFunctions.java,"@@ -0,0 +1,335 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.udfs;
+
+import org.apache.drill.exec.expr.DrillAggFunc;
+import org.apache.drill.exec.expr.DrillSimpleFunc;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate.FunctionScope;
+import org.apache.drill.exec.expr.annotations.FunctionTemplate.NullHandling;
+import org.apache.drill.exec.expr.annotations.Output;
+import org.apache.drill.exec.expr.annotations.Param;
+import org.apache.drill.exec.expr.annotations.Workspace;
+import org.apache.drill.exec.expr.holders.Float8Holder;
+import org.apache.drill.exec.expr.holders.IntHolder;
+
+public class DistributionFunctions {
+
+  @FunctionTemplate(names = {""width_bucket"", ""widthBucket""},
+      scope = FunctionScope.SIMPLE,
+      nulls = NullHandling.NULL_IF_NULL)
+  public static class WidthBucketFunction implements DrillSimpleFunc {
+
+    @Param
+    Float8Holder inputValue;
+
+    @Param
+    Float8Holder MinRangeValueHolder;
+
+    @Param
+    Float8Holder MaxRangeValueHolder;
+
+    @Param
+    IntHolder bucketCountHolder;
+
+    @Workspace
+    double binWidth;
+
+    @Output
+    IntHolder bucket;
+
+    @Override
+    public void setup() {
+      double max = MaxRangeValueHolder.value;
+      double min = MinRangeValueHolder.value;
+      int bucketCount = bucketCountHolder.value;
+      binWidth = (max - min) / bucketCount;
+    }
+
+    @Override
+    public void eval() {
+      // There is probably a more elegant way of doing this...
+      double binFloor = MinRangeValueHolder.value;
+      double binCeiling = binFloor + binWidth;
+
+      for (int i = 1; i <= bucketCountHolder.value; i++) {
+        if (inputValue.value <= binCeiling && inputValue.value > binFloor) {
+           bucket.value = i;
+           break;
+        } else {
+          binFloor = binCeiling;
+          binCeiling = binWidth * (i + 1);
+        }
+      }
+    }
+  }
+
+  @FunctionTemplate(
+      names = {""kendall_correlation"",""kendallCorrelation"", ""kendallTau"", ""kendall_tau""},
+      scope = FunctionScope.POINT_AGGREGATE,
+      nulls = NullHandling.INTERNAL
+  )
+  public static class KendallTauFunction implements DrillAggFunc {
+    @Param
+    Float8Holder xInput;
+
+    @Param
+    Float8Holder yInput;
+
+    @Workspace
+    Float8Holder prevXValue;
+
+    @Workspace
+    Float8Holder prevYValue;
+
+    @Workspace
+    IntHolder concordantPairs;
+
+    @Workspace
+    IntHolder discordantPairs;
+
+    @Workspace
+    IntHolder n;
+
+    @Output
+    Float8Holder tau;
+
+    @Override
+    public void add() {
+      double xValue = xInput.value;
+      double yValue = yInput.value;
+
+      if (n.value > 0) {
+        if ((xValue > prevXValue.value && yValue > prevYValue.value) || (xValue < prevXValue.value && yValue < prevYValue.value)) {
+          concordantPairs.value = concordantPairs.value + 1;
+        } else if ((xValue > prevXValue.value && yValue < prevYValue.value) || (xValue < prevXValue.value && yValue > prevYValue.value)) {
+          discordantPairs.value = discordantPairs.value + 1;
+        } else {
+          //Tie...
+        }
+
+        prevXValue.value = xInput.value;
+        prevYValue.value = yInput.value;
+        n.value = n.value + 1;","[{'comment': 'Given that xValue = xInput.value and yValue = yInput.value, I think this code is common to both branches of the parent if statement.', 'commenter': 'jnturton'}, {'comment': 'Fixed.', 'commenter': 'cgivre'}]"
2729,contrib/udfs/src/main/java/org/apache/drill/exec/udfs/DistributionFunctions.java,"@@ -51,31 +51,29 @@ public static class WidthBucketFunction implements DrillSimpleFunc {
     @Workspace
     double binWidth;
 
+    @Workspace
+    int bucketCount;
+
     @Output
     IntHolder bucket;
 
     @Override
     public void setup() {
       double max = MaxRangeValueHolder.value;
       double min = MinRangeValueHolder.value;
-      int bucketCount = bucketCountHolder.value;
+      bucketCount = bucketCountHolder.value;
       binWidth = (max - min) / bucketCount;
     }
 
     @Override
     public void eval() {
-      // There is probably a more elegant way of doing this...
-      double binFloor = MinRangeValueHolder.value;
-      double binCeiling = binFloor + binWidth;
-
-      for (int i = 1; i <= bucketCountHolder.value; i++) {
-        if (inputValue.value <= binCeiling && inputValue.value > binFloor) {
-           bucket.value = i;
-           break;
-        } else {
-          binFloor = binCeiling;
-          binCeiling = binWidth * (i + 1);
-        }
+      if (inputValue.value < MinRangeValueHolder.value) {
+        bucket.value = 0;
+      } else if (inputValue.value > MaxRangeValueHolder.value) {
+        bucket.value = bucketCount + 1;
+      } else {
+        double f = (1 + (inputValue.value - MinRangeValueHolder.value) / binWidth);","[{'comment': 'It looks like `f` is recomputed rather than used in what follows.', 'commenter': 'jnturton'}, {'comment': 'Oops... That was a test variable.  Removed. ', 'commenter': 'cgivre'}]"
2733,exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java,"@@ -403,8 +404,24 @@ private View getView(DotDrillFile f) throws IOException {
       return f.getView(mapper);
     }
 
+    private String getTemporaryName(String name) {
+      if (isTemporaryWorkspace()) {
+        String tableName = DrillStringUtils.removeLeadingSlash(name);
+        return schemaConfig.getTemporaryTableName(tableName);
+      }
+      return null;
+    }
+
+    private boolean isTemporaryWorkspace() {","[{'comment': ""Could this utility method move to SchemaConfig or SchemaUtilities so that it's available for reuse elsewhere or is that unlikely?"", 'commenter': 'jnturton'}, {'comment': 'I think it is unlikely that it would be reused.', 'commenter': 'vvysotskyi'}]"
2733,exec/java-exec/src/main/java/org/apache/drill/exec/work/metadata/MetadataProvider.java,"@@ -607,6 +608,16 @@ public String getQueryUserName() {
       @Override public UserCredentials getQueryUserCredentials() {
         return session.getCredentials();
       }
+
+      @Override
+      public String getTemporaryTableName(String table) {
+        return session.resolveTemporaryTableName(table);
+      }
+
+      @Override
+      public String getTemporaryWorkspace() {
+        return config.getString(ExecConstants.DEFAULT_TEMPORARY_WORKSPACE);","[{'comment': 'Have I got it right that this config option value is the only value returned by implementations of getTemporaryWorkspace? If so, do we need this method or could its callers look up the config value themselves instead?', 'commenter': 'jnturton'}, {'comment': 'Yes, config is the only source for this property. But I think it is better to have an interface that provides only information related to schema config info rather than allow callers to access config by themselves. The current approach helps to encapsulate it, so I would prefer to leave it as it is. ', 'commenter': 'vvysotskyi'}]"
2733,exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/conversion/DrillCalciteCatalogReader.java,"@@ -135,14 +112,15 @@ public Prepare.PreparingTable getTable(List<String> names) {
   }
 
   private void checkTemporaryTable(List<String> names) {
-    if (allowTemporaryTables) {
+    if (allowTemporaryTables || !needsTemporaryTableCheck(names, session.getDefaultSchemaPath(), drillConfig)) {
       return;
     }
-    String originalTableName = session.getOriginalTableNameFromTemporaryTable(names.get(names.size() - 1));
+    String tableName = names.get(names.size() - 1);
+    String originalTableName = session.resolveTemporaryTableName(tableName);
     if (originalTableName != null) {
       throw UserException
           .validationError()
-          .message(""Temporary tables usage is disallowed. Used temporary table name: [%s]."", originalTableName)
+          .message(""Temporary tables usage is disallowed. Used temporary table name: [%s]."", tableName)","[{'comment': '```suggestion\r\n          .message(""A reference to temporary table [%s] was made in a context where temporary table references are not allowed."", tableName)\r\n```', 'commenter': 'jnturton'}, {'comment': 'Thanks, replaced it.', 'commenter': 'vvysotskyi'}]"
2733,exec/java-exec/src/main/java/org/apache/drill/exec/work/metadata/MetadataProvider.java,"@@ -607,6 +608,16 @@ public String getQueryUserName() {
       @Override public UserCredentials getQueryUserCredentials() {
         return session.getCredentials();
       }
+
+      @Override
+      public String getTemporaryTableName(String table) {","[{'comment': ""This looks like another case where we wouldn't need to keep expanding interfaces like SchemaConfigInfoProvider and adding partial implementations where some throw UnsupportedOperationExceptions if we just had a good way of accessing the UserSession from most layers of Drill. It's not something for this PR for sure, but I wanted to remark to get your opinion since I remember having to work the same way when I was trying to expose UserCredentials (visible above) for user translation in plugins."", 'commenter': 'jnturton'}, {'comment': 'I agree it is not good to have such interfaces with unsupported methods. Ideally, we should split them into several interfaces instead and use broader ones in places where it is required.', 'commenter': 'vvysotskyi'}]"
2747,contrib/storage-http/README.md,"@@ -15,6 +15,7 @@ To configure the plugin, create a new storage plugin, and add the following conf
 {
   ""type"": ""http"",
   ""cacheResults"": true,
+  ""useLegacyRequestParamSyntax"": false,","[{'comment': ""See comment.  I'd like to make the default behavior what Drill currently does and have the user explicitly enable this functionality.  Could we rename this param `advanced` or `enhanced` and thus change the boolean to `enableEnhancedParamSyntax`?"", 'commenter': 'cgivre'}, {'comment': ""Sure, I'll finish it soon."", 'commenter': 'LYCJeff'}]"
2747,contrib/storage-http/README.md,"@@ -138,11 +140,26 @@ key2=value2""
 ```
 
 `postBodyLocation`:  If the API uses the `POST` method, you can send parameters in several different ways:
-* `query_string`:  Parameters from the query are pushed down to the query string.  Static parameters are pushed to the post body.","[{'comment': ""If the user enables this feature, are they required to specify the destination of the supplied parameters?  What happens if they don't?"", 'commenter': 'cgivre'}, {'comment': 'In the enhanced mode, default position is `json_body`.', 'commenter': 'LYCJeff'}]"
2763,docs/dev/ClusterFixture.md,"@@ -125,6 +125,27 @@ In some cases, you may want to change an option in a test. Rather than writing o
 
 Again, you can pass a Java value which the test code will convert to a string, then will build the `ALTER SESSION` command.
 
+# Try-with-resource Style of Creating Single-use Client Fixtures.
+
+The benefit of Cluster Fixture framework is to define specific config for specific clusterFixture and clientFixture as needed flexibly.
+
+In some cases, clusterFixture has been initialized, and we need to create several different config clients for different test cases,","[{'comment': '```suggestion\r\nIn some cases, a clusterFixture has been initialized and we need to create several different config clients for different test cases.\r\n```', 'commenter': 'jnturton'}]"
2763,docs/dev/ClusterFixture.md,"@@ -125,6 +125,27 @@ In some cases, you may want to change an option in a test. Rather than writing o
 
 Again, you can pass a Java value which the test code will convert to a string, then will build the `ALTER SESSION` command.
 
+# Try-with-resource Style of Creating Single-use Client Fixtures.
+
+The benefit of Cluster Fixture framework is to define specific config for specific clusterFixture and clientFixture as needed flexibly.","[{'comment': '```suggestion\r\nA benefit of the Cluster Fixture framework is the ability to define specific configs for specific clusterFixtures and clientFixtures as needed flexibly.\r\n```', 'commenter': 'jnturton'}]"
2763,docs/dev/ClusterFixture.md,"@@ -125,6 +125,27 @@ In some cases, you may want to change an option in a test. Rather than writing o
 
 Again, you can pass a Java value which the test code will convert to a string, then will build the `ALTER SESSION` command.
 
+# Try-with-resource Style of Creating Single-use Client Fixtures.
+
+The benefit of Cluster Fixture framework is to define specific config for specific clusterFixture and clientFixture as needed flexibly.
+
+In some cases, clusterFixture has been initialized, and we need to create several different config clients for different test cases,
+
+We could use try-with-resource style to creating single-use clientFixture.","[{'comment': ""```suggestion\r\nUsing Java's try-with-resources syntax to create a single-use clientFixture is a convenient way to ensure that the clientFixture will automatically be closed once we've finished with it.\r\n```"", 'commenter': 'jnturton'}]"
2763,docs/dev/ClusterFixture.md,"@@ -156,6 +177,28 @@ It is often very handy, during development, to accumulate a collection of test f
 * The (local) file system location
 * The default format
 
+# Exception Matcher
+
+The `QueryBuilder` provides a clean and concise way to handle Exception match which includes type match and pattern match:","[{'comment': '```suggestion\r\nThe `QueryBuilder` provides a clean and concise way to handle UserException matching which includes error type matching and error message pattern matching:\r\n```', 'commenter': 'jnturton'}]"
2763,docs/dev/ClusterFixture.md,"@@ -156,6 +177,28 @@ It is often very handy, during development, to accumulate a collection of test f
 * The (local) file system location
 * The default format
 
+# Exception Matcher
+
+The `QueryBuilder` provides a clean and concise way to handle Exception match which includes type match and pattern match:
+
+```
+    @Test
+    public void unsupportedLiteralValidation() throws Exception {
+      String query = ""ALTER session SET `%s` = %s"";
+
+      client.queryBuilder()
+        .sql(query, ENABLE_VERBOSE_ERRORS_KEY, ""DATE '1995-01-01'"")
+        .userExceptionMatcher()
+        .expectedType(ErrorType.VALIDATION)
+        .include(""Drill doesn't support assigning literals of type"")
+        .match();
+    }
+```
+* Use `.userExceptionMatcher` to call UserExceptionMatcher
+* Use `.expectedType` to define expected Error type
+* Use `.include` to define expected Error pattern","[{'comment': '```suggestion\r\n* Use `.include` to define an expected error message regex pattern\r\n* Use `.exclude` to define an unexpected error message regex pattern\r\n```', 'commenter': 'jnturton'}]"
2800,common/src/test/java/org/apache/drill/test/DrillTest.java,"@@ -37,11 +38,10 @@
 
 public class DrillTest extends BaseTest {
 
-  protected static final ObjectMapper objectMapper;
+  private static final ObjectMapper objectMapper = JacksonUtils.createObjectMapper();","[{'comment': ""Some tests were using this mapper and modifying it - which seems like a dangerous thing to do because other tests will be affected by the modified mapper. I've changed the tests that used this mapper to create their own separate mappers."", 'commenter': 'pjfanning'}, {'comment': 'Nice catch, thank you.', 'commenter': 'jnturton'}]"
2800,exec/java-exec/src/main/java/org/apache/drill/exec/store/http/oauth/OAuthUtils.java,"@@ -36,6 +37,7 @@
 
 public class OAuthUtils {
   private static final Logger logger = LoggerFactory.getLogger(OAuthUtils.class);
+  private static final ObjectMapper MAPPER = JacksonUtils.createObjectMapper();","[{'comment': ""We're converting method scope ObjectMappers to static class members which is efficient in terms of rework but will add a bit to Drill's fixed heap requirement since they can never be collected. It looks like ObjectMappers are thread safe so, for the cases where the caller does not need to do mapper customisation, could we get even better reuse from a new singleton `JacksonUtils.DEFAULT_MAPPER`?"", 'commenter': 'jnturton'}, {'comment': 'Follow up: [this post](https://stackoverflow.com/a/3909846/1153953) suggests that global ObjectReader and ObjectWriter objects might be a better choice than a global ObjectMapper.', 'commenter': 'jnturton'}, {'comment': 'the problem with global object mappers, writers and readers is that if they are public, then someone can modify their config - exactly the issue I found in our test code\r\n\r\nI opened https://issues.apache.org/jira/browse/DRILL-8431 to look at wrapping the Jackson classes to create immutable instances that can be more safely shared. So far, that looks like a lot of work and the benefits may not be worth it.', 'commenter': 'pjfanning'}, {'comment': ""Hmm, reading [on a little](https://stackoverflow.com/a/36162525/1153953) reveals that lock contention in a shared ObjectMapper degrades performance for multithreaded applications . Now I'm in two minds."", 'commenter': 'jnturton'}]"
2824,contrib/format-xml/src/main/java/org/apache/drill/exec/store/xml/xsd/DrillXSDSchemaUtils.java,"@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.xml.xsd;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;
+import org.apache.ws.commons.schema.XmlSchema;
+import org.apache.ws.commons.schema.XmlSchemaCollection;
+import org.apache.ws.commons.schema.XmlSchemaElement;
+
+import org.apache.ws.commons.schema.XmlSchemaObject;
+import org.apache.ws.commons.schema.walker.XmlSchemaWalker;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.xml.transform.stream.StreamSource;
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+
+public class DrillXSDSchemaUtils {
+  private static final MinorType DEFAULT_TYPE = MinorType.VARCHAR;
+  private static final Logger logger = LoggerFactory.getLogger(DrillXSDSchemaUtils.class);
+
+  /**
+   * This map maps the data types defined by the XSD definition to Drill data types.
+   */
+  public static final ImmutableMap<String, MinorType> XML_TYPE_MAPPINGS = ImmutableMap.<String, MinorType>builder()
+    .put(""BASE64BINARY"", MinorType.VARBINARY)
+    .put(""BOOLEAN"", MinorType.BIT)
+    .put(""DATE"", MinorType.DATE)
+    .put(""DATETIME"", MinorType.TIMESTAMP)
+    .put(""DECIMAL"", MinorType.VARDECIMAL)
+    .put(""DOUBLE"", MinorType.FLOAT8)
+    .put(""DURATION"", MinorType.INTERVAL)
+    .put(""FLOAT"", MinorType.FLOAT4)
+    .put(""HEXBINARY"", MinorType.VARBINARY)
+    .put(""STRING"", MinorType.VARCHAR)
+    .put(""TIME"", MinorType.TIME)
+    .build();
+
+  /**
+   * This function is only used for testing, but accepts a XSD file as input rather than a {@link InputStream}
+   * @param filename A {@link String} containing an XSD file.
+   * @return A {@link TupleMetadata} containing a Drill representation of the XSD schema.
+   * @throws IOException If anything goes wrong or the file is not found.
+   */
+  public static TupleMetadata getSchema(String filename) throws IOException {","[{'comment': '```suggestion\r\n  @VisibleForTesting\r\n  public static TupleMetadata getSchema(String filename) throws IOException {\r\n```', 'commenter': 'jnturton'}, {'comment': 'Done.', 'commenter': 'cgivre'}]"
2824,contrib/format-xml/src/test/resources/logback-test.xml,"@@ -0,0 +1,69 @@
+<?xml version=""1.0"" encoding=""UTF-8"" ?>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    ""License""); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<configuration>
+  <if condition='property(""drill.lilith.enable"").equalsIgnoreCase(""true"")'>
+    <then>
+      <appender name=""SOCKET"" class=""de.huxhorn.lilith.logback.appender.ClassicMultiplexSocketAppender"">
+        <Compressing>true</Compressing>
+        <ReconnectionDelay>10000</ReconnectionDelay>
+        <IncludeCallerData>true</IncludeCallerData>
+        <RemoteHosts>${LILITH_HOSTNAME:-localhost}</RemoteHosts>
+      </appender>","[{'comment': ""I haven't seen this before - can you explain what it's for?"", 'commenter': 'jnturton'}, {'comment': 'This is a file which should have not been included in the PR. ;-). ', 'commenter': 'cgivre'}]"
2824,contrib/format-xml/src/test/java/org/apache/drill/exec/store/xml/xsd/TestXSDSchema.java,"@@ -0,0 +1,124 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.xml.xsd;
+
+import org.apache.drill.common.types.TypeProtos.MinorType;
+import org.apache.drill.common.util.DrillFileUtils;
+import org.apache.drill.exec.record.metadata.MapBuilder;
+import org.apache.drill.exec.record.metadata.SchemaBuilder;
+import org.apache.drill.exec.record.metadata.TupleMetadata;
+import org.junit.Test;
+
+import java.io.File;
+
+import static org.junit.jupiter.api.Assertions.assertTrue;
+
+public class TestXSDSchema {
+
+  @Test
+  public void testSimpleXSD() throws Exception {
+    File simple_xsd = DrillFileUtils.getResourceAsFile(""/xsd/simple.xsd"");
+    TupleMetadata schema = DrillXSDSchemaUtils.getSchema(simple_xsd.getPath());
+
+    TupleMetadata expectedSchema  = new SchemaBuilder()
+        .addMap(""shiporder"")
+          .addMap(""attributes"")
+            .addNullable(""orderid"", MinorType.VARCHAR)
+          .resumeMap()
+          .addNullable(""orderperson"", MinorType.VARCHAR)
+          .addMap(""shipto"")
+            .addNullable(""name"", MinorType.VARCHAR)
+            .addNullable(""address"", MinorType.VARCHAR)
+            .addNullable(""city"", MinorType.VARCHAR)
+            .addNullable(""country"", MinorType.VARCHAR)
+        .resumeMap()
+          .addMapArray(""item"")
+            .addNullable(""title"", MinorType.VARCHAR)
+            .addNullable(""note"", MinorType.VARCHAR)
+            .addNullable(""quantity"", MinorType.VARDECIMAL)
+            .addNullable(""price"", MinorType.VARDECIMAL)
+          .resumeMap()
+        .resumeSchema()
+      .buildSchema();
+    assertTrue(expectedSchema.isEquivalent(schema));
+  }
+
+
+  @Test
+  public void testComplexXSD() throws Exception {
+    File complex_xsd = DrillFileUtils.getResourceAsFile(""/xsd/complex.xsd"");
+    TupleMetadata schema = DrillXSDSchemaUtils.getSchema(complex_xsd.getPath());
+
+    SchemaBuilder sb1 = new SchemaBuilder();
+    MapBuilder sb2 = sb1
+        .addNullable(""comment"", MinorType.VARCHAR) // global comment element
+        .addMap(""infoType"")
+          .addMap(""attributes"")
+            .addNullable(""kind"", MinorType.VARCHAR)
+          .resumeMap()
+        .resumeSchema()
+        .addMap(""purchaseOrder"") // global purchaseOrder element
+          .addMap(""attributes"")
+            .addNullable(""orderDate"", MinorType.DATE) // an attribute
+            .addNullable(""confirmDate"", MinorType.DATE) // an attribute
+          .resumeMap()
+          .addMap(""shipTo"")
+            .addMap(""attributes"")
+              .addNullable(""country"", MinorType.VARCHAR) // an attribute
+            .resumeMap()
+            .addNullable(""name"", MinorType.VARCHAR)
+            .addNullable(""street"", MinorType.VARCHAR)
+            .addNullable(""city"", MinorType.VARCHAR)
+            .addNullable(""state"", MinorType.VARCHAR)
+            .addNullable(""zip"", MinorType.VARDECIMAL)
+          .resumeMap(); // end shipTo
+    MapBuilder sb3 = sb2
+          .addMap(""billTo"")
+            .addMap(""attributes"")
+              .addNullable(""country"", MinorType.VARCHAR) // an attribute
+            .resumeMap()
+            .addNullable(""name"", MinorType.VARCHAR)
+            .addNullable(""street"", MinorType.VARCHAR)
+             .addNullable(""city"", MinorType.VARCHAR)
+            .addNullable(""state"", MinorType.VARCHAR)
+            .addNullable(""zip"", MinorType.VARDECIMAL)
+          .resumeMap();
+    MapBuilder sb4 = sb3
+          .addNullable(""comment"", MinorType.VARCHAR)
+          .addMap(""items"")
+            .addMapArray(""item"")
+              .addMap(""attributes"")
+                .addNullable(""partNum"", MinorType.VARCHAR) // an attribute
+             .resumeMap()
+              .addNullable(""productName"", MinorType.VARCHAR)
+              .addNullable(""quantity"", MinorType.VARDECIMAL)
+              .addNullable(""USPrice"", MinorType.VARDECIMAL)
+              .addNullable(""comment"", MinorType.VARCHAR)
+              .addNullable(""shipDate"", MinorType.DATE)
+            .resumeMap() // end item
+          .resumeMap(); // end items
+
+    TupleMetadata expectedSchema = sb4.resumeSchema().build();
+
+    System.out.println(schema);
+    System.out.println(expectedSchema);
+","[{'comment': 'Are these still meant to be present?', 'commenter': 'jnturton'}, {'comment': 'Oops... Nope.', 'commenter': 'cgivre'}]"
2824,pom.xml,"@@ -689,6 +690,34 @@
               <exclude>**/*.accdb</exclude>
               <exclude>**/*.access_log</exclude>
               <exclude>**/.asf.yaml</exclude>
+              <exclude>**/*.woff2</exclude>","[{'comment': 'This list was recently sorted alphabetically in master, can we retain that?', 'commenter': 'jnturton'}, {'comment': 'Done!', 'commenter': 'cgivre'}]"
2840,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpCSVOptions.java,"@@ -0,0 +1,287 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;
+
+import java.util.Objects;
+
+@JsonInclude(JsonInclude.Include.NON_DEFAULT)
+@JsonDeserialize(builder = HttpCSVOptions.HttpCSVOptionsBuilder.class)
+public class HttpCSVOptions {
+
+
+  @JsonProperty
+  private final String delimiter;
+
+  @JsonProperty
+  private final char quote;
+
+  @JsonProperty
+  private final char quoteEscape;
+
+  @JsonProperty
+  private final String lineSeparator;
+
+  @JsonProperty
+  private final Boolean headerExtractionEnabled;
+
+  @JsonProperty
+  private final long numberOfRowsToSkip;
+
+  @JsonProperty
+  private final long numberOfRecordsToRead;
+
+  @JsonProperty
+  private final boolean lineSeparatorDetectionEnabled;
+
+  @JsonProperty
+  private final int maxColumns;
+
+  @JsonProperty
+  private final int maxCharsPerColumn;
+
+  @JsonProperty
+  private final boolean skipEmptyLines;
+
+  @JsonProperty
+  private final boolean ignoreLeadingWhitespaces;
+
+  @JsonProperty
+  private final boolean ignoreTrailingWhitespaces;
+
+  @JsonProperty
+  private final String nullValue;
+
+  HttpCSVOptions(HttpCSVOptionsBuilder builder) {
+    this.delimiter = builder.delimiter;
+    this.quote = builder.quote;
+    this.quoteEscape = builder.quoteEscape;
+    this.lineSeparator = builder.lineSeparator;
+    this.headerExtractionEnabled = builder.headerExtractionEnabled;
+    this.numberOfRowsToSkip = builder.numberOfRowsToSkip;
+    this.numberOfRecordsToRead = builder.numberOfRecordsToRead;
+    this.lineSeparatorDetectionEnabled = builder.lineSeparatorDetectionEnabled;
+    this.maxColumns = builder.maxColumns;
+    this.maxCharsPerColumn = builder.maxCharsPerColumn;
+    this.skipEmptyLines = builder.skipEmptyLines;
+    this.ignoreLeadingWhitespaces = builder.ignoreLeadingWhitespaces;
+    this.ignoreTrailingWhitespaces = builder.ignoreTrailingWhitespaces;
+    this.nullValue = builder.nullValue;
+  }
+
+  public static HttpCSVOptionsBuilder builder() {
+    return new HttpCSVOptionsBuilder();
+  }
+
+  public String getDelimiter() {
+    return delimiter;
+  }
+
+  public char getQuote() {
+    return quote;
+  }
+
+  public char getQuoteEscape() {
+    return quoteEscape;
+  }
+
+  public String getLineSeparator() {
+    return lineSeparator;
+  }
+
+  public Boolean getHeaderExtractionEnabled() {
+    return headerExtractionEnabled;
+  }
+
+  public long getNumberOfRowsToSkip() {
+    return numberOfRowsToSkip;
+  }
+
+  public long getNumberOfRecordsToRead() {
+    return numberOfRecordsToRead;
+  }
+
+  public boolean isLineSeparatorDetectionEnabled() {
+    return lineSeparatorDetectionEnabled;
+  }
+
+  public int getMaxColumns() {
+    return maxColumns;
+  }
+
+  public int getMaxCharsPerColumn() {
+    return maxCharsPerColumn;
+  }
+
+  public boolean isSkipEmptyLines() {
+    return skipEmptyLines;
+  }
+
+  public boolean isIgnoreLeadingWhitespaces() {
+    return ignoreLeadingWhitespaces;
+  }
+
+  public boolean isIgnoreTrailingWhitespaces() {
+    return ignoreTrailingWhitespaces;
+  }
+
+  public String getNullValue() {
+    return nullValue;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) {
+      return true;
+    }
+    if (o == null || getClass() != o.getClass()) {
+      return false;
+    }
+    HttpCSVOptions that = (HttpCSVOptions) o;
+    return quote == that.quote && quoteEscape == that.quoteEscape && numberOfRowsToSkip == that.numberOfRowsToSkip && numberOfRecordsToRead == that.numberOfRecordsToRead && lineSeparatorDetectionEnabled == that.lineSeparatorDetectionEnabled && maxColumns == that.maxColumns && maxCharsPerColumn == that.maxCharsPerColumn && skipEmptyLines == that.skipEmptyLines && ignoreLeadingWhitespaces == that.ignoreLeadingWhitespaces && ignoreTrailingWhitespaces == that.ignoreTrailingWhitespaces && delimiter.equals(that.delimiter) && lineSeparator.equals(that.lineSeparator) && Objects.equals(headerExtractionEnabled, that.headerExtractionEnabled) && nullValue.equals(that.nullValue);","[{'comment': 'Nit:  Please break this up into new lines. ', 'commenter': 'cgivre'}]"
2840,contrib/storage-http/src/main/java/org/apache/drill/exec/store/http/HttpCSVOptions.java,"@@ -0,0 +1,287 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.drill.exec.store.http;
+
+
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
+import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;
+
+import java.util.Objects;
+
+@JsonInclude(JsonInclude.Include.NON_DEFAULT)
+@JsonDeserialize(builder = HttpCSVOptions.HttpCSVOptionsBuilder.class)
+public class HttpCSVOptions {
+
+
+  @JsonProperty
+  private final String delimiter;
+
+  @JsonProperty
+  private final char quote;
+
+  @JsonProperty
+  private final char quoteEscape;
+
+  @JsonProperty
+  private final String lineSeparator;
+
+  @JsonProperty
+  private final Boolean headerExtractionEnabled;
+
+  @JsonProperty
+  private final long numberOfRowsToSkip;
+
+  @JsonProperty
+  private final long numberOfRecordsToRead;
+
+  @JsonProperty
+  private final boolean lineSeparatorDetectionEnabled;
+
+  @JsonProperty
+  private final int maxColumns;
+
+  @JsonProperty
+  private final int maxCharsPerColumn;
+
+  @JsonProperty
+  private final boolean skipEmptyLines;
+
+  @JsonProperty
+  private final boolean ignoreLeadingWhitespaces;
+
+  @JsonProperty
+  private final boolean ignoreTrailingWhitespaces;
+
+  @JsonProperty
+  private final String nullValue;
+
+  HttpCSVOptions(HttpCSVOptionsBuilder builder) {
+    this.delimiter = builder.delimiter;
+    this.quote = builder.quote;
+    this.quoteEscape = builder.quoteEscape;
+    this.lineSeparator = builder.lineSeparator;
+    this.headerExtractionEnabled = builder.headerExtractionEnabled;
+    this.numberOfRowsToSkip = builder.numberOfRowsToSkip;
+    this.numberOfRecordsToRead = builder.numberOfRecordsToRead;
+    this.lineSeparatorDetectionEnabled = builder.lineSeparatorDetectionEnabled;
+    this.maxColumns = builder.maxColumns;
+    this.maxCharsPerColumn = builder.maxCharsPerColumn;
+    this.skipEmptyLines = builder.skipEmptyLines;
+    this.ignoreLeadingWhitespaces = builder.ignoreLeadingWhitespaces;
+    this.ignoreTrailingWhitespaces = builder.ignoreTrailingWhitespaces;
+    this.nullValue = builder.nullValue;
+  }
+
+  public static HttpCSVOptionsBuilder builder() {
+    return new HttpCSVOptionsBuilder();
+  }
+
+  public String getDelimiter() {
+    return delimiter;
+  }
+
+  public char getQuote() {
+    return quote;
+  }
+
+  public char getQuoteEscape() {
+    return quoteEscape;
+  }
+
+  public String getLineSeparator() {
+    return lineSeparator;
+  }
+
+  public Boolean getHeaderExtractionEnabled() {
+    return headerExtractionEnabled;
+  }
+
+  public long getNumberOfRowsToSkip() {
+    return numberOfRowsToSkip;
+  }
+
+  public long getNumberOfRecordsToRead() {
+    return numberOfRecordsToRead;
+  }
+
+  public boolean isLineSeparatorDetectionEnabled() {
+    return lineSeparatorDetectionEnabled;
+  }
+
+  public int getMaxColumns() {
+    return maxColumns;
+  }
+
+  public int getMaxCharsPerColumn() {
+    return maxCharsPerColumn;
+  }
+
+  public boolean isSkipEmptyLines() {
+    return skipEmptyLines;
+  }
+
+  public boolean isIgnoreLeadingWhitespaces() {
+    return ignoreLeadingWhitespaces;
+  }
+
+  public boolean isIgnoreTrailingWhitespaces() {
+    return ignoreTrailingWhitespaces;
+  }
+
+  public String getNullValue() {
+    return nullValue;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) {
+      return true;
+    }
+    if (o == null || getClass() != o.getClass()) {
+      return false;
+    }
+    HttpCSVOptions that = (HttpCSVOptions) o;
+    return quote == that.quote && quoteEscape == that.quoteEscape && numberOfRowsToSkip == that.numberOfRowsToSkip && numberOfRecordsToRead == that.numberOfRecordsToRead && lineSeparatorDetectionEnabled == that.lineSeparatorDetectionEnabled && maxColumns == that.maxColumns && maxCharsPerColumn == that.maxCharsPerColumn && skipEmptyLines == that.skipEmptyLines && ignoreLeadingWhitespaces == that.ignoreLeadingWhitespaces && ignoreTrailingWhitespaces == that.ignoreTrailingWhitespaces && delimiter.equals(that.delimiter) && lineSeparator.equals(that.lineSeparator) && Objects.equals(headerExtractionEnabled, that.headerExtractionEnabled) && nullValue.equals(that.nullValue);
+  }
+
+  @Override
+  public int hashCode() {
+    return Objects.hash(delimiter, quote, quoteEscape, lineSeparator, headerExtractionEnabled,
+        numberOfRowsToSkip, numberOfRecordsToRead, lineSeparatorDetectionEnabled, maxColumns,
+        maxCharsPerColumn, skipEmptyLines, ignoreLeadingWhitespaces, ignoreTrailingWhitespaces,
+        nullValue);
+  }
+
+  @Override
+  public String toString() {
+    return ""HttpCSVOptions{"" + ""delimiter='"" + delimiter + '\'' + "", quote="" + quote + "", "" +","[{'comment': 'Nit:  Please use the `PlanStringBuilder` for the `toString()` method.', 'commenter': 'cgivre'}]"
