Pull,Path,Diff_hunk,Comment
161,src/java/org/apache/cassandra/auth/CassandraRoleManager.java,"@@ -448,10 +449,6 @@ private void removeAllMembers(String role) throws RequestValidationException, Re
                 consistencyForRole(role));
     }
 
-    /*
-     * Convert a map of Options from a CREATE/ALTER statement into
-     * assignment clauses used to construct a CQL UPDATE statement
-     */","[{'comment': 'why delete this comment?', 'commenter': 'clohfink'}]"
161,src/java/org/apache/cassandra/cql3/CQL3Type.java,"@@ -826,13 +827,18 @@ public CQL3Type prepare(String keyspace, Types udts) throws InvalidRequestExcept
                     freeze();
 
                 List<AbstractType<?>> ts = new ArrayList<>(types.size());
-                for (CQL3Type.Raw t : types)
-                {
-                    if (t.isCounter())
-                        throw new InvalidRequestException(""Counters are not allowed inside tuples"");
-
-                    ts.add(t.prepare(keyspace, udts).getType());
-                }
+                types.stream()
+                        .map(
+                                t -> {
+                                    if (t.isCounter())
+                                        throw new InvalidRequestException(
+                                                ""Counters are not allowed inside tuples"");
+                                    return t;
+                                })
+                        .forEach(
+                                t -> {
+                                    ts.add(t.prepare(keyspace, udts).getType());
+                                });","[{'comment': 'This looks worse, and if your going to convert to a new list you can just do the mapping like\r\n\r\n```\r\ntypes.stream().map(t ->\r\n{\r\n    if (t.isCounter())\r\n        throw new InvalidRequestException(""Counters are not allowed inside tuples"");\r\n    return t.prepare(keyspace, udts).getType()\r\n}\r\n```', 'commenter': 'clohfink'}]"
212,src/java/org/apache/cassandra/config/Config.java,"@@ -376,9 +376,31 @@
 
     public String full_query_log_dir = null;
 
-    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
-    public int block_for_peers_percentage = 70;
+    /**
+     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
+     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
+     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
+     * to ensure that startup is not delayed too much.
+     *
+     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
+     *
+     *     Consistency Level | local_dc     | all_dcs
+     *     --------------------------------------------------------
+     *     LOCAL_ONE         | default (2)  | default (any)
+     *     LOCAL_QUORUM      | 1            | default (any)
+     *     ONE               | any          | RF - 1
+     *     QUORUM            | any          | (RF / 2) - 1
+     *     ALL               | default      | 0
+     *
+     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
+     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
+     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
+     */
+    public int block_for_peers_local_dc = 2;","[{'comment': ""If the goal is to not return unavailable shouldn't the default really be 1 since LOCAL_QUORUM and QUORUM are more common? I feel like if this doesn't do what people want with the defaults it makes it a lot less useful?\r\n\r\nAll this does is add 10 seconds to startup which seems really minor. A part of me even wonders why fuss with any of this if we are just going to wait 10 seconds? Just wait until everything is up and if it doesn't happen in 10 seconds continue to do what we were going to do anyways?\r\n\r\nThe connection priming stuff I get. That is good and I'm glad we added that.\r\n\r\nThe name also doesn't really make sense anymore since this is not the number we are blocking for it's the number we aren't blocking for."", 'commenter': 'aweisberg'}, {'comment': ""> If the goal is to not return unavailable shouldn't the default really be 1 since LOCAL_QUORUM and QUORUM are more common? I feel like if this doesn't do what people want with the defaults it makes it a lot less useful?\r\n\r\nI thought that defaulting to handling the case that the drivers default to `LOCAL_ONE` was most sensible, although I can make it `1` if you prefer and say the defaults are for `LOCAL_QUORUM` (I don't have strong opinions other than we shouldn't be waiting by default on remote DCs).\r\n\r\n> All this does is add 10 seconds to startup which seems really minor. A part of me even wonders why fuss with any of this if we are just going to wait 10 seconds? Just wait until everything is up and if it doesn't happen in 10 seconds continue to do what we were going to do anyways?\r\n\r\nIn practice you wait way less than 10 seconds (e.g. on the test 200 node 4.0 cluster we handshake with the whole local DC in about ... 500ms). I personally think that most users would rather their database wait O(minutes) than throw errors in the general case, but from the earlier conversations in CASSANDRA-13993 it seemed like folks were hesitant to wait so long on startup e.g. when doing ccm clusters or the such.\r\n\r\n> The name also doesn't really make sense anymore since this is not the number we are blocking for it's the number we aren't blocking for.\r\n\r\nHow about ... `startup_max_down_local_dc_peers` and `startup_max_down_peers`?\r\n\r\n\r\n"", 'commenter': 'jolynch'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -39,7 +42,9 @@
 import org.apache.cassandra.locator.InetAddressAndPort;
 import org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType;
 import org.apache.cassandra.utils.FBUtilities;
+import org.jboss.byteman.synchronization.CountDown;","[{'comment': 'Unused import', 'commenter': 'aweisberg'}, {'comment': 'ack', 'commenter': 'jolynch'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null)","[{'comment': ""This change appears to remove the ability to disable this? It's an interesting change because it means we will always prime the connections which sounds kind of useful.\r\n\r\nThe most you can do is crank down the timeout or set the number of hosts you won't wait for very high which has a similar impact."", 'commenter': 'aweisberg'}, {'comment': 'Yea that was my idea, that we should always prime the connections but we can just not wait (if you set the local dc option to a really large number then it basically primes and immediately returns).', 'commenter': 'jolynch'}, {'comment': ""In the latest version I added the ability to disable this by setting a negative timeout. I guess it's probably a good idea to have an opt out option since we're post freeze."", 'commenter': 'jolynch'}]"
212,src/java/org/apache/cassandra/config/Config.java,"@@ -376,9 +376,31 @@
 
     public String full_query_log_dir = null;
 
-    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
-    public int block_for_peers_percentage = 70;
+    /**
+     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
+     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
+     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
+     * to ensure that startup is not delayed too much.
+     *
+     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
+     *
+     *     Consistency Level | local_dc     | all_dcs
+     *     --------------------------------------------------------
+     *     LOCAL_ONE         | default (2)  | default (any)
+     *     LOCAL_QUORUM      | 1            | default (any)
+     *     ONE               | any          | RF - 1
+     *     QUORUM            | any          | (RF / 2) - 1
+     *     ALL               | default      | 0
+     *
+     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
+     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
+     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
+     */
+    public int block_for_peers_local_dc = 2;
+    public int block_for_peers_all_dcs = Integer.MAX_VALUE;","[{'comment': ""QUORUM is at least a little common? Seems like defaulting to no waiting for remote DCs might not be the right default? It's a question of which do people dislike more? Getting unavailables when a node restarts or potentially waiting extra time on startup?"", 'commenter': 'aweisberg'}, {'comment': 'I agree and 10s isn\'t that big a deal (imo) for users that don\'t want to wait for remote dcs, but I was trying to compromise between the old settings (70% == could literally have a whole DC down with 3 DCs) which appear to be motivated from a perspective of ""don\'t block my startup"" and what I personally think that we shouldn\'t be saying we\'re ready to coordinate until we\'re actually ready to coordinate.\r\n\r\nI am happy to put whatever default gets this merged ;-) We\'ll probably internally be setting the local setting to `1` (so `LOCAL_QUORUM`) and the remote to a really large number. But that\'s just our perspective...', 'commenter': 'jolynch'}, {'comment': ""This one of those things where I want to ask around and see if we can get away with just waiting 10 seconds if a node is down.\r\n\r\nSeems like the solution for CCM is to just have CCM set the config it wants. I'm also wondering if CCM actually waits the 10 seconds anyways just because it starts one node at a time and waits for a specific log message a lot of the time and I'm not sure if that message is before or after the connectivity checker.\r\n\r\nI have to go take a look at how CCM works. "", 'commenter': 'aweisberg'}, {'comment': 'I played around with CCM and it appears to work fine (starting a cluster, stopping it, starting and stopping nodes etc). I think we should be good on this front.', 'commenter': 'jolynch'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null)
             return true;
 
         // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
         peers = new HashSet<>(peers);
-        peers.remove(FBUtilities.getBroadcastAddressAndPort());
-
-        if (peers.isEmpty())
+        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
+        if (peers.size() == 1 && peers.contains(localAddress))
             return true;
 
-        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
-                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        Set<InetAddressAndPort> myLocalPeers = peers.stream()
+                                                    .collect(Collectors.groupingBy(getDatacenter, toSet()))
+                                                    .getOrDefault(getDatacenter.apply(FBUtilities.getBroadcastAddressAndPort()),
+                                                                  Collections.emptySet());
+
+
+        logger.info(""choosing to block until no more than {}/{} local and no more than {}/{} global peers are still DOWN; max time to wait = {} seconds"",","[{'comment': ""This logs the local node as a peer since you aren't removing it. Seems like it's not a desired side effect."", 'commenter': 'aweisberg'}, {'comment': ""Yea ... I was debating if this should be here or down below 122 where we remove it. I was thinking that users would want to see their cluster size in the denominator (e.g. `choosing to block until no more than 1/200` nodes) even if strictly speaking we just immediately succeed on the local host.\r\n\r\nI don't have strong preferences, I'll take the local node out of the count."", 'commenter': 'jolynch'}, {'comment': 'I changed it to remove the local peer like before :-)', 'commenter': 'jolynch'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -17,13 +17,16 @@
  */
 package org.apache.cassandra.net;
 
+import java.util.Collections;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
+import java.util.function.Function;
+import java.util.stream.Collectors;
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.Sets;","[{'comment': 'Unused import', 'commenter': 'aweisberg'}, {'comment': 'ack', 'commenter': 'jolynch'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -213,4 +261,11 @@ boolean incrementAndCheck(InetAddressAndPort address)
             return acks.computeIfAbsent(address, addr -> new AtomicInteger(0)).incrementAndGet() == threshold;
         }
     }
+
+    private String fmtBlocker(Integer size, Integer count)","[{'comment': 'Is this a more complicated Math.max?', 'commenter': 'aweisberg'}, {'comment': ""Heh, yea it is I just thought that it was slightly clearer what it was doing (so we don't print `Integer.MAX_VALUE` to the log. I'll change it to `Math.max` instead."", 'commenter': 'jolynch'}, {'comment': 'I killed it.', 'commenter': 'jolynch'}]"
212,src/java/org/apache/cassandra/config/Config.java,"@@ -380,9 +380,23 @@
     public RepairCommandPoolFullStrategy repair_command_pool_full_strategy = RepairCommandPoolFullStrategy.queue;
     public int repair_command_pool_size = concurrent_validations;
 
-    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
-    public int block_for_peers_percentage = 70;
+    /**
+     * When a node first starts up it intially considers all other peers as DOWN, and then as the initial gossip
+     * broadcast messages comes back nodes transition to UP. These options configure how long we wait for peers to
+     * connect before we make this node available as a coordinator. Furthermore, if this feature is enabled
+     * (timeout >= 0) Cassandra initiates the non gossip channel internode connections on startup as well and waits","[{'comment': ""This part of the description doesn't match the description for block_for_peers_timeot_in_secs. It sounds like now it differentiates between disabled, and prime the connections."", 'commenter': 'aweisberg'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -48,81 +51,133 @@
 {
     private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
 
-    private final int targetPercent;
+    private final boolean blockForRemoteDcs;
     private final long timeoutNanos;
 
-    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
+    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
     {
-        timeoutSecs = Math.max(1, timeoutSecs);
+        if (timeoutSecs < 0)
+            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +","[{'comment': 'Should this be a warning? They have pretty clearly asked for it?', 'commenter': 'aweisberg'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -48,81 +51,133 @@
 {
     private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
 
-    private final int targetPercent;
+    private final boolean blockForRemoteDcs;
     private final long timeoutNanos;
 
-    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
+    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
     {
-        timeoutSecs = Math.max(1, timeoutSecs);
+        if (timeoutSecs < 0)
+            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
+                        "" the first user query"");
         if (timeoutSecs > 100)
             logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
         long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
 
-        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
+        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
     }
 
     @VisibleForTesting
-    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
+    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
     {
-        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
+        this.blockForRemoteDcs = blockForRemoteDcs;
         this.timeoutNanos = timeoutNanos;
     }
 
     /**
      * @param peers The currently known peers in the cluster; argument is not modified.
+     * @param getDatacenterSource A function for mapping peers to their datacenter.
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null || this.timeoutNanos < 0)
             return true;
 
         // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
         peers = new HashSet<>(peers);
-        peers.remove(FBUtilities.getBroadcastAddressAndPort());
+        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
+        String localDc = getDatacenterSource.apply(localAddress);
 
+        peers.remove(localAddress);
         if (peers.isEmpty())
             return true;
 
-        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
-                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
+        Map<InetAddressAndPort, String> datacenterMap = peers.stream()","[{'comment': ""Bikeshedding, but datacenterMap is not very descriptive. What it is is a peerToDatacenter. You can also omit mpa it's kind of redundant with XtoY which automatically implies it's a map relationship. Multimaps are handles by making Y plural. I really think it's worth using better naming here.\r\n\r\nAlso pulling out getDataCenter is a few characters more succinct, but I would rather see XtoY::get repeated a few times. If it's not referring to the source then the name also needs to be more representative like getDatacenterFromPeer which is basically just another way of saying peerToDatacenter::get."", 'commenter': 'aweisberg'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -48,81 +51,133 @@
 {
     private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
 
-    private final int targetPercent;
+    private final boolean blockForRemoteDcs;
     private final long timeoutNanos;
 
-    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
+    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
     {
-        timeoutSecs = Math.max(1, timeoutSecs);
+        if (timeoutSecs < 0)
+            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
+                        "" the first user query"");
         if (timeoutSecs > 100)
             logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
         long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
 
-        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
+        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
     }
 
     @VisibleForTesting
-    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
+    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
     {
-        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
+        this.blockForRemoteDcs = blockForRemoteDcs;
         this.timeoutNanos = timeoutNanos;
     }
 
     /**
      * @param peers The currently known peers in the cluster; argument is not modified.
+     * @param getDatacenterSource A function for mapping peers to their datacenter.
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null || this.timeoutNanos < 0)
             return true;
 
         // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
         peers = new HashSet<>(peers);
-        peers.remove(FBUtilities.getBroadcastAddressAndPort());
+        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
+        String localDc = getDatacenterSource.apply(localAddress);
 
+        peers.remove(localAddress);
         if (peers.isEmpty())
             return true;
 
-        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
-                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
+        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
+                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
+        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
 
-        long startNanos = System.nanoTime();
+        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()","[{'comment': 'Could this be a set multimap?', 'commenter': 'aweisberg'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -48,81 +51,133 @@
 {
     private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
 
-    private final int targetPercent;
+    private final boolean blockForRemoteDcs;
     private final long timeoutNanos;
 
-    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
+    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
     {
-        timeoutSecs = Math.max(1, timeoutSecs);
+        if (timeoutSecs < 0)
+            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
+                        "" the first user query"");
         if (timeoutSecs > 100)
             logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
         long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
 
-        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
+        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
     }
 
     @VisibleForTesting
-    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
+    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
     {
-        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
+        this.blockForRemoteDcs = blockForRemoteDcs;
         this.timeoutNanos = timeoutNanos;
     }
 
     /**
      * @param peers The currently known peers in the cluster; argument is not modified.
+     * @param getDatacenterSource A function for mapping peers to their datacenter.
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null || this.timeoutNanos < 0)
             return true;
 
         // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
         peers = new HashSet<>(peers);
-        peers.remove(FBUtilities.getBroadcastAddressAndPort());
+        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
+        String localDc = getDatacenterSource.apply(localAddress);
 
+        peers.remove(localAddress);
         if (peers.isEmpty())
             return true;
 
-        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
-                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
+        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
+                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
+        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
 
-        long startNanos = System.nanoTime();
+        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
+                                                              .collect(Collectors.groupingBy(getDatacenter,
+                                                                                             Collectors.toSet()));
+
+        if (!blockForRemoteDcs)
+        {
+            peersByDc.keySet().retainAll(Collections.singleton(localDc));
+            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
+                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        }
+        else
+        {
+            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
+                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        }
 
         AckMap acks = new AckMap(3);
-        int target = (int) ((targetPercent / 100.0) * peers.size());
-        CountDownLatch latch = new CountDownLatch(target);
+        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
+        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
+        {
+            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
+        }
+
+        long startNanos = System.nanoTime();
 
         // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
-        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
-        AliveListener listener = new AliveListener(alivePeers, latch, acks);
+        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
+        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
         Gossiper.instance.register(listener);
 
         // send out a ping message to open up the non-gossip connections
-        sendPingMessages(peers, latch, acks);
+        sendPingMessages(peers, latchMap, acks, getDatacenter);
 
         for (InetAddressAndPort peer : peers)
+        {
             if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
-                latch.countDown();
+            {
+                String datacenter = getDatacenter.apply(peer);
+                if (latchMap.containsKey(datacenter))
+                    latchMap.get(datacenter).countDown();
+            }
+        }
+
+        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);","[{'comment': ""Why do you need the loop and this? Since you removed other DCs from peersByDC shouldn't there already be only the local DC in the latch map?"", 'commenter': 'aweisberg'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -48,81 +51,133 @@
 {
     private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
 
-    private final int targetPercent;
+    private final boolean blockForRemoteDcs;
     private final long timeoutNanos;
 
-    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
+    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
     {
-        timeoutSecs = Math.max(1, timeoutSecs);
+        if (timeoutSecs < 0)
+            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
+                        "" the first user query"");
         if (timeoutSecs > 100)
             logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
         long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
 
-        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
+        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
     }
 
     @VisibleForTesting
-    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
+    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
     {
-        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
+        this.blockForRemoteDcs = blockForRemoteDcs;
         this.timeoutNanos = timeoutNanos;
     }
 
     /**
      * @param peers The currently known peers in the cluster; argument is not modified.
+     * @param getDatacenterSource A function for mapping peers to their datacenter.
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null || this.timeoutNanos < 0)
             return true;
 
         // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
         peers = new HashSet<>(peers);
-        peers.remove(FBUtilities.getBroadcastAddressAndPort());
+        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
+        String localDc = getDatacenterSource.apply(localAddress);
 
+        peers.remove(localAddress);
         if (peers.isEmpty())
             return true;
 
-        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
-                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
+        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
+                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
+        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
 
-        long startNanos = System.nanoTime();
+        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
+                                                              .collect(Collectors.groupingBy(getDatacenter,
+                                                                                             Collectors.toSet()));
+
+        if (!blockForRemoteDcs)
+        {
+            peersByDc.keySet().retainAll(Collections.singleton(localDc));
+            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
+                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        }
+        else
+        {
+            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
+                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        }
 
         AckMap acks = new AckMap(3);
-        int target = (int) ((targetPercent / 100.0) * peers.size());
-        CountDownLatch latch = new CountDownLatch(target);
+        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
+        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
+        {
+            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
+        }
+
+        long startNanos = System.nanoTime();
 
         // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
-        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
-        AliveListener listener = new AliveListener(alivePeers, latch, acks);
+        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
+        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
         Gossiper.instance.register(listener);
 
         // send out a ping message to open up the non-gossip connections
-        sendPingMessages(peers, latch, acks);
+        sendPingMessages(peers, latchMap, acks, getDatacenter);
 
         for (InetAddressAndPort peer : peers)
+        {
             if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
-                latch.countDown();
+            {
+                String datacenter = getDatacenter.apply(peer);
+                if (latchMap.containsKey(datacenter))
+                    latchMap.get(datacenter).countDown();
+            }
+        }
+
+        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
+        for (String datacenter: latchMap.keySet())
+        {
+            if (datacenter.equals(localDc))","[{'comment': ""Then you don't need this exception?"", 'commenter': 'aweisberg'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -48,81 +51,133 @@
 {
     private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
 
-    private final int targetPercent;
+    private final boolean blockForRemoteDcs;
     private final long timeoutNanos;
 
-    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
+    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
     {
-        timeoutSecs = Math.max(1, timeoutSecs);
+        if (timeoutSecs < 0)
+            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
+                        "" the first user query"");
         if (timeoutSecs > 100)
             logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
         long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
 
-        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
+        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
     }
 
     @VisibleForTesting
-    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
+    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
     {
-        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
+        this.blockForRemoteDcs = blockForRemoteDcs;
         this.timeoutNanos = timeoutNanos;
     }
 
     /**
      * @param peers The currently known peers in the cluster; argument is not modified.
+     * @param getDatacenterSource A function for mapping peers to their datacenter.
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null || this.timeoutNanos < 0)
             return true;
 
         // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
         peers = new HashSet<>(peers);
-        peers.remove(FBUtilities.getBroadcastAddressAndPort());
+        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
+        String localDc = getDatacenterSource.apply(localAddress);
 
+        peers.remove(localAddress);
         if (peers.isEmpty())
             return true;
 
-        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
-                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
+        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
+                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
+        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
 
-        long startNanos = System.nanoTime();
+        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
+                                                              .collect(Collectors.groupingBy(getDatacenter,
+                                                                                             Collectors.toSet()));
+
+        if (!blockForRemoteDcs)
+        {
+            peersByDc.keySet().retainAll(Collections.singleton(localDc));
+            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
+                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        }
+        else
+        {
+            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
+                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        }
 
         AckMap acks = new AckMap(3);
-        int target = (int) ((targetPercent / 100.0) * peers.size());
-        CountDownLatch latch = new CountDownLatch(target);
+        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());","[{'comment': 'Another candidate for datacenterToY. Not quite sure of the perfect name for Y. I am fine with using dc if you want shorter.', 'commenter': 'aweisberg'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -48,81 +51,133 @@
 {
     private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
 
-    private final int targetPercent;
+    private final boolean blockForRemoteDcs;
     private final long timeoutNanos;
 
-    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
+    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
     {
-        timeoutSecs = Math.max(1, timeoutSecs);
+        if (timeoutSecs < 0)
+            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
+                        "" the first user query"");
         if (timeoutSecs > 100)
             logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
         long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
 
-        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
+        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
     }
 
     @VisibleForTesting
-    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
+    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
     {
-        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
+        this.blockForRemoteDcs = blockForRemoteDcs;
         this.timeoutNanos = timeoutNanos;
     }
 
     /**
      * @param peers The currently known peers in the cluster; argument is not modified.
+     * @param getDatacenterSource A function for mapping peers to their datacenter.
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null || this.timeoutNanos < 0)
             return true;
 
         // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
         peers = new HashSet<>(peers);
-        peers.remove(FBUtilities.getBroadcastAddressAndPort());
+        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
+        String localDc = getDatacenterSource.apply(localAddress);
 
+        peers.remove(localAddress);
         if (peers.isEmpty())
             return true;
 
-        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
-                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
+        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
+                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
+        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
 
-        long startNanos = System.nanoTime();
+        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
+                                                              .collect(Collectors.groupingBy(getDatacenter,
+                                                                                             Collectors.toSet()));
+
+        if (!blockForRemoteDcs)
+        {
+            peersByDc.keySet().retainAll(Collections.singleton(localDc));","[{'comment': 'This important bit of behavior is tucked away here and I only know about it because I went looking. Can you add a mostly redundant comment mentioning that you remove all but the local DC so that we only wait on nodes in the local DC?', 'commenter': 'aweisberg'}]"
212,src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java,"@@ -48,81 +51,133 @@
 {
     private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
 
-    private final int targetPercent;
+    private final boolean blockForRemoteDcs;
     private final long timeoutNanos;
 
-    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
+    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
     {
-        timeoutSecs = Math.max(1, timeoutSecs);
+        if (timeoutSecs < 0)
+            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
+                        "" the first user query"");
         if (timeoutSecs > 100)
             logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
         long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
 
-        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
+        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
     }
 
     @VisibleForTesting
-    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
+    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
     {
-        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
+        this.blockForRemoteDcs = blockForRemoteDcs;
         this.timeoutNanos = timeoutNanos;
     }
 
     /**
      * @param peers The currently known peers in the cluster; argument is not modified.
+     * @param getDatacenterSource A function for mapping peers to their datacenter.
      * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
      * else false.
      */
-    public boolean execute(Set<InetAddressAndPort> peers)
+    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
     {
-        if (targetPercent == 0 || peers == null)
+        if (peers == null || this.timeoutNanos < 0)
             return true;
 
         // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
         peers = new HashSet<>(peers);
-        peers.remove(FBUtilities.getBroadcastAddressAndPort());
+        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
+        String localDc = getDatacenterSource.apply(localAddress);
 
+        peers.remove(localAddress);
         if (peers.isEmpty())
             return true;
 
-        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
-                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
+        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
+                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
+        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
 
-        long startNanos = System.nanoTime();
+        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
+                                                              .collect(Collectors.groupingBy(getDatacenter,
+                                                                                             Collectors.toSet()));
+
+        if (!blockForRemoteDcs)
+        {
+            peersByDc.keySet().retainAll(Collections.singleton(localDc));
+            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
+                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        }
+        else
+        {
+            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
+                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
+        }
 
         AckMap acks = new AckMap(3);
-        int target = (int) ((targetPercent / 100.0) * peers.size());
-        CountDownLatch latch = new CountDownLatch(target);
+        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
+        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
+        {
+            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
+        }
+
+        long startNanos = System.nanoTime();
 
         // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
-        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
-        AliveListener listener = new AliveListener(alivePeers, latch, acks);
+        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
+        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
         Gossiper.instance.register(listener);
 
         // send out a ping message to open up the non-gossip connections
-        sendPingMessages(peers, latch, acks);
+        sendPingMessages(peers, latchMap, acks, getDatacenter);","[{'comment': ""Comment that pings are sent to all peers even if latchMap doesn't contain a latch for them."", 'commenter': 'aweisberg'}]"
212,test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java,"@@ -36,13 +36,34 @@
 import org.apache.cassandra.gms.Gossiper;
 import org.apache.cassandra.gms.HeartBeatState;
 import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.utils.FBUtilities;
 
 import static org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType.SMALL_MESSAGE;
 
 public class StartupClusterConnectivityCheckerTest
 {
-    private StartupClusterConnectivityChecker connectivityChecker;
+    private StartupClusterConnectivityChecker localQuorumConnectivityChecker;
+    private StartupClusterConnectivityChecker globalQuorumConnectivityChecker;
+    private StartupClusterConnectivityChecker noopChecker;
+    private StartupClusterConnectivityChecker zeroWaitChecker;
+
+    private static final int NUM_PER_DC = 6;
     private Set<InetAddressAndPort> peers;
+    private Set<InetAddressAndPort> peersA;
+    private Set<InetAddressAndPort> peersAMinusLocal;
+    private Set<InetAddressAndPort> peersB;
+    private Set<InetAddressAndPort> peersC;
+
+    private String getDatacenter(InetAddressAndPort endpoint)
+    {
+        if (peersA.contains(endpoint))
+            return ""datacenterA"";
+        if (peersB.contains(endpoint))
+            return ""datacenterB"";
+        else if (peersC.contains(endpoint))
+            return ""datacenterC"";
+        return ""NA"";","[{'comment': 'Should probably return null', 'commenter': 'aweisberg'}]"
212,test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java,"@@ -69,33 +113,102 @@ public void tearDown()
     @Test
     public void execute_HappyPath()
     {
-        Sink sink = new Sink(true, true);
+        Sink sink = new Sink(true, true, peers);
         MessagingService.instance().addMessageSink(sink);
-        Assert.assertTrue(connectivityChecker.execute(peers));
+        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
         checkAllConnectionTypesSeen(sink);
     }
 
     @Test
     public void execute_NotAlive()
     {
-        Sink sink = new Sink(false, true);
+        Sink sink = new Sink(false, true, peers);
         MessagingService.instance().addMessageSink(sink);
-        Assert.assertFalse(connectivityChecker.execute(peers));
+        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
         checkAllConnectionTypesSeen(sink);
     }
 
     @Test
     public void execute_NoConnectionsAcks()
     {
-        Sink sink = new Sink(true, false);
+        Sink sink = new Sink(true, false, peers);
         MessagingService.instance().addMessageSink(sink);
-        Assert.assertFalse(connectivityChecker.execute(peers));
+        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
+    }
+
+    @Test
+    public void execute_LocalQuorum()
+    {
+        // local peer plus 3 peers from same dc shouldn't pass (4/6)
+        Set<InetAddressAndPort> available = new HashSet<>();
+        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
+        checkAvailable(localQuorumConnectivityChecker, available, false, true);
+
+        // local peer plus 4 peers from same dc should pass (5/6)
+        available.clear();
+        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
+        checkAvailable(localQuorumConnectivityChecker, available, true, true);
+    }
+
+    @Test
+    public void execute_GlobalQuorum()
+    {
+        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB","[{'comment': 'typo', 'commenter': 'aweisberg'}]"
224,doc/source/architecture/dynamo.rst,"@@ -74,6 +74,26 @@ nodes in each rack, the data load on the smallest rack may be much higher.  Simi
 into a new rack, it will be considered a replica for the entire ring.  For this reason, many operators choose to
 configure all nodes on a single ""rack"".
 
+.. _transient-replication:
+
+Transient Replication
+~~~~~~~~~~~~~~~~~~~~~
+
+Transient replication allows you to configure a subset of replicas to only replicate data that hasn't been incrementally
+repaired. This allows you to trade data redundancy for storage usage, and increased read and write throughput. For instance,
+if you have a replication factor of 3, with 1 transient replica, 2 replicas will replicate all data for a given token","[{'comment': ""Use the 3 replicas upgraded to 5 case. It's not a tradeoff of redundancy at all. We can't message that at all and it's not true."", 'commenter': 'aweisberg'}]"
224,doc/source/architecture/dynamo.rst,"@@ -74,6 +74,26 @@ nodes in each rack, the data load on the smallest rack may be much higher.  Simi
 into a new rack, it will be considered a replica for the entire ring.  For this reason, many operators choose to
 configure all nodes on a single ""rack"".
 
+.. _transient-replication:
+
+Transient Replication
+~~~~~~~~~~~~~~~~~~~~~
+
+Transient replication allows you to configure a subset of replicas to only replicate data that hasn't been incrementally
+repaired. This allows you to trade data redundancy for storage usage, and increased read and write throughput. For instance,
+if you have a replication factor of 3, with 1 transient replica, 2 replicas will replicate all data for a given token
+range, while the 3rd will only keep data that hasn't been incrementally repaired. Since you're reducing the copies kept
+of data by the number of transient replicas, transient replication is best suited to multiple dc deployments.","[{'comment': ""No you don't need multiple DCs!"", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/cql3/statements/AlterKeyspaceStatement.java,"@@ -96,7 +98,35 @@ private void warnIfIncreasingRF(KeyspaceMetadata ksm, KeyspaceParams params)
                                                                                                         StorageService.instance.getTokenMetadata(),
                                                                                                         DatabaseDescriptor.getEndpointSnitch(),
                                                                                                         params.replication.options);
-        if (newStrategy.getReplicationFactor() > oldStrategy.getReplicationFactor())
+
+        validateTransientReplication(oldStrategy, newStrategy);
+        warnIfIncreasingRF(oldStrategy, newStrategy);
+    }
+
+    private void validateTransientReplication(AbstractReplicationStrategy oldStrategy, AbstractReplicationStrategy newStrategy)
+    {
+        if (oldStrategy.getReplicationFactor().trans == 0 && newStrategy.getReplicationFactor().trans > 0)
+        {
+            Keyspace ks = Keyspace.open(keyspace());
+            for (ColumnFamilyStore cfs: ks.getColumnFamilyStores())
+            {
+                if (cfs.viewManager.hasViews())
+                {
+                    throw new ConfigurationException(""Cannot use transient replication on keyspaces using materialized views"");
+                }
+
+                if (cfs.indexManager.hasIndexes())
+                {
+                    throw new ConfigurationException(""Cannot use transient replication on keyspaces using secondary indexes"");
+                }
+            }
+
+        }
+    }","[{'comment': ""I think we can allow people to increase # of transient replicas (they make it real with nodetool cleanup or repair), but how does decreasing number of transient replicas work safely? I think we should disallow it until we can articulate how and why it is safe. There has to be a temporary pending state where a node is transitioning from transient to full where it receives writes but not reads because it can't correctly service reads as a full replcia."", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/locator/ReplicationFactor.java,"@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.Objects;
+
+import com.google.common.base.Preconditions;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+public class ReplicationFactor
+{
+    public static final ReplicationFactor ZERO = new ReplicationFactor(0);
+
+    public final int trans;
+    public final int replicas;
+    public transient final int full;","[{'comment': 'Why is this field transient?', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/cql3/statements/AlterKeyspaceStatement.java,"@@ -96,7 +98,35 @@ private void warnIfIncreasingRF(KeyspaceMetadata ksm, KeyspaceParams params)
                                                                                                         StorageService.instance.getTokenMetadata(),
                                                                                                         DatabaseDescriptor.getEndpointSnitch(),
                                                                                                         params.replication.options);
-        if (newStrategy.getReplicationFactor() > oldStrategy.getReplicationFactor())
+
+        validateTransientReplication(oldStrategy, newStrategy);
+        warnIfIncreasingRF(oldStrategy, newStrategy);
+    }
+
+    private void validateTransientReplication(AbstractReplicationStrategy oldStrategy, AbstractReplicationStrategy newStrategy)
+    {
+        if (oldStrategy.getReplicationFactor().trans == 0 && newStrategy.getReplicationFactor().trans > 0)
+        {
+            Keyspace ks = Keyspace.open(keyspace());
+            for (ColumnFamilyStore cfs: ks.getColumnFamilyStores())
+            {
+                if (cfs.viewManager.hasViews())
+                {
+                    throw new ConfigurationException(""Cannot use transient replication on keyspaces using materialized views"");
+                }
+
+                if (cfs.indexManager.hasIndexes())
+                {
+                    throw new ConfigurationException(""Cannot use transient replication on keyspaces using secondary indexes"");
+                }
+            }
+
+        }
+    }
+
+    private void warnIfIncreasingRF(AbstractReplicationStrategy oldStrategy, AbstractReplicationStrategy newStrategy)
+    {
+        if (newStrategy.getReplicationFactor().full > oldStrategy.getReplicationFactor().full)","[{'comment': 'Should this be the number of full replicas or the total number of replicas?', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -1591,7 +1589,7 @@ public long getExpectedCompactedFileSize(Iterable<SSTableReader> sstables, Opera
 
         // cleanup size estimation only counts bytes for keys local to this node
         long expectedFileSize = 0;
-        Collection<Range<Token>> ranges = StorageService.instance.getLocalRanges(keyspace.getName());
+        Collection<Range<Token>> ranges = StorageService.instance.getLocalReplicas(keyspace.getName()).asRangeSet();","[{'comment': 'Since this is a collection could you use Collections2.transform instead of materializing a set?', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/batchlog/BatchlogManager.java,"@@ -490,16 +497,16 @@ private static int gcgs(Collection<Mutation> mutations)
         {
             private final Set<InetAddressAndPort> undelivered = Collections.newSetFromMap(new ConcurrentHashMap<>());
 
-            ReplayWriteResponseHandler(Collection<InetAddressAndPort> writeEndpoints, long queryStartNanoTime)
+            ReplayWriteResponseHandler(Replicas writeReplicas, long queryStartNanoTime)
             {
-                super(writeEndpoints, Collections.<InetAddressAndPort>emptySet(), null, null, null, WriteType.UNLOGGED_BATCH, queryStartNanoTime);
-                undelivered.addAll(writeEndpoints);
+                super(writeReplicas, ReplicaList.of(), null, null, null, WriteType.UNLOGGED_BATCH, queryStartNanoTime);
+                Iterables.addAll(undelivered, writeReplicas.asEndpoints());","[{'comment': ""This is a case where it seems like we could call a Collections2.transform version and not have to create a new list. One thing to keep in mind is that Collections2.transform makes contains and remove slow because it's O(n) no matter what the underlying collection is."", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -1868,7 +1866,7 @@ public void compactionDiskSpaceCheck(boolean enable)
 
     public void cleanupCache()
     {
-        Collection<Range<Token>> ranges = StorageService.instance.getLocalRanges(keyspace.getName());
+        Collection<Range<Token>> ranges = StorageService.instance.getLocalReplicas(keyspace.getName()).asRangeSet();","[{'comment': 'Same.', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/ConsistencyLevel.java,"@@ -190,50 +197,50 @@ public int countLocalEndpoints(Iterable<InetAddressAndPort> liveEndpoints)
          * the blockFor first ones).
          */
         if (isDCLocal)
-            liveEndpoints.sort(DatabaseDescriptor.getLocalComparator());
+            liveReplicas.sort(DatabaseDescriptor.getLocalComparator());
 
-        return liveEndpoints.subList(0, Math.min(liveEndpoints.size(), blockFor(keyspace)));
+        return liveReplicas.subList(0, Math.min(liveReplicas.size(), blockFor(keyspace)));
     }
 
-    private List<InetAddressAndPort> filterForEachQuorum(Keyspace keyspace, List<InetAddressAndPort> liveEndpoints)
+    private ReplicaList filterForEachQuorum(Keyspace keyspace, ReplicaList liveReplicas)
     {
         NetworkTopologyStrategy strategy = (NetworkTopologyStrategy) keyspace.getReplicationStrategy();
 
-        Map<String, List<InetAddressAndPort>> dcsEndpoints = new HashMap<>();
+        Map<String, ReplicaList> dcsReplicas = new HashMap<>();
         for (String dc: strategy.getDatacenters())
-            dcsEndpoints.put(dc, new ArrayList<>());
+            dcsReplicas.put(dc, new ReplicaList());","[{'comment': 'Completely overkill and unrelated, but ArrayList will over allocate here if liveReplicas is < the size of the default allocation of 10', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/ConsistencyLevel.java,"@@ -190,50 +197,50 @@ public int countLocalEndpoints(Iterable<InetAddressAndPort> liveEndpoints)
          * the blockFor first ones).
          */
         if (isDCLocal)
-            liveEndpoints.sort(DatabaseDescriptor.getLocalComparator());
+            liveReplicas.sort(DatabaseDescriptor.getLocalComparator());
 
-        return liveEndpoints.subList(0, Math.min(liveEndpoints.size(), blockFor(keyspace)));
+        return liveReplicas.subList(0, Math.min(liveReplicas.size(), blockFor(keyspace)));
     }
 
-    private List<InetAddressAndPort> filterForEachQuorum(Keyspace keyspace, List<InetAddressAndPort> liveEndpoints)
+    private ReplicaList filterForEachQuorum(Keyspace keyspace, ReplicaList liveReplicas)
     {
         NetworkTopologyStrategy strategy = (NetworkTopologyStrategy) keyspace.getReplicationStrategy();
 
-        Map<String, List<InetAddressAndPort>> dcsEndpoints = new HashMap<>();
+        Map<String, ReplicaList> dcsReplicas = new HashMap<>();
         for (String dc: strategy.getDatacenters())
-            dcsEndpoints.put(dc, new ArrayList<>());
+            dcsReplicas.put(dc, new ReplicaList());
 
-        for (InetAddressAndPort add : liveEndpoints)
+        for (Replica replica : liveReplicas)
         {
-            String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(add);
-            dcsEndpoints.get(dc).add(add);
+            String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(replica);
+            dcsReplicas.get(dc).add(replica);
         }
 
-        List<InetAddressAndPort> waitSet = new ArrayList<>();
-        for (Map.Entry<String, List<InetAddressAndPort>> dcEndpoints : dcsEndpoints.entrySet())
+        ReplicaList waitSet = new ReplicaList();","[{'comment': 'Another potential over allocation.', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/service/reads/DataResolver.java,"@@ -64,12 +72,19 @@ public PartitionIterator resolve()
         // at the beginning of this method), so grab the response count once and use that through the method.
         int count = responses.size();
         List<UnfilteredPartitionIterator> iters = new ArrayList<>(count);
-        InetAddressAndPort[] sources = new InetAddressAndPort[count];
+        Replica[] sources = new Replica[count];
         for (int i = 0; i < count; i++)
         {
             MessageIn<ReadResponse> msg = responses.get(i);
             iters.add(msg.payload.makeIterator(command));
-            sources[i] = msg.from;
+
+            Replica replica = replicaMap.get(msg.from);
+            if (replica == null)","[{'comment': ""It seems like we knew what kind of Replica it was when we sent the message. The kind of Replica it might be when we get the response could be different? Probably not given how pending states work.\r\n\r\nI just don't get how we could fail to decorate it. That's not right. If we sent the message eliciting this response then we must have known at the time."", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/service/reads/DataResolver.java,"@@ -64,12 +72,19 @@ public PartitionIterator resolve()
         // at the beginning of this method), so grab the response count once and use that through the method.
         int count = responses.size();
         List<UnfilteredPartitionIterator> iters = new ArrayList<>(count);
-        InetAddressAndPort[] sources = new InetAddressAndPort[count];
+        Replica[] sources = new Replica[count];
         for (int i = 0; i < count; i++)
         {
             MessageIn<ReadResponse> msg = responses.get(i);
             iters.add(msg.payload.makeIterator(command));
-            sources[i] = msg.from;
+
+            Replica replica = replicaMap.get(msg.from);
+            if (replica == null)
+                replica = command.decorateEndpoint(msg.from);
+            if (replica == null)
+                replica = Replica.fullStandin(msg.from);
+
+            sources[i] = replica != null ? replica : Replica.fullStandin(msg.from);","[{'comment': 'Seems like you doubled up on the stand in?', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/locator/Replica.java,"@@ -0,0 +1,221 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+import java.util.Set;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+
+import org.apache.cassandra.db.ConsistencyLevel;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.exceptions.UnavailableException;
+
+/**
+ * Decorated Endpoint
+ */
+public class Replica
+{
+    private final InetAddressAndPort endpoint;
+    private final Range<Token> range;
+    private final boolean full;
+
+    public Replica(InetAddressAndPort endpoint, Range<Token> range, boolean full)
+    {
+        Preconditions.checkNotNull(endpoint);
+        this.endpoint = endpoint;
+        this.range = range;
+        this.full = full;
+    }
+
+    public Replica(InetAddressAndPort endpoint, Token start, Token end, boolean full)
+    {
+        this(endpoint, new Range<>(start, end), full);
+    }
+
+    public boolean equals(Object o)
+    {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+        Replica replica = (Replica) o;
+        return full == replica.full &&
+               Objects.equals(endpoint, replica.endpoint) &&
+               Objects.equals(range, replica.range);
+    }
+
+    public int hashCode()
+    {
+
+        return Objects.hash(endpoint, range, full);
+    }
+
+    @Override
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder();
+        sb.append(full ? ""Full"" : ""Transient"");
+        sb.append('(').append(getEndpoint()).append(',').append(range).append(')');
+        return sb.toString();
+    }
+
+    public final InetAddressAndPort getEndpoint()
+    {
+        return endpoint;
+    }
+
+    public Range<Token> getRange()
+    {
+        return range;
+    }
+
+    public boolean isFull()
+    {
+        return full;
+    }
+
+    public final boolean isTransient()
+    {
+        return !isFull();
+    }
+
+    public ReplicaSet subtract(Replica that)
+    {
+        assert isFull() && that.isFull();  // FIXME: this
+        Set<Range<Token>> ranges = range.subtract(that.range);
+        ReplicaSet replicatedRanges = new ReplicaSet(ranges.size());
+        for (Range<Token> range: ranges)
+        {
+            replicatedRanges.add(new Replica(getEndpoint(), range, isFull()));
+        }
+        return replicatedRanges;
+    }
+
+    /**
+     * Subtract the ranges of the given replicas from the range of this replica,
+     * returning a set of replicas with the endpoint and transient information of
+     * this replica, and the ranges resulting from the subtraction.
+     */
+    public ReplicaSet subtractByRange(Replicas toSubtract)
+    {
+        if (isFull() && Iterables.all(toSubtract, Replica::isFull))
+        {
+            Set<Range<Token>> subtractedRanges = getRange().subtractAll(toSubtract.asRangeSet());
+            ReplicaSet replicaSet = new ReplicaSet(subtractedRanges.size());
+            for (Range<Token> range: subtractedRanges)
+            {
+                replicaSet.add(new Replica(getEndpoint(), range, isFull()));
+            }
+            return replicaSet;
+        }
+        else
+        {
+            // FIXME: add support for transient replicas
+            throw new UnsupportedOperationException(""transient replicas are currently unsupported"");
+        }
+    }
+
+    public ReplicaList normalizeByRange()
+    {
+        List<Range<Token>> normalized = Range.normalize(Collections.singleton(getRange()));
+        ReplicaList replicas = new ReplicaList(normalized.size());
+        for (Range<Token> normalizedRange: normalized)
+        {
+            replicas.add(new Replica(getEndpoint(), normalizedRange, isFull()));
+        }
+        return replicas;
+    }
+
+    public boolean contains(Range<Token> that)
+    {
+        return getRange().contains(that);
+    }
+
+    public boolean intersectsOnRange(Replica replica)
+    {
+        return getRange().intersects(replica.getRange());
+    }
+
+    public Replica decorateSubrange(Range<Token> subrange)
+    {
+        Preconditions.checkArgument(range.contains(subrange));
+        return new Replica(getEndpoint(), subrange, isFull());
+    }
+
+    public static Replica full(InetAddressAndPort endpoint, Range<Token> range)
+    {
+        return new Replica(endpoint, range, true);
+    }
+
+    /**
+     * We need to assume an endpoint is a full replica in a with unknown ranges in a
+     * few cases, so this returns one that throw an exception if you try to get it's range
+     */
+    public static Replica fullStandin(InetAddressAndPort endpoint)","[{'comment': ""This feels so unsafe. At the first level it's turning a compile time error into a runtime error. But then it's turning the fullness or not of the replica into a silent error. Do things work if you have isFull and isTransient throw as well?\r\n\r\nAnother issue is this is going to make isFull() and isTransient() bi-morphic which isn't the best. Feels like it might be premature optimization to worry about that right now though."", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/ReadCommand.java,"@@ -128,6 +130,7 @@ protected ReadCommand(Kind kind,
 
     protected abstract void serializeSelection(DataOutputPlus out, int version) throws IOException;
     protected abstract long selectionSerializedSize(int version);
+    public abstract Replica decorateEndpoint(InetAddressAndPort endpoint);","[{'comment': ""I'm not sure if this is a good idea based on it being used after we have already committed to that endpoint replicating a specific range a specific way. This seems like it races with ring changes."", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/compaction/CompactionManager.java,"@@ -533,7 +530,7 @@ public AllSSTableOpStatus relocateSSTables(final ColumnFamilyStore cfs, int jobs
             logger.info(""Partitioner does not support splitting"");
             return AllSSTableOpStatus.ABORTED;
         }
-        final Collection<Range<Token>> r = StorageService.instance.getLocalRanges(cfs.keyspace.getName());
+        final Collection<Range<Token>> r = StorageService.instance.getLocalReplicas(cfs.keyspace.getName()).asRangeSet();","[{'comment': 'This is hardly even used. It just checks for isEmpty(). No need to unwrap.', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/compaction/CompactionManager.java,"@@ -468,7 +465,7 @@ public AllSSTableOpStatus performCleanup(final ColumnFamilyStore cfStore, int jo
             return AllSSTableOpStatus.ABORTED;
         }
         // if local ranges is empty, it means no data should remain
-        final Collection<Range<Token>> ranges = StorageService.instance.getLocalRanges(keyspace.getName());
+        final Collection<Range<Token>> ranges = StorageService.instance.getLocalReplicas(keyspace.getName()).asRangeSet();","[{'comment': ""This could use a Collection2.transform version. It's just iterated."", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/compaction/CompactionManager.java,"@@ -871,7 +868,7 @@ public void forceUserDefinedCleanup(String dataFiles)
         {
             ColumnFamilyStore cfs = entry.getKey();
             Keyspace keyspace = cfs.keyspace;
-            Collection<Range<Token>> ranges = StorageService.instance.getLocalRanges(keyspace.getName());
+            Collection<Range<Token>> ranges = StorageService.instance.getLocalReplicas(keyspace.getName()).asRangeSet();","[{'comment': 'Also just iterated', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/compaction/Verifier.java,"@@ -209,7 +208,9 @@ public void verify()
                     markAndThrow();
             }
 
-            List<Range<Token>> ownedRanges = isOffline ? Collections.emptyList() : Range.normalize(StorageService.instance.getLocalAndPendingRanges(cfs.metadata().keyspace));
+            List<Range<Token>> ownedRanges = isOffline
+                                             ? Collections.emptyList()
+                                             : Range.normalize(StorageService.instance.getLocalAndPendingReplicas(cfs.metadata().keyspace).asRangeSet());","[{'comment': 'Another candidate', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/db/view/ViewBuilder.java,"@@ -135,14 +137,15 @@ private synchronized void build()
         }
 
         // Get the local ranges for which the view hasn't already been built nor it's building
-        Set<Range<Token>> newRanges = StorageService.instance.getLocalRanges(ksName)
-                                                             .stream()
-                                                             .map(r -> r.subtractAll(builtRanges))
-                                                             .flatMap(Set::stream)
-                                                             .map(r -> r.subtractAll(pendingRanges.keySet()))
-                                                             .flatMap(Set::stream)
-                                                             .collect(Collectors.toSet());
-
+        ReplicaSet replicatedRanges = StorageService.instance.getLocalReplicas(ksName);
+        Replicas.checkFull(StorageService.instance.getLocalReplicas(ksName));
+        Set<Range<Token>> newRanges = replicatedRanges.asRangeSet()","[{'comment': ""Also just iterating here. You don't even need Collection2 you can unwrap it in the first map."", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/dht/RangeFetchMapCalculator.java,"@@ -158,14 +159,15 @@ static boolean isTrivial(Range<Token> range)
             boolean localDCCheck = true;
             while (!added)
             {
-                List<InetAddressAndPort> srcs = new ArrayList<>(rangesWithSources.get(trivialRange));
+                ReplicaList replicas = new ReplicaList(rangesWithSources.get(trivialRange));
                 // sort with the endpoint having the least number of streams first:
-                srcs.sort(Comparator.comparingInt(o -> optimisedMap.get(o).size()));
-                for (InetAddressAndPort src : srcs)
+                replicas.sort(Comparator.comparingInt(o -> optimisedMap.get(o.getEndpoint()).size()));
+                Replicas.checkFull(replicas);
+                for (Replica replica : replicas)
                 {
-                    if (passFilters(src, localDCCheck))
+                    if (passFilters(replica, localDCCheck))
                     {
-                        fetchMap.put(src, trivialRange);
+                        fetchMap.put(replica.getEndpoint(), trivialRange);","[{'comment': 'So this is something I have been running into on my end, but why unwrap here?  I have been need to fix that because I need the transientness of what I am fetching for to determine whether the remote side is going to send me the transient data or the full data.', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/dht/RangeFetchMapCalculator.java,"@@ -347,15 +349,16 @@ else if (vertex.isRangeVertex())
     private boolean addEndpoints(MutableCapacityGraph<Vertex, Integer> capacityGraph, RangeVertex rangeVertex, boolean localDCCheck)
     {
         boolean sourceFound = false;
-        for (InetAddressAndPort endpoint : rangesWithSources.get(rangeVertex.getRange()))
+        Replicas.checkFull(rangesWithSources.get(rangeVertex.getRange()));
+        for (Replica replica : rangesWithSources.get(rangeVertex.getRange()))
         {
-            if (passFilters(endpoint, localDCCheck))
+            if (passFilters(replica, localDCCheck))
             {
                 sourceFound = true;
                 // if we pass filters, it means that we don't filter away localhost and we can count it as a source:
-                if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort()))
+                if (replica.getEndpoint().equals(FBUtilities.getBroadcastAddressAndPort()))","[{'comment': 'Crazy thought, what if Replica had an isLocal or something?', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/locator/Replicas.java,"@@ -0,0 +1,313 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Set;
+
+import com.google.common.base.Predicate;
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Sets;
+
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.utils.FBUtilities;
+
+/**
+ * A collection like class for Replica objects. Since the Replica class contains inetaddress, range, and
+ * transient replication status, basic contains and remove methods can be ambiguous. Replicas forces you
+ * to be explicit about what you're checking the container for, or removing from it.
+ */
+public abstract class Replicas implements Iterable<Replica>
+{
+
+    public abstract boolean add(Replica replica);
+    public abstract void addAll(Iterable<Replica> replicas);
+    public abstract void removeEndpoint(InetAddressAndPort endpoint);
+    public abstract void removeReplica(Replica replica);
+    public abstract int size();
+
+    public Iterable<InetAddressAndPort> asEndpoints()
+    {
+        return Iterables.transform(this, Replica::getEndpoint);
+    }
+
+    public Set<InetAddressAndPort> asEndpointSet()
+    {
+        Set<InetAddressAndPort> result = Sets.newHashSetWithExpectedSize(size());
+        for (Replica replica: this)
+        {
+            result.add(replica.getEndpoint());
+        }
+        return result;
+    }
+
+    public List<InetAddressAndPort> asEndpointList()
+    {
+        List<InetAddressAndPort> result = new ArrayList<>(size());
+        for (Replica replica: this)
+        {
+            result.add(replica.getEndpoint());
+        }
+        return result;
+    }
+
+    public Iterable<Range<Token>> asRanges()
+    {
+        return Iterables.transform(this, Replica::getRange);
+    }
+
+    public Set<Range<Token>> asRangeSet()
+    {
+        Set<Range<Token>> result = Sets.newHashSetWithExpectedSize(size());
+        for (Replica replica: this)
+        {
+            result.add(replica.getRange());
+        }
+        return result;
+    }
+
+    public Iterable<Range<Token>> fullRanges()","[{'comment': ""Is this really a good thing to have around? The one use I see of this doesn't look appropriate."", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/dht/RangeStreamer.java,"@@ -176,25 +179,28 @@ public void addSourceFilter(ISourceFilter filter)
      * Add ranges to be streamed for given keyspace.
      *
      * @param keyspaceName keyspace name
-     * @param ranges ranges to be streamed
+     * @param replicas ranges to be streamed
      */
-    public void addRanges(String keyspaceName, Collection<Range<Token>> ranges)
+    public void addRanges(String keyspaceName, Replicas replicas)","[{'comment': 'Should this be addReplicas? I would also change the comment to ranges to be fetched. Streaming seems to get used to imply both sending and fetching. But fetching always means fetching.', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/dht/RangeStreamer.java,"@@ -176,25 +179,28 @@ public void addSourceFilter(ISourceFilter filter)
      * Add ranges to be streamed for given keyspace.
      *
      * @param keyspaceName keyspace name
-     * @param ranges ranges to be streamed
+     * @param replicas ranges to be streamed
      */
-    public void addRanges(String keyspaceName, Collection<Range<Token>> ranges)
+    public void addRanges(String keyspaceName, Replicas replicas)
     {
         if(Keyspace.open(keyspaceName).getReplicationStrategy() instanceof LocalStrategy)
         {
             logger.info(""Not adding ranges for Local Strategy keyspace={}"", keyspaceName);
             return;
         }
 
+        Replicas.checkFull(replicas);
+
         boolean useStrictSource = useStrictSourcesForRanges(keyspaceName);
-        Multimap<Range<Token>, InetAddressAndPort> rangesForKeyspace = useStrictSource
-                ? getAllRangesWithStrictSourcesFor(keyspaceName, ranges) : getAllRangesWithSourcesFor(keyspaceName, ranges);
+        ReplicaMultimap<Range<Token>, ReplicaList> rangesForKeyspace = useStrictSource
+                                                                       ? getAllRangesWithStrictSourcesFor(keyspaceName, replicas.fullRanges())","[{'comment': 'fullRanges(), not a fan.', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/locator/AbstractEndpointSnitch.java,"@@ -23,17 +23,17 @@
 
 public abstract class AbstractEndpointSnitch implements IEndpointSnitch
 {
-    public abstract int compareEndpoints(InetAddressAndPort target, InetAddressAndPort a1, InetAddressAndPort a2);
+    public abstract int compareEndpoints(InetAddressAndPort target, Replica r1, Replica r2);","[{'comment': ""Are they endpoints or replicas now? I think maybe this should just be endpoints to make explicit the snitch isn't going to use the range or transient information. It doesn't have many usages so it's not a big deal to manually"", 'commenter': 'aweisberg'}]"
224,conf/cassandra.yaml,"@@ -1032,6 +1032,10 @@ enable_scripted_user_defined_functions: false
 # Materialized views are considered experimental and are not recommended for production use.
 enable_materialized_views: true
 
+# Enables creation of transiently replicated keyspaces on this node.
+# Transient replication is experimental and is not recommended for production use.","[{'comment': 'Thanks for the experimental line. ', 'commenter': 'jeffjirsa'}]"
224,src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java,"@@ -202,61 +204,63 @@ private Keyspace getKeyspace()
      *
      * @return the replication factor
      */
-    public abstract int getReplicationFactor();
+    public abstract ReplicationFactor getReplicationFactor();
 
     /*
      * NOTE: this is pretty inefficient. also the inverse (getRangeAddresses) below.
      * this is fine as long as we don't use this on any critical path.
      * (fixing this would probably require merging tokenmetadata into replicationstrategy,
      * so we could cache/invalidate cleanly.)
      */
-    public Multimap<InetAddressAndPort, Range<Token>> getAddressRanges(TokenMetadata metadata)
+    public ReplicaMultimap<InetAddressAndPort, ReplicaSet> getAddressReplicas(TokenMetadata metadata)
     {
-        Multimap<InetAddressAndPort, Range<Token>> map = HashMultimap.create();
+        ReplicaMultimap<InetAddressAndPort, ReplicaSet> map = ReplicaMultimap.set();
 
         for (Token token : metadata.sortedTokens())
         {
             Range<Token> range = metadata.getPrimaryRangeFor(token);
-            for (InetAddressAndPort ep : calculateNaturalEndpoints(token, metadata))
+            for (Replica replica : calculateNaturalReplicas(token, metadata))
             {
-                map.put(ep, range);
+                Preconditions.checkState(range.equals(replica.getRange()) || this instanceof LocalStrategy);","[{'comment': 'Can you explain this check? Why would LocalStrategy not have the ranges match?', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java,"@@ -424,17 +419,17 @@ public boolean isWorthMergingForRangeQuery(List<InetAddressAndPort> merged, List
             return true;
 
         // Make sure we return the subsnitch decision (i.e true if we're here) if we lack too much scores
-        double maxMerged = maxScore(merged);
-        double maxL1 = maxScore(l1);
-        double maxL2 = maxScore(l2);
+        double maxMerged = maxScore(merged.asEndpoints());","[{'comment': 'Maybe just update maxScore?', 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/locator/ReplicaSet.java,"@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedHashSet;
+import java.util.Objects;
+import java.util.Set;
+
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Sets;
+
+public class ReplicaSet extends Replicas
+{
+    static final ReplicaSet EMPTY = new ReplicaSet(ImmutableSet.of());
+
+    private final Set<Replica> replicaSet;
+
+    public ReplicaSet()
+    {
+        replicaSet = new HashSet<>();
+    }
+
+    public ReplicaSet(int expectedSize)
+    {
+        replicaSet = Sets.newHashSetWithExpectedSize(expectedSize);
+    }
+
+    public ReplicaSet(Replicas replicas)
+    {
+        replicaSet = Sets.newHashSetWithExpectedSize(replicas.size());
+        Iterables.addAll(replicaSet, replicas);
+    }
+
+    private ReplicaSet(Set<Replica> replicaSet)
+    {
+        this.replicaSet = replicaSet;
+    }
+
+    public boolean equals(Object o)
+    {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+        ReplicaSet that = (ReplicaSet) o;
+        return Objects.equals(replicaSet, that.replicaSet);
+    }
+
+    public int hashCode()
+    {
+        return Objects.hash(replicaSet);
+    }
+
+    @Override
+    public String toString()
+    {
+        return replicaSet.toString();
+    }
+
+    @Override
+    public boolean add(Replica replica)
+    {
+        return replicaSet.add(replica);
+    }
+
+    @Override
+    public void addAll(Iterable<Replica> replicas)
+    {
+        Iterables.addAll(replicaSet, replicas);
+    }
+
+    @Override
+    public void removeEndpoint(InetAddressAndPort endpoint)
+    {
+        replicaSet.removeIf(r -> r.getEndpoint().equals(endpoint));
+    }
+
+    @Override
+    public void removeReplica(Replica replica)
+    {
+        replicaSet.remove(replica);
+    }
+
+    @Override
+    public int size()
+    {
+        return replicaSet.size();
+    }
+
+    @Override
+    public Iterator<Replica> iterator()
+    {
+        return replicaSet.iterator();
+    }
+
+    public ReplicaSet differenceOnEndpoint(Replicas differenceOn)
+    {
+        if (Iterables.all(this, Replica::isFull) && Iterables.all(differenceOn, Replica::isFull))
+        {
+            Set<InetAddressAndPort> diffEndpoints = differenceOn.asEndpointSet();
+            return new ReplicaSet(Replicas.filterOnEndpoints(this, e -> !diffEndpoints.contains(e)));
+        }
+        else
+        {
+            // FIXME: add support for transient replicas
+            throw new UnsupportedOperationException(""transient replicas are currently unsupported"");
+        }
+
+    }
+
+    public static ReplicaSet immutableCopyOf(ReplicaSet from)
+    {
+        return new ReplicaSet(ImmutableSet.copyOf(from.replicaSet));
+    }
+
+    public static ReplicaSet immutableCopyOf(Replicas from)
+    {
+        return new ReplicaSet(ImmutableSet.<Replica>builder().addAll(from).build());
+    }
+
+    public static ReplicaSet ordered()","[{'comment': ""I had to look this up to realize that ordered doesn't mean sorted. It's the correct name, they are ordered just not according to their natural order. Maybe the right shade for the bike shed here is order preserving?"", 'commenter': 'aweisberg'}]"
224,src/java/org/apache/cassandra/locator/PendingRangeMaps.java,"@@ -23,196 +23,176 @@
 import com.google.common.collect.Iterators;
 import org.apache.cassandra.dht.Range;
 import org.apache.cassandra.dht.Token;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import java.util.*;
 
-public class PendingRangeMaps implements Iterable<Map.Entry<Range<Token>, List<InetAddressAndPort>>>
+public class PendingRangeMaps implements Iterable<Map.Entry<Range<Token>, List<Replica>>>
 {
-    private static final Logger logger = LoggerFactory.getLogger(PendingRangeMaps.class);
-
     /**
      * We have for NavigableMap to be able to search for ranges containing a token efficiently.
      *
      * First two are for non-wrap-around ranges, and the last two are for wrap-around ranges.
      */
     // ascendingMap will sort the ranges by the ascending order of right token
-    final NavigableMap<Range<Token>, List<InetAddressAndPort>> ascendingMap;
+    private final NavigableMap<Range<Token>, List<Replica>> ascendingMap;
+
     /**
      * sorting end ascending, if ends are same, sorting begin descending, so that token (end, end) will
      * come before (begin, end] with the same end, and (begin, end) will be selected in the tailMap.
      */
-    static final Comparator<Range<Token>> ascendingComparator = new Comparator<Range<Token>>()
-        {
-            @Override
-            public int compare(Range<Token> o1, Range<Token> o2)
-            {
-                int res = o1.right.compareTo(o2.right);
-                if (res != 0)
-                    return res;
+    private static final Comparator<Range<Token>> ascendingComparator = (o1, o2) -> {
+        int res = o1.right.compareTo(o2.right);
+        if (res != 0)
+            return res;
 
-                return o2.left.compareTo(o1.left);
-            }
-        };
+        return o2.left.compareTo(o1.left);
+    };
 
     // ascendingMap will sort the ranges by the descending order of left token
-    final NavigableMap<Range<Token>, List<InetAddressAndPort>> descendingMap;
+    private final NavigableMap<Range<Token>, List<Replica>> descendingMap;
+
     /**
      * sorting begin descending, if begins are same, sorting end descending, so that token (begin, begin) will
      * come after (begin, end] with the same begin, and (begin, end) won't be selected in the tailMap.
      */
-    static final Comparator<Range<Token>> descendingComparator = new Comparator<Range<Token>>()
-        {
-            @Override
-            public int compare(Range<Token> o1, Range<Token> o2)
-            {
-                int res = o2.left.compareTo(o1.left);
-                if (res != 0)
-                    return res;
+    private static final Comparator<Range<Token>> descendingComparator = (o1, o2) -> {
+        int res = o2.left.compareTo(o1.left);
+        if (res != 0)
+            return res;
 
-                // if left tokens are same, sort by the descending of the right tokens.
-                return o2.right.compareTo(o1.right);
-            }
-        };
+        // if left tokens are same, sort by the descending of the right tokens.
+        return o2.right.compareTo(o1.right);
+    };
 
     // these two maps are for warp around ranges.
-    final NavigableMap<Range<Token>, List<InetAddressAndPort>> ascendingMapForWrapAround;
+    private final NavigableMap<Range<Token>, List<Replica>> ascendingMapForWrapAround;
+
     /**
      * for wrap around range (begin, end], which begin > end.
      * Sorting end ascending, if ends are same, sorting begin ascending,
      * so that token (end, end) will come before (begin, end] with the same end, and (begin, end] will be selected in
      * the tailMap.
      */
-    static final Comparator<Range<Token>> ascendingComparatorForWrapAround = new Comparator<Range<Token>>()
-    {
-        @Override
-        public int compare(Range<Token> o1, Range<Token> o2)
-        {
-            int res = o1.right.compareTo(o2.right);
-            if (res != 0)
-                return res;
+    private static final Comparator<Range<Token>> ascendingComparatorForWrapAround = (o1, o2) -> {
+        int res = o1.right.compareTo(o2.right);
+        if (res != 0)
+            return res;
 
-            return o1.left.compareTo(o2.left);
-        }
+        return o1.left.compareTo(o2.left);
     };
 
-    final NavigableMap<Range<Token>, List<InetAddressAndPort>> descendingMapForWrapAround;
+    private final NavigableMap<Range<Token>, List<Replica>> descendingMapForWrapAround;
+
     /**
      * for wrap around ranges, which begin > end.
      * Sorting end ascending, so that token (begin, begin) will come after (begin, end] with the same begin,
      * and (begin, end) won't be selected in the tailMap.
      */
-    static final Comparator<Range<Token>> descendingComparatorForWrapAround = new Comparator<Range<Token>>()
-    {
-        @Override
-        public int compare(Range<Token> o1, Range<Token> o2)
-        {
-            int res = o2.left.compareTo(o1.left);
-            if (res != 0)
-                return res;
-            return o1.right.compareTo(o2.right);
-        }
+    private static final Comparator<Range<Token>> descendingComparatorForWrapAround = (o1, o2) -> {","[{'comment': ""These are kind of orthogonal changes, but sure. I don't see a down side to doing them here. I'll bet these comparators are low traffic so it won't be much merge pain."", 'commenter': 'aweisberg'}]"
242,src/java/org/apache/cassandra/db/virtual/AbstractVirtualTable.java,"@@ -42,7 +42,7 @@
  */
 public abstract class AbstractVirtualTable implements VirtualTable
 {
-    private final TableMetadata metadata;
+    protected final TableMetadata metadata;","[{'comment': ""Not that I mind the change, but it isn't necessary for this patch, and there is `metadata()` if it were."", 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/db/virtual/SettingsTable.java,"@@ -0,0 +1,103 @@
+package org.apache.cassandra.db.virtual;
+
+import java.lang.reflect.Field;
+import java.lang.reflect.Modifier;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Consumer;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.config.Config;","[{'comment': 'A lot of unused imports here.', 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/db/virtual/SettingsTable.java,"@@ -0,0 +1,103 @@
+package org.apache.cassandra.db.virtual;","[{'comment': 'Missing ASF license.', 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/db/virtual/SettingsTable.java,"@@ -0,0 +1,103 @@
+package org.apache.cassandra.db.virtual;
+
+import java.lang.reflect.Field;
+import java.lang.reflect.Modifier;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Consumer;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.config.Config;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.db.partitions.PartitionUpdate;
+import org.apache.cassandra.db.rows.Row;
+import org.apache.cassandra.db.virtual.AbstractVirtualTable.DataSet;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.exceptions.InvalidRequestException;
+import org.apache.cassandra.schema.ColumnMetadata;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.transport.ServerError;
+
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+
+final class SettingsTable extends AbstractVirtualTable
+{
+    private static final String VALUE = ""value"";
+    private static final String SETTING = ""setting"";
+
+    private static final List<Field> FIELDS = Arrays.stream(Config.class.getFields())","[{'comment': ""Doesn't really need to be static. Can also be a map of name -> `Field`, so that `data(DecoratedKey partitionKey)` is cleaner."", 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/db/virtual/SettingsTable.java,"@@ -0,0 +1,103 @@
+package org.apache.cassandra.db.virtual;
+
+import java.lang.reflect.Field;
+import java.lang.reflect.Modifier;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Consumer;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.config.Config;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.db.partitions.PartitionUpdate;
+import org.apache.cassandra.db.rows.Row;
+import org.apache.cassandra.db.virtual.AbstractVirtualTable.DataSet;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.exceptions.InvalidRequestException;
+import org.apache.cassandra.schema.ColumnMetadata;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.transport.ServerError;
+
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+
+final class SettingsTable extends AbstractVirtualTable
+{
+    private static final String VALUE = ""value"";
+    private static final String SETTING = ""setting"";
+
+    private static final List<Field> FIELDS = Arrays.stream(Config.class.getFields())
+            .filter(f -> !Modifier.isStatic(f.getModifiers()))
+            .collect(Collectors.toList());
+
+    SettingsTable(String keyspace)
+    {
+        super(TableMetadata.builder(keyspace, ""settings"")
+                .comment(""current settings"")
+                .kind(TableMetadata.Kind.VIRTUAL)
+                .partitioner(new LocalPartitioner(UTF8Type.instance))
+                .addPartitionKeyColumn(SETTING, UTF8Type.instance)
+                .addRegularColumn(VALUE, UTF8Type.instance)
+                .build());
+    }
+
+    public DataSet data(DecoratedKey partitionKey)
+    {
+        SimpleDataSet result = new SimpleDataSet(metadata());
+        Config config = DatabaseDescriptor.getRawConfig();
+        String setting = null;
+        try
+        {
+            setting = UTF8Type.instance.compose(partitionKey.getKey());
+            Field field = config.getClass().getDeclaredField(setting);
+            if (!FIELDS.contains(field))
+                throw new InvalidRequestException(""Invalid setting: "" + setting);
+            Object value = field.get(config);
+            result.row(setting);
+            if (value != null)","[{'comment': ""Doesn't handle array values like the other `data()` implementation."", 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/db/virtual/SettingsTable.java,"@@ -0,0 +1,103 @@
+package org.apache.cassandra.db.virtual;
+
+import java.lang.reflect.Field;
+import java.lang.reflect.Modifier;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Consumer;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.config.Config;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.db.partitions.PartitionUpdate;
+import org.apache.cassandra.db.rows.Row;
+import org.apache.cassandra.db.virtual.AbstractVirtualTable.DataSet;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.exceptions.InvalidRequestException;
+import org.apache.cassandra.schema.ColumnMetadata;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.transport.ServerError;
+
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+
+final class SettingsTable extends AbstractVirtualTable
+{
+    private static final String VALUE = ""value"";
+    private static final String SETTING = ""setting"";
+
+    private static final List<Field> FIELDS = Arrays.stream(Config.class.getFields())
+            .filter(f -> !Modifier.isStatic(f.getModifiers()))
+            .collect(Collectors.toList());
+
+    SettingsTable(String keyspace)
+    {
+        super(TableMetadata.builder(keyspace, ""settings"")
+                .comment(""current settings"")
+                .kind(TableMetadata.Kind.VIRTUAL)
+                .partitioner(new LocalPartitioner(UTF8Type.instance))
+                .addPartitionKeyColumn(SETTING, UTF8Type.instance)
+                .addRegularColumn(VALUE, UTF8Type.instance)
+                .build());
+    }
+
+    public DataSet data(DecoratedKey partitionKey)
+    {
+        SimpleDataSet result = new SimpleDataSet(metadata());
+        Config config = DatabaseDescriptor.getRawConfig();
+        String setting = null;
+        try
+        {
+            setting = UTF8Type.instance.compose(partitionKey.getKey());
+            Field field = config.getClass().getDeclaredField(setting);
+            if (!FIELDS.contains(field))
+                throw new InvalidRequestException(""Invalid setting: "" + setting);
+            Object value = field.get(config);
+            result.row(setting);
+            if (value != null)
+                result.column(VALUE, value.toString());
+        }
+        catch (IllegalAccessException | IllegalArgumentException e)
+        {
+            throw new ServerError(e);
+        }
+        catch (NoSuchFieldException e)
+        {
+            throw new InvalidRequestException(""No such setting: "" + setting);
+        }
+        return result;
+    }
+
+    @Override
+    public DataSet data()
+    {
+        SimpleDataSet result = new SimpleDataSet(metadata());
+        Config config = DatabaseDescriptor.getRawConfig();
+        for (Field f : FIELDS)
+        {
+            try
+            {
+                Object value = f.get(config);","[{'comment': 'Should probably factor out this whole block into a method usable by both `data()` implementations. ` String fieldValue(Field field, Config config)`?', 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/audit/AuditLogOptions.java,"@@ -19,6 +19,9 @@
 
 import org.apache.commons.lang3.StringUtils;
 
+import com.google.common.base.MoreObjects;","[{'comment': 'Wrong import order. See https://wiki.apache.org/cassandra/CodeStyle', 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -20,6 +20,11 @@
 import java.util.Arrays;
 import java.util.Objects;
 
+import org.apache.commons.lang.builder.ToStringBuilder;
+import org.apache.commons.lang.builder.ToStringStyle;
+
+import com.google.common.base.MoreObjects;","[{'comment': 'Also import order.', 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/audit/AuditLogOptions.java,"@@ -58,4 +61,11 @@
      * For more options, refer: net.openhft.chronicle.queue.RollCycles
      */
     public String roll_cycle = ""HOURLY"";
+
+    public String toString()","[{'comment': ""Is this the `toString()` implementation we want? Like the other three, it's missing fields, intentionally or not (if intentionally, maybe comment why so?). Also, this string helper prefixes the map with the class name, and it doesn't look pretty in CQL output, subjectively."", 'commenter': 'iamaleksey'}]"
242,src/java/org/apache/cassandra/db/virtual/SettingsTable.java,"@@ -0,0 +1,90 @@
+package org.apache.cassandra.db.virtual;
+
+import java.lang.reflect.Field;
+import java.lang.reflect.Modifier;
+import java.util.Arrays;
+import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.config.Config;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.exceptions.InvalidRequestException;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.transport.ServerError;
+
+final class SettingsTable extends AbstractVirtualTable
+{
+    private static final String VALUE = ""value"";
+    private static final String SETTING = ""setting"";
+
+    private static final Map<String, Callable<String>> FIELDS = Arrays.stream(Config.class.getFields())
+            .filter(f -> !Modifier.isStatic(f.getModifiers()))
+            .collect(Collectors.toMap(Field::getName, f -> stringFunction(f)));
+
+    private static Callable<String> stringFunction(Field f)
+    {
+        return () -> {
+            Object value = f.get(DatabaseDescriptor.getRawConfig());
+            if (value != null && value.getClass().isArray())
+                value = Arrays.toString((Object[]) value);
+
+            return value == null ? null : value.toString();
+        };
+    }
+
+    SettingsTable(String keyspace)
+    {
+        super(TableMetadata.builder(keyspace, ""settings"")
+                .comment(""current settings"")
+                .kind(TableMetadata.Kind.VIRTUAL)
+                .partitioner(new LocalPartitioner(UTF8Type.instance))
+                .addPartitionKeyColumn(SETTING, UTF8Type.instance)
+                .addRegularColumn(VALUE, UTF8Type.instance)
+                .build());
+    }
+
+    public DataSet data(DecoratedKey partitionKey)
+    {
+        SimpleDataSet result = new SimpleDataSet(metadata());
+        String setting = UTF8Type.instance.compose(partitionKey.getKey());
+        if (!FIELDS.containsKey(setting))
+            throw new InvalidRequestException(""Invalid setting: "" + setting);","[{'comment': ""It's not invalid to query a table for a non-existent partition key, you just return an empty data set in CQL."", 'commenter': 'iamaleksey'}]"
244,src/java/org/apache/cassandra/metrics/TableMetrics.java,"@@ -281,7 +301,7 @@ public Long getValue()
     public final Meter readRepairRequests;
     public final Meter shortReadProtectionRequests;
 
-    public final Map<Sampler, TopKSampler<ByteBuffer>> samplers;
+    public final Map<SamplerType, Sampler<?>> samplers;","[{'comment': 'This can be replaced with an `EnumMap`.', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/db/ReadExecutionController.java,"@@ -113,6 +123,11 @@ static ReadExecutionController forCommand(ReadCommand command)
                 throw e;
             }
         }
+        if (baseCfs.metric.topLocalReadQueryTime.isEnabled())
+        {","[{'comment': ""Single line if conditions don't need braces."", 'commenter': 'dineshjoshi'}, {'comment': ""> Single line if conditions don't need braces.\r\n\r\nIn this case it is better to write braces for readability issues even that we don't need them "", 'commenter': 'kallelzied'}, {'comment': ""It's more to follow cassandra style conventions: http://cassandra.apache.org/doc/latest/development/code_style.html `Always include braces for nested levels of conditionals and loops. Only avoid braces for single level.`\r\n\r\nThis ticket has already been resolved though. I'll close the PR."", 'commenter': 'clohfink'}]"
244,src/java/org/apache/cassandra/db/ReadExecutionController.java,"@@ -113,6 +123,11 @@ static ReadExecutionController forCommand(ReadCommand command)
                 throw e;
             }
         }
+        if (baseCfs.metric.topLocalReadQueryTime.isEnabled())
+        {
+            result.startTime = System.nanoTime();","[{'comment': 'I prefer naming primitive variables with units for example - `startTimeNanos`.', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/db/ReadExecutionController.java,"@@ -132,6 +147,17 @@ public void close()
         {
             if (baseOp != null)
                 baseOp.close();
+
+            if (startTime != -1)
+            {
+                String cql = command.toCQLString();
+                int time = (int) Math.min(TimeUnit.NANOSECONDS.toMicros(System.nanoTime() - startTime), Integer.MAX_VALUE);","[{'comment': '`timeMillis` or `millis` for brevity?', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/db/ReadExecutionController.java,"@@ -132,6 +147,17 @@ public void close()
         {
             if (baseOp != null)
                 baseOp.close();
+
+            if (startTime != -1)
+            {
+                String cql = command.toCQLString();
+                int time = (int) Math.min(TimeUnit.NANOSECONDS.toMicros(System.nanoTime() - startTime), Integer.MAX_VALUE);
+                ColumnFamilyStore cfs = ColumnFamilyStore.getIfExists(baseMetadata.id);
+                if(cfs != null)
+                {","[{'comment': 'You can skip braces for single line if conditions.', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/metrics/FrequencySampler.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.metrics;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.clearspring.analytics.stream.StreamSummary;
+
+/**
+ * Find the most frequent sample. A sample adds to the sum of its key ie
+ * <p>add(""x"", 10); and add(""x"", 20); will result in ""x"" = 30</p> This uses StreamSummary to only store the
+ * approximate cardinality (capacity) of keys. If the number of distinct keys exceed the capacity, the error of the
+ * sample may increase depending on distribution of keys among the total set.
+ * 
+ * @param <T>
+ */
+public abstract class FrequencySampler<T> extends Sampler<T>
+{
+    private static final Logger logger = LoggerFactory.getLogger(FrequencySampler.class);
+    private boolean enabled = false;
+
+    private StreamSummary<T> summary;
+
+    /**
+     * Start to record samples
+     *
+     * @param capacity
+     *            Number of sample items to keep in memory, the lower this is
+     *            the less accurate results are. For best results use value
+     *            close to cardinality, but understand the memory trade offs.
+     */
+    public synchronized void beginSampling(int capacity)
+    {
+        if (!enabled)
+        {
+            summary = new StreamSummary<T>(capacity);
+            enabled = true;
+        }
+    }
+
+    /**
+     * Call to stop collecting samples, and gather the results
+     * @param count Number of most frequent items to return
+     */
+    public synchronized List<Sample<T>> finishSampling(int count)
+    {
+        List<Sample<T>> results = Collections.EMPTY_LIST;","[{'comment': '`Collections.emptyList()` is safer.', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/metrics/FrequencySampler.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.metrics;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.clearspring.analytics.stream.StreamSummary;
+
+/**
+ * Find the most frequent sample. A sample adds to the sum of its key ie
+ * <p>add(""x"", 10); and add(""x"", 20); will result in ""x"" = 30</p> This uses StreamSummary to only store the
+ * approximate cardinality (capacity) of keys. If the number of distinct keys exceed the capacity, the error of the
+ * sample may increase depending on distribution of keys among the total set.
+ * 
+ * @param <T>
+ */
+public abstract class FrequencySampler<T> extends Sampler<T>
+{
+    private static final Logger logger = LoggerFactory.getLogger(FrequencySampler.class);
+    private boolean enabled = false;
+
+    private StreamSummary<T> summary;
+
+    /**
+     * Start to record samples
+     *
+     * @param capacity
+     *            Number of sample items to keep in memory, the lower this is
+     *            the less accurate results are. For best results use value
+     *            close to cardinality, but understand the memory trade offs.
+     */
+    public synchronized void beginSampling(int capacity)
+    {
+        if (!enabled)
+        {
+            summary = new StreamSummary<T>(capacity);
+            enabled = true;
+        }
+    }
+
+    /**
+     * Call to stop collecting samples, and gather the results
+     * @param count Number of most frequent items to return
+     */
+    public synchronized List<Sample<T>> finishSampling(int count)
+    {
+        List<Sample<T>> results = Collections.EMPTY_LIST;
+        if (enabled)
+        {
+            enabled = false;
+            results = summary.topK(count)
+                             .stream()
+                             .map(c -> new Sample<T>(c.getItem(), c.getCount(), c.getError()))
+                             .collect(Collectors.toList());
+        }
+        return results;
+    }
+
+    protected synchronized void insert(final T item, final long value)
+    {
+        // samplerExecutor is single threaded but still need
+        // synchronization against jmx calls to finishSampling
+        if (enabled && value > 0)
+        {
+            try
+            {
+                summary.offer(item, (int) Math.min(value, Integer.MAX_VALUE));
+            } catch (Exception e)
+            {
+                logger.trace(""Failure to offer sample"", e);
+            }
+        }
+    }
+
+    public boolean isEnabled()
+    {
+        return enabled;
+    }
+
+    public void setEnabled(boolean enabled)
+    {
+        this.enabled = enabled;","[{'comment': 'This allows the user of the class to enable the `FrequencySampler` without actually initializing the `summary` variable. This will cause an NPE.', 'commenter': 'dineshjoshi'}, {'comment': 'That method was unnecessary so I just deleted it', 'commenter': 'clohfink'}]"
244,src/java/org/apache/cassandra/metrics/Sampler.java,"@@ -0,0 +1,67 @@
+package org.apache.cassandra.metrics;
+
+import java.io.Serializable;
+import java.util.List;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.NamedThreadFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+
+public abstract class Sampler<T>
+{
+    public enum SamplerType
+    {
+        READS, WRITES, LOCAL_READ_TIME, WRITE_SIZE, CAS_CONTENTIONS
+    }
+
+    @VisibleForTesting
+    static final ThreadPoolExecutor samplerExecutor = new JMXEnabledThreadPoolExecutor(1, 1,
+            TimeUnit.SECONDS,
+            new LinkedBlockingQueue<Runnable>(),
+            new NamedThreadFactory(""Sampler""),
+            ""internal"");
+
+    public void addSample(final T item, final int value)
+    {
+        if (isEnabled())
+            samplerExecutor.execute(() -> insert(item, value));
+    }
+
+    protected abstract void insert(T item, long value);
+
+    public abstract boolean isEnabled();
+
+    public abstract void beginSampling(int capacity);
+
+    public abstract List<Sample<T>> finishSampling(int count);
+
+    public abstract String toString(T value);
+
+    /**
+     * Represents the ranked items collected during a sample period
+     */
+    public static class Sample<S> implements Serializable
+    {
+","[{'comment': 'Extra white space?', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/tools/nodetool/ProfileLoad.java,"@@ -0,0 +1,178 @@
+package org.apache.cassandra.tools.nodetool;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static org.apache.commons.lang3.StringUtils.join;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.stream.Collectors;
+
+import javax.management.openmbean.CompositeData;
+import javax.management.openmbean.OpenDataException;
+
+import org.apache.cassandra.metrics.Sampler.SamplerType;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.NodeTool.NodeToolCmd;
+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;
+import org.apache.cassandra.utils.Pair;
+
+import com.google.common.collect.Lists;
+
+import io.airlift.airline.Arguments;
+import io.airlift.airline.Command;
+import io.airlift.airline.Option;
+
+@Command(name = ""profileload"", description = ""Low footprint profiling of activity for a period of time"")
+public class ProfileLoad extends NodeToolCmd
+{
+    @Arguments(usage = ""<keyspace> <cfname> <duration>"", description = ""The keyspace, column family name, and duration in milliseconds"")
+    private List<String> args = new ArrayList<>();
+
+    @Option(name = ""-s"", description = ""Capacity of the sampler, higher for more accuracy (Default: 256)"")
+    private int capacity = 256;
+
+    @Option(name = ""-k"", description = ""Number of the top samples to list (Default: 10)"")
+    private int topCount = 10;
+
+    @Option(name = ""-a"", description = ""Comma separated list of samplers to use (Default: all)"")
+    private String samplers = join(SamplerType.values(), ',');
+
+    @Override
+    public void execute(NodeProbe probe)
+    {
+        checkArgument(args.size() == 3 || args.size() == 1 || args.size() == 0, ""Invalid arguments, either [keyspace table duration] or [duration] or no args"");
+        checkArgument(topCount < capacity, ""TopK count (-k) option must be smaller then the summary capacity (-s)"");
+        String keyspace = null;
+        String table = null;
+        Integer duration = 10000;","[{'comment': 'What is the unit for `duration`? It might be better to just use `java.time.Duration`?', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/tools/nodetool/ProfileLoad.java,"@@ -0,0 +1,178 @@
+package org.apache.cassandra.tools.nodetool;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static org.apache.commons.lang3.StringUtils.join;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.stream.Collectors;
+
+import javax.management.openmbean.CompositeData;
+import javax.management.openmbean.OpenDataException;
+
+import org.apache.cassandra.metrics.Sampler.SamplerType;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.NodeTool.NodeToolCmd;
+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;
+import org.apache.cassandra.utils.Pair;
+
+import com.google.common.collect.Lists;
+
+import io.airlift.airline.Arguments;
+import io.airlift.airline.Command;
+import io.airlift.airline.Option;
+
+@Command(name = ""profileload"", description = ""Low footprint profiling of activity for a period of time"")
+public class ProfileLoad extends NodeToolCmd
+{
+    @Arguments(usage = ""<keyspace> <cfname> <duration>"", description = ""The keyspace, column family name, and duration in milliseconds"")
+    private List<String> args = new ArrayList<>();
+
+    @Option(name = ""-s"", description = ""Capacity of the sampler, higher for more accuracy (Default: 256)"")
+    private int capacity = 256;
+
+    @Option(name = ""-k"", description = ""Number of the top samples to list (Default: 10)"")
+    private int topCount = 10;
+
+    @Option(name = ""-a"", description = ""Comma separated list of samplers to use (Default: all)"")
+    private String samplers = join(SamplerType.values(), ',');
+
+    @Override
+    public void execute(NodeProbe probe)
+    {
+        checkArgument(args.size() == 3 || args.size() == 1 || args.size() == 0, ""Invalid arguments, either [keyspace table duration] or [duration] or no args"");
+        checkArgument(topCount < capacity, ""TopK count (-k) option must be smaller then the summary capacity (-s)"");
+        String keyspace = null;
+        String table = null;
+        Integer duration = 10000;
+        if(args.size() == 3)
+        {
+            keyspace = args.get(0);
+            table = args.get(1);
+            duration = Integer.valueOf(args.get(2));
+        }
+        else if (args.size() == 1)
+        {
+            duration = Integer.valueOf(args.get(0));
+        }
+        // generate the list of samplers
+        List<String> targets = Lists.newArrayList();
+        List<String> available = Arrays.stream(SamplerType.values()).map(Enum::toString).collect(Collectors.toList());
+        for (String s : samplers.split("",""))
+        {
+            String sampler = s.trim().toUpperCase();
+            checkArgument(available.contains(sampler), String.format(""'%s' sampler is not available from: %s"", s, Arrays.toString(SamplerType.values())));
+            targets.add(sampler);
+        }
+
+        Map<String, List<CompositeData>> results;
+        try
+        {
+            if (keyspace == null)
+            {","[{'comment': 'Braces are unnecessary.', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/metrics/Sampler.java,"@@ -0,0 +1,67 @@
+package org.apache.cassandra.metrics;
+
+import java.io.Serializable;
+import java.util.List;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.NamedThreadFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+
+public abstract class Sampler<T>","[{'comment': 'It would be nice to have a jmh benchmark for the new `Sampler`s', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/db/ReadExecutionController.java,"@@ -132,6 +145,15 @@ public void close()
         {
             if (baseOp != null)
                 baseOp.close();
+
+            if (startTimeNanos != -1)
+            {
+                String cql = command.toCQLString();
+                int timeMicros = (int) Math.min(TimeUnit.NANOSECONDS.toMicros(System.nanoTime() - startTimeNanos), Integer.MAX_VALUE);","[{'comment': 'This should be `o.a.c.u.Clock`', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/db/ReadExecutionController.java,"@@ -90,11 +100,11 @@ static ReadExecutionController forCommand(ReadCommand command)
             try
             {
                 baseOp = baseCfs.readOrdering.start();
-                indexController = new ReadExecutionController(indexCfs.readOrdering.start(), indexCfs.metadata(), null, null);
+                indexController = new ReadExecutionController(command, indexCfs.readOrdering.start(), indexCfs.metadata(), null, null);
                 // TODO: this should perhaps not open and maintain a writeOp for the full duration, but instead only *try* to delete stale entries, without blocking if there's no room
                 // as it stands, we open a writeOp and keep it open for the duration to ensure that should this CF get flushed to make room we don't block the reclamation of any room being made
                 writeContext = baseCfs.keyspace.getWriteHandler().createContextForRead();
-                return new ReadExecutionController(baseOp, baseCfs.metadata(), indexController, writeContext);
+                result =  new ReadExecutionController(command, baseOp, baseCfs.metadata(), indexController, writeContext);","[{'comment': 'Nit: Extra space.', 'commenter': 'dineshjoshi'}]"
244,src/java/org/apache/cassandra/metrics/MaxSampler.java,"@@ -0,0 +1,59 @@
+package org.apache.cassandra.metrics;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+
+import com.google.common.collect.MinMaxPriorityQueue;
+
+public abstract class MaxSampler<T> extends Sampler<T>
+{
+    private int capacity;
+    private MinMaxPriorityQueue<Sample<T>> queue;
+    private long endTimeMillis = -1;
+    private final Comparator<Sample<T>> comp = Collections.reverseOrder(Comparator.comparing(p -> p.count));
+
+    public boolean isEnabled()
+    {
+        return endTimeMillis != -1 && clock.currentTimeMillis() <= endTimeMillis;
+    }
+","[{'comment': 'Nit: Extra space.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/commands/Replay.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool.commands;
+
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+
+import io.airlift.airline.Arguments;
+import io.airlift.airline.Command;
+import io.airlift.airline.Option;
+import net.openhft.chronicle.core.io.Closeable;
+import net.openhft.chronicle.queue.ChronicleQueue;
+import net.openhft.chronicle.queue.ChronicleQueueBuilder;
+
+import org.apache.cassandra.tools.fqltool.FQLQuery;
+import org.apache.cassandra.tools.fqltool.FQLQueryIterator;
+import org.apache.cassandra.tools.fqltool.QueryReplayer;
+import org.apache.cassandra.utils.AbstractIterator;
+import org.apache.cassandra.utils.MergeIterator;
+
+/**
+ * replay the contents of a list of paths containing full query logs
+ */
+@Command(name = ""replay"", description = ""Replay full query logs"")
+public class Replay implements Runnable
+{
+    @Arguments(usage = ""<path1> [<path2>...<pathN>]"", description = ""Paths containing the full query logs to replay."", required = true)
+    private List<String> arguments = new ArrayList<>();
+
+    @Option(title = ""target"", name = {""--target""}, description = ""Hosts to replay the logs to, can be repeated to replay to more hosts."")
+    private List<String> targetHosts;
+
+    @Option(title = ""results"", name = { ""--results""}, description = ""Where to store the results of the queries, this should be a directory. Leave this option out to avoid storing results."")
+    private String resultPath;
+
+    @Option(title = ""keyspace"", name = { ""--keyspace""}, description = ""Only replay queries against this keyspace and queries without keyspace set."")
+    private String keyspace;
+
+    @Option(title = ""debug"", name = {""--debug""}, description = ""Debug mode, print all queries executed."")
+    private boolean debug;
+
+    @Option(title = ""use"", name = {""--use""}, description = ""Connect to the cluster(s) using this keyspace."")
+    private String useKeyspace;
+
+    @Option(title = ""legacy"", name = {""--legacyfiles""}, description = ""If the FQL files don't contain keyspace information."")","[{'comment': 'Is this flag necessary post CASSANDRA-14656?\r\n\r\nIf it is needed, a minor nit cleanup to the description: ""A flag to indicate if the FQL files do not contain keyspace information.""', 'commenter': 'jasobrown'}, {'comment': ""no we don't need this here since full query logs have not really been released yet, I'll remove it"", 'commenter': 'krummas'}, {'comment': 'and yes, at some point we should add versioning to the full query logs', 'commenter': 'krummas'}]"
255,src/java/org/apache/cassandra/tools/fqltool/commands/Replay.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool.commands;
+
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+
+import io.airlift.airline.Arguments;
+import io.airlift.airline.Command;
+import io.airlift.airline.Option;
+import net.openhft.chronicle.core.io.Closeable;
+import net.openhft.chronicle.queue.ChronicleQueue;
+import net.openhft.chronicle.queue.ChronicleQueueBuilder;
+
+import org.apache.cassandra.tools.fqltool.FQLQuery;
+import org.apache.cassandra.tools.fqltool.FQLQueryIterator;
+import org.apache.cassandra.tools.fqltool.QueryReplayer;
+import org.apache.cassandra.utils.AbstractIterator;
+import org.apache.cassandra.utils.MergeIterator;
+
+/**
+ * replay the contents of a list of paths containing full query logs
+ */
+@Command(name = ""replay"", description = ""Replay full query logs"")
+public class Replay implements Runnable
+{
+    @Arguments(usage = ""<path1> [<path2>...<pathN>]"", description = ""Paths containing the full query logs to replay."", required = true)
+    private List<String> arguments = new ArrayList<>();
+
+    @Option(title = ""target"", name = {""--target""}, description = ""Hosts to replay the logs to, can be repeated to replay to more hosts."")
+    private List<String> targetHosts;
+
+    @Option(title = ""results"", name = { ""--results""}, description = ""Where to store the results of the queries, this should be a directory. Leave this option out to avoid storing results."")
+    private String resultPath;
+
+    @Option(title = ""keyspace"", name = { ""--keyspace""}, description = ""Only replay queries against this keyspace and queries without keyspace set."")
+    private String keyspace;
+
+    @Option(title = ""debug"", name = {""--debug""}, description = ""Debug mode, print all queries executed."")
+    private boolean debug;
+
+    @Option(title = ""use"", name = {""--use""}, description = ""Connect to the cluster(s) using this keyspace."")
+    private String useKeyspace;
+
+    @Option(title = ""legacy"", name = {""--legacyfiles""}, description = ""If the FQL files don't contain keyspace information."")
+    private boolean legacyFiles;
+
+    @Option(title = ""store_queries"", name = {""--store-queries""}, description = ""Path to store the queries executed. Stores queries in the same order as the result sets are in the result files. Requires --results"")
+    private String queryStorePath;
+
+    @Override
+    public void run()
+    {
+        try","[{'comment': 'Do we need parameter checking here? For example, what if i set no`targetHosts`?', 'commenter': 'jasobrown'}]"
255,src/java/org/apache/cassandra/tools/fqltool/commands/Replay.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool.commands;
+
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+
+import io.airlift.airline.Arguments;
+import io.airlift.airline.Command;
+import io.airlift.airline.Option;
+import net.openhft.chronicle.core.io.Closeable;
+import net.openhft.chronicle.queue.ChronicleQueue;
+import net.openhft.chronicle.queue.ChronicleQueueBuilder;
+
+import org.apache.cassandra.tools.fqltool.FQLQuery;
+import org.apache.cassandra.tools.fqltool.FQLQueryIterator;
+import org.apache.cassandra.tools.fqltool.QueryReplayer;
+import org.apache.cassandra.utils.AbstractIterator;
+import org.apache.cassandra.utils.MergeIterator;
+
+/**
+ * replay the contents of a list of paths containing full query logs
+ */
+@Command(name = ""replay"", description = ""Replay full query logs"")
+public class Replay implements Runnable
+{
+    @Arguments(usage = ""<path1> [<path2>...<pathN>]"", description = ""Paths containing the full query logs to replay."", required = true)
+    private List<String> arguments = new ArrayList<>();
+
+    @Option(title = ""target"", name = {""--target""}, description = ""Hosts to replay the logs to, can be repeated to replay to more hosts."")
+    private List<String> targetHosts;
+
+    @Option(title = ""results"", name = { ""--results""}, description = ""Where to store the results of the queries, this should be a directory. Leave this option out to avoid storing results."")
+    private String resultPath;
+
+    @Option(title = ""keyspace"", name = { ""--keyspace""}, description = ""Only replay queries against this keyspace and queries without keyspace set."")
+    private String keyspace;
+
+    @Option(title = ""debug"", name = {""--debug""}, description = ""Debug mode, print all queries executed."")
+    private boolean debug;
+
+    @Option(title = ""use"", name = {""--use""}, description = ""Connect to the cluster(s) using this keyspace."")
+    private String useKeyspace;
+
+    @Option(title = ""legacy"", name = {""--legacyfiles""}, description = ""If the FQL files don't contain keyspace information."")
+    private boolean legacyFiles;
+
+    @Option(title = ""store_queries"", name = {""--store-queries""}, description = ""Path to store the queries executed. Stores queries in the same order as the result sets are in the result files. Requires --results"")
+    private String queryStorePath;
+
+    @Override
+    public void run()
+    {
+        try
+        {
+            List<File> resultPaths = null;
+            if (resultPath != null)
+            {
+                File basePath = new File(resultPath);
+                if (!basePath.exists() || !basePath.isDirectory())
+                {
+                    System.err.println(""The results path ("" + basePath + "") should be an existing directory"");
+                    System.exit(1);
+                }
+                resultPaths = targetHosts.stream().map(target -> new File(basePath, target)).collect(Collectors.toList());
+                resultPaths.forEach(File::mkdir);
+            }
+            replay(keyspace, arguments, targetHosts, resultPaths, useKeyspace, legacyFiles, queryStorePath, debug);
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public static void replay(String keyspace, List<String> arguments, List<String> targetHosts, List<File> resultPaths, String useKeyspace, boolean legacyFiles, String queryStorePath, boolean debug)
+    {
+        int readAhead = 200; // how many fql queries should we read in to memory to be able to sort them?
+        List<ChronicleQueue> readQueues = null;
+        List<FQLQueryIterator> iterators = null;
+        List<Predicate<FQLQuery>> filters = new ArrayList<>();
+
+        if (keyspace != null)
+            filters.add(fqlQuery -> fqlQuery.keyspace == null || fqlQuery.keyspace.equals(keyspace));
+
+        try
+        {
+            readQueues = arguments.stream().map(s -> ChronicleQueueBuilder.single(s).readOnly(true).build()).collect(Collectors.toList());
+            iterators = readQueues.stream().map(ChronicleQueue::createTailer).map(tailer -> new FQLQueryIterator(tailer, readAhead, legacyFiles)).collect(Collectors.toList());
+            try (MergeIterator<FQLQuery, List<FQLQuery>> iter = MergeIterator.get(iterators, FQLQuery::compareTo, new Reducer());
+                 QueryReplayer replayer = new QueryReplayer(iter, targetHosts, resultPaths, filters, System.out, useKeyspace, queryStorePath, debug))
+            {
+                replayer.replay();
+            }
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+        finally
+        {
+            if (iterators != null)
+                iterators.forEach(AbstractIterator::close);
+            if (readQueues != null)
+                readQueues.forEach(Closeable::close);
+        }
+    }
+
+    @VisibleForTesting
+    public static class Reducer extends MergeIterator.Reducer<FQLQuery, List<FQLQuery>>
+    {
+        List<FQLQuery> queries = new ArrayList<>();
+        public void reduce(int idx, FQLQuery current)
+        {
+            queries.add(current);
+        }
+
+        protected List<FQLQuery> getReduced()
+        {
+            return queries;
+        }
+        protected void onKeyChange()
+        {
+            queries.clear();
+        }
+    }
+","[{'comment': 'petty nit: superfluous blank lines', 'commenter': 'jasobrown'}]"
255,src/java/org/apache/cassandra/tools/fqltool/QueryReplayer.java,"@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import javax.annotation.Nullable;
+
+import com.google.common.util.concurrent.FutureCallback;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+
+import com.codahale.metrics.MetricRegistry;
+import com.codahale.metrics.Timer;
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class QueryReplayer implements Closeable
+{
+    private final ExecutorService es = Executors.newFixedThreadPool(1);
+    private final Iterator<List<FQLQuery>> queryIterator;
+    private final List<String> targetHosts;
+    private final List<Cluster> targetClusters;
+    private final List<Predicate<FQLQuery>> filters;
+    private final List<Session> sessions;
+    private final ResultHandler resultHandler;
+    private final MetricRegistry metrics = new MetricRegistry();
+    private final boolean debug;
+    private final PrintStream out;
+
+    public QueryReplayer(Iterator<List<FQLQuery>> queryIterator,
+                         List<String> targetHosts,
+                         List<File> resultPaths,
+                         List<Predicate<FQLQuery>> filters,
+                         PrintStream out,
+                         String useKeyspace,
+                         String queryFilePathString,
+                         boolean debug)
+    {
+        this.queryIterator = queryIterator;
+        this.targetHosts = targetHosts;
+        targetClusters = targetHosts.stream().map(h -> Cluster.builder().addContactPoint(h).build()).collect(Collectors.toList());
+        this.filters = filters;
+        sessions = useKeyspace != null ?
+                   targetClusters.stream().map(c -> c.connect(useKeyspace)).collect(Collectors.toList()) :
+                   targetClusters.stream().map(Cluster::connect).collect(Collectors.toList());
+        File queryFilePath = queryFilePathString != null ? new File(queryFilePathString) : null;
+        resultHandler = new ResultHandler(targetHosts, resultPaths, queryFilePath);
+        this.debug = debug;
+        this.out = out;
+    }
+
+    public void replay()
+    {
+        while (queryIterator.hasNext())
+        {
+            List<FQLQuery> queries = queryIterator.next();
+            for (FQLQuery query : queries)
+            {
+                if (filters.stream().anyMatch(f -> !f.test(query)))
+                    continue;
+                try (Timer.Context ctx = metrics.timer(""queries"").time())
+                {
+                    List<ListenableFuture<ResultHandler.ComparableResultSet>> results = new ArrayList<>(sessions.size());
+                    for (Session session : sessions)
+                    {
+                        try
+                        {
+                            if (query.keyspace != null && !query.keyspace.equals(session.getLoggedKeyspace()))
+                            {
+                                if (debug)
+                                {
+                                    out.println(String.format(""Switching keyspace from %s to %s"", session.getLoggedKeyspace(), query.keyspace));
+                                }
+                                session.execute(""USE "" + query.keyspace);
+                            }
+                        }
+                        catch (Throwable t)
+                        {
+                            out.println(""USE ""+query.keyspace+"" failed: ""+t.getMessage());","[{'comment': 'petty nit: spacing around the `+` signs.', 'commenter': 'jasobrown'}, {'comment': 'Nit: spacing', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/QueryReplayer.java,"@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import javax.annotation.Nullable;
+
+import com.google.common.util.concurrent.FutureCallback;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+
+import com.codahale.metrics.MetricRegistry;
+import com.codahale.metrics.Timer;
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class QueryReplayer implements Closeable
+{
+    private final ExecutorService es = Executors.newFixedThreadPool(1);
+    private final Iterator<List<FQLQuery>> queryIterator;
+    private final List<String> targetHosts;
+    private final List<Cluster> targetClusters;
+    private final List<Predicate<FQLQuery>> filters;
+    private final List<Session> sessions;
+    private final ResultHandler resultHandler;
+    private final MetricRegistry metrics = new MetricRegistry();
+    private final boolean debug;
+    private final PrintStream out;
+
+    public QueryReplayer(Iterator<List<FQLQuery>> queryIterator,
+                         List<String> targetHosts,
+                         List<File> resultPaths,
+                         List<Predicate<FQLQuery>> filters,
+                         PrintStream out,
+                         String useKeyspace,
+                         String queryFilePathString,
+                         boolean debug)
+    {
+        this.queryIterator = queryIterator;
+        this.targetHosts = targetHosts;
+        targetClusters = targetHosts.stream().map(h -> Cluster.builder().addContactPoint(h).build()).collect(Collectors.toList());
+        this.filters = filters;
+        sessions = useKeyspace != null ?
+                   targetClusters.stream().map(c -> c.connect(useKeyspace)).collect(Collectors.toList()) :
+                   targetClusters.stream().map(Cluster::connect).collect(Collectors.toList());
+        File queryFilePath = queryFilePathString != null ? new File(queryFilePathString) : null;
+        resultHandler = new ResultHandler(targetHosts, resultPaths, queryFilePath);
+        this.debug = debug;
+        this.out = out;
+    }
+
+    public void replay()
+    {
+        while (queryIterator.hasNext())
+        {
+            List<FQLQuery> queries = queryIterator.next();
+            for (FQLQuery query : queries)
+            {
+                if (filters.stream().anyMatch(f -> !f.test(query)))
+                    continue;
+                try (Timer.Context ctx = metrics.timer(""queries"").time())
+                {
+                    List<ListenableFuture<ResultHandler.ComparableResultSet>> results = new ArrayList<>(sessions.size());
+                    for (Session session : sessions)
+                    {
+                        try
+                        {
+                            if (query.keyspace != null && !query.keyspace.equals(session.getLoggedKeyspace()))
+                            {
+                                if (debug)
+                                {
+                                    out.println(String.format(""Switching keyspace from %s to %s"", session.getLoggedKeyspace(), query.keyspace));
+                                }
+                                session.execute(""USE "" + query.keyspace);
+                            }
+                        }
+                        catch (Throwable t)
+                        {
+                            out.println(""USE ""+query.keyspace+"" failed: ""+t.getMessage());
+                        }
+                        if (debug)
+                        {
+                            out.println(""Executing query:"");
+                            out.println(query);
+                        }
+                        results.add(query.execute(session));
+                    }
+
+                    ListenableFuture<List<ResultHandler.ComparableResultSet>> resultList = Futures.allAsList(results);
+
+                    Futures.addCallback(resultList, new FutureCallback<List<ResultHandler.ComparableResultSet>>()
+                    {
+                        public void onSuccess(@Nullable List<ResultHandler.ComparableResultSet> resultSets)
+                        {
+                            resultHandler.handleResults(query, resultSets);
+                        }
+
+                        public void onFailure(Throwable throwable)
+                        {
+                            throw new AssertionError(""Errors should be handled in FQLQuery.execute"", throwable);
+                        }
+                    }, es);
+
+                    FBUtilities.waitOnFuture(resultList);
+                }
+                catch (Throwable t)
+                {
+                    out.println(""QUERY "" + query +"" got exception: ""+t.getMessage());","[{'comment': 'petty nit: spacing around the `+` signs.', 'commenter': 'jasobrown'}, {'comment': 'Nit: spacing', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/commands/Replay.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool.commands;
+
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+
+import io.airlift.airline.Arguments;
+import io.airlift.airline.Command;
+import io.airlift.airline.Option;
+import net.openhft.chronicle.core.io.Closeable;
+import net.openhft.chronicle.queue.ChronicleQueue;
+import net.openhft.chronicle.queue.ChronicleQueueBuilder;
+
+import org.apache.cassandra.tools.fqltool.FQLQuery;
+import org.apache.cassandra.tools.fqltool.FQLQueryIterator;
+import org.apache.cassandra.tools.fqltool.QueryReplayer;
+import org.apache.cassandra.utils.AbstractIterator;
+import org.apache.cassandra.utils.MergeIterator;
+
+/**
+ * replay the contents of a list of paths containing full query logs
+ */
+@Command(name = ""replay"", description = ""Replay full query logs"")
+public class Replay implements Runnable
+{
+    @Arguments(usage = ""<path1> [<path2>...<pathN>]"", description = ""Paths containing the full query logs to replay."", required = true)
+    private List<String> arguments = new ArrayList<>();
+
+    @Option(title = ""target"", name = {""--target""}, description = ""Hosts to replay the logs to, can be repeated to replay to more hosts."")
+    private List<String> targetHosts;
+
+    @Option(title = ""results"", name = { ""--results""}, description = ""Where to store the results of the queries, this should be a directory. Leave this option out to avoid storing results."")
+    private String resultPath;
+
+    @Option(title = ""keyspace"", name = { ""--keyspace""}, description = ""Only replay queries against this keyspace and queries without keyspace set."")
+    private String keyspace;
+
+    @Option(title = ""debug"", name = {""--debug""}, description = ""Debug mode, print all queries executed."")
+    private boolean debug;
+
+    @Option(title = ""use"", name = {""--use""}, description = ""Connect to the cluster(s) using this keyspace."")","[{'comment': ""This isn't entirely clear to me. The user can state which keyspace to initially connect to, but then we'll switch the keyspaces on -demand in `QueryReplayer.replayer()`. What is the intent with the `use` field?"", 'commenter': 'jasobrown'}, {'comment': 'this was a left over from before we had keyspace in the full query logs, removed', 'commenter': 'krummas'}]"
255,src/java/org/apache/cassandra/tools/fqltool/FQLQueryIterator.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.util.PriorityQueue;
+
+import net.openhft.chronicle.queue.ExcerptTailer;
+import org.apache.cassandra.utils.AbstractIterator;
+
+public class FQLQueryIterator extends AbstractIterator<FQLQuery>
+{
+    private final PriorityQueue<FQLQuery> pq;","[{'comment': 'Please add a small comment here as to why a `PriorityQueue` (""so we can sort the entries by timestamp of the query in logs, not just the order in the file""). This would just be a nice, quick little helper.', 'commenter': 'jasobrown'}]"
255,src/java/org/apache/cassandra/tools/fqltool/FQLQuery.java,"@@ -0,0 +1,263 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+import java.util.stream.Collectors;
+
+import com.google.common.primitives.Longs;
+import com.google.common.util.concurrent.FluentFuture;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+import com.google.common.util.concurrent.MoreExecutors;
+
+import com.datastax.driver.core.BatchStatement;
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.ResultSetFuture;
+import com.datastax.driver.core.Session;
+import com.datastax.driver.core.SimpleStatement;
+import org.apache.cassandra.audit.FullQueryLogger;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.binlog.BinLog;
+
+public abstract class FQLQuery implements Comparable<FQLQuery>
+{
+    public final long queryTime;
+    public final QueryOptions queryOptions;
+    public final int protocolVersion;
+    public final String keyspace;
+
+    public FQLQuery(String keyspace, int protocolVersion, QueryOptions queryOptions, long queryTime)
+    {
+        this.queryTime = queryTime;
+        this.queryOptions = queryOptions;
+        this.protocolVersion = protocolVersion;
+        this.keyspace = keyspace;
+    }
+
+    public abstract ListenableFuture<ResultHandler.ComparableResultSet> execute(Session session);
+
+    /**
+     * used when storing the queries executed
+     */
+    public abstract BinLog.ReleaseableWriteMarshallable toMarshallable();
+
+    /**
+     * Make sure we catch any query errors
+     *
+     * On error, this creates a failed ComparableResultSet with the exception set to be able to store
+     * this fact in the result file and handle comparison of failed result sets.
+     */
+    ListenableFuture<ResultHandler.ComparableResultSet> handleErrors(ListenableFuture<ResultSet> result)
+    {
+        FluentFuture<ResultHandler.ComparableResultSet> fluentFuture = FluentFuture.from(result)
+                                                                                   .transform(DriverResultSet::new, MoreExecutors.directExecutor());
+        return fluentFuture.catching(Throwable.class, DriverResultSet::failed, MoreExecutors.directExecutor());
+    }
+
+    public boolean equals(Object o)
+    {
+        if (this == o) return true;
+        if (!(o instanceof FQLQuery)) return false;
+        FQLQuery fqlQuery = (FQLQuery) o;
+        return queryTime == fqlQuery.queryTime &&
+               protocolVersion == fqlQuery.protocolVersion &&
+               queryOptions.getValues().equals(fqlQuery.queryOptions.getValues()) &&
+               Objects.equals(keyspace, fqlQuery.keyspace);
+    }
+
+    public int hashCode()
+    {
+        return Objects.hash(queryTime, queryOptions, protocolVersion, keyspace);
+    }
+
+    public int compareTo(FQLQuery other)
+    {
+        return Longs.compare(queryTime, other.queryTime);
+    }
+
+    public static class Single extends FQLQuery
+    {
+        public final String query;
+        public final List<ByteBuffer> values;
+
+        public Single(String keyspace, int protocolVersion, QueryOptions queryOptions, long queryTime, String queryString, List<ByteBuffer> values)
+        {
+            super(keyspace, protocolVersion, queryOptions, queryTime);
+            this.query = queryString;
+            this.values = values;
+        }
+
+        @Override
+        public String toString()
+        {
+            return String.format(""Query = %s, Options = %s, Values = %s"",
+                                 query,
+                                 queryOptions,
+                                 values.stream().map(ByteBufferUtil::bytesToHex).collect(Collectors.joining("","")));
+        }
+
+        public ListenableFuture<ResultHandler.ComparableResultSet> execute(Session session)
+        {
+            SimpleStatement ss = new SimpleStatement(query, values.toArray());
+            ss.setConsistencyLevel(ConsistencyLevel.valueOf(queryOptions.getConsistency().name()));
+            ListenableFuture<ResultSet> future = session.executeAsync(ss);
+            return handleErrors(future);
+        }
+
+        public BinLog.ReleaseableWriteMarshallable toMarshallable()
+        {
+            return new FullQueryLogger.WeighableMarshallableQuery(query, keyspace, queryOptions, queryTime);
+        }
+
+        public int compareTo(FQLQuery other)
+        {
+            int cmp = super.compareTo(other);
+
+            if (cmp == 0)
+            {
+                if (other instanceof Batch)
+                    return -1;
+
+                Single singleQuery = (Single) other;
+
+                cmp = query.compareTo(singleQuery.query);
+                if (cmp == 0)
+                {
+                    if (values.size() != singleQuery.values.size())
+                        return values.size() - singleQuery.values.size();
+                    for (int i = 0; i < values.size(); i++)
+                    {
+                        cmp = values.get(i).compareTo(singleQuery.values.get(i));
+                        if (cmp != 0)
+                            return cmp;
+                    }
+                }
+            }
+            return cmp;
+        }
+
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (!(o instanceof Single)) return false;
+            if (!super.equals(o)) return false;
+            Single single = (Single) o;
+            return Objects.equals(query, single.query) &&
+                   Objects.equals(values, single.values);
+        }
+
+        public int hashCode()
+        {
+            return Objects.hash(super.hashCode(), query, values);
+        }
+    }
+
+    public static class Batch extends FQLQuery
+    {
+        public final BatchStatement.Type batchType;
+        public final List<Single> queries;
+
+        public Batch(String keyspace, int protocolVersion, QueryOptions queryOptions, long queryTime, BatchStatement.Type batchType, List<String> queries, List<List<ByteBuffer>> values)
+        {
+            super(keyspace, protocolVersion, queryOptions, queryTime);
+            this.batchType = batchType;
+            this.queries = new ArrayList<>(queries.size());
+            for (int i = 0; i < queries.size(); i++)
+                this.queries.add(new Single(keyspace, protocolVersion, queryOptions, queryTime, queries.get(i), values.get(i)));
+        }
+
+        public ListenableFuture<ResultHandler.ComparableResultSet> execute(Session session)
+        {
+            BatchStatement bs = new BatchStatement(batchType);
+            bs.setConsistencyLevel(ConsistencyLevel.valueOf(queryOptions.getConsistency().name()));
+            for (Single query : queries)
+            {
+                bs.add(new SimpleStatement(query.query, query.values.toArray()));
+            }
+            ListenableFuture<ResultSet> future = session.executeAsync(bs);
+            return handleErrors(future);
+        }
+
+        public int compareTo(FQLQuery other)
+        {
+            int cmp = super.compareTo(other);
+
+            if (cmp == 0)
+            {
+                if (other instanceof Single)
+                    return 1;
+
+                Batch otherBatch = (Batch) other;
+                if (queries.size() != otherBatch.queries.size())","[{'comment': 'Only the length of of the queries lists are compared, not the actual contents of the queries lists. (`Single.compareTo()` compares the query strings).', 'commenter': 'jasobrown'}, {'comment': 'right below this we iterate over the Single-queries and compare them one-by-one', 'commenter': 'krummas'}]"
255,src/java/org/apache/cassandra/tools/fqltool/QueryReplayer.java,"@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import javax.annotation.Nullable;
+
+import com.google.common.util.concurrent.FutureCallback;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+
+import com.codahale.metrics.MetricRegistry;
+import com.codahale.metrics.Timer;
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class QueryReplayer implements Closeable
+{
+    private final ExecutorService es = Executors.newFixedThreadPool(1);
+    private final Iterator<List<FQLQuery>> queryIterator;
+    private final List<String> targetHosts;","[{'comment': 'This variable is unused. Its only used in the constructor.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/QueryReplayer.java,"@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import javax.annotation.Nullable;
+
+import com.google.common.util.concurrent.FutureCallback;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+
+import com.codahale.metrics.MetricRegistry;
+import com.codahale.metrics.Timer;
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class QueryReplayer implements Closeable
+{
+    private final ExecutorService es = Executors.newFixedThreadPool(1);
+    private final Iterator<List<FQLQuery>> queryIterator;
+    private final List<String> targetHosts;
+    private final List<Cluster> targetClusters;
+    private final List<Predicate<FQLQuery>> filters;
+    private final List<Session> sessions;
+    private final ResultHandler resultHandler;
+    private final MetricRegistry metrics = new MetricRegistry();
+    private final boolean debug;
+    private final PrintStream out;
+
+    public QueryReplayer(Iterator<List<FQLQuery>> queryIterator,
+                         List<String> targetHosts,
+                         List<File> resultPaths,
+                         List<Predicate<FQLQuery>> filters,
+                         PrintStream out,
+                         String useKeyspace,
+                         String queryFilePathString,
+                         boolean debug)
+    {
+        this.queryIterator = queryIterator;
+        this.targetHosts = targetHosts;
+        targetClusters = targetHosts.stream().map(h -> Cluster.builder().addContactPoint(h).build()).collect(Collectors.toList());
+        this.filters = filters;
+        sessions = useKeyspace != null ?
+                   targetClusters.stream().map(c -> c.connect(useKeyspace)).collect(Collectors.toList()) :
+                   targetClusters.stream().map(Cluster::connect).collect(Collectors.toList());
+        File queryFilePath = queryFilePathString != null ? new File(queryFilePathString) : null;
+        resultHandler = new ResultHandler(targetHosts, resultPaths, queryFilePath);
+        this.debug = debug;
+        this.out = out;
+    }
+
+    public void replay()
+    {
+        while (queryIterator.hasNext())
+        {
+            List<FQLQuery> queries = queryIterator.next();
+            for (FQLQuery query : queries)
+            {
+                if (filters.stream().anyMatch(f -> !f.test(query)))
+                    continue;
+                try (Timer.Context ctx = metrics.timer(""queries"").time())
+                {
+                    List<ListenableFuture<ResultHandler.ComparableResultSet>> results = new ArrayList<>(sessions.size());
+                    for (Session session : sessions)
+                    {
+                        try
+                        {
+                            if (query.keyspace != null && !query.keyspace.equals(session.getLoggedKeyspace()))
+                            {
+                                if (debug)
+                                {","[{'comment': 'Single statement blocks do not require braces.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/QueryReplayer.java,"@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import javax.annotation.Nullable;
+
+import com.google.common.util.concurrent.FutureCallback;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+
+import com.codahale.metrics.MetricRegistry;
+import com.codahale.metrics.Timer;
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class QueryReplayer implements Closeable
+{
+    private final ExecutorService es = Executors.newFixedThreadPool(1);
+    private final Iterator<List<FQLQuery>> queryIterator;
+    private final List<String> targetHosts;
+    private final List<Cluster> targetClusters;
+    private final List<Predicate<FQLQuery>> filters;
+    private final List<Session> sessions;
+    private final ResultHandler resultHandler;
+    private final MetricRegistry metrics = new MetricRegistry();
+    private final boolean debug;
+    private final PrintStream out;
+
+    public QueryReplayer(Iterator<List<FQLQuery>> queryIterator,
+                         List<String> targetHosts,
+                         List<File> resultPaths,
+                         List<Predicate<FQLQuery>> filters,
+                         PrintStream out,
+                         String useKeyspace,
+                         String queryFilePathString,
+                         boolean debug)
+    {
+        this.queryIterator = queryIterator;
+        this.targetHosts = targetHosts;
+        targetClusters = targetHosts.stream().map(h -> Cluster.builder().addContactPoint(h).build()).collect(Collectors.toList());
+        this.filters = filters;
+        sessions = useKeyspace != null ?
+                   targetClusters.stream().map(c -> c.connect(useKeyspace)).collect(Collectors.toList()) :
+                   targetClusters.stream().map(Cluster::connect).collect(Collectors.toList());
+        File queryFilePath = queryFilePathString != null ? new File(queryFilePathString) : null;
+        resultHandler = new ResultHandler(targetHosts, resultPaths, queryFilePath);
+        this.debug = debug;
+        this.out = out;
+    }
+
+    public void replay()
+    {
+        while (queryIterator.hasNext())
+        {
+            List<FQLQuery> queries = queryIterator.next();
+            for (FQLQuery query : queries)
+            {
+                if (filters.stream().anyMatch(f -> !f.test(query)))
+                    continue;
+                try (Timer.Context ctx = metrics.timer(""queries"").time())
+                {
+                    List<ListenableFuture<ResultHandler.ComparableResultSet>> results = new ArrayList<>(sessions.size());
+                    for (Session session : sessions)
+                    {
+                        try
+                        {
+                            if (query.keyspace != null && !query.keyspace.equals(session.getLoggedKeyspace()))
+                            {
+                                if (debug)
+                                {
+                                    out.println(String.format(""Switching keyspace from %s to %s"", session.getLoggedKeyspace(), query.keyspace));","[{'comment': ""I'm curious why not use `out.printf` instead?"", 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/QueryReplayer.java,"@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import javax.annotation.Nullable;
+
+import com.google.common.util.concurrent.FutureCallback;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+
+import com.codahale.metrics.MetricRegistry;
+import com.codahale.metrics.Timer;
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class QueryReplayer implements Closeable
+{
+    private final ExecutorService es = Executors.newFixedThreadPool(1);
+    private final Iterator<List<FQLQuery>> queryIterator;
+    private final List<String> targetHosts;
+    private final List<Cluster> targetClusters;
+    private final List<Predicate<FQLQuery>> filters;
+    private final List<Session> sessions;
+    private final ResultHandler resultHandler;
+    private final MetricRegistry metrics = new MetricRegistry();
+    private final boolean debug;
+    private final PrintStream out;
+
+    public QueryReplayer(Iterator<List<FQLQuery>> queryIterator,
+                         List<String> targetHosts,
+                         List<File> resultPaths,
+                         List<Predicate<FQLQuery>> filters,
+                         PrintStream out,
+                         String useKeyspace,
+                         String queryFilePathString,
+                         boolean debug)
+    {
+        this.queryIterator = queryIterator;
+        this.targetHosts = targetHosts;
+        targetClusters = targetHosts.stream().map(h -> Cluster.builder().addContactPoint(h).build()).collect(Collectors.toList());
+        this.filters = filters;
+        sessions = useKeyspace != null ?
+                   targetClusters.stream().map(c -> c.connect(useKeyspace)).collect(Collectors.toList()) :
+                   targetClusters.stream().map(Cluster::connect).collect(Collectors.toList());
+        File queryFilePath = queryFilePathString != null ? new File(queryFilePathString) : null;
+        resultHandler = new ResultHandler(targetHosts, resultPaths, queryFilePath);
+        this.debug = debug;
+        this.out = out;
+    }
+
+    public void replay()
+    {
+        while (queryIterator.hasNext())
+        {
+            List<FQLQuery> queries = queryIterator.next();
+            for (FQLQuery query : queries)
+            {
+                if (filters.stream().anyMatch(f -> !f.test(query)))
+                    continue;
+                try (Timer.Context ctx = metrics.timer(""queries"").time())
+                {
+                    List<ListenableFuture<ResultHandler.ComparableResultSet>> results = new ArrayList<>(sessions.size());
+                    for (Session session : sessions)
+                    {
+                        try
+                        {
+                            if (query.keyspace != null && !query.keyspace.equals(session.getLoggedKeyspace()))
+                            {
+                                if (debug)
+                                {
+                                    out.println(String.format(""Switching keyspace from %s to %s"", session.getLoggedKeyspace(), query.keyspace));
+                                }
+                                session.execute(""USE "" + query.keyspace);
+                            }
+                        }
+                        catch (Throwable t)
+                        {
+                            out.println(""USE ""+query.keyspace+"" failed: ""+t.getMessage());
+                        }
+                        if (debug)
+                        {
+                            out.println(""Executing query:"");
+                            out.println(query);
+                        }
+                        results.add(query.execute(session));
+                    }
+
+                    ListenableFuture<List<ResultHandler.ComparableResultSet>> resultList = Futures.allAsList(results);
+
+                    Futures.addCallback(resultList, new FutureCallback<List<ResultHandler.ComparableResultSet>>()
+                    {
+                        public void onSuccess(@Nullable List<ResultHandler.ComparableResultSet> resultSets)
+                        {
+                            resultHandler.handleResults(query, resultSets);
+                        }
+
+                        public void onFailure(Throwable throwable)
+                        {
+                            throw new AssertionError(""Errors should be handled in FQLQuery.execute"", throwable);
+                        }
+                    }, es);
+
+                    FBUtilities.waitOnFuture(resultList);
+                }
+                catch (Throwable t)
+                {
+                    out.println(""QUERY "" + query +"" got exception: ""+t.getMessage());
+                }
+
+                Timer timer = metrics.timer(""queries"");
+                if (timer.getCount() % 5000 == 0)","[{'comment': 'Is there a reason to choose 5000? Also, it would be better if it were constant and not a magic number.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/FQLQueryIterator.java,"@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.util.PriorityQueue;
+
+import net.openhft.chronicle.queue.ExcerptTailer;
+import org.apache.cassandra.utils.AbstractIterator;
+
+public class FQLQueryIterator extends AbstractIterator<FQLQuery>
+{
+    private final PriorityQueue<FQLQuery> pq;
+    private final ExcerptTailer tailer;
+    private final FQLQueryReader reader;
+
+    /**
+     * Create an iterator over the FQLQueries in tailer
+     *
+     * Reads up to readAhead queries in to memory to be able to sort them (the files are mostly sorted already)
+     */
+    public FQLQueryIterator(ExcerptTailer tailer, int readAhead, boolean legacyFiles)
+    {
+        assert readAhead > 0 : ""ReadAhead needs to be > 0"";","[{'comment': 'Nit: `ReadAhead` -> `readAhead` in the message', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/DriverResultSet.java,"@@ -0,0 +1,253 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import com.google.common.collect.AbstractIterator;
+
+import com.datastax.driver.core.ColumnDefinitions;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.Row;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+/**
+ * Wraps a result set from the driver so that we can reuse the compare code when reading
+ * up a result set produced by ResultStore.
+ */
+public class DriverResultSet implements ResultHandler.ComparableResultSet
+{
+    private final ResultSet resultSet;
+    private final Throwable failureException;
+
+    public DriverResultSet(ResultSet resultSet)
+    {
+        this(resultSet, null);
+    }
+
+    private DriverResultSet(ResultSet res, Throwable failureException)
+    {
+        resultSet = res;
+        this.failureException = failureException;
+    }
+
+    public static DriverResultSet failed(Throwable ex)
+    {
+        return new DriverResultSet(null, ex);
+    }
+
+    public ResultHandler.ComparableColumnDefinitions getColumnDefinitions()
+    {
+        if (wasFailed())
+            return new DriverColumnDefinitions(null, true);
+
+        return new DriverColumnDefinitions(resultSet.getColumnDefinitions());
+    }
+
+    public boolean wasFailed()
+    {
+        return failureException != null;
+    }
+
+    public Throwable getFailureException()
+    {
+        return failureException;
+    }
+
+    public Iterator<ResultHandler.ComparableRow> iterator()
+    {
+        if (wasFailed())
+            return Collections.emptyListIterator();
+        return new AbstractIterator<ResultHandler.ComparableRow>()
+        {
+            Iterator<Row> iter = resultSet.iterator();
+            protected ResultHandler.ComparableRow computeNext()
+            {
+                if (iter.hasNext())
+                {","[{'comment': 'Unnecessary braces for single statement block (there are bunch of these in this file).', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/FQLQuery.java,"@@ -0,0 +1,263 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+import java.util.stream.Collectors;
+
+import com.google.common.primitives.Longs;
+import com.google.common.util.concurrent.FluentFuture;
+import com.google.common.util.concurrent.Futures;","[{'comment': 'unused import.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/FQLQuery.java,"@@ -0,0 +1,263 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+import java.util.stream.Collectors;
+
+import com.google.common.primitives.Longs;
+import com.google.common.util.concurrent.FluentFuture;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+import com.google.common.util.concurrent.MoreExecutors;
+
+import com.datastax.driver.core.BatchStatement;
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.ResultSetFuture;","[{'comment': 'unused import.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/FQLQuery.java,"@@ -0,0 +1,263 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+import java.util.stream.Collectors;
+
+import com.google.common.primitives.Longs;
+import com.google.common.util.concurrent.FluentFuture;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+import com.google.common.util.concurrent.MoreExecutors;
+
+import com.datastax.driver.core.BatchStatement;
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.ResultSetFuture;
+import com.datastax.driver.core.Session;
+import com.datastax.driver.core.SimpleStatement;
+import org.apache.cassandra.audit.FullQueryLogger;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.binlog.BinLog;
+
+public abstract class FQLQuery implements Comparable<FQLQuery>
+{
+    public final long queryTime;
+    public final QueryOptions queryOptions;
+    public final int protocolVersion;
+    public final String keyspace;
+
+    public FQLQuery(String keyspace, int protocolVersion, QueryOptions queryOptions, long queryTime)","[{'comment': ""Is there a reason why you didn't use `o.a.c.t.ProtocolVersion`?"", 'commenter': 'dineshjoshi'}, {'comment': ""not really, it is stored as an int in the fql files and we don't use it (yet)"", 'commenter': 'krummas'}]"
255,src/java/org/apache/cassandra/tools/fqltool/FQLQuery.java,"@@ -0,0 +1,263 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+import java.util.stream.Collectors;
+
+import com.google.common.primitives.Longs;
+import com.google.common.util.concurrent.FluentFuture;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+import com.google.common.util.concurrent.MoreExecutors;
+
+import com.datastax.driver.core.BatchStatement;
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.ResultSetFuture;
+import com.datastax.driver.core.Session;
+import com.datastax.driver.core.SimpleStatement;
+import org.apache.cassandra.audit.FullQueryLogger;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.binlog.BinLog;
+
+public abstract class FQLQuery implements Comparable<FQLQuery>
+{
+    public final long queryTime;
+    public final QueryOptions queryOptions;
+    public final int protocolVersion;
+    public final String keyspace;
+
+    public FQLQuery(String keyspace, int protocolVersion, QueryOptions queryOptions, long queryTime)
+    {
+        this.queryTime = queryTime;
+        this.queryOptions = queryOptions;
+        this.protocolVersion = protocolVersion;
+        this.keyspace = keyspace;
+    }
+
+    public abstract ListenableFuture<ResultHandler.ComparableResultSet> execute(Session session);
+
+    /**
+     * used when storing the queries executed
+     */
+    public abstract BinLog.ReleaseableWriteMarshallable toMarshallable();
+
+    /**
+     * Make sure we catch any query errors
+     *
+     * On error, this creates a failed ComparableResultSet with the exception set to be able to store
+     * this fact in the result file and handle comparison of failed result sets.
+     */
+    ListenableFuture<ResultHandler.ComparableResultSet> handleErrors(ListenableFuture<ResultSet> result)
+    {
+        FluentFuture<ResultHandler.ComparableResultSet> fluentFuture = FluentFuture.from(result)
+                                                                                   .transform(DriverResultSet::new, MoreExecutors.directExecutor());
+        return fluentFuture.catching(Throwable.class, DriverResultSet::failed, MoreExecutors.directExecutor());
+    }
+
+    public boolean equals(Object o)
+    {
+        if (this == o) return true;
+        if (!(o instanceof FQLQuery)) return false;
+        FQLQuery fqlQuery = (FQLQuery) o;
+        return queryTime == fqlQuery.queryTime &&
+               protocolVersion == fqlQuery.protocolVersion &&
+               queryOptions.getValues().equals(fqlQuery.queryOptions.getValues()) &&
+               Objects.equals(keyspace, fqlQuery.keyspace);
+    }
+
+    public int hashCode()
+    {
+        return Objects.hash(queryTime, queryOptions, protocolVersion, keyspace);
+    }
+
+    public int compareTo(FQLQuery other)
+    {
+        return Longs.compare(queryTime, other.queryTime);
+    }
+
+    public static class Single extends FQLQuery
+    {
+        public final String query;
+        public final List<ByteBuffer> values;
+
+        public Single(String keyspace, int protocolVersion, QueryOptions queryOptions, long queryTime, String queryString, List<ByteBuffer> values)
+        {
+            super(keyspace, protocolVersion, queryOptions, queryTime);
+            this.query = queryString;
+            this.values = values;
+        }
+
+        @Override
+        public String toString()
+        {
+            return String.format(""Query = %s, Options = %s, Values = %s"",
+                                 query,
+                                 queryOptions,
+                                 values.stream().map(ByteBufferUtil::bytesToHex).collect(Collectors.joining("","")));
+        }
+
+        public ListenableFuture<ResultHandler.ComparableResultSet> execute(Session session)
+        {
+            SimpleStatement ss = new SimpleStatement(query, values.toArray());
+            ss.setConsistencyLevel(ConsistencyLevel.valueOf(queryOptions.getConsistency().name()));
+            ListenableFuture<ResultSet> future = session.executeAsync(ss);
+            return handleErrors(future);
+        }
+
+        public BinLog.ReleaseableWriteMarshallable toMarshallable()
+        {
+            return new FullQueryLogger.WeighableMarshallableQuery(query, keyspace, queryOptions, queryTime);
+        }
+
+        public int compareTo(FQLQuery other)
+        {
+            int cmp = super.compareTo(other);
+
+            if (cmp == 0)
+            {
+                if (other instanceof Batch)
+                    return -1;
+
+                Single singleQuery = (Single) other;
+
+                cmp = query.compareTo(singleQuery.query);
+                if (cmp == 0)
+                {
+                    if (values.size() != singleQuery.values.size())
+                        return values.size() - singleQuery.values.size();
+                    for (int i = 0; i < values.size(); i++)
+                    {
+                        cmp = values.get(i).compareTo(singleQuery.values.get(i));
+                        if (cmp != 0)
+                            return cmp;
+                    }
+                }
+            }
+            return cmp;
+        }
+
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (!(o instanceof Single)) return false;
+            if (!super.equals(o)) return false;
+            Single single = (Single) o;
+            return Objects.equals(query, single.query) &&
+                   Objects.equals(values, single.values);
+        }
+
+        public int hashCode()
+        {
+            return Objects.hash(super.hashCode(), query, values);
+        }
+    }
+
+    public static class Batch extends FQLQuery
+    {
+        public final BatchStatement.Type batchType;
+        public final List<Single> queries;
+
+        public Batch(String keyspace, int protocolVersion, QueryOptions queryOptions, long queryTime, BatchStatement.Type batchType, List<String> queries, List<List<ByteBuffer>> values)
+        {
+            super(keyspace, protocolVersion, queryOptions, queryTime);
+            this.batchType = batchType;
+            this.queries = new ArrayList<>(queries.size());
+            for (int i = 0; i < queries.size(); i++)
+                this.queries.add(new Single(keyspace, protocolVersion, queryOptions, queryTime, queries.get(i), values.get(i)));
+        }
+
+        public ListenableFuture<ResultHandler.ComparableResultSet> execute(Session session)
+        {
+            BatchStatement bs = new BatchStatement(batchType);
+            bs.setConsistencyLevel(ConsistencyLevel.valueOf(queryOptions.getConsistency().name()));
+            for (Single query : queries)
+            {
+                bs.add(new SimpleStatement(query.query, query.values.toArray()));
+            }
+            ListenableFuture<ResultSet> future = session.executeAsync(bs);
+            return handleErrors(future);
+        }
+
+        public int compareTo(FQLQuery other)
+        {
+            int cmp = super.compareTo(other);
+
+            if (cmp == 0)
+            {
+                if (other instanceof Single)
+                    return 1;
+
+                Batch otherBatch = (Batch) other;
+                if (queries.size() != otherBatch.queries.size())
+                    return queries.size() - otherBatch.queries.size();
+                for (int i = 0; i < queries.size(); i++)
+                {
+                    cmp = queries.get(i).compareTo(otherBatch.queries.get(i));
+                    if (cmp != 0)
+                        return cmp;
+                }
+            }
+            return cmp;
+        }
+
+        public BinLog.ReleaseableWriteMarshallable toMarshallable()
+        {
+            List<String> queryStrings = new ArrayList<>();
+            List<List<ByteBuffer>> values = new ArrayList<>();
+            for (Single q : queries)
+            {
+                queryStrings.add(q.query);
+                values.add(q.values);
+            }
+            return new FullQueryLogger.WeighableMarshallableBatch(batchType.name(), keyspace, queryStrings, values, queryOptions, queryTime);
+        }
+
+        public String toString()
+        {
+            StringBuilder sb = new StringBuilder(""batch: "").append(batchType).append('\n');
+            for (Single q : queries)
+            {","[{'comment': 'Redundant braces for a single statement block.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/FQLQueryReader.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+
+import com.datastax.driver.core.BatchStatement;
+import io.netty.buffer.Unpooled;
+import net.openhft.chronicle.core.io.IORuntimeException;
+import net.openhft.chronicle.wire.ReadMarshallable;
+import net.openhft.chronicle.wire.ValueIn;
+import net.openhft.chronicle.wire.WireIn;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.transport.ProtocolVersion;
+
+public class FQLQueryReader implements ReadMarshallable
+{
+    private FQLQuery query;
+    private final boolean legacyFiles;
+
+    public FQLQueryReader()
+    {
+        this(false);
+    }
+
+    public FQLQueryReader(boolean legacyFiles)
+    {
+        this.legacyFiles = legacyFiles;
+    }
+
+    public void readMarshallable(WireIn wireIn) throws IORuntimeException
+    {
+        String type = wireIn.read(""type"").text();
+        int protocolVersion = wireIn.read(""protocol-version"").int32();
+        QueryOptions queryOptions = QueryOptions.codec.decode(Unpooled.wrappedBuffer(wireIn.read(""query-options"").bytes()), ProtocolVersion.decode(protocolVersion));
+        long queryTime = wireIn.read(""query-time"").int64();
+        String keyspace = legacyFiles ? null : wireIn.read(""keyspace"").text();
+        switch (type)
+        {
+            case ""single"":
+                String queryString = wireIn.read(""query"").text();
+                query = new FQLQuery.Single(keyspace, protocolVersion, queryOptions, queryTime, queryString, queryOptions.getValues());
+                break;
+            case ""batch"":
+                BatchStatement.Type batchType = BatchStatement.Type.valueOf(wireIn.read(""batch-type"").text());
+                ValueIn in = wireIn.read(""queries"");
+                int queryCount = in.int32();
+
+                List<String> queries = new ArrayList<>(queryCount);
+                for (int i = 0; i < queryCount; i++)
+                    queries.add(in.text());
+                in = wireIn.read(""values"");
+                int valueCount = in.int32();
+                List<List<ByteBuffer>> values = new ArrayList<>(valueCount);
+                for (int ii = 0; ii < valueCount; ii++)
+                {
+                    List<ByteBuffer> subValues = new ArrayList<>();
+                    values.add(subValues);
+                    int numSubValues = in.int32();
+                    for (int zz = 0; zz < numSubValues; zz++)
+                    {","[{'comment': 'Redundant braces', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/ResultComparator.java,"@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+
+import java.util.List;
+import java.util.Objects;
+import java.util.stream.Collectors;
+
+import com.google.common.collect.Streams;
+
+public class ResultComparator
+{
+    /**
+     * Compares the rows in rows
+     * the row at position x in rows will have come from host at position x in targetHosts
+     */
+    public boolean compareRows(List<String> targetHosts, FQLQuery query, List<ResultHandler.ComparableRow> rows)
+    {
+        if (rows.size() < 2 || rows.stream().allMatch(Objects::isNull))
+            return true;
+
+        if (rows.stream().anyMatch(Objects::isNull))
+        {
+            handleMismatch(targetHosts, query, rows);
+            return false;
+        }
+
+        ResultHandler.ComparableRow ref = rows.get(0);
+        boolean equal = true;
+        for (int i = 1; i < rows.size(); i++)
+        {
+            ResultHandler.ComparableRow compare = rows.get(i);
+            if (!ref.equals(compare))
+            {
+                equal = false;
+                handleMismatch(targetHosts, query, rows);
+            }
+        }
+        return equal;
+    }
+
+    /**
+     * Compares the column definitions
+     *
+     * the column definitions at position x in cds will have come from host at position x in targetHosts
+     */
+    public boolean compareColumnDefinitions(List<String> targetHosts, FQLQuery query, List<ResultHandler.ComparableColumnDefinitions> cds)
+    {
+        if (cds.size() < 2)
+            return true;
+
+        boolean equal = true;
+        List<ResultHandler.ComparableDefinition> refDefs = cds.get(0).asList();
+        for (int i = 1; i < cds.size(); i++)
+        {
+            List<ResultHandler.ComparableDefinition> toCompare = cds.get(i).asList();
+            if (!refDefs.equals(toCompare))
+            {
+                handleColumnDefMismatch(targetHosts, query, cds);
+                equal = false;
+            }
+        }
+        return equal;
+    }
+
+    private void handleMismatch(List<String> targetHosts, FQLQuery query, List<ResultHandler.ComparableRow> rows)
+    {
+        System.out.println(""MISMATCH:"");
+        System.out.println(""Query = "" + query);
+        System.out.println(""Results:"");
+        System.out.println(Streams.zip(rows.stream(), targetHosts.stream(), (r, host) -> String.format(""%s: %s%n"", host, r == null ? ""null"" : r)).collect(Collectors.joining()));
+    }
+
+    private void handleColumnDefMismatch(List<String> targetHosts, FQLQuery query, List<ResultHandler.ComparableColumnDefinitions> cds)
+    {
+        System.out.println(""COLUMN DEFINITION MISMATCH:"");
+        System.out.println(""Query = "" + query);
+        System.out.println(""Results: "");
+        System.out.println(Streams.zip(cds.stream(), targetHosts.stream(), (cd, host) -> String.format(""%s: %s%n"", host, columnDefinitionsString(cd))).collect(Collectors.joining()));
+    }
+
+    private String columnDefinitionsString(ResultHandler.ComparableColumnDefinitions cd)
+    {
+        StringBuilder sb = new StringBuilder();
+        if (cd == null)
+        {","[{'comment': 'Redundant braces', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/ResultHandler.java,"@@ -0,0 +1,124 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.io.File;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Objects;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+
+import com.datastax.driver.core.ResultSet;","[{'comment': 'Unused import.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/ResultStore.java,"@@ -0,0 +1,144 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+
+import java.io.File;
+import java.nio.ByteBuffer;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import net.openhft.chronicle.bytes.BytesStore;
+import net.openhft.chronicle.core.io.Closeable;
+import net.openhft.chronicle.queue.ChronicleQueue;
+import net.openhft.chronicle.queue.ChronicleQueueBuilder;
+import net.openhft.chronicle.queue.ExcerptAppender;
+import net.openhft.chronicle.wire.ValueOut;
+import org.apache.cassandra.utils.binlog.BinLog;
+
+/**
+ * see FQLReplayTest#readResultFile for how to read files produced by this class
+ */
+public class ResultStore
+{
+    private final List<ChronicleQueue> queues;
+    private final List<ExcerptAppender> appenders;
+    private final ChronicleQueue queryStoreQueue;
+    private final ExcerptAppender queryStoreAppender;
+    private final Set<Integer> finishedHosts = new HashSet<>();
+    private final File queryFilePath;","[{'comment': 'Unused variable.', 'commenter': 'dineshjoshi'}]"
255,src/java/org/apache/cassandra/tools/fqltool/DriverResultSet.java,"@@ -0,0 +1,253 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools.fqltool;
+
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import com.google.common.collect.AbstractIterator;
+
+import com.datastax.driver.core.ColumnDefinitions;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.Row;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+/**
+ * Wraps a result set from the driver so that we can reuse the compare code when reading
+ * up a result set produced by ResultStore.
+ */
+public class DriverResultSet implements ResultHandler.ComparableResultSet
+{
+    private final ResultSet resultSet;
+    private final Throwable failureException;
+
+    public DriverResultSet(ResultSet resultSet)
+    {
+        this(resultSet, null);
+    }
+
+    private DriverResultSet(ResultSet res, Throwable failureException)
+    {
+        resultSet = res;
+        this.failureException = failureException;
+    }
+
+    public static DriverResultSet failed(Throwable ex)
+    {
+        return new DriverResultSet(null, ex);
+    }
+
+    public ResultHandler.ComparableColumnDefinitions getColumnDefinitions()
+    {
+        if (wasFailed())
+            return new DriverColumnDefinitions(null, true);
+
+        return new DriverColumnDefinitions(resultSet.getColumnDefinitions());
+    }
+
+    public boolean wasFailed()
+    {
+        return failureException != null;
+    }
+
+    public Throwable getFailureException()
+    {
+        return failureException;
+    }
+
+    public Iterator<ResultHandler.ComparableRow> iterator()
+    {
+        if (wasFailed())
+            return Collections.emptyListIterator();
+        return new AbstractIterator<ResultHandler.ComparableRow>()
+        {
+            Iterator<Row> iter = resultSet.iterator();
+            protected ResultHandler.ComparableRow computeNext()
+            {
+                if (iter.hasNext())
+                {
+                    return new DriverRow(iter.next());
+                }
+                return endOfData();
+            }
+        };
+    }
+
+    public static class DriverRow implements ResultHandler.ComparableRow
+    {
+        private final Row row;
+
+        public DriverRow(Row row)
+        {
+            this.row = row;
+        }
+
+        public ResultHandler.ComparableColumnDefinitions getColumnDefinitions()
+        {
+            return new DriverColumnDefinitions(row.getColumnDefinitions());
+        }
+
+        public ByteBuffer getBytesUnsafe(int i)
+        {
+            return row.getBytesUnsafe(i);
+        }
+
+        @Override
+        public boolean equals(Object oo)
+        {
+            if (!(oo instanceof ResultHandler.ComparableRow))
+                return false;
+
+            ResultHandler.ComparableRow o = (ResultHandler.ComparableRow)oo;
+            if (getColumnDefinitions().size() != o.getColumnDefinitions().size())
+                return false;
+
+            for (int j = 0; j < getColumnDefinitions().size(); j++)
+            {
+                ByteBuffer b1 = getBytesUnsafe(j);
+                ByteBuffer b2 = o.getBytesUnsafe(j);
+
+                if (b1 != null && b2 != null && !b1.equals(b2))
+                {
+                    return false;
+                }
+                if (b1 == null && b2 != null || b2 == null && b1 != null)
+                {
+                    return false;
+                }
+            }
+            return true;
+        }
+
+        public String toString()
+        {
+            StringBuilder sb = new StringBuilder();
+            List<ResultHandler.ComparableDefinition> colDefs = getColumnDefinitions().asList();
+            for (int i = 0; i < getColumnDefinitions().size(); i++)
+            {
+                ByteBuffer bb = getBytesUnsafe(i);
+                String row = bb != null ? ByteBufferUtil.bytesToHex(bb) : ""NULL"";
+                sb.append(colDefs.get(i)).append(':').append(row).append("","");
+            }
+            return sb.toString();
+        }
+    }
+
+    public static class DriverColumnDefinitions implements ResultHandler.ComparableColumnDefinitions
+    {
+        private final ColumnDefinitions columnDefinitions;
+        private final boolean failed;
+
+        public DriverColumnDefinitions(ColumnDefinitions columnDefinitions)
+        {
+            this(columnDefinitions, false);
+        }
+
+        private DriverColumnDefinitions(ColumnDefinitions columnDefinitions, boolean failed)
+        {
+            this.columnDefinitions = columnDefinitions;
+            this.failed = failed;
+        }
+
+        public List<ResultHandler.ComparableDefinition> asList()
+        {
+            if (wasFailed())
+                return Collections.emptyList();
+            return columnDefinitions.asList().stream().map(DriverDefinition::new).collect(Collectors.toList());
+        }
+
+        public boolean wasFailed()
+        {
+            return failed;
+        }
+
+        public int size()
+        {
+            return columnDefinitions.size();
+        }
+
+        public Iterator<ResultHandler.ComparableDefinition> iterator()
+        {
+            return asList().iterator();
+        }
+
+        public boolean equals(Object oo)
+        {
+            if (!(oo instanceof ResultHandler.ComparableColumnDefinitions))
+                return false;
+
+            ResultHandler.ComparableColumnDefinitions o = (ResultHandler.ComparableColumnDefinitions)oo;
+            if (wasFailed() && o.wasFailed())
+                return true;
+
+            if (size() != o.size())
+                return false;
+
+            List<ResultHandler.ComparableDefinition> def1 = asList();
+            List<ResultHandler.ComparableDefinition> def2 = o.asList();
+
+            for (int j = 0; j < def1.size(); j++)","[{'comment': 'I think you can do `return asList().equals(o.asList())` instead of the `for` loop. (j.u.AbstractList.equals() basically does that)', 'commenter': 'jasobrown'}]"
267,src/java/org/apache/cassandra/locator/ReplicaPlans.java,"@@ -61,26 +73,62 @@
         return forSingleReplicaWrite(keyspace, token, replica);
     }
 
+    public static ReplicaPlan.ForTokenWrite forLocalBatchlogWrite()
+    {
+        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();
+        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);
+        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());
+
+        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(
+        EndpointsForToken.of(token, localSystemReplica),","[{'comment': 'indentation?', 'commenter': 'belliottsmith'}]"
267,src/java/org/apache/cassandra/locator/ReplicaPlans.java,"@@ -61,26 +73,62 @@
         return forSingleReplicaWrite(keyspace, token, replica);
     }
 
+    public static ReplicaPlan.ForTokenWrite forLocalBatchlogWrite()
+    {
+        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();
+        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);
+        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());
+
+        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(
+        EndpointsForToken.of(token, localSystemReplica),
+        EndpointsForToken.empty(token)
+        );
+
+        return forWrite(systemKeypsace, ConsistencyLevel.ONE, liveAndDown, liveAndDown, writeAll);
+    }
+
     /**
      * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.
      * Note that the liveAndDown collection and live are equal to the provided endpoints.
-     *
-     * The semantics are a bit weird, in that CL=ONE iff we have one node provided, and otherwise is equal to TWO.
-     * How these CL were chosen, and why we drop the CL if only one live node is available, are both unclear.
      */
-    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(Keyspace keyspace, Collection<InetAddressAndPort> endpoints) throws UnavailableException
+    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(String localDataCenter, ConsistencyLevel consistencyLevel) throws UnavailableException
     {
         // A single case we write not for range or token, but multiple mutations to many tokens
         Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();
 
+        TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cachedOnlyTokenMap().getTopology();
+        Multimap<String, InetAddressAndPort> localEndpoints = HashMultimap.create(topology.getDatacenterRacks().get(localDataCenter));
+        String localRack = DatabaseDescriptor.getEndpointSnitch().getRack(FBUtilities.getBroadcastAddressAndPort());
+
+        // Replicas are picked manually:
+        //  - replicas should be alive according to the failure detector
+        //  - replicas should be in the local datacenter
+        //  - choose min(2, number of qualifying candiates above)
+        //  - allow the local node to be the only replica only if it's a single-node DC
+        Collection<InetAddressAndPort> chosenEndpoints = new BatchlogManager.EndpointFilter(localRack, localEndpoints).filter();","[{'comment': ""should EndpointFilter be brought into ReplicaPlans?  It's not used anywhere else, and it seems that the two pieces of logic should be proximal"", 'commenter': 'belliottsmith'}, {'comment': ""I wanted to, but it's also fairly large, so wanted to avoid polluting it. If you think it's a good idea, i can definitely do it."", 'commenter': 'ifesdjeen'}, {'comment': ""I don't love it either, but it should live together.  Perhaps we can clean it up, and we can certainly at least make it a static method instead of a class."", 'commenter': 'belliottsmith'}, {'comment': 'Moved it', 'commenter': 'ifesdjeen'}]"
267,src/java/org/apache/cassandra/service/StorageProxy.java,"@@ -780,8 +780,9 @@ public static void mutateMV(ByteBuffer dataKey, Collection<Mutation> mutations,
                 ConsistencyLevel consistencyLevel = ConsistencyLevel.ONE;
 
                 //Since the base -> view replication is 1:1 we only need to store the BL locally
-                final Collection<InetAddressAndPort> batchlogEndpoints = Collections.singleton(FBUtilities.getBroadcastAddressAndPort());
-                BatchlogResponseHandler.BatchlogCleanup cleanup = new BatchlogResponseHandler.BatchlogCleanup(mutations.size(), () -> asyncRemoveFromBatchlog(batchlogEndpoints, batchUUID));
+                ReplicaPlan.ForTokenWrite replicaPlan = ReplicaPlans.forLocalBatchlogWrite();
+                BatchlogResponseHandler.BatchlogCleanup cleanup = new BatchlogResponseHandler.BatchlogCleanup(mutations.size(),","[{'comment': ""If we static import the inner class, we can avoid the unnecessary newline (which irks me unreasonably, because it's anyway going over the line length limit)"", 'commenter': 'belliottsmith'}, {'comment': '\r\nFixed', 'commenter': 'ifesdjeen'}]"
267,src/java/org/apache/cassandra/service/StorageProxy.java,"@@ -1021,18 +1021,18 @@ private static void syncWriteToBatchlog(Collection<Mutation> mutations, Collecti
         handler.get();
     }
 
-    private static void asyncRemoveFromBatchlog(Collection<InetAddressAndPort> endpoints, UUID uuid)
+    private static void asyncRemoveFromBatchlog(ReplicaPlan.ForTokenWrite replicaPlan, UUID uuid)
     {
         MessageOut<UUID> message = new MessageOut<>(MessagingService.Verb.BATCH_REMOVE, uuid, UUIDSerializer.serializer);
-        for (InetAddressAndPort target : endpoints)
+        for (Replica target : replicaPlan.contacts())
         {
             if (logger.isTraceEnabled())
                 logger.trace(""Sending batchlog remove request {} to {}"", uuid, target);
 
-            if (target.equals(FBUtilities.getBroadcastAddressAndPort()))
-                performLocally(Stage.MUTATION, SystemReplicas.getSystemReplica(target), () -> BatchlogManager.remove(uuid));
+            if (target.isLocal())","[{'comment': ""I thought we had ended up calling this `isSelf` because `isLocal` is overloaded in this code (meaning, variably, is this instance (has our broadcast address); is the same DC as this instance; has the 'local' address).  Not strictly related to this patch, but perhaps we should rename to `isSelf` while we're here and remember?  This is consistent with the nomenclature amongst the TR patch, and in `ReplicaCollection` (`withoutSelf` and `selfIfPresent`)"", 'commenter': 'belliottsmith'}, {'comment': 'Renamed', 'commenter': 'ifesdjeen'}]"
267,src/java/org/apache/cassandra/locator/ReplicaPlans.java,"@@ -61,26 +73,62 @@
         return forSingleReplicaWrite(keyspace, token, replica);
     }
 
+    public static ReplicaPlan.ForTokenWrite forLocalBatchlogWrite()
+    {
+        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();
+        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);
+        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());
+
+        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(
+        EndpointsForToken.of(token, localSystemReplica),
+        EndpointsForToken.empty(token)
+        );
+
+        return forWrite(systemKeypsace, ConsistencyLevel.ONE, liveAndDown, liveAndDown, writeAll);
+    }
+
     /**
      * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.
      * Note that the liveAndDown collection and live are equal to the provided endpoints.
-     *
-     * The semantics are a bit weird, in that CL=ONE iff we have one node provided, and otherwise is equal to TWO.
-     * How these CL were chosen, and why we drop the CL if only one live node is available, are both unclear.
      */
-    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(Keyspace keyspace, Collection<InetAddressAndPort> endpoints) throws UnavailableException
+    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(String localDataCenter, ConsistencyLevel consistencyLevel) throws UnavailableException
     {
         // A single case we write not for range or token, but multiple mutations to many tokens
         Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();
 
+        TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cachedOnlyTokenMap().getTopology();
+        Multimap<String, InetAddressAndPort> localEndpoints = HashMultimap.create(topology.getDatacenterRacks().get(localDataCenter));
+        String localRack = DatabaseDescriptor.getEndpointSnitch().getRack(FBUtilities.getBroadcastAddressAndPort());
+
+        // Replicas are picked manually:
+        //  - replicas should be alive according to the failure detector
+        //  - replicas should be in the local datacenter
+        //  - choose min(2, number of qualifying candiates above)
+        //  - allow the local node to be the only replica only if it's a single-node DC
+        Collection<InetAddressAndPort> chosenEndpoints = new BatchlogManager.EndpointFilter(localRack, localEndpoints).filter();
+
+        if (chosenEndpoints.isEmpty())
+        {
+            if (consistencyLevel == ConsistencyLevel.ANY)
+                chosenEndpoints = Collections.singleton(FBUtilities.getBroadcastAddressAndPort());
+            else
+                throw UnavailableException.create(ConsistencyLevel.ONE, 1, 0);","[{'comment': 'Do we need this check, if we have `assureSufficientReplicas` below?', 'commenter': 'belliottsmith'}, {'comment': ""We don't strictly need it: included it only to short-circuit. We can skip it."", 'commenter': 'ifesdjeen'}]"
267,src/java/org/apache/cassandra/locator/ReplicaPlans.java,"@@ -61,26 +73,62 @@
         return forSingleReplicaWrite(keyspace, token, replica);
     }
 
+    public static ReplicaPlan.ForTokenWrite forLocalBatchlogWrite()
+    {
+        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();
+        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);
+        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());
+
+        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(
+        EndpointsForToken.of(token, localSystemReplica),
+        EndpointsForToken.empty(token)
+        );
+
+        return forWrite(systemKeypsace, ConsistencyLevel.ONE, liveAndDown, liveAndDown, writeAll);
+    }
+
     /**
      * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.
      * Note that the liveAndDown collection and live are equal to the provided endpoints.
-     *
-     * The semantics are a bit weird, in that CL=ONE iff we have one node provided, and otherwise is equal to TWO.
-     * How these CL were chosen, and why we drop the CL if only one live node is available, are both unclear.
      */
-    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(Keyspace keyspace, Collection<InetAddressAndPort> endpoints) throws UnavailableException
+    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(String localDataCenter, ConsistencyLevel consistencyLevel) throws UnavailableException
     {
         // A single case we write not for range or token, but multiple mutations to many tokens
         Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();
 
+        TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cachedOnlyTokenMap().getTopology();
+        Multimap<String, InetAddressAndPort> localEndpoints = HashMultimap.create(topology.getDatacenterRacks().get(localDataCenter));
+        String localRack = DatabaseDescriptor.getEndpointSnitch().getRack(FBUtilities.getBroadcastAddressAndPort());
+
+        // Replicas are picked manually:
+        //  - replicas should be alive according to the failure detector
+        //  - replicas should be in the local datacenter
+        //  - choose min(2, number of qualifying candiates above)
+        //  - allow the local node to be the only replica only if it's a single-node DC
+        Collection<InetAddressAndPort> chosenEndpoints = new BatchlogManager.EndpointFilter(localRack, localEndpoints).filter();
+
+        if (chosenEndpoints.isEmpty())
+        {
+            if (consistencyLevel == ConsistencyLevel.ANY)
+                chosenEndpoints = Collections.singleton(FBUtilities.getBroadcastAddressAndPort());
+            else
+                throw UnavailableException.create(ConsistencyLevel.ONE, 1, 0);
+        }
+
         ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(
-                SystemReplicas.getSystemReplicas(endpoints).forToken(token),
+                SystemReplicas.getSystemReplicas(chosenEndpoints).forToken(token),
                 EndpointsForToken.empty(token)
         );
-        ConsistencyLevel consistencyLevel = liveAndDown.all().size() == 1 ? ConsistencyLevel.ONE : ConsistencyLevel.TWO;
+
+        // Batchlog is hosted by either one node or two nodes from different racks.
+        consistencyLevel = liveAndDown.all().size() == 1 ? ConsistencyLevel.ONE : ConsistencyLevel.TWO;","[{'comment': ""Are we sure it's OK (although consistent with prior behaviour), to only require `CL.ONE` if all nodes besides us are down?  The logic above suggests we only require `CL.ONE` if we're a single-node DC, but we could have multiple failing nodes in our DC, and be the only one left.  It seems like we should probably claim insufficient consistency in this case, though this question probably deserves a separate ticket for discussion."", 'commenter': 'belliottsmith'}, {'comment': ""Right, I had similar thoughts when writing that. I've also been thinking if it's enough to only have two nodes in local DC blocking for batchlog writes. I wanted to make a change to original CL, but that'd might reduce availability (for instance when it's a quorum). But from looking at batch replay, we should be able to do it."", 'commenter': 'ifesdjeen'}]"
267,src/java/org/apache/cassandra/locator/ReplicaPlans.java,"@@ -61,26 +73,62 @@
         return forSingleReplicaWrite(keyspace, token, replica);
     }
 
+    public static ReplicaPlan.ForTokenWrite forLocalBatchlogWrite()
+    {
+        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();
+        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);
+        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());
+
+        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(
+        EndpointsForToken.of(token, localSystemReplica),
+        EndpointsForToken.empty(token)
+        );
+
+        return forWrite(systemKeypsace, ConsistencyLevel.ONE, liveAndDown, liveAndDown, writeAll);
+    }
+
     /**
      * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.
      * Note that the liveAndDown collection and live are equal to the provided endpoints.
-     *
-     * The semantics are a bit weird, in that CL=ONE iff we have one node provided, and otherwise is equal to TWO.
-     * How these CL were chosen, and why we drop the CL if only one live node is available, are both unclear.
      */
-    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(Keyspace keyspace, Collection<InetAddressAndPort> endpoints) throws UnavailableException
+    public static ReplicaPlan.ForTokenWrite forBatchlogWrite(String localDataCenter, ConsistencyLevel consistencyLevel) throws UnavailableException","[{'comment': ""Hmm.  It looks like `consistencyLevel` is almost completely ignored, besides for `==CL.ANY`, but callers seem to then use the provided `consistencyLevel` to construct an `AbstractWriteResponseHandler`.  \r\n\r\nShould we really be accepting `consistencyLevel` here?  All we really need to know is if the caller needs durability (i.e. if `CL.ANY` was provided), and perhaps we should have a separate enum (or just a boolean) for this, as it is otherwise rather confusing.  Though what `CL.ANY` even means for batch log, I haven't a clue, anyway.\r\n\r\nEither way, while we're here we should document in the caller the fact that the provided `consistencyLevel` is not used, and that the `batchlogConsistencyLevel` is used to clear the entries from the remote batch logs once the real write has reached that consistency level."", 'commenter': 'belliottsmith'}, {'comment': ""> Either way, while we're here we should document in the caller the fact that the provided consistencyLevel is not used, and that the batchlogConsistencyLevel is used to clear the entries from the remote batch logs once the real write has reached that consistency level.\r\n\r\nnot sure I understand what you mean, as we're using it both for cleanup and for writing to batchlog. "", 'commenter': 'ifesdjeen'}]"
276,src/java/org/apache/cassandra/repair/LocalSyncTask.java,"@@ -52,15 +52,9 @@
     private static final Logger logger = LoggerFactory.getLogger(LocalSyncTask.class);
 
     private final UUID pendingRepair;
-    private final boolean requestRanges;
-    private final boolean transferRanges;
 
-    public LocalSyncTask(RepairJobDesc desc, TreeResponse local, TreeResponse remote, UUID pendingRepair,
-                         boolean requestRanges, boolean transferRanges, PreviewKind previewKind)
-    {
-        this(desc, local.endpoint, remote.endpoint, MerkleTrees.difference(local.trees, remote.trees),
-             pendingRepair, requestRanges, transferRanges, previewKind);
-    }
+    protected final boolean requestRanges;","[{'comment': 'these could be VisibleForTesting (and perhaps package private)', 'commenter': 'krummas'}, {'comment': '+1, made a change ', 'commenter': 'ifesdjeen'}]"
276,test/unit/org/apache/cassandra/repair/RepairJobTest.java,"@@ -0,0 +1,569 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.net.UnknownHostException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+import com.google.common.collect.Sets;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.streaming.PreviewKind;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.apache.cassandra.utils.MerkleTrees;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class RepairJobTest
+{
+    private static final IPartitioner PARTITIONER = ByteOrderedPartitioner.instance;
+
+    static InetAddressAndPort addr1;
+    static InetAddressAndPort addr2;
+    static InetAddressAndPort addr3;
+    static InetAddressAndPort addr4;
+    static InetAddressAndPort addr5;
+
+    static Range<Token> range1 = range(0, 1);
+    static Range<Token> range2 = range(2, 3);
+    static Range<Token> range3 = range(4, 5);
+    static RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+
+    @AfterClass
+    public static void reset()
+    {
+        FBUtilities.reset();
+    }
+
+    static
+    {
+        try
+        {
+            addr1 = InetAddressAndPort.getByName(""127.0.0.1"");
+            addr2 = InetAddressAndPort.getByName(""127.0.0.2"");
+            addr3 = InetAddressAndPort.getByName(""127.0.0.3"");
+            addr4 = InetAddressAndPort.getByName(""127.0.0.4"");
+            addr5 = InetAddressAndPort.getByName(""127.0.0.5"");
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+        catch (UnknownHostException e)
+        {
+            e.printStackTrace();
+        }
+    }
+
+    @Test
+    public void testCreateStandardSyncTasks()
+    {
+        testCreateStandardSyncTasks(false);
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksPullRepair()
+    {
+        testCreateStandardSyncTasks(true);
+    }
+
+    public static void testCreateStandardSyncTasks(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    noTransient(), // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(2, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertEquals(!pullRepair, ((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertTrue(task instanceof SymmetricRemoteSyncTask);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        Assert.assertNull(tasks.get(pair(addr1, addr3)));
+    }
+
+    @Test
+    public void testStanardSyncTransient()","[{'comment': 's/Stanard/Standard/ (in the whole file)', 'commenter': 'krummas'}, {'comment': 'Thank you for spotting ', 'commenter': 'ifesdjeen'}]"
276,test/unit/org/apache/cassandra/repair/RepairJobTest.java,"@@ -0,0 +1,569 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.net.UnknownHostException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+import com.google.common.collect.Sets;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.streaming.PreviewKind;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.apache.cassandra.utils.MerkleTrees;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class RepairJobTest
+{
+    private static final IPartitioner PARTITIONER = ByteOrderedPartitioner.instance;
+
+    static InetAddressAndPort addr1;
+    static InetAddressAndPort addr2;
+    static InetAddressAndPort addr3;
+    static InetAddressAndPort addr4;
+    static InetAddressAndPort addr5;
+
+    static Range<Token> range1 = range(0, 1);
+    static Range<Token> range2 = range(2, 3);
+    static Range<Token> range3 = range(4, 5);
+    static RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+
+    @AfterClass
+    public static void reset()
+    {
+        FBUtilities.reset();
+    }
+
+    static
+    {
+        try
+        {
+            addr1 = InetAddressAndPort.getByName(""127.0.0.1"");
+            addr2 = InetAddressAndPort.getByName(""127.0.0.2"");
+            addr3 = InetAddressAndPort.getByName(""127.0.0.3"");
+            addr4 = InetAddressAndPort.getByName(""127.0.0.4"");
+            addr5 = InetAddressAndPort.getByName(""127.0.0.5"");
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+        catch (UnknownHostException e)
+        {
+            e.printStackTrace();
+        }
+    }
+
+    @Test
+    public void testCreateStandardSyncTasks()
+    {
+        testCreateStandardSyncTasks(false);
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksPullRepair()
+    {
+        testCreateStandardSyncTasks(true);
+    }
+
+    public static void testCreateStandardSyncTasks(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    noTransient(), // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(2, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertEquals(!pullRepair, ((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertTrue(task instanceof SymmetricRemoteSyncTask);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        Assert.assertNull(tasks.get(pair(addr1, addr3)));
+    }
+
+    @Test
+    public void testStanardSyncTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncTransient(true);
+        testStanardSyncTransient(false);
+    }
+
+    public void testStanardSyncTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr2),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertFalse(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testStanardSyncLocalTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncLocalTransient(true);
+        testStanardSyncLocalTransient(false);
+    }
+
+    public void testStanardSyncLocalTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr1),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        if (pullRepair)
+        {
+            Assert.assertTrue(tasks.isEmpty());
+            return;
+        }
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertFalse(((LocalSyncTask) task).requestRanges);
+        Assert.assertTrue(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testEmptyDifference()
+    {
+        // one of the nodes is a local coordinator
+        testEmptyDifference(addr1, noTransient(), true);
+        testEmptyDifference(addr1, noTransient(), false);
+        testEmptyDifference(addr2, noTransient(), true);
+        testEmptyDifference(addr2, noTransient(), false);
+        testEmptyDifference(addr1, transientPredicate(addr1), true);
+        testEmptyDifference(addr2, transientPredicate(addr1), true);
+        testEmptyDifference(addr1, transientPredicate(addr1), false);
+        testEmptyDifference(addr2, transientPredicate(addr1), false);
+        testEmptyDifference(addr1, transientPredicate(addr2), true);
+        testEmptyDifference(addr2, transientPredicate(addr2), true);
+        testEmptyDifference(addr1, transientPredicate(addr2), false);
+        testEmptyDifference(addr2, transientPredicate(addr2), false);
+
+        // nonlocal coordinator
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+    }
+
+    public void testEmptyDifference(InetAddressAndPort local, Predicate<InetAddressAndPort> isTransient, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""same"", range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient,
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertTrue(tasks.isEmpty());
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    ep -> ep.equals(addr3), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr1, addr3));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testCreate5NodeStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        SyncNodePair[] pairs = new SyncNodePair[] {pair(addr1, addr2),
+                                                   pair(addr1, addr3),
+                                                   pair(addr1, addr4),
+                                                   pair(addr1, addr5),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr5),
+                                                   pair(addr3, addr4),
+                                                   pair(addr3, addr5)};
+
+        for (SyncNodePair pair : pairs)
+        {
+            SyncTask task = tasks.get(pair);
+            // Local only if addr1 is a coordinator
+            assertEquals(task.isLocal(), pair.coordinator.equals(addr1));
+
+            // Symmetric only if there are no transient participants
+            assertEquals(String.format(""Coordinator: %s\n, Peer: %s\n"",","[{'comment': 'maybe make this something like:\r\n```\r\nboolean isRemote = !pair.coordinator.equals(addr1) && !pair.peer.equals(addr1);\r\nboolean involvesTransient = isTransient.test(pair.coordinator) || isTransient.test(pair.peer);\r\nassertEquals(String.format(""Coordinator: %s\\n, Peer: %s\\n"",pair.coordinator, pair.peer),\r\n                         isRemote && involvesTransient,\r\n                         task instanceof AsymmetricRemoteSyncTask);\r\n```\r\nto make it a bit more readable?', 'commenter': 'krummas'}, {'comment': '+1', 'commenter': 'ifesdjeen'}]"
276,test/unit/org/apache/cassandra/repair/RepairJobTest.java,"@@ -0,0 +1,569 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.net.UnknownHostException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+import com.google.common.collect.Sets;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.streaming.PreviewKind;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.apache.cassandra.utils.MerkleTrees;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class RepairJobTest
+{
+    private static final IPartitioner PARTITIONER = ByteOrderedPartitioner.instance;
+
+    static InetAddressAndPort addr1;
+    static InetAddressAndPort addr2;
+    static InetAddressAndPort addr3;
+    static InetAddressAndPort addr4;
+    static InetAddressAndPort addr5;
+
+    static Range<Token> range1 = range(0, 1);
+    static Range<Token> range2 = range(2, 3);
+    static Range<Token> range3 = range(4, 5);
+    static RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+
+    @AfterClass
+    public static void reset()
+    {
+        FBUtilities.reset();
+    }
+
+    static
+    {
+        try
+        {
+            addr1 = InetAddressAndPort.getByName(""127.0.0.1"");
+            addr2 = InetAddressAndPort.getByName(""127.0.0.2"");
+            addr3 = InetAddressAndPort.getByName(""127.0.0.3"");
+            addr4 = InetAddressAndPort.getByName(""127.0.0.4"");
+            addr5 = InetAddressAndPort.getByName(""127.0.0.5"");
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+        catch (UnknownHostException e)
+        {
+            e.printStackTrace();
+        }
+    }
+
+    @Test
+    public void testCreateStandardSyncTasks()
+    {
+        testCreateStandardSyncTasks(false);
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksPullRepair()
+    {
+        testCreateStandardSyncTasks(true);
+    }
+
+    public static void testCreateStandardSyncTasks(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    noTransient(), // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(2, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertEquals(!pullRepair, ((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertTrue(task instanceof SymmetricRemoteSyncTask);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        Assert.assertNull(tasks.get(pair(addr1, addr3)));
+    }
+
+    @Test
+    public void testStanardSyncTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncTransient(true);
+        testStanardSyncTransient(false);
+    }
+
+    public void testStanardSyncTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr2),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertFalse(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testStanardSyncLocalTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncLocalTransient(true);
+        testStanardSyncLocalTransient(false);
+    }
+
+    public void testStanardSyncLocalTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr1),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        if (pullRepair)
+        {
+            Assert.assertTrue(tasks.isEmpty());
+            return;
+        }
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertFalse(((LocalSyncTask) task).requestRanges);
+        Assert.assertTrue(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testEmptyDifference()
+    {
+        // one of the nodes is a local coordinator
+        testEmptyDifference(addr1, noTransient(), true);
+        testEmptyDifference(addr1, noTransient(), false);
+        testEmptyDifference(addr2, noTransient(), true);
+        testEmptyDifference(addr2, noTransient(), false);
+        testEmptyDifference(addr1, transientPredicate(addr1), true);
+        testEmptyDifference(addr2, transientPredicate(addr1), true);
+        testEmptyDifference(addr1, transientPredicate(addr1), false);
+        testEmptyDifference(addr2, transientPredicate(addr1), false);
+        testEmptyDifference(addr1, transientPredicate(addr2), true);
+        testEmptyDifference(addr2, transientPredicate(addr2), true);
+        testEmptyDifference(addr1, transientPredicate(addr2), false);
+        testEmptyDifference(addr2, transientPredicate(addr2), false);
+
+        // nonlocal coordinator
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+    }
+
+    public void testEmptyDifference(InetAddressAndPort local, Predicate<InetAddressAndPort> isTransient, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""same"", range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient,
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertTrue(tasks.isEmpty());
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    ep -> ep.equals(addr3), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr1, addr3));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testCreate5NodeStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        SyncNodePair[] pairs = new SyncNodePair[] {pair(addr1, addr2),
+                                                   pair(addr1, addr3),
+                                                   pair(addr1, addr4),
+                                                   pair(addr1, addr5),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr5),
+                                                   pair(addr3, addr4),
+                                                   pair(addr3, addr5)};
+
+        for (SyncNodePair pair : pairs)
+        {
+            SyncTask task = tasks.get(pair);
+            // Local only if addr1 is a coordinator
+            assertEquals(task.isLocal(), pair.coordinator.equals(addr1));
+
+            // Symmetric only if there are no transient participants
+            assertEquals(String.format(""Coordinator: %s\n, Peer: %s\n"",
+                                       pair.coordinator,
+                                       pair.peer),
+                         (!pair.coordinator.equals(addr1) && !pair.peer.equals(addr1)) &&
+                         (isTransient.test(pair.coordinator) || isTransient.test(pair.peer)),
+                         task instanceof AsymmetricRemoteSyncTask);
+
+            // All ranges to be synchronised
+            Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransient()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, false);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransientPullRepair()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, true);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+
+    }
+
+    public static void testLocalSyncWithTransient(InetAddressAndPort local, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        assertEquals(9, tasks.size());
+        for (InetAddressAndPort addr : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+        {
+            if (local.equals(addr))
+                continue;
+
+            LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr));
+            assertTrue(task.requestRanges);
+            assertEquals(!pullRepair, task.transferRanges);
+        }
+
+        LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr4));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+
+        task = (LocalSyncTask) tasks.get(pair(local, addr5));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+    }
+
+    @Test
+    public void testLocalAndRemoteTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"", range2, ""one"", range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"", range2, ""two"", range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"", range2, ""four"", range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"", range2, ""five"", range3, ""five""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr4, // local
+                                                                                    ep -> ep.equals(addr4) || ep.equals(addr5), // transient
+                                                                                    false,
+                                                                                    true,","[{'comment': 'this should probably be tested with `pullRepair = false` - otherwise the assert below passes with only `addr4` transient', 'commenter': 'krummas'}, {'comment': '+1, addressed', 'commenter': 'ifesdjeen'}]"
276,test/unit/org/apache/cassandra/repair/RepairJobTest.java,"@@ -0,0 +1,569 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.net.UnknownHostException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+import com.google.common.collect.Sets;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.streaming.PreviewKind;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.apache.cassandra.utils.MerkleTrees;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class RepairJobTest
+{
+    private static final IPartitioner PARTITIONER = ByteOrderedPartitioner.instance;
+
+    static InetAddressAndPort addr1;
+    static InetAddressAndPort addr2;
+    static InetAddressAndPort addr3;
+    static InetAddressAndPort addr4;
+    static InetAddressAndPort addr5;
+
+    static Range<Token> range1 = range(0, 1);
+    static Range<Token> range2 = range(2, 3);
+    static Range<Token> range3 = range(4, 5);
+    static RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+
+    @AfterClass
+    public static void reset()
+    {
+        FBUtilities.reset();
+    }
+
+    static
+    {
+        try
+        {
+            addr1 = InetAddressAndPort.getByName(""127.0.0.1"");
+            addr2 = InetAddressAndPort.getByName(""127.0.0.2"");
+            addr3 = InetAddressAndPort.getByName(""127.0.0.3"");
+            addr4 = InetAddressAndPort.getByName(""127.0.0.4"");
+            addr5 = InetAddressAndPort.getByName(""127.0.0.5"");
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+        catch (UnknownHostException e)
+        {
+            e.printStackTrace();
+        }
+    }
+
+    @Test
+    public void testCreateStandardSyncTasks()
+    {
+        testCreateStandardSyncTasks(false);
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksPullRepair()
+    {
+        testCreateStandardSyncTasks(true);
+    }
+
+    public static void testCreateStandardSyncTasks(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    noTransient(), // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(2, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertEquals(!pullRepair, ((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertTrue(task instanceof SymmetricRemoteSyncTask);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        Assert.assertNull(tasks.get(pair(addr1, addr3)));
+    }
+
+    @Test
+    public void testStanardSyncTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncTransient(true);
+        testStanardSyncTransient(false);
+    }
+
+    public void testStanardSyncTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr2),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertFalse(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testStanardSyncLocalTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncLocalTransient(true);
+        testStanardSyncLocalTransient(false);
+    }
+
+    public void testStanardSyncLocalTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr1),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        if (pullRepair)
+        {
+            Assert.assertTrue(tasks.isEmpty());
+            return;
+        }
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertFalse(((LocalSyncTask) task).requestRanges);
+        Assert.assertTrue(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testEmptyDifference()
+    {
+        // one of the nodes is a local coordinator
+        testEmptyDifference(addr1, noTransient(), true);
+        testEmptyDifference(addr1, noTransient(), false);
+        testEmptyDifference(addr2, noTransient(), true);
+        testEmptyDifference(addr2, noTransient(), false);
+        testEmptyDifference(addr1, transientPredicate(addr1), true);
+        testEmptyDifference(addr2, transientPredicate(addr1), true);
+        testEmptyDifference(addr1, transientPredicate(addr1), false);
+        testEmptyDifference(addr2, transientPredicate(addr1), false);
+        testEmptyDifference(addr1, transientPredicate(addr2), true);
+        testEmptyDifference(addr2, transientPredicate(addr2), true);
+        testEmptyDifference(addr1, transientPredicate(addr2), false);
+        testEmptyDifference(addr2, transientPredicate(addr2), false);
+
+        // nonlocal coordinator
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+    }
+
+    public void testEmptyDifference(InetAddressAndPort local, Predicate<InetAddressAndPort> isTransient, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""same"", range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient,
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertTrue(tasks.isEmpty());
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    ep -> ep.equals(addr3), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr1, addr3));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testCreate5NodeStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        SyncNodePair[] pairs = new SyncNodePair[] {pair(addr1, addr2),
+                                                   pair(addr1, addr3),
+                                                   pair(addr1, addr4),
+                                                   pair(addr1, addr5),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr5),
+                                                   pair(addr3, addr4),
+                                                   pair(addr3, addr5)};
+
+        for (SyncNodePair pair : pairs)
+        {
+            SyncTask task = tasks.get(pair);
+            // Local only if addr1 is a coordinator
+            assertEquals(task.isLocal(), pair.coordinator.equals(addr1));
+
+            // Symmetric only if there are no transient participants
+            assertEquals(String.format(""Coordinator: %s\n, Peer: %s\n"",
+                                       pair.coordinator,
+                                       pair.peer),
+                         (!pair.coordinator.equals(addr1) && !pair.peer.equals(addr1)) &&
+                         (isTransient.test(pair.coordinator) || isTransient.test(pair.peer)),
+                         task instanceof AsymmetricRemoteSyncTask);
+
+            // All ranges to be synchronised
+            Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransient()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, false);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransientPullRepair()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, true);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+
+    }
+
+    public static void testLocalSyncWithTransient(InetAddressAndPort local, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        assertEquals(9, tasks.size());
+        for (InetAddressAndPort addr : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+        {
+            if (local.equals(addr))
+                continue;
+
+            LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr));
+            assertTrue(task.requestRanges);
+            assertEquals(!pullRepair, task.transferRanges);
+        }
+
+        LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr4));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+
+        task = (LocalSyncTask) tasks.get(pair(local, addr5));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+    }
+
+    @Test
+    public void testLocalAndRemoteTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"", range2, ""one"", range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"", range2, ""two"", range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"", range2, ""four"", range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"", range2, ""five"", range3, ""five""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr4, // local
+                                                                                    ep -> ep.equals(addr4) || ep.equals(addr5), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        assertNull(tasks.get(pair(addr4, addr5)));
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr1, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        for (SyncNodePair pair : new SyncNodePair[]{ pair(addr1, addr2),
+                                                     pair(addr1, addr3),
+                                                     pair(addr2, addr1),
+                                                     pair(addr2, addr3),
+                                                     pair(addr3, addr1),
+                                                     pair(addr3, addr2) })
+        {
+            assertEquals(Arrays.asList(range1, range2, range3), tasks.get(pair).rangesToSync);
+        }
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasks()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one""),
+                                                         treeResponse(addr2, range1, ""one"",   range2, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""two""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr4, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+","[{'comment': 'maybe assert that all tasks are `AsymmetricRemoteSyncTask`s', 'commenter': 'krummas'}, {'comment': '+1, good point', 'commenter': 'ifesdjeen'}]"
276,test/unit/org/apache/cassandra/repair/RepairJobTest.java,"@@ -0,0 +1,569 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.net.UnknownHostException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+import com.google.common.collect.Sets;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.streaming.PreviewKind;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.apache.cassandra.utils.MerkleTrees;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class RepairJobTest
+{
+    private static final IPartitioner PARTITIONER = ByteOrderedPartitioner.instance;
+
+    static InetAddressAndPort addr1;
+    static InetAddressAndPort addr2;
+    static InetAddressAndPort addr3;
+    static InetAddressAndPort addr4;
+    static InetAddressAndPort addr5;
+
+    static Range<Token> range1 = range(0, 1);
+    static Range<Token> range2 = range(2, 3);
+    static Range<Token> range3 = range(4, 5);
+    static RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+
+    @AfterClass
+    public static void reset()
+    {
+        FBUtilities.reset();
+    }
+
+    static
+    {
+        try
+        {
+            addr1 = InetAddressAndPort.getByName(""127.0.0.1"");
+            addr2 = InetAddressAndPort.getByName(""127.0.0.2"");
+            addr3 = InetAddressAndPort.getByName(""127.0.0.3"");
+            addr4 = InetAddressAndPort.getByName(""127.0.0.4"");
+            addr5 = InetAddressAndPort.getByName(""127.0.0.5"");
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+        catch (UnknownHostException e)
+        {
+            e.printStackTrace();
+        }
+    }
+
+    @Test
+    public void testCreateStandardSyncTasks()
+    {
+        testCreateStandardSyncTasks(false);
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksPullRepair()
+    {
+        testCreateStandardSyncTasks(true);
+    }
+
+    public static void testCreateStandardSyncTasks(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    noTransient(), // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(2, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertEquals(!pullRepair, ((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertTrue(task instanceof SymmetricRemoteSyncTask);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        Assert.assertNull(tasks.get(pair(addr1, addr3)));
+    }
+
+    @Test
+    public void testStanardSyncTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncTransient(true);
+        testStanardSyncTransient(false);
+    }
+
+    public void testStanardSyncTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr2),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertFalse(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testStanardSyncLocalTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncLocalTransient(true);
+        testStanardSyncLocalTransient(false);
+    }
+
+    public void testStanardSyncLocalTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr1),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        if (pullRepair)
+        {
+            Assert.assertTrue(tasks.isEmpty());
+            return;
+        }
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertFalse(((LocalSyncTask) task).requestRanges);
+        Assert.assertTrue(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testEmptyDifference()
+    {
+        // one of the nodes is a local coordinator
+        testEmptyDifference(addr1, noTransient(), true);
+        testEmptyDifference(addr1, noTransient(), false);
+        testEmptyDifference(addr2, noTransient(), true);
+        testEmptyDifference(addr2, noTransient(), false);
+        testEmptyDifference(addr1, transientPredicate(addr1), true);
+        testEmptyDifference(addr2, transientPredicate(addr1), true);
+        testEmptyDifference(addr1, transientPredicate(addr1), false);
+        testEmptyDifference(addr2, transientPredicate(addr1), false);
+        testEmptyDifference(addr1, transientPredicate(addr2), true);
+        testEmptyDifference(addr2, transientPredicate(addr2), true);
+        testEmptyDifference(addr1, transientPredicate(addr2), false);
+        testEmptyDifference(addr2, transientPredicate(addr2), false);
+
+        // nonlocal coordinator
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+    }
+
+    public void testEmptyDifference(InetAddressAndPort local, Predicate<InetAddressAndPort> isTransient, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""same"", range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient,
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertTrue(tasks.isEmpty());
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    ep -> ep.equals(addr3), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr1, addr3));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testCreate5NodeStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        SyncNodePair[] pairs = new SyncNodePair[] {pair(addr1, addr2),
+                                                   pair(addr1, addr3),
+                                                   pair(addr1, addr4),
+                                                   pair(addr1, addr5),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr5),
+                                                   pair(addr3, addr4),
+                                                   pair(addr3, addr5)};
+
+        for (SyncNodePair pair : pairs)
+        {
+            SyncTask task = tasks.get(pair);
+            // Local only if addr1 is a coordinator
+            assertEquals(task.isLocal(), pair.coordinator.equals(addr1));
+
+            // Symmetric only if there are no transient participants
+            assertEquals(String.format(""Coordinator: %s\n, Peer: %s\n"",
+                                       pair.coordinator,
+                                       pair.peer),
+                         (!pair.coordinator.equals(addr1) && !pair.peer.equals(addr1)) &&
+                         (isTransient.test(pair.coordinator) || isTransient.test(pair.peer)),
+                         task instanceof AsymmetricRemoteSyncTask);
+
+            // All ranges to be synchronised
+            Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransient()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, false);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransientPullRepair()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, true);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+
+    }
+
+    public static void testLocalSyncWithTransient(InetAddressAndPort local, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        assertEquals(9, tasks.size());
+        for (InetAddressAndPort addr : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+        {
+            if (local.equals(addr))
+                continue;
+
+            LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr));
+            assertTrue(task.requestRanges);
+            assertEquals(!pullRepair, task.transferRanges);
+        }
+
+        LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr4));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+
+        task = (LocalSyncTask) tasks.get(pair(local, addr5));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+    }
+
+    @Test
+    public void testLocalAndRemoteTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"", range2, ""one"", range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"", range2, ""two"", range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"", range2, ""four"", range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"", range2, ""five"", range3, ""five""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr4, // local
+                                                                                    ep -> ep.equals(addr4) || ep.equals(addr5), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        assertNull(tasks.get(pair(addr4, addr5)));
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr1, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        for (SyncNodePair pair : new SyncNodePair[]{ pair(addr1, addr2),
+                                                     pair(addr1, addr3),
+                                                     pair(addr2, addr1),
+                                                     pair(addr2, addr3),
+                                                     pair(addr3, addr1),
+                                                     pair(addr3, addr2) })
+        {
+            assertEquals(Arrays.asList(range1, range2, range3), tasks.get(pair).rangesToSync);
+        }
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasks()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one""),
+                                                         treeResponse(addr2, range1, ""one"",   range2, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""two""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr4, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr1, addr3)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr1, addr2)).rangesToSync);","[{'comment': 'this assert depends on the iteration order in https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/repair/asymmetric/ReduceHelper.java#L55 - when we have an option to stream between two hosts, we always pick the one with the fewest outgoing streams, so if by chance we have added a few outgoing streams to `addr2` before this, we might sync `range2` from `addr3` instead\r\n\r\nwe should probably assert that `addr1` gets `range2` from either `addr2` or `addr3` (but not both) instead', 'commenter': 'krummas'}, {'comment': 'added a special assertion ', 'commenter': 'ifesdjeen'}]"
276,test/unit/org/apache/cassandra/repair/RepairJobTest.java,"@@ -0,0 +1,569 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.net.UnknownHostException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+import com.google.common.collect.Sets;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.streaming.PreviewKind;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.apache.cassandra.utils.MerkleTrees;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class RepairJobTest
+{
+    private static final IPartitioner PARTITIONER = ByteOrderedPartitioner.instance;
+
+    static InetAddressAndPort addr1;
+    static InetAddressAndPort addr2;
+    static InetAddressAndPort addr3;
+    static InetAddressAndPort addr4;
+    static InetAddressAndPort addr5;
+
+    static Range<Token> range1 = range(0, 1);
+    static Range<Token> range2 = range(2, 3);
+    static Range<Token> range3 = range(4, 5);
+    static RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+
+    @AfterClass
+    public static void reset()
+    {
+        FBUtilities.reset();
+    }
+
+    static
+    {
+        try
+        {
+            addr1 = InetAddressAndPort.getByName(""127.0.0.1"");
+            addr2 = InetAddressAndPort.getByName(""127.0.0.2"");
+            addr3 = InetAddressAndPort.getByName(""127.0.0.3"");
+            addr4 = InetAddressAndPort.getByName(""127.0.0.4"");
+            addr5 = InetAddressAndPort.getByName(""127.0.0.5"");
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+        catch (UnknownHostException e)
+        {
+            e.printStackTrace();
+        }
+    }
+
+    @Test
+    public void testCreateStandardSyncTasks()
+    {
+        testCreateStandardSyncTasks(false);
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksPullRepair()
+    {
+        testCreateStandardSyncTasks(true);
+    }
+
+    public static void testCreateStandardSyncTasks(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    noTransient(), // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(2, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertEquals(!pullRepair, ((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertTrue(task instanceof SymmetricRemoteSyncTask);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        Assert.assertNull(tasks.get(pair(addr1, addr3)));
+    }
+
+    @Test
+    public void testStanardSyncTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncTransient(true);
+        testStanardSyncTransient(false);
+    }
+
+    public void testStanardSyncTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr2),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertFalse(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testStanardSyncLocalTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncLocalTransient(true);
+        testStanardSyncLocalTransient(false);
+    }
+
+    public void testStanardSyncLocalTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr1),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        if (pullRepair)
+        {
+            Assert.assertTrue(tasks.isEmpty());
+            return;
+        }
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertFalse(((LocalSyncTask) task).requestRanges);
+        Assert.assertTrue(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testEmptyDifference()
+    {
+        // one of the nodes is a local coordinator
+        testEmptyDifference(addr1, noTransient(), true);
+        testEmptyDifference(addr1, noTransient(), false);
+        testEmptyDifference(addr2, noTransient(), true);
+        testEmptyDifference(addr2, noTransient(), false);
+        testEmptyDifference(addr1, transientPredicate(addr1), true);
+        testEmptyDifference(addr2, transientPredicate(addr1), true);
+        testEmptyDifference(addr1, transientPredicate(addr1), false);
+        testEmptyDifference(addr2, transientPredicate(addr1), false);
+        testEmptyDifference(addr1, transientPredicate(addr2), true);
+        testEmptyDifference(addr2, transientPredicate(addr2), true);
+        testEmptyDifference(addr1, transientPredicate(addr2), false);
+        testEmptyDifference(addr2, transientPredicate(addr2), false);
+
+        // nonlocal coordinator
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+    }
+
+    public void testEmptyDifference(InetAddressAndPort local, Predicate<InetAddressAndPort> isTransient, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""same"", range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient,
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertTrue(tasks.isEmpty());
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    ep -> ep.equals(addr3), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr1, addr3));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testCreate5NodeStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        SyncNodePair[] pairs = new SyncNodePair[] {pair(addr1, addr2),
+                                                   pair(addr1, addr3),
+                                                   pair(addr1, addr4),
+                                                   pair(addr1, addr5),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr5),
+                                                   pair(addr3, addr4),
+                                                   pair(addr3, addr5)};
+
+        for (SyncNodePair pair : pairs)
+        {
+            SyncTask task = tasks.get(pair);
+            // Local only if addr1 is a coordinator
+            assertEquals(task.isLocal(), pair.coordinator.equals(addr1));
+
+            // Symmetric only if there are no transient participants
+            assertEquals(String.format(""Coordinator: %s\n, Peer: %s\n"",
+                                       pair.coordinator,
+                                       pair.peer),
+                         (!pair.coordinator.equals(addr1) && !pair.peer.equals(addr1)) &&
+                         (isTransient.test(pair.coordinator) || isTransient.test(pair.peer)),
+                         task instanceof AsymmetricRemoteSyncTask);
+
+            // All ranges to be synchronised
+            Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransient()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, false);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransientPullRepair()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, true);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+
+    }
+
+    public static void testLocalSyncWithTransient(InetAddressAndPort local, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        assertEquals(9, tasks.size());
+        for (InetAddressAndPort addr : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+        {
+            if (local.equals(addr))
+                continue;
+
+            LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr));
+            assertTrue(task.requestRanges);
+            assertEquals(!pullRepair, task.transferRanges);
+        }
+
+        LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr4));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+
+        task = (LocalSyncTask) tasks.get(pair(local, addr5));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+    }
+
+    @Test
+    public void testLocalAndRemoteTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"", range2, ""one"", range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"", range2, ""two"", range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"", range2, ""four"", range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"", range2, ""five"", range3, ""five""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr4, // local
+                                                                                    ep -> ep.equals(addr4) || ep.equals(addr5), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        assertNull(tasks.get(pair(addr4, addr5)));
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr1, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        for (SyncNodePair pair : new SyncNodePair[]{ pair(addr1, addr2),
+                                                     pair(addr1, addr3),
+                                                     pair(addr2, addr1),
+                                                     pair(addr2, addr3),
+                                                     pair(addr3, addr1),
+                                                     pair(addr3, addr2) })
+        {
+            assertEquals(Arrays.asList(range1, range2, range3), tasks.get(pair).rangesToSync);
+        }
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasks()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one""),
+                                                         treeResponse(addr2, range1, ""one"",   range2, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""two""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr4, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr1, addr3)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr1, addr2)).rangesToSync);
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr2, addr3)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr2, addr1)).rangesToSync);
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr3, addr2)).rangesToSync);","[{'comment': 'as above - `addr3` could stream `range1` from either `addr1` or `addr2`', 'commenter': 'krummas'}, {'comment': 'right; added this assertion', 'commenter': 'ifesdjeen'}]"
276,test/unit/org/apache/cassandra/repair/RepairJobTest.java,"@@ -0,0 +1,569 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.net.UnknownHostException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+import com.google.common.collect.Sets;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.streaming.PreviewKind;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.apache.cassandra.utils.MerkleTrees;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class RepairJobTest
+{
+    private static final IPartitioner PARTITIONER = ByteOrderedPartitioner.instance;
+
+    static InetAddressAndPort addr1;
+    static InetAddressAndPort addr2;
+    static InetAddressAndPort addr3;
+    static InetAddressAndPort addr4;
+    static InetAddressAndPort addr5;
+
+    static Range<Token> range1 = range(0, 1);
+    static Range<Token> range2 = range(2, 3);
+    static Range<Token> range3 = range(4, 5);
+    static RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+
+    @AfterClass
+    public static void reset()
+    {
+        FBUtilities.reset();
+    }
+
+    static
+    {
+        try
+        {
+            addr1 = InetAddressAndPort.getByName(""127.0.0.1"");
+            addr2 = InetAddressAndPort.getByName(""127.0.0.2"");
+            addr3 = InetAddressAndPort.getByName(""127.0.0.3"");
+            addr4 = InetAddressAndPort.getByName(""127.0.0.4"");
+            addr5 = InetAddressAndPort.getByName(""127.0.0.5"");
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+        catch (UnknownHostException e)
+        {
+            e.printStackTrace();
+        }
+    }
+
+    @Test
+    public void testCreateStandardSyncTasks()
+    {
+        testCreateStandardSyncTasks(false);
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksPullRepair()
+    {
+        testCreateStandardSyncTasks(true);
+    }
+
+    public static void testCreateStandardSyncTasks(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    noTransient(), // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(2, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertEquals(!pullRepair, ((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertTrue(task instanceof SymmetricRemoteSyncTask);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        Assert.assertNull(tasks.get(pair(addr1, addr3)));
+    }
+
+    @Test
+    public void testStanardSyncTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncTransient(true);
+        testStanardSyncTransient(false);
+    }
+
+    public void testStanardSyncTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr2),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertFalse(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testStanardSyncLocalTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncLocalTransient(true);
+        testStanardSyncLocalTransient(false);
+    }
+
+    public void testStanardSyncLocalTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr1),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        if (pullRepair)
+        {
+            Assert.assertTrue(tasks.isEmpty());
+            return;
+        }
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertFalse(((LocalSyncTask) task).requestRanges);
+        Assert.assertTrue(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testEmptyDifference()
+    {
+        // one of the nodes is a local coordinator
+        testEmptyDifference(addr1, noTransient(), true);
+        testEmptyDifference(addr1, noTransient(), false);
+        testEmptyDifference(addr2, noTransient(), true);
+        testEmptyDifference(addr2, noTransient(), false);
+        testEmptyDifference(addr1, transientPredicate(addr1), true);
+        testEmptyDifference(addr2, transientPredicate(addr1), true);
+        testEmptyDifference(addr1, transientPredicate(addr1), false);
+        testEmptyDifference(addr2, transientPredicate(addr1), false);
+        testEmptyDifference(addr1, transientPredicate(addr2), true);
+        testEmptyDifference(addr2, transientPredicate(addr2), true);
+        testEmptyDifference(addr1, transientPredicate(addr2), false);
+        testEmptyDifference(addr2, transientPredicate(addr2), false);
+
+        // nonlocal coordinator
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+    }
+
+    public void testEmptyDifference(InetAddressAndPort local, Predicate<InetAddressAndPort> isTransient, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""same"", range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient,
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertTrue(tasks.isEmpty());
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    ep -> ep.equals(addr3), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr1, addr3));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testCreate5NodeStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        SyncNodePair[] pairs = new SyncNodePair[] {pair(addr1, addr2),
+                                                   pair(addr1, addr3),
+                                                   pair(addr1, addr4),
+                                                   pair(addr1, addr5),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr5),
+                                                   pair(addr3, addr4),
+                                                   pair(addr3, addr5)};
+
+        for (SyncNodePair pair : pairs)
+        {
+            SyncTask task = tasks.get(pair);
+            // Local only if addr1 is a coordinator
+            assertEquals(task.isLocal(), pair.coordinator.equals(addr1));
+
+            // Symmetric only if there are no transient participants
+            assertEquals(String.format(""Coordinator: %s\n, Peer: %s\n"",
+                                       pair.coordinator,
+                                       pair.peer),
+                         (!pair.coordinator.equals(addr1) && !pair.peer.equals(addr1)) &&
+                         (isTransient.test(pair.coordinator) || isTransient.test(pair.peer)),
+                         task instanceof AsymmetricRemoteSyncTask);
+
+            // All ranges to be synchronised
+            Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransient()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, false);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransientPullRepair()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, true);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+
+    }
+
+    public static void testLocalSyncWithTransient(InetAddressAndPort local, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        assertEquals(9, tasks.size());
+        for (InetAddressAndPort addr : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+        {
+            if (local.equals(addr))
+                continue;
+
+            LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr));
+            assertTrue(task.requestRanges);
+            assertEquals(!pullRepair, task.transferRanges);
+        }
+
+        LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr4));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+
+        task = (LocalSyncTask) tasks.get(pair(local, addr5));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+    }
+
+    @Test
+    public void testLocalAndRemoteTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"", range2, ""one"", range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"", range2, ""two"", range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"", range2, ""four"", range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"", range2, ""five"", range3, ""five""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr4, // local
+                                                                                    ep -> ep.equals(addr4) || ep.equals(addr5), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        assertNull(tasks.get(pair(addr4, addr5)));
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr1, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        for (SyncNodePair pair : new SyncNodePair[]{ pair(addr1, addr2),
+                                                     pair(addr1, addr3),
+                                                     pair(addr2, addr1),
+                                                     pair(addr2, addr3),
+                                                     pair(addr3, addr1),
+                                                     pair(addr3, addr2) })
+        {
+            assertEquals(Arrays.asList(range1, range2, range3), tasks.get(pair).rangesToSync);
+        }
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasks()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one""),
+                                                         treeResponse(addr2, range1, ""one"",   range2, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""two""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr4, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr1, addr3)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr1, addr2)).rangesToSync);
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr2, addr3)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr2, addr1)).rangesToSync);
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr3, addr2)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr3, addr1)).rangesToSync);
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr1, // local
+                                                                                            ep -> ep.equals(addr3),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        assertTrue(task.isLocal());
+        assertElementEquals(Arrays.asList(range1, range3), task.rangesToSync);
+        assertTrue(((LocalSyncTask)task).requestRanges);
+        assertFalse(((LocalSyncTask)task).transferRanges);
+
+        task = tasks.get(pair(addr2, addr1));","[{'comment': 'as above, `addr2` could stream `range3` from either `addr1` or `addr3`', 'commenter': 'krummas'}]"
276,test/unit/org/apache/cassandra/repair/RepairJobTest.java,"@@ -0,0 +1,569 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.net.UnknownHostException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+import com.google.common.collect.Sets;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.streaming.PreviewKind;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.apache.cassandra.utils.MerkleTrees;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class RepairJobTest
+{
+    private static final IPartitioner PARTITIONER = ByteOrderedPartitioner.instance;
+
+    static InetAddressAndPort addr1;
+    static InetAddressAndPort addr2;
+    static InetAddressAndPort addr3;
+    static InetAddressAndPort addr4;
+    static InetAddressAndPort addr5;
+
+    static Range<Token> range1 = range(0, 1);
+    static Range<Token> range2 = range(2, 3);
+    static Range<Token> range3 = range(4, 5);
+    static RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+
+    @AfterClass
+    public static void reset()
+    {
+        FBUtilities.reset();
+    }
+
+    static
+    {
+        try
+        {
+            addr1 = InetAddressAndPort.getByName(""127.0.0.1"");
+            addr2 = InetAddressAndPort.getByName(""127.0.0.2"");
+            addr3 = InetAddressAndPort.getByName(""127.0.0.3"");
+            addr4 = InetAddressAndPort.getByName(""127.0.0.4"");
+            addr5 = InetAddressAndPort.getByName(""127.0.0.5"");
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+        catch (UnknownHostException e)
+        {
+            e.printStackTrace();
+        }
+    }
+
+    @Test
+    public void testCreateStandardSyncTasks()
+    {
+        testCreateStandardSyncTasks(false);
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksPullRepair()
+    {
+        testCreateStandardSyncTasks(true);
+    }
+
+    public static void testCreateStandardSyncTasks(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    noTransient(), // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(2, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertEquals(!pullRepair, ((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertTrue(task instanceof SymmetricRemoteSyncTask);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+
+        Assert.assertNull(tasks.get(pair(addr1, addr3)));
+    }
+
+    @Test
+    public void testStanardSyncTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncTransient(true);
+        testStanardSyncTransient(false);
+    }
+
+    public void testStanardSyncTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr2),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertTrue(((LocalSyncTask) task).requestRanges);
+        Assert.assertFalse(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testStanardSyncLocalTransient()
+    {
+        // Do not stream towards transient nodes
+        testStanardSyncLocalTransient(true);
+        testStanardSyncLocalTransient(false);
+    }
+
+    public void testStanardSyncLocalTransient(boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    transientPredicate(addr1),
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        if (pullRepair)
+        {
+            Assert.assertTrue(tasks.isEmpty());
+            return;
+        }
+
+        Assert.assertEquals(1, tasks.size());
+
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertFalse(((LocalSyncTask) task).requestRanges);
+        Assert.assertTrue(((LocalSyncTask) task).transferRanges);
+        Assert.assertEquals(Arrays.asList(range1, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testEmptyDifference()
+    {
+        // one of the nodes is a local coordinator
+        testEmptyDifference(addr1, noTransient(), true);
+        testEmptyDifference(addr1, noTransient(), false);
+        testEmptyDifference(addr2, noTransient(), true);
+        testEmptyDifference(addr2, noTransient(), false);
+        testEmptyDifference(addr1, transientPredicate(addr1), true);
+        testEmptyDifference(addr2, transientPredicate(addr1), true);
+        testEmptyDifference(addr1, transientPredicate(addr1), false);
+        testEmptyDifference(addr2, transientPredicate(addr1), false);
+        testEmptyDifference(addr1, transientPredicate(addr2), true);
+        testEmptyDifference(addr2, transientPredicate(addr2), true);
+        testEmptyDifference(addr1, transientPredicate(addr2), false);
+        testEmptyDifference(addr2, transientPredicate(addr2), false);
+
+        // nonlocal coordinator
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, noTransient(), true);
+        testEmptyDifference(addr3, noTransient(), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), true);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr1), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), true);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+        testEmptyDifference(addr3, transientPredicate(addr2), false);
+    }
+
+    public void testEmptyDifference(InetAddressAndPort local, Predicate<InetAddressAndPort> isTransient, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"", range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""same"", range2, ""same"", range3, ""same""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient,
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertTrue(tasks.isEmpty());
+    }
+
+    @Test
+    public void testCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    ep -> ep.equals(addr3), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        Assert.assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));
+        Assert.assertFalse(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr1, addr3));
+        Assert.assertTrue(task.isLocal());
+        Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+    }
+
+    @Test
+    public void testCreate5NodeStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr1, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        SyncNodePair[] pairs = new SyncNodePair[] {pair(addr1, addr2),
+                                                   pair(addr1, addr3),
+                                                   pair(addr1, addr4),
+                                                   pair(addr1, addr5),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr4),
+                                                   pair(addr2, addr5),
+                                                   pair(addr3, addr4),
+                                                   pair(addr3, addr5)};
+
+        for (SyncNodePair pair : pairs)
+        {
+            SyncTask task = tasks.get(pair);
+            // Local only if addr1 is a coordinator
+            assertEquals(task.isLocal(), pair.coordinator.equals(addr1));
+
+            // Symmetric only if there are no transient participants
+            assertEquals(String.format(""Coordinator: %s\n, Peer: %s\n"",
+                                       pair.coordinator,
+                                       pair.peer),
+                         (!pair.coordinator.equals(addr1) && !pair.peer.equals(addr1)) &&
+                         (isTransient.test(pair.coordinator) || isTransient.test(pair.peer)),
+                         task instanceof AsymmetricRemoteSyncTask);
+
+            // All ranges to be synchronised
+            Assert.assertEquals(Arrays.asList(range1, range2, range3), task.rangesToSync);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransient()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, false);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+    }
+
+    @Test
+    public void testLocalSyncWithTransientPullRepair()
+    {
+        try
+        {
+            for (InetAddressAndPort local : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+            {
+                FBUtilities.reset();
+                DatabaseDescriptor.setBroadcastAddress(local.address);
+                testLocalSyncWithTransient(local, true);
+            }
+        }
+        finally
+        {
+            FBUtilities.reset();
+            DatabaseDescriptor.setBroadcastAddress(addr1.address);
+        }
+
+    }
+
+    public static void testLocalSyncWithTransient(InetAddressAndPort local, boolean pullRepair)
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"",  range2, ""four"",  range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"",  range2, ""five"",  range3, ""five""));
+
+        Predicate<InetAddressAndPort> isTransient = ep -> ep.equals(addr4) || ep.equals(addr5);
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    local, // local
+                                                                                    isTransient, // transient
+                                                                                    false,
+                                                                                    pullRepair,
+                                                                                    PreviewKind.ALL));
+
+        assertEquals(9, tasks.size());
+        for (InetAddressAndPort addr : new InetAddressAndPort[]{ addr1, addr2, addr3 })
+        {
+            if (local.equals(addr))
+                continue;
+
+            LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr));
+            assertTrue(task.requestRanges);
+            assertEquals(!pullRepair, task.transferRanges);
+        }
+
+        LocalSyncTask task = (LocalSyncTask) tasks.get(pair(local, addr4));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+
+        task = (LocalSyncTask) tasks.get(pair(local, addr5));
+        assertTrue(task.requestRanges);
+        assertFalse(task.transferRanges);
+    }
+
+    @Test
+    public void testLocalAndRemoteTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"", range2, ""one"", range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"", range2, ""two"", range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""),
+                                                         treeResponse(addr4, range1, ""four"", range2, ""four"", range3, ""four""),
+                                                         treeResponse(addr5, range1, ""five"", range2, ""five"", range3, ""five""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createStandardSyncTasks(desc,
+                                                                                    treeResponses,
+                                                                                    addr4, // local
+                                                                                    ep -> ep.equals(addr4) || ep.equals(addr5), // transient
+                                                                                    false,
+                                                                                    true,
+                                                                                    PreviewKind.ALL));
+
+        assertNull(tasks.get(pair(addr4, addr5)));
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasksAllDifferent()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one"",   range3, ""one""),
+                                                         treeResponse(addr2, range1, ""two"",   range2, ""two"",   range3, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""three"", range3, ""three""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr1, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        for (SyncNodePair pair : new SyncNodePair[]{ pair(addr1, addr2),
+                                                     pair(addr1, addr3),
+                                                     pair(addr2, addr1),
+                                                     pair(addr2, addr3),
+                                                     pair(addr3, addr1),
+                                                     pair(addr3, addr2) })
+        {
+            assertEquals(Arrays.asList(range1, range2, range3), tasks.get(pair).rangesToSync);
+        }
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasks()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""one"",   range2, ""one""),
+                                                         treeResponse(addr2, range1, ""one"",   range2, ""two""),
+                                                         treeResponse(addr3, range1, ""three"", range2, ""two""));
+
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr4, // local
+                                                                                            noTransient(),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr1, addr3)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr1, addr2)).rangesToSync);
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr2, addr3)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr2, addr1)).rangesToSync);
+
+        assertEquals(Arrays.asList(range1), tasks.get(pair(addr3, addr2)).rangesToSync);
+        assertEquals(Arrays.asList(range2), tasks.get(pair(addr3, addr1)).rangesToSync);
+    }
+
+    @Test
+    public void testOptimizedCreateStandardSyncTasksWithTransient()
+    {
+        List<TreeResponse> treeResponses = Arrays.asList(treeResponse(addr1, range1, ""same"",      range2, ""same"", range3, ""same""),
+                                                         treeResponse(addr2, range1, ""different"", range2, ""same"", range3, ""different""),
+                                                         treeResponse(addr3, range1, ""same"",      range2, ""same"", range3, ""same""));
+
+        RepairJobDesc desc = new RepairJobDesc(UUID.randomUUID(), UUID.randomUUID(), ""ks"", ""cf"", Arrays.asList());
+        Map<SyncNodePair, SyncTask> tasks = toMap(RepairJob.createOptimisedSyncingSyncTasks(desc,
+                                                                                            treeResponses,
+                                                                                            addr1, // local
+                                                                                            ep -> ep.equals(addr3),
+                                                                                            addr -> ""DC1"",
+                                                                                            false,
+                                                                                            PreviewKind.ALL));
+
+        assertEquals(3, tasks.size());
+        SyncTask task = tasks.get(pair(addr1, addr2));
+        assertTrue(task.isLocal());
+        assertElementEquals(Arrays.asList(range1, range3), task.rangesToSync);
+        assertTrue(((LocalSyncTask)task).requestRanges);
+        assertFalse(((LocalSyncTask)task).transferRanges);
+
+        task = tasks.get(pair(addr2, addr1));
+        assertFalse(task.isLocal());
+        assertElementEquals(Arrays.asList(range3), task.rangesToSync);
+
+        task = tasks.get(pair(addr2, addr3));","[{'comment': 'and same as above', 'commenter': 'krummas'}]"
278,src/java/org/apache/cassandra/net/MessagingService.java,"@@ -1078,7 +1078,7 @@ public void sendOneWay(MessageOut message, int id, InetAddressAndPort to)
             logger.trace(""{} sending {} to {}@{}"", FBUtilities.getBroadcastAddressAndPort(), message.verb, id, to);
 
         if (to.equals(FBUtilities.getBroadcastAddressAndPort()))
-            logger.trace(""Message-to-self {} going over MessagingService"", message);
+            logger.warn(""Message-to-self {} going over MessagingService"", message);","[{'comment': 'Use NoSpamLogger and maybe for the key incorporate the stack trace so we rate limit every call site separately?', 'commenter': 'aweisberg'}, {'comment': ""My last thought is that as a user I can't do much with this information. It's mostly harmless, but I also can't fix it other than by filing a bug report. \r\n\r\nI think this is the biggest issue with this change. It will cause people worry when nothing is wrong. I think this should be debug not warn because no action is required on part of a user (as opposed to a developer)."", 'commenter': 'aweisberg'}, {'comment': ""Right, we can do debug here. I wanted to first throw in this case, but then thought that it's more useful to find all the cases where we still do that and eliminate those, since failing in that case brings more or less nothing."", 'commenter': 'ifesdjeen'}]"
278,src/java/org/apache/cassandra/service/reads/repair/AbstractReadRepair.java,"@@ -102,12 +104,24 @@ void sendReadCommand(Replica to, ReadCallback readCallback, boolean speculative)
             else type = to.isFull() ? ""full"" : ""transient"";
             Tracing.trace(""Enqueuing {} data read to {}"", type, to);
         }
-        MessageOut<ReadCommand> message = command.createMessage();
-        // if enabled, request additional info about repaired data from any full replicas
-        if (command.isTrackingRepairedStatus() && to.isFull())
-            message = message.withParameter(ParameterType.TRACK_REPAIRED_DATA, MessagingService.ONE_BYTE);
 
-        MessagingService.instance().sendRRWithFailure(message, to.endpoint(), readCallback);
+        if (to.isSelf())
+        {
+            try (ReadExecutionController executionController = command.executionController();","[{'comment': ""Just asking the question, should this occur in this thread or the read stage?\r\n\r\nIs there a possibility of this operation succeeding if the local portion times out? If there is maybe it shouldn't be done synchronously in the request stage?"", 'commenter': 'aweisberg'}, {'comment': ""Yes, you're absolutely right: it is wrong to block this thread for I/O. We in fact have a pattern in order to deal with these things on local execution path: https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/reads/AbstractReadExecutor.java#L157 https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/reads/ShortReadPartitionsProtection.java#L186 \r\n\r\nThanks for pointing that out!"", 'commenter': 'ifesdjeen'}]"
278,src/java/org/apache/cassandra/net/MessagingService.java,"@@ -1078,7 +1078,7 @@ public void sendOneWay(MessageOut message, int id, InetAddressAndPort to)
             logger.trace(""{} sending {} to {}@{}"", FBUtilities.getBroadcastAddressAndPort(), message.verb, id, to);
 
         if (to.equals(FBUtilities.getBroadcastAddressAndPort()))
-            logger.trace(""Message-to-self {} going over MessagingService"", message);
+            logger.debug(""Message-to-self {} going over MessagingService"", message);","[{'comment': ""This really needs to be NoSpamLogger? How many times a second might we do this in some cases? What if new code comes that causes this to occur many times a second?\r\n\r\nDebug logging is on all the time. That's really not a risk we should be taking."", 'commenter': 'aweisberg'}, {'comment': ""To be honest, I think relying on the logging might have been a bad idea. I've left this statement with `trace` and have rewritten the tests instead."", 'commenter': 'ifesdjeen'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -88,37 +110,51 @@ public void testAllocate(int threadCount, long duration, int poolSize) throws In
         final CountDownLatch latch = new CountDownLatch(threadCount);
         final SPSCQueue<BufferCheck>[] sharedRecycle = new SPSCQueue[threadCount];
         final AtomicBoolean[] makingProgress = new AtomicBoolean[threadCount];
+        AtomicBoolean burnFreed = new AtomicBoolean(false);","[{'comment': 'This can be made `final`.', 'commenter': 'dineshjoshi'}, {'comment': 'thx, fixed.', 'commenter': 'jonmeredith'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -129,16 +165,22 @@ void testOne() throws Exception
                         return;
                     }
 
-                    BufferPool.put(buffer);
-                    burn.add(buffer);
+                    // 50/50 chance of returning the buffer from the producer thread, or
+                    // pass it on to the consumer.
+                    if (rand.nextBoolean()) {","[{'comment': 'According to the code style, you can skip braces here. We also follow follow Sun Style so braces go on new lines.\r\nReference: http://cassandra.apache.org/doc/latest/development/code_style.html', 'commenter': 'dineshjoshi'}, {'comment': 'fixed', 'commenter': 'jonmeredith'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -230,6 +272,10 @@ else if (!recycleFromNeighbour())
                         }
                     }
 
+                    if (currentTargetSize == 0) {","[{'comment': 'According to the code style, you can skip braces here. We also follow follow Sun Style so braces go on new lines.\r\nReference: http://cassandra.apache.org/doc/latest/development/code_style.html', 'commenter': 'dineshjoshi'}, {'comment': 'fixed', 'commenter': 'jonmeredith'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -327,24 +373,41 @@ BufferCheck sample()
 
                     return checks.get(index);
                 }
-
-                private int sum1toN(int n)
-                {
-                    return (n * (n + 1)) / 2;
-                }
             }));
         }
 
-        boolean first = true;
         while (!latch.await(10L, TimeUnit.SECONDS))
         {
-            if (!first)
-                BufferPool.assertAllRecycled();
-            first = false;
+            int stalledThreads = 0;
+            int doneThreads = 0;
+
             for (AtomicBoolean progress : makingProgress)
             {
-                assert progress.get();
-                progress.set(false);
+                if (progress.getAndSet(false) == false)
+                    stalledThreads++;
+            }
+
+            for (Future<Boolean> r : ret)
+            {
+                if (r.isDone())
+                    doneThreads++;
+            }
+            if (doneThreads == 0) // If any threads have completed, they will stop making progress/recycling buffers.
+            {                     // Assertions failures on the threads will be caught below.
+                assert stalledThreads == 0;
+                boolean allFreed = burnFreed.getAndSet(false);
+                for (AtomicBoolean freedMemory : freedAllMemory)
+                {","[{'comment': 'According to the code style, you can skip braces here.\r\nReference: http://cassandra.apache.org/doc/latest/development/code_style.html', 'commenter': 'dineshjoshi'}, {'comment': 'fixed', 'commenter': 'jonmeredith'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -327,24 +373,41 @@ BufferCheck sample()
 
                     return checks.get(index);
                 }
-
-                private int sum1toN(int n)
-                {
-                    return (n * (n + 1)) / 2;
-                }
             }));
         }
 
-        boolean first = true;
         while (!latch.await(10L, TimeUnit.SECONDS))
         {
-            if (!first)
-                BufferPool.assertAllRecycled();
-            first = false;
+            int stalledThreads = 0;
+            int doneThreads = 0;
+
             for (AtomicBoolean progress : makingProgress)
             {
-                assert progress.get();
-                progress.set(false);
+                if (progress.getAndSet(false) == false)
+                    stalledThreads++;
+            }
+
+            for (Future<Boolean> r : ret)
+            {
+                if (r.isDone())
+                    doneThreads++;
+            }
+            if (doneThreads == 0) // If any threads have completed, they will stop making progress/recycling buffers.
+            {                     // Assertions failures on the threads will be caught below.
+                assert stalledThreads == 0;
+                boolean allFreed = burnFreed.getAndSet(false);
+                for (AtomicBoolean freedMemory : freedAllMemory)
+                {
+                    allFreed = allFreed && freedMemory.getAndSet(false);
+                }
+                if (allFreed)
+                {","[{'comment': 'According to the code style, you can skip braces here.\r\nReference: http://cassandra.apache.org/doc/latest/development/code_style.html', 'commenter': 'dineshjoshi'}, {'comment': 'fixed', 'commenter': 'jonmeredith'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -327,24 +373,41 @@ BufferCheck sample()
 
                     return checks.get(index);
                 }
-
-                private int sum1toN(int n)
-                {
-                    return (n * (n + 1)) / 2;
-                }
             }));
         }
 
-        boolean first = true;
         while (!latch.await(10L, TimeUnit.SECONDS))
         {
-            if (!first)
-                BufferPool.assertAllRecycled();
-            first = false;
+            int stalledThreads = 0;
+            int doneThreads = 0;
+
             for (AtomicBoolean progress : makingProgress)
             {
-                assert progress.get();
-                progress.set(false);
+                if (progress.getAndSet(false) == false)
+                    stalledThreads++;
+            }
+
+            for (Future<Boolean> r : ret)
+            {
+                if (r.isDone())
+                    doneThreads++;
+            }
+            if (doneThreads == 0) // If any threads have completed, they will stop making progress/recycling buffers.
+            {                     // Assertions failures on the threads will be caught below.
+                assert stalledThreads == 0;
+                boolean allFreed = burnFreed.getAndSet(false);
+                for (AtomicBoolean freedMemory : freedAllMemory)
+                {
+                    allFreed = allFreed && freedMemory.getAndSet(false);
+                }
+                if (allFreed)
+                {
+                    BufferPool.assertAllRecycled();
+                }
+                else
+                {","[{'comment': 'According to the code style, you can skip braces here.\r\nReference: http://cassandra.apache.org/doc/latest/development/code_style.html', 'commenter': 'dineshjoshi'}, {'comment': 'fixed', 'commenter': 'jonmeredith'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -181,7 +222,7 @@ void checkpoint()
                 void testOne() throws Exception
                 {
 
-                    long currentTargetSize = rand.nextInt(poolSize / 1024) == 0 ? 0 : targetSize;
+                    long currentTargetSize = (rand.nextInt(poolSize / 1024) == 0 || freedAllMemory[threadIdx].get() == false) ? 0 : targetSize;","[{'comment': '```suggestion\r\n                    long currentTargetSize = (rand.nextInt(poolSize / 1024) == 0 || !freedAllMemory[threadIdx].get()) ? 0 : targetSize;\r\n```\r\n\r\nThis is the canonical way in Java.', 'commenter': 'dineshjoshi'}, {'comment': 'thx', 'commenter': 'jonmeredith'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -327,24 +371,35 @@ BufferCheck sample()
 
                     return checks.get(index);
                 }
-
-                private int sum1toN(int n)
-                {
-                    return (n * (n + 1)) / 2;
-                }
             }));
         }
 
-        boolean first = true;
         while (!latch.await(10L, TimeUnit.SECONDS))
         {
-            if (!first)
-                BufferPool.assertAllRecycled();
-            first = false;
+            int stalledThreads = 0;
+            int doneThreads = 0;
+
             for (AtomicBoolean progress : makingProgress)
             {
-                assert progress.get();
-                progress.set(false);
+                if (progress.getAndSet(false) == false)","[{'comment': '```suggestion\r\n                if (!progress.getAndSet(false))\r\n```', 'commenter': 'dineshjoshi'}, {'comment': ""I left the assert alone as on 346, seems like there shouldn't be any side-effects from executing an assert (and they're optional so it could affect correctness if somebody ran without -ea). The getAndSet isn't required for correctness as the worker thread only changes from false -> true and doesn't take any action based on the flag, and it will fix it in the next cycle."", 'commenter': 'jonmeredith'}]"
279,test/burn/org/apache/cassandra/utils/memory/LongBufferPoolTest.java,"@@ -36,9 +36,34 @@
 
 import static org.junit.Assert.*;
 
+/**
+ * Long BufferPool test - make sure that the BufferPool allocates and recycles
+ * ByteBuffers under heavy concurrent usage.
+ *
+ * The test creates two groups of threads
+ *
+ * - the burn producer/consumer pair that allocates 1/10 poolSize and then returns
+ *   all the memory to the pool. 50% is freed by the producer, 50% passed to the consumer thread.
+ *
+ * - a ring of worker threads that allocate buffers and either immediately free them,
+ *   or pass to the next worker thread for it to be freed on it's behalf.  Periodically
+ *   all memory is freed by the thread.
+ *
+ * While the burn/worker threads run, the original main thread checks that all of the threads are still
+ * making progress every 10s (no locking issues, or exits from assertion failures),
+ * and that every chunk has been freed at least once during the previous cycle (if that was possible).
+ *
+ * The test does not expect to survive out-of-memory errors, so needs sufficient heap memory
+ * for non-direct buffers and the debug tracking objects that check the allocate buffers.
+ * (The timing is very interesting when Xmx is lowered to increase garbage collection pauses, but do
+ * not set it too low).
+ */
 public class LongBufferPoolTest
 {
     private static final Logger logger = LoggerFactory.getLogger(LongBufferPoolTest.class);
+    final int avgBufferSize = 16 << 10;","[{'comment': 'Constants are typically uppercased with underscores as separators eg. `AVG_BUFFER_SIZE`. Do you also need this to be package private? It can be made private and static.', 'commenter': 'dineshjoshi'}]"
290,src/java/org/apache/cassandra/gms/Gossiper.java,"@@ -136,6 +139,38 @@
 
     private volatile long lastProcessedMessageAt = System.currentTimeMillis();
 
+    private boolean haveMajorVersion3Nodes = true;
+
+    final com.google.common.base.Supplier<Boolean> haveMajorVersion3NodesSupplier = () ->
+    {
+        //Once there are no prior version nodes we don't need to keep rechecking
+        if (!haveMajorVersion3Nodes)
+            return false;
+
+        Iterable<InetAddressAndPort> allHosts = Iterables.concat(Gossiper.instance.getLiveMembers(), Gossiper.instance.getUnreachableMembers());
+        CassandraVersion referenceVersion = null;
+
+        for (InetAddressAndPort host : allHosts)
+        {
+            CassandraVersion version = getReleaseVersion(host);
+
+            //Raced with changes to gossip state
+            if (version == null)
+                continue;
+
+            if (referenceVersion == null)
+                referenceVersion = version;
+
+            if (version.major < 4)","[{'comment': 'This will capture nodes with major version 2 as well. If that is ok, we should probably rename variables to reflect that.', 'commenter': 'dineshjoshi'}, {'comment': '> This will capture nodes with major version 2 as well. If that is ok, we should probably rename variables to reflect that.\r\n\r\nNvm this. We discussed offline, version 2 nodes cannot connection to version 4 nodes. Enforced by \r\nhttps://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/InboundHandshakeHandler.java#L180', 'commenter': 'dineshjoshi'}]"
292,build.xml,"@@ -412,6 +412,7 @@
           <dependency groupId=""org.xerial.snappy"" artifactId=""snappy-java"" version=""1.1.2.6""/>
           <dependency groupId=""org.lz4"" artifactId=""lz4-java"" version=""1.4.0""/>
           <dependency groupId=""com.ning"" artifactId=""compress-lzf"" version=""0.8.4""/>
+          <dependency groupId=""com.github.luben"" artifactId=""zstd-jni"" version=""1.3.5-2""/>","[{'comment': 'Nit: might be worth grabbing the latest version that was recently released. I think latest is 1.3.7-2 or 3.', 'commenter': 'jolynch'}, {'comment': ""The latest version is `1.3.7`. Let's upgrade to it."", 'commenter': 'dineshjoshi'}]"
292,src/java/org/apache/cassandra/io/compress/ZSTDCompressor.java,"@@ -0,0 +1,138 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.compress;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.github.luben.zstd.Zstd;
+
+public class ZSTDCompressor implements ICompressor
+{
+    public static final int FAST_COMPRESSION = 1; // fastest compression time
+    public static final int DEFAULT_LEVEL = 3;
+    public static final int BEST_COMPRESSION = 22;// very good compression ratio
+    private static final ZSTDCompressor instance = new ZSTDCompressor();
+    private static final String COMPRESSION_LEVEL_OPTION_NAME = ""compression_level"";
+    @VisibleForTesting
+    protected static final int compressionLevel = DEFAULT_LEVEL;
+
+    public static ZSTDCompressor create(Map<String, String> compressionOptions)
+    {
+        validateCompressionLevel(parseCompressionLevelOption(compressionOptions));
+        return instance;
+    }
+
+    private static void validateCompressionLevel(int compressionLevel)
+    {
+        if (compressionLevel < FAST_COMPRESSION || compressionLevel > BEST_COMPRESSION)
+        {
+            throw new IllegalArgumentException(
+                ""ZSTD compression_level "" + Integer.toString(compressionLevel) + "" invalid "",
+                null
+            );
+        }
+    }
+
+    private static int parseCompressionLevelOption(Map<String,String> compressionOptions)
+    {
+        return Integer.parseInt(compressionOptions.getOrDefault(COMPRESSION_LEVEL_OPTION_NAME,
+                                                                Integer.toString(DEFAULT_LEVEL)));
+    }
+
+    @Override
+    public int initialCompressedBufferLength(int chunkLength)
+    {
+        return (int)Zstd.compressBound(chunkLength);
+    }
+
+    @Override
+    public int uncompress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset)
+    throws IOException
+    {
+        long decompressSize = Zstd.decompressByteArray(output,
+                                                       outputOffset,
+                                                       output.length - outputOffset,
+                                                       input,
+                                                       inputOffset,
+                                                       inputLength);
+        if (Zstd.isError(decompressSize))
+        {
+            throw new IOException(""ZSTD uncompress failed with error reason "" + Zstd.getErrorName(decompressSize));
+        }
+
+        return (int) decompressSize;
+    }
+
+    @Override
+    public void uncompress(ByteBuffer input, ByteBuffer output) throws IOException
+    {
+        Zstd.decompress(output, input);
+    }
+
+    @Override
+    public void compress(ByteBuffer input, ByteBuffer output) throws IOException","[{'comment': 'I think you may be able to replace this with the [``compress(ByteBuffer dstBuf, ByteBuffer srcBuf, int level)``](https://github.com/luben/zstd-jni/blob/master/src/main/java/com/github/luben/zstd/Zstd.java#L535) method, you just have to make sure to pass the arguments in the opposite order (output comes first) and re-throw the `RuntimeException` as an `IOException`.', 'commenter': 'jolynch'}, {'comment': 'ICompressor Interface supports only compress(ByteBuffer input, ByteBuffer output), am i missing something?', 'commenter': 'sushmaad'}, {'comment': 'Indeed, but I think you can simplify this implementation by using the built in `Zstd.compress` method I linked above which afaict does what the code here does.', 'commenter': 'jolynch'}, {'comment': 'Yes will do that, that seems to simplify the code here', 'commenter': 'sushmaad'}]"
292,src/java/org/apache/cassandra/io/compress/ZSTDCompressor.java,"@@ -0,0 +1,138 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.compress;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.github.luben.zstd.Zstd;
+
+public class ZSTDCompressor implements ICompressor
+{
+    public static final int FAST_COMPRESSION = 1; // fastest compression time
+    public static final int DEFAULT_LEVEL = 3;
+    public static final int BEST_COMPRESSION = 22;// very good compression ratio
+    private static final ZSTDCompressor instance = new ZSTDCompressor();
+    private static final String COMPRESSION_LEVEL_OPTION_NAME = ""compression_level"";
+    @VisibleForTesting
+    protected static final int compressionLevel = DEFAULT_LEVEL;
+
+    public static ZSTDCompressor create(Map<String, String> compressionOptions)
+    {
+        validateCompressionLevel(parseCompressionLevelOption(compressionOptions));
+        return instance;","[{'comment': 'If we want to support different sstables/column families having different levels I think that this needs to be something closer to what the LZ4 compressor does where it creates a `ConcurrentHashMap` of instances keyed by parameter rather than just one instance. In practice I _think_ this will lead to every compressor having the default level (might want to TDD and ensure that different parameters yield different instances).', 'commenter': 'jolynch'}, {'comment': ""Yes, right now the configuration parameter isn't used at all...\r\nRegarding the ConcurrentHasMap - it makes sense only when [explicit context](http://facebook.github.io/zstd/zstd_manual.html#Chapter5) is used, but this functionality isn't exposed by JNI bindings yet."", 'commenter': 'alexott'}, {'comment': 'Sorry, just to clarify this is a Cassandra level concern, not a Zstd issue. Cassandra calls the `create` method via reflection [here](https://github.com/apache/cassandra/blob/caf50de31b034ed77140b3c1597e7ca6ddc44e17/src/java/org/apache/cassandra/schema/CompressionParams.java#L288-L289), and since the `ICompressors` are ""stateless"" we can make a reasonable cache using a `ConcurrentHashMap` like [LZ4Compressor](https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/io/compress/LZ4Compressor.java#L61) does keyed by parameter. This assumes of course that the `Zstd` apis are threadsafe, but I\'m pretty confident they are.', 'commenter': 'jolynch'}, {'comment': '@jolynch @alexott indeed compression_level is currently always set to default, I will refactor this a bit so that each CF will have CompressionParams with dedicated compressor by removing the static reference. \r\nSeems to me that there is a tradeoff between  having static reference with statefulness based on compression_level versus just creating multiple compressor with each CompressionParams.\r\nHaving individual compressor might increase the number of compressor objects floating around but we have the flexibility of directly using compression_level without any complicated pre-determined conditions. let me know what you think.', 'commenter': 'sushmaad'}, {'comment': ""From my brief reading of the JNI Bindings it seems that zstd-jni uses `ZSTD_compress` and not `ZSTD_compressCCtx`. The latter allows you to pass in a context which would've been more efficient. For now, I think it would be ok to simply pass in the compression level. The only value of keeping multiple compressor objects around is to retain the `CompressionParams`. It would be useful to have a caching factory to avoid creating multiple objects with the same compression params.\r\n\r\nAs far as thread safety goes, the JNI Code looks thread safe specifically compress and decompress methods. As pointed out earlier they create and destroy the Compression Context on each invocation which is memory unfriendly. So, although the zstd benchmarks may look great, I am not so sure about this JNI binding.\r\n\r\nWe should definitely add a JMH Benchmark for zstd."", 'commenter': 'dineshjoshi'}]"
292,src/java/org/apache/cassandra/io/compress/ZSTDCompressor.java,"@@ -0,0 +1,138 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.compress;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.github.luben.zstd.Zstd;
+
+public class ZSTDCompressor implements ICompressor
+{
+    public static final int FAST_COMPRESSION = 1; // fastest compression time
+    public static final int DEFAULT_LEVEL = 3;
+    public static final int BEST_COMPRESSION = 22;// very good compression ratio
+    private static final ZSTDCompressor instance = new ZSTDCompressor();
+    private static final String COMPRESSION_LEVEL_OPTION_NAME = ""compression_level"";
+    @VisibleForTesting
+    protected static final int compressionLevel = DEFAULT_LEVEL;
+
+    public static ZSTDCompressor create(Map<String, String> compressionOptions)
+    {
+        validateCompressionLevel(parseCompressionLevelOption(compressionOptions));
+        return instance;
+    }
+
+    private static void validateCompressionLevel(int compressionLevel)
+    {
+        if (compressionLevel < FAST_COMPRESSION || compressionLevel > BEST_COMPRESSION)
+        {
+            throw new IllegalArgumentException(
+                ""ZSTD compression_level "" + Integer.toString(compressionLevel) + "" invalid "",
+                null
+            );
+        }
+    }
+
+    private static int parseCompressionLevelOption(Map<String,String> compressionOptions)
+    {
+        return Integer.parseInt(compressionOptions.getOrDefault(COMPRESSION_LEVEL_OPTION_NAME,
+                                                                Integer.toString(DEFAULT_LEVEL)));
+    }
+
+    @Override
+    public int initialCompressedBufferLength(int chunkLength)
+    {
+        return (int)Zstd.compressBound(chunkLength);
+    }
+
+    @Override
+    public int uncompress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset)
+    throws IOException
+    {
+        long decompressSize = Zstd.decompressByteArray(output,
+                                                       outputOffset,
+                                                       output.length - outputOffset,
+                                                       input,
+                                                       inputOffset,
+                                                       inputLength);
+        if (Zstd.isError(decompressSize))
+        {
+            throw new IOException(""ZSTD uncompress failed with error reason "" + Zstd.getErrorName(decompressSize));
+        }
+
+        return (int) decompressSize;
+    }
+
+    @Override
+    public void uncompress(ByteBuffer input, ByteBuffer output) throws IOException
+    {
+        Zstd.decompress(output, input);
+    }
+
+    @Override
+    public void compress(ByteBuffer input, ByteBuffer output) throws IOException
+    {
+        if (!input.isDirect())
+        {
+            throw new IllegalArgumentException(""input must be a direct buffer"");
+        }
+
+        if (!output.isDirect())
+        {
+            throw new IllegalArgumentException(""output must be a direct buffer"");
+        }
+        long compressedSize = Zstd.compressDirectByteBuffer(output,
+                                                            output.position(),
+                                                            output.limit() - output.position(),
+                                                            input,
+                                                            input.position(),
+                                                            input.limit() - input.position(),
+                                                            compressionLevel);","[{'comment': ""From what I can tell this doesn't ever change even if the level setting is changed? Am I missing something or is it effectively always 3?"", 'commenter': 'jolynch'}, {'comment': ""Yes, the compression level setting isn't applied"", 'commenter': 'alexott'}]"
292,src/java/org/apache/cassandra/io/compress/ZSTDCompressor.java,"@@ -0,0 +1,138 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.compress;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.github.luben.zstd.Zstd;
+
+public class ZSTDCompressor implements ICompressor
+{
+    public static final int FAST_COMPRESSION = 1; // fastest compression time
+    public static final int DEFAULT_LEVEL = 3;
+    public static final int BEST_COMPRESSION = 22;// very good compression ratio
+    private static final ZSTDCompressor instance = new ZSTDCompressor();
+    private static final String COMPRESSION_LEVEL_OPTION_NAME = ""compression_level"";
+    @VisibleForTesting
+    protected static final int compressionLevel = DEFAULT_LEVEL;
+
+    public static ZSTDCompressor create(Map<String, String> compressionOptions)
+    {
+        validateCompressionLevel(parseCompressionLevelOption(compressionOptions));
+        return instance;
+    }
+
+    private static void validateCompressionLevel(int compressionLevel)
+    {
+        if (compressionLevel < FAST_COMPRESSION || compressionLevel > BEST_COMPRESSION)
+        {
+            throw new IllegalArgumentException(
+                ""ZSTD compression_level "" + Integer.toString(compressionLevel) + "" invalid "",
+                null
+            );
+        }
+    }
+
+    private static int parseCompressionLevelOption(Map<String,String> compressionOptions)
+    {
+        return Integer.parseInt(compressionOptions.getOrDefault(COMPRESSION_LEVEL_OPTION_NAME,
+                                                                Integer.toString(DEFAULT_LEVEL)));
+    }
+
+    @Override
+    public int initialCompressedBufferLength(int chunkLength)
+    {
+        return (int)Zstd.compressBound(chunkLength);
+    }
+
+    @Override
+    public int uncompress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset)
+    throws IOException
+    {
+        long decompressSize = Zstd.decompressByteArray(output,
+                                                       outputOffset,
+                                                       output.length - outputOffset,
+                                                       input,
+                                                       inputOffset,
+                                                       inputLength);
+        if (Zstd.isError(decompressSize))
+        {
+            throw new IOException(""ZSTD uncompress failed with error reason "" + Zstd.getErrorName(decompressSize));
+        }
+
+        return (int) decompressSize;
+    }
+
+    @Override
+    public void uncompress(ByteBuffer input, ByteBuffer output) throws IOException
+    {
+        Zstd.decompress(output, input);","[{'comment': 'I think this needs to catch the [`RuntimeException`](https://github.com/luben/zstd-jni/blob/master/src/main/java/com/github/luben/zstd/Zstd.java#L895) and rethrow as an `IOException`.', 'commenter': 'jolynch'}, {'comment': ""[decompress functions](https://github.com/luben/zstd-jni/blob/master/src/main/java/com/github/luben/zstd/Zstd.java#L189) doesn't throw exceptions - it should be checked the same way as above - with `Zstd.isError`."", 'commenter': 'alexott'}, {'comment': 'My reading of the [`decompress(ByteBuffer dstBuffer, ByteBuffer srcBuf)`](https://github.com/luben/zstd-jni/blob/c642dc38eddd9142c55644034e6485444a25f052/src/main/java/com/github/luben/zstd/Zstd.java#L879) method used here is that it calls `Zstd.isError` and throws a `RuntimeException`. To be consistent with the `ICompressor` contract these methods should afaict throw `IOException` instead.', 'commenter': 'jolynch'}]"
292,src/java/org/apache/cassandra/io/compress/ZSTDCompressor.java,"@@ -0,0 +1,138 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.compress;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.github.luben.zstd.Zstd;
+
+public class ZSTDCompressor implements ICompressor
+{
+    public static final int FAST_COMPRESSION = 1; // fastest compression time
+    public static final int DEFAULT_LEVEL = 3;
+    public static final int BEST_COMPRESSION = 22;// very good compression ratio
+    private static final ZSTDCompressor instance = new ZSTDCompressor();
+    private static final String COMPRESSION_LEVEL_OPTION_NAME = ""compression_level"";
+    @VisibleForTesting
+    protected static final int compressionLevel = DEFAULT_LEVEL;
+
+    public static ZSTDCompressor create(Map<String, String> compressionOptions)
+    {
+        validateCompressionLevel(parseCompressionLevelOption(compressionOptions));
+        return instance;
+    }
+
+    private static void validateCompressionLevel(int compressionLevel)
+    {
+        if (compressionLevel < FAST_COMPRESSION || compressionLevel > BEST_COMPRESSION)","[{'comment': 'ZStd also supports negative compression levels that are fast but at cost of compression:\r\n\r\n> The library supports regular compression levels from 1 up to ZSTD_maxCLevel(),\r\n  which is currently 22. Levels >= 20, labeled `--ultra`, should be used with\r\n  caution, as they require more memory. The library also offers negative\r\n  compression levels, which extend the range of speed vs. ratio preferences.\r\n  The lower the level, the faster the speed (at the cost of compression).\r\n\r\nI think that it makes sense to add them as well. I think that the maximal negative value is 17...', 'commenter': 'alexott'}, {'comment': 'Zstd accepts integer levels from negative infinity up to 22. Levels 1 to 22 are normal levels, ranging from fastest at level 1 to highest compression ratio at level 22. 0 selects the default compression level (which is level 3). Negative levels are a new addition, and are even faster than level 1, getting faster the more negative you go. \r\nDefinitely negative compression levels can be supported here.', 'commenter': 'sushmaad'}]"
354,test/distributed/org/apache/cassandra/distributed/impl/TracingUtil.java,"@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.impl;
+
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.UUID;
+
+import org.apache.cassandra.db.ConsistencyLevel;
+
+
+/**
+ * Utilities for accessing the system_traces table from in-JVM dtests
+ */
+public class TracingUtil
+{
+    /**
+     * Represents an entry from system_traces
+     */
+    public static class TraceEntry
+    {
+        public final UUID sessionId;
+        public final UUID eventId;
+        public final String activity;
+        public final InetAddress source;
+        public final int sourceElapsed;
+        public final String thread;
+
+        private TraceEntry(UUID sessionId, UUID eventId, String activity, InetAddress sourceIP, int sourceElapsed, String thread)
+        {
+            this.sessionId = sessionId;
+            this.eventId = eventId;
+            this.activity = activity;
+            this.source = sourceIP;
+            this.sourceElapsed = sourceElapsed;
+            this.thread = thread;
+        }
+
+        static TraceEntry fromObjects(Object[] objects)","[{'comment': ""The method name doesn't clearly convey the use of the method. Could we call this something different? This method converts a row result (list of deserialized objects) to a trace entry. Perhaps call this `fromRowResultObjects()`? I'm open to other name suggestions too."", 'commenter': 'dineshjoshi'}, {'comment': 'Renamed to `fromResultObjects` are requested.', 'commenter': 'jonmeredith'}]"
354,test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java,"@@ -371,6 +389,97 @@ public Builder(int nodeCount, Factory<I, C> factory)
             return this;
         }
 
+        public Builder<I, C> withNodes(int nodeCount) {
+            this.nodeCount = nodeCount;
+            return this;
+        }
+
+        public Builder<I, C> withDCs(int dcCount)
+        {
+            return withRacks(dcCount, 1);
+        }
+
+        public Builder<I, C> withRacks(int dcCount, int racksPerDC)
+        {
+            if (nodeCount == 0)
+                throw new IllegalStateException(""Node count will be calculated. Do not supply total node count in the builder"");
+
+            int totalRacks = dcCount * racksPerDC;
+            int nodesPerRack = (nodeCount + totalRacks - 1) / totalRacks; // round up to next integer
+            return withRacks(dcCount, racksPerDC, nodesPerRack);
+        }
+
+        public Builder<I, C> withRacks(int dcCount, int racksPerDC, int nodesPerRack)
+        {
+            if (nodeIdTopology != null)
+                throw new IllegalStateException(""Network topology already created. Call withDCs/withRacks once or before withDC/withRack calls"");
+
+            nodeIdTopology = new HashMap<>();
+            int nodeId = 1;
+            for (int dc = 1; dc <= dcCount; dc++)
+            {
+                for (int rack = 1; rack <= racksPerDC; rack++)
+                {
+                    for (int rackNodeIdx = 0; rackNodeIdx < nodesPerRack; rackNodeIdx++)
+                        nodeIdTopology.put(nodeId++, Pair.create(dcName(dc), rackName(rack)));
+                }
+            }
+            // adjust the node count to match the allocatation
+            final int adjustedNodeCount = dcCount * racksPerDC * nodesPerRack;
+            if (adjustedNodeCount != nodeCount)
+            {
+                assert adjustedNodeCount > nodeCount : ""withRacks should only ever increase the node count"";
+                logger.info(""Network topology of {} DCs with {} racks per DC and {} nodes per rack required increasing total nodes to {}"",
+                            dcCount, racksPerDC, nodesPerRack, adjustedNodeCount);
+                nodeCount = adjustedNodeCount;
+            }
+            return this;
+        }
+
+        public Builder<I, C> withDC(String dcName, int nodeCount)
+        {
+            return withRack(dcName, rackName(1), nodeCount);
+        }
+
+        public Builder<I, C> withRack(String dcName, String rackName, int nodesInRack)
+        {
+            if (nodeIdTopology == null)
+            {
+                if (nodeCount > 0)
+                    throw new IllegalStateException(""Node count must not be explicitly set, or allocated using withDCs/withRacks"");
+
+                nodeIdTopology = new HashMap<>();
+            }
+            for (int nodeId = nodeCount + 1; nodeId <= nodeCount + nodesInRack; nodeId++)
+                nodeIdTopology.put(nodeId, Pair.create(dcName, rackName));
+
+            nodeCount += nodesInRack;
+            return this;
+        }
+
+        // Map of node ids to dc and rack - must be contiguous with an entry nodeId 1 to nodeCount
+        public Builder<I, C> withNodeIdTopology(Map<Integer,Pair<String,String>> nodeIdTopology)
+        {
+            if (nodeIdTopology.isEmpty())
+                throw new IllegalStateException(""Topology is empty. It must have an entry for every nodeId."");
+
+            IntStream.rangeClosed(1, nodeIdTopology.size()).forEach(nodeId -> {","[{'comment': 'Not that this really matters here, but why should we use stream here instead of a simple for loop? I really see no advantages.', 'commenter': 'ifesdjeen'}, {'comment': ""It just seemed more concise than the for loop in somewhere that wasn't performance sensitive, but I'm still finding my way with appropriate use of streams."", 'commenter': 'jonmeredith'}]"
354,test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java,"@@ -396,27 +505,42 @@ public C createWithoutStarting() throws IOException
 
             if (root == null)
                 root = Files.createTempDirectory(""dtests"").toFile();
+
             if (version == null)
                 version = Versions.CURRENT;
 
+            if (nodeCount <= 0)
+                throw new IllegalStateException(""Cluster must have at least one node"");
+
+            if (nodeIdTopology == null)
+                nodeIdTopology = IntStream.rangeClosed(1, nodeCount).boxed()","[{'comment': 'Same here. I might be just thinking a bit old-school)', 'commenter': 'ifesdjeen'}, {'comment': 'Same reasoning as above.', 'commenter': 'jonmeredith'}]"
371,src/java/org/apache/cassandra/concurrent/LocalAwareExecutorService.java,"@@ -62,15 +62,41 @@
      */
     int getPendingTaskCount();
 
+
+    /**
+     * Returns the number of core threads
+     *
+     * @return the number of core threads
+     */
+    int getCorePoolSize();
+
+    /**
+     * Sets the number of core threads.
+     */
+    void setCorePoolSize(int newCorePoolSize);
+
     /**
      * Returns the maximum allowed number of threads.
      *
      * @return the maximum allowed number of threads
      */
     int getMaximumPoolSize();
 
+    /**
+     * Sets the maximum allowed number of threads.
+     */
+    void setMaximumPoolSize(int newMaximumPoolSize);
+
     default int getMaxTasksQueued()
     {
         return -1;
     }
+
+    interface MaxWorkersListener","[{'comment': 'Should the names be more consistent? `MaxWorkers` vs `MaximumPoolSize`?', 'commenter': 'belliottsmith'}, {'comment': 'Good call, renamed.', 'commenter': 'jonmeredith'}]"
371,src/java/org/apache/cassandra/concurrent/SEPExecutor.java,"@@ -21,26 +21,34 @@
 import java.util.List;
 import java.util.concurrent.ConcurrentLinkedQueue;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import org.apache.cassandra.metrics.ThreadPoolMetrics;
+import org.apache.cassandra.utils.MBeanWrapper;
 import org.apache.cassandra.utils.concurrent.SimpleCondition;
 import org.apache.cassandra.utils.concurrent.WaitQueue;
 
 import static org.apache.cassandra.concurrent.SEPWorker.Work;
 
-public class SEPExecutor extends AbstractLocalAwareExecutorService
+public class SEPExecutor extends AbstractLocalAwareExecutorService implements SEPExecutorMBean
 {
+    private static final Logger logger = LoggerFactory.getLogger(SEPExecutor.class);
     private final SharedExecutorPool pool;
 
-    public final int maxWorkers;
+    public final AtomicInteger maxWorkers;
+    MaximumPoolSizeListener updatedMaxWorkers;","[{'comment': 'final?', 'commenter': 'belliottsmith'}, {'comment': 'Might be nice to also name it like its callback method, i.e. onUpdatedMaxWorkers, since it only has that function.', 'commenter': 'belliottsmith'}, {'comment': 'ack', 'commenter': 'jonmeredith'}, {'comment': 'renamed so the update is called as `maximumPoolSizeListener.onUpdateMaximumPoolSize(newMaximumPoolSize);`', 'commenter': 'jonmeredith'}]"
371,src/java/org/apache/cassandra/concurrent/SEPExecutor.java,"@@ -260,23 +282,64 @@ public long getCompletedTaskCount()
 
     public int getActiveTaskCount()
     {
-        return maxWorkers - workPermits(permits.get());
+        return maxWorkers.get() - workPermits(permits.get());
+    }
+
+    public int getCorePoolSize()
+    {
+        return 0;
+    }
+
+    public void setCorePoolSize(int newCorePoolSize)
+    {
+        throw new IllegalArgumentException(""Cannot resize core pool size of SEPExecutor"");
     }
 
     @Override
     public int getMaximumPoolSize()
     {
-        return maxWorkers;
+        return maxWorkers.get();
+    }
+
+    @Override
+    public void setMaximumPoolSize(int newMaximumPoolSize)","[{'comment': 'synchronised?', 'commenter': 'belliottsmith'}, {'comment': ""can make it synchronized - I was relying on maxWorkers CAS to protect against concurrent updates, but it's unlikely to be called frequently and makes it more obvious."", 'commenter': 'jonmeredith'}, {'comment': ""We're throwing `ISE` on concurrent updates to `maxWorkers` though?  Admittedly this would be a hard race to encounter given current usages.  Don't mind what we choose, just asking the questions."", 'commenter': 'belliottsmith'}]"
371,src/java/org/apache/cassandra/concurrent/SEPExecutor.java,"@@ -21,26 +21,34 @@
 import java.util.List;
 import java.util.concurrent.ConcurrentLinkedQueue;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import org.apache.cassandra.metrics.ThreadPoolMetrics;
+import org.apache.cassandra.utils.MBeanWrapper;
 import org.apache.cassandra.utils.concurrent.SimpleCondition;
 import org.apache.cassandra.utils.concurrent.WaitQueue;
 
 import static org.apache.cassandra.concurrent.SEPWorker.Work;
 
-public class SEPExecutor extends AbstractLocalAwareExecutorService
+public class SEPExecutor extends AbstractLocalAwareExecutorService implements SEPExecutorMBean
 {
+    private static final Logger logger = LoggerFactory.getLogger(SEPExecutor.class);
     private final SharedExecutorPool pool;
 
-    public final int maxWorkers;
+    public final AtomicInteger maxWorkers;","[{'comment': 'Could rename all of these to `maxPoolSize` for consistency - not necessary at all, but might be nice now we have concept leakage', 'commenter': 'belliottsmith'}, {'comment': ""Agreed - I don't think these changes introduce any additional concept leakage, the concepts already leaked and hopefully this tidies things up a little."", 'commenter': 'jonmeredith'}]"
371,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1620,21 +1620,57 @@ public static int getConcurrentReaders()
         return conf.concurrent_reads;
     }
 
+    public static void setConcurrentReaders(int concurrent_reads)","[{'comment': ""What's the intended purpose of these methods?"", 'commenter': 'belliottsmith'}, {'comment': ""It was to keep the configuration up to date with the system. I don't think it's as important as it was but when discussing with @clohfink he thought it was still valuable to keep the config in sync. \r\n\r\nThe getters for the writers like `getConcurrentWriters` are called from `PaxosState`/`ViewManager` to size their `LOCKS` however I thought that given the operational use case for this is reducing concurrency from the starting value the locks will have been initialized large enough."", 'commenter': 'jonmeredith'}, {'comment': ""They look to be used in a fairly imprecise manner, so even if we increase the number of threads I don't expect it should massively worsen things.  Ass to whether or not we need to keep the config in sync - I guess there's a purity in that, but we could also rename our getters to indicate the value is one-use.  We certainly don't update the config in all places we modify behaviour that was seeded by a value from the config.  That's not to say we shouldn't start now, but it seems like a policy change worth raising up the flag pole.  I don't personally see _huge_ value in it, except perhaps when debugging a heap dump, but I don't usually go and look at the `Config` object then anyway.  That's all to say that it doesn't really matter too much to me either way, but I have a personal preference for simpler code where possible."", 'commenter': 'belliottsmith'}, {'comment': ""Agreed it isn't uniformly updated and perfect, but keeping the Config updated with the current values may avoid confusion for debugging heap dumps as you mentioned. As you don't object, I'll keep the code as is."", 'commenter': 'jonmeredith'}]"
371,src/java/org/apache/cassandra/tools/NodeProbe.java,"@@ -1717,6 +1718,16 @@ public void resumeBootstrap(PrintStream out) throws IOException
         }
     }
 
+    public Map<String, List<Integer>> getMaximumPoolSizes(List<String> filter)","[{'comment': 'Is this a filter, or a set of names to select? ', 'commenter': 'belliottsmith'}, {'comment': ""More accurately a set of names to select - I was thinking of it filtering the list of available stage names, but an incorrect name will trigger a message letting the user know the canonical names for the stages.  I've renamed to `stageNames` instead. \r\n\r\n```\r\nJons-PIE-MBP:trunk-work jmeredith$ bin/nodetool getconcurrency\r\nPool Name                    CorePoolSize MaximumPoolSize\r\nAntiEntropyStage                        1               1\r\nCounterMutationStage                    0              32\r\nGossipStage                             1               1\r\nImmediateStage                          0               0\r\nInternalResponseStage                  12              12\r\nMigrationStage                          1               1\r\nMiscStage                               1               1\r\nMutationStage                           0              32\r\nReadStage                               0              32\r\nRequestResponseStage                    0              12\r\nTracingStage                            1               1\r\nViewMutationStage                       0              32\r\nJons-PIE-MBP:trunk-work jmeredith$ bin/nodetool getconcurrency x\r\nPool Name                    CorePoolSize MaximumPoolSize\r\nnodetool: Must be one of READ,MUTATION,COUNTER_MUTATION,VIEW_MUTATION,GOSSIP,REQUEST_RESPONSE,ANTI_ENTROPY,MIGRATION,MISC,TRACING,INTERNAL_RESPONSE,IMMEDIATE\r\nSee 'nodetool help' or 'nodetool help <command>'.\r\n```"", 'commenter': 'jonmeredith'}]"
371,src/java/org/apache/cassandra/concurrent/SEPExecutor.java,"@@ -260,23 +282,64 @@ public long getCompletedTaskCount()
 
     public int getActiveTaskCount()
     {
-        return maxWorkers - workPermits(permits.get());
+        return maxWorkers.get() - workPermits(permits.get());
+    }
+
+    public int getCorePoolSize()
+    {
+        return 0;
+    }
+
+    public void setCorePoolSize(int newCorePoolSize)
+    {
+        throw new IllegalArgumentException(""Cannot resize core pool size of SEPExecutor"");
     }
 
     @Override
     public int getMaximumPoolSize()
     {
-        return maxWorkers;
+        return maxWorkers.get();
+    }
+
+    @Override
+    public void setMaximumPoolSize(int newMaximumPoolSize)
+    {
+        final int oldMaxWorkers = maxWorkers.get();
+        long current;
+        int workPermits;
+
+        if (newMaximumPoolSize < 0)
+        {
+            throw new IllegalArgumentException(""Maximum number of workers must not be negative"");
+        }
+
+        int deltaWorkPermits = newMaximumPoolSize - oldMaxWorkers;
+        if (!maxWorkers.compareAndSet(oldMaxWorkers, newMaximumPoolSize))
+        {
+            throw new IllegalStateException(""Maximum pool size has been changed while resizing"");
+        }
+
+        if (deltaWorkPermits == 0)
+            return;
+
+        do","[{'comment': 'Could avoid loop with simply `permits.updateAndGet(cur -> updateWorkPermits(cur, workPermits(cur) + deltaWorkPermits)`', 'commenter': 'belliottsmith'}, {'comment': 'thx, that cleans things up a bit.', 'commenter': 'jonmeredith'}]"
371,src/java/org/apache/cassandra/concurrent/ResizableThreadPool.java,"@@ -15,22 +15,28 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.cassandra.concurrent;
 
-import java.util.concurrent.BlockingQueue;
-import java.util.concurrent.TimeUnit;
+package org.apache.cassandra.concurrent;
 
-public class JMXConfigurableThreadPoolExecutor extends JMXEnabledThreadPoolExecutor implements JMXConfigurableThreadPoolExecutorMBean
+public interface ResizableThreadPool
 {
+    /**
+     * Returns maximum pool size of thread pool.
+     */
+    public int getCorePoolSize();","[{'comment': ""Most of these methods aren't used.  Should `LocalAwareExecutorService` extend `ResizableThreadPool`?"", 'commenter': 'belliottsmith'}, {'comment': ""Originally I added it make sure the JMX support for resizing pools was common. It's the same as the interface in LocalAwareExecutorService so it makes sense to use the interface.  Updated.\r\n"", 'commenter': 'jonmeredith'}]"
371,src/java/org/apache/cassandra/concurrent/SharedExecutorPool.java,"@@ -110,7 +110,12 @@ void maybeStartSpinningWorker()
 
     public synchronized LocalAwareExecutorService newExecutor(int maxConcurrency, int maxQueuedTasks, String jmxPath, String name)
     {
-        SEPExecutor executor = new SEPExecutor(this, maxConcurrency, maxQueuedTasks, jmxPath, name);
+        return newExecutor(maxConcurrency, i -> {}, maxQueuedTasks, jmxPath, name);
+    }
+
+    public LocalAwareExecutorService newExecutor(int maxConcurrency, LocalAwareExecutorService.MaximumPoolSizeListener setMaxConcurrency, int maxQueuedTasks, String jmxPath, String name)","[{'comment': '`setMaxConcurrent` -> `maximumPoolSizeListener`?', 'commenter': 'belliottsmith'}, {'comment': 'agreed. updated.', 'commenter': 'jonmeredith'}]"
381,src/java/org/apache/cassandra/db/Keyspace.java,"@@ -309,8 +311,11 @@ public boolean snapshotExists(String snapshotName)
      */
     public static void clearSnapshot(String snapshotName, String keyspace)
     {
+        RateLimiter snapshotRateLimiter = RateLimiter.create(DatabaseDescriptor.getSnapshotLinksPerSecond() == 0 ?","[{'comment': 'the local variable name can change from snapshotRateLimiter to clearSnapshotRateLimiter, though they share the same value but they are of different use. ', 'commenter': 'Maxwell-Guo'}, {'comment': 'This is addressed. ', 'commenter': 'jeffjirsa'}]"
381,src/java/org/apache/cassandra/db/SystemKeyspace.java,"@@ -1409,7 +1413,7 @@ public static void snapshotOnVersionChange() throws IOException
                                                                              previous,
                                                                              next));
             for (String keyspace : SchemaConstants.LOCAL_SYSTEM_KEYSPACE_NAMES)
-                Keyspace.open(keyspace).snapshot(snapshotName, null);
+                Keyspace.open(keyspace).snapshot(snapshotName, null, false, snapshotRateLimiter);","[{'comment': ""should system keyspace do got a snapshot rate limiter ? System keyspace will not got many sstables. for example, if we set the rate to 2 this may affect the speed of setup for the snapshotOnVersionChange , But the data keyspace's snapshot speed of 2 is what we want them to be . "", 'commenter': 'Maxwell-Guo'}, {'comment': 'This is addressed.', 'commenter': 'jeffjirsa'}]"
381,src/java/org/apache/cassandra/service/StorageServiceMBean.java,"@@ -279,7 +279,7 @@
      * Get the current hardlink-per-second throttle for snapshots
      * A setting of zero indicates no throttling.
      *
-     * @return
+     * @return snapshot links-per-second throttle","[{'comment': ""There's a suggestion that's not very important here, the variable in the notes is hardlink-per-second  , but the reaturn is links-per-second ，Actually the configure variable name is : snapshot_links_per_second .Besides I search the cassandra code , I dont know if it is a appropriate variable name. "", 'commenter': 'Maxwell-Guo'}]"
381,src/java/org/apache/cassandra/io/util/FileUtils.java,"@@ -177,15 +178,24 @@ public static File createDeletableTempFile(String prefix, String suffix)
 
     public static Throwable deleteWithConfirm(String filePath, boolean expect, Throwable accumulate)
     {
-        return deleteWithConfirm(new File(filePath), expect, accumulate);
+        return deleteWithConfirm(new File(filePath), expect, accumulate, null);
     }
 
     public static Throwable deleteWithConfirm(File file, boolean expect, Throwable accumulate)
     {
+        return deleteWithConfirm(file, expect, accumulate, null);
+    }
+    
+    public static Throwable deleteWithConfirm(File file, boolean expect, Throwable accumulate, RateLimiter rateLimiter)
+    {
+        if (rateLimiter == null)","[{'comment': 'Instead of creating a new RateLimiter for each file deleted, we can guard the rateLimiter.acquire with the null check.', 'commenter': 'clohfink'}, {'comment': '(more for non-clear snapshots that call `deleteWithConfirm`)', 'commenter': 'clohfink'}]"
381,src/java/org/apache/cassandra/io/util/FileUtils.java,"@@ -177,15 +178,24 @@ public static File createDeletableTempFile(String prefix, String suffix)
 
     public static Throwable deleteWithConfirm(String filePath, boolean expect, Throwable accumulate)
     {
-        return deleteWithConfirm(new File(filePath), expect, accumulate);
+        return deleteWithConfirm(new File(filePath), expect, accumulate, null);
     }
 
     public static Throwable deleteWithConfirm(File file, boolean expect, Throwable accumulate)
     {
+        return deleteWithConfirm(file, expect, accumulate, null);
+    }
+    
+    public static Throwable deleteWithConfirm(File file, boolean expect, Throwable accumulate, RateLimiter rateLimiter)
+    {
+        if (rateLimiter == null)
+            rateLimiter = RateLimiter.create(Double.MAX_VALUE);
+
         boolean exists = file.exists();
         assert exists || !expect : ""attempted to delete non-existing file "" + file.getName();
         try
         {
+            rateLimiter.acquire();","[{'comment': 'can we add a metric or a nospam logger for when this is >0 to more easily detect if the throttling has ever occurred or not (ie debugging snapshot cleanup issues or something)', 'commenter': 'clohfink'}, {'comment': 'Just skip `acquire()` call if `rateLimiter == null` instead of creating one conditionally?', 'commenter': 'iamaleksey'}]"
381,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -2167,6 +2170,16 @@ public static boolean getAutoSnapshot()
         return conf.auto_snapshot;
     }
 
+    public static long getSnapshotLinksPerSecond()
+    {
+        return conf.snapshot_links_per_second;
+    }
+
+    public static void setSnapshotLinksPerSecond(long throttle)
+    {
+        conf.snapshot_links_per_second = throttle;","[{'comment': ""We don't do it frequently, but perhaps we should: validate the acceptable argument range here, instead of `StorageService`, and throw IAE instead of IRE?"", 'commenter': 'iamaleksey'}, {'comment': 'Added. ', 'commenter': 'jeffjirsa'}]"
381,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -1708,14 +1708,18 @@ public ClusteringComparator getComparator()
 
     public void snapshotWithoutFlush(String snapshotName)
     {
-        snapshotWithoutFlush(snapshotName, null, false);
+        snapshotWithoutFlush(snapshotName, null, false, null);
     }
 
     /**
      * @param ephemeral If this flag is set to true, the snapshot will be cleaned during next startup
      */
-    public Set<SSTableReader> snapshotWithoutFlush(String snapshotName, Predicate<SSTableReader> predicate, boolean ephemeral)
+    public Set<SSTableReader> snapshotWithoutFlush(String snapshotName, Predicate<SSTableReader> predicate, boolean ephemeral, RateLimiter rateLimiter)
     {
+        if (rateLimiter == null)
+            rateLimiter = RateLimiter.create(DatabaseDescriptor.getSnapshotLinksPerSecond() == 0 ?","[{'comment': ""Pull this logic out, next to `DatabaseDescriptor#{get,set}SnapshotLinksPerSecond`? So that the special meaning of `0` is apparent and the logic isn't copied three extra times?"", 'commenter': 'iamaleksey'}]"
388,src/java/org/apache/cassandra/tools/NodeTool.java,"@@ -46,7 +47,20 @@
 {
     private static final String HISTORYFILE = ""nodetool.history"";
 
+    private static NodeProbeFactory nodeProbeFactory = new NodeProbeFactory();","[{'comment': 'Rather than rely on mutable state, can we just have the CLI and core implementation decoupled?  CLI can be simply a ```new NodeProbeFactory.execute(args)``` and the in-jvm dtests use a different implementation?', 'commenter': 'dcapwell'}, {'comment': 'or make execute a instance method and provide this in the `NodeTool` constructor, so main is ```new System.exit(NodeTool().execute(args))``` and dtest is `new NodeTool(override).execute(args)`', 'commenter': 'dcapwell'}, {'comment': 'The `NodeToolCmd` depends on `NodeProbe`. In order to inject the mock dependency, we either passing the mock instance to the cmd instances or like what the patch current have, depends on a static field. Why the `nodeProbeFactory` cannot be an instance field? Because the `NodeToolCmd` is a static inner class, it cannot see unless the field is a class field. \r\n\r\nIf passing the mock instance reference, we will end up having the following code in the `NodeTool#execute` method, which I think is ugly. The reason of injecting dependency by doing the type check and casting is that cli parses command into runnable. \r\n\r\n```\r\n            Runnable parse = parser.parse(args);\r\n            if (parse instanceof NodeToolCmd) // hmmm....\r\n            {\r\n                ((NodeToolCmd) parse).setNodeProbeFactory(nodeProbeFactory); // or pass in the probe reference\r\n            }\r\n            printHistory(args);\r\n            parse.run();\r\n``` ', 'commenter': 'yifan-c'}, {'comment': ""with a small change, you can get this without statics.\r\n\r\nby replacing our usage of `Runnable` with `Consumer<NodeProbeFactory>` (need to extend `Help` to do this), we can replace `parse.run()` with `parse.accept(nodeProbeFactory)`.  Since the usage is just the `connect` method, this won't change the different commands, and will be local to `NodeToolCmd`"", 'commenter': 'dcapwell'}]"
388,src/java/org/apache/cassandra/tools/NodeProbe.java,"@@ -1323,6 +1323,11 @@ public TabularData getFailureDetectorPhilValues()
             throw new RuntimeException(e);
         }
     }
+
+    public MessagingServiceMBean getMsProxy()","[{'comment': 'can we use a better name, something like `getMessagingService` or `getMessagingServiceProxy`?', 'commenter': 'dcapwell'}, {'comment': 'The method name, `getMsProxy`, is not good. The reason to do so is to follow the existing method naming convention in the class. ', 'commenter': 'yifan-c'}, {'comment': ""there are other naming conversion as well... I guess this class isn't too consistent =)"", 'commenter': 'dcapwell'}]"
388,test/distributed/org/apache/cassandra/distributed/test/NodeToolTest.java,"@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.apache.cassandra.db.ConsistencyLevel;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.upgrade.UpgradeTestBase;
+import org.junit.Test;
+
+import static org.junit.Assert.assertEquals;
+
+public class NodeToolTest extends UpgradeTestBase","[{'comment': 'why upgrade?', 'commenter': 'dcapwell'}, {'comment': 'I will change it to `DistributedTestBase`. ', 'commenter': 'yifan-c'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,241 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    // dummy constructors
+    public InternalNodeProbe(String host, int port, String username, String password) throws IOException
+    {
+        super(host, port, username, password);
+    }
+
+    public InternalNodeProbe(String host, int port) throws IOException
+    {
+        super(host, port);
+    }
+
+    protected void connect() throws IOException
+    {
+        // note that we are not connecting via JMX for testing
+        mbeanServerConn = null;
+        jmxc = null;
+
+        ssProxy = StorageService.instance;
+        msProxy = MessagingService.instance();
+        streamProxy = StreamManager.instance;
+        compactionProxy = CompactionManager.instance;
+        fdProxy = (FailureDetectorMBean) FailureDetector.instance;
+        cacheService = CacheService.instance;
+        spProxy = StorageProxy.instance;
+        hhProxy = HintedHandOffManager.instance;
+        gcProxy = new GCInspector();
+        gossProxy = Gossiper.instance;
+        memProxy = ManagementFactory.getMemoryMXBean();
+        runtimeProxy = ManagementFactory.getRuntimeMXBean();
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        // nothing to close. no-op
+    }
+
+    @Override
+    // overrides all the methods referenced mbeanServerConn/jmxc in super
+    public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()
+    {
+        return new EndpointSnitchInfo();
+    }
+
+    @Override
+    public CacheServiceMBean getCacheServiceMBean()
+    {
+        return cacheService;
+    }
+
+    @Override
+    public void setCacheCapacities(int keyCacheCapacity, int rowCacheCapacity, int counterCacheCapacity)
+    {
+        CacheServiceMBean cacheMBean = cacheService;
+        cacheMBean.setKeyCacheCapacityInMB(keyCacheCapacity);
+        cacheMBean.setRowCacheCapacityInMB(rowCacheCapacity);
+        cacheMBean.setCounterCacheCapacityInMB(counterCacheCapacity);
+    }
+
+    @Override
+    public void setCacheKeysToSave(int keyCacheKeysToSave, int rowCacheKeysToSave, int counterCacheKeysToSave)
+    {
+        CacheServiceMBean cacheMBean = cacheService;
+        cacheMBean.setKeyCacheKeysToSave(keyCacheKeysToSave);
+        cacheMBean.setRowCacheKeysToSave(rowCacheKeysToSave);
+        cacheMBean.setCounterCacheKeysToSave(counterCacheKeysToSave);
+    }
+
+    @Override
+    public void repairAsync(final PrintStream out, final String keyspace, Map<String, String> options) throws IOException
+    {
+        RepairRunner runner = new RepairRunner(out, ssProxy, keyspace, options);
+        try
+        {
+            ssProxy.addNotificationListener(runner, null, null);
+            runner.run();
+        }
+        catch (Exception e)
+        {
+            throw new IOException(e) ;
+        }
+        finally
+        {
+            try
+            {
+                ssProxy.removeNotificationListener(runner);
+            }
+            catch (Throwable e)
+            {
+                out.println(""Exception occurred during clean-up. "" + e);
+            }
+        }
+    }
+
+    @Override
+    // simply resume bootstrap, instead of streaming the JMX notification to stdout
+    public void resumeBootstrap(PrintStream out) throws IOException
+    {
+        if (ssProxy.resumeBootstrap())
+        {
+            out.println(""Resuming bootstrap"");
+        }
+        else
+        {
+            out.println(""Node is already bootstrapped."");
+        }
+    }
+
+    // dtest runs with disable_mbean_registration. No ColumnFamilyStoreMBean is registered, thus not supporting get the MBean.
+    @Override
+    public ColumnFamilyStoreMBean getCfsProxy(String ks, String cf)
+    {
+        throw new UnsupportedOperationException();","[{'comment': 'I see your comment, but even then your implementation could still delegate', 'commenter': 'dcapwell'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,241 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    // dummy constructors
+    public InternalNodeProbe(String host, int port, String username, String password) throws IOException
+    {
+        super(host, port, username, password);
+    }
+
+    public InternalNodeProbe(String host, int port) throws IOException
+    {
+        super(host, port);
+    }
+
+    protected void connect() throws IOException
+    {
+        // note that we are not connecting via JMX for testing
+        mbeanServerConn = null;
+        jmxc = null;
+
+        ssProxy = StorageService.instance;
+        msProxy = MessagingService.instance();
+        streamProxy = StreamManager.instance;
+        compactionProxy = CompactionManager.instance;
+        fdProxy = (FailureDetectorMBean) FailureDetector.instance;
+        cacheService = CacheService.instance;
+        spProxy = StorageProxy.instance;
+        hhProxy = HintedHandOffManager.instance;
+        gcProxy = new GCInspector();
+        gossProxy = Gossiper.instance;
+        memProxy = ManagementFactory.getMemoryMXBean();
+        runtimeProxy = ManagementFactory.getRuntimeMXBean();
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        // nothing to close. no-op
+    }
+
+    @Override
+    // overrides all the methods referenced mbeanServerConn/jmxc in super
+    public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()
+    {
+        return new EndpointSnitchInfo();
+    }
+
+    @Override
+    public CacheServiceMBean getCacheServiceMBean()
+    {
+        return cacheService;
+    }
+
+    @Override
+    public void setCacheCapacities(int keyCacheCapacity, int rowCacheCapacity, int counterCacheCapacity)","[{'comment': 'can we refactor `NodeProbe` so this can be easier to maintain?  By adding `getCacheServiceMBean()` there, you could remove this override and just provide that by just returning the field cacheService.\r\n\r\nMy main concern is that this will become hard to maintain.  As new nodeool commands get added, or existing ones tweaked, we have to know if we should rewrite it here. ', 'commenter': 'dcapwell'}, {'comment': 'The `NodeProbe` is messy already. No test cases, new methods added in the class only for the purpose of the new commands (, where the methods should actually belong to), etc.\r\nThis is the exact reason I created a mock version and avoid touch the one in prod source. \r\n\r\nIn principle, refactoring the `NodeProbe` is welcoming. But I think it deserves a separate PR. \r\nAdding the cache service field should be part of that...', 'commenter': 'yifan-c'}, {'comment': 'The main issues with this are that it is easier to not be maintained (new `NodeProbe` command added but not updated internal, bugs in existing commands get fixed but not ported to internal, etc.), and by forking you add a larger surface area which could add bugs.  Its possible that the forks change the behavior, and by doing so cause w/e tests use them to pass, but when we use the command on a real cluster it fails.\r\n\r\n> The NodeProbe is messy already. No test cases\r\n\r\nI 100% agree this is an issue, but by trying to ""mock"", you actually make it so no tests which use this are actually testing the real code; see point above.\r\n\r\n> new methods added in the class only for the purpose of the new commands\r\n\r\nAnd that can keep happening.  Since it can keep happening, the maintenance becomes high if we go with ""mock""\r\n\r\n> In principle, refactoring the NodeProbe is welcoming. But I think it deserves a separate PR.\r\nAdding the cache service field should be part of that...\r\n\r\nMy personal preference would be to block this PR on that change.  If you want to do that first, that\'s fine by me, but I am concerned with the maintenance cost by not doing it, so would rather not move the ""mock"" in.  \r\n\r\nIn looking at your changes, a few small refactors would actually be enough to get all your overrides to go away.', 'commenter': 'dcapwell'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    // dummy constructors
+    public InternalNodeProbe(String host, int port, String username, String password) throws IOException
+    {
+        super(host, port, username, password);
+    }
+
+    public InternalNodeProbe(String host, int port) throws IOException
+    {
+        super(host, port);
+    }
+
+    protected void connect() throws IOException
+    {
+        // note that we are not connecting via JMX for testing
+        mbeanServerConn = null;
+        jmxc = null;
+
+        ssProxy = StorageService.instance;
+        msProxy = MessagingService.instance();
+        streamProxy = StreamManager.instance;
+        compactionProxy = CompactionManager.instance;
+        fdProxy = (FailureDetectorMBean) FailureDetector.instance;
+        cacheService = CacheService.instance;
+        spProxy = StorageProxy.instance;
+        hhProxy = HintedHandOffManager.instance;
+        gcProxy = new GCInspector();
+        gossProxy = Gossiper.instance;
+        memProxy = ManagementFactory.getMemoryMXBean();
+        runtimeProxy = ManagementFactory.getRuntimeMXBean();
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        // nothing to close. no-op
+    }
+
+    @Override
+    // overrides all the methods referenced mbeanServerConn/jmxc in super
+    public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()
+    {
+        return new EndpointSnitchInfo();
+    }
+
+    @Override
+    public CacheServiceMBean getCacheServiceMBean()
+    {
+        return cacheService;
+    }
+
+    @Override
+    public void repairAsync(final PrintStream out, final String keyspace, Map<String, String> options) throws IOException
+    {
+        RepairRunner runner = new RepairRunner(out, ssProxy, keyspace, options);
+        try
+        {
+            ssProxy.addNotificationListener(runner, null, null);
+            runner.run();
+        }
+        catch (Exception e)
+        {
+            throw new IOException(e) ;
+        }
+        finally
+        {
+            try
+            {
+                ssProxy.removeNotificationListener(runner);
+            }
+            catch (Throwable e)
+            {
+                out.println(""Exception occurred during clean-up. "" + e);
+            }
+        }
+    }
+
+    @Override
+    // simply resume bootstrap, instead of streaming the JMX notification to stdout
+    public void resumeBootstrap(PrintStream out) throws IOException
+    {
+        if (ssProxy.resumeBootstrap())
+        {
+            out.println(""Resuming bootstrap"");
+        }
+        else
+        {
+            out.println(""Node is already bootstrapped."");
+        }
+    }
+
+    // dtest runs with disable_mbean_registration. No ColumnFamilyStoreMBean is registered, thus not supporting get the MBean.","[{'comment': 'remove comment', 'commenter': 'dcapwell'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    // dummy constructors
+    public InternalNodeProbe(String host, int port, String username, String password) throws IOException
+    {
+        super(host, port, username, password);
+    }
+
+    public InternalNodeProbe(String host, int port) throws IOException
+    {
+        super(host, port);
+    }
+
+    protected void connect() throws IOException
+    {
+        // note that we are not connecting via JMX for testing
+        mbeanServerConn = null;
+        jmxc = null;
+
+        ssProxy = StorageService.instance;
+        msProxy = MessagingService.instance();
+        streamProxy = StreamManager.instance;
+        compactionProxy = CompactionManager.instance;
+        fdProxy = (FailureDetectorMBean) FailureDetector.instance;
+        cacheService = CacheService.instance;
+        spProxy = StorageProxy.instance;
+        hhProxy = HintedHandOffManager.instance;
+        gcProxy = new GCInspector();
+        gossProxy = Gossiper.instance;
+        memProxy = ManagementFactory.getMemoryMXBean();
+        runtimeProxy = ManagementFactory.getRuntimeMXBean();
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        // nothing to close. no-op
+    }
+
+    @Override
+    // overrides all the methods referenced mbeanServerConn/jmxc in super
+    public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()
+    {
+        return new EndpointSnitchInfo();
+    }
+
+    @Override
+    public CacheServiceMBean getCacheServiceMBean()
+    {
+        return cacheService;
+    }
+
+    @Override
+    public void repairAsync(final PrintStream out, final String keyspace, Map<String, String> options) throws IOException
+    {
+        RepairRunner runner = new RepairRunner(out, ssProxy, keyspace, options);
+        try
+        {
+            ssProxy.addNotificationListener(runner, null, null);
+            runner.run();
+        }
+        catch (Exception e)
+        {
+            throw new IOException(e) ;
+        }
+        finally
+        {
+            try
+            {
+                ssProxy.removeNotificationListener(runner);
+            }
+            catch (Throwable e)
+            {
+                out.println(""Exception occurred during clean-up. "" + e);
+            }
+        }
+    }
+
+    @Override
+    // simply resume bootstrap, instead of streaming the JMX notification to stdout
+    public void resumeBootstrap(PrintStream out) throws IOException
+    {
+        if (ssProxy.resumeBootstrap())
+        {
+            out.println(""Resuming bootstrap"");
+        }
+        else
+        {
+            out.println(""Node is already bootstrapped."");
+        }
+    }
+
+    // dtest runs with disable_mbean_registration. No ColumnFamilyStoreMBean is registered, thus not supporting get the MBean.
+    @Override
+    public ColumnFamilyStoreMBean getCfsProxy(String ks, String cf)
+    {
+        return Keyspace.open(ks).getColumnFamilyStore(cf);
+    }
+
+    @Override
+    // The below methods are only used by the commands (i.e. Info, TableHistogram, TableStats, etc.) that display informations. Not useful for dtest, so disable it.
+    public Object getCacheMetric(String cacheType, String metricName)
+    {
+        throw new UnsupportedOperationException();","[{'comment': ""i'm ok with this for now, but its possible to implement this by using `CacheService`"", 'commenter': 'dcapwell'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    // dummy constructors
+    public InternalNodeProbe(String host, int port, String username, String password) throws IOException
+    {
+        super(host, port, username, password);
+    }
+
+    public InternalNodeProbe(String host, int port) throws IOException
+    {
+        super(host, port);
+    }
+
+    protected void connect() throws IOException
+    {
+        // note that we are not connecting via JMX for testing
+        mbeanServerConn = null;
+        jmxc = null;
+
+        ssProxy = StorageService.instance;
+        msProxy = MessagingService.instance();
+        streamProxy = StreamManager.instance;
+        compactionProxy = CompactionManager.instance;
+        fdProxy = (FailureDetectorMBean) FailureDetector.instance;
+        cacheService = CacheService.instance;
+        spProxy = StorageProxy.instance;
+        hhProxy = HintedHandOffManager.instance;
+        gcProxy = new GCInspector();
+        gossProxy = Gossiper.instance;
+        memProxy = ManagementFactory.getMemoryMXBean();
+        runtimeProxy = ManagementFactory.getRuntimeMXBean();
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        // nothing to close. no-op
+    }
+
+    @Override
+    // overrides all the methods referenced mbeanServerConn/jmxc in super
+    public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()
+    {
+        return new EndpointSnitchInfo();
+    }
+
+    @Override
+    public CacheServiceMBean getCacheServiceMBean()
+    {
+        return cacheService;
+    }
+
+    @Override
+    public void repairAsync(final PrintStream out, final String keyspace, Map<String, String> options) throws IOException
+    {
+        RepairRunner runner = new RepairRunner(out, ssProxy, keyspace, options);
+        try
+        {
+            ssProxy.addNotificationListener(runner, null, null);
+            runner.run();
+        }
+        catch (Exception e)
+        {
+            throw new IOException(e) ;
+        }
+        finally
+        {
+            try
+            {
+                ssProxy.removeNotificationListener(runner);
+            }
+            catch (Throwable e)
+            {
+                out.println(""Exception occurred during clean-up. "" + e);
+            }
+        }
+    }
+
+    @Override
+    // simply resume bootstrap, instead of streaming the JMX notification to stdout
+    public void resumeBootstrap(PrintStream out) throws IOException
+    {
+        if (ssProxy.resumeBootstrap())
+        {
+            out.println(""Resuming bootstrap"");
+        }
+        else
+        {
+            out.println(""Node is already bootstrapped."");
+        }
+    }
+
+    // dtest runs with disable_mbean_registration. No ColumnFamilyStoreMBean is registered, thus not supporting get the MBean.
+    @Override
+    public ColumnFamilyStoreMBean getCfsProxy(String ks, String cf)
+    {
+        return Keyspace.open(ks).getColumnFamilyStore(cf);
+    }
+
+    @Override
+    // The below methods are only used by the commands (i.e. Info, TableHistogram, TableStats, etc.) that display informations. Not useful for dtest, so disable it.
+    public Object getCacheMetric(String cacheType, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Iterator<Map.Entry<String, ColumnFamilyStoreMBean>> getColumnFamilyStoreMBeanProxies()
+    {
+        throw new UnsupportedOperationException();","[{'comment': 'FYI\r\n\r\n```\r\nreturn StreamSupport.stream(Keyspace.all().spliterator(), false)\r\n                     .flatMap(ks -> ks.getColumnFamilyStores().stream()\r\n                                      .<Map.Entry<String, ColumnFamilyStoreMBean>>map(store -> ImmutablePair.of(ks.getName(), store)))\r\n                     .iterator();\r\n```', 'commenter': 'dcapwell'}, {'comment': 'Since it is only used for output stats, I will keep it as unsupported. But good to know how to make a columnFamilyStoreIterator.', 'commenter': 'yifan-c'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    // dummy constructors
+    public InternalNodeProbe(String host, int port, String username, String password) throws IOException
+    {
+        super(host, port, username, password);
+    }
+
+    public InternalNodeProbe(String host, int port) throws IOException
+    {
+        super(host, port);
+    }
+
+    protected void connect() throws IOException
+    {
+        // note that we are not connecting via JMX for testing
+        mbeanServerConn = null;
+        jmxc = null;
+
+        ssProxy = StorageService.instance;
+        msProxy = MessagingService.instance();
+        streamProxy = StreamManager.instance;
+        compactionProxy = CompactionManager.instance;
+        fdProxy = (FailureDetectorMBean) FailureDetector.instance;
+        cacheService = CacheService.instance;
+        spProxy = StorageProxy.instance;
+        hhProxy = HintedHandOffManager.instance;
+        gcProxy = new GCInspector();
+        gossProxy = Gossiper.instance;
+        memProxy = ManagementFactory.getMemoryMXBean();
+        runtimeProxy = ManagementFactory.getRuntimeMXBean();
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        // nothing to close. no-op
+    }
+
+    @Override
+    // overrides all the methods referenced mbeanServerConn/jmxc in super
+    public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()
+    {
+        return new EndpointSnitchInfo();
+    }
+
+    @Override
+    public CacheServiceMBean getCacheServiceMBean()
+    {
+        return cacheService;
+    }
+
+    @Override
+    public void repairAsync(final PrintStream out, final String keyspace, Map<String, String> options) throws IOException
+    {
+        RepairRunner runner = new RepairRunner(out, ssProxy, keyspace, options);
+        try
+        {
+            ssProxy.addNotificationListener(runner, null, null);
+            runner.run();
+        }
+        catch (Exception e)
+        {
+            throw new IOException(e) ;
+        }
+        finally
+        {
+            try
+            {
+                ssProxy.removeNotificationListener(runner);
+            }
+            catch (Throwable e)
+            {
+                out.println(""Exception occurred during clean-up. "" + e);
+            }
+        }
+    }
+
+    @Override
+    // simply resume bootstrap, instead of streaming the JMX notification to stdout
+    public void resumeBootstrap(PrintStream out) throws IOException
+    {
+        if (ssProxy.resumeBootstrap())
+        {
+            out.println(""Resuming bootstrap"");
+        }
+        else
+        {
+            out.println(""Node is already bootstrapped."");
+        }
+    }
+
+    // dtest runs with disable_mbean_registration. No ColumnFamilyStoreMBean is registered, thus not supporting get the MBean.
+    @Override
+    public ColumnFamilyStoreMBean getCfsProxy(String ks, String cf)
+    {
+        return Keyspace.open(ks).getColumnFamilyStore(cf);
+    }
+
+    @Override
+    // The below methods are only used by the commands (i.e. Info, TableHistogram, TableStats, etc.) that display informations. Not useful for dtest, so disable it.
+    public Object getCacheMetric(String cacheType, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Iterator<Map.Entry<String, ColumnFamilyStoreMBean>> getColumnFamilyStoreMBeanProxies()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Multimap<String, String> getThreadPools()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Object getThreadPoolMetric(String pathName, String poolName, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Object getColumnFamilyMetric(String ks, String cf, String metricName)
+    {
+        throw new UnsupportedOperationException();","[{'comment': 'you could implement this, but it seems you need to handle 3 distinct cases, so fine if this is deferred.', 'commenter': 'dcapwell'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    // dummy constructors
+    public InternalNodeProbe(String host, int port, String username, String password) throws IOException
+    {
+        super(host, port, username, password);
+    }
+
+    public InternalNodeProbe(String host, int port) throws IOException
+    {
+        super(host, port);
+    }
+
+    protected void connect() throws IOException
+    {
+        // note that we are not connecting via JMX for testing
+        mbeanServerConn = null;
+        jmxc = null;
+
+        ssProxy = StorageService.instance;
+        msProxy = MessagingService.instance();
+        streamProxy = StreamManager.instance;
+        compactionProxy = CompactionManager.instance;
+        fdProxy = (FailureDetectorMBean) FailureDetector.instance;
+        cacheService = CacheService.instance;
+        spProxy = StorageProxy.instance;
+        hhProxy = HintedHandOffManager.instance;
+        gcProxy = new GCInspector();
+        gossProxy = Gossiper.instance;
+        memProxy = ManagementFactory.getMemoryMXBean();
+        runtimeProxy = ManagementFactory.getRuntimeMXBean();
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        // nothing to close. no-op
+    }
+
+    @Override
+    // overrides all the methods referenced mbeanServerConn/jmxc in super
+    public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()
+    {
+        return new EndpointSnitchInfo();
+    }
+
+    @Override
+    public CacheServiceMBean getCacheServiceMBean()
+    {
+        return cacheService;
+    }
+
+    @Override
+    public void repairAsync(final PrintStream out, final String keyspace, Map<String, String> options) throws IOException
+    {
+        RepairRunner runner = new RepairRunner(out, ssProxy, keyspace, options);
+        try
+        {
+            ssProxy.addNotificationListener(runner, null, null);
+            runner.run();
+        }
+        catch (Exception e)
+        {
+            throw new IOException(e) ;
+        }
+        finally
+        {
+            try
+            {
+                ssProxy.removeNotificationListener(runner);
+            }
+            catch (Throwable e)
+            {
+                out.println(""Exception occurred during clean-up. "" + e);
+            }
+        }
+    }
+
+    @Override
+    // simply resume bootstrap, instead of streaming the JMX notification to stdout
+    public void resumeBootstrap(PrintStream out) throws IOException
+    {
+        if (ssProxy.resumeBootstrap())
+        {
+            out.println(""Resuming bootstrap"");
+        }
+        else
+        {
+            out.println(""Node is already bootstrapped."");
+        }
+    }
+
+    // dtest runs with disable_mbean_registration. No ColumnFamilyStoreMBean is registered, thus not supporting get the MBean.
+    @Override
+    public ColumnFamilyStoreMBean getCfsProxy(String ks, String cf)
+    {
+        return Keyspace.open(ks).getColumnFamilyStore(cf);
+    }
+
+    @Override
+    // The below methods are only used by the commands (i.e. Info, TableHistogram, TableStats, etc.) that display informations. Not useful for dtest, so disable it.
+    public Object getCacheMetric(String cacheType, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Iterator<Map.Entry<String, ColumnFamilyStoreMBean>> getColumnFamilyStoreMBeanProxies()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Multimap<String, String> getThreadPools()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Object getThreadPoolMetric(String pathName, String poolName, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Object getColumnFamilyMetric(String ks, String cf, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public CassandraMetricsRegistry.JmxTimerMBean getProxyMetric(String scope)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Object getCompactionMetric(String metricName)
+    {
+        throw new UnsupportedOperationException();","[{'comment': 'CompactionManager can return its metrics; see `org.apache.cassandra.db.compaction.CompactionManager#getMetrics`\r\n\r\nAnnoying with all the special casing, so fine to not implement for now.', 'commenter': 'dcapwell'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    // dummy constructors
+    public InternalNodeProbe(String host, int port, String username, String password) throws IOException
+    {
+        super(host, port, username, password);
+    }
+
+    public InternalNodeProbe(String host, int port) throws IOException
+    {
+        super(host, port);
+    }
+
+    protected void connect() throws IOException
+    {
+        // note that we are not connecting via JMX for testing
+        mbeanServerConn = null;
+        jmxc = null;
+
+        ssProxy = StorageService.instance;
+        msProxy = MessagingService.instance();
+        streamProxy = StreamManager.instance;
+        compactionProxy = CompactionManager.instance;
+        fdProxy = (FailureDetectorMBean) FailureDetector.instance;
+        cacheService = CacheService.instance;
+        spProxy = StorageProxy.instance;
+        hhProxy = HintedHandOffManager.instance;
+        gcProxy = new GCInspector();
+        gossProxy = Gossiper.instance;
+        memProxy = ManagementFactory.getMemoryMXBean();
+        runtimeProxy = ManagementFactory.getRuntimeMXBean();
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        // nothing to close. no-op
+    }
+
+    @Override
+    // overrides all the methods referenced mbeanServerConn/jmxc in super
+    public EndpointSnitchInfoMBean getEndpointSnitchInfoProxy()
+    {
+        return new EndpointSnitchInfo();
+    }
+
+    @Override
+    public CacheServiceMBean getCacheServiceMBean()
+    {
+        return cacheService;
+    }
+
+    @Override
+    public void repairAsync(final PrintStream out, final String keyspace, Map<String, String> options) throws IOException
+    {
+        RepairRunner runner = new RepairRunner(out, ssProxy, keyspace, options);
+        try
+        {
+            ssProxy.addNotificationListener(runner, null, null);
+            runner.run();
+        }
+        catch (Exception e)
+        {
+            throw new IOException(e) ;
+        }
+        finally
+        {
+            try
+            {
+                ssProxy.removeNotificationListener(runner);
+            }
+            catch (Throwable e)
+            {
+                out.println(""Exception occurred during clean-up. "" + e);
+            }
+        }
+    }
+
+    @Override
+    // simply resume bootstrap, instead of streaming the JMX notification to stdout
+    public void resumeBootstrap(PrintStream out) throws IOException
+    {
+        if (ssProxy.resumeBootstrap())
+        {
+            out.println(""Resuming bootstrap"");
+        }
+        else
+        {
+            out.println(""Node is already bootstrapped."");
+        }
+    }
+
+    // dtest runs with disable_mbean_registration. No ColumnFamilyStoreMBean is registered, thus not supporting get the MBean.
+    @Override
+    public ColumnFamilyStoreMBean getCfsProxy(String ks, String cf)
+    {
+        return Keyspace.open(ks).getColumnFamilyStore(cf);
+    }
+
+    @Override
+    // The below methods are only used by the commands (i.e. Info, TableHistogram, TableStats, etc.) that display informations. Not useful for dtest, so disable it.
+    public Object getCacheMetric(String cacheType, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Iterator<Map.Entry<String, ColumnFamilyStoreMBean>> getColumnFamilyStoreMBeanProxies()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Multimap<String, String> getThreadPools()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Object getThreadPoolMetric(String pathName, String poolName, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Object getColumnFamilyMetric(String ks, String cf, String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public CassandraMetricsRegistry.JmxTimerMBean getProxyMetric(String scope)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public Object getCompactionMetric(String metricName)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long getStorageMetric(String metricName)
+    {
+        throw new UnsupportedOperationException();","[{'comment': 'FYI `org.apache.cassandra.metrics.StorageMetrics`. Though the special casing would be annoying.', 'commenter': 'dcapwell'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbeFactory.java,"@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.NodeProbeFactory;
+
+public class InternalNodeProbeFactory extends NodeProbeFactory
+{
+    public NodeProbe create(String host, int port) throws IOException {
+        return new InternalNodeProbe(host, port);
+    }
+
+    public NodeProbe create(String host, int port, String username, String password) throws IOException {","[{'comment': 'personal preference (aka your call)\r\n\r\n```\r\nreturn new InternalNodeProbe();\r\n```\r\n\r\nRather than have the dumby constructors in `InternalNodeProbe`', 'commenter': 'dcapwell'}]"
388,src/java/org/apache/cassandra/tools/NodeProbe.java,"@@ -1323,6 +1307,11 @@ public TabularData getFailureDetectorPhilValues()
             throw new RuntimeException(e);
         }
     }
+
+    public MessagingServiceMBean getMessagingServiceProxy()","[{'comment': 'looking at this again, you could remove this since the field is set properly', 'commenter': 'dcapwell'}, {'comment': 'Not sure if I got it. The method is used by another command. ', 'commenter': 'yifan-c'}]"
388,src/java/org/apache/cassandra/tools/NodeProbe.java,"@@ -313,7 +313,7 @@ public void repairAsync(final PrintStream out, final String keyspace, Map<String
         RepairRunner runner = new RepairRunner(out, ssProxy, keyspace, options);
         try
         {
-            jmxc.addConnectionNotificationListener(runner, null, null);
+            if (jmxc != null) jmxc.addConnectionNotificationListener(runner, null, null);","[{'comment': 'Nit: the body should appear on the next line.', 'commenter': 'dineshjoshi'}]"
388,src/java/org/apache/cassandra/tools/NodeProbe.java,"@@ -326,7 +326,7 @@ public void repairAsync(final PrintStream out, final String keyspace, Map<String
             try
             {
                 ssProxy.removeNotificationListener(runner);
-                jmxc.removeConnectionNotificationListener(runner);
+                if (jmxc != null) jmxc.removeConnectionNotificationListener(runner);","[{'comment': 'Nit: the body should appear on the next line.', 'commenter': 'dineshjoshi'}]"
388,src/java/org/apache/cassandra/tools/NodeProbeFactory.java,"@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.IOException;
+
+public class NodeProbeFactory
+{
+    public NodeProbe create(String host, int port) throws IOException
+    {
+        return new NodeProbe(host, port);
+    }
+
+    public NodeProbe create(String host, int port, String username, String password) throws IOException
+    {
+        return new NodeProbe(host, port, username, password);
+    }
+}","[{'comment': 'Nit: add new line.', 'commenter': 'dineshjoshi'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,155 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;","[{'comment': 'Please remove unused imports. Following unused imports exist:\r\n\r\n```\r\nimport java.io.PrintStream;\r\nimport java.util.concurrent.TimeUnit;\r\nimport javax.management.MalformedObjectNameException;\r\nimport org.apache.cassandra.db.ColumnFamilyStore;\r\nimport org.apache.cassandra.metrics.DefaultNameFactory;\r\nimport org.apache.cassandra.metrics.MetricNameFactory;\r\nimport org.apache.cassandra.tools.RepairRunner;\r\n```', 'commenter': 'dineshjoshi'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,155 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    public InternalNodeProbe() throws IOException
+    {
+        super("""", 0);","[{'comment': 'Is this acceptable? I think we should always have a valid host & port for a `NodeProbe`.', 'commenter': 'dineshjoshi'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbe.java,"@@ -0,0 +1,155 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.management.ManagementFactory;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import javax.management.MalformedObjectNameException;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ColumnFamilyStoreMBean;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.FailureDetectorMBean;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.locator.EndpointSnitchInfo;
+import org.apache.cassandra.locator.EndpointSnitchInfoMBean;
+import org.apache.cassandra.metrics.CassandraMetricsRegistry;
+import org.apache.cassandra.metrics.DefaultNameFactory;
+import org.apache.cassandra.metrics.MetricNameFactory;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.CacheService;
+import org.apache.cassandra.service.CacheServiceMBean;
+import org.apache.cassandra.service.GCInspector;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.StreamManager;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.RepairRunner;
+
+public class InternalNodeProbe extends NodeProbe
+{
+    public InternalNodeProbe() throws IOException
+    {
+        super("""", 0);
+    }
+
+    protected void connect() throws IOException","[{'comment': ""Don't think any method is going to throw an `IOException`. We can get rid of this."", 'commenter': 'dineshjoshi'}]"
388,test/distributed/org/apache/cassandra/distributed/mock/nodetool/InternalNodeProbeFactory.java,"@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.mock.nodetool;
+
+import java.io.IOException;
+
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.tools.NodeProbeFactory;
+
+public class InternalNodeProbeFactory extends NodeProbeFactory","[{'comment': ""Why use inheritance? I would suggest creating a factory interface and have two implementations (Mock and Real). You're not really sharing anything anyway."", 'commenter': 'dineshjoshi'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL","[{'comment': 'It does not expose JMX data, can just list expose metrics through CQL', 'commenter': 'clohfink'}, {'comment': 'Removed JMX', 'commenter': 'Deepak-Vohra'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data","[{'comment': 'like above mbeans/jmx things are not exposed. The theory is eventually this can replace JMX but it currently does not act as a proxy for JMX.', 'commenter': 'clohfink'}, {'comment': 'Removed mbeans/jmx', 'commenter': 'Deepak-Vohra'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only ","[{'comment': 'Virtual tables are not read-only, although I think all the ones currently implemented are that will change very soon so best not to put that in docs', 'commenter': 'clohfink'}, {'comment': 'Rather than list that is not currently supported, I have added:\r\n\r\n- Virtual tables are read-only, but it is likely to change', 'commenter': 'Deepak-Vohra'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:","[{'comment': 'Thats due to a bug in the driver that is fixed upstream and will soon be resolved and in trunk, DESCRIBE works on both virtual keyspaces and virtual tables - its just broken in trunk due to using older driver see https://issues.apache.org/jira/browse/CASSANDRA-14872', 'commenter': 'clohfink'}, {'comment': 'Removed statement about DESCRIBE.', 'commenter': 'Deepak-Vohra'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements","[{'comment': 'this isnt true, just the existing ones dont support it. Each table can choose to support different features really in terms of what kind of SELECT and INSERT/UPDATE/DELETE they handle (ie expiring cells and TTL isnt explicitly forbidden, just nothing _currently_ existing supports it since we going out initially with read only metrics essentially', 'commenter': 'clohfink'}, {'comment': 'I have added earlier that some of these are ""likely to change"". Best to keep the detail as currently supported. And when any of these change the page could be updated.', 'commenter': 'Deepak-Vohra'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces","[{'comment': 'kinda covered with unable to create functions and maybe mis understood as not able to run aggregate functions on select which it does allow', 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables","[{'comment': 'I think you can put the above 4 in a single line like ""cannot alter or drop virtual keyspace and tables""', 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:","[{'comment': 'remove `however`', 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+
+Table 1 : Virtual Tables in system_views
+
++------------------+---------------------------------------------------+
+|Virtual Table     | Description                                       | 
++------------------+---------------------------------------------------+
+| clients          |Lists information about all connected clients.     |           
++------------------+---------------------------------------------------+
+| disk_usage       |Disk usage including disk_space, keyspace_name,    |
+|                  |and table_name by system keyspaces.                |
++------------------+---------------------------------------------------+
+| local_writes     |A table metric for local writes                    |
+|                  |including count, keyspace_name,                    | 
+|                  |max, median, per_second, and                       |
+|                  |table_name.                                        |                                                                         
++------------------+---------------------------------------------------+
+| caches           |Displays the general cache information including   |
+|                  |cache name, capacity_bytes, entry_count, hit_count,| 
+|                  |hit_ratio double, recent_hit_rate_per_second,      |
+|                  |recent_request_rate_per_second, request_count, and | 
+|                  |size_bytes.                                        |                                                                         
++------------------+---------------------------------------------------+
+| local_reads      |A table metric for  local reads information.       |                                                                   
++------------------+---------------------------------------------------+
+| sstable_tasks    |Lists currently running tasks such as compactions  |
+|                  |and upgrades on SSTables.                          |
++------------------+---------------------------------------------------+
+|internode_inbound |Lists information about the inbound                | 
+|                  |internode messaging.                               |                
++------------------+---------------------------------------------------+
+| thread_pools     |Lists metrics for each thread pool.                |                                                                        
++------------------+---------------------------------------------------+
+| settings         |Displays configuration settings in cassandra.yaml. |                                                                         
++------------------+---------------------------------------------------+
+|max_partition_size|A table metric for maximum partition size.         |                                                                       
++------------------+---------------------------------------------------+
+|internode_outbound|Information about the outbound internode messaging.|
+|                  |                                                   |                                          
++------------------+---------------------------------------------------+
+ 
+We shall discuss some of the virtual tables in more detail next.","[{'comment': ""I don't think this sentence is necessary"", 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+
+Table 1 : Virtual Tables in system_views
+
++------------------+---------------------------------------------------+
+|Virtual Table     | Description                                       | 
++------------------+---------------------------------------------------+
+| clients          |Lists information about all connected clients.     |           
++------------------+---------------------------------------------------+
+| disk_usage       |Disk usage including disk_space, keyspace_name,    |
+|                  |and table_name by system keyspaces.                |
++------------------+---------------------------------------------------+
+| local_writes     |A table metric for local writes                    |
+|                  |including count, keyspace_name,                    | 
+|                  |max, median, per_second, and                       |
+|                  |table_name.                                        |                                                                         
++------------------+---------------------------------------------------+
+| caches           |Displays the general cache information including   |
+|                  |cache name, capacity_bytes, entry_count, hit_count,| 
+|                  |hit_ratio double, recent_hit_rate_per_second,      |
+|                  |recent_request_rate_per_second, request_count, and | 
+|                  |size_bytes.                                        |                                                                         
++------------------+---------------------------------------------------+
+| local_reads      |A table metric for  local reads information.       |                                                                   
++------------------+---------------------------------------------------+
+| sstable_tasks    |Lists currently running tasks such as compactions  |
+|                  |and upgrades on SSTables.                          |
++------------------+---------------------------------------------------+
+|internode_inbound |Lists information about the inbound                | 
+|                  |internode messaging.                               |                
++------------------+---------------------------------------------------+
+| thread_pools     |Lists metrics for each thread pool.                |                                                                        
++------------------+---------------------------------------------------+
+| settings         |Displays configuration settings in cassandra.yaml. |                                                                         
++------------------+---------------------------------------------------+
+|max_partition_size|A table metric for maximum partition size.         |                                                                       
++------------------+---------------------------------------------------+
+|internode_outbound|Information about the outbound internode messaging.|
+|                  |                                                   |                                          
++------------------+---------------------------------------------------+
+ 
+We shall discuss some of the virtual tables in more detail next.
+
+Clients Virtual Table
+*********************
+
+The ``clients`` virtual table lists all active connections (connected clients) including their ip address, port, connection stage, driver name, driver version, hostname, protocol version, request count, ssl enabled, ssl protocol and user name. A query on the ``clients`` table returns the following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.clients;
+  address   | port  | connection_stage | driver_name | driver_version | hostname  | protocol_version | request_count | ssl_cipher_suite | ssl_enabled | ssl_protocol | username
+ -----------+-------+------------------+-------------+----------------+-----------+------------------+---------------+------------------+-------------+--------------+-----------
+  127.0.0.1 | 50628 |            ready |        null |           null | localhost |                4 |            55 |             null |       False |         null | anonymous
+  127.0.0.1 | 50630 |            ready |        null |           null | localhost |                4 |            70 |             null |       False |         null | anonymous
+
+ (2 rows)
+
+The virtual tables may be described with ``DESCRIBE`` statement. The DDL listed however cannot be run to create a virtual table. As an example describe the ``system_views.clients`` virtual table.
+
+::
+
+  cqlsh:system_views> DESC TABLE system_views.clients;
+ CREATE TABLE system_views.clients (
+    address inet,
+    connection_stage text,
+    driver_name text,
+    driver_version text,
+    hostname text,
+    port int,
+    protocol_version int,
+    request_count bigint,
+    ssl_cipher_suite text,
+    ssl_enabled boolean,
+    ssl_protocol text,
+    username text,
+    PRIMARY KEY (address, port)) WITH CLUSTERING ORDER BY (port ASC)
+    AND compaction = {'class': 'None'}
+    AND compression = {};","[{'comment': 'We can leave these in here for now but with the driver fixes this changes quite a bit to be a commented out block with less of the other table options. If that gets in 4.0 GA we should circle around and replace this', 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+
+Table 1 : Virtual Tables in system_views
+
++------------------+---------------------------------------------------+
+|Virtual Table     | Description                                       | 
++------------------+---------------------------------------------------+
+| clients          |Lists information about all connected clients.     |           
++------------------+---------------------------------------------------+
+| disk_usage       |Disk usage including disk_space, keyspace_name,    |
+|                  |and table_name by system keyspaces.                |
++------------------+---------------------------------------------------+
+| local_writes     |A table metric for local writes                    |
+|                  |including count, keyspace_name,                    | 
+|                  |max, median, per_second, and                       |
+|                  |table_name.                                        |                                                                         
++------------------+---------------------------------------------------+
+| caches           |Displays the general cache information including   |
+|                  |cache name, capacity_bytes, entry_count, hit_count,| 
+|                  |hit_ratio double, recent_hit_rate_per_second,      |
+|                  |recent_request_rate_per_second, request_count, and | 
+|                  |size_bytes.                                        |                                                                         
++------------------+---------------------------------------------------+
+| local_reads      |A table metric for  local reads information.       |                                                                   
++------------------+---------------------------------------------------+
+| sstable_tasks    |Lists currently running tasks such as compactions  |
+|                  |and upgrades on SSTables.                          |
++------------------+---------------------------------------------------+
+|internode_inbound |Lists information about the inbound                | 
+|                  |internode messaging.                               |                
++------------------+---------------------------------------------------+
+| thread_pools     |Lists metrics for each thread pool.                |                                                                        
++------------------+---------------------------------------------------+
+| settings         |Displays configuration settings in cassandra.yaml. |                                                                         
++------------------+---------------------------------------------------+
+|max_partition_size|A table metric for maximum partition size.         |                                                                       
++------------------+---------------------------------------------------+
+|internode_outbound|Information about the outbound internode messaging.|
+|                  |                                                   |                                          
++------------------+---------------------------------------------------+
+ 
+We shall discuss some of the virtual tables in more detail next.
+
+Clients Virtual Table
+*********************
+
+The ``clients`` virtual table lists all active connections (connected clients) including their ip address, port, connection stage, driver name, driver version, hostname, protocol version, request count, ssl enabled, ssl protocol and user name. A query on the ``clients`` table returns the following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.clients;
+  address   | port  | connection_stage | driver_name | driver_version | hostname  | protocol_version | request_count | ssl_cipher_suite | ssl_enabled | ssl_protocol | username
+ -----------+-------+------------------+-------------+----------------+-----------+------------------+---------------+------------------+-------------+--------------+-----------
+  127.0.0.1 | 50628 |            ready |        null |           null | localhost |                4 |            55 |             null |       False |         null | anonymous
+  127.0.0.1 | 50630 |            ready |        null |           null | localhost |                4 |            70 |             null |       False |         null | anonymous
+
+ (2 rows)
+
+The virtual tables may be described with ``DESCRIBE`` statement. The DDL listed however cannot be run to create a virtual table. As an example describe the ``system_views.clients`` virtual table.
+
+::
+
+  cqlsh:system_views> DESC TABLE system_views.clients;
+ CREATE TABLE system_views.clients (
+    address inet,
+    connection_stage text,
+    driver_name text,
+    driver_version text,
+    hostname text,
+    port int,
+    protocol_version int,
+    request_count bigint,
+    ssl_cipher_suite text,
+    ssl_enabled boolean,
+    ssl_protocol text,
+    username text,
+    PRIMARY KEY (address, port)) WITH CLUSTERING ORDER BY (port ASC)
+    AND compaction = {'class': 'None'}
+    AND compression = {};
+
+Caches Virtual Table
+********************
+The ``caches`` virtual table lists information about the  caches. The four caches presently created are chunks, counters, keys and rows. A query on the ``caches`` virtual table returns the following details.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.caches;
+ name     | capacity_bytes | entry_count | hit_count | hit_ratio | recent_hit_rate_per_second | recent_request_rate_per_second | request_count | size_bytes
+ ---------+----------------+-------------+-----------+-----------+----------------------------+--------------------------------+---------------+------------
+   chunks |      229638144 |          29 |       166 |      0.83 |                          5 |                              6 |           200 |     475136
+ counters |       26214400 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+     keys |       52428800 |          14 |       124 |  0.873239 |                          4 |                              4 |           142 |       1248
+     rows |              0 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+
+ (4 rows)
+
+Settings Virtual Table
+**********************
+The ``settings table`` is rather useful and lists all the configuration settings from the ``cassandra.yaml``.  The encryption options are overridden to hide the sensitive truststore information or passwords.  The configuration settings however cannot be set using DML  on the 
+virtual table presently. A total of 224 settings get listed presently. ","[{'comment': 'I wouldnt include number of settings since that can change constantly and will certainly be out of sync before 4.0 even.\r\n\r\nMay want to include that its ""all current configuration"". As configs can be changed at runtime (ie using nodetool setcompactionthroughput) which will be reflected here', 'commenter': 'clohfink'}, {'comment': 'This can be really useful if yaml file has been changed since startup and dont know running configuration or if concerned that they have been modified via jmx/nodetool or virtual tables.', 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+
+Table 1 : Virtual Tables in system_views
+
++------------------+---------------------------------------------------+
+|Virtual Table     | Description                                       | 
++------------------+---------------------------------------------------+
+| clients          |Lists information about all connected clients.     |           
++------------------+---------------------------------------------------+
+| disk_usage       |Disk usage including disk_space, keyspace_name,    |
+|                  |and table_name by system keyspaces.                |
++------------------+---------------------------------------------------+
+| local_writes     |A table metric for local writes                    |
+|                  |including count, keyspace_name,                    | 
+|                  |max, median, per_second, and                       |
+|                  |table_name.                                        |                                                                         
++------------------+---------------------------------------------------+
+| caches           |Displays the general cache information including   |
+|                  |cache name, capacity_bytes, entry_count, hit_count,| 
+|                  |hit_ratio double, recent_hit_rate_per_second,      |
+|                  |recent_request_rate_per_second, request_count, and | 
+|                  |size_bytes.                                        |                                                                         
++------------------+---------------------------------------------------+
+| local_reads      |A table metric for  local reads information.       |                                                                   
++------------------+---------------------------------------------------+
+| sstable_tasks    |Lists currently running tasks such as compactions  |
+|                  |and upgrades on SSTables.                          |
++------------------+---------------------------------------------------+
+|internode_inbound |Lists information about the inbound                | 
+|                  |internode messaging.                               |                
++------------------+---------------------------------------------------+
+| thread_pools     |Lists metrics for each thread pool.                |                                                                        
++------------------+---------------------------------------------------+
+| settings         |Displays configuration settings in cassandra.yaml. |                                                                         
++------------------+---------------------------------------------------+
+|max_partition_size|A table metric for maximum partition size.         |                                                                       
++------------------+---------------------------------------------------+
+|internode_outbound|Information about the outbound internode messaging.|
+|                  |                                                   |                                          
++------------------+---------------------------------------------------+
+ 
+We shall discuss some of the virtual tables in more detail next.
+
+Clients Virtual Table
+*********************
+
+The ``clients`` virtual table lists all active connections (connected clients) including their ip address, port, connection stage, driver name, driver version, hostname, protocol version, request count, ssl enabled, ssl protocol and user name. A query on the ``clients`` table returns the following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.clients;
+  address   | port  | connection_stage | driver_name | driver_version | hostname  | protocol_version | request_count | ssl_cipher_suite | ssl_enabled | ssl_protocol | username
+ -----------+-------+------------------+-------------+----------------+-----------+------------------+---------------+------------------+-------------+--------------+-----------
+  127.0.0.1 | 50628 |            ready |        null |           null | localhost |                4 |            55 |             null |       False |         null | anonymous
+  127.0.0.1 | 50630 |            ready |        null |           null | localhost |                4 |            70 |             null |       False |         null | anonymous
+
+ (2 rows)
+
+The virtual tables may be described with ``DESCRIBE`` statement. The DDL listed however cannot be run to create a virtual table. As an example describe the ``system_views.clients`` virtual table.
+
+::
+
+  cqlsh:system_views> DESC TABLE system_views.clients;
+ CREATE TABLE system_views.clients (
+    address inet,
+    connection_stage text,
+    driver_name text,
+    driver_version text,
+    hostname text,
+    port int,
+    protocol_version int,
+    request_count bigint,
+    ssl_cipher_suite text,
+    ssl_enabled boolean,
+    ssl_protocol text,
+    username text,
+    PRIMARY KEY (address, port)) WITH CLUSTERING ORDER BY (port ASC)
+    AND compaction = {'class': 'None'}
+    AND compression = {};
+
+Caches Virtual Table
+********************
+The ``caches`` virtual table lists information about the  caches. The four caches presently created are chunks, counters, keys and rows. A query on the ``caches`` virtual table returns the following details.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.caches;
+ name     | capacity_bytes | entry_count | hit_count | hit_ratio | recent_hit_rate_per_second | recent_request_rate_per_second | request_count | size_bytes
+ ---------+----------------+-------------+-----------+-----------+----------------------------+--------------------------------+---------------+------------
+   chunks |      229638144 |          29 |       166 |      0.83 |                          5 |                              6 |           200 |     475136
+ counters |       26214400 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+     keys |       52428800 |          14 |       124 |  0.873239 |                          4 |                              4 |           142 |       1248
+     rows |              0 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+
+ (4 rows)
+
+Settings Virtual Table
+**********************
+The ``settings table`` is rather useful and lists all the configuration settings from the ``cassandra.yaml``.  The encryption options are overridden to hide the sensitive truststore information or passwords.  The configuration settings however cannot be set using DML  on the 
+virtual table presently. A total of 224 settings get listed presently. 
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.settings;
+
+ name                                                                   | value
+ -----------------------------------------------------------------------+-------------------- 
+   allocate_tokens_for_keyspace                                         |        null                                                                                          
+   audit_logging_options_audit_logs_dir                                 |  
+ /home/ec2-user/cassandra/logs/audit/                                                                                                       
+                                          audit_logging_options_enabled |                                                                                                                                         
+ false
+                              audit_logging_options_excluded_categories |                                                                                                                                              
+                               audit_logging_options_excluded_keyspaces |                                                                                                    
+ system,system_schema,system_virtual_schema
+                                   audit_logging_options_excluded_users |                                                                                                                                              
+                              audit_logging_options_included_categories |                                                                                                                                              
+                               audit_logging_options_included_keyspaces |                                                                                                                                              
+                                   audit_logging_options_included_users |                                                                                                                                              
+                                           audit_logging_options_logger |                                                                                                                                
+ BinAuditLogger
+                                                          authenticator |                                                                                                                         
+ AllowAllAuthenticator
+                                                             authorizer |                                                                                                                            
+ AllowAllAuthorizer
+                                                         auto_bootstrap |                                                                                                                                          
+ true
+                                                          auto_snapshot |                                                                                                                                          
+ true
+                                              automatic_sstable_upgrade |                                                                                                                                         
+ false
+                                                  back_pressure_enabled |                                                                                                                                         
+ false
+                                                 back_pressure_strategy |                                                           
+ org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}
+                                                      broadcast_address |                                                                                                                                          
+ null
+                                                  broadcast_rpc_address |                                                                                                                                          
+ null
+                                                           cluster_name |                                                                                                                                  
+ Test Cluster
+                                          column_index_cache_size_in_kb |                                                                                                                                             
+ 2
+                                                column_index_size_in_kb |                                                                                                                                            
+ 64
+                                                  commit_failure_policy |                                                                                                                                          
+ stop
+                                                  commitlog_compression |                                                                                                                                          
+ null
+                                                    commitlog_directory |                                                                                                       
+ /home/ec2-user/cassandra/data/commitlog
+                              commitlog_max_compression_buffers_in_pool |                                                                                                                                             
+ 3
+                                          commitlog_periodic_queue_size |                                                                                                                                            
+ -1
+                                           commitlog_segment_size_in_mb |                                                                                                                                            
+ 32
+                                                         commitlog_sync |                                                                                                                                      
+ periodic
+                                      commitlog_sync_batch_window_in_ms |                                                                                                                                           
+ NaN
+                                      commitlog_sync_group_window_in_ms |                                                                                                                                           
+ NaN
+                                            commitlog_sync_period_in_ms |                                                                                                                                         
+ 10000
+                                            commitlog_total_space_in_mb |                                                                                                                                          
+ 2556
+                        compaction_large_partition_warning_threshold_mb |                                                                                                                                           
+ 100
+                                       compaction_throughput_mb_per_sec |                                                                                                                                            
+ 16
+                                                  concurrent_compactors |                                                                                                                                             
+ 2
+                                              concurrent_counter_writes |                                                                                                                                            
+ 32
+                                  concurrent_materialized_view_builders |                                                                                                                                             
+ 1
+                                    concurrent_materialized_view_writes |                                                                                                                                            
+ 32
+                                                       concurrent_reads |                                                                                                                                            
+ 32
+                                                  concurrent_replicates |                                                                                                                                          
+ null
+                                                 concurrent_validations |                                                                                                                                    
+ 2147483647
+                                                      concurrent_writes |                                                                                                                                            
+ 32
+                                      credentials_update_interval_in_ms |                                                                                                                                            
+ -1
+                                             credentials_validity_in_ms |                                                                                                                                          
+ 2000
+                                                     cross_node_timeout |                                                                                                                                         
+ false
+                                                  data_file_directories |                                                                                                          
+ [/home/ec2-user/cassandra/data/data]
+                                              diagnostic_events_enabled |                                                                                                                                         
+ false
+                                                       disk_access_mode |                                                                                                                                          
+ mmap
+                                                    disk_failure_policy |                                                                                                                                          
+ stop
+                                              enable_materialized_views |                                                                                                                                         
+ false
+                                                    enable_sasi_indexes |                                                                                                                                         
+ false
+                                 enable_scripted_user_defined_functions |                                                                                                                                         
+ false
+                                           enable_transient_replication |                                                                                                                                         
+ false
+                                          enable_user_defined_functions |                                                                                                                                         
+ false
+                                  enable_user_defined_functions_threads |                                                                                                                                          
+ true
+                                                        endpoint_snitch |                                                                                                                                  
+ SimpleSnitch
+                                                    file_cache_round_up |                                                                                                                                         
+ false
+                                                  file_cache_size_in_mb |                                                                                                                                           
+ 251
+                                             full_query_logging_options | 
+ FullQueryLoggerOptions{log_dir='', archive_command='', roll_cycle='HOURLY', block=true, 
+ max_queue_weight=268435456, max_log_size=17179869184}
+                                                 gc_log_threshold_in_ms |                                                                                                                                           
+ 200
+                                                gc_warn_threshold_in_ms |                                                                                                                                          
+ 1000
+                                    hinted_handoff_disabled_datacenters |                                                                                                                                            
+ []
+                                                 hinted_handoff_enabled |                                                                                                                                          
+ true
+                                          hinted_handoff_throttle_in_kb |                                                                                                                                          
+ 1024
+                                                      hints_compression |                                                                                                                                          
+ null
+                                                        hints_directory |                                                                                                           
+ /home/ec2-user/cassandra/data/hints
+                                               hints_flush_period_in_ms |                                                                                                                                         
+ 10000
+                                                ideal_consistency_level |                                                                                                                                          
+ null
+                                                    incremental_backups |                                                                                                                                         
+ false
+                                                          initial_token |                                                                                                                                          
+ null
+                   inter_dc_stream_throughput_outbound_megabits_per_sec |                                                                                                                                           
+ 200
+                                                   inter_dc_tcp_nodelay |                                                                                                                                         
+ false
+                  internode_application_receive_queue_capacity_in_bytes |                                                                                                                                       
+ 4194304
+  internode_application_receive_queue_reserve_endpoint_capacity_in_bytes |                                                                                                                                     
+ 134217728
+  internode_application_receive_queue_reserve_global_capacity_in_bytes |                                                                                                                                     
+ 536870912
+                  internode_application_send_queue_capacity_in_bytes |                                                                                                                                       
+ 4194304
+ internode_application_send_queue_reserve_endpoint_capacity_in_bytes |                                                                                                                                     
+  134217728
+
+ 
+  internode_application_send_queue_reserve_global_capacity_in_bytes |                                                             
+  536870912
+                                           internode_authenticator |                                                                  
+ null
+                                             internode_compression |                                                                    
+ dc
+                                                    listen_address |                                                             
+ localhost
+                                                  listen_interface |                                                                  
+ null
+                                       listen_on_broadcast_address |                                                                 
+ false
+                         max_concurrent_automatic_sstable_upgrades |                                                                     
+ 1
+                                             max_hint_window_in_ms |                                                              
+ 10800000
+                                          memtable_allocation_type |                                                          
+ heap_buffers
+                                    min_free_space_per_drive_in_mb |                                                                    
+ 50
+                            native_transport_allow_older_protocols |                                                                  
+ true
+                          native_transport_flush_in_batches_legacy |                                                                 
+ false
+                           native_transport_frame_block_size_in_kb |                                                                    32
+                               native_transport_idle_timeout_in_ms |                                                                     0
+                       native_transport_max_concurrent_connections |                                                                    -1
+                native_transport_max_concurrent_connections_per_ip |                                                                    -1
+                 native_transport_max_concurrent_requests_in_bytes |                                                             105277030
+          native_transport_max_concurrent_requests_in_bytes_per_ip |                                                              26319257
+                             native_transport_max_frame_size_in_mb |                                                                   256
+                                      native_transport_max_threads |                                                                   128
+                                             native_transport_port |                                                                  9042
+                                         native_transport_port_ssl |                                                                  null
+                                                network_authorizer |                                             AllowAllNetworkAuthorizer
+                                                        num_tokens |                                                                   256
+                                                       partitioner |                           org.apache.cassandra.dht.Murmur3Partitioner
+                                 prepared_statements_cache_size_mb |                                                                  null
+                                       range_request_timeout_in_ms |                                                                 10000
+                                        read_request_timeout_in_ms |                                                                  5000
+                                 repair_command_pool_full_strategy |                                                                 queue
+                                          repair_command_pool_size |                                                            2147483647
+                                     repair_session_max_tree_depth |                                                                    20
+                                        repair_session_space_in_mb |                                                                    62
+                repaired_data_tracking_for_partition_reads_enabled |                                                                 false
+                    repaired_data_tracking_for_range_reads_enabled |                                                                 false
+                       report_unconfirmed_repaired_data_mismatches |                                                                 false
+                                             request_timeout_in_ms |                                                                 10000
+                                                      role_manager |                                                  CassandraRoleManager
+                                                       rpc_address |                                                             localhost
+                                                     rpc_interface |                                                                  null
+                                            saved_caches_directory |                            /home/ec2-user/cassandra/data/saved_caches
+                                                     seed_provider | org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1:7000}
+                               server_encryption_options_algorithm |                                                                  null
+                                server_encryption_options_protocol |                                                                   TLS
+                                      slow_query_log_timeout_in_ms |                                                                   500
+                                                  ssl_storage_port |                                                                  7001
+                                            start_native_transport |                                                                  true
+                                                      storage_port |                                                                  7000
+                                            stream_entire_sstables |                                                                  true
+               transparent_data_encryption_options_chunk_length_kb |                                                                    64
+                        transparent_data_encryption_options_cipher |                                                  AES/CBC/PKCS5Padding
+                       transparent_data_encryption_options_enabled |                                                                 false
+                     transparent_data_encryption_options_iv_length |                                                                    
+ 16
+ (224 rows)","[{'comment': 'maybe add a ``...`` in the middle here and not list the complete set. Good chance to snip things like internode_application_receive_queue_reserve_global_capacity_in_bytes and full_query_logging_options that make the table so wide', 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+
+Table 1 : Virtual Tables in system_views
+
++------------------+---------------------------------------------------+
+|Virtual Table     | Description                                       | 
++------------------+---------------------------------------------------+
+| clients          |Lists information about all connected clients.     |           
++------------------+---------------------------------------------------+
+| disk_usage       |Disk usage including disk_space, keyspace_name,    |
+|                  |and table_name by system keyspaces.                |
++------------------+---------------------------------------------------+
+| local_writes     |A table metric for local writes                    |
+|                  |including count, keyspace_name,                    | 
+|                  |max, median, per_second, and                       |
+|                  |table_name.                                        |                                                                         
++------------------+---------------------------------------------------+
+| caches           |Displays the general cache information including   |
+|                  |cache name, capacity_bytes, entry_count, hit_count,| 
+|                  |hit_ratio double, recent_hit_rate_per_second,      |
+|                  |recent_request_rate_per_second, request_count, and | 
+|                  |size_bytes.                                        |                                                                         
++------------------+---------------------------------------------------+
+| local_reads      |A table metric for  local reads information.       |                                                                   
++------------------+---------------------------------------------------+
+| sstable_tasks    |Lists currently running tasks such as compactions  |
+|                  |and upgrades on SSTables.                          |
++------------------+---------------------------------------------------+
+|internode_inbound |Lists information about the inbound                | 
+|                  |internode messaging.                               |                
++------------------+---------------------------------------------------+
+| thread_pools     |Lists metrics for each thread pool.                |                                                                        
++------------------+---------------------------------------------------+
+| settings         |Displays configuration settings in cassandra.yaml. |                                                                         
++------------------+---------------------------------------------------+
+|max_partition_size|A table metric for maximum partition size.         |                                                                       
++------------------+---------------------------------------------------+
+|internode_outbound|Information about the outbound internode messaging.|
+|                  |                                                   |                                          
++------------------+---------------------------------------------------+
+ 
+We shall discuss some of the virtual tables in more detail next.
+
+Clients Virtual Table
+*********************
+
+The ``clients`` virtual table lists all active connections (connected clients) including their ip address, port, connection stage, driver name, driver version, hostname, protocol version, request count, ssl enabled, ssl protocol and user name. A query on the ``clients`` table returns the following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.clients;
+  address   | port  | connection_stage | driver_name | driver_version | hostname  | protocol_version | request_count | ssl_cipher_suite | ssl_enabled | ssl_protocol | username
+ -----------+-------+------------------+-------------+----------------+-----------+------------------+---------------+------------------+-------------+--------------+-----------
+  127.0.0.1 | 50628 |            ready |        null |           null | localhost |                4 |            55 |             null |       False |         null | anonymous
+  127.0.0.1 | 50630 |            ready |        null |           null | localhost |                4 |            70 |             null |       False |         null | anonymous
+
+ (2 rows)
+
+The virtual tables may be described with ``DESCRIBE`` statement. The DDL listed however cannot be run to create a virtual table. As an example describe the ``system_views.clients`` virtual table.
+
+::
+
+  cqlsh:system_views> DESC TABLE system_views.clients;
+ CREATE TABLE system_views.clients (
+    address inet,
+    connection_stage text,
+    driver_name text,
+    driver_version text,
+    hostname text,
+    port int,
+    protocol_version int,
+    request_count bigint,
+    ssl_cipher_suite text,
+    ssl_enabled boolean,
+    ssl_protocol text,
+    username text,
+    PRIMARY KEY (address, port)) WITH CLUSTERING ORDER BY (port ASC)
+    AND compaction = {'class': 'None'}
+    AND compression = {};
+
+Caches Virtual Table
+********************
+The ``caches`` virtual table lists information about the  caches. The four caches presently created are chunks, counters, keys and rows. A query on the ``caches`` virtual table returns the following details.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.caches;
+ name     | capacity_bytes | entry_count | hit_count | hit_ratio | recent_hit_rate_per_second | recent_request_rate_per_second | request_count | size_bytes
+ ---------+----------------+-------------+-----------+-----------+----------------------------+--------------------------------+---------------+------------
+   chunks |      229638144 |          29 |       166 |      0.83 |                          5 |                              6 |           200 |     475136
+ counters |       26214400 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+     keys |       52428800 |          14 |       124 |  0.873239 |                          4 |                              4 |           142 |       1248
+     rows |              0 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+
+ (4 rows)
+
+Settings Virtual Table
+**********************
+The ``settings table`` is rather useful and lists all the configuration settings from the ``cassandra.yaml``.  The encryption options are overridden to hide the sensitive truststore information or passwords.  The configuration settings however cannot be set using DML  on the 
+virtual table presently. A total of 224 settings get listed presently. 
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.settings;
+
+ name                                                                   | value
+ -----------------------------------------------------------------------+-------------------- 
+   allocate_tokens_for_keyspace                                         |        null                                                                                          
+   audit_logging_options_audit_logs_dir                                 |  
+ /home/ec2-user/cassandra/logs/audit/                                                                                                       
+                                          audit_logging_options_enabled |                                                                                                                                         
+ false
+                              audit_logging_options_excluded_categories |                                                                                                                                              
+                               audit_logging_options_excluded_keyspaces |                                                                                                    
+ system,system_schema,system_virtual_schema
+                                   audit_logging_options_excluded_users |                                                                                                                                              
+                              audit_logging_options_included_categories |                                                                                                                                              
+                               audit_logging_options_included_keyspaces |                                                                                                                                              
+                                   audit_logging_options_included_users |                                                                                                                                              
+                                           audit_logging_options_logger |                                                                                                                                
+ BinAuditLogger
+                                                          authenticator |                                                                                                                         
+ AllowAllAuthenticator
+                                                             authorizer |                                                                                                                            
+ AllowAllAuthorizer
+                                                         auto_bootstrap |                                                                                                                                          
+ true
+                                                          auto_snapshot |                                                                                                                                          
+ true
+                                              automatic_sstable_upgrade |                                                                                                                                         
+ false
+                                                  back_pressure_enabled |                                                                                                                                         
+ false
+                                                 back_pressure_strategy |                                                           
+ org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}
+                                                      broadcast_address |                                                                                                                                          
+ null
+                                                  broadcast_rpc_address |                                                                                                                                          
+ null
+                                                           cluster_name |                                                                                                                                  
+ Test Cluster
+                                          column_index_cache_size_in_kb |                                                                                                                                             
+ 2
+                                                column_index_size_in_kb |                                                                                                                                            
+ 64
+                                                  commit_failure_policy |                                                                                                                                          
+ stop
+                                                  commitlog_compression |                                                                                                                                          
+ null
+                                                    commitlog_directory |                                                                                                       
+ /home/ec2-user/cassandra/data/commitlog
+                              commitlog_max_compression_buffers_in_pool |                                                                                                                                             
+ 3
+                                          commitlog_periodic_queue_size |                                                                                                                                            
+ -1
+                                           commitlog_segment_size_in_mb |                                                                                                                                            
+ 32
+                                                         commitlog_sync |                                                                                                                                      
+ periodic
+                                      commitlog_sync_batch_window_in_ms |                                                                                                                                           
+ NaN
+                                      commitlog_sync_group_window_in_ms |                                                                                                                                           
+ NaN
+                                            commitlog_sync_period_in_ms |                                                                                                                                         
+ 10000
+                                            commitlog_total_space_in_mb |                                                                                                                                          
+ 2556
+                        compaction_large_partition_warning_threshold_mb |                                                                                                                                           
+ 100
+                                       compaction_throughput_mb_per_sec |                                                                                                                                            
+ 16
+                                                  concurrent_compactors |                                                                                                                                             
+ 2
+                                              concurrent_counter_writes |                                                                                                                                            
+ 32
+                                  concurrent_materialized_view_builders |                                                                                                                                             
+ 1
+                                    concurrent_materialized_view_writes |                                                                                                                                            
+ 32
+                                                       concurrent_reads |                                                                                                                                            
+ 32
+                                                  concurrent_replicates |                                                                                                                                          
+ null
+                                                 concurrent_validations |                                                                                                                                    
+ 2147483647
+                                                      concurrent_writes |                                                                                                                                            
+ 32
+                                      credentials_update_interval_in_ms |                                                                                                                                            
+ -1
+                                             credentials_validity_in_ms |                                                                                                                                          
+ 2000
+                                                     cross_node_timeout |                                                                                                                                         
+ false
+                                                  data_file_directories |                                                                                                          
+ [/home/ec2-user/cassandra/data/data]
+                                              diagnostic_events_enabled |                                                                                                                                         
+ false
+                                                       disk_access_mode |                                                                                                                                          
+ mmap
+                                                    disk_failure_policy |                                                                                                                                          
+ stop
+                                              enable_materialized_views |                                                                                                                                         
+ false
+                                                    enable_sasi_indexes |                                                                                                                                         
+ false
+                                 enable_scripted_user_defined_functions |                                                                                                                                         
+ false
+                                           enable_transient_replication |                                                                                                                                         
+ false
+                                          enable_user_defined_functions |                                                                                                                                         
+ false
+                                  enable_user_defined_functions_threads |                                                                                                                                          
+ true
+                                                        endpoint_snitch |                                                                                                                                  
+ SimpleSnitch
+                                                    file_cache_round_up |                                                                                                                                         
+ false
+                                                  file_cache_size_in_mb |                                                                                                                                           
+ 251
+                                             full_query_logging_options | 
+ FullQueryLoggerOptions{log_dir='', archive_command='', roll_cycle='HOURLY', block=true, 
+ max_queue_weight=268435456, max_log_size=17179869184}
+                                                 gc_log_threshold_in_ms |                                                                                                                                           
+ 200
+                                                gc_warn_threshold_in_ms |                                                                                                                                          
+ 1000
+                                    hinted_handoff_disabled_datacenters |                                                                                                                                            
+ []
+                                                 hinted_handoff_enabled |                                                                                                                                          
+ true
+                                          hinted_handoff_throttle_in_kb |                                                                                                                                          
+ 1024
+                                                      hints_compression |                                                                                                                                          
+ null
+                                                        hints_directory |                                                                                                           
+ /home/ec2-user/cassandra/data/hints
+                                               hints_flush_period_in_ms |                                                                                                                                         
+ 10000
+                                                ideal_consistency_level |                                                                                                                                          
+ null
+                                                    incremental_backups |                                                                                                                                         
+ false
+                                                          initial_token |                                                                                                                                          
+ null
+                   inter_dc_stream_throughput_outbound_megabits_per_sec |                                                                                                                                           
+ 200
+                                                   inter_dc_tcp_nodelay |                                                                                                                                         
+ false
+                  internode_application_receive_queue_capacity_in_bytes |                                                                                                                                       
+ 4194304
+  internode_application_receive_queue_reserve_endpoint_capacity_in_bytes |                                                                                                                                     
+ 134217728
+  internode_application_receive_queue_reserve_global_capacity_in_bytes |                                                                                                                                     
+ 536870912
+                  internode_application_send_queue_capacity_in_bytes |                                                                                                                                       
+ 4194304
+ internode_application_send_queue_reserve_endpoint_capacity_in_bytes |                                                                                                                                     
+  134217728
+
+ 
+  internode_application_send_queue_reserve_global_capacity_in_bytes |                                                             
+  536870912
+                                           internode_authenticator |                                                                  
+ null
+                                             internode_compression |                                                                    
+ dc
+                                                    listen_address |                                                             
+ localhost
+                                                  listen_interface |                                                                  
+ null
+                                       listen_on_broadcast_address |                                                                 
+ false
+                         max_concurrent_automatic_sstable_upgrades |                                                                     
+ 1
+                                             max_hint_window_in_ms |                                                              
+ 10800000
+                                          memtable_allocation_type |                                                          
+ heap_buffers
+                                    min_free_space_per_drive_in_mb |                                                                    
+ 50
+                            native_transport_allow_older_protocols |                                                                  
+ true
+                          native_transport_flush_in_batches_legacy |                                                                 
+ false
+                           native_transport_frame_block_size_in_kb |                                                                    32
+                               native_transport_idle_timeout_in_ms |                                                                     0
+                       native_transport_max_concurrent_connections |                                                                    -1
+                native_transport_max_concurrent_connections_per_ip |                                                                    -1
+                 native_transport_max_concurrent_requests_in_bytes |                                                             105277030
+          native_transport_max_concurrent_requests_in_bytes_per_ip |                                                              26319257
+                             native_transport_max_frame_size_in_mb |                                                                   256
+                                      native_transport_max_threads |                                                                   128
+                                             native_transport_port |                                                                  9042
+                                         native_transport_port_ssl |                                                                  null
+                                                network_authorizer |                                             AllowAllNetworkAuthorizer
+                                                        num_tokens |                                                                   256
+                                                       partitioner |                           org.apache.cassandra.dht.Murmur3Partitioner
+                                 prepared_statements_cache_size_mb |                                                                  null
+                                       range_request_timeout_in_ms |                                                                 10000
+                                        read_request_timeout_in_ms |                                                                  5000
+                                 repair_command_pool_full_strategy |                                                                 queue
+                                          repair_command_pool_size |                                                            2147483647
+                                     repair_session_max_tree_depth |                                                                    20
+                                        repair_session_space_in_mb |                                                                    62
+                repaired_data_tracking_for_partition_reads_enabled |                                                                 false
+                    repaired_data_tracking_for_range_reads_enabled |                                                                 false
+                       report_unconfirmed_repaired_data_mismatches |                                                                 false
+                                             request_timeout_in_ms |                                                                 10000
+                                                      role_manager |                                                  CassandraRoleManager
+                                                       rpc_address |                                                             localhost
+                                                     rpc_interface |                                                                  null
+                                            saved_caches_directory |                            /home/ec2-user/cassandra/data/saved_caches
+                                                     seed_provider | org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1:7000}
+                               server_encryption_options_algorithm |                                                                  null
+                                server_encryption_options_protocol |                                                                   TLS
+                                      slow_query_log_timeout_in_ms |                                                                   500
+                                                  ssl_storage_port |                                                                  7001
+                                            start_native_transport |                                                                  true
+                                                      storage_port |                                                                  7000
+                                            stream_entire_sstables |                                                                  true
+               transparent_data_encryption_options_chunk_length_kb |                                                                    64
+                        transparent_data_encryption_options_cipher |                                                  AES/CBC/PKCS5Padding
+                       transparent_data_encryption_options_enabled |                                                                 false
+                     transparent_data_encryption_options_iv_length |                                                                    
+ 16
+ (224 rows)
+
+
+Thread Pools Virtual Table
+**************************
+
+The ``thread_pools`` table lists information about all thread pools. Thread pool information includes active tasks, active tasks limit, blocked tasks, blocked tasks all time,  completed tasks, and pending tasks. A query on the ``thread_pools`` returns following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.thread_pools;
+
+ name                         | active_tasks | active_tasks_limit | blocked_tasks | blocked_tasks_all_time | completed_tasks | pending_tasks
+ ------------------------------+--------------+--------------------+---------------+------------------------+-----------------+---------------
+             AntiEntropyStage |            0 |                  1 |             0 |                      0 |               0 |             0
+         CacheCleanupExecutor |            0 |                  1 |             0 |                      0 |               0 |             0
+           CompactionExecutor |            0 |                  2 |             0 |                      0 |             881 |             0
+         CounterMutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+                  GossipStage |            0 |                  1 |             0 |                      0 |               0 |             0
+              HintsDispatcher |            0 |                  2 |             0 |                      0 |               0 |             0
+        InternalResponseStage |            0 |                  2 |             0 |                      0 |               0 |             0
+          MemtableFlushWriter |            0 |                  2 |             0 |                      0 |               1 |             0
+            MemtablePostFlush |            0 |                  1 |             0 |                      0 |               2 |             0
+        MemtableReclaimMemory |            0 |                  1 |             0 |                      0 |               1 |             0
+               MigrationStage |            0 |                  1 |             0 |                      0 |               0 |             0
+                    MiscStage |            0 |                  1 |             0 |                      0 |               0 |             0
+                MutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+    Native-Transport-Requests |            1 |                128 |             0 |                      0 |             130 |             0
+       PendingRangeCalculator |            0 |                  1 |             0 |                      0 |               1 |             0
+ PerDiskMemtableFlushWriter_0 |            0 |                  2 |             0 |                      0 |               1 |             0
+                    ReadStage |            0 |                 32 |             0 |                      0 |              13 |             0
+                  Repair-Task |            0 |         2147483647 |             0 |                      0 |               0 |             0
+         RequestResponseStage |            0 |                  2 |             0 |                      0 |               0 |             0
+                      Sampler |            0 |                  1 |             0 |                      0 |               0 |             0
+     SecondaryIndexManagement |            0 |                  1 |             0 |                      0 |               0 |             0
+           ValidationExecutor |            0 |         2147483647 |             0 |                      0 |               0 |             0
+            ViewBuildExecutor |            0 |                  1 |             0 |                      0 |               0 |             0
+            ViewMutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+
+(24 rows)
+
+Internode Inbound Messaging Virtual Table
+*****************************************
+
+The ``internode_inbound``  virtual table is for the internode inbound messaging. Initially no internode inbound messaging may get listed. In addition to the address, port, datacenter and rack information includes  corrupt frames recovered, corrupt frames unrecovered, error bytes, error count, expired bytes, expired count, processed bytes, processed count, received bytes, received count, scheduled bytes, scheduled count, throttled count, throttled nanos, using bytes, using reserve bytes. A query on the ``internode_inbound`` returns following details.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.internode_inbound;
+ address | port | dc | rack | corrupt_frames_recovered | corrupt_frames_unrecovered | 
+ error_bytes | error_count | expired_bytes | expired_count | processed_bytes | 
+ processed_count | received_bytes | received_count | scheduled_bytes | scheduled_count | throttled_count | throttled_nanos | using_bytes | using_reserve_bytes
+ ---------+------+----+------+--------------------------+----------------------------+- 
+ ----------
+ (0 rows)
+
+SSTables Tasks Virtual Table
+****************************
+
+As no sstable tasks are running initially the ``system_views.sstable_tasks`` table lists 0 rows.","[{'comment': 'this is very sparse, can we list kind of operations that could be listed here? and maybe include an example of compactions running. Easiest way to get that btw is to `nodetool disableautocompaction` and run a stress tool for a bit, calling `nodetool flush` frequently to generate lots of sstables. Then `nodetool enableautocompaction` and see them listed.', 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+
+Table 1 : Virtual Tables in system_views
+
++------------------+---------------------------------------------------+
+|Virtual Table     | Description                                       | 
++------------------+---------------------------------------------------+
+| clients          |Lists information about all connected clients.     |           
++------------------+---------------------------------------------------+
+| disk_usage       |Disk usage including disk_space, keyspace_name,    |
+|                  |and table_name by system keyspaces.                |
++------------------+---------------------------------------------------+
+| local_writes     |A table metric for local writes                    |
+|                  |including count, keyspace_name,                    | 
+|                  |max, median, per_second, and                       |
+|                  |table_name.                                        |                                                                         
++------------------+---------------------------------------------------+
+| caches           |Displays the general cache information including   |
+|                  |cache name, capacity_bytes, entry_count, hit_count,| 
+|                  |hit_ratio double, recent_hit_rate_per_second,      |
+|                  |recent_request_rate_per_second, request_count, and | 
+|                  |size_bytes.                                        |                                                                         
++------------------+---------------------------------------------------+
+| local_reads      |A table metric for  local reads information.       |                                                                   
++------------------+---------------------------------------------------+
+| sstable_tasks    |Lists currently running tasks such as compactions  |
+|                  |and upgrades on SSTables.                          |
++------------------+---------------------------------------------------+
+|internode_inbound |Lists information about the inbound                | 
+|                  |internode messaging.                               |                
++------------------+---------------------------------------------------+
+| thread_pools     |Lists metrics for each thread pool.                |                                                                        
++------------------+---------------------------------------------------+
+| settings         |Displays configuration settings in cassandra.yaml. |                                                                         
++------------------+---------------------------------------------------+
+|max_partition_size|A table metric for maximum partition size.         |                                                                       
++------------------+---------------------------------------------------+
+|internode_outbound|Information about the outbound internode messaging.|
+|                  |                                                   |                                          
++------------------+---------------------------------------------------+
+ 
+We shall discuss some of the virtual tables in more detail next.
+
+Clients Virtual Table
+*********************
+
+The ``clients`` virtual table lists all active connections (connected clients) including their ip address, port, connection stage, driver name, driver version, hostname, protocol version, request count, ssl enabled, ssl protocol and user name. A query on the ``clients`` table returns the following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.clients;
+  address   | port  | connection_stage | driver_name | driver_version | hostname  | protocol_version | request_count | ssl_cipher_suite | ssl_enabled | ssl_protocol | username
+ -----------+-------+------------------+-------------+----------------+-----------+------------------+---------------+------------------+-------------+--------------+-----------
+  127.0.0.1 | 50628 |            ready |        null |           null | localhost |                4 |            55 |             null |       False |         null | anonymous
+  127.0.0.1 | 50630 |            ready |        null |           null | localhost |                4 |            70 |             null |       False |         null | anonymous
+
+ (2 rows)
+
+The virtual tables may be described with ``DESCRIBE`` statement. The DDL listed however cannot be run to create a virtual table. As an example describe the ``system_views.clients`` virtual table.
+
+::
+
+  cqlsh:system_views> DESC TABLE system_views.clients;
+ CREATE TABLE system_views.clients (
+    address inet,
+    connection_stage text,
+    driver_name text,
+    driver_version text,
+    hostname text,
+    port int,
+    protocol_version int,
+    request_count bigint,
+    ssl_cipher_suite text,
+    ssl_enabled boolean,
+    ssl_protocol text,
+    username text,
+    PRIMARY KEY (address, port)) WITH CLUSTERING ORDER BY (port ASC)
+    AND compaction = {'class': 'None'}
+    AND compression = {};
+
+Caches Virtual Table
+********************
+The ``caches`` virtual table lists information about the  caches. The four caches presently created are chunks, counters, keys and rows. A query on the ``caches`` virtual table returns the following details.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.caches;
+ name     | capacity_bytes | entry_count | hit_count | hit_ratio | recent_hit_rate_per_second | recent_request_rate_per_second | request_count | size_bytes
+ ---------+----------------+-------------+-----------+-----------+----------------------------+--------------------------------+---------------+------------
+   chunks |      229638144 |          29 |       166 |      0.83 |                          5 |                              6 |           200 |     475136
+ counters |       26214400 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+     keys |       52428800 |          14 |       124 |  0.873239 |                          4 |                              4 |           142 |       1248
+     rows |              0 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+
+ (4 rows)
+
+Settings Virtual Table
+**********************
+The ``settings table`` is rather useful and lists all the configuration settings from the ``cassandra.yaml``.  The encryption options are overridden to hide the sensitive truststore information or passwords.  The configuration settings however cannot be set using DML  on the 
+virtual table presently. A total of 224 settings get listed presently. 
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.settings;
+
+ name                                                                   | value
+ -----------------------------------------------------------------------+-------------------- 
+   allocate_tokens_for_keyspace                                         |        null                                                                                          
+   audit_logging_options_audit_logs_dir                                 |  
+ /home/ec2-user/cassandra/logs/audit/                                                                                                       
+                                          audit_logging_options_enabled |                                                                                                                                         
+ false
+                              audit_logging_options_excluded_categories |                                                                                                                                              
+                               audit_logging_options_excluded_keyspaces |                                                                                                    
+ system,system_schema,system_virtual_schema
+                                   audit_logging_options_excluded_users |                                                                                                                                              
+                              audit_logging_options_included_categories |                                                                                                                                              
+                               audit_logging_options_included_keyspaces |                                                                                                                                              
+                                   audit_logging_options_included_users |                                                                                                                                              
+                                           audit_logging_options_logger |                                                                                                                                
+ BinAuditLogger
+                                                          authenticator |                                                                                                                         
+ AllowAllAuthenticator
+                                                             authorizer |                                                                                                                            
+ AllowAllAuthorizer
+                                                         auto_bootstrap |                                                                                                                                          
+ true
+                                                          auto_snapshot |                                                                                                                                          
+ true
+                                              automatic_sstable_upgrade |                                                                                                                                         
+ false
+                                                  back_pressure_enabled |                                                                                                                                         
+ false
+                                                 back_pressure_strategy |                                                           
+ org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}
+                                                      broadcast_address |                                                                                                                                          
+ null
+                                                  broadcast_rpc_address |                                                                                                                                          
+ null
+                                                           cluster_name |                                                                                                                                  
+ Test Cluster
+                                          column_index_cache_size_in_kb |                                                                                                                                             
+ 2
+                                                column_index_size_in_kb |                                                                                                                                            
+ 64
+                                                  commit_failure_policy |                                                                                                                                          
+ stop
+                                                  commitlog_compression |                                                                                                                                          
+ null
+                                                    commitlog_directory |                                                                                                       
+ /home/ec2-user/cassandra/data/commitlog
+                              commitlog_max_compression_buffers_in_pool |                                                                                                                                             
+ 3
+                                          commitlog_periodic_queue_size |                                                                                                                                            
+ -1
+                                           commitlog_segment_size_in_mb |                                                                                                                                            
+ 32
+                                                         commitlog_sync |                                                                                                                                      
+ periodic
+                                      commitlog_sync_batch_window_in_ms |                                                                                                                                           
+ NaN
+                                      commitlog_sync_group_window_in_ms |                                                                                                                                           
+ NaN
+                                            commitlog_sync_period_in_ms |                                                                                                                                         
+ 10000
+                                            commitlog_total_space_in_mb |                                                                                                                                          
+ 2556
+                        compaction_large_partition_warning_threshold_mb |                                                                                                                                           
+ 100
+                                       compaction_throughput_mb_per_sec |                                                                                                                                            
+ 16
+                                                  concurrent_compactors |                                                                                                                                             
+ 2
+                                              concurrent_counter_writes |                                                                                                                                            
+ 32
+                                  concurrent_materialized_view_builders |                                                                                                                                             
+ 1
+                                    concurrent_materialized_view_writes |                                                                                                                                            
+ 32
+                                                       concurrent_reads |                                                                                                                                            
+ 32
+                                                  concurrent_replicates |                                                                                                                                          
+ null
+                                                 concurrent_validations |                                                                                                                                    
+ 2147483647
+                                                      concurrent_writes |                                                                                                                                            
+ 32
+                                      credentials_update_interval_in_ms |                                                                                                                                            
+ -1
+                                             credentials_validity_in_ms |                                                                                                                                          
+ 2000
+                                                     cross_node_timeout |                                                                                                                                         
+ false
+                                                  data_file_directories |                                                                                                          
+ [/home/ec2-user/cassandra/data/data]
+                                              diagnostic_events_enabled |                                                                                                                                         
+ false
+                                                       disk_access_mode |                                                                                                                                          
+ mmap
+                                                    disk_failure_policy |                                                                                                                                          
+ stop
+                                              enable_materialized_views |                                                                                                                                         
+ false
+                                                    enable_sasi_indexes |                                                                                                                                         
+ false
+                                 enable_scripted_user_defined_functions |                                                                                                                                         
+ false
+                                           enable_transient_replication |                                                                                                                                         
+ false
+                                          enable_user_defined_functions |                                                                                                                                         
+ false
+                                  enable_user_defined_functions_threads |                                                                                                                                          
+ true
+                                                        endpoint_snitch |                                                                                                                                  
+ SimpleSnitch
+                                                    file_cache_round_up |                                                                                                                                         
+ false
+                                                  file_cache_size_in_mb |                                                                                                                                           
+ 251
+                                             full_query_logging_options | 
+ FullQueryLoggerOptions{log_dir='', archive_command='', roll_cycle='HOURLY', block=true, 
+ max_queue_weight=268435456, max_log_size=17179869184}
+                                                 gc_log_threshold_in_ms |                                                                                                                                           
+ 200
+                                                gc_warn_threshold_in_ms |                                                                                                                                          
+ 1000
+                                    hinted_handoff_disabled_datacenters |                                                                                                                                            
+ []
+                                                 hinted_handoff_enabled |                                                                                                                                          
+ true
+                                          hinted_handoff_throttle_in_kb |                                                                                                                                          
+ 1024
+                                                      hints_compression |                                                                                                                                          
+ null
+                                                        hints_directory |                                                                                                           
+ /home/ec2-user/cassandra/data/hints
+                                               hints_flush_period_in_ms |                                                                                                                                         
+ 10000
+                                                ideal_consistency_level |                                                                                                                                          
+ null
+                                                    incremental_backups |                                                                                                                                         
+ false
+                                                          initial_token |                                                                                                                                          
+ null
+                   inter_dc_stream_throughput_outbound_megabits_per_sec |                                                                                                                                           
+ 200
+                                                   inter_dc_tcp_nodelay |                                                                                                                                         
+ false
+                  internode_application_receive_queue_capacity_in_bytes |                                                                                                                                       
+ 4194304
+  internode_application_receive_queue_reserve_endpoint_capacity_in_bytes |                                                                                                                                     
+ 134217728
+  internode_application_receive_queue_reserve_global_capacity_in_bytes |                                                                                                                                     
+ 536870912
+                  internode_application_send_queue_capacity_in_bytes |                                                                                                                                       
+ 4194304
+ internode_application_send_queue_reserve_endpoint_capacity_in_bytes |                                                                                                                                     
+  134217728
+
+ 
+  internode_application_send_queue_reserve_global_capacity_in_bytes |                                                             
+  536870912
+                                           internode_authenticator |                                                                  
+ null
+                                             internode_compression |                                                                    
+ dc
+                                                    listen_address |                                                             
+ localhost
+                                                  listen_interface |                                                                  
+ null
+                                       listen_on_broadcast_address |                                                                 
+ false
+                         max_concurrent_automatic_sstable_upgrades |                                                                     
+ 1
+                                             max_hint_window_in_ms |                                                              
+ 10800000
+                                          memtable_allocation_type |                                                          
+ heap_buffers
+                                    min_free_space_per_drive_in_mb |                                                                    
+ 50
+                            native_transport_allow_older_protocols |                                                                  
+ true
+                          native_transport_flush_in_batches_legacy |                                                                 
+ false
+                           native_transport_frame_block_size_in_kb |                                                                    32
+                               native_transport_idle_timeout_in_ms |                                                                     0
+                       native_transport_max_concurrent_connections |                                                                    -1
+                native_transport_max_concurrent_connections_per_ip |                                                                    -1
+                 native_transport_max_concurrent_requests_in_bytes |                                                             105277030
+          native_transport_max_concurrent_requests_in_bytes_per_ip |                                                              26319257
+                             native_transport_max_frame_size_in_mb |                                                                   256
+                                      native_transport_max_threads |                                                                   128
+                                             native_transport_port |                                                                  9042
+                                         native_transport_port_ssl |                                                                  null
+                                                network_authorizer |                                             AllowAllNetworkAuthorizer
+                                                        num_tokens |                                                                   256
+                                                       partitioner |                           org.apache.cassandra.dht.Murmur3Partitioner
+                                 prepared_statements_cache_size_mb |                                                                  null
+                                       range_request_timeout_in_ms |                                                                 10000
+                                        read_request_timeout_in_ms |                                                                  5000
+                                 repair_command_pool_full_strategy |                                                                 queue
+                                          repair_command_pool_size |                                                            2147483647
+                                     repair_session_max_tree_depth |                                                                    20
+                                        repair_session_space_in_mb |                                                                    62
+                repaired_data_tracking_for_partition_reads_enabled |                                                                 false
+                    repaired_data_tracking_for_range_reads_enabled |                                                                 false
+                       report_unconfirmed_repaired_data_mismatches |                                                                 false
+                                             request_timeout_in_ms |                                                                 10000
+                                                      role_manager |                                                  CassandraRoleManager
+                                                       rpc_address |                                                             localhost
+                                                     rpc_interface |                                                                  null
+                                            saved_caches_directory |                            /home/ec2-user/cassandra/data/saved_caches
+                                                     seed_provider | org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1:7000}
+                               server_encryption_options_algorithm |                                                                  null
+                                server_encryption_options_protocol |                                                                   TLS
+                                      slow_query_log_timeout_in_ms |                                                                   500
+                                                  ssl_storage_port |                                                                  7001
+                                            start_native_transport |                                                                  true
+                                                      storage_port |                                                                  7000
+                                            stream_entire_sstables |                                                                  true
+               transparent_data_encryption_options_chunk_length_kb |                                                                    64
+                        transparent_data_encryption_options_cipher |                                                  AES/CBC/PKCS5Padding
+                       transparent_data_encryption_options_enabled |                                                                 false
+                     transparent_data_encryption_options_iv_length |                                                                    
+ 16
+ (224 rows)
+
+
+Thread Pools Virtual Table
+**************************
+
+The ``thread_pools`` table lists information about all thread pools. Thread pool information includes active tasks, active tasks limit, blocked tasks, blocked tasks all time,  completed tasks, and pending tasks. A query on the ``thread_pools`` returns following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.thread_pools;
+
+ name                         | active_tasks | active_tasks_limit | blocked_tasks | blocked_tasks_all_time | completed_tasks | pending_tasks
+ ------------------------------+--------------+--------------------+---------------+------------------------+-----------------+---------------
+             AntiEntropyStage |            0 |                  1 |             0 |                      0 |               0 |             0
+         CacheCleanupExecutor |            0 |                  1 |             0 |                      0 |               0 |             0
+           CompactionExecutor |            0 |                  2 |             0 |                      0 |             881 |             0
+         CounterMutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+                  GossipStage |            0 |                  1 |             0 |                      0 |               0 |             0
+              HintsDispatcher |            0 |                  2 |             0 |                      0 |               0 |             0
+        InternalResponseStage |            0 |                  2 |             0 |                      0 |               0 |             0
+          MemtableFlushWriter |            0 |                  2 |             0 |                      0 |               1 |             0
+            MemtablePostFlush |            0 |                  1 |             0 |                      0 |               2 |             0
+        MemtableReclaimMemory |            0 |                  1 |             0 |                      0 |               1 |             0
+               MigrationStage |            0 |                  1 |             0 |                      0 |               0 |             0
+                    MiscStage |            0 |                  1 |             0 |                      0 |               0 |             0
+                MutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+    Native-Transport-Requests |            1 |                128 |             0 |                      0 |             130 |             0
+       PendingRangeCalculator |            0 |                  1 |             0 |                      0 |               1 |             0
+ PerDiskMemtableFlushWriter_0 |            0 |                  2 |             0 |                      0 |               1 |             0
+                    ReadStage |            0 |                 32 |             0 |                      0 |              13 |             0
+                  Repair-Task |            0 |         2147483647 |             0 |                      0 |               0 |             0
+         RequestResponseStage |            0 |                  2 |             0 |                      0 |               0 |             0
+                      Sampler |            0 |                  1 |             0 |                      0 |               0 |             0
+     SecondaryIndexManagement |            0 |                  1 |             0 |                      0 |               0 |             0
+           ValidationExecutor |            0 |         2147483647 |             0 |                      0 |               0 |             0
+            ViewBuildExecutor |            0 |                  1 |             0 |                      0 |               0 |             0
+            ViewMutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+
+(24 rows)
+
+Internode Inbound Messaging Virtual Table
+*****************************************
+
+The ``internode_inbound``  virtual table is for the internode inbound messaging. Initially no internode inbound messaging may get listed. In addition to the address, port, datacenter and rack information includes  corrupt frames recovered, corrupt frames unrecovered, error bytes, error count, expired bytes, expired count, processed bytes, processed count, received bytes, received count, scheduled bytes, scheduled count, throttled count, throttled nanos, using bytes, using reserve bytes. A query on the ``internode_inbound`` returns following details.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.internode_inbound;
+ address | port | dc | rack | corrupt_frames_recovered | corrupt_frames_unrecovered | 
+ error_bytes | error_count | expired_bytes | expired_count | processed_bytes | 
+ processed_count | received_bytes | received_count | scheduled_bytes | scheduled_count | throttled_count | throttled_nanos | using_bytes | using_reserve_bytes
+ ---------+------+----+------+--------------------------+----------------------------+- 
+ ----------
+ (0 rows)
+
+SSTables Tasks Virtual Table
+****************************
+
+As no sstable tasks are running initially the ``system_views.sstable_tasks`` table lists 0 rows.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.sstable_tasks;
+ keyspace_name | table_name | task_id | kind | progress | total | unit
+ ---------------+------------+---------+------+----------+-------+------
+
+ (0 rows)
+
+The system_virtual_schema keyspace","[{'comment': 'I think we can drop the entire virtual schema keyspace section here. Or at most just hand wave that its used for internal Cassandra book keeping and communications with driver. This is not something a user needs to know about', 'commenter': 'clohfink'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+
+Table 1 : Virtual Tables in system_views
+
++------------------+---------------------------------------------------+
+|Virtual Table     | Description                                       | 
++------------------+---------------------------------------------------+
+| clients          |Lists information about all connected clients.     |           
++------------------+---------------------------------------------------+
+| disk_usage       |Disk usage including disk_space, keyspace_name,    |
+|                  |and table_name by system keyspaces.                |
++------------------+---------------------------------------------------+
+| local_writes     |A table metric for local writes                    |
+|                  |including count, keyspace_name,                    | 
+|                  |max, median, per_second, and                       |
+|                  |table_name.                                        |                                                                         
++------------------+---------------------------------------------------+
+| caches           |Displays the general cache information including   |
+|                  |cache name, capacity_bytes, entry_count, hit_count,| 
+|                  |hit_ratio double, recent_hit_rate_per_second,      |
+|                  |recent_request_rate_per_second, request_count, and | 
+|                  |size_bytes.                                        |                                                                         
++------------------+---------------------------------------------------+
+| local_reads      |A table metric for  local reads information.       |                                                                   
++------------------+---------------------------------------------------+
+| sstable_tasks    |Lists currently running tasks such as compactions  |
+|                  |and upgrades on SSTables.                          |
++------------------+---------------------------------------------------+
+|internode_inbound |Lists information about the inbound                | 
+|                  |internode messaging.                               |                
++------------------+---------------------------------------------------+
+| thread_pools     |Lists metrics for each thread pool.                |                                                                        
++------------------+---------------------------------------------------+
+| settings         |Displays configuration settings in cassandra.yaml. |                                                                         
++------------------+---------------------------------------------------+
+|max_partition_size|A table metric for maximum partition size.         |                                                                       
++------------------+---------------------------------------------------+
+|internode_outbound|Information about the outbound internode messaging.|
+|                  |                                                   |                                          
++------------------+---------------------------------------------------+
+ 
+We shall discuss some of the virtual tables in more detail next.
+
+Clients Virtual Table
+*********************
+
+The ``clients`` virtual table lists all active connections (connected clients) including their ip address, port, connection stage, driver name, driver version, hostname, protocol version, request count, ssl enabled, ssl protocol and user name. A query on the ``clients`` table returns the following details.","[{'comment': 'Instead of just listing what these virtual tables are, I think it would be real valuable to list _why_ you would want to look at them. for example this can be used for finding applications using old incompatible versions of drivers before upgrading and with nodetool enableoldprotocolversions and nodetool\r\ndisableoldprotocolversions during upgrades. It can also be used to identify misbehaving clients sending too many requests or during the migration to and from ssl.', 'commenter': 'clohfink'}, {'comment': 'Added examples.', 'commenter': 'Deepak-Vohra'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner","[{'comment': 'I think we should mention here about making advanced queries with ALLOW FILTERING and aggregation functions are safe to use even though in normal tables we dont recommend it.', 'commenter': 'clohfink'}, {'comment': 'For LocalPartitioner I am not sure if people will necessarily know what that means so maybe spell out that since its not replicated (which you have above) the partitioner sorts in order of partition keys instead of by their hash.', 'commenter': 'clohfink'}, {'comment': 'Not even sure we should say thats different then regular tables since many system tables use LocalPartitioner and are non-replicated. But we should definitely highlight that they these tables are local only and non distributed', 'commenter': 'clohfink'}, {'comment': 'Added \r\n- the partitioner sorts in order of partition keys instead of by their hash\r\n- tables are local only and non distributed', 'commenter': 'Deepak-Vohra'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+","[{'comment': 'current list is\r\n\r\n```\r\ncqlsh> desc tables;\r\n...\r\nKeyspace system_views\r\n---------------------\r\nsstable_tasks\r\ndisk_usage\r\nthread_pools\r\nlocal_scan_latency\r\ncoordinator_scan_latency\r\nmax_partition_size\r\ninternode_inbound\r\ncoordinator_read_latency\r\ncaches\r\nlocal_read_latency\r\nrows_per_read\r\nclients\r\nlocal_write_latency\r\ninternode_outbound\r\ncoordinator_write_latency\r\ntombstones_per_read\r\nsettings\r\n```', 'commenter': 'clohfink'}, {'comment': 'Seems like some of those got cut off in the list above and shortened.', 'commenter': 'clohfink'}, {'comment': 'Added all.', 'commenter': 'Deepak-Vohra'}]"
402,doc/source/new/virtualtables.rst,"@@ -0,0 +1,586 @@
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+Virtual Tables
+--------------
+
+Apache Cassandra 4.0 implements virtual tables (`CASSANDRA-7622
+<https://issues.apache.org/jira/browse/CASSANDRA-7622>`_).
+
+Definition
+^^^^^^^^^^
+
+A virtual table is a table that is backed by an API instead of data explicitly managed and stored as SSTables. Apache Cassandra 4.0 implements a virtual keyspace interface for virtual tables. Virtual tables are specific to each node. 
+
+Objective
+^^^^^^^^^
+
+A virtual table could have several uses including:
+
+- Expose JMX data/metrics through CQL
+- Expose YAML configuration information
+- Expose other MBean data
+ 
+How  are Virtual Tables different from regular tables?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces are quite different from regular tables and keyspaces respectively such as:
+
+- Virtual tables are read-only 
+- Virtual tables are not replicated
+- Virtual tables have no associated SSTables
+- Consistency level of the queries sent virtual tables are ignored
+- Virtual tables are managed by Cassandra and a user cannot run  DDL to create new virtual tables or DML to modify existing virtual tables
+- Virtual tables are created in special keyspaces and not just any keyspace
+- All existing virtual tables use LocalPartitioner
+
+Virtual Keyspaces
+^^^^^^^^^^^^^^^^^
+
+Apache Cassandra 4.0 has added two new keyspaces for virtual tables: ``system_virtual_schema`` and ``system_views``. Run the following command to list the keyspaces.
+
+::
+
+ cqlsh> DESC KEYSPACES;
+ system_schema  system       system_distributed  system_virtual_schema
+ system_auth      system_traces       system_views
+
+The ``system_virtual_schema keyspace`` contains schema information on virtual tables. The ``system_views`` keyspace contains the actual virtual tables. The virtual keyspaces metadata is not exposed through ``DESCRIBE`` statement, which returns an error message:
+
+::
+
+ cqlsh> DESCRIBE KEYSPACE system_views
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh> DESCRIBE KEYSPACE system_virtual_schema;
+ 'NoneType' object has no attribute 'export_for_schema'
+ cqlsh>
+
+Virtual Table Limitations
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables and virtual keyspaces have some limitations such as:
+
+- Cannot alter virtual keyspaces
+- Cannot drop virtual keyspaces
+- Cannot alter virtual tables
+- Cannot drop virtual tables
+- Cannot truncate virtual tables
+- Expiring columns are not supported by virtual tables
+- Conditional updates are not supported by virtual tables
+- Cannot create tables in virtual keyspaces
+- Cannot perform any operations against virtual keyspace
+- Secondary indexes are not supported on virtual tables
+- Cannot create functions in virtual keyspaces
+- Cannot create types in virtual keyspaces
+- Materialized views are not supported on virtual tables
+- Virtual tables don't support DELETE statements
+- Cannot CREATE TRIGGER against a virtual table
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot include a virtual table statement in a logged batch
+- Mutations for virtual and regular tables cannot exist in the same batch
+- Conditional BATCH statements cannot include mutations for virtual tables
+- Cannot create aggregates in virtual keyspaces
+
+Listing and Describing Virtual Tables
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+Virtual tables in a virtual keyspace may however be listed with ``DESC TABLES``.  The ``system_views`` virtual keyspace tables include the following:
+
+::
+
+ cqlsh> USE system_views;
+ cqlsh:system_views> DESC TABLES;
+   clients    internode_inbound
+ disk_usage       sstable_tasks        caches           
+ local_writes        max_partition_size  local_reads       
+ coordinator_writes  internode_outbound  thread_pools      
+ local_scans         coordinator_reads   settings 
+
+Some of the salient virtual tables in system_views virtual keyspace are described in Table 1.
+
+Table 1 : Virtual Tables in system_views
+
++------------------+---------------------------------------------------+
+|Virtual Table     | Description                                       | 
++------------------+---------------------------------------------------+
+| clients          |Lists information about all connected clients.     |           
++------------------+---------------------------------------------------+
+| disk_usage       |Disk usage including disk_space, keyspace_name,    |
+|                  |and table_name by system keyspaces.                |
++------------------+---------------------------------------------------+
+| local_writes     |A table metric for local writes                    |
+|                  |including count, keyspace_name,                    | 
+|                  |max, median, per_second, and                       |
+|                  |table_name.                                        |                                                                         
++------------------+---------------------------------------------------+
+| caches           |Displays the general cache information including   |
+|                  |cache name, capacity_bytes, entry_count, hit_count,| 
+|                  |hit_ratio double, recent_hit_rate_per_second,      |
+|                  |recent_request_rate_per_second, request_count, and | 
+|                  |size_bytes.                                        |                                                                         
++------------------+---------------------------------------------------+
+| local_reads      |A table metric for  local reads information.       |                                                                   
++------------------+---------------------------------------------------+
+| sstable_tasks    |Lists currently running tasks such as compactions  |
+|                  |and upgrades on SSTables.                          |
++------------------+---------------------------------------------------+
+|internode_inbound |Lists information about the inbound                | 
+|                  |internode messaging.                               |                
++------------------+---------------------------------------------------+
+| thread_pools     |Lists metrics for each thread pool.                |                                                                        
++------------------+---------------------------------------------------+
+| settings         |Displays configuration settings in cassandra.yaml. |                                                                         
++------------------+---------------------------------------------------+
+|max_partition_size|A table metric for maximum partition size.         |                                                                       
++------------------+---------------------------------------------------+
+|internode_outbound|Information about the outbound internode messaging.|
+|                  |                                                   |                                          
++------------------+---------------------------------------------------+
+ 
+We shall discuss some of the virtual tables in more detail next.
+
+Clients Virtual Table
+*********************
+
+The ``clients`` virtual table lists all active connections (connected clients) including their ip address, port, connection stage, driver name, driver version, hostname, protocol version, request count, ssl enabled, ssl protocol and user name. A query on the ``clients`` table returns the following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.clients;
+  address   | port  | connection_stage | driver_name | driver_version | hostname  | protocol_version | request_count | ssl_cipher_suite | ssl_enabled | ssl_protocol | username
+ -----------+-------+------------------+-------------+----------------+-----------+------------------+---------------+------------------+-------------+--------------+-----------
+  127.0.0.1 | 50628 |            ready |        null |           null | localhost |                4 |            55 |             null |       False |         null | anonymous
+  127.0.0.1 | 50630 |            ready |        null |           null | localhost |                4 |            70 |             null |       False |         null | anonymous
+
+ (2 rows)
+
+The virtual tables may be described with ``DESCRIBE`` statement. The DDL listed however cannot be run to create a virtual table. As an example describe the ``system_views.clients`` virtual table.
+
+::
+
+  cqlsh:system_views> DESC TABLE system_views.clients;
+ CREATE TABLE system_views.clients (
+    address inet,
+    connection_stage text,
+    driver_name text,
+    driver_version text,
+    hostname text,
+    port int,
+    protocol_version int,
+    request_count bigint,
+    ssl_cipher_suite text,
+    ssl_enabled boolean,
+    ssl_protocol text,
+    username text,
+    PRIMARY KEY (address, port)) WITH CLUSTERING ORDER BY (port ASC)
+    AND compaction = {'class': 'None'}
+    AND compression = {};
+
+Caches Virtual Table
+********************
+The ``caches`` virtual table lists information about the  caches. The four caches presently created are chunks, counters, keys and rows. A query on the ``caches`` virtual table returns the following details.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.caches;
+ name     | capacity_bytes | entry_count | hit_count | hit_ratio | recent_hit_rate_per_second | recent_request_rate_per_second | request_count | size_bytes
+ ---------+----------------+-------------+-----------+-----------+----------------------------+--------------------------------+---------------+------------
+   chunks |      229638144 |          29 |       166 |      0.83 |                          5 |                              6 |           200 |     475136
+ counters |       26214400 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+     keys |       52428800 |          14 |       124 |  0.873239 |                          4 |                              4 |           142 |       1248
+     rows |              0 |           0 |         0 |       NaN |                          0 |                              0 |             0 |          0
+
+ (4 rows)
+
+Settings Virtual Table
+**********************
+The ``settings table`` is rather useful and lists all the configuration settings from the ``cassandra.yaml``.  The encryption options are overridden to hide the sensitive truststore information or passwords.  The configuration settings however cannot be set using DML  on the 
+virtual table presently. A total of 224 settings get listed presently. 
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.settings;
+
+ name                                                                   | value
+ -----------------------------------------------------------------------+-------------------- 
+   allocate_tokens_for_keyspace                                         |        null                                                                                          
+   audit_logging_options_audit_logs_dir                                 |  
+ /home/ec2-user/cassandra/logs/audit/                                                                                                       
+                                          audit_logging_options_enabled |                                                                                                                                         
+ false
+                              audit_logging_options_excluded_categories |                                                                                                                                              
+                               audit_logging_options_excluded_keyspaces |                                                                                                    
+ system,system_schema,system_virtual_schema
+                                   audit_logging_options_excluded_users |                                                                                                                                              
+                              audit_logging_options_included_categories |                                                                                                                                              
+                               audit_logging_options_included_keyspaces |                                                                                                                                              
+                                   audit_logging_options_included_users |                                                                                                                                              
+                                           audit_logging_options_logger |                                                                                                                                
+ BinAuditLogger
+                                                          authenticator |                                                                                                                         
+ AllowAllAuthenticator
+                                                             authorizer |                                                                                                                            
+ AllowAllAuthorizer
+                                                         auto_bootstrap |                                                                                                                                          
+ true
+                                                          auto_snapshot |                                                                                                                                          
+ true
+                                              automatic_sstable_upgrade |                                                                                                                                         
+ false
+                                                  back_pressure_enabled |                                                                                                                                         
+ false
+                                                 back_pressure_strategy |                                                           
+ org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}
+                                                      broadcast_address |                                                                                                                                          
+ null
+                                                  broadcast_rpc_address |                                                                                                                                          
+ null
+                                                           cluster_name |                                                                                                                                  
+ Test Cluster
+                                          column_index_cache_size_in_kb |                                                                                                                                             
+ 2
+                                                column_index_size_in_kb |                                                                                                                                            
+ 64
+                                                  commit_failure_policy |                                                                                                                                          
+ stop
+                                                  commitlog_compression |                                                                                                                                          
+ null
+                                                    commitlog_directory |                                                                                                       
+ /home/ec2-user/cassandra/data/commitlog
+                              commitlog_max_compression_buffers_in_pool |                                                                                                                                             
+ 3
+                                          commitlog_periodic_queue_size |                                                                                                                                            
+ -1
+                                           commitlog_segment_size_in_mb |                                                                                                                                            
+ 32
+                                                         commitlog_sync |                                                                                                                                      
+ periodic
+                                      commitlog_sync_batch_window_in_ms |                                                                                                                                           
+ NaN
+                                      commitlog_sync_group_window_in_ms |                                                                                                                                           
+ NaN
+                                            commitlog_sync_period_in_ms |                                                                                                                                         
+ 10000
+                                            commitlog_total_space_in_mb |                                                                                                                                          
+ 2556
+                        compaction_large_partition_warning_threshold_mb |                                                                                                                                           
+ 100
+                                       compaction_throughput_mb_per_sec |                                                                                                                                            
+ 16
+                                                  concurrent_compactors |                                                                                                                                             
+ 2
+                                              concurrent_counter_writes |                                                                                                                                            
+ 32
+                                  concurrent_materialized_view_builders |                                                                                                                                             
+ 1
+                                    concurrent_materialized_view_writes |                                                                                                                                            
+ 32
+                                                       concurrent_reads |                                                                                                                                            
+ 32
+                                                  concurrent_replicates |                                                                                                                                          
+ null
+                                                 concurrent_validations |                                                                                                                                    
+ 2147483647
+                                                      concurrent_writes |                                                                                                                                            
+ 32
+                                      credentials_update_interval_in_ms |                                                                                                                                            
+ -1
+                                             credentials_validity_in_ms |                                                                                                                                          
+ 2000
+                                                     cross_node_timeout |                                                                                                                                         
+ false
+                                                  data_file_directories |                                                                                                          
+ [/home/ec2-user/cassandra/data/data]
+                                              diagnostic_events_enabled |                                                                                                                                         
+ false
+                                                       disk_access_mode |                                                                                                                                          
+ mmap
+                                                    disk_failure_policy |                                                                                                                                          
+ stop
+                                              enable_materialized_views |                                                                                                                                         
+ false
+                                                    enable_sasi_indexes |                                                                                                                                         
+ false
+                                 enable_scripted_user_defined_functions |                                                                                                                                         
+ false
+                                           enable_transient_replication |                                                                                                                                         
+ false
+                                          enable_user_defined_functions |                                                                                                                                         
+ false
+                                  enable_user_defined_functions_threads |                                                                                                                                          
+ true
+                                                        endpoint_snitch |                                                                                                                                  
+ SimpleSnitch
+                                                    file_cache_round_up |                                                                                                                                         
+ false
+                                                  file_cache_size_in_mb |                                                                                                                                           
+ 251
+                                             full_query_logging_options | 
+ FullQueryLoggerOptions{log_dir='', archive_command='', roll_cycle='HOURLY', block=true, 
+ max_queue_weight=268435456, max_log_size=17179869184}
+                                                 gc_log_threshold_in_ms |                                                                                                                                           
+ 200
+                                                gc_warn_threshold_in_ms |                                                                                                                                          
+ 1000
+                                    hinted_handoff_disabled_datacenters |                                                                                                                                            
+ []
+                                                 hinted_handoff_enabled |                                                                                                                                          
+ true
+                                          hinted_handoff_throttle_in_kb |                                                                                                                                          
+ 1024
+                                                      hints_compression |                                                                                                                                          
+ null
+                                                        hints_directory |                                                                                                           
+ /home/ec2-user/cassandra/data/hints
+                                               hints_flush_period_in_ms |                                                                                                                                         
+ 10000
+                                                ideal_consistency_level |                                                                                                                                          
+ null
+                                                    incremental_backups |                                                                                                                                         
+ false
+                                                          initial_token |                                                                                                                                          
+ null
+                   inter_dc_stream_throughput_outbound_megabits_per_sec |                                                                                                                                           
+ 200
+                                                   inter_dc_tcp_nodelay |                                                                                                                                         
+ false
+                  internode_application_receive_queue_capacity_in_bytes |                                                                                                                                       
+ 4194304
+  internode_application_receive_queue_reserve_endpoint_capacity_in_bytes |                                                                                                                                     
+ 134217728
+  internode_application_receive_queue_reserve_global_capacity_in_bytes |                                                                                                                                     
+ 536870912
+                  internode_application_send_queue_capacity_in_bytes |                                                                                                                                       
+ 4194304
+ internode_application_send_queue_reserve_endpoint_capacity_in_bytes |                                                                                                                                     
+  134217728
+
+ 
+  internode_application_send_queue_reserve_global_capacity_in_bytes |                                                             
+  536870912
+                                           internode_authenticator |                                                                  
+ null
+                                             internode_compression |                                                                    
+ dc
+                                                    listen_address |                                                             
+ localhost
+                                                  listen_interface |                                                                  
+ null
+                                       listen_on_broadcast_address |                                                                 
+ false
+                         max_concurrent_automatic_sstable_upgrades |                                                                     
+ 1
+                                             max_hint_window_in_ms |                                                              
+ 10800000
+                                          memtable_allocation_type |                                                          
+ heap_buffers
+                                    min_free_space_per_drive_in_mb |                                                                    
+ 50
+                            native_transport_allow_older_protocols |                                                                  
+ true
+                          native_transport_flush_in_batches_legacy |                                                                 
+ false
+                           native_transport_frame_block_size_in_kb |                                                                    32
+                               native_transport_idle_timeout_in_ms |                                                                     0
+                       native_transport_max_concurrent_connections |                                                                    -1
+                native_transport_max_concurrent_connections_per_ip |                                                                    -1
+                 native_transport_max_concurrent_requests_in_bytes |                                                             105277030
+          native_transport_max_concurrent_requests_in_bytes_per_ip |                                                              26319257
+                             native_transport_max_frame_size_in_mb |                                                                   256
+                                      native_transport_max_threads |                                                                   128
+                                             native_transport_port |                                                                  9042
+                                         native_transport_port_ssl |                                                                  null
+                                                network_authorizer |                                             AllowAllNetworkAuthorizer
+                                                        num_tokens |                                                                   256
+                                                       partitioner |                           org.apache.cassandra.dht.Murmur3Partitioner
+                                 prepared_statements_cache_size_mb |                                                                  null
+                                       range_request_timeout_in_ms |                                                                 10000
+                                        read_request_timeout_in_ms |                                                                  5000
+                                 repair_command_pool_full_strategy |                                                                 queue
+                                          repair_command_pool_size |                                                            2147483647
+                                     repair_session_max_tree_depth |                                                                    20
+                                        repair_session_space_in_mb |                                                                    62
+                repaired_data_tracking_for_partition_reads_enabled |                                                                 false
+                    repaired_data_tracking_for_range_reads_enabled |                                                                 false
+                       report_unconfirmed_repaired_data_mismatches |                                                                 false
+                                             request_timeout_in_ms |                                                                 10000
+                                                      role_manager |                                                  CassandraRoleManager
+                                                       rpc_address |                                                             localhost
+                                                     rpc_interface |                                                                  null
+                                            saved_caches_directory |                            /home/ec2-user/cassandra/data/saved_caches
+                                                     seed_provider | org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1:7000}
+                               server_encryption_options_algorithm |                                                                  null
+                                server_encryption_options_protocol |                                                                   TLS
+                                      slow_query_log_timeout_in_ms |                                                                   500
+                                                  ssl_storage_port |                                                                  7001
+                                            start_native_transport |                                                                  true
+                                                      storage_port |                                                                  7000
+                                            stream_entire_sstables |                                                                  true
+               transparent_data_encryption_options_chunk_length_kb |                                                                    64
+                        transparent_data_encryption_options_cipher |                                                  AES/CBC/PKCS5Padding
+                       transparent_data_encryption_options_enabled |                                                                 false
+                     transparent_data_encryption_options_iv_length |                                                                    
+ 16
+ (224 rows)
+
+
+Thread Pools Virtual Table
+**************************
+
+The ``thread_pools`` table lists information about all thread pools. Thread pool information includes active tasks, active tasks limit, blocked tasks, blocked tasks all time,  completed tasks, and pending tasks. A query on the ``thread_pools`` returns following details.
+
+::
+
+ cqlsh:system_views> select * from system_views.thread_pools;
+
+ name                         | active_tasks | active_tasks_limit | blocked_tasks | blocked_tasks_all_time | completed_tasks | pending_tasks
+ ------------------------------+--------------+--------------------+---------------+------------------------+-----------------+---------------
+             AntiEntropyStage |            0 |                  1 |             0 |                      0 |               0 |             0
+         CacheCleanupExecutor |            0 |                  1 |             0 |                      0 |               0 |             0
+           CompactionExecutor |            0 |                  2 |             0 |                      0 |             881 |             0
+         CounterMutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+                  GossipStage |            0 |                  1 |             0 |                      0 |               0 |             0
+              HintsDispatcher |            0 |                  2 |             0 |                      0 |               0 |             0
+        InternalResponseStage |            0 |                  2 |             0 |                      0 |               0 |             0
+          MemtableFlushWriter |            0 |                  2 |             0 |                      0 |               1 |             0
+            MemtablePostFlush |            0 |                  1 |             0 |                      0 |               2 |             0
+        MemtableReclaimMemory |            0 |                  1 |             0 |                      0 |               1 |             0
+               MigrationStage |            0 |                  1 |             0 |                      0 |               0 |             0
+                    MiscStage |            0 |                  1 |             0 |                      0 |               0 |             0
+                MutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+    Native-Transport-Requests |            1 |                128 |             0 |                      0 |             130 |             0
+       PendingRangeCalculator |            0 |                  1 |             0 |                      0 |               1 |             0
+ PerDiskMemtableFlushWriter_0 |            0 |                  2 |             0 |                      0 |               1 |             0
+                    ReadStage |            0 |                 32 |             0 |                      0 |              13 |             0
+                  Repair-Task |            0 |         2147483647 |             0 |                      0 |               0 |             0
+         RequestResponseStage |            0 |                  2 |             0 |                      0 |               0 |             0
+                      Sampler |            0 |                  1 |             0 |                      0 |               0 |             0
+     SecondaryIndexManagement |            0 |                  1 |             0 |                      0 |               0 |             0
+           ValidationExecutor |            0 |         2147483647 |             0 |                      0 |               0 |             0
+            ViewBuildExecutor |            0 |                  1 |             0 |                      0 |               0 |             0
+            ViewMutationStage |            0 |                 32 |             0 |                      0 |               0 |             0
+
+(24 rows)
+
+Internode Inbound Messaging Virtual Table
+*****************************************
+
+The ``internode_inbound``  virtual table is for the internode inbound messaging. Initially no internode inbound messaging may get listed. In addition to the address, port, datacenter and rack information includes  corrupt frames recovered, corrupt frames unrecovered, error bytes, error count, expired bytes, expired count, processed bytes, processed count, received bytes, received count, scheduled bytes, scheduled count, throttled count, throttled nanos, using bytes, using reserve bytes. A query on the ``internode_inbound`` returns following details.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.internode_inbound;
+ address | port | dc | rack | corrupt_frames_recovered | corrupt_frames_unrecovered | 
+ error_bytes | error_count | expired_bytes | expired_count | processed_bytes | 
+ processed_count | received_bytes | received_count | scheduled_bytes | scheduled_count | throttled_count | throttled_nanos | using_bytes | using_reserve_bytes
+ ---------+------+----+------+--------------------------+----------------------------+- 
+ ----------
+ (0 rows)
+
+SSTables Tasks Virtual Table
+****************************
+
+As no sstable tasks are running initially the ``system_views.sstable_tasks`` table lists 0 rows.
+
+::
+
+ cqlsh:system_views> SELECT * FROM system_views.sstable_tasks;
+ keyspace_name | table_name | task_id | kind | progress | total | unit
+ ---------------+------------+---------+------+----------+-------+------
+
+ (0 rows)
+","[{'comment': 'maybe add entry about table metrics post https://issues.apache.org/jira/browse/CASSANDRA-15194 from with examples like:\r\n\r\n```\r\ncqlsh> SELECT * FROM disk_usage WHERE mebibytes > 1 ALLOW FILTERING;\r\n\r\n keyspace_name | table_name | mebibytes\r\n---------------+------------+-----------\r\n     keyspace1 |  standard1 |       288\r\n    tlp_stress |   keyvalue |      3211\r\n\r\ncqlsh> SELECT * FROM local_read_latency WHERE per_second > 1 ALLOW FILTERING;\r\n\r\n keyspace_name | table_name | p50th_ms | p99th_ms | count    | max_ms  | per_second\r\n---------------+------------+----------+----------+----------+---------+------------\r\n    tlp_stress |   keyvalue |    0.043 |    0.152 | 49785158 | 186.563 |  11418.356\r\n \r\n```', 'commenter': 'clohfink'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. ","[{'comment': 'Suggested re-wording to something like the following:\r\n```\r\nHints are a data repair technique applied during write operations. When         \r\nreplica nodes are unavailable to accept a mutation, either due to failure or    \r\nmore commonly routine maintenance, coordinators attempting to write to those    \r\nreplicas store temporary hints on their local filesystem for later application  \r\nto the unavailable replica. Hints are an important way to help reduce the       \r\nduration of data inconsistency between replicas as they replay quickly after    \r\nunavailable nodes return to the ring, however they are best effort and do not   \r\nguarantee eventual consistency like :ref:`anti-entropy repair <repair>` does. \r\n```', 'commenter': 'jolynch'}, {'comment': 'merged.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.","[{'comment': ""I'd slightly modify\r\n```\r\nHints are used to implement the eventual consistency guarantee ...\r\n```\r\nto be\r\n```\r\nHints are one of the primary ways Cassandra implements the eventual consistency guarantee ...\r\n```"", 'commenter': 'jolynch'}, {'comment': 'merged ""Hints are one of the three ways, in addition to read-repair and full anti-entropy repair, Cassandra implements the eventual consistency guarantee...""', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   ","[{'comment': ""General feedback: Can we link to one of the architecture pages here instead of repeating it?\r\n\r\nCopy feedback (my opinion):\r\n* I'd nix some of the expository copy like `As a primer on how replicas are placed in a cluster,`\r\n* I don't think you need to go into virtual nodes to explain hints. There are a set of physical `endpoints` which should be part of the `replica set` for a key, and when an `endpoint` (or `replica`) is unavailable hints have to be stored for those."", 'commenter': 'jolynch'}, {'comment': 'Added \r\nFor more details, refer TokenRingRanges_.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   
+
+Why are hints needed?
+=====================
+
+Hints reduce the inconsistency window caused by temporary node unavailability.
+
+Consider that an update or mutation is to be made using the following configuration:
+
+- Consistency level : 2","[{'comment': 'Consistency level: `LOCAL_QUORUM` (2/3)', 'commenter': 'jolynch'}, {'comment': 'merged.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   
+
+Why are hints needed?
+=====================
+
+Hints reduce the inconsistency window caused by temporary node unavailability.
+
+Consider that an update or mutation is to be made using the following configuration:
+
+- Consistency level : 2
+- Replication factor: 3
+- Replication strategy: SimpleStrategy
+- Number of nodes in cluster: 5
+
+The update or mutation is sent to a node (node A) in the cluster, and is meant to be forwarded to three other nodes, the replica nodes B, C and D.  The node that receives the request is the proxy node and becomes the coordinator of the request.  Under normal operation the update gets sent to the three replica nodes and the coordinator receives the response from the three nodes satisfying the consistency level.  But suppose node B is down and unavailable.  The update is sent to nodes C and D and a response returned to the coordinator, again satisfying the consistency level of 2.   But that is not the end of the request. Because the replica mutation is meant for replica node B also, a hint is stored by the coordinator node in the local filesystem   indicating that the update or mutation is also to be replicated on node B.  The coordinator node waits for 3 hours by default (as set with ``max_hint_window_in_ms``). If node B becomes available within 3 hours the coordinator sends the hint to node B and the hint is replayed on node B, eventually making all replicas consistent. Such a transfer of an update using hints is called a hinted handoff.  Hinted handoff is used to ensure that read and write operations are not failed and the consistency, availability and durability guarantees are not compromised.  We still need to satisfy the consistency level, because hints & hinted handoffs are not used to satisfy the write consistency level unless the consistency level is ``ANY``.  If the replica node for which a hint is generated does not become available within 3 hours, or the ``max_hint_window_in_ms``, the hint is deleted and a full or read repair becomes necessary.
+
+Hints for Timed Out Write Requests
+==================================
+
+Hints are also stored for write requests that are timed out. The ``write_request_timeout_in_ms`` setting in ``cassandra.yaml`` configures the timeout for write requests.
+
+::
+
+  write_request_timeout_in_ms: 2000
+
+The coordinator waits for the configured amount of time for write requests to complete and throws a ``TimeOutException``.  The coordinator node also generates a hint for the timed out request. Lowest acceptable value for ``write_request_timeout_in_ms`` is 10 ms.
+
+What data is stored in a hint?
+==============================
+
+Hints are stored in flat files in the coordinator node’s ``$CASSANDRA_HOME/data/hints`` directory. A hint includes a hint id, the target replica node on which the mutation is meant to be stored, the serialized mutation (stored as a blob) that couldn’t be delivered to the replica node, and the Cassandra version used to serialize the mutation. By default hints are compressed using ``LZ4Compress``. Multiple hints are appended to the same hints file.
+ 
+Replaying Hints
+===============
+
+Hints are streamed in bulk, a segment at a time, to the target replica node and the target node replays them locally. After the target node has replayed a segment it deletes the segment and receives the next segment.
+
+Configuring Hints
+=================
+
+Hints are enabled by default. The ``cassandra.yaml`` configuration file provides several settings for configuring hints as discussed in Table 1.
+
+Table 1. Settings for Hints
+
++----------------------+-------------------------------------------+-----------------+
+|Setting               | Description                               |Default Value    |
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff_enabled|Enables/Disables hinted handoffs           | true            |
+|                      |                                           |                 | 
+|                      |                                           |                 |
+|                      |                                           |                 |
+|                      |                                           |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff        |A list of data centers that do not perform |                 |
+|_disabled_datacenters |hinted handoffs even when hinted_handoff_  |                 | 
+|                      |enabled is set to true.                    |                 |
+|                      |Example:                                   |                 |
+|                      |hinted_handoff_disabled_datacenters:       |                 |
+|                      |                 - DC1                     |                 |
+|                      |                 - DC2|                    |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|max_hint_window_in_ms |Defines the maximum amount of time (ms)    |10800000 #3 hours|
+|                      |a node shall have hints generated after it |                 |
+|                      |has failed.                                |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff        |Maximum throttle in KBs per second, per    |                 |
+|_throttle_in_kb       |delivery thread. This will be reduced      | 1024            |
+|                      |proportionally to the number of nodes in   |                 | 
+|                      |the cluster.                               |                 |
+|                      |(If there are two nodes in the cluster,    |                 |
+|                      |each delivery thread will use the maximum  |                 |
+|                      |rate; if there are 3, each will throttle   |                 |
+|                      |to half of the maximum,since it is expected|                 |
+|                      |for two nodes to be delivering hints       |                 |
+|                      |simultaneously.)                           |                 |
++----------------------+-------------------------------------------+-----------------+
+|max_hints_delivery    |Number of threads with which to deliver    |     2           |
+|_threads              |hints; Consider increasing this number when|                 |
+|                      |you have multi-dc deployments, since       |                 |
+|                      |cross-dc handoff tends to be slower        |                 |
++----------------------+-------------------------------------------+-----------------+
+|hints_directory       |Directory where Cassandra stores hints.    |$CASSANDRA_HOME/ |
+|                      |                                           |data/hints       |
++----------------------+-------------------------------------------+-----------------+
+|hints_flush_period_in |How often hints should be flushed from the |  10000          |
+|_ms                   |internal buffers to disk. Will *not*       |                 |
+|                      |trigger fsync.                             |                 |
++----------------------+-------------------------------------------+-----------------+
+|max_hints_file_size   |Maximum size for a single hints file, in   |   128           |
+|_in_mb                |megabytes.                                 |                 |
++----------------------+-------------------------------------------+-----------------+
+|hints_compression     |Compression to apply to the hint files.    |  LZ4Compress    | 
+|                      |If omitted, hints files will be written    |                 |
+|                      |uncompressed. LZ4, Snappy, and Deflate     |                 |
+|                      |compressors are supported.                 |                 |
++----------------------+-------------------------------------------+-----------------+
+ 
+Changing Max Hint Window at Runtime
+===================================
+
+Cassandra 4.0 has added support for changing ``max_hint_window_in_ms`` at runtime 
+(`CASSANDRA-11720
+<https://issues.apache.org/jira/browse/CASSANDRA-11720>`_). The ``max_hint_window_in_ms`` configuration property in ``cassandra.yaml`` may be modified at runtime followed by a rolling restart. The default value of ``max_hint_window_in_ms`` is 3 hours.
+
+::
+
+  max_hint_window_in_ms: 10800000 # 3 hours
+
+The need to be able to modify ``max_hint_window_in_ms`` at runtime is explained with the following example.  A larger node (in terms of data it holds) goes down. And it will take slightly more than ``max_hint_window_in_ms`` to fix it. The disk space to store some additional hints id available.
+
+Added Histogram for Delay to deliver Hints
+==========================================
+
+Version 4.0 adds histograms available to understand how long it takes to deliver hints is useful for operators to better identify problems (`CASSANDRA-13234
+<https://issues.apache.org/jira/browse/CASSANDRA-13234>`_).
+ 
+Using nodetool for Configuring hints
+====================================
+
+The nodetool provides several commands for configuring hints or getting hints related information. The nodetool commands override the corresponding settings if any in ``cassandra.yaml``. These commands are discussed in Table 2.
+
+Table 2. Nodetool Commands for Hints
+
++----------------------------+-------------------------------------------+
+|Command                     | Description                               | 
++----------------------------+-------------------------------------------+
+|nodetool disablehandoff     |Disables storing hinted handoffs           |                                                               
++----------------------------+-------------------------------------------+
+|nodetool disablehintsfordc  |Disables hints for a data center           |                                                               
++----------------------------+-------------------------------------------+
+|nodetool enablehandoff      |Re-enables future hints storing on the     |
+|                            |current node                               |                                              
++----------------------------+-------------------------------------------+
+|nodetool enablehintsfordc   |Enables hints for a data center that was   |
+|                            |previously disabled                        | 
++----------------------------+-------------------------------------------+
+|nodetool getmaxhintwindow   |Prints the max hint window in ms.          |
+|                            |A new nodetool command in Cassandra 4.0.   |
++----------------------------+-------------------------------------------+
+|nodetool handoffwindow      |Prints current hinted handoff window       |
++----------------------------+-------------------------------------------+
+|nodetool pausehandoff       |Pauses hints delivery process              |                                                               
++----------------------------+-------------------------------------------+
+|nodetool resumehandoff      |Resumes hints delivery process             |                                                               
++----------------------------+-------------------------------------------+
+|nodetool                    |Sets hinted handoff throttle in kb         |
+|sethintedhandoffthrottlekb  |per second, per delivery thread            |                                                             
++----------------------------+-------------------------------------------+
+|nodetool setmaxhintwindow   |Sets the specified max hint window in ms   | 
++----------------------------+-------------------------------------------+
+|nodetool statushandoff      |Status of storing future hints on the      |
+|                            |current node                               |
++----------------------------+-------------------------------------------+
+|nodetool truncatehints      |Truncates all hints on the local node, or  |
+|                            |truncates hints for the endpoint(s)        |
+|                            |specified.                                 |
++----------------------------+-------------------------------------------+
+
+Hints is not an alternative to performing a full repair or read repair but is only a stopgap measure.","[{'comment': '```Hints, like read-repair, are not an alternative to performing full repair, but do help reduce the duration of inconsistency between replicas```', 'commenter': 'jolynch'}, {'comment': 'merged.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   
+
+Why are hints needed?
+=====================
+
+Hints reduce the inconsistency window caused by temporary node unavailability.
+
+Consider that an update or mutation is to be made using the following configuration:","[{'comment': '`Consider that a mutation is made with the following configuration`', 'commenter': 'jolynch'}, {'comment': 'merged.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   
+
+Why are hints needed?
+=====================
+
+Hints reduce the inconsistency window caused by temporary node unavailability.
+
+Consider that an update or mutation is to be made using the following configuration:
+
+- Consistency level : 2
+- Replication factor: 3
+- Replication strategy: SimpleStrategy
+- Number of nodes in cluster: 5
+
+The update or mutation is sent to a node (node A) in the cluster, and is meant to be forwarded to three other nodes, the replica nodes B, C and D.  The node that receives the request is the proxy node and becomes the coordinator of the request.  Under normal operation the update gets sent to the three replica nodes and the coordinator receives the response from the three nodes satisfying the consistency level.  But suppose node B is down and unavailable.  The update is sent to nodes C and D and a response returned to the coordinator, again satisfying the consistency level of 2.   But that is not the end of the request. Because the replica mutation is meant for replica node B also, a hint is stored by the coordinator node in the local filesystem   indicating that the update or mutation is also to be replicated on node B.  The coordinator node waits for 3 hours by default (as set with ``max_hint_window_in_ms``). If node B becomes available within 3 hours the coordinator sends the hint to node B and the hint is replayed on node B, eventually making all replicas consistent. Such a transfer of an update using hints is called a hinted handoff.  Hinted handoff is used to ensure that read and write operations are not failed and the consistency, availability and durability guarantees are not compromised.  We still need to satisfy the consistency level, because hints & hinted handoffs are not used to satisfy the write consistency level unless the consistency level is ``ANY``.  If the replica node for which a hint is generated does not become available within 3 hours, or the ``max_hint_window_in_ms``, the hint is deleted and a full or read repair becomes necessary.","[{'comment': 'A couple of suggestions:\r\n* I\'d omit replication strategy and number of nodes in the cluster for this example, the only thing we need to know is that we have a `LOCAL_QUORUM` request going to three replicas and one of the replicas does not acknowledge the write.\r\n* `are not failed and the consistency, availability and durability guarantees are not compromised.` I suggest you re-word this to something like ""hints ensure eventual consistency"".\r\n* Can you structure this as an ordered timeline with a diagram instead of a large paragraph? I think something like [this diagram](https://github.com/jolynch/cassandra/blob/hints/doc/source/operating/images/hints.svg) would help explain the concept.\r\n \r\n', 'commenter': 'jolynch'}, {'comment': 'merged.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   
+
+Why are hints needed?
+=====================
+
+Hints reduce the inconsistency window caused by temporary node unavailability.
+
+Consider that an update or mutation is to be made using the following configuration:
+
+- Consistency level : 2
+- Replication factor: 3
+- Replication strategy: SimpleStrategy
+- Number of nodes in cluster: 5
+
+The update or mutation is sent to a node (node A) in the cluster, and is meant to be forwarded to three other nodes, the replica nodes B, C and D.  The node that receives the request is the proxy node and becomes the coordinator of the request.  Under normal operation the update gets sent to the three replica nodes and the coordinator receives the response from the three nodes satisfying the consistency level.  But suppose node B is down and unavailable.  The update is sent to nodes C and D and a response returned to the coordinator, again satisfying the consistency level of 2.   But that is not the end of the request. Because the replica mutation is meant for replica node B also, a hint is stored by the coordinator node in the local filesystem   indicating that the update or mutation is also to be replicated on node B.  The coordinator node waits for 3 hours by default (as set with ``max_hint_window_in_ms``). If node B becomes available within 3 hours the coordinator sends the hint to node B and the hint is replayed on node B, eventually making all replicas consistent. Such a transfer of an update using hints is called a hinted handoff.  Hinted handoff is used to ensure that read and write operations are not failed and the consistency, availability and durability guarantees are not compromised.  We still need to satisfy the consistency level, because hints & hinted handoffs are not used to satisfy the write consistency level unless the consistency level is ``ANY``.  If the replica node for which a hint is generated does not become available within 3 hours, or the ``max_hint_window_in_ms``, the hint is deleted and a full or read repair becomes necessary.
+
+Hints for Timed Out Write Requests
+==================================
+
+Hints are also stored for write requests that are timed out. The ``write_request_timeout_in_ms`` setting in ``cassandra.yaml`` configures the timeout for write requests.","[{'comment': '`write requests that time out`', 'commenter': 'jolynch'}, {'comment': 'merged.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   
+
+Why are hints needed?","[{'comment': 'Change this from `=` to `-` headers, and perhaps re-word the title to `Hinted Handoff`.', 'commenter': 'jolynch'}, {'comment': 'merged.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   
+
+Why are hints needed?
+=====================
+
+Hints reduce the inconsistency window caused by temporary node unavailability.
+
+Consider that an update or mutation is to be made using the following configuration:
+
+- Consistency level : 2
+- Replication factor: 3
+- Replication strategy: SimpleStrategy
+- Number of nodes in cluster: 5
+
+The update or mutation is sent to a node (node A) in the cluster, and is meant to be forwarded to three other nodes, the replica nodes B, C and D.  The node that receives the request is the proxy node and becomes the coordinator of the request.  Under normal operation the update gets sent to the three replica nodes and the coordinator receives the response from the three nodes satisfying the consistency level.  But suppose node B is down and unavailable.  The update is sent to nodes C and D and a response returned to the coordinator, again satisfying the consistency level of 2.   But that is not the end of the request. Because the replica mutation is meant for replica node B also, a hint is stored by the coordinator node in the local filesystem   indicating that the update or mutation is also to be replicated on node B.  The coordinator node waits for 3 hours by default (as set with ``max_hint_window_in_ms``). If node B becomes available within 3 hours the coordinator sends the hint to node B and the hint is replayed on node B, eventually making all replicas consistent. Such a transfer of an update using hints is called a hinted handoff.  Hinted handoff is used to ensure that read and write operations are not failed and the consistency, availability and durability guarantees are not compromised.  We still need to satisfy the consistency level, because hints & hinted handoffs are not used to satisfy the write consistency level unless the consistency level is ``ANY``.  If the replica node for which a hint is generated does not become available within 3 hours, or the ``max_hint_window_in_ms``, the hint is deleted and a full or read repair becomes necessary.
+
+Hints for Timed Out Write Requests
+==================================
+
+Hints are also stored for write requests that are timed out. The ``write_request_timeout_in_ms`` setting in ``cassandra.yaml`` configures the timeout for write requests.
+
+::
+
+  write_request_timeout_in_ms: 2000
+
+The coordinator waits for the configured amount of time for write requests to complete and throws a ``TimeOutException``.  The coordinator node also generates a hint for the timed out request. Lowest acceptable value for ``write_request_timeout_in_ms`` is 10 ms.
+
+What data is stored in a hint?
+==============================
+
+Hints are stored in flat files in the coordinator node’s ``$CASSANDRA_HOME/data/hints`` directory. A hint includes a hint id, the target replica node on which the mutation is meant to be stored, the serialized mutation (stored as a blob) that couldn’t be delivered to the replica node, and the Cassandra version used to serialize the mutation. By default hints are compressed using ``LZ4Compress``. Multiple hints are appended to the same hints file.
+ 
+Replaying Hints
+===============
+
+Hints are streamed in bulk, a segment at a time, to the target replica node and the target node replays them locally. After the target node has replayed a segment it deletes the segment and receives the next segment.
+
+Configuring Hints
+=================
+
+Hints are enabled by default. The ``cassandra.yaml`` configuration file provides several settings for configuring hints as discussed in Table 1.
+
+Table 1. Settings for Hints
+
++----------------------+-------------------------------------------+-----------------+
+|Setting               | Description                               |Default Value    |
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff_enabled|Enables/Disables hinted handoffs           | true            |
+|                      |                                           |                 | 
+|                      |                                           |                 |
+|                      |                                           |                 |
+|                      |                                           |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff        |A list of data centers that do not perform |                 |
+|_disabled_datacenters |hinted handoffs even when hinted_handoff_  |                 | 
+|                      |enabled is set to true.                    |                 |
+|                      |Example:                                   |                 |
+|                      |hinted_handoff_disabled_datacenters:       |                 |
+|                      |                 - DC1                     |                 |
+|                      |                 - DC2|                    |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|max_hint_window_in_ms |Defines the maximum amount of time (ms)    |10800000 #3 hours|
+|                      |a node shall have hints generated after it |                 |
+|                      |has failed.                                |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff        |Maximum throttle in KBs per second, per    |                 |
+|_throttle_in_kb       |delivery thread. This will be reduced      | 1024            |
+|                      |proportionally to the number of nodes in   |                 | 
+|                      |the cluster.                               |                 |
+|                      |(If there are two nodes in the cluster,    |                 |
+|                      |each delivery thread will use the maximum  |                 |
+|                      |rate; if there are 3, each will throttle   |                 |
+|                      |to half of the maximum,since it is expected|                 |
+|                      |for two nodes to be delivering hints       |                 |
+|                      |simultaneously.)                           |                 |
++----------------------+-------------------------------------------+-----------------+
+|max_hints_delivery    |Number of threads with which to deliver    |     2           |
+|_threads              |hints; Consider increasing this number when|                 |
+|                      |you have multi-dc deployments, since       |                 |
+|                      |cross-dc handoff tends to be slower        |                 |
++----------------------+-------------------------------------------+-----------------+
+|hints_directory       |Directory where Cassandra stores hints.    |$CASSANDRA_HOME/ |
+|                      |                                           |data/hints       |
++----------------------+-------------------------------------------+-----------------+
+|hints_flush_period_in |How often hints should be flushed from the |  10000          |
+|_ms                   |internal buffers to disk. Will *not*       |                 |
+|                      |trigger fsync.                             |                 |
++----------------------+-------------------------------------------+-----------------+
+|max_hints_file_size   |Maximum size for a single hints file, in   |   128           |
+|_in_mb                |megabytes.                                 |                 |
++----------------------+-------------------------------------------+-----------------+
+|hints_compression     |Compression to apply to the hint files.    |  LZ4Compress    | 
+|                      |If omitted, hints files will be written    |                 |
+|                      |uncompressed. LZ4, Snappy, and Deflate     |                 |
+|                      |compressors are supported.                 |                 |
++----------------------+-------------------------------------------+-----------------+
+ 
+Changing Max Hint Window at Runtime
+===================================
+
+Cassandra 4.0 has added support for changing ``max_hint_window_in_ms`` at runtime 
+(`CASSANDRA-11720
+<https://issues.apache.org/jira/browse/CASSANDRA-11720>`_). The ``max_hint_window_in_ms`` configuration property in ``cassandra.yaml`` may be modified at runtime followed by a rolling restart. The default value of ``max_hint_window_in_ms`` is 3 hours.
+
+::
+
+  max_hint_window_in_ms: 10800000 # 3 hours
+
+The need to be able to modify ``max_hint_window_in_ms`` at runtime is explained with the following example.  A larger node (in terms of data it holds) goes down. And it will take slightly more than ``max_hint_window_in_ms`` to fix it. The disk space to store some additional hints id available.","[{'comment': ""This is not clear, let's re-word it."", 'commenter': 'jolynch'}, {'comment': 'modified.', 'commenter': 'Deepak-Vohra'}]"
419,doc/source/operating/hints.rst,"@@ -1,22 +1,184 @@
-.. Licensed to the Apache Software Foundation (ASF) under one
-.. or more contributor license agreements.  See the NOTICE file
-.. distributed with this work for additional information
-.. regarding copyright ownership.  The ASF licenses this file
-.. to you under the Apache License, Version 2.0 (the
-.. ""License""); you may not use this file except in compliance
-.. with the License.  You may obtain a copy of the License at
-..
-..     http://www.apache.org/licenses/LICENSE-2.0
-..
-.. Unless required by applicable law or agreed to in writing, software
-.. distributed under the License is distributed on an ""AS IS"" BASIS,
-.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-.. See the License for the specific language governing permissions and
-.. limitations under the License.
-
-.. highlight:: none
-
-Hints
------
-
-.. todo:: todo
+.. Licensed to the Apache Software Foundation (ASF) under one
+.. or more contributor license agreements.  See the NOTICE file
+.. distributed with this work for additional information
+.. regarding copyright ownership.  The ASF licenses this file
+.. to you under the Apache License, Version 2.0 (the
+.. ""License""); you may not use this file except in compliance
+.. with the License.  You may obtain a copy of the License at
+..
+..     http://www.apache.org/licenses/LICENSE-2.0
+..
+.. Unless required by applicable law or agreed to in writing, software
+.. distributed under the License is distributed on an ""AS IS"" BASIS,
+.. WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+.. See the License for the specific language governing permissions and
+.. limitations under the License.
+
+.. highlight:: none
+
+Hints
+=====
+
+Hints are a type of repair during a write operation. At times a write or an update cannot be replicated to all nodes satisfying the replication factor because a replica node is unavailable. Under such a condition the mutation (a write or update) is stored temporarily on the coordinator node in its filesystem. 
+
+Hints are metadata associated with a mutation (a write or update) indicating that the mutation is not placed on a replica node (the target node) it is meant to be placed on because the node is temporarily unavailable, or is unresponsive.  Hints are used to implement the eventual consistency guarantee that all updates are eventually received by all replicas and all replicas are eventually made consistent.    When the replica node becomes available the hints are replayed on the node.
+
+As a primer on how replicas are placed in a cluster, Apache Cassandra replicates data to provide fault tolerance, high availability and durability. Cassandra partitions data across the cluster using consistent hashing in which a hash function is used on the partition keys to generate consistently ordered hash values (or tokens).  An abstract ring represents the complete hash value range (token range) of the keys stored with each node in the cluster being assigned a certain subset range of hash values (range of tokens) it can store.  The list of nodes responsible for a particular key is called its preference list.  The preference list may include virtual nodes as a virtual node is also a node albeit an abstract node and not a physical node.  Virtual nodes may need to be skipped to create a preference list in which the first N (N being the replication factor) nodes taken clockwise in the consistent hashing ring are all distinct physical nodes. All nodes in a cluster know which node/s should be in the preference list for a given key.  The node that receives a request for a write operation (key/value data) forwards the request to the replica node that is in the preference list for the key.  The node becomes a coordinator node and coordinates the reads and writes.   
+
+Why are hints needed?
+=====================
+
+Hints reduce the inconsistency window caused by temporary node unavailability.
+
+Consider that an update or mutation is to be made using the following configuration:
+
+- Consistency level : 2
+- Replication factor: 3
+- Replication strategy: SimpleStrategy
+- Number of nodes in cluster: 5
+
+The update or mutation is sent to a node (node A) in the cluster, and is meant to be forwarded to three other nodes, the replica nodes B, C and D.  The node that receives the request is the proxy node and becomes the coordinator of the request.  Under normal operation the update gets sent to the three replica nodes and the coordinator receives the response from the three nodes satisfying the consistency level.  But suppose node B is down and unavailable.  The update is sent to nodes C and D and a response returned to the coordinator, again satisfying the consistency level of 2.   But that is not the end of the request. Because the replica mutation is meant for replica node B also, a hint is stored by the coordinator node in the local filesystem   indicating that the update or mutation is also to be replicated on node B.  The coordinator node waits for 3 hours by default (as set with ``max_hint_window_in_ms``). If node B becomes available within 3 hours the coordinator sends the hint to node B and the hint is replayed on node B, eventually making all replicas consistent. Such a transfer of an update using hints is called a hinted handoff.  Hinted handoff is used to ensure that read and write operations are not failed and the consistency, availability and durability guarantees are not compromised.  We still need to satisfy the consistency level, because hints & hinted handoffs are not used to satisfy the write consistency level unless the consistency level is ``ANY``.  If the replica node for which a hint is generated does not become available within 3 hours, or the ``max_hint_window_in_ms``, the hint is deleted and a full or read repair becomes necessary.
+
+Hints for Timed Out Write Requests
+==================================
+
+Hints are also stored for write requests that are timed out. The ``write_request_timeout_in_ms`` setting in ``cassandra.yaml`` configures the timeout for write requests.
+
+::
+
+  write_request_timeout_in_ms: 2000
+
+The coordinator waits for the configured amount of time for write requests to complete and throws a ``TimeOutException``.  The coordinator node also generates a hint for the timed out request. Lowest acceptable value for ``write_request_timeout_in_ms`` is 10 ms.
+
+What data is stored in a hint?
+==============================
+
+Hints are stored in flat files in the coordinator node’s ``$CASSANDRA_HOME/data/hints`` directory. A hint includes a hint id, the target replica node on which the mutation is meant to be stored, the serialized mutation (stored as a blob) that couldn’t be delivered to the replica node, and the Cassandra version used to serialize the mutation. By default hints are compressed using ``LZ4Compress``. Multiple hints are appended to the same hints file.
+ 
+Replaying Hints
+===============
+
+Hints are streamed in bulk, a segment at a time, to the target replica node and the target node replays them locally. After the target node has replayed a segment it deletes the segment and receives the next segment.
+
+Configuring Hints
+=================
+
+Hints are enabled by default. The ``cassandra.yaml`` configuration file provides several settings for configuring hints as discussed in Table 1.
+
+Table 1. Settings for Hints
+
++----------------------+-------------------------------------------+-----------------+
+|Setting               | Description                               |Default Value    |
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff_enabled|Enables/Disables hinted handoffs           | true            |
+|                      |                                           |                 | 
+|                      |                                           |                 |
+|                      |                                           |                 |
+|                      |                                           |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff        |A list of data centers that do not perform |                 |
+|_disabled_datacenters |hinted handoffs even when hinted_handoff_  |                 | 
+|                      |enabled is set to true.                    |                 |
+|                      |Example:                                   |                 |
+|                      |hinted_handoff_disabled_datacenters:       |                 |
+|                      |                 - DC1                     |                 |
+|                      |                 - DC2|                    |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|max_hint_window_in_ms |Defines the maximum amount of time (ms)    |10800000 #3 hours|
+|                      |a node shall have hints generated after it |                 |
+|                      |has failed.                                |                 |                                                   
++----------------------+-------------------------------------------+-----------------+
+|hinted_handoff        |Maximum throttle in KBs per second, per    |                 |
+|_throttle_in_kb       |delivery thread. This will be reduced      | 1024            |
+|                      |proportionally to the number of nodes in   |                 | 
+|                      |the cluster.                               |                 |
+|                      |(If there are two nodes in the cluster,    |                 |
+|                      |each delivery thread will use the maximum  |                 |
+|                      |rate; if there are 3, each will throttle   |                 |
+|                      |to half of the maximum,since it is expected|                 |
+|                      |for two nodes to be delivering hints       |                 |
+|                      |simultaneously.)                           |                 |
++----------------------+-------------------------------------------+-----------------+
+|max_hints_delivery    |Number of threads with which to deliver    |     2           |
+|_threads              |hints; Consider increasing this number when|                 |
+|                      |you have multi-dc deployments, since       |                 |
+|                      |cross-dc handoff tends to be slower        |                 |
++----------------------+-------------------------------------------+-----------------+
+|hints_directory       |Directory where Cassandra stores hints.    |$CASSANDRA_HOME/ |
+|                      |                                           |data/hints       |
++----------------------+-------------------------------------------+-----------------+
+|hints_flush_period_in |How often hints should be flushed from the |  10000          |
+|_ms                   |internal buffers to disk. Will *not*       |                 |
+|                      |trigger fsync.                             |                 |
++----------------------+-------------------------------------------+-----------------+
+|max_hints_file_size   |Maximum size for a single hints file, in   |   128           |
+|_in_mb                |megabytes.                                 |                 |
++----------------------+-------------------------------------------+-----------------+
+|hints_compression     |Compression to apply to the hint files.    |  LZ4Compress    | 
+|                      |If omitted, hints files will be written    |                 |
+|                      |uncompressed. LZ4, Snappy, and Deflate     |                 |
+|                      |compressors are supported.                 |                 |
++----------------------+-------------------------------------------+-----------------+
+ 
+Changing Max Hint Window at Runtime","[{'comment': ""Can we change this section to talk about when you may want more time for hints to play instead of changing max hint window at runtime? It's actually somewhat rare for nodes to be down for more than three hours but its very common for hints playing at 1024 `kbps` cannot complete before 3 hours.\r\n\r\nYou could mention raising the `hinted_handoff_throttle` as well as raising the window to ensure hints are delivered."", 'commenter': 'jolynch'}, {'comment': 'modified.', 'commenter': 'Deepak-Vohra'}]"
446,test/distributed/org/apache/cassandra/distributed/impl/Instance.java,"@@ -374,6 +375,8 @@ public void startup(ICluster cluster)
                 }
                 JVMStabilityInspector.replaceKiller(new InstanceKiller());
 
+                ActiveRepairService.instance.start();","[{'comment': 'this enables IR', 'commenter': 'dcapwell'}, {'comment': ""Do we want to hide this behind a feature flag? I'd say yes."", 'commenter': 'ifesdjeen'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -91,10 +101,15 @@
     private final String keyspace;
 
     private final String tag;
-    private final AtomicInteger progress = new AtomicInteger();
+    private final AtomicInteger progressCounter = new AtomicInteger();
     private final int totalProgress;
 
+    private final long creationTimeMillis = System.currentTimeMillis();","[{'comment': '`System.currentTimeMillis();` prevents us from Injecting and mocking the clock. Could you evaluate whether we can replace these calls with `MonotonicClock`?', 'commenter': 'dineshjoshi'}, {'comment': 'Rather not in this patch since I don’t test time. \r\n\r\nBetter argument for the parent jira since it tracks state change time ', 'commenter': 'dcapwell'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);","[{'comment': 'Can be `final`.', 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
+
+        // Note we rely on the first message being the reason for the failure
+        // when inspecting this state from RepairRunner.queryForCompletedRepair
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.FAILED,
+                                                        ImmutableList.of(reason, completionMessage));
+
+        complete(completionMessage);
+    }
 
-    protected void runMayThrow() throws Exception
+    private void complete(String msg)
     {
-        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
-        final TraceState traceState;
-        final UUID parentSession = UUIDGen.getTimeUUID();
-        final String tag = ""repair:"" + cmd;
+        long durationMillis = System.currentTimeMillis() - creationTimeMillis;","[{'comment': 'Can be `final`.', 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
+
+        // Note we rely on the first message being the reason for the failure
+        // when inspecting this state from RepairRunner.queryForCompletedRepair
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.FAILED,
+                                                        ImmutableList.of(reason, completionMessage));
+
+        complete(completionMessage);
+    }
 
-    protected void runMayThrow() throws Exception
+    private void complete(String msg)
     {
-        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
-        final TraceState traceState;
-        final UUID parentSession = UUIDGen.getTimeUUID();
-        final String tag = ""repair:"" + cmd;
+        long durationMillis = System.currentTimeMillis() - creationTimeMillis;
+        if (msg == null) {
+            String duration = DurationFormatUtils.formatDurationWords(durationMillis,true, true);","[{'comment': 'Can be `final`.\r\n\r\nNit: Need space between `,` & `true`', 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
+
+        // Note we rely on the first message being the reason for the failure
+        // when inspecting this state from RepairRunner.queryForCompletedRepair
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.FAILED,
+                                                        ImmutableList.of(reason, completionMessage));
+
+        complete(completionMessage);
+    }
 
-    protected void runMayThrow() throws Exception
+    private void complete(String msg)
     {
-        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
-        final TraceState traceState;
-        final UUID parentSession = UUIDGen.getTimeUUID();
-        final String tag = ""repair:"" + cmd;
+        long durationMillis = System.currentTimeMillis() - creationTimeMillis;
+        if (msg == null) {
+            String duration = DurationFormatUtils.formatDurationWords(durationMillis,true, true);
+            msg = String.format(""Repair command #%d finished in %s"", cmd, duration);
+        }
 
-        final AtomicInteger progress = new AtomicInteger();
-        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair
+        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCounter.get(), totalProgress, msg));
 
-        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);
-        Iterable<ColumnFamilyStore> validColumnFamilies;
-        try
+        if (ActiveRepairService.instance.hasParentRepairSession(parentSession))
+            ActiveRepairService.instance.removeParentRepairSession(parentSession);
+        Context ctx = context;","[{'comment': 'Can be `final`.', 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
+
+        // Note we rely on the first message being the reason for the failure
+        // when inspecting this state from RepairRunner.queryForCompletedRepair
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.FAILED,
+                                                        ImmutableList.of(reason, completionMessage));
+
+        complete(completionMessage);
+    }
 
-    protected void runMayThrow() throws Exception
+    private void complete(String msg)
     {
-        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
-        final TraceState traceState;
-        final UUID parentSession = UUIDGen.getTimeUUID();
-        final String tag = ""repair:"" + cmd;
+        long durationMillis = System.currentTimeMillis() - creationTimeMillis;
+        if (msg == null) {
+            String duration = DurationFormatUtils.formatDurationWords(durationMillis,true, true);
+            msg = String.format(""Repair command #%d finished in %s"", cmd, duration);
+        }
 
-        final AtomicInteger progress = new AtomicInteger();
-        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair
+        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCounter.get(), totalProgress, msg));
 
-        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);
-        Iterable<ColumnFamilyStore> validColumnFamilies;
-        try
+        if (ActiveRepairService.instance.hasParentRepairSession(parentSession))
+            ActiveRepairService.instance.removeParentRepairSession(parentSession);
+        Context ctx = context;
+        if (options.isTraced() && ctx != null && ctx.traceState != null)
         {
-            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);
-            progress.incrementAndGet();
+            for (ProgressListener listener : listeners)
+                ctx.traceState.removeProgressListener(listener);
+            // Because DebuggableThreadPoolExecutor#afterExecute and this callback
+            // run in a nondeterministic order (within the same thread), the
+            // TraceState may have been nulled out at this point. The TraceState
+            // should be traceState, so just set it without bothering to check if it
+            // actually was nulled out.
+            Tracing.instance.set(ctx.traceState);
+            Tracing.traceRepair(msg);
+            Tracing.instance.stopSession();
+        }
+
+        Keyspace.open(keyspace).metric.repairTime.update(durationMillis, TimeUnit.MILLISECONDS);
+    }
+
+    public void run()
+    {
+        try {
+            // Why is this before setup/start when its publishing the start event?  For backwards compatability
+            // One of the first things we did before was publish this before validating, so publish early to keep
+            // that the same.
+            String message = String.format(""Starting repair command #%d (%s), repairing keyspace %s with %s"", cmd, parentSession, keyspace,","[{'comment': 'Can be `final`.', 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
+
+        // Note we rely on the first message being the reason for the failure
+        // when inspecting this state from RepairRunner.queryForCompletedRepair
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.FAILED,
+                                                        ImmutableList.of(reason, completionMessage));
+
+        complete(completionMessage);
+    }
 
-    protected void runMayThrow() throws Exception
+    private void complete(String msg)
     {
-        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
-        final TraceState traceState;
-        final UUID parentSession = UUIDGen.getTimeUUID();
-        final String tag = ""repair:"" + cmd;
+        long durationMillis = System.currentTimeMillis() - creationTimeMillis;
+        if (msg == null) {
+            String duration = DurationFormatUtils.formatDurationWords(durationMillis,true, true);
+            msg = String.format(""Repair command #%d finished in %s"", cmd, duration);
+        }
 
-        final AtomicInteger progress = new AtomicInteger();
-        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair
+        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCounter.get(), totalProgress, msg));
 
-        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);
-        Iterable<ColumnFamilyStore> validColumnFamilies;
-        try
+        if (ActiveRepairService.instance.hasParentRepairSession(parentSession))
+            ActiveRepairService.instance.removeParentRepairSession(parentSession);
+        Context ctx = context;
+        if (options.isTraced() && ctx != null && ctx.traceState != null)
         {
-            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);
-            progress.incrementAndGet();
+            for (ProgressListener listener : listeners)
+                ctx.traceState.removeProgressListener(listener);
+            // Because DebuggableThreadPoolExecutor#afterExecute and this callback
+            // run in a nondeterministic order (within the same thread), the
+            // TraceState may have been nulled out at this point. The TraceState
+            // should be traceState, so just set it without bothering to check if it
+            // actually was nulled out.
+            Tracing.instance.set(ctx.traceState);
+            Tracing.traceRepair(msg);
+            Tracing.instance.stopSession();
+        }
+
+        Keyspace.open(keyspace).metric.repairTime.update(durationMillis, TimeUnit.MILLISECONDS);
+    }
+
+    public void run()
+    {
+        try {
+            // Why is this before setup/start when its publishing the start event?  For backwards compatability
+            // One of the first things we did before was publish this before validating, so publish early to keep
+            // that the same.
+            String message = String.format(""Starting repair command #%d (%s), repairing keyspace %s with %s"", cmd, parentSession, keyspace,
+                                           options);
+            logger.info(message);
+            Tracing.traceRepair(message);
+            fireProgressEvent(new ProgressEvent(ProgressEventType.START, 0, 100, message));
+
+            Pair<Context, String> setup = setup();","[{'comment': ""Its not immediately obvious to the reader what the `Pair` represents. I would prefer if you had a nested static class with descriptive fields like you've done with the `Context`."", 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
+
+        // Note we rely on the first message being the reason for the failure
+        // when inspecting this state from RepairRunner.queryForCompletedRepair
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.FAILED,
+                                                        ImmutableList.of(reason, completionMessage));
+
+        complete(completionMessage);
+    }
 
-    protected void runMayThrow() throws Exception
+    private void complete(String msg)
     {
-        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
-        final TraceState traceState;
-        final UUID parentSession = UUIDGen.getTimeUUID();
-        final String tag = ""repair:"" + cmd;
+        long durationMillis = System.currentTimeMillis() - creationTimeMillis;
+        if (msg == null) {
+            String duration = DurationFormatUtils.formatDurationWords(durationMillis,true, true);
+            msg = String.format(""Repair command #%d finished in %s"", cmd, duration);
+        }
 
-        final AtomicInteger progress = new AtomicInteger();
-        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair
+        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCounter.get(), totalProgress, msg));
 
-        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);
-        Iterable<ColumnFamilyStore> validColumnFamilies;
-        try
+        if (ActiveRepairService.instance.hasParentRepairSession(parentSession))
+            ActiveRepairService.instance.removeParentRepairSession(parentSession);
+        Context ctx = context;
+        if (options.isTraced() && ctx != null && ctx.traceState != null)
         {
-            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);
-            progress.incrementAndGet();
+            for (ProgressListener listener : listeners)
+                ctx.traceState.removeProgressListener(listener);
+            // Because DebuggableThreadPoolExecutor#afterExecute and this callback
+            // run in a nondeterministic order (within the same thread), the
+            // TraceState may have been nulled out at this point. The TraceState
+            // should be traceState, so just set it without bothering to check if it
+            // actually was nulled out.
+            Tracing.instance.set(ctx.traceState);
+            Tracing.traceRepair(msg);
+            Tracing.instance.stopSession();
+        }
+
+        Keyspace.open(keyspace).metric.repairTime.update(durationMillis, TimeUnit.MILLISECONDS);
+    }
+
+    public void run()
+    {
+        try {
+            // Why is this before setup/start when its publishing the start event?  For backwards compatability
+            // One of the first things we did before was publish this before validating, so publish early to keep
+            // that the same.
+            String message = String.format(""Starting repair command #%d (%s), repairing keyspace %s with %s"", cmd, parentSession, keyspace,
+                                           options);
+            logger.info(message);
+            Tracing.traceRepair(message);
+            fireProgressEvent(new ProgressEvent(ProgressEventType.START, 0, 100, message));
+
+            Pair<Context, String> setup = setup();
+            if (setup.right != null)
+            {
+                skip(setup.right);
+                return;
+            }
+            Context ctx = setup.left;
+            assert ctx != null : ""Context is required but was not found"";
+            this.context = ctx;
+            start(ctx);
         }
-        catch (IllegalArgumentException | IOException e)
+        catch (Exception | Error e)
         {
-            logger.error(""Repair {} failed:"", parentSession, e);
-            fireErrorAndComplete(progress.get(), totalProgress, e.getMessage());
-            return;
+            notifyError(e);
+            fail(e.getMessage());
         }
+    }
+
+    private Pair<Context, String> setup() throws Exception
+    {
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
+
+        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);","[{'comment': 'Can be `final`.', 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
+
+        // Note we rely on the first message being the reason for the failure
+        // when inspecting this state from RepairRunner.queryForCompletedRepair
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.FAILED,
+                                                        ImmutableList.of(reason, completionMessage));
+
+        complete(completionMessage);
+    }
 
-    protected void runMayThrow() throws Exception
+    private void complete(String msg)
     {
-        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
-        final TraceState traceState;
-        final UUID parentSession = UUIDGen.getTimeUUID();
-        final String tag = ""repair:"" + cmd;
+        long durationMillis = System.currentTimeMillis() - creationTimeMillis;
+        if (msg == null) {
+            String duration = DurationFormatUtils.formatDurationWords(durationMillis,true, true);
+            msg = String.format(""Repair command #%d finished in %s"", cmd, duration);
+        }
 
-        final AtomicInteger progress = new AtomicInteger();
-        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair
+        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCounter.get(), totalProgress, msg));
 
-        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);
-        Iterable<ColumnFamilyStore> validColumnFamilies;
-        try
+        if (ActiveRepairService.instance.hasParentRepairSession(parentSession))
+            ActiveRepairService.instance.removeParentRepairSession(parentSession);
+        Context ctx = context;
+        if (options.isTraced() && ctx != null && ctx.traceState != null)
         {
-            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);
-            progress.incrementAndGet();
+            for (ProgressListener listener : listeners)
+                ctx.traceState.removeProgressListener(listener);
+            // Because DebuggableThreadPoolExecutor#afterExecute and this callback
+            // run in a nondeterministic order (within the same thread), the
+            // TraceState may have been nulled out at this point. The TraceState
+            // should be traceState, so just set it without bothering to check if it
+            // actually was nulled out.
+            Tracing.instance.set(ctx.traceState);
+            Tracing.traceRepair(msg);
+            Tracing.instance.stopSession();
+        }
+
+        Keyspace.open(keyspace).metric.repairTime.update(durationMillis, TimeUnit.MILLISECONDS);
+    }
+
+    public void run()
+    {
+        try {
+            // Why is this before setup/start when its publishing the start event?  For backwards compatability
+            // One of the first things we did before was publish this before validating, so publish early to keep
+            // that the same.
+            String message = String.format(""Starting repair command #%d (%s), repairing keyspace %s with %s"", cmd, parentSession, keyspace,
+                                           options);
+            logger.info(message);
+            Tracing.traceRepair(message);
+            fireProgressEvent(new ProgressEvent(ProgressEventType.START, 0, 100, message));
+
+            Pair<Context, String> setup = setup();
+            if (setup.right != null)
+            {
+                skip(setup.right);
+                return;
+            }
+            Context ctx = setup.left;
+            assert ctx != null : ""Context is required but was not found"";
+            this.context = ctx;
+            start(ctx);
         }
-        catch (IllegalArgumentException | IOException e)
+        catch (Exception | Error e)
         {
-            logger.error(""Repair {} failed:"", parentSession, e);
-            fireErrorAndComplete(progress.get(), totalProgress, e.getMessage());
-            return;
+            notifyError(e);
+            fail(e.getMessage());
         }
+    }
+
+    private Pair<Context, String> setup() throws Exception
+    {
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
+
+        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);
+        Iterable<ColumnFamilyStore> validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);","[{'comment': 'Can be `final`.', 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -199,63 +286,39 @@ protected void runMayThrow() throws Exception
         }
         else
         {
-            fireProgressEvent(new ProgressEvent(ProgressEventType.START, 0, 100, message));
             traceState = null;
         }
 
         Set<InetAddressAndPort> allNeighbors = new HashSet<>();
         List<CommonRange> commonRanges = new ArrayList<>();
 
-        try
-        {
-            //pre-calculate output of getLocalReplicas and pass it to getNeighbors to increase performance and prevent
-            //calculation multiple times
-            Iterable<Range<Token>> keyspaceLocalRanges = storageService.getLocalReplicas(keyspace).ranges();
+        //pre-calculate output of getLocalReplicas and pass it to getNeighbors to increase performance and prevent
+        //calculation multiple times
+        Iterable<Range<Token>> keyspaceLocalRanges = storageService.getLocalReplicas(keyspace).ranges();
 
-            for (Range<Token> range : options.getRanges())
-            {
-                EndpointsForRange neighbors = ActiveRepairService.getNeighbors(keyspace, keyspaceLocalRanges, range,
-                                                                               options.getDataCenters(),
-                                                                               options.getHosts());
-
-                addRangeToNeighbors(commonRanges, range, neighbors);
-                allNeighbors.addAll(neighbors.endpoints());
-            }
-
-            progress.incrementAndGet();
-        }
-        catch (IllegalArgumentException e)
+        for (Range<Token> range : options.getRanges())
         {
-            logger.error(""Repair {} failed:"", parentSession, e);
-            fireErrorAndComplete(progress.get(), totalProgress, e.getMessage());
-            return;
+            EndpointsForRange neighbors = ActiveRepairService.getNeighbors(keyspace, keyspaceLocalRanges, range,","[{'comment': 'Can be `final`.', 'commenter': 'dineshjoshi'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -418,6 +481,12 @@ public void onSuccess(List<RepairSessionResult> results)
             {
                 try
                 {
+                    if (results == null || results.stream().anyMatch(s -> s == null))
+                    {
+                        // something failed
+                        fail(null);","[{'comment': '""Something failed"" is not very useful to the operator. Can we instead have a descriptive message so the operator can tell the difference between failing at this location vs failing at line [634](https://github.com/apache/cassandra/pull/446/files#diff-4bc513a60150419e20a9449c70a64a66R634)?', 'commenter': 'dineshjoshi'}, {'comment': 'In this case no without a larger rewrite. All paths of repair rely on list of futures to future as list where null means “error”. To propagate error is more invasive and would prefer in the JIRA about repair hanging.\r\n\r\nI did change things a little bit. The errors are sent via notification (lossy) so may see a more detailed error; this is a fall back if notification was lost. \r\n\r\nI test with and without notification for reasons like this', 'commenter': 'dcapwell'}]"
446,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -131,53 +146,128 @@ protected void fireProgressEvent(ProgressEvent event)
         }
     }
 
-    protected void fireErrorAndComplete(int progressCount, int totalProgress, String message)
+    private void skip(String msg)
+    {
+        logger.info(""Repair {} skipped: {}"", parentSession, msg);
+        fireProgressEvent(new ProgressEvent(ProgressEventType.NOTIFICATION, 100, 100, ""Repair "" + parentSession + "" skipped: "" + msg));
+
+        success(msg);
+    }
+
+    private void success(String msg)
     {
+        fireProgressEvent(new ProgressEvent(ProgressEventType.SUCCESS, progressCounter.get(), totalProgress, msg));
+        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.COMPLETED,
+                                                        ImmutableList.of(msg));
+        complete(null);
+    }
+
+    public void notifyError(Throwable error)
+    {
+        // exception should be ignored
+        if (error instanceof SomeRepairFailedException)
+            return;
+        logger.error(""Repair {} failed:"", parentSession, error);
+
         StorageMetrics.repairExceptions.inc();
-        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, message);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, errorMessage));
-        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
-        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, completionMessage));
-        recordFailure(errorMessage, completionMessage);
+        String errorMessage = String.format(""Repair command #%d failed with error %s"", cmd, error.getMessage());
+        fireProgressEvent(new ProgressEvent(ProgressEventType.ERROR, progressCounter.get(), totalProgress, errorMessage));
+
+        // since this can fail, update table only after updating in-memory and notification state
+        if (!options.isPreview())
+        {
+            SystemDistributedKeyspace.failParentRepair(parentSession, error);
+        }
     }
 
+    private void fail(String reason)
+    {
+        if (reason == null)
+            reason = ""Some repair failed"";
+        String completionMessage = String.format(""Repair command #%d finished with error"", cmd);
+
+        // Note we rely on the first message being the reason for the failure
+        // when inspecting this state from RepairRunner.queryForCompletedRepair
+        ActiveRepairService.instance.recordRepairStatus(cmd, ParentRepairStatus.FAILED,
+                                                        ImmutableList.of(reason, completionMessage));
+
+        complete(completionMessage);
+    }
 
-    protected void runMayThrow() throws Exception
+    private void complete(String msg)
     {
-        ActiveRepairService.instance.recordRepairStatus(cmd, ActiveRepairService.ParentRepairStatus.IN_PROGRESS, ImmutableList.of());
-        final TraceState traceState;
-        final UUID parentSession = UUIDGen.getTimeUUID();
-        final String tag = ""repair:"" + cmd;
+        long durationMillis = System.currentTimeMillis() - creationTimeMillis;
+        if (msg == null) {
+            String duration = DurationFormatUtils.formatDurationWords(durationMillis,true, true);
+            msg = String.format(""Repair command #%d finished in %s"", cmd, duration);
+        }
 
-        final AtomicInteger progress = new AtomicInteger();
-        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair
+        fireProgressEvent(new ProgressEvent(ProgressEventType.COMPLETE, progressCounter.get(), totalProgress, msg));
 
-        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);
-        Iterable<ColumnFamilyStore> validColumnFamilies;
-        try
+        if (ActiveRepairService.instance.hasParentRepairSession(parentSession))","[{'comment': ""This code is inherently race prone as you're actually checking and removing from a `ConcurrentMap`. It would be better to simply have `removeIfPresent`."", 'commenter': 'dineshjoshi'}]"
446,test/distributed/org/apache/cassandra/distributed/api/Row.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.api;
+
+import java.util.Arrays;
+import java.util.Date;
+import java.util.NoSuchElementException;
+import java.util.Set;
+import java.util.UUID;
+import javax.annotation.Nullable;
+
+public class Row
+{
+    private final String[] names;
+    @Nullable private Object[] results; // mutable to avoid allocations in loops
+
+    public Row(String[] names)
+    {
+        this.names = names;
+    }
+
+    void setResults(@Nullable Object[] results)
+    {
+        this.results = results;
+    }
+
+    public <T> T get(String name)
+    {
+        checkAccess();
+        int idx = findIndex(name);","[{'comment': ""Shouold we use hashmap? Main reason is when we're using it with something like Harry, where we're doing quite a few queries, and big-Oh can start to matter."", 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/api/ResultSet.java,"@@ -0,0 +1,129 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.api;
+
+import java.util.Date;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+import java.util.Set;
+import java.util.UUID;
+import java.util.function.Predicate;
+
+public class ResultSet implements Iterator<Row>
+{
+    public static final ResultSet EMPTY = new ResultSet(new String[0], null);
+
+    private final String[] names;
+    private final Object[][] results;
+    private final Predicate<Row> filter;
+    private final Row row;
+    private int offset = -1;
+
+    public ResultSet(String[] names, Object[][] results)
+    {
+        this.names = names;
+        this.results = results;
+        this.row = new Row(names);
+        this.filter = ignore -> true;
+    }
+
+    private ResultSet(String[] names, Object[][] results, Predicate<Row> filter, int offset)
+    {
+        this.names = names;
+        this.results = results;
+        this.filter = filter;
+        this.offset = offset;
+        this.row = new Row(names);
+    }
+
+    public String[] getNames()
+    {
+        return names;
+    }
+
+    public boolean isEmpty()
+    {
+        return results.length == 0;
+    }
+
+    public int size()
+    {
+        return results.length;
+    }
+
+    public ResultSet filter(Predicate<Row> fn)
+    {
+        return new ResultSet(names, results, filter.and(fn), offset);
+    }
+
+    public Object[][] toObjectArrays()
+    {
+        return results;
+    }
+
+    @Override
+    public boolean hasNext()
+    {
+        if (results == null)
+            return false;
+        while ((offset += 1) < results.length)
+        {
+            row.setResults(results[offset]);
+            if (filter.test(row))
+            {
+                return true;
+            }
+        }
+        row.setResults(null);
+        return false;
+    }
+
+    @Override
+    public Row next()
+    {
+        if (offset < 0 || offset >= results.length)
+            throw new NoSuchElementException();
+        return row;","[{'comment': ""I don't mind pooling / reusing objects in general. Should we change API from `Iterator` to something that prevents leaking instances unless person is deliberately trying to do something harmful for themselves?\r\n\r\nSomething like `forEach(Row)` that is exposed by `Iterable` and maybe `map` to produce a derivative collection, but without exposing `Iterator` itself, since if someone tries to hold on for `Row` instance, its contents will change, somewhat unexpectedly for them, since they've received a legit row instance."", 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/test/RepairCoordinatorFast.java,"@@ -0,0 +1,384 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import java.util.Set;
+
+import com.google.common.collect.Iterables;
+import org.junit.Assert;
+import org.junit.Assume;
+import org.junit.Test;
+
+import org.apache.cassandra.db.ConsistencyLevel;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.distributed.api.IMessageFilters;
+import org.apache.cassandra.distributed.api.LongTokenRange;
+import org.apache.cassandra.distributed.api.NodeToolResult;
+import org.apache.cassandra.distributed.api.NodeToolResult.ProgressEventType;
+import org.apache.cassandra.distributed.test.DistributedRepairUtils.RepairParallelism;
+import org.apache.cassandra.distributed.test.DistributedRepairUtils.RepairType;
+import org.apache.cassandra.net.Verb;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+import static org.apache.cassandra.distributed.api.IMessageFilters.Matcher.of;
+import static org.apache.cassandra.distributed.test.DistributedRepairUtils.assertParentRepairFailedWithMessageContains;
+import static org.apache.cassandra.distributed.test.DistributedRepairUtils.assertParentRepairNotExist;
+import static org.apache.cassandra.distributed.test.DistributedRepairUtils.assertParentRepairSuccess;
+import static org.apache.cassandra.distributed.test.DistributedRepairUtils.getRepairExceptions;
+
+public abstract class RepairCoordinatorFast extends RepairCoordinatorBase
+{
+    public RepairCoordinatorFast(RepairType repairType, RepairParallelism parallelism, boolean withNotifications)
+    {
+        super(repairType, parallelism, withNotifications);
+    }
+
+    @Test(timeout = 1 * 60 * 1000)
+    public void simple() {
+        String table = tableName(""simple"");
+        CLUSTER.schemaChange(format(""CREATE TABLE %s.%s (key text, PRIMARY KEY (key))"", KEYSPACE, table));
+        CLUSTER.coordinator(1).execute(format(""INSERT INTO %s.%s (key) VALUES (?)"", KEYSPACE, table), ConsistencyLevel.ANY, ""some text"");
+
+        long repairExceptions = getRepairExceptions(CLUSTER, 2);
+        NodeToolResult result = repair(2, KEYSPACE, table);
+        result.asserts().ok();
+        if (withNotifications)
+        {
+            result.asserts()
+                  .notificationContains(ProgressEventType.START, ""Starting repair command"")
+                  .notificationContains(ProgressEventType.START, ""repairing keyspace "" + KEYSPACE + "" with repair options"")
+                  .notificationContains(ProgressEventType.SUCCESS, repairType != RepairType.PREVIEW ? ""Repair completed successfully"": ""Repair preview completed successfully"")
+                  .notificationContains(ProgressEventType.COMPLETE, ""finished"");
+        }
+
+        if (repairType != RepairType.PREVIEW)
+        {
+            assertParentRepairSuccess(CLUSTER, KEYSPACE, table);
+        }
+        else
+        {
+            assertParentRepairNotExist(CLUSTER, KEYSPACE, table);
+        }
+
+        Assert.assertEquals(repairExceptions, getRepairExceptions(CLUSTER, 2));
+    }
+
+    @Test(timeout = 1 * 60 * 1000)
+    public void missingKeyspace()
+    {
+        // as of this moment the check is done in nodetool so the JMX notifications are not imporant
+        // nor is the history stored
+        long repairExceptions = getRepairExceptions(CLUSTER, 2);
+        NodeToolResult result = repair(2, ""doesnotexist"");
+        result.asserts()
+              .notOk()
+              .errorContains(""Keyspace [doesnotexist] does not exist."");
+
+        Assert.assertEquals(repairExceptions, getRepairExceptions(CLUSTER, 2));
+
+        assertParentRepairNotExist(CLUSTER, ""doesnotexist"");
+    }
+
+    @Test(timeout = 1 * 60 * 1000)
+    public void missingTable()
+    {
+        long repairExceptions = getRepairExceptions(CLUSTER, 2);
+        NodeToolResult result = repair(2, KEYSPACE, ""doesnotexist"");
+        result.asserts()
+              .notOk();
+        if (withNotifications)
+        {
+            result.asserts()
+                  .errorContains(""failed with error Unknown keyspace/cf pair (distributed_test_keyspace.doesnotexist)"")
+                  // Start notification is ignored since this is checked during setup (aka before start)
+                  .notificationContains(ProgressEventType.ERROR, ""failed with error Unknown keyspace/cf pair (distributed_test_keyspace.doesnotexist)"")
+                  .notificationContains(ProgressEventType.COMPLETE, ""finished with error"");
+        }
+
+        assertParentRepairNotExist(CLUSTER, KEYSPACE, ""doesnotexist"");
+
+        Assert.assertEquals(repairExceptions + 1, getRepairExceptions(CLUSTER, 2));
+    }
+
+    @Test(timeout = 1 * 60 * 1000)
+    public void noTablesToRepair()
+    {
+        // index CF currently don't support repair, so they get dropped when listed
+        // this is done in this test to cause the keyspace to have 0 tables to repair, which causes repair to no-op
+        // early and skip.
+        String table = tableName(""withindex"");
+        CLUSTER.schemaChange(format(""CREATE TABLE %s.%s (key text, value text, PRIMARY KEY (key))"", KEYSPACE, table));
+        CLUSTER.schemaChange(format(""CREATE INDEX value_%s ON %s.%s (value)"", postfix(), KEYSPACE, table));
+
+        long repairExceptions = getRepairExceptions(CLUSTER, 2);
+        // if CF has a . in it, it is assumed to be a 2i which rejects repairs
+        NodeToolResult result = repair(2, KEYSPACE, table + "".value"");
+        result.asserts().ok();
+        if (withNotifications)
+        {
+            result.asserts()
+                  .notificationContains(""Empty keyspace"")
+                  .notificationContains(""skipping repair: "" + KEYSPACE)
+                  // Start notification is ignored since this is checked during setup (aka before start)
+                  .notificationContains(ProgressEventType.SUCCESS, ""Empty keyspace"") // will fail since success isn't returned; only complete
+                  .notificationContains(ProgressEventType.COMPLETE, ""finished""); // will fail since it doesn't do this
+        }
+
+        assertParentRepairNotExist(CLUSTER, KEYSPACE, table + "".value"");
+
+        // this is actually a SKIP and not a FAILURE, so shouldn't increment
+        Assert.assertEquals(repairExceptions, getRepairExceptions(CLUSTER, 2));
+    }
+
+    @Test(timeout = 1 * 60 * 1000)
+    public void intersectingRange()
+    {
+        // this test exists to show that this case will cause repair to finish; success or failure isn't imporant
+        // if repair is enhanced to allow intersecting ranges w/ local then this test will fail saying that we expected
+        // repair to fail but it didn't, this would be fine and this test should be updated to reflect the new
+        // semantic
+        String table = tableName(""intersectingrange"");
+        CLUSTER.schemaChange(format(""CREATE TABLE %s.%s (key text, value text, PRIMARY KEY (key))"", KEYSPACE, table));
+
+        //TODO dtest api for this?","[{'comment': 'Looks like this TODO has been done.', 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/test/RepairCoordinatorBase.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.runners.Parameterized;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.NodeToolResult;
+import org.apache.cassandra.distributed.test.DistributedRepairUtils.RepairParallelism;
+import org.apache.cassandra.distributed.test.DistributedRepairUtils.RepairType;
+
+public class RepairCoordinatorBase extends DistributedTestBase implements Serializable","[{'comment': ""I'm not a very big fan of marking test classes as `Serializable`. Can we find a way around it? This usually means we haven't serialized and passed the required state while passing it to the lambda, and instead have passed the instance of the test case. This might be also caused by the fact we're using parametrized tests (since that's already the case in one other test somewhere in the repo), but I think we shuoldn't use pass test class instance into the in-jvm cluster instances."", 'commenter': 'ifesdjeen'}, {'comment': ""At least a preliminary test hasn't confirmed that we need `Serializable` here."", 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/test/RepairCoordinatorBase.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.runners.Parameterized;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.NodeToolResult;
+import org.apache.cassandra.distributed.test.DistributedRepairUtils.RepairParallelism;
+import org.apache.cassandra.distributed.test.DistributedRepairUtils.RepairType;
+
+public class RepairCoordinatorBase extends DistributedTestBase implements Serializable
+{
+    protected static Cluster CLUSTER;
+
+    protected final RepairType repairType;
+    protected final RepairParallelism parallelism;
+    protected final boolean withNotifications;
+
+    public RepairCoordinatorBase(RepairType repairType, RepairParallelism parallelism, boolean withNotifications)
+    {
+        this.repairType = repairType;
+        this.parallelism = parallelism;
+        this.withNotifications = withNotifications;
+    }
+
+    @Parameterized.Parameters(name = ""{0}/{1}"")
+    public static Collection<Object[]> testsWithoutType()
+    {
+        List<Object[]> tests = new ArrayList<>();
+        for (RepairParallelism p : RepairParallelism.values())
+        {
+            tests.add(new Object[] { p, true });
+            tests.add(new Object[] { p, false });
+        }
+        return tests;
+    }
+
+    @BeforeClass
+    public static void before()
+    {
+        // This only works because the way CI works
+        // In CI a new JVM is spun up for each test file, so this doesn't have to worry about another test file
+        // getting this set first
+        System.setProperty(""cassandra.nodetool.jmx_notification_poll_interval_seconds"", ""1"");
+        DatabaseDescriptor.clientInitialization();","[{'comment': ""I'm curious why we need to initialize client here. Usually we need to do this in Cassandra instance itself, but not on the test class. Do we need it for JMX to work?"", 'commenter': 'ifesdjeen'}, {'comment': ""At least a preliminary test hasn't confirmed that we need to call `clientInitialization` here."", 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/api/ICoordinator.java,"@@ -27,7 +27,11 @@
 {
     // a bit hacky, but ConsistencyLevel draws in too many dependent classes, so we cannot have a cross-version
     // method signature that accepts ConsistencyLevel directly.  So we just accept an Enum<?> and cast.
-    Object[][] execute(String query, Enum<?> consistencyLevel, Object... boundValues);
+    default Object[][] execute(String query, Enum<?> consistencyLevel, Object... boundValues)
+    {
+        return executeWithResult(query, consistencyLevel, boundValues).toObjectArrays();
+    }
+    ResultSet executeWithResult(String query, Enum<?> consistencyLevel, Object... boundValues);","[{'comment': 'In a long run, do we even want to keep `object[][]`? ', 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/api/LongTokenRange.java,"@@ -0,0 +1,38 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.api;
+
+import java.io.Serializable;
+
+public final class LongTokenRange implements Serializable","[{'comment': 'I think we might want to limit partitioner to `Murmur3Partitioner` and allow using long tokens only. We already have API for bootstrap that works with long tokens, and having this in addition sort of hints at a need to do something like that.', 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java,"@@ -151,9 +152,16 @@ public boolean isShutdown()
         @Override
         public synchronized void startup()
         {
+            startup(AbstractCluster.this);
+        }
+
+        public synchronized void startup(ICluster cluster)
+        {
+            if (cluster != AbstractCluster.this)
+                throw new IllegalArgumentException(""Only the owning cluster can be used for startup""); //TODO why have this in the API?","[{'comment': ""To my best memory, this is because it's called from `DelegatingInvokableInstance`. But we can make it package-private."", 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java,"@@ -151,9 +152,16 @@ public boolean isShutdown()
         @Override
         public synchronized void startup()
         {
+            startup(AbstractCluster.this);
+        }
+
+        public synchronized void startup(ICluster cluster)
+        {
+            if (cluster != AbstractCluster.this)
+                throw new IllegalArgumentException(""Only the owning cluster can be used for startup""); //TODO why have this in the API?
             if (!isShutdown)
                 throw new IllegalStateException();
-            delegate().startup(AbstractCluster.this);
+            delegate().startup(cluster);","[{'comment': 'I think you can just do `delegate().startup()`. ', 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/api/NodeToolResult.java,"@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.api;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+import javax.management.Notification;
+
+import org.junit.Assert;
+
+public class NodeToolResult
+{
+    public final String[] commandAndArgs;","[{'comment': 'Should all these fields be public?..', 'commenter': 'ifesdjeen'}, {'comment': 'Should all these fields be public?..', 'commenter': 'ifesdjeen'}]"
446,test/distributed/org/apache/cassandra/distributed/impl/Instance.java,"@@ -552,9 +555,66 @@ public int liveMemberCount()
         }).call();
     }
 
-    public int nodetool(String... commandAndArgs)
+    public NodeToolResult nodetoolResult(boolean withNotifications, String... commandAndArgs)
     {
-        return sync(() -> new NodeTool(new InternalNodeProbeFactory()).execute(commandAndArgs)).call();
+        return sync(() -> {
+            DtestNodeTool nodetool = new DtestNodeTool(withNotifications);
+            int rc =  nodetool.execute(commandAndArgs);
+            return new NodeToolResult(commandAndArgs, rc, new ArrayList<>(nodetool.notifications.notifications), nodetool.latestError);
+        }).call();
+    }
+
+    private static class DtestNodeTool extends NodeTool {","[{'comment': 'What do you think about `DTestNodeTool` (different casing)?', 'commenter': 'ifesdjeen'}]"
448,bin/cqlsh.py,"@@ -96,7 +96,7 @@
 # >>> webbrowser._tryorder
 # >>> webbrowser._browser
 #
-if len(webbrowser._tryorder) == 0:
+if webbrowser._tryorder is None or len(webbrowser._tryorder) == 0:","[{'comment': 'Can you just call `webbrowser.get()` and leave the rest of the code as-is? This will initialize the `_tryorder` variable.', 'commenter': 'dineshjoshi'}, {'comment': ""in a Docker testing environment it's likely that no default browser is installed and so you could end up in https://github.com/python/cpython/blob/3.7/Lib/webbrowser.py#L65, so I'm +1 with the fix here"", 'commenter': 'nastra'}, {'comment': 'I tried `webbroser.get()` and ran in docker container, and I hit the error mentioned. So I think it is safer to check if it is `None`.', 'commenter': 'yukim'}]"
481,src/java/org/apache/cassandra/db/streaming/CassandraOutgoingFile.java,"@@ -185,46 +177,18 @@ public boolean shouldStreamEntireSSTable()
         if (!DatabaseDescriptor.streamEntireSSTables() || ref.get().getSSTableMetadata().hasLegacyCounterShards)
             return false;
 
-        ColumnFamilyStore cfs = ColumnFamilyStore.getIfExists(getTableId());
-
-        if (cfs == null)
-            return false;
-
-        AbstractCompactionStrategy compactionStrategy = cfs.getCompactionStrategyManager()
-                                                           .getCompactionStrategyFor(ref.get());
-
-        if (compactionStrategy instanceof LeveledCompactionStrategy)
-            return contained(normalizedRanges, ref.get());
-
-        return false;
+        return contained(sections, ref.get());
     }
 
     @VisibleForTesting
-    public boolean contained(List<Range<Token>> normalizedRanges, SSTableReader sstable)
+    public boolean contained(List<SSTableReader.PartitionPositionBounds> sections, SSTableReader sstable)
     {
-        if (isFullyContained != null)
-            return isFullyContained;
-
-        isFullyContained = computeContainment(normalizedRanges, sstable);
-        return isFullyContained;
-    }
-
-    private boolean computeContainment(List<Range<Token>> normalizedRanges, SSTableReader sstable)
-    {
-        if (normalizedRanges == null)
+        if (sections == null || sections.isEmpty())
             return false;
 
-        RangeOwnHelper rangeOwnHelper = new RangeOwnHelper(normalizedRanges);
-        try (KeyIterator iter = new KeyIterator(sstable.descriptor, sstable.metadata()))
-        {
-            while (iter.hasNext())
-            {
-                DecoratedKey key = iter.next();
-                if (!rangeOwnHelper.check(key))
-                    return false;
-            }
-        }
-        return true;
+        // if transfer sections contain entire sstable
+        long transferLength = sections.stream().mapToLong(p -> p.upperPosition - p.lowerPosition).sum();","[{'comment': 'These sections should be normalized, so you will only have 1 when its the full sstable right?', 'commenter': 'tjake'}, {'comment': ""didn't realize `Range.normalize` will combine overlapping ranges.. updated here 188ed6d"", 'commenter': 'jasonstack'}, {'comment': ""I think there can be more than 1 section if ranges don't overlap..\r\neg. transfer ranges (0, 1000], (2000, 3000] and sstable has token 100, 200, 2100, 2200.. there will be two sections..\r\n\r\nI have reverted to `long transferLength = sections.stream().mapToLong(p -> p.upperPosition - p.lowerPosition).sum();`  WDYT?"", 'commenter': 'jasonstack'}]"
481,.circleci/config.yml,"@@ -3,10 +3,10 @@ jobs:
   j8_jvm_upgrade_dtests:
     docker:
     - image: spod/cassandra-testing-ubuntu1810-java11-w-dependencies:20190306
-    resource_class: medium","[{'comment': ""I assume you didn't intend to commit this..."", 'commenter': 'tjake'}, {'comment': ""right, not to commit. it's to speed up CI during our circle-ci trial period.."", 'commenter': 'jasonstack'}]"
519,src/java/org/apache/cassandra/service/AbstractWriteResponseHandler.java,"@@ -279,8 +291,9 @@ private final void decrementResponseOrExpired()
         int decrementedValue = responsesAndExpirations.decrementAndGet();
         if (decrementedValue == 0)
         {
-            //The condition being signaled is a valid proxy for the CL being achieved
-            if (!condition.isSignaled())
+            // The condition being signaled is a valid proxy for the CL being achieved
+            // Only mark it as failed if the requested CL was achieved.
+            if (!condition.isSignaled() & requestedCLAchieved)","[{'comment': 'Is this intentionally a bitwise AND?', 'commenter': 'dineshjoshi'}]"
524,src/java/org/apache/cassandra/transport/ClientRequestSizeMetricsHandler.java,"@@ -0,0 +1,58 @@
+/*
+  * Licensed to the Apache Software Foundation (ASF) under one
+  * or more contributor license agreements.  See the NOTICE file
+  * distributed with this work for additional information
+  * regarding copyright ownership.  The ASF licenses this file
+  * to you under the Apache License, Version 2.0 (the
+  * ""License""); you may not use this file except in compliance
+  * with the License.  You may obtain a copy of the License at
+  *
+  *     http://www.apache.org/licenses/LICENSE-2.0
+  *
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an ""AS IS"" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+
+package org.apache.cassandra.transport;
+
+import java.util.List;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.channel.ChannelHandler;
+import io.netty.channel.ChannelHandlerContext;
+import io.netty.handler.codec.MessageToMessageCodec;
+import org.apache.cassandra.metrics.ClientRequestSizeMetrics;
+
+/**
+ * Records the number of bytes read off of and written to the network
+ * Normally we only use the MessageToMessageCodec to apply a transformation in the Netty messaging pipeline
+ * Here we need to ensure the ByteBuf sticks around past this Handler, so we need to remember to call retain()
+ */
+@ChannelHandler.Sharable
+public class ClientRequestSizeMetricsHandler extends MessageToMessageCodec<ByteBuf, ByteBuf>
+{
+    @Override
+    public void decode(ChannelHandlerContext ctx, ByteBuf buf, List<Object> results)
+    {
+        final long messageSize = buf.writerIndex() - buf.readerIndex();","[{'comment': 'This can double count.\r\n\r\n* https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Frame.java#L183\r\n* https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Frame.java#L223\r\n\r\nLets say a single message is 5 bytes, and each byte is seen one after the other, that means decode is called 5 times with the following\r\n\r\n```\r\n1) [0]\r\n2) [0, 1]\r\n3) [0, 1, 2]\r\n4) [0, 1, 2, 3]\r\n5) [0, 1, 2, 3, 4] -> finally consumable\r\n6) [5] // next message\r\n```\r\n\r\nThis logic should move here https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Frame.java#L226. At this point we know the frame size (`frameLength`) and know there is enough bytes so actually consume those bytes.', 'commenter': 'dcapwell'}]"
524,src/java/org/apache/cassandra/transport/ClientRequestSizeMetricsHandler.java,"@@ -0,0 +1,58 @@
+/*
+  * Licensed to the Apache Software Foundation (ASF) under one
+  * or more contributor license agreements.  See the NOTICE file
+  * distributed with this work for additional information
+  * regarding copyright ownership.  The ASF licenses this file
+  * to you under the Apache License, Version 2.0 (the
+  * ""License""); you may not use this file except in compliance
+  * with the License.  You may obtain a copy of the License at
+  *
+  *     http://www.apache.org/licenses/LICENSE-2.0
+  *
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an ""AS IS"" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+
+package org.apache.cassandra.transport;
+
+import java.util.List;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.channel.ChannelHandler;
+import io.netty.channel.ChannelHandlerContext;
+import io.netty.handler.codec.MessageToMessageCodec;
+import org.apache.cassandra.metrics.ClientRequestSizeMetrics;
+
+/**
+ * Records the number of bytes read off of and written to the network
+ * Normally we only use the MessageToMessageCodec to apply a transformation in the Netty messaging pipeline
+ * Here we need to ensure the ByteBuf sticks around past this Handler, so we need to remember to call retain()
+ */
+@ChannelHandler.Sharable
+public class ClientRequestSizeMetricsHandler extends MessageToMessageCodec<ByteBuf, ByteBuf>
+{
+    @Override
+    public void decode(ChannelHandlerContext ctx, ByteBuf buf, List<Object> results)
+    {
+        final long messageSize = buf.writerIndex() - buf.readerIndex();
+        ClientRequestSizeMetrics.totalBytesRead.inc(messageSize);
+        ClientRequestSizeMetrics.bytesReadPerQueryHistogram.update(messageSize);
+        // the buffer needs to be retained here due to Netty's internal requirements.  Without it the buffer may be freed
+        buf.retain();
+        results.add(buf);
+    }
+
+    @Override
+    public void encode(ChannelHandlerContext ctx, ByteBuf buf, List<Object> results)
+    {
+        final long messageSize = buf.writerIndex() - buf.readerIndex();","[{'comment': ""sadly this isn't correct.  https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Frame.java#L302 has a buffer for the header and a separate buffer for the body; that should then cause this method to get called twice per write.  \r\n\r\nThis should be fine for totalBytesWritten, but bytesWrittenPerQueryHistogram will be off since it will see N messages of size 9 and N messages of variable length (where N is the total number of messages)."", 'commenter': 'dcapwell'}, {'comment': 'If you move this logic to the encoder you would produce the correct length', 'commenter': 'dcapwell'}]"
524,src/java/org/apache/cassandra/metrics/ClientRequestSizeMetrics.java,"@@ -0,0 +1,36 @@
+/*
+  * Licensed to the Apache Software Foundation (ASF) under one
+  * or more contributor license agreements.  See the NOTICE file
+  * distributed with this work for additional information
+  * regarding copyright ownership.  The ASF licenses this file
+  * to you under the Apache License, Version 2.0 (the
+  * ""License""); you may not use this file except in compliance
+  * with the License.  You may obtain a copy of the License at
+  *
+  *     http://www.apache.org/licenses/LICENSE-2.0
+  *
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an ""AS IS"" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+package org.apache.cassandra.metrics;
+
+
+import static org.apache.cassandra.metrics.CassandraMetricsRegistry.Metrics;
+
+import com.codahale.metrics.Counter;
+import com.codahale.metrics.Histogram;
+
+/**
+ * Metrics to track the size of incoming and outgoing bytes at Cassandra server.
+ */
+public class ClientRequestSizeMetrics
+{
+    private static final String TYPE = ""ClientRequestSize"";
+    public static final Counter totalBytesRead = Metrics.counter(DefaultNameFactory.createMetricName(TYPE, ""IncomingBytes"", null));
+    public static final Counter totalBytesWritten = Metrics.counter(DefaultNameFactory.createMetricName(TYPE, ""OutgoingBytes"", null));
+    public static final Histogram bytesReadPerQueryHistogram = Metrics.histogram(DefaultNameFactory.createMetricName(TYPE, ""BytesReadPerQuery"", null), true);","[{'comment': 'Read == decode, written == encode, that makes sense but this could also be confusing.  Maybe rename to something like `BytesReceivedPerFrame` and `BytesTransmittedPerFrame`?', 'commenter': 'dcapwell'}]"
524,src/java/org/apache/cassandra/transport/Frame.java,"@@ -301,6 +305,10 @@ public void encode(ChannelHandlerContext ctx, Frame frame, List<Object> results)
 
             results.add(header);
             results.add(frame.body);
+
+            int messageSize = header.writerIndex() + frame.body.writerIndex();","[{'comment': ""should use `io.netty.buffer.ByteBuf#readableBytes`.  With a allocator there isn't a promise that the readerIndex is always 0, so its possible to get a ByteBuf pointing to a fragment in memory where the readerIndex is something like 102471081; readableBytes is safer"", 'commenter': 'dcapwell'}]"
524,src/java/org/apache/cassandra/transport/Frame.java,"@@ -301,6 +305,10 @@ public void encode(ChannelHandlerContext ctx, Frame frame, List<Object> results)
 
             results.add(header);
             results.add(frame.body);
+
+            int messageSize = header.writerIndex() + frame.body.writerIndex();
+            ClientRequestSizeMetrics.totalBytesWritten.inc(messageSize);","[{'comment': ""this is a me thing, but I would prefer this is done before `results.add`.  The code is safe, but I like assuming that once you give the stuff to netty it owns it and you shouldn't touch it anymore."", 'commenter': 'dcapwell'}]"
557,src/java/org/apache/cassandra/db/compaction/Verifier.java,"@@ -543,15 +545,17 @@ public LongPredicate getPurgeEvaluator(DecoratedKey key)
         public final boolean mutateRepairStatus;
         public final boolean checkOwnsTokens;
         public final boolean quick;
+        public final Function<String, ? extends Collection<Range<Token>>> tokenLookup;","[{'comment': 'At the very least add a comment that documents the function. Ideally, you could just create a simple Interface/Class that can wrap this and have a meaningful method invocation like this: `Tokens.forKeyspace(String name)`', 'commenter': 'dineshjoshi'}, {'comment': 'I can add some docs, but think its best to use function rather than a new interface for this.', 'commenter': 'dcapwell'}, {'comment': 'added docs @dineshjoshi ', 'commenter': 'dcapwell'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -1126,17 +1165,26 @@ public UpdateTransaction newUpdateTransaction(PartitionUpdate update, WriteConte
     {
         if (!hasIndexes())
             return UpdateTransaction.NO_OP;
-
-        Index.Indexer[] indexers = indexes.values().stream()
-                                          .map(i -> i.indexerFor(update.partitionKey(),
-                                                                 update.columns(),
-                                                                 nowInSec,
-                                                                 ctx,
-                                                                 IndexTransaction.Type.UPDATE))
-                                          .filter(Objects::nonNull)
-                                          .toArray(Index.Indexer[]::new);
-
-        return indexers.length == 0 ? UpdateTransaction.NO_OP : new WriteTimeTransaction(indexers);
+        
+        ArrayList<Index.Indexer> idxrs = new ArrayList<>();
+        for (Index i : indexes.values())","[{'comment': 'Removing streams from the hot path.', 'commenter': 'bereng'}]"
570,src/java/org/apache/cassandra/index/internal/CassandraIndex.java,"@@ -689,33 +705,42 @@ private boolean isPrimaryKeyIndex()
     @SuppressWarnings(""resource"")
     private void buildBlocking()
     {
-        baseCfs.forceBlockingFlush();
-
-        try (ColumnFamilyStore.RefViewFragment viewFragment = baseCfs.selectAndReference(View.selectFunction(SSTableSet.CANONICAL));
-             Refs<SSTableReader> sstables = viewFragment.refs)
+        try","[{'comment': 'All these changes are just a try-catch sandwiching to change `supportedLoads` to NONE on exceptions.', 'commenter': 'bereng'}]"
570,src/java/org/apache/cassandra/index/internal/CassandraIndex.java,"@@ -146,6 +146,22 @@ protected abstract ByteBuffer getIndexedValue(ByteBuffer partitionKey,
                                                   CellPath path,
                                                   ByteBuffer cellValue);
 
+    
+    public boolean supportsLoad(Loads load)
+    {
+        switch (load)
+        {
+            case ALL:
+                return supportedLoads.equals(Loads.ALL);","[{'comment': 'Being an enum, we can use `==` instead of `equals`', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/index/internal/CustomCassandraIndex.java,"@@ -94,6 +95,21 @@ protected boolean supportsOperator(ColumnMetadata indexedColumn, Operator operat
     {
         return operator.equals(Operator.EQ);
     }
+    
+    public boolean supportsLoad(Loads load)
+    {
+        switch (load)
+        {
+            case ALL:
+                return supportedLoads.equals(Loads.ALL);","[{'comment': 'Same as before, we can use `==`', 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/Index.java,"@@ -136,6 +135,7 @@
  */
 public interface Index
 {
+    public static enum Loads {READS, WRITES, ALL, NONE};","[{'comment': ""Using a plural name for a enum feels a little bit odd to me, what about something like `LoadType` or the more operation `OperationType`? Also I can't think of other future values for the enum given it's current usage, so I'm wondering if we could replace it by two booleans, implicit through a pair of `supportsWrites`/`supportsReads` methods. WDYT?"", 'commenter': 'adelapena'}, {'comment': ""I think it's a personal style thing. Rather than having 2 booleans and having to reason about them, the Enum makes it more 'in your face' imo: `ALL` is clear what it means vs 2 booleans etc. :shrug:\r\n\r\nAlso maybe I have too much imagination, but I can think of 'DELETE' as a load. You could have 'append-only' indexes (think banking i.e.) in the future that don't support deletes. Maybe premature optimization is being my enemy here... Or sthg with statics for custom indexes...\r\n\r\nI prefer you decide what is best here :-). I'll rename to singulars meanwhile in case we kept that option."", 'commenter': 'bereng'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -1126,17 +1165,26 @@ public UpdateTransaction newUpdateTransaction(PartitionUpdate update, WriteConte
     {
         if (!hasIndexes())
             return UpdateTransaction.NO_OP;
-
-        Index.Indexer[] indexers = indexes.values().stream()
-                                          .map(i -> i.indexerFor(update.partitionKey(),
-                                                                 update.columns(),
-                                                                 nowInSec,
-                                                                 ctx,
-                                                                 IndexTransaction.Type.UPDATE))
-                                          .filter(Objects::nonNull)
-                                          .toArray(Index.Indexer[]::new);
-
-        return indexers.length == 0 ? UpdateTransaction.NO_OP : new WriteTimeTransaction(indexers);
+        
+        ArrayList<Index.Indexer> idxrs = new ArrayList<>();","[{'comment': 'Ignorable nit: perhaps we could name this variable `indexers`, and use `idxrs` for the array below. Also, I wonder if it would make sense to use an iterable instead of an array in `WriteTimeTransaction`, so we can directly pass this `ArrayList` without the `toArray` conversion. Would it hurt performance?', 'commenter': 'adelapena'}, {'comment': ""I was trying to get all this path without iterators which would be ideal, but `indexes.values()` is a `Collection` which I can't access via `get()`. The previous streams would have the same array conversion, which is suspicious as the immediate solution would be to use the `ArrayList`. Being the hot path maybe we want to keep the 'pure' array index access even though `ArrayLists`'s overhead might be minimal if anyting at all. Sounds good?"", 'commenter': 'bereng'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -424,16 +427,47 @@ public static String getIndexName(String cfName)
         assert isIndexColumnFamily(cfName);
         return StringUtils.substringAfter(cfName, Directories.SECONDARY_INDEX_NAME_SEPARATOR);
     }
+    
+    /**
+     * Does a blocking full rebuild/reovery of the specifed indexes from all the sstables in the base table.
+     * Note also that this method of (re)building/recovering indexes:
+     * a) takes a set of index *names* rather than Indexers
+     * b) marks existing indexes removed prior to rebuilding
+     * c) fails if such marking operation conflicts with any ongoing index builds, as full rebuilds cannot be run
+     * concurrently
+     *
+     * @param indexNames the list of indexes to be rebuilt
+     * @param isRecovery True if want to run recovery rather than rebuilding 
+     */
+    private void rebuildIndexesBlocking(Set<String> indexNames, boolean isRecovery)
+    {
+        try (ColumnFamilyStore.RefViewFragment viewFragment = baseCfs.selectAndReference(View.selectFunction(SSTableSet.CANONICAL));
+             Refs<SSTableReader> allSSTables = viewFragment.refs)
+        {
+            Set<Index> toRebuild = indexes.values().stream()
+                                          .filter(index -> indexNames.contains(index.getIndexMetadata().name))
+                                          .filter(Index::shouldBuildBlocking)
+                                          .collect(Collectors.toSet());
+            if (toRebuild.isEmpty())
+            {
+                logger.info(""No defined indexes with the supplied names: {}"", Joiner.on(',').join(indexNames));
+                return;
+            }
+
+            buildIndexesBlocking(allSSTables, toRebuild, true, isRecovery);
+        }
+    }
 
     /**
-     * Performs a blocking (re)indexing of the specified SSTables for the specified indexes.
+     * Performs a blocking (re)indexing/recovery of the specified SSTables for the specified indexes.
      *
      * @param sstables      the SSTables to be (re)indexed
      * @param indexes       the indexes to be (re)built for the specifed SSTables
      * @param isFullRebuild True if this method is invoked as a full index rebuild, false otherwise
+     * @param isFullRebuild True if this method is invoked as an index rebuild or recovery","[{'comment': '```suggestion\r\n     * @param isRecovery {@code true} if this method is invoked as an index rebuild or recovery\r\n```', 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -424,16 +427,47 @@ public static String getIndexName(String cfName)
         assert isIndexColumnFamily(cfName);
         return StringUtils.substringAfter(cfName, Directories.SECONDARY_INDEX_NAME_SEPARATOR);
     }
+    
+    /**
+     * Does a blocking full rebuild/reovery of the specifed indexes from all the sstables in the base table.","[{'comment': '```suggestion\r\n     * Does a blocking full rebuild/recovery of the specifed indexes from all the sstables in the base table.\r\n```', 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -485,14 +471,15 @@ private void buildIndexesBlocking(Collection<SSTableReader> sstables, Set<Index>
 
         try
         {
-            logger.info(""Submitting index build of {} for data in {}"",
+            logger.info(""Submitting index recovery/build of {} for data in {}"",
                         indexes.stream().map(i -> i.getIndexMetadata().name).collect(Collectors.joining("","")),
                         sstables.stream().map(SSTableReader::toString).collect(Collectors.joining("","")));
 
             // Group all building tasks
             Map<Index.IndexBuildingSupport, Set<Index>> byType = new HashMap<>();
             for (Index index : indexes)
             {
+                boolean isRecovery = !index.supportsLoad(Loads.ALL);","[{'comment': ""This is the only contentious point imo. I don't know if we should call a `Index#isRecoveryNeeded()` method instead rather than trying to infer it at the SIM level. The former assumes `ALL` is mandatory for an index to the 'right', the latter allows for the index implementation to decide that. Both make sense... It's our choice tbh and switching from one to the other in the future it's easy enough anyway."", 'commenter': 'bereng'}]"
570,src/java/org/apache/cassandra/index/Index.java,"@@ -136,6 +135,7 @@
  */
 public interface Index
 {
+    public static enum LoadType {READ, WRITE, ALL, NONE};","[{'comment': '+1 to the new enum names. It would be nice to add a very brief comment about what it is, for the sake of the unaware reader. The static and the semicolon are not needed:\r\n```suggestion\r\n    public enum LoadType {READ, WRITE, ALL, NONE}\r\n```', 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/internal/CassandraIndex.java,"@@ -146,6 +146,22 @@ protected abstract ByteBuffer getIndexedValue(ByteBuffer partitionKey,
                                                   CellPath path,
                                                   ByteBuffer cellValue);
 
+    
+    public boolean supportsLoad(LoadType load)
+    {
+        switch (load)
+        {
+            case ALL:
+                return supportedLoads == LoadType.ALL;
+            case READ:
+                return supportedLoads == LoadType.ALL || supportedLoads == LoadType.READ;
+            case WRITE:
+                return supportedLoads == LoadType.ALL || supportedLoads == LoadType.WRITE;
+            default:
+                return false;
+        }","[{'comment': ""This block of code is repeated in a couple of test index implementations, and it's to expect that other implementations would need to repeat it. I think that defining that `ALL` includes `READ` and `WRITE`, etc. seems more a characteristic of `LoadType` than the index itself, so perhaps we could move this logic to the enum itself, for example:\r\n```java\r\npublic enum LoadType {\r\n    READ, WRITE, ALL, NONE;\r\n\r\n    public boolean accepts(LoadType load)\r\n    {\r\n        switch (this)\r\n        {\r\n            case ALL:\r\n                return true;\r\n            case READ:\r\n                return load == LoadType.READ || load == LoadType.NONE;\r\n            case WRITE:\r\n                return load == LoadType.WRITE || load == LoadType.NONE;\r\n            default:\r\n                return false;\r\n        }\r\n    }\r\n}\r\n```\r\nThat way we would simplify usage later:\r\n```java\r\n@Override\r\npublic boolean supportsLoad(LoadType load)\r\n{\r\n    return supportedLoadType.accepts(load);\r\n}\r\n```\r\n"", 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -338,20 +355,20 @@ public void markAllIndexesRemoved()
     public void rebuildIndexesBlocking(Set<String> indexNames)
     {
         try (ColumnFamilyStore.RefViewFragment viewFragment = baseCfs.selectAndReference(View.selectFunction(SSTableSet.CANONICAL));
-             Refs<SSTableReader> allSSTables = viewFragment.refs)
-        {
-            Set<Index> toRebuild = indexes.values().stream()
-                                          .filter(index -> indexNames.contains(index.getIndexMetadata().name))
-                                          .filter(Index::shouldBuildBlocking)
-                                          .collect(Collectors.toSet());
-            if (toRebuild.isEmpty())
-            {
-                logger.info(""No defined indexes with the supplied names: {}"", Joiner.on(',').join(indexNames));
-                return;
-            }
-
-            buildIndexesBlocking(allSSTables, toRebuild, true);
-        }
+                Refs<SSTableReader> allSSTables = viewFragment.refs)
+           {
+               Set<Index> toRebuild = indexes.values().stream()
+                                             .filter(index -> indexNames.contains(index.getIndexMetadata().name))
+                                             .filter(Index::shouldBuildBlocking)
+                                             .collect(Collectors.toSet());
+               if (toRebuild.isEmpty())
+               {
+                   logger.info(""No defined indexes with the supplied names: {}"", Joiner.on(',').join(indexNames));
+                   return;
+               }
+
+               buildIndexesBlocking(allSSTables, toRebuild, true);
+           }","[{'comment': 'Nit: this seems to have missed alignment with an extra tab', 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -424,10 +441,13 @@ public static String getIndexName(String cfName)
         assert isIndexColumnFamily(cfName);
         return StringUtils.substringAfter(cfName, Directories.SECONDARY_INDEX_NAME_SEPARATOR);
     }
-
+    
     /**
-     * Performs a blocking (re)indexing of the specified SSTables for the specified indexes.
+     * Performs a blocking (re)indexing/recovery of the specified SSTables for the specified indexes.
      *
+     * If the index doesn't support ALL {@link Index.LoadType} it performs a recovery {@link getRecoveryTaskSupport()}
+     * instead of a build {@link getBuildTaskSupport()}","[{'comment': ""```suggestion\r\n     * If the index doesn't support ALL {@link Index.LoadType} it performs a recovery {@link Index#getRecoveryTaskSupport()}\r\n     * instead of a build {@link Index#getBuildTaskSupport()}\r\n```"", 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -619,8 +642,16 @@ private synchronized void markIndexesBuilding(Set<Index> indexes, boolean isFull
     private synchronized void markIndexBuilt(Index index, boolean isFullRebuild)
     {
         String indexName = index.getIndexMetadata().name;
-        if (isFullRebuild)
+        if (isFullRebuild && index.supportsLoad(Index.LoadType.READ))
+        {
             queryableIndexes.add(indexName);
+            logger.info(""Index ["" + indexName + ""] became queryable."");
+        }
+        if (isFullRebuild && index.supportsLoad(Index.LoadType.WRITE))
+        {
+            writableIndexes.add(indexName);
+            logger.info(""Index ["" + indexName + ""] became writable."");","[{'comment': 'I think that `became` suggests a change of status, as if it weren\'t writable before. We could either say that is writable without telling if it was so before or, even better, use the boolean returned by `queryableIndexes.add`:\r\n```suggestion\r\n            if (writableIndexes.add(indexName))\r\n                logger.info(""Index ["" + indexName + ""] became writable."");\r\n```\r\nThe same applies to queryable indexes above.', 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -1126,17 +1158,21 @@ public UpdateTransaction newUpdateTransaction(PartitionUpdate update, WriteConte
     {
         if (!hasIndexes())
             return UpdateTransaction.NO_OP;
-
-        Index.Indexer[] indexers = indexes.values().stream()
-                                          .map(i -> i.indexerFor(update.partitionKey(),
-                                                                 update.columns(),
-                                                                 nowInSec,
-                                                                 ctx,
-                                                                 IndexTransaction.Type.UPDATE))
-                                          .filter(Objects::nonNull)
-                                          .toArray(Index.Indexer[]::new);
-
-        return indexers.length == 0 ? UpdateTransaction.NO_OP : new WriteTimeTransaction(indexers);
+        
+        ArrayList<Index.Indexer> idxrs = new ArrayList<>();
+        for (Index i : indexes.values())
+        {
+            Index.Indexer idxr = i.indexerFor(update.partitionKey(), update.columns(), nowInSec, ctx, IndexTransaction.Type.UPDATE);
+            if (idxr != null && isIndexWritable(i))","[{'comment': ""We could check if the index is writable before creating the indexer:\r\n```suggestion\r\n            if (!isIndexWritable(i))\r\n                continue;\r\n\r\n            Index.Indexer idxr = i.indexerFor(update.partitionKey(), update.columns(), nowInSec, ctx, IndexTransaction.Type.UPDATE);\r\n            if (idxr != null)\r\n```\r\nThe same applies to the other usages of `isIndexWritable` in `IndexGCTransaction#commit()` and `CleanupGCTransaction#commit()`. Alternatively, we could transform the set `SIM#writableIndexes` into a `Map<String, Index>`, and iterate its values this instead of `SIM#indexes` values in the three places. That way we wouldn't even need the `isIndexWritable` method."", 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/SecondaryIndexManager.java,"@@ -1356,7 +1396,7 @@ public void commit()
                 for (Index index : indexes)
                 {
                     Index.Indexer indexer = index.indexerFor(key, columns, nowInSec, ctx, Type.COMPACTION);
-                    if (indexer == null)
+                    if (indexer == null || !indexManager.isIndexWritable(index))","[{'comment': ""See comment above about either calling `isIndexWritable` before creating the indexer, or directly passing `writableIndexes` to the `IndexGCTransaction ` constructor, so we don't need to this check."", 'commenter': 'adelapena'}]"
570,src/java/org/apache/cassandra/index/internal/CassandraIndex.java,"@@ -79,6 +78,7 @@
     protected ColumnFamilyStore indexCfs;
     protected ColumnMetadata indexedColumn;
     protected CassandraIndexFunctions functions;
+    protected LoadType supportedLoads = LoadType.ALL;","[{'comment': 'Nit:\r\n```suggestion\r\n    protected LoadType supportedLoadType = LoadType.ALL;\r\n```', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java,"@@ -1052,6 +1052,58 @@ public void testIndexQueriesWithIndexNotReady() throws Throwable
             execute(""DROP index "" + KEYSPACE + "".testIndex"");
         }
     }
+    
+    @Test
+    public void testIndexWritesWithIndexNotReady() throws Throwable
+    {
+        createTable(""CREATE TABLE %s (pk int, ck int, value int, PRIMARY KEY (pk, ck))"");
+        String indexName = createIndex(""CREATE CUSTOM INDEX ON %s (value) USING '"" + BlockingStubIndex.class.getName() + ""'"");
+
+        execute(""INSERT INTO %s (pk, ck, value) VALUES (?, ?, ?)"", 1, 1, 1);
+        BlockingStubIndex index = (BlockingStubIndex) getCurrentColumnFamilyStore().indexManager.getIndexByName(indexName);
+        assertEquals(0, index.rowsInserted.size());
+        execute(""DROP index "" + KEYSPACE + ""."" + indexName);
+    }
+    
+    @Test // A Bad init could leave an index only accepting reads
+    public void testReadOnlyIndex() throws Throwable
+    {
+        String tableName = createTable(""CREATE TABLE %s (pk int, ck int, value int, PRIMARY KEY (pk, ck))"");
+        String indexName = createIndex(""CREATE CUSTOM INDEX ON %s (value) USING '"" + ReadOnlyIndex.class.getName() + ""'"");
+        assertTrue(waitForIndex(keyspace(), tableName, indexName));
+
+        execute(""SELECT value FROM %s WHERE value = 1"");
+        execute(""INSERT INTO %s (pk, ck, value) VALUES (?, ?, ?)"", 1, 1, 1);
+
+        ReadOnlyIndex index = (ReadOnlyIndex) getCurrentColumnFamilyStore().indexManager.getIndexByName(indexName);
+        assertEquals(0, index.rowsInserted.size());
+        execute(""DROP index "" + KEYSPACE + ""."" + indexName);","[{'comment': ""We could try a recovery here, and verify that the index starts to accepts writes after that recovery. Also, in `ReadOnlyIndex`, we could throw an exception in `getInitializationTask`, and provide a mock implementation `getRecoveryTaskSupport` that enables writes. Note that forcing an exception during initialization would make us to miss the call to `markIndexBuilt`, so the index won't be added to `SIM#queryableIndexes` and it won't accepts reads independently of its `ReadOnlyIndex#supportsLoad` implementation. That's why I think we need some new logic in `SIM#markIndexFailed` too."", 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java,"@@ -1052,6 +1052,58 @@ public void testIndexQueriesWithIndexNotReady() throws Throwable
             execute(""DROP index "" + KEYSPACE + "".testIndex"");
         }
     }
+    
+    @Test
+    public void testIndexWritesWithIndexNotReady() throws Throwable
+    {
+        createTable(""CREATE TABLE %s (pk int, ck int, value int, PRIMARY KEY (pk, ck))"");
+        String indexName = createIndex(""CREATE CUSTOM INDEX ON %s (value) USING '"" + BlockingStubIndex.class.getName() + ""'"");
+
+        execute(""INSERT INTO %s (pk, ck, value) VALUES (?, ?, ?)"", 1, 1, 1);
+        BlockingStubIndex index = (BlockingStubIndex) getCurrentColumnFamilyStore().indexManager.getIndexByName(indexName);
+        assertEquals(0, index.rowsInserted.size());
+        execute(""DROP index "" + KEYSPACE + ""."" + indexName);
+    }
+    
+    @Test // A Bad init could leave an index only accepting reads
+    public void testReadOnlyIndex() throws Throwable
+    {
+        String tableName = createTable(""CREATE TABLE %s (pk int, ck int, value int, PRIMARY KEY (pk, ck))"");
+        String indexName = createIndex(""CREATE CUSTOM INDEX ON %s (value) USING '"" + ReadOnlyIndex.class.getName() + ""'"");
+        assertTrue(waitForIndex(keyspace(), tableName, indexName));
+
+        execute(""SELECT value FROM %s WHERE value = 1"");
+        execute(""INSERT INTO %s (pk, ck, value) VALUES (?, ?, ?)"", 1, 1, 1);
+
+        ReadOnlyIndex index = (ReadOnlyIndex) getCurrentColumnFamilyStore().indexManager.getIndexByName(indexName);
+        assertEquals(0, index.rowsInserted.size());
+        execute(""DROP index "" + KEYSPACE + ""."" + indexName);
+    }
+    
+    @Test  // A Bad init could leave an index only accepting writes
+    public void testWriteOnlyIndex() throws Throwable
+    {
+        String tableName = createTable(""CREATE TABLE %s (pk int, ck int, value int, PRIMARY KEY (pk, ck))"");
+        String indexName = createIndex(""CREATE CUSTOM INDEX ON %s (value) USING '"" + WriteOnlyIndex.class.getName() + ""'"");
+        assertTrue(waitForIndex(keyspace(), tableName, indexName));
+
+        execute(""INSERT INTO %s (pk, ck, value) VALUES (?, ?, ?)"", 1, 1, 1);
+        WriteOnlyIndex index = (WriteOnlyIndex) getCurrentColumnFamilyStore().indexManager.getIndexByName(indexName);
+        assertEquals(1, index.rowsInserted.size());
+        try
+        {
+            execute(""SELECT value FROM %s WHERE value = 1"");    
+            fail();
+        }
+        catch (IndexNotAvailableException e)
+        {
+            assertTrue(true);
+        }
+        finally
+        {
+            execute(""DROP index "" + KEYSPACE + ""."" + indexName);","[{'comment': 'Similarly to the comment before, we could try a recovery, and simulate the initializitation failure throwing an exception in `WriteOnlyIndex#getInitializationTask`.', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java,"@@ -119,37 +124,47 @@ public void addingSSTablesMarksTheIndexAsBuilt() throws Throwable
     }
 
     @Test
-    public void cannotRebuildWhileInitializationIsInProgress() throws Throwable
+    public void cannotRebuilRecoverdWhileInitializationIsInProgress() throws Throwable","[{'comment': '```suggestion\r\n    public void cannotRebuildRecoverWhileInitializationIsInProgress() throws Throwable\r\n```', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java,"@@ -1375,9 +1427,10 @@ public void testIndexOnFrozenUDT() throws Throwable
 
         execute(""INSERT INTO %s (k, v) VALUES (?, ?)"", 0, udt1);
         String indexName = createIndex(""CREATE INDEX ON %s (v)"");
+        assertTrue(waitForIndex(keyspace(), tableName, indexName));","[{'comment': 'I think that this is an important change of behaviour in indexes, they are missing writes done during their initialization. I think that this problem can be solved if we add them to `SIM#writableIndexes` at the beginning of their registration (before initialization), and we remove them from `writableIndexes` in `SIM#markIndexFailed`. WDYT?', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java,"@@ -1560,4 +1613,77 @@ public IndexBlockingOnInitialization(ColumnFamilyStore baseCfs, IndexMetadata in
             return super.getInvalidateTask();
         }
     }
+    
+    /**
+     * <code>StubIndex</code> that blocks during the initialization.
+     */
+    public static class BlockingStubIndex extends StubIndex
+    {
+        private final CountDownLatch latch = new CountDownLatch(1);
+
+        public BlockingStubIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        @Override
+        public Callable<?> getInitializationTask()
+        {
+            return () -> {
+                latch.await();
+                return null;
+            };
+        }
+
+        @Override
+        public Callable<?> getInvalidateTask()
+        {
+            latch.countDown();
+            return super.getInvalidateTask();
+        }
+    }
+
+    /**
+     * <code>CassandraIndex</code> that only supports reads. Could be intentional or a result of a bad init
+     */
+    public static class ReadOnlyIndex extends StubIndex
+    {
+        public ReadOnlyIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        @Override
+        public Callable<?> getInitializationTask()
+        {
+            return null;
+        }
+
+        public boolean supportsLoad(LoadType load)
+        {
+            return load.equals(LoadType.READ);
+        }
+    }
+
+    /**
+     * <code>CassandraIndex</code> that only supports writes. Could be intentional or a result of a bad init","[{'comment': 'Nit: is not a `CassandraIndex`:\r\n```suggestion\r\n     * {@code StubIndex} that only supports writes. Could be intentional or a result of a bad init.\r\n```', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java,"@@ -1560,4 +1613,77 @@ public IndexBlockingOnInitialization(ColumnFamilyStore baseCfs, IndexMetadata in
             return super.getInvalidateTask();
         }
     }
+    
+    /**
+     * <code>StubIndex</code> that blocks during the initialization.
+     */
+    public static class BlockingStubIndex extends StubIndex
+    {
+        private final CountDownLatch latch = new CountDownLatch(1);
+
+        public BlockingStubIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        @Override
+        public Callable<?> getInitializationTask()
+        {
+            return () -> {
+                latch.await();
+                return null;
+            };
+        }
+
+        @Override
+        public Callable<?> getInvalidateTask()
+        {
+            latch.countDown();
+            return super.getInvalidateTask();
+        }
+    }
+
+    /**
+     * <code>CassandraIndex</code> that only supports reads. Could be intentional or a result of a bad init","[{'comment': ""Nit: it's not a `CassandraIndex `:\r\n```suggestion\r\n     * {@code StubIndex} that only supports reads. Could be intentional or a result of a bad init.\r\n```"", 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java,"@@ -719,4 +746,20 @@ public boolean shouldBuildBlocking()
             return true;
         }
     }
+
+    /**
+     * <code>CassandraIndex</code> that only supports reads. Could be intentional or a result of a bad init
+     */
+    public static class ReadOnlyIndex extends TestingIndex
+    {
+        public ReadOnlyIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        public boolean supportsLoad(LoadType load)
+        {
+            return load.equals(LoadType.READ);","[{'comment': '```suggestion\r\n            return load == LoadType.READ;\r\n```', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java,"@@ -1560,4 +1613,77 @@ public IndexBlockingOnInitialization(ColumnFamilyStore baseCfs, IndexMetadata in
             return super.getInvalidateTask();
         }
     }
+    
+    /**
+     * <code>StubIndex</code> that blocks during the initialization.
+     */
+    public static class BlockingStubIndex extends StubIndex
+    {
+        private final CountDownLatch latch = new CountDownLatch(1);
+
+        public BlockingStubIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        @Override
+        public Callable<?> getInitializationTask()
+        {
+            return () -> {
+                latch.await();
+                return null;
+            };
+        }
+
+        @Override
+        public Callable<?> getInvalidateTask()
+        {
+            latch.countDown();
+            return super.getInvalidateTask();
+        }
+    }
+
+    /**
+     * <code>CassandraIndex</code> that only supports reads. Could be intentional or a result of a bad init
+     */
+    public static class ReadOnlyIndex extends StubIndex
+    {
+        public ReadOnlyIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        @Override
+        public Callable<?> getInitializationTask()
+        {
+            return null;
+        }
+
+        public boolean supportsLoad(LoadType load)
+        {
+            return load.equals(LoadType.READ);","[{'comment': '```suggestion\r\n            return load == LoadType.READ;\r\n```', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java,"@@ -1560,4 +1613,77 @@ public IndexBlockingOnInitialization(ColumnFamilyStore baseCfs, IndexMetadata in
             return super.getInvalidateTask();
         }
     }
+    
+    /**
+     * <code>StubIndex</code> that blocks during the initialization.
+     */
+    public static class BlockingStubIndex extends StubIndex
+    {
+        private final CountDownLatch latch = new CountDownLatch(1);
+
+        public BlockingStubIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        @Override
+        public Callable<?> getInitializationTask()
+        {
+            return () -> {
+                latch.await();
+                return null;
+            };
+        }
+
+        @Override
+        public Callable<?> getInvalidateTask()
+        {
+            latch.countDown();
+            return super.getInvalidateTask();
+        }
+    }
+
+    /**
+     * <code>CassandraIndex</code> that only supports reads. Could be intentional or a result of a bad init
+     */
+    public static class ReadOnlyIndex extends StubIndex
+    {
+        public ReadOnlyIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        @Override
+        public Callable<?> getInitializationTask()
+        {
+            return null;
+        }
+
+        public boolean supportsLoad(LoadType load)
+        {
+            return load.equals(LoadType.READ);
+        }
+    }
+
+    /**
+     * <code>CassandraIndex</code> that only supports writes. Could be intentional or a result of a bad init
+     */
+    public static class WriteOnlyIndex extends StubIndex
+    {
+        public WriteOnlyIndex(ColumnFamilyStore baseCfs, IndexMetadata indexDef)
+        {
+            super(baseCfs, indexDef);
+        }
+
+        @Override
+        public Callable<?> getInitializationTask()
+        {
+            return null;
+        }
+
+        public boolean supportsLoad(LoadType load)
+        {
+            return load.equals(LoadType.WRITE);","[{'comment': '```suggestion\r\n            return load == LoadType.WRITE;\r\n```', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/index/internal/CustomCassandraIndex.java,"@@ -77,6 +77,7 @@
     protected ColumnFamilyStore indexCfs;
     protected ColumnMetadata indexedColumn;
     protected CassandraIndexFunctions functions;
+    protected LoadType supportedLoads = LoadType.ALL;","[{'comment': '```suggestion\r\n    protected LoadType supportedLoadType = LoadType.ALL;\r\n```', 'commenter': 'adelapena'}]"
570,test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java,"@@ -719,4 +746,20 @@ public boolean shouldBuildBlocking()
             return true;
         }
     }
+
+    /**
+     * <code>CassandraIndex</code> that only supports reads. Could be intentional or a result of a bad init","[{'comment': '```suggestion\r\n     * <code>TestingIndex</code> that only supports reads. Could be intentional or a result of a bad init\r\n```', 'commenter': 'adelapena'}]"
575,src/java/org/apache/cassandra/db/SystemKeyspace.java,"@@ -1344,12 +1344,17 @@ public static void clearEstimates(String keyspace)
     public static synchronized SetMultimap<String, String> getTablesWithSizeEstimates()
     {
         SetMultimap<String, String> keyspaceTableMap = HashMultimap.create();
-        String cql = String.format(""SELECT keyspace_name, table_name FROM %s"", TableEstimates.toString(), TABLE_ESTIMATES_TYPE_PRIMARY);
-        UntypedResultSet rs = executeInternal(cql);
-        for (UntypedResultSet.Row row : rs)
+        // Its possible that size_estimates knows about a different set of keyspace/tables than table_estimates (mostly
+        // caused by external systems modifying the tables, such as dtest) so query both
+        for (String cql : Arrays.asList(
+            ""SELECT keyspace_name, table_name FROM "" + TableEstimates.toString(),
+            ""SELECT keyspace_name, table_name FROM "" + LegacySizeEstimates.toString()))","[{'comment': ""what's the performance impact of going from one table scan to two here?"", 'commenter': 'michaelsembwever'}, {'comment': 'I believe we have to do 2 table scans now until the table `size_estimates` goes away.\r\n\r\n', 'commenter': 'nastra'}, {'comment': 'nit: imo it would be more obvious to use `SchemaConstants.SYSTEM_KEYSPACE_NAME, TABLE_ESTIMATES` /  `SchemaConstants.SYSTEM_KEYSPACE_NAME, LEGACY_SIZE_ESTIMATES` in the constructed CQL query than using `TableEstimates.toString()` / `LegacySizeEstimates.toString()`.', 'commenter': 'nastra'}, {'comment': ""Do we expect the set of keyspaces in `table_estimates` and `size_estimates` to be mutually exclusive? If not, aren't you clobbering the estimates from `table_estimates` with the values from `size_estimates` table? Is that the expected behavior?"", 'commenter': 'dineshjoshi'}, {'comment': '@michaelsembwever the impact is related to the number of tables in the cluster; if you have 10 tables this is small, if you have 1k its much larger cost.  To @nastra point, its sadly unavoidable since the two tables could drift.  If you think about it from the migration standpoint, on T1 we have size_estimates as the source of truth, in T2 its now table_estimates, so if there happens to exist tables in size_estimates which are no longer in the new table_estimates they would live there forever.\r\n\r\nThe function is only called at two places:\r\n\r\n1) CassandraDaemon startup.  This could impact startup times as its now twice the cost; impacts clusters with more tables; Sadly there is no monitoring around this, so only know via stack traces...\r\n2) JMX. triggered via nodetool or via random JMX call. ', 'commenter': 'dcapwell'}, {'comment': '@nastra not sure I agree with the comment that toString is less obvious, is there a reason you say that?  In this case we don\'t care about quoting since it is known, but the `TableMetadata.toString` is a safer way to do this in general, so should be the new norm (in my opinion) rather than the common `format(""%s.%s"", ks_name, name)` (which is unsafe in general).', 'commenter': 'dcapwell'}, {'comment': ""> Do we expect the set of keyspaces in table_estimates and size_estimates to be mutually exclusive? If not, aren't you clobbering the estimates from table_estimates with the values from size_estimates table? Is that the expected behavior?\r\n\r\n@dineshjoshi I expect them to conflict 99% of the time (1% for random users writing random data), which is why a set is used to dedup; after a reboot they should have identical tables.\r\n\r\n>  clobbering the estimates from table_estimates with the values from size_estimates table?\r\n\r\nThis function only looks for table names, not values, so no clobbering estimates."", 'commenter': 'dcapwell'}, {'comment': 'I switched to truncate as this was a more expensive truncate....  That should resolve all comments another than the toString one.', 'commenter': 'dcapwell'}, {'comment': '> @nastra not sure I agree with the comment that toString is less obvious, is there a reason you say that? In this case we don\'t care about quoting since it is known, but the `TableMetadata.toString` is a safer way to do this in general, so should be the new norm (in my opinion) rather than the common `format(""%s.%s"", ks_name, name)` (which is unsafe in general).\r\n\r\n@dcapwell it\'s really just a personal preference and I don\'t have any strong objections around how it\'s done at the end. That\'s why I marked it as a nit. Your approach makes sense and is perfectly fine', 'commenter': 'nastra'}]"
575,src/java/org/apache/cassandra/db/SystemKeyspace.java,"@@ -1344,12 +1344,17 @@ public static void clearEstimates(String keyspace)
     public static synchronized SetMultimap<String, String> getTablesWithSizeEstimates()
     {
         SetMultimap<String, String> keyspaceTableMap = HashMultimap.create();
-        String cql = String.format(""SELECT keyspace_name, table_name FROM %s"", TableEstimates.toString(), TABLE_ESTIMATES_TYPE_PRIMARY);
-        UntypedResultSet rs = executeInternal(cql);
-        for (UntypedResultSet.Row row : rs)
+        // Its possible that size_estimates knows about a different set of keyspace/tables than table_estimates (mostly
+        // caused by external systems modifying the tables, such as dtest) so query both
+        for (String cql : Arrays.asList(
+            ""SELECT keyspace_name, table_name FROM "" + TableEstimates.toString(),
+            ""SELECT keyspace_name, table_name FROM "" + LegacySizeEstimates.toString()))
         {
-            keyspaceTableMap.put(row.getString(""keyspace_name""), row.getString(""table_name""));
+            UntypedResultSet rs = executeInternal(cql);
+            for (UntypedResultSet.Row row : rs)
+                keyspaceTableMap.put(row.getString(""keyspace_name""), row.getString(""table_name""));","[{'comment': 'are we potentially overwriting estimates that we read from `TABLE_ESTIMATES` with estimates that we read from `LEGACY_SIZE_ESTIMATES` on purpose?', 'commenter': 'nastra'}, {'comment': ""This function doesn't look at estimates but rather finds all table names.  This uses the `SetMultimap` to dedup keyspace and table names, so its fine (and expected) if the two tables have the same string values."", 'commenter': 'dcapwell'}]"
575,src/java/org/apache/cassandra/db/SystemKeyspace.java,"@@ -1327,30 +1327,15 @@ public static void clearEstimates(String keyspace, String table)
     }
 
     /**
-     * Clears size estimates for a keyspace (used to manually clean when we miss a keyspace drop)
+     * truncates size_estimates and table_estimates tabless","[{'comment': '```suggestion\r\n     * truncates size_estimates and table_estimates tables\r\n```', 'commenter': 'michaelsembwever'}]"
575,src/java/org/apache/cassandra/db/SystemKeyspace.java,"@@ -1327,30 +1327,15 @@ public static void clearEstimates(String keyspace, String table)
     }
 
     /**
-     * Clears size estimates for a keyspace (used to manually clean when we miss a keyspace drop)
+     * truncates size_estimates and table_estimates tabless
      */
-    public static void clearEstimates(String keyspace)
+    public static void truncateSizeEstimates()","[{'comment': '`clearAllEstimates()` would be more intuitive to me as a method name, and inline with its sibling method.', 'commenter': 'michaelsembwever'}]"
575,test/distributed/org/apache/cassandra/distributed/test/NodeToolSharedSingleNodeTest.java,"@@ -0,0 +1,84 @@
+package org.apache.cassandra.distributed.test;
+
+import java.io.IOException;
+
+import org.junit.AfterClass;
+import org.junit.Assume;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.distributed.api.IInstance;
+import org.apache.cassandra.distributed.api.QueryResult;
+import org.assertj.core.api.Assertions;
+
+public class NodeToolSharedSingleNodeTest extends TestBaseImpl","[{'comment': '`NodeToolSharedSingleNodeTest` is difficult to grok…\r\n\r\nWhat about just `TableEstimatesTest` ?', 'commenter': 'michaelsembwever'}, {'comment': ""The intent was to help rewrite the nodetool tests in jvm-dtest.  I have a style of reusing the cluster (since the tests are faster), if the nodetool tests don't share a cluster we will need 1 class file per test...\r\n\r\nThere is already a NodeTool test, which doesn't share the cluster..."", 'commenter': 'dcapwell'}]"
583,doc/source/getting_started/installing.rst,"@@ -187,7 +187,7 @@ Installing the Debian packages
 
 ::
 
-   $ curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -
+   $ curl -L https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -","[{'comment': ""rather than add the redirect, let's use the final destination.\r\n\r\neg\r\n`curl https://downloads.apache.org/cassandra/KEYS | sudo apt-key add -`\r\n\r\nAnd… can you base this against the cassandra-3.11 branch please. (It applies to all branches, but we only have docs currently for cassandra-3.11 and trunk)"", 'commenter': 'michaelsembwever'}, {'comment': 'gladly', 'commenter': 'phact'}, {'comment': ""should we keep the -L in case we redirect again in the future or do you think that's a security risk?"", 'commenter': 'phact'}, {'comment': 'Looks like this already got done in the 3.11 branch https://github.com/apache/cassandra/commit/c93c2983cd3f795ef81c7cd298a115e05a42ed5c#diff-6d9ad0776a3e0cdd1be608c3424af5b0', 'commenter': 'phact'}, {'comment': ""and propagated to the website https://cassandra.apache.org/download/ so I'm closing this issue."", 'commenter': 'phact'}]"
611,.circleci/config-2_1.yml,"@@ -277,7 +277,7 @@ executors:
         type: string
         default: medium
     docker:
-      - image: nastra/cassandra-testing-ubuntu1910-java11-w-dependencies:20200406","[{'comment': 'should now use `nastra/cassandra-testing-ubuntu1910-java11-w-dependencies:20200603`. ', 'commenter': 'nastra'}]"
635,src/java/org/apache/cassandra/service/DataResolver.java,"@@ -759,9 +759,10 @@ public UnfilteredPartitionIterator moreContents()
             return executeReadCommand(cmd);
         }
 
-        // Counts the number of rows for regular queries and the number of groups for GROUP BY queries
+        /** Returns the number of results counted by the counter */
         private int counted(Counter counter)
         {
+            // We are interested by the number of rows but for GROUP BY queries 'counted' returns the number of groups.
             return command.limits().isGroupByLimit()
                  ? counter.rowCounted()
                  : counter.counted();","[{'comment': ""@adelapena Even if we can only do this in trunk (because of the Thrift counter in 3.11), why don't we just get rid of the `counted()` method altogether and call `counter.rowCounted()` (which I would rename to `rowsCounted()`, btw) in all cases? `rowCounted()` and `counted()` return the same thing in the non-grouping case anyway :/"", 'commenter': 'maedhroz'}, {'comment': 'Makes sense, done for trunk.', 'commenter': 'adelapena'}]"
635,src/java/org/apache/cassandra/service/DataResolver.java,"@@ -904,9 +905,11 @@ public UnfilteredRowIterator moreContents()
                 return UnfilteredPartitionIterators.getOnlyElement(executeReadCommand(cmd), cmd);
             }
 
-            // Counts the number of rows for regular queries and the number of groups for GROUP BY queries
+            /** Returns the number of results counted in the partition by the counter */
             private int countedInCurrentPartition(Counter counter)
             {
+                // We are interested by the number of rows but for GROUP BY queries 'countedInCurrentPartition' returns
+                // the number of groups in the current partition.
                 return command.limits().isGroupByLimit()
                      ? counter.rowCountedInCurrentPartition()
                      : counter.countedInCurrentPartition();","[{'comment': '@adelapena Similar comment as above. `counter.rowCountedInCurrentPartition()` returns the same thing as `counter.countedInCurrentPartition()` in the non-grouping case.', 'commenter': 'maedhroz'}]"
635,src/java/org/apache/cassandra/db/filter/DataLimits.java,"@@ -934,7 +934,7 @@ protected Row applyToStatic(Row row)
                 // It's possible that we're ""done"" if the partition we just started bumped the number of groups (in
                 // applyToPartition() above), in which case Transformation will still call this method. In that case, we
                 // want to ignore the static row, it should (and will) be returned with the next page/group if needs be.
-                if (isDone())
+                if (enforceLimits && isDone())","[{'comment': ""@adelapena It doesn't look like anything in `TestConsistency::test_group_by_srp` fails if we remove this. (i.e. It looks like the new logic is correct, just doesn't have any coverage.)"", 'commenter': 'maedhroz'}, {'comment': 'Right, just added an in-JVM dtest for each condition.', 'commenter': 'adelapena'}]"
675,NEWS.txt,"@@ -38,6 +38,15 @@ using the provided 'sstableupgrade' tool.
 
 New features
 ------------
+    - The data of the system keyspaces using a local strategy (at the exception of the system.paxos table)
+      is now stored by default in the first data directory. This approach will allow the server
+      to tolerate the failure of the other disks. To ensure that a disk failure will not bring
+      a node down, it is possible to use the system_data_file_directory yaml property to store
+      the system keyspaces data on a disk that provide redundancy.
+      On node startup the system keyspace data will be automatically migrated if needed to the
+      correct location. If a specific disk has been used for some time and the system keyspaces","[{'comment': '```suggestion\r\n      the local system keyspaces data on a directory that provides redundancy.\r\n      On node startup the local system keyspaces data will be automatically migrated if needed to the\r\n      correct location. If a specific disk has been used for some time and the local system keyspaces\r\n```', 'commenter': 'adelapena'}]"
675,conf/cassandra.yaml,"@@ -203,6 +203,12 @@ partitioner: org.apache.cassandra.dht.Murmur3Partitioner
 # data_file_directories:
 #     - /var/lib/cassandra/data
 
+# Directory were Cassandra should store the data of the local system keyspaces.
+# By default Cassandra will store the data of the local system keyspaces in the first of the data directories.
+# This approach ensure that if one of the other disk is lost Cassandra can continue to operate. For extra security
+# this setting allow to store those data on a different directory that provide redundancy.","[{'comment': '```suggestion\r\nBy default Cassandra will store the data of the local system keyspaces in the first of the data directories specified\r\n# by data_file_directories.\r\n# This approach ensures that if one of the other disks is lost Cassandra can continue to operate. For extra security\r\n# this setting allows to store those data on a different directory that provides redundancy.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.","[{'comment': '```suggestion\r\n     * Checks if the local system data must be stored in a specific location which supports redundancy.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.","[{'comment': '```suggestion\r\n     * Returns the locations where the local system keyspaces data should be stored.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>","[{'comment': '```suggestion\r\n     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>\r\n```', 'commenter': 'adelapena'}, {'comment': ""Nit: Although strictly correct, the `can tolerate the lost of n - 1 disks` might sound like the descriptions of RAID levels where they can tolerate the loss of any n - 1 disks, where here we can only tolerate the loss of the last n - 1 disks, which is worst (but better than before). I'd rephrase to something like `can tolerate the loss of any disk but the first one`, wdyt?"", 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data","[{'comment': '```suggestion\r\n     * Returns the locations where the non local system keyspaces data should be stored.\r\n     *\r\n     * @return the locations where the non local system keyspaces data should be stored.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data
+     */
+    public static String[] getNonSystemKeyspacesDataFileLocations()","[{'comment': ""Naming this is tricky, since these locations actually contain data of system keyspaces that are not local. Maybe `getSystemKeyspacesDataFileLocations` can be changed to the even longer `getLocalSystemKeyspacesDataFileLocations`, but I can't really think of a better name for this one."", 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -2740,4 +2730,81 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. System keyspaces can have their own disk
+     * to allow for special redundency mechanism. If it is the case the executor services returned for
+     * system keyspace will be differents from the ones for the other keyspaces.</p>","[{'comment': '```suggestion\r\n     * to allow for special redundancy mechanism. If it is the case the executor services returned for\r\n     * local system keyspaces will be different from the ones for the other keyspaces.</p>\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -2740,4 +2730,81 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. System keyspaces can have their own disk
+     * to allow for special redundency mechanism. If it is the case the executor services returned for
+     * system keyspace will be differents from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non system keyspaces.
+         */
+        private final ExecutorService[] nonSystemflushExecutors;
+
+        /**
+         * The flush executors for system keyspaces.
+         */
+        private final ExecutorService[] systemDiskFlushExecutors;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonSystemflushExecutors = flushExecutors;
+            systemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""SystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
+        {
+            ExecutorService[] flushExecutors = new ExecutorService[numberOfExecutors];
+
+            for (int i = 0; i < numberOfExecutors; i++)
+            {
+                flushExecutors[i] = newThreadPool(""PerDiskMemtableFlushWriter_"" + i, flushWriters);
+            }
+            return flushExecutors;
+        }
+
+        private static JMXEnabledThreadPoolExecutor newThreadPool(String poolName, int size)
+        {
+            return new JMXEnabledThreadPoolExecutor(size,
+                                                    Stage.KEEP_ALIVE_SECONDS,
+                                                    TimeUnit.SECONDS,
+                                                    new LinkedBlockingQueue<Runnable>(),
+                                                    new NamedThreadFactory(poolName),
+                                                    ""internal"");
+        }
+
+        /**
+         * Returns the flush executors for the specified keyspace.
+         *
+         * @param keyspaceName the keyspace name
+         * @param tableName the table name
+         * @return the flush executors that should be used for flushing the memtables of the specified keyspace.
+         */
+        public ExecutorService[] getExecutorsFor(String keyspaceName, String tableName)
+        {
+            return Directories.isStoredInSystemKeyspacesDataLocation(keyspaceName, tableName) ? systemDiskFlushExecutors
+                                                                  : nonSystemflushExecutors;
+        }
+
+        /**
+         * Appends all the {@code ExecutorService} used for flushes to the colection.
+         *
+         * @param collection the colection to append to.
+         */
+        public void appendAllExecutors(Collection<ExecutorService> collection)
+        {
+            Collections.addAll(collection, nonSystemflushExecutors);
+            if (nonSystemflushExecutors != systemDiskFlushExecutors)","[{'comment': 'Nothing wrong with how the `!=` operator is used here but a comment about its intentionality and/or a `@SuppressWarnings(""ArrayEquality"")` annotation could be helpful for suspicious readers.', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/Directories.java,"@@ -512,6 +507,9 @@ public boolean hasAvailableDiskSpace(long estimatedSSTables, long expectedTotalW
                 allowedDirs.add(dir);
         }
 
+        if (allowedDirs.isEmpty())
+            throw new FSNoDiskAvailableForWriteError(metadata.keyspace);
+","[{'comment': 'Not related to the patch, but we could simplify the call to `Collections.sort` right below with:\r\n```java\r\nallowedDirs.sort(Comparator.comparing(o -> o.location));\r\n``` ', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/Directories.java,"@@ -591,10 +589,30 @@ public static File getBackupsDirectory(File location)
         }
     }
 
+    /**
+     * Checks if the specified table should be stored with locale system data.
+     *
+     * <p> To minimize the risk of failures, SSTables for local system keyspaces must be stored in a single data
+     * directory. The only exception to this is the system paxos table as it can be a high traffic table.</p>
+     *
+     * @param keyspace the keyspace name
+     * @param table the table name
+     * @return {@code true} if the specified table should be stored with locale system data, {@code false} otherwise.","[{'comment': '```suggestion\r\n     * Checks if the specified table should be stored with local system data.\r\n     *\r\n     * <p> To minimize the risk of failures, SSTables for local system keyspaces must be stored in a single data\r\n     * directory. The only exception to this is the system paxos table as it can be a high traffic table.</p>\r\n     *\r\n     * @param keyspace the keyspace name\r\n     * @param table the table name\r\n     * @return {@code true} if the specified table should be stored with local system data, {@code false} otherwise.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/Directories.java,"@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.","[{'comment': '```suggestion\r\n         * The directories for storing the local system keyspaces.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/Directories.java,"@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where should be stored the data of the non system keyspaces.","[{'comment': '```suggestion\r\n         * The directories where the data of the non local system keyspaces should be stored.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/Directories.java,"@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where should be stored the data of the non system keyspaces.
+         */
+        private final DataDirectory[] nonSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            systemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories used to store the data of the specified keyspace.
+         *
+         * @param keyspace the keyspace name
+         * @return the data directories used to store the data of the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesUsedBy(String keyspace)
+        {
+            if (SchemaConstants.SYSTEM_KEYSPACE_NAME.equals(keyspace)
+                    && !ArrayUtils.isEmpty(systemKeyspaceDataDirectories)
+                    && !ArrayUtils.contains(nonSystemKeyspacesDirectories, systemKeyspaceDataDirectories[0]))
+            {
+                DataDirectory[] directories = Arrays.copyOf(nonSystemKeyspacesDirectories, nonSystemKeyspacesDirectories.length + 1);
+                directories[directories.length - 1] = systemKeyspaceDataDirectories[0];
+                return directories;
+            }
+            return SchemaConstants.isLocalSystemKeyspace(keyspace) ? systemKeyspaceDataDirectories
+                                                                   : nonSystemKeyspacesDirectories;
+        }
+
+        /**
+         * Returns the data directories for the specified keyspace.
+         *
+         * @param table the table metadata
+         * @return the data directories for the specified keyspace","[{'comment': '```suggestion\r\n         * Returns the data directories for the specified table.\r\n         *\r\n         * @param table the table metadata\r\n         * @return the data directories for the specified table\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/Directories.java,"@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where should be stored the data of the non system keyspaces.
+         */
+        private final DataDirectory[] nonSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            systemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories used to store the data of the specified keyspace.
+         *
+         * @param keyspace the keyspace name
+         * @return the data directories used to store the data of the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesUsedBy(String keyspace)
+        {
+            if (SchemaConstants.SYSTEM_KEYSPACE_NAME.equals(keyspace)
+                    && !ArrayUtils.isEmpty(systemKeyspaceDataDirectories)
+                    && !ArrayUtils.contains(nonSystemKeyspacesDirectories, systemKeyspaceDataDirectories[0]))
+            {
+                DataDirectory[] directories = Arrays.copyOf(nonSystemKeyspacesDirectories, nonSystemKeyspacesDirectories.length + 1);
+                directories[directories.length - 1] = systemKeyspaceDataDirectories[0];
+                return directories;
+            }
+            return SchemaConstants.isLocalSystemKeyspace(keyspace) ? systemKeyspaceDataDirectories
+                                                                   : nonSystemKeyspacesDirectories;
+        }
+
+        /**
+         * Returns the data directories for the specified keyspace.
+         *
+         * @param table the table metadata
+         * @return the data directories for the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesFor(TableMetadata table)
+        {
+            return isStoredInSystemKeyspacesDataLocation(table.keyspace, table.name) ? systemKeyspaceDataDirectories
+                                                                                     : nonSystemKeyspacesDirectories;
+        }
+
+        @Override
+        public Iterator<DataDirectory> iterator()
+        {
+            Iterator<DataDirectory> iter = Iterators.forArray(nonSystemKeyspacesDirectories);
+
+            if (nonSystemKeyspacesDirectories == systemKeyspaceDataDirectories)","[{'comment': 'Same as before, we could add comment about the intentional use of `==` and/or a `@SuppressWarnings(""ArrayEquality"")` annotation.', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/Directories.java,"@@ -1002,8 +1120,7 @@ public long getTrueAllocatedSizeIn(File input)
 
     public static List<File> getKSChildDirectories(String ksName)
     {
-        return getKSChildDirectories(ksName, dataDirectories);
-
+        return getKSChildDirectories(ksName, dataDirectories.getDataDirectoriesUsedBy(ksName));","[{'comment': 'Maybe the two versions of `getKSChildDirectories` can be merged, given that one is the only caller of the other?', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -2740,4 +2730,81 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. System keyspaces can have their own disk
+     * to allow for special redundency mechanism. If it is the case the executor services returned for
+     * system keyspace will be differents from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non system keyspaces.
+         */
+        private final ExecutorService[] nonSystemflushExecutors;
+
+        /**
+         * The flush executors for system keyspaces.
+         */
+        private final ExecutorService[] systemDiskFlushExecutors;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonSystemflushExecutors = flushExecutors;
+            systemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""SystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
+        {
+            ExecutorService[] flushExecutors = new ExecutorService[numberOfExecutors];
+
+            for (int i = 0; i < numberOfExecutors; i++)
+            {
+                flushExecutors[i] = newThreadPool(""PerDiskMemtableFlushWriter_"" + i, flushWriters);
+            }
+            return flushExecutors;
+        }
+
+        private static JMXEnabledThreadPoolExecutor newThreadPool(String poolName, int size)
+        {
+            return new JMXEnabledThreadPoolExecutor(size,
+                                                    Stage.KEEP_ALIVE_SECONDS,
+                                                    TimeUnit.SECONDS,
+                                                    new LinkedBlockingQueue<Runnable>(),","[{'comment': '```suggestion\r\n                                                    new LinkedBlockingQueue<>(),\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/io/FSDiskFullWriteError.java,"@@ -18,16 +18,22 @@
 
 package org.apache.cassandra.io;
 
+import java.io.File;
+import java.io.IOException;
+
 public class FSDiskFullWriteError extends FSWriteError
 {
-    public FSDiskFullWriteError(Throwable cause, String path)
+    public FSDiskFullWriteError(String keyspace, long mutationSize)
     {
-        super(cause, path);
+        super(new IOException(String.format(""Insufficient disk space to write %s bytes into the %s keyspace"",","[{'comment': '```suggestion\r\n        super(new IOException(String.format(""Insufficient disk space to write %d bytes into the %s keyspace"",\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/io/FSDiskFullWriteError.java,"@@ -18,16 +18,22 @@
 
 package org.apache.cassandra.io;
 
+import java.io.File;
+import java.io.IOException;
+
 public class FSDiskFullWriteError extends FSWriteError
 {
-    public FSDiskFullWriteError(Throwable cause, String path)
+    public FSDiskFullWriteError(String keyspace, long mutationSize)
     {
-        super(cause, path);
+        super(new IOException(String.format(""Insufficient disk space to write %s bytes into the %s keyspace"",
+                                            mutationSize,
+                                            keyspace)),
+              new File(""""));","[{'comment': 'Nit: Both `FSDiskFullWriteError` and `FSNoDiskAvailableForWriteError` call the super constructor of `FSWriteError` with `new File("""")`. Maybe we could add a new `FSWriteError` constructor without the file argument.', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/io/util/FileUtils.java,"@@ -920,4 +922,69 @@ public Object getAttribute(String attribute) throws IOException
             return fileStore.getAttribute(attribute);
         }
     }
+
+
+    /**
+     * Moves the files from a directory to another directory.
+     * <p>Once a file has been copied to the target directory it will be deleted from the source directory.
+     * If a file already exist in the target directory a warning will be logged and the file will not
+     * be deleted.</p>","[{'comment': 'Nit: it also moves subdirectories.\r\n```suggestion\r\n\r\n    /**\r\n     * Moves the contents of a directory to another directory.\r\n     * <p>Once a file has been copied to the target directory it will be deleted from the source directory.\r\n     * If a file already exists in the target directory a warning will be logged and the file will not\r\n     * be deleted.</p>\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/io/util/FileUtils.java,"@@ -920,4 +922,69 @@ public Object getAttribute(String attribute) throws IOException
             return fileStore.getAttribute(attribute);
         }
     }
+
+
+    /**
+     * Moves the files from a directory to another directory.
+     * <p>Once a file has been copied to the target directory it will be deleted from the source directory.
+     * If a file already exist in the target directory a warning will be logged and the file will not
+     * be deleted.</p>
+     *
+     * @param source the directory containing the files to move
+     * @param target the directory where the files must be moved
+     */
+    public static void moveRecursively(Path source, Path target) throws IOException
+    {
+        logger.info(""Moving {} to {}"" , source, target);
+
+        if (Files.isDirectory(source))
+        {
+            Files.createDirectories(target);
+
+            try (Stream<Path> paths = Files.list(source))
+            {
+                Path[] children = paths.toArray(Path[]::new);
+
+                for (Path child : children)
+                    moveRecursively(child, target.resolve(source.relativize(child)));
+            }
+
+            deleteDirectoryIfEmpty(source);
+        }
+        else
+        {
+            if (Files.exists(target))
+            {
+                logger.warn(""Cannot move the file {} to {} as the target file already exists."" , source, target);
+            }
+            else
+            {
+                Files.copy(source, target, StandardCopyOption.COPY_ATTRIBUTES);
+                Files.delete(source);
+            }
+        }
+    }
+
+    /**
+     * Deletes the specified directory if it is empty
+     *
+     * @param path the path to the directory
+     */
+    public static void deleteDirectoryIfEmpty(Path path) throws IOException","[{'comment': 'I think this could be called with the path of a file, we should probably check that the path belongs to a directory. It would be nice to have a simple unit test for this method in `FileUtilsTest`.', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/service/CassandraDaemon.java,"@@ -212,6 +219,19 @@ protected void setup()
     {
         FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());
 
+        // Since CASSANDRA-14793 the local system file data are not dispatched accross the data directories
+        // anymore to reduce the risks in case of disk failures. By consequence, the system need to ensure in case of
+        // upgrade that the old data files have been migrated to the new directories before we start deleting
+        // snapshot and upgrading system tables.","[{'comment': '```suggestion\r\n        // Since CASSANDRA-14793 the local system keyspaces data are not dispatched across the data directories\r\n        // anymore to reduce the risks in case of disk failures. By consequence, the system need to ensure in case of\r\n        // upgrade that the old data files have been migrated to the new directories before we start deleting\r\n        // snapshots and upgrading system tables.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/service/CassandraDaemon.java,"@@ -243,7 +263,7 @@ protected void setup()
         }
         catch (IOException e)
         {
-            exitOrFail(3, e.getMessage(), e.getCause());
+            exitOrFail(StartupException.ERR_WRONG_DISK_STATE, e.getMessage(), e.getCause());","[{'comment': 'Good catch', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/service/CassandraDaemon.java,"@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.importSystemDataFilesFrom"");
+
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (DatabaseDescriptor.useSpecificLocationForSystemData()
+                || DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations().length > 1
+                || importSystemDataFrom != null)","[{'comment': 'We could invert this condition and return directly to reduce nesting:\r\n```java\r\nif (!DatabaseDescriptor.useSpecificLocationForSystemData()\r\n    && DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations().length <= 1\r\n    && importSystemDataFrom == null)\r\n    return;\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/service/CassandraDaemon.java,"@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.importSystemDataFilesFrom"");
+
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (DatabaseDescriptor.useSpecificLocationForSystemData()
+                || DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations().length > 1
+                || importSystemDataFrom != null)
+        {
+            // We can face several cases:
+            //  1) The system data are spread accross the data file locations and need to be moved to
+            //     the first data location (upgrade to 4.0)
+            //  2) The system data are spread accross the data file locations and need to be moved to
+            //     the system keyspace location configured by the user (upgrade to 4.0)
+            //  3) The system data are stored in the first data location and need to be moved to
+            //     the system keyspace location configured by the user (system_data_file_directory has been configured)
+            //  4) The system data have been stored in the system keyspace location configured by the user
+            //     and need to be moved to the first data location (the import of the data has been requested)","[{'comment': 'Nice comment!', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/service/CassandraDaemon.java,"@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException","[{'comment': 'It would be nice to have some upgrade dtests for this, provided that we have the machinery to do so in either Java or Python dtests.', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java,"@@ -26,10 +26,9 @@
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.db.DisallowedDirectories;
 import org.apache.cassandra.db.Keyspace;
-import org.apache.cassandra.io.FSError;
-import org.apache.cassandra.io.FSErrorHandler;
-import org.apache.cassandra.io.FSReadError;
+import org.apache.cassandra.io.*;
 import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.schema.SchemaConstants;","[{'comment': 'Nit: unused', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java,"@@ -67,6 +66,18 @@ public void handleFSError(FSError e)
                 StorageService.instance.stopTransports();
                 break;
             case best_effort:
+
+                // There are a few scenarios where we know that the node will not be able to operate properly","[{'comment': '```suggestion\r\n                // There are a few scenarios where we know that the node will not be able to operate properly.\r\n```', 'commenter': 'adelapena'}]"
675,src/java/org/apache/cassandra/service/StorageService.java,"@@ -3291,12 +3291,29 @@ public String getKeyspaceReplicationInfo(String keyspaceName)
         return stringify(Gossiper.instance.getUnreachableMembers(), true);
     }
 
+    @Override
     public String[] getAllDataFileLocations()
     {
-        String[] locations = DatabaseDescriptor.getAllDataFileLocations();
-        for (int i = 0; i < locations.length; i++)
-            locations[i] = FileUtils.getCanonicalPath(locations[i]);
-        return locations;
+        return getCanonicalPaths(DatabaseDescriptor.getAllDataFileLocations());
+    }
+
+    private String[] getCanonicalPaths(String[] paths)
+    {
+        for (int i = 0; i < paths.length; i++)
+            paths[i] = FileUtils.getCanonicalPath(paths[i]);
+        return paths;
+    }","[{'comment': 'Overwriting the input array seems like it could have risky side effects, like overriding the contents of `DatabaseDescriptor.conf.data_file_directories`. I think we should return a new array.', 'commenter': 'adelapena'}]"
737,src/java/org/apache/cassandra/io/util/CompressedChunkReader.java,"@@ -85,28 +88,28 @@ public Rebufferer instantiateRebufferer()
 
     public static class Standard extends CompressedChunkReader
     {
-        // we read the raw compressed bytes into this buffer, then uncompressed them into the provided one.
-        private final ThreadLocal<ByteBuffer> compressedHolder;
+        // We read the raw compressed bytes into a buffer, then uncompressed them into the provided one.
+        // Notice we have 1 SimpleCachedBufferPool per BBType which wraps a FastThreadLocal BB
+        private static final EnumMap<BufferType, SimpleCachedBufferPool> reusableCompressBBs = new EnumMap<>(BufferType.class);","[{'comment': 'This does not fix having to recreate buffers if the type changes, because the underlying thread local map is still a single static.\r\nI would use the static `EnumMap` in `SimpleCachedBufferPool` instead.', 'commenter': 'blambov'}, {'comment': ':facepalm: apologies...', 'commenter': 'bereng'}]"
737,src/java/org/apache/cassandra/db/commitlog/SimpleCachedBufferPool.java,"@@ -79,12 +88,12 @@ public ByteBuffer createBuffer(BufferType bufferType)
 
     public ByteBuffer getThreadLocalReusableBuffer(int size)
     {
-        ByteBuffer result = reusableBufferHolder.get();
-        if (result.capacity() < size || BufferType.typeOf(result) != preferredReusableBufferType)
+        ByteBuffer result = reusableBBHolder.get(preferredReusableBufferType).get();","[{'comment': 'Nit: Instead of `preferredReusableBufferType` we can store the result of `reusableBBHolder.get(preferredReusableBufferType)` on `setPreferredReusableBufferType` calls to avoid the `EnumMap` lookup.', 'commenter': 'blambov'}, {'comment': 'Not so nit imo. Getting the buffer is sure more prone to being part of a hot path than changing the type. So fiddly but I gave it a go.', 'commenter': 'bereng'}]"
737,src/java/org/apache/cassandra/db/commitlog/SimpleCachedBufferPool.java,"@@ -37,6 +37,8 @@
 public class SimpleCachedBufferPool
 {
     private static final EnumMap<BufferType, FastThreadLocal<ByteBuffer>> reusableBBHolder = new EnumMap<>(BufferType.class);
+    // Convenience variable holding a ref to the current resuableBB to avoid map lookups
+    private static FastThreadLocal<ByteBuffer> reusableBB;","[{'comment': 'This should not be static.', 'commenter': 'blambov'}, {'comment': ""Ok I see it now. That BB can be of any BBType and as we no longer switch on the fly between types it can't be static."", 'commenter': 'bereng'}]"
737,src/java/org/apache/cassandra/db/commitlog/SimpleCachedBufferPool.java,"@@ -88,19 +91,21 @@ public ByteBuffer createBuffer(BufferType bufferType)
 
     public ByteBuffer getThreadLocalReusableBuffer(int size)
     {
-        ByteBuffer result = reusableBBHolder.get(preferredReusableBufferType).get();
+        ByteBuffer result = reusableBB.get();
         if (result.capacity() < size)
         {
             FileUtils.clean(result);
             result = preferredReusableBufferType.allocate(size);
             reusableBBHolder.get(preferredReusableBufferType).set(result);","[{'comment': 'This is now redundant.', 'commenter': 'blambov'}, {'comment': 'right you are!', 'commenter': 'bereng'}]"
737,src/java/org/apache/cassandra/db/commitlog/SimpleCachedBufferPool.java,"@@ -72,6 +74,7 @@ public SimpleCachedBufferPool(int maxBufferPoolSize, int bufferSize)
     {","[{'comment': '`preferredReusableBufferType` above should no longer be necessary.', 'commenter': 'blambov'}, {'comment': ""That I tried and then you need to do a `BufferType.typeOf()`... So it's an extra operation vs having that stored in a variable."", 'commenter': 'bereng'}]"
737,src/java/org/apache/cassandra/db/commitlog/SimpleCachedBufferPool.java,"@@ -63,6 +74,7 @@ public SimpleCachedBufferPool(int maxBufferPoolSize, int bufferSize)
     {
         this.maxBufferPoolSize = maxBufferPoolSize;
         this.bufferSize = bufferSize;
+        SimpleCachedBufferPool.reusableBB = reusableBBHolder.get(preferredReusableBufferType);","[{'comment': 'Nit: replace `SimpleCachedBufferPool` with `this`.', 'commenter': 'blambov'}]"
737,src/java/org/apache/cassandra/db/commitlog/SimpleCachedBufferPool.java,"@@ -79,19 +91,20 @@ public ByteBuffer createBuffer(BufferType bufferType)
 
     public ByteBuffer getThreadLocalReusableBuffer(int size)
     {
-        ByteBuffer result = reusableBufferHolder.get();
-        if (result.capacity() < size || BufferType.typeOf(result) != preferredReusableBufferType)
+        ByteBuffer result = reusableBB.get();
+        if (result.capacity() < size)
         {
             FileUtils.clean(result);
             result = preferredReusableBufferType.allocate(size);
-            reusableBufferHolder.set(result);
+            reusableBB.set(result);
         }
         return result;
     }
 
     public void setPreferredReusableBufferType(BufferType type)
     {
         preferredReusableBufferType = type;
+        reusableBB = reusableBBHolder.get(preferredReusableBufferType);","[{'comment': 'Should we not have `preferredReusableBufferType` and  `reusableBB` be `volatile` variables and the   `setPreferredReusableBufferType` method `synchronized` ? With the current implementation, it seems to me that, if a thread call `setPreferredReusableBufferType` and another `getThreadLocalReusableBuffer` the output of `getThreadLocalReusableBuffer` is unpredictable and the wrong buffer type can be added the `reusableBB` field.\r\nAm I missing something?', 'commenter': 'blerer'}, {'comment': ""Making them volatile won't change much, the right thing to do is to not allow the preferred type to change. Is it possible to pass the type as a constructor argument and make `reuseableBB` final?"", 'commenter': 'blambov'}, {'comment': 'You are right we can still have a race.\r\nThe preferedType is set in the `CompressedSegment` and in the `EncryptedSegment` constructor. I do not recall if we can change the commit log configuration without restarting the server. If not, we can figure out which type will be the prefered one based on the `commitLog.configuration`. It is not super clean has it somehow duplicates the logic from `createSegment` but it should work. ', 'commenter': 'blerer'}, {'comment': ""Given this `getThreadLocalReusableBuffer` seems to be isolated within the class, as it is not tied in with the buffer pool functinality, I don't like adding this param to the constructor. It reads as if this was the preferred type to the BBs in the pool... sounds confusing to me, changes the 'main' API, it might not get used if you only use the pool functionality\r\n\r\nI would like to propose the latest commit which does away with both variables. The price to pay is the Map lookup. I read a bit about `EnumMap`'s implementation and it is an Array indexed by the Enum int. So it should be pretty fast imo.\r\n\r\nWdyt? you guys know better how this class plays within the hot-paths."", 'commenter': 'bereng'}, {'comment': 'You are right, there is an issue with the class API. Even with the `setPreferredReusableBufferType` the API is confusing as it only applies to the `ThreadLocal` logic and not to the buffer pool. The calls to `createBuffer` are also performed from within the `CompressedSegment` and `EncryptedSegment` using the same type as the `preferredReusableBufferType`. So we can pass the buffer type as part of the constructor and remove the parameter from `createBuffer`. It will fix the threading issue and the API at the same time. Wdyt? ', 'commenter': 'blerer'}, {'comment': '@bereng The approach of your last commit is also fine for me. I just tend to prefer the constructor approach because it avoid a lookup and makes the code easier to read (less parameters being passed around). That being said, I am open for both solutions.', 'commenter': 'blerer'}, {'comment': 'Indeed the API for the pool could benefit from that imo as well. Adding the param to the constructor and making both the pool and the reusableBB follow that type sgtm and a better option overall. A quick look at the code seems to suggest BBType is reachable at construction time :-)', 'commenter': 'bereng'}, {'comment': ""I have pushed the latest change where preferred BBType is set at construction time.  Despite the API being much nicer imo, given we want to move away from singletons, static state/inits, etc I wonder if this is a step in the wrong direction.\r\n\r\nEdit: Btw, the original implementation wasn't thread safe either, as `setPreferredReusableBufferType()` could be called by some other thread anytime changing things under your feet. That was a bug right?\r\n\r\nCI [fails](https://app.circleci.com/pipelines/github/bereng/cassandra/120/workflows/7f806bb7-4339-421f-9486-ecc47d073afd/jobs/916): Investigating"", 'commenter': 'bereng'}, {'comment': ""So apparently yes, it was a step in the 'wrong' direction. We have tests that apparently change config [on the fly](https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/db/RecoveryManagerFlushedTest.java#L71). So the static init [won't play nice](https://app.circleci.com/pipelines/github/bereng/cassandra/120/workflows/7f806bb7-4339-421f-9486-ecc47d073afd/jobs/916) with that.\r\n\r\nHence I propose we settle on the solution with the `EnumMap` [lookup](https://github.com/apache/cassandra/commit/1e4b2f483d221bb1adce212dac4b2109ca0e9789)\r\n\r\nwdyt?"", 'commenter': 'bereng'}, {'comment': '@blerer suggested using a simple instance variable instead of a static one. The assumption is there will always only be one `AbstractCommitLogSegmentManager` instance. This allows for the imrpoved API and for tests to change config on the fly. Latest commit available.', 'commenter': 'bereng'}]"
737,src/java/org/apache/cassandra/db/commitlog/SimpleCachedBufferPool.java,"@@ -107,6 +120,9 @@ public void releaseBuffer(ByteBuffer buffer)
     public void shutdown()
     {
         bufferPool.clear();
+        for (FastThreadLocal<ByteBuffer> bbHolder : reusableBBHolder.values())
+            bbHolder.remove();
+        reusableBB.remove();","[{'comment': '`remove` is called twice on the same `ThreadLocal` instance as the `reusableBB` is also in the `bbHolder` variable\r\n\r\nMoreover, the `remove` call will remove the buffers values associated to the thread calling the `shutdown` method and I do not believe that it is what we want. The goal of the approach is to reuse buffer. Calling `remove` will prevent the reuse on the next call forcing the buffer to be recreated.    ', 'commenter': 'blerer'}]"
737,src/java/org/apache/cassandra/io/util/CompressedChunkReader.java,"@@ -86,27 +87,21 @@ public Rebufferer instantiateRebufferer()
     public static class Standard extends CompressedChunkReader
     {
         // we read the raw compressed bytes into this buffer, then uncompressed them into the provided one.
-        private final ThreadLocal<ByteBuffer> compressedHolder;
+        private final SimpleCachedBufferPool reusableCompressBB;
+        private final int compressSize = getCompressSize();
 
         public Standard(ChannelProxy channel, CompressionMetadata metadata)
         {
             super(channel, metadata);
-            compressedHolder = ThreadLocal.withInitial(this::allocateBuffer);
+            reusableCompressBB = new SimpleCachedBufferPool(0, getCompressSize());","[{'comment': 'We compute twice the compress size. I should do something like:\r\n```\r\ncompressSize = getCompressSize(metadata);\r\nreusableCompressBB = new SimpleCachedBufferPool(0, compressSize);\r\n```', 'commenter': 'blerer'}]"
737,src/java/org/apache/cassandra/db/commitlog/AbstractCommitLogSegmentManager.java,"@@ -166,7 +148,22 @@ public void runMayThrow() throws Exception
             }
         };
 
+        BufferType BBType = SimpleCachedBufferPool.DEFAULT_PREFERRED_BB_TYPE;
+        if (commitLog.configuration.useEncryption())
+            // Keep reusable buffers on-heap regardless of compression preference so we avoid copy off/on repeatedly during decryption
+            // Also: we want to keep the compression buffers on-heap as we need those bytes for encryption,
+            // and we want to avoid copying from off-heap (compression buffer) to on-heap encryption APIs
+            BBType = BufferType.ON_HEAP;
+        else if (commitLog.configuration.useCompression())
+            BBType = commitLog.configuration.getCompressor().preferredBufferType();
+
+        synchronized(this)
+        {
+            this.bufferPool = new SimpleCachedBufferPool(DatabaseDescriptor.getCommitLogMaxCompressionBuffersInPool(), DatabaseDescriptor.getCommitLogSegmentSize(), BBType);","[{'comment': ""We can make this more 'solid' by closing the pool before re-creation to defend against `start()` calls without a matching ' close()`."", 'commenter': 'bereng'}]"
742,test/distributed/org/apache/cassandra/distributed/test/ForceRepairTest.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import java.io.IOException;
+import java.util.Set;
+import java.util.function.Consumer;
+import java.util.stream.Stream;
+
+import com.google.common.collect.ImmutableMap;
+import org.junit.After;
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ICluster;
+import org.apache.cassandra.distributed.api.IInstanceConfig;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+
+public class ForceRepairTest extends TestBaseImpl
+{
+    private Cluster cluster;
+
+    private static Cluster create(Consumer<IInstanceConfig> configModifier, int clusterSize, int replicationFactor) throws IOException
+    {
+        configModifier = configModifier.andThen(
+        config -> config.set(""hinted_handoff_enabled"", false)
+                        .set(""commitlog_sync_batch_window_in_ms"", 5)
+                        .with(NETWORK)
+                        .with(GOSSIP)
+        );
+
+        return init(Cluster.build().withNodes(clusterSize).withConfig(configModifier).start(), replicationFactor);
+    }
+
+    void forceRepair(ICluster<IInvokableInstance> cluster, Integer... downClusterIds) throws Exception
+    {
+        RepairTest.populate(cluster, ""{'enabled': false}"");
+        Stream.of(downClusterIds).forEach(id -> cluster.get(id).shutdown());
+        RepairTest.repair(cluster, ImmutableMap.of(""forceRepair"", ""true""));
+    }
+
+    @After
+    public void closeCluster()
+    {
+        if (cluster != null)
+            cluster.close();
+    }
+
+    @Test
+    public void testForcedNormalRepairWithOneNodeDown() throws Exception
+    {
+        cluster = create(config -> {}, 4, 3);","[{'comment': ""@yifan-c Can we simplify this test to use 3 nodes and an RF of 2? (i.e. We'd shut down node 2, but nodes 3 and 1 should constitute a full replica set.) If so, it seems like we could just add `testForcedNormalRepairWithOneNodeDown()` to `RepairTest` and avoid some of the duplication that exists between it and `ForceRepairTest`.\r\n\r\nIf we can't make that simplification, there is probably still room to create something like a `RepairTestBase` that can house some of the common elements."", 'commenter': 'maedhroz'}, {'comment': '3/2 is good. Thanks for that. \r\n\r\nFinally remember the reason why having a separate test class. \r\n`RepairTest` share the same `cluster` across all tests. However, the `ForceRepairTest` need to shutdown a node during test. For clarity, I created a dedicated class.', 'commenter': 'yifan-c'}, {'comment': ""@yifan-c That's fair. We can either just make sure that node restarts before other tests start, or go with the `RepairTestBase`/`AbstractRepairTest` idea?"", 'commenter': 'maedhroz'}, {'comment': 'Pushed a new commit to address it.', 'commenter': 'yifan-c'}]"
742,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -415,10 +414,10 @@ private void normalRepair(UUID parentSession,
             @SuppressWarnings(""unchecked"")
             public ListenableFuture apply(List<RepairSessionResult> results)
             {
+                logger.debug(""Repair result: {}"", results);
                 // filter out null(=failed) results and get successful ranges
                 for (RepairSessionResult sessionResult : results)
                 {
-                    logger.debug(""Repair result: {}"", results);","[{'comment': ""Question: Having this here meant we simply didn't log anything if the results were empty? (I wonder if the original intent was to log the individual session results...)"", 'commenter': 'maedhroz'}, {'comment': '>  if the original intent was to log the individual session results...\r\n\r\nMight be. I had the individual result logging within the for loop earlier and moved it outside for 2 reasons. \r\n- log all results in a single message. \r\n- log the message even if the list is empty so we know that execution step. ', 'commenter': 'yifan-c'}]"
742,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -461,7 +462,14 @@ public ListenableFuture apply(List<RepairSessionResult> results)
                 // this node is implicitly a participant in this repair, so a single endpoint is ok here
                 if (!endpoints.isEmpty())
                 {
-                    filtered.add(new CommonRange(endpoints, transEndpoints, commonRange.ranges));
+                    Set<InetAddressAndPort> skippedReplicas = Sets.union(Sets.difference(commonRange.endpoints, endpoints),
+                                                                         Sets.difference(commonRange.transEndpoints, transEndpoints));
+                    skippedReplicas.forEach(endpoint -> logger.info(""Removing a dead node {} from Repair for ranges {} due to -force"", endpoint, commonRange.ranges));
+                    filtered.add(new CommonRange(endpoints, transEndpoints, commonRange.ranges, !skippedReplicas.isEmpty()));
+                }
+                else
+                {
+                    logger.warn(""Unable to force repair for {}, as no neighbor nodes are live"", commonRange.ranges);","[{'comment': 'When an operator reads this warning in the logs, it seems like they might have trouble figuring out for certain what keyspace/tables the repair affects.', 'commenter': 'maedhroz'}]"
742,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -461,7 +462,14 @@ public ListenableFuture apply(List<RepairSessionResult> results)
                 // this node is implicitly a participant in this repair, so a single endpoint is ok here
                 if (!endpoints.isEmpty())
                 {
-                    filtered.add(new CommonRange(endpoints, transEndpoints, commonRange.ranges));
+                    Set<InetAddressAndPort> skippedReplicas = Sets.union(Sets.difference(commonRange.endpoints, endpoints),
+                                                                         Sets.difference(commonRange.transEndpoints, transEndpoints));
+                    skippedReplicas.forEach(endpoint -> logger.info(""Removing a dead node {} from Repair for ranges {} due to -force"", endpoint, commonRange.ranges));","[{'comment': 'nit: Not entirely sure if we want DEBUG or INFO for these.', 'commenter': 'maedhroz'}, {'comment': 'nit/OCD: probably don\'t need to capitalize ""Repair""', 'commenter': 'maedhroz'}, {'comment': ""I'd prefer to keep it as INFO. The message helps to reason about if some endpoints do not participate in a repair in a running server. `DEBUG` is typically disabled. "", 'commenter': 'yifan-c'}]"
742,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -850,5 +852,11 @@ private NeighborsAndRanges(boolean force, Set<InetAddressAndPort> allNeighbors,
             this.allNeighbors = allNeighbors;
             this.commonRanges = commonRanges;
         }
+
+        List<CommonRange> filterCommonRanges()
+        {
+            // filter and only keep the neighbor endpoints that are contained in `allNeighbors` in each commonRange when in force mode.
+            return RepairRunnable.filterCommonRanges(commonRanges, allNeighbors, force);","[{'comment': 'What if we just made `filterCommonRanges()` an instance method on `NeighborsAndRanges` that operated directly on its private state and renamed `RepairRunnableTest` to `NeighborsAndRangesTest`? (It would clean up the `@VisibleForTesting` business at least.)', 'commenter': 'maedhroz'}, {'comment': 'The intention of `NeighborsAndRanges` is to be used as a simple and class-internal data structure. Moving the method here will open up the access (no more internal to the class) and make it a bit more complex.\r\nMeanwhile, I do see the benefit. I will push a commit to see if we like it. ', 'commenter': 'yifan-c'}]"
742,src/java/org/apache/cassandra/repair/RepairSession.java,"@@ -148,37 +146,11 @@ public RepairSession(UUID parentRepairSession,
         this.parallelismDegree = parallelismDegree;
         this.keyspace = keyspace;
         this.cfnames = cfnames;
-
-        //If force then filter out dead endpoints
-        boolean forceSkippedReplicas = false;
-        if (force)
-        {
-            logger.debug(""force flag set, removing dead endpoints"");
-            final Set<InetAddressAndPort> removeCandidates = new HashSet<>();
-            for (final InetAddressAndPort endpoint : commonRange.endpoints)
-            {
-                if (!FailureDetector.instance.isAlive(endpoint))
-                {
-                    logger.info(""Removing a dead node from Repair due to -force {}"", endpoint);
-                    removeCandidates.add(endpoint);
-                }
-            }
-            if (!removeCandidates.isEmpty())
-            {
-                // we shouldn't be recording a successful repair if
-                // any replicas are excluded from the repair
-                forceSkippedReplicas = true;
-                Set<InetAddressAndPort> filteredEndpoints = new HashSet<>(commonRange.endpoints);
-                filteredEndpoints.removeAll(removeCandidates);
-                commonRange = new CommonRange(filteredEndpoints, commonRange.transEndpoints, commonRange.ranges);
-            }
-        }
-
         this.commonRange = commonRange;
         this.isIncremental = isIncremental;
         this.previewKind = previewKind;
         this.pullRepair = pullRepair;
-        this.skippedReplicas = forceSkippedReplicas;
+        this.skippedReplicas = commonRange.hasSkippedReplicas;","[{'comment': 'You could even get rid of `skippedReplicas` and replaces its usages with `commonRange.hasSkippedReplicas`. It would read then, in the code like ""the common range has skipped replicas"", which might be nice. Feel free to ignore this though :)', 'commenter': 'maedhroz'}, {'comment': 'Thanks for taking an extra step. Yep. It sounds good to remove `skippedReplicas`', 'commenter': 'yifan-c'}]"
742,src/java/org/apache/cassandra/repair/RepairSession.java,"@@ -280,7 +276,7 @@ public void start(ListeningExecutorService executor)
         // Checking all nodes are live
         for (InetAddressAndPort endpoint : commonRange.endpoints)
         {
-            if (!FailureDetector.instance.isAlive(endpoint) && !skippedReplicas)
+            if (!FailureDetector.instance.isAlive(endpoint) && !commonRange.hasSkippedReplicas)","[{'comment': 'This is nice. It makes it read, ""The endpoint is alive and the common range has skipped replicas"" without needing any additional context.', 'commenter': 'maedhroz'}]"
742,src/java/org/apache/cassandra/repair/RepairRunnable.java,"@@ -840,23 +794,58 @@ public void runMayThrow() throws Exception
         }
     }
 
-    private static final class NeighborsAndRanges
+    static final class NeighborsAndRanges
     {
         private final boolean force;
         private final Set<InetAddressAndPort> allNeighbors;","[{'comment': ""nit: Would it make things clearer to name `allNeighbors` `liveEndpoints`? (Is there ever a time when it's not the set of live endpoints?)"", 'commenter': 'maedhroz'}, {'comment': 'At least in the current code base, the `allNeighbors` are the live ones. \r\nGiven that, maybe rename the class `NeighborsAndRanges` `LiveNeighborsAndRanges`? Only having `liveEndpoints` implies there could be a set of dead endpoints in the neighbors. ', 'commenter': 'yifan-c'}, {'comment': ""I'm fine wither either one."", 'commenter': 'maedhroz'}]"
742,test/distributed/org/apache/cassandra/distributed/test/RepairTest.java,"@@ -182,4 +197,28 @@ public void testParallelRepairWithoutCompression() throws Exception
     {
         repair(cluster, false, ""{'enabled': false}"");
     }
+
+    @Test
+    public void testForcedNormalRepairWithOneNodeDown() throws Exception
+    {
+        closeCluster();
+        int rf = 2;
+        try (ICluster testCluster = create(config -> {}, 3, rf))
+        {
+            forceRepair(testCluster, 3); // shutdown node 3 after inserting
+            DistributedRepairUtils.assertParentRepairSuccess(testCluster, 1, KEYSPACE, ""test"", row -> {
+                Set<String> successfulRanges = row.getSet(""successful_ranges"");
+                Set<String> requestedRanges = row.getSet(""requested_ranges"");
+                Assert.assertNotNull(""Found no successful ranges"", successfulRanges);
+                Assert.assertNotNull(""Found no requested ranges"", requestedRanges);
+                Assert.assertEquals(""Requested ranges count should equals to replication factor"", rf, requestedRanges.size());
+                Assert.assertTrue(""Given clusterSize = 3, RF = 2 and 1 node down in the replica set, it should yield only 1 successful repaired range."",
+                                  successfulRanges.size() == 1 && !successfulRanges.contains("""")); // the successful ranges set should not only contain empty string
+            });
+        }
+        finally
+        {
+            setupCluster();","[{'comment': 'Would it be more efficient to just `startup()` node 3 instead?', 'commenter': 'maedhroz'}, {'comment': 'The cluster created for this test has a different RF. I do not want to change the existing tests, so that having a stop/start cluster wraps the actual test.  ', 'commenter': 'yifan-c'}]"
742,test/distributed/org/apache/cassandra/distributed/test/RepairTest.java,"@@ -201,12 +195,15 @@ public void testParallelRepairWithoutCompression() throws Exception
     @Test
     public void testForcedNormalRepairWithOneNodeDown() throws Exception
     {
-        closeCluster();
+        // The test uses its own keyspace with rf == 2
+        String forceRepairKeyspace = ""test_force_repair_keyspace"";","[{'comment': 'Yup, this should work nicely.', 'commenter': 'maedhroz'}]"
745,doc/source/cql/types.rst,"@@ -400,20 +398,46 @@ Further, lists support:
 
 Lastly, as for :ref:`maps <maps>`, TTLs when used only apply to the newly inserted values.
 
+.. _tuples:
+
+Tuples
+^^^^^^
+
+CQL also support tuples and tuple types (where the elements can be of different types). Tuple types and tuple literals
+are defined by:
+
+.. productionlist::
+   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
+   tuple_literal: '(' `term` ( ',' `term` )* ')'
+
+and can be used thusly::
+
+    CREATE TABLE durations (
+        event text,
+        duration tuple<int, text>,
+    )
+
+    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));
+
+Unlike other ""composed"" types (collections and UDT), a tuple is always :ref:`frozen <frozen>` (without the need of the
+`frozen` keyword) and it is not possible to update only some elements of a tuple (without updating the whole tuple).","[{'comment': '```suggestion\r\n`frozen` keyword). It is not possible to update only some elements of a tuple (without updating the whole tuple).\r\n```', 'commenter': 'belliottsmith'}]"
745,doc/source/cql/types.rst,"@@ -400,20 +398,46 @@ Further, lists support:
 
 Lastly, as for :ref:`maps <maps>`, TTLs when used only apply to the newly inserted values.
 
+.. _tuples:
+
+Tuples
+^^^^^^
+
+CQL also support tuples and tuple types (where the elements can be of different types). Tuple types and tuple literals
+are defined by:
+
+.. productionlist::
+   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
+   tuple_literal: '(' `term` ( ',' `term` )* ')'
+
+and can be used thusly::
+
+    CREATE TABLE durations (
+        event text,
+        duration tuple<int, text>,
+    )
+
+    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));
+
+Unlike other ""composed"" types (collections and UDT), a tuple is always :ref:`frozen <frozen>` (without the need of the
+`frozen` keyword) and it is not possible to update only some elements of a tuple (without updating the whole tuple).
+Also, a tuple literal should always have the same number of value than declared in the type it is a tuple of (some of","[{'comment': '```suggestion\r\nA tuple literal must have the same number of items as its declaring type (some of\r\n```', 'commenter': 'belliottsmith'}]"
745,doc/source/cql/types.rst,"@@ -400,20 +398,46 @@ Further, lists support:
 
 Lastly, as for :ref:`maps <maps>`, TTLs when used only apply to the newly inserted values.
 
+.. _tuples:
+
+Tuples
+^^^^^^
+
+CQL also support tuples and tuple types (where the elements can be of different types). Tuple types and tuple literals
+are defined by:
+
+.. productionlist::
+   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
+   tuple_literal: '(' `term` ( ',' `term` )* ')'
+
+and can be used thusly::
+
+    CREATE TABLE durations (
+        event text,
+        duration tuple<int, text>,
+    )
+
+    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));
+
+Unlike other ""composed"" types (collections and UDT), a tuple is always :ref:`frozen <frozen>` (without the need of the
+`frozen` keyword) and it is not possible to update only some elements of a tuple (without updating the whole tuple).
+Also, a tuple literal should always have the same number of value than declared in the type it is a tuple of (some of
+those values can be null but they need to be explicitly declared as so).","[{'comment': '```suggestion\r\nthose values can be null but they must be explicitly declared).\r\n```', 'commenter': 'belliottsmith'}]"
745,doc/source/cql/types.rst,"@@ -400,20 +398,46 @@ Further, lists support:
 
 Lastly, as for :ref:`maps <maps>`, TTLs when used only apply to the newly inserted values.
 
+.. _tuples:
+
+Tuples
+^^^^^^
+
+CQL also support tuples and tuple types (where the elements can be of different types). Tuple types and tuple literals
+are defined by:
+
+.. productionlist::
+   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
+   tuple_literal: '(' `term` ( ',' `term` )* ')'
+
+and can be used thusly::
+
+    CREATE TABLE durations (
+        event text,
+        duration tuple<int, text>,
+    )
+
+    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));
+
+Unlike other ""composed"" types (collections and UDT), a tuple is always :ref:`frozen <frozen>` (without the need of the
+`frozen` keyword) and it is not possible to update only some elements of a tuple (without updating the whole tuple).
+Also, a tuple literal should always have the same number of value than declared in the type it is a tuple of (some of
+those values can be null but they need to be explicitly declared as so).
+
 .. _udts:
 
 User-Defined Types
 ^^^^^^^^^^^^^^^^^^
 
-CQL support the definition of user-defined types (UDT for short). Such a type can be created, modified and removed using
-the :token:`create_type_statement`, :token:`alter_type_statement` and :token:`drop_type_statement` described below. But
+CQL support the definition of user-defined types (UDT for short), which is basically a tuple on steroids - it's
+literally an extension of the class that represents a tuple. Such a type can be created, modified and removed using the","[{'comment': 'It\'s better to keep implementation details out of these docs, I think - I\'m not sure it\'s necessary to specify that a UDT is a form of tuple.  Much as I like it, I think docs should also perhaps use less interesting language? (so ""on steroids"" might better be avoided, but just my 2c)', 'commenter': 'belliottsmith'}, {'comment': 'Hey, @belliottsmith! All other comments have been addressed. Thanks a lot!\r\nAbout this one... How do you feel about this?\r\n```\r\nCQL support the definition of user-defined types (UDT for short), which is basically an improved tuple.\r\n```', 'commenter': 'fabioueno'}, {'comment': 'The fact that a UDT inherit from a Tuple is an implementation detail that can be changed in the future. I would not mention a link between UDT and Tuple.\r\nUDTs were introduce to help users to demormalized their data. To help them to avoid using an extra table (which is a dangerous thing in a distributed database).   ', 'commenter': 'blerer'}, {'comment': ""Hey, @blerer! Thanks for the review!\r\nDo you have any suggestions on how should I write this? I thought the aforementioned suggestion was fine, but I'm not sure how to improve it. :confused:"", 'commenter': 'fabioueno'}, {'comment': 'The important part for me was to not mention the link with the `tuple` because UDTs and tuple are different in a lot of aspects.\r\nWhat I had in mind was something like:\r\n`A User Defined Type (UDT) is a set data fields where each field is named and typed. UDTs allow to store related information together within one colum. UDTs can be created, modified and removed using ...`\r\n', 'commenter': 'blerer'}, {'comment': 'Done, just as you suggested, @blerer! :upside_down_face:', 'commenter': 'fabioueno'}]"
745,doc/source/cql/types.rst,"@@ -515,31 +539,48 @@ still in use by another type, table or function will result in an error.
 If the type dropped does not exist, an error will be returned unless ``IF EXISTS`` is used, in which case the operation
 is a no-op.
 
-.. _tuples:
+.. _frozen:
 
-Tuples
-^^^^^^
+Frozen Types
+^^^^^^^^^^^^
 
-CQL also support tuples and tuple types (where the elements can be of different types). Functionally, tuples can be
-though as anonymous UDT with anonymous fields. Tuple types and tuple literals are defined by:
+The ``frozen`` keyword is used to change the way a collection or user-defined type column is serialized. When it is
+present multiple values will be serialized as one, disabling updates on parts of UDTs or individual items of
+collections.
 
-.. productionlist::
-   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
-   tuple_literal: '(' `term` ( ',' `term` )* ')'
+To freeze a column, use the keyword, followed by the type in angle brackets, for instance::
 
-and can be used thusly::
+    CREATE TABLE posts (
+        id int PRIMARY KEY,
+        title text,
+        content text,
+        tags frozen<set<text>>
+    );
 
-    CREATE TABLE durations (
-        event text,
-        duration tuple<int, text>,
-    )
+To insert a frozen value, it's just like a non-frozen column::
 
-    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));
+    INSERT INTO posts (id, title, content, tags)
+            VALUES (1, 'Even Higher Availability with 5x Faster Streaming in Cassandra 4.0',
+                    'Streaming is a process...', {'cassandra', 'availability'});
 
-Unlike other ""composed"" types (collections and UDT), a tuple is always :ref:`frozen <frozen>` (without the need of the
-`frozen` keyword) and it is not possible to update only some elements of a tuple (without updating the whole tuple).
-Also, a tuple literal should always have the same number of value than declared in the type it is a tuple of (some of
-those values can be null but they need to be explicitly declared as so).
+Updating a frozen column
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+As mentioned before, it's not possible to update an individual item of a collection::","[{'comment': 'A user may scroll docs rapidly, so ""As mentioned before"" and similar kinds of comments probably aren\'t helpful.', 'commenter': 'belliottsmith'}]"
745,doc/source/cql/types.rst,"@@ -400,20 +398,46 @@ Further, lists support:
 
 Lastly, as for :ref:`maps <maps>`, TTLs when used only apply to the newly inserted values.
 
+.. _tuples:
+
+Tuples
+^^^^^^
+
+CQL also support tuples and tuple types (where the elements can be of different types). Tuple types and tuple literals","[{'comment': 'I would replace: `CQL also support tuples and tuple types (where the elements can be of different types).` by something like: `A Tuple is a fix-length set of values (fields) where each value can be of a different data type.` ', 'commenter': 'blerer'}]"
745,doc/source/cql/types.rst,"@@ -400,20 +398,46 @@ Further, lists support:
 
 Lastly, as for :ref:`maps <maps>`, TTLs when used only apply to the newly inserted values.
 
+.. _tuples:
+
+Tuples
+^^^^^^
+
+CQL also support tuples and tuple types (where the elements can be of different types). Tuple types and tuple literals
+are defined by:
+
+.. productionlist::
+   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
+   tuple_literal: '(' `term` ( ',' `term` )* ')'
+
+and can be used thusly::
+
+    CREATE TABLE durations (
+        event text,
+        duration tuple<int, text>,
+    )
+
+    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));","[{'comment': '`duration` is now an official Apache Cassandra data type so that example might be a bit confusing.', 'commenter': 'blerer'}]"
745,doc/source/cql/types.rst,"@@ -400,20 +398,46 @@ Further, lists support:
 
 Lastly, as for :ref:`maps <maps>`, TTLs when used only apply to the newly inserted values.
 
+.. _tuples:
+
+Tuples
+^^^^^^
+
+CQL also support tuples and tuple types (where the elements can be of different types). Tuple types and tuple literals
+are defined by:
+
+.. productionlist::
+   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
+   tuple_literal: '(' `term` ( ',' `term` )* ')'
+
+and can be used thusly::
+
+    CREATE TABLE durations (
+        event text,
+        duration tuple<int, text>,
+    )
+
+    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));
+
+Unlike other ""composed"" types (collections and UDT), a tuple is always :ref:`frozen <frozen>` (without the need of the
+`frozen` keyword). It is not possible to update only some elements of a tuple (without updating the whole tuple).","[{'comment': 'I would remove `(without the need of the frozen keyword)` as I believed it changed over time and might not be true on all versions. ', 'commenter': 'blerer'}]"
745,doc/source/cql/types.rst,"@@ -400,20 +398,46 @@ Further, lists support:
 
 Lastly, as for :ref:`maps <maps>`, TTLs when used only apply to the newly inserted values.
 
+.. _tuples:
+
+Tuples
+^^^^^^
+
+CQL also support tuples and tuple types (where the elements can be of different types). Tuple types and tuple literals
+are defined by:
+
+.. productionlist::
+   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
+   tuple_literal: '(' `term` ( ',' `term` )* ')'
+
+and can be used thusly::
+
+    CREATE TABLE durations (
+        event text,
+        duration tuple<int, text>,
+    )
+
+    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));
+
+Unlike other ""composed"" types (collections and UDT), a tuple is always :ref:`frozen <frozen>` (without the need of the
+`frozen` keyword). It is not possible to update only some elements of a tuple (without updating the whole tuple).
+A tuple literal must have the same number of items as its declaring type (some of
+those values can be null but they must be explicitly declared).
+
 .. _udts:
 
 User-Defined Types
 ^^^^^^^^^^^^^^^^^^
 
-CQL support the definition of user-defined types (UDT for short). Such a type can be created, modified and removed using
-the :token:`create_type_statement`, :token:`alter_type_statement` and :token:`drop_type_statement` described below. But
+CQL support the definition of user-defined types (UDT for short), which is basically a tuple on steroids - it's
+literally an extension of the class that represents a tuple. Such a type can be created, modified and removed using the
+:token:`create_type_statement`, :token:`alter_type_statement` and :token:`drop_type_statement` described below. But","[{'comment': 'It is probably important to mention that a UDT is defined at the keyspace level and is not shared between keyspaces. ', 'commenter': 'blerer'}, {'comment': ""On line 474, there's a note that states the following:\r\n```\r\n- A type is intrinsically bound to the keyspace in which it is created, and can only be used in that keyspace. At\r\n  creation, if the type name is prefixed by a keyspace name, it is created in that keyspace. Otherwise, it is created in\r\n  the current keyspace.\r\n```\r\n\r\nIt was already there, so I didn't change a word. Do you think it's okay? :slightly_smiling_face:"", 'commenter': 'fabioueno'}, {'comment': 'You are right, no need to add anything. Sorry for missing that.', 'commenter': 'blerer'}]"
745,doc/source/cql/types.rst,"@@ -515,31 +539,48 @@ still in use by another type, table or function will result in an error.
 If the type dropped does not exist, an error will be returned unless ``IF EXISTS`` is used, in which case the operation
 is a no-op.
 
-.. _tuples:
+.. _frozen:
 
-Tuples
-^^^^^^
+Frozen Types
+^^^^^^^^^^^^
 
-CQL also support tuples and tuple types (where the elements can be of different types). Functionally, tuples can be
-though as anonymous UDT with anonymous fields. Tuple types and tuple literals are defined by:
+The ``frozen`` keyword is used to change the way a collection or user-defined type column is serialized. When it is
+present multiple values will be serialized as one, disabling updates on parts of UDTs or individual items of
+collections.","[{'comment': 'It might be good to start with some explanation about the non-frozen types to help the user understand.\r\nSomething like:\r\n\r\n```\r\nThe ``frozen`` keyword is used to change the way a collection or user-defined type column is serialized.\r\n\r\n`For non-frozen collections or UDTs, each value is serialized independently from the other values. This allow update or delete operations on a sub-set of the collections or UDTs values. For frozen collections or UDTs all the value are serialized as one, disabling the ability to perform partial updates on the values.`\r\n```', 'commenter': 'blerer'}]"
745,doc/source/cql/types.rst,"@@ -515,31 +539,48 @@ still in use by another type, table or function will result in an error.
 If the type dropped does not exist, an error will be returned unless ``IF EXISTS`` is used, in which case the operation
 is a no-op.
 
-.. _tuples:
+.. _frozen:
 
-Tuples
-^^^^^^
+Frozen Types
+^^^^^^^^^^^^
 
-CQL also support tuples and tuple types (where the elements can be of different types). Functionally, tuples can be
-though as anonymous UDT with anonymous fields. Tuple types and tuple literals are defined by:
+The ``frozen`` keyword is used to change the way a collection or user-defined type column is serialized. When it is
+present multiple values will be serialized as one, disabling updates on parts of UDTs or individual items of
+collections.
 
-.. productionlist::
-   tuple_type: TUPLE '<' `cql_type` ( ',' `cql_type` )* '>'
-   tuple_literal: '(' `term` ( ',' `term` )* ')'
+To freeze a column, use the keyword, followed by the type in angle brackets, for instance::
 
-and can be used thusly::
+    CREATE TABLE posts (
+        id int PRIMARY KEY,
+        title text,
+        content text,
+        tags frozen<set<text>>
+    );
 
-    CREATE TABLE durations (
-        event text,
-        duration tuple<int, text>,
-    )
+To insert a frozen value, it's just like a non-frozen column::
 
-    INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));
+    INSERT INTO posts (id, title, content, tags)
+            VALUES (1, 'Even Higher Availability with 5x Faster Streaming in Cassandra 4.0',
+                    'Streaming is a process...', {'cassandra', 'availability'});
 
-Unlike other ""composed"" types (collections and UDT), a tuple is always :ref:`frozen <frozen>` (without the need of the
-`frozen` keyword) and it is not possible to update only some elements of a tuple (without updating the whole tuple).
-Also, a tuple literal should always have the same number of value than declared in the type it is a tuple of (some of
-those values can be null but they need to be explicitly declared as so).
+Updating a frozen column
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+It's not possible to update an individual item of a collection::
+
+    UPDATE posts SET tags = tags - {'availability'} WHERE id = 1;
+
+The above command would result in the following error::
+
+    InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid operation (tags = tags -
+    {'availability'}) for frozen collection column tags""
+
+When there's a need to update, the full value must be provided::
+
+    UPDATE posts SET tags = {'cassandra'} WHERE id = 1;
+
+Note we're replacing the whole value, not just removing the unwanted item. The same is true for appending elements in
+a collection.","[{'comment': 'The `we` sound weird in a technical doc but it might be only me.', 'commenter': 'blerer'}]"
749,src/java/org/apache/cassandra/tools/NodeProbe.java,"@@ -270,6 +272,21 @@ public void close() throws IOException
         }
     }
 
+    public void setConsoleOutputProvider(ConsoleOutputProvider consoleOutputProvider)","[{'comment': 'this makes unit testing nodetool much easier!', 'commenter': 'dcapwell'}]"
749,src/java/org/apache/cassandra/tools/ConsoleOutputProvider.java,"@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.Closeable;
+import java.io.PrintStream;
+
+public interface ConsoleOutputProvider extends Closeable","[{'comment': ""why `Closeable `?  I don't see anything closing it so this will cause compiler warnings."", 'commenter': 'dcapwell'}, {'comment': ""saw it in Instance, the usage doesn't need to be closed so you can avoid that; removing Closable will fix some of the compiler warnings as well."", 'commenter': 'dcapwell'}, {'comment': 'Can we remove closable from the interface as the only thing that closes is Instance, and it uses its own type?', 'commenter': 'dcapwell'}, {'comment': 'Done. Move it to `CapturingConsoleOutputProvider` only', 'commenter': 'yifan-c'}]"
749,src/java/org/apache/cassandra/tools/NodeTool.java,"@@ -405,10 +415,12 @@ private NodeProbe connect()
                     nodeClient = nodeProbeFactory.create(host, parseInt(port));
                 else
                     nodeClient = nodeProbeFactory.create(host, parseInt(port), username, password);
+
+                nodeClient.setConsoleOutputProvider(consoleOutputProvider);","[{'comment': 'rather than mutable ref, why not update the create method to take it?', 'commenter': 'dcapwell'}, {'comment': 'OK with either. I will update the create method then.  \r\n(mutate the ref, so the change is slightly less)', 'commenter': 'yifan-c'}, {'comment': ""I take it back. If also changing the `NodeProbe` constructors, it will touch other 8+ files. So I'd prefer to have a separate `setConsoleOutputProvider()` method that alters at will."", 'commenter': 'yifan-c'}]"
749,src/java/org/apache/cassandra/tools/nodetool/Sjk.java,"@@ -143,7 +145,7 @@ else if (isListCommands())
                 {
                     for (String cmd : commands.keySet())
                     {
-                        System.out.println(String.format(""%8s - %s"", cmd, parser.getCommandDescription(cmd)));","[{'comment': 'missing `parser.usage`, you can override it but sadly its annoying.\r\n\r\n`parser.usage();` replace with\r\n\r\n```\r\nStringBuilder sb = new StringBuilder();\r\nparser.usage(sb);\r\noutStream.println(sb);\r\n```\r\n\r\nand `parser.usage(cmd);` replace with \r\n\r\n```\r\nStringBuilder sb = new StringBuilder();\r\nparser.usage(cmd, sb);\r\noutStream.println(sb);\r\n```', 'commenter': 'dcapwell'}]"
749,src/java/org/apache/cassandra/tools/nodetool/Sjk.java,"@@ -143,7 +145,7 @@ else if (isListCommands())
                 {
                     for (String cmd : commands.keySet())
                     {
-                        System.out.println(String.format(""%8s - %s"", cmd, parser.getCommandDescription(cmd)));
+                        outStream.println(String.format(""%8s - %s"", cmd, parser.getCommandDescription(cmd)));","[{'comment': 'in the catch block for `CommandAbortedError` there are other logging commands, can replace them as well.', 'commenter': 'dcapwell'}]"
749,test/distributed/org/apache/cassandra/distributed/impl/Instance.java,"@@ -599,20 +602,61 @@ public int liveMemberCount()
     public NodeToolResult nodetoolResult(boolean withNotifications, String... commandAndArgs)
     {
         return sync(() -> {
-            DTestNodeTool nodetool = new DTestNodeTool(withNotifications);
-            int rc =  nodetool.execute(commandAndArgs);
-            return new NodeToolResult(commandAndArgs, rc, new ArrayList<>(nodetool.notifications.notifications), nodetool.latestError);
+            try (CapturingConsoleOutputProvider provider = new CapturingConsoleOutputProvider())
+            {
+                DTestNodeTool nodetool = new DTestNodeTool(withNotifications, provider);
+                int rc = nodetool.execute(commandAndArgs);
+                return new NodeToolResult(commandAndArgs, rc,
+                                          new ArrayList<>(nodetool.notifications.notifications),
+                                          nodetool.latestError,
+                                          provider.getOutString(),
+                                          provider.getErrString());
+            }
         }).call();
     }
 
+    private static class CapturingConsoleOutputProvider implements ConsoleOutputProvider
+    {
+        private final ByteArrayOutputStream outBase = new ByteArrayOutputStream();
+        private final ByteArrayOutputStream errBase = new ByteArrayOutputStream();
+        private final PrintStream out = new PrintStream(outBase, true);
+        private final PrintStream err = new PrintStream(errBase, true);
+
+        public PrintStream outStream()
+        {
+            return out;
+        }
+
+        public PrintStream errStream()
+        {
+            return err;
+        }
+
+        public String getOutString()
+        {
+            return outBase.toString();
+        }
+
+        public String getErrString()
+        {
+            return errBase.toString();
+        }
+
+        public void close()","[{'comment': ""don't need to close `ByteArrayOutputStream`, here is the implementation of close http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/io/ByteArrayOutputStream.java#l267"", 'commenter': 'dcapwell'}]"
749,test/distributed/org/apache/cassandra/distributed/impl/Instance.java,"@@ -599,20 +602,61 @@ public int liveMemberCount()
     public NodeToolResult nodetoolResult(boolean withNotifications, String... commandAndArgs)
     {
         return sync(() -> {
-            DTestNodeTool nodetool = new DTestNodeTool(withNotifications);
-            int rc =  nodetool.execute(commandAndArgs);
-            return new NodeToolResult(commandAndArgs, rc, new ArrayList<>(nodetool.notifications.notifications), nodetool.latestError);
+            try (CapturingConsoleOutputProvider provider = new CapturingConsoleOutputProvider())
+            {
+                DTestNodeTool nodetool = new DTestNodeTool(withNotifications, provider);
+                int rc = nodetool.execute(commandAndArgs);
+                return new NodeToolResult(commandAndArgs, rc,
+                                          new ArrayList<>(nodetool.notifications.notifications),
+                                          nodetool.latestError,
+                                          provider.getOutString(),
+                                          provider.getErrString());
+            }
         }).call();
     }
 
+    private static class CapturingConsoleOutputProvider implements ConsoleOutputProvider
+    {
+        private final ByteArrayOutputStream outBase = new ByteArrayOutputStream();","[{'comment': 'this will cause a compiler warning, will need something like `@SuppressWarnings(""resources)` to avoid', 'commenter': 'dcapwell'}]"
749,src/java/org/apache/cassandra/tools/ConsoleOutputProvider.java,"@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.PrintStream;
+
+public interface ConsoleOutputProvider","[{'comment': 'We can even call it just `OutputProvier`, given the default implementation is ""console"".', 'commenter': 'ifesdjeen'}]"
749,src/java/org/apache/cassandra/tools/NodeTool.java,"@@ -284,26 +286,31 @@ private static void printHistory(String... args)
 
     protected void badUse(Exception e)
     {
-        System.out.println(""nodetool: "" + e.getMessage());
-        System.out.println(""See 'nodetool help' or 'nodetool help <command>'."");
+        consoleOutputProvider.outStream().println(""nodetool: "" + e.getMessage());","[{'comment': 'Same here, could be just `outputProvider` (or even `output` fwiw)', 'commenter': 'ifesdjeen'}, {'comment': 'sure.', 'commenter': 'yifan-c'}]"
749,src/java/org/apache/cassandra/tools/ConsoleOutputProvider.java,"@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.PrintStream;
+
+public interface ConsoleOutputProvider
+{
+    PrintStream outStream();","[{'comment': 'You can even just use `out` and `err`. Wdyt?', 'commenter': 'ifesdjeen'}]"
749,src/java/org/apache/cassandra/tools/NodeProbe.java,"@@ -270,6 +271,21 @@ public void close() throws IOException
         }
     }
 
+    public void setConsoleOutputProvider(OutputProvider outputProvider)","[{'comment': 'Can also be just `setOutputProvider`', 'commenter': 'ifesdjeen'}]"
763,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -27,6 +28,23 @@
 
 public class EncryptionOptions
 {
+    public enum TlsEncryptionPolicy
+    {
+        unencrypted(""unencrypted""), optional(""optionally encrypted""), encrypted(""encrypted"");","[{'comment': 'Usual convention is to use upper case identifiers for enum elements.', 'commenter': 'dineshjoshi'}]"
763,src/java/org/apache/cassandra/net/InboundConnectionSettings.java,"@@ -83,8 +83,8 @@ public boolean authenticate(InetAddress address, int port)
 
     public String toString()
     {
-        return format(""address: (%s), nic: %s, encryption: %s"",
-                      bindAddress, FBUtilities.getNetworkInterface(bindAddress.address), SocketFactory.encryptionLogStatement(null, encryption));
+        return format(""address: (%s), nic: %s, %s"",","[{'comment': 'Could we add back `encryption: ` label?', 'commenter': 'dineshjoshi'}, {'comment': ""Can do - I thought it was redundant with the descriptions 'unencrypted', 'optionally encrypted' or 'encrypted' \r\n\r\nwould go from\r\n```\r\nListening on address: (/127.0.0.1:7012), nic: lo0, unencrypted\r\nListening on address: (/127.0.0.1:7012), nic: lo0, optionally encrypted(jdk)\r\nListening on address: (/127.0.0.2:7012), nic: lo0, encrypted(jdk)\r\n```\r\nto\r\n```\r\nListening on address: (/127.0.0.1:7012), nic: lo0, encryption: unencrypted\r\nListening on address: (/127.0.0.1:7012), nic: lo0, encryption: optionally encrypted(jdk)\r\nListening on address: (/127.0.0.2:7012), nic: lo0, encryption: encrypted(jdk)\r\n```\r\n\r\nStill prefer it back?"", 'commenter': 'jonmeredith'}, {'comment': ""From a parsing standpoint, I prefer the second. It's easier to parse the log entries, for example, in Splunk. It is also consistent with what we print in `OutboundConnectionSettings` https://github.com/apache/cassandra/pull/763/files#diff-77d6e44359d621fcf54fef9e65dd0a10L173"", 'commenter': 'dineshjoshi'}, {'comment': 'Ok, will keep the original `encryption:` tag.', 'commenter': 'jonmeredith'}]"
763,src/java/org/apache/cassandra/net/SocketFactory.java,"@@ -228,39 +228,41 @@ static SslHandler newSslHandler(Channel channel, SslContext sslContext, @Nullabl
         return sslHandler;
     }
 
-    static String encryptionLogStatement(EncryptionOptions options)
+    /**
+     * Summarizes the intended encryption options, suitable for logging. Once a connection is established, use
+     * {@link SocketFactory#encryptionConnectionSummary} below.
+     * @param options options to summarize
+     * @return description of encryption options
+     */
+    static String encryptionOptionsSummary(EncryptionOptions options)","[{'comment': 'Would it be better to simply change this to:\r\n```java\r\nif (options == null)\r\n    return ""disabled"";\r\n\r\nString encryptionType = SSLFactory.openSslIsAvailable() ? ""openssl"" : ""jdk"";\r\nreturn options.tlsEncryptionPolicy().description() + \'(\' + encryptionType + \')\';\r\n```', 'commenter': 'dineshjoshi'}, {'comment': 'Outputing `unencrypted(openssl)` read badly to me, and I wanted to enforce the same description of encrypted/optional/unencrypted from the enum so I was also getting rid of `disabled`.', 'commenter': 'jonmeredith'}]"
763,src/java/org/apache/cassandra/transport/Server.java,"@@ -139,18 +139,19 @@ public synchronized void start()
         if (workerGroup != null)
             bootstrap = bootstrap.group(workerGroup);
 
+        final EncryptionOptions clientEnc = DatabaseDescriptor.getNativeProtocolEncryptionOptions();
+
         if (this.useSSL)
         {
-            final EncryptionOptions clientEnc = DatabaseDescriptor.getNativeProtocolEncryptionOptions();
-
-            if (clientEnc.optional)
+            if (clientEnc.tlsEncryptionPolicy() == EncryptionOptions.TlsEncryptionPolicy.optional)
             {
-                logger.info(""Enabling optionally encrypted CQL connections between client and server"");
+                logger.debug(""Enabling optionally encrypted CQL connections between client and server"");
                 bootstrap.childHandler(new OptionalSecureInitializer(this, clientEnc));
             }
             else
             {
-                logger.info(""Enabling encrypted CQL connections between client and server"");
+                logger.debug(""Enabling encrypted CQL connections between client and server"");
+                assert clientEnc.tlsEncryptionPolicy() == EncryptionOptions.TlsEncryptionPolicy.encrypted : ""Unexpected policy: "" + clientEnc.tlsEncryptionPolicy();","[{'comment': 'Is `clientEnc.tlsEncryptionPolicy()` guaranteed to be non-null?', 'commenter': 'dineshjoshi'}, {'comment': ""Yes, I can tag it with an `@Nonnull`, as that seems used elsewhere, though I don't know what current best practice is there for static analysis tools."", 'commenter': 'jonmeredith'}, {'comment': '`@NotNull` is the way to go.', 'commenter': 'dineshjoshi'}]"
763,src/java/org/apache/cassandra/net/InboundConnectionInitiator.java,"@@ -503,4 +494,31 @@ protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) t
             }
         }
     }
+
+    private static class rejectSslHandler extends ByteToMessageDecoder","[{'comment': 'Class names should begin with upper-case characters.', 'commenter': 'dineshjoshi'}, {'comment': ""ack, thought I was copying existing convention for the Optional handle, but I'm obviously not."", 'commenter': 'jonmeredith'}]"
763,src/java/org/apache/cassandra/net/InboundConnectionInitiator.java,"@@ -98,17 +95,22 @@ public void initChannel(SocketChannel channel) throws Exception
 
             // order of handlers: ssl -> logger -> handshakeHandler
             // For either unencrypted or transitional modes, allow Ssl optionally.
-            if (settings.encryption.optional)
+            switch(settings.encryption.tlsEncryptionPolicy())
             {
-                pipeline.addFirst(""ssl"", new OptionalSslHandler(settings.encryption));
-            }
-            else
-            {
-                SslContext sslContext = SSLFactory.getOrCreateSslContext(settings.encryption, true, SSLFactory.SocketType.SERVER);
-                InetSocketAddress peer = settings.encryption.require_endpoint_verification ? channel.remoteAddress() : null;
-                SslHandler sslHandler = newSslHandler(channel, sslContext, peer);
-                logger.trace(""creating inbound netty SslContext: context={}, engine={}"", sslContext.getClass().getName(), sslHandler.engine().getClass().getName());
-                pipeline.addFirst(""ssl"", sslHandler);
+                case UNENCRYPTED:
+                    // Handler checks for SSL connection attempts and cleanly rejects them if encryption is disabled
+                    pipeline.addFirst(""rejectssl"", new rejectSslHandler());
+                    break;
+                case OPTIONAL:
+                    pipeline.addFirst(""ssl"", new OptionalSslHandler(settings.encryption));
+                    break;
+                case ENCRYPTED:
+                    SslContext sslContext = SSLFactory.getOrCreateSslContext(settings.encryption, true, SSLFactory.SocketType.SERVER);","[{'comment': 'Nit: If its not a big deal, could you create a constant `BUILD_TRUST_STORE=true` and pass it in. Makes the code a bit more readable. If you feel too ambitious, make this into a builder.', 'commenter': 'dineshjoshi'}, {'comment': ""Went with the low effort route as there's already more than I wanted to refactor in the beta."", 'commenter': 'jonmeredith'}]"
763,src/java/org/apache/cassandra/db/virtual/SimpleDataSet.java,"@@ -84,7 +84,7 @@ private DecoratedKey makeDecoratedKey(Object... partitionKeyValues)
     {
         ByteBuffer partitionKey = partitionKeyValues.length == 1
                                 ? decompose(metadata.partitionKeyType, partitionKeyValues[0])
-                                : ((CompositeType) metadata.partitionKeyType).decompose(ByteBufferAccessor.instance, partitionKeyValues);","[{'comment': 'teehee', 'commenter': 'dcapwell'}]"
763,src/java/org/apache/cassandra/net/InboundConnectionInitiator.java,"@@ -460,14 +447,24 @@ void setupMessagingPipeline(InetAddressAndPort from, int useMessagingVersion, in
                         handler.id(true),
                         useMessagingVersion,
                         initiate.framing,
-                        pipeline.get(""ssl"") != null ? encryptionLogStatement(pipeline.channel(), settings.encryption) : ""disabled"");
+                        SocketFactory.encryptionConnectionSummary(pipeline.channel()));
 
             pipeline.addLast(""deserialize"", handler);
 
             pipeline.remove(this);
         }
     }
 
+    private static SslHandler getSslHandler(String description, Channel channel, EncryptionOptions.ServerEncryptionOptions encryptionOptions) throws IOException
+    {
+        final boolean BUILD_TRUST_STORE = true;","[{'comment': '`s/BUILD_TRUST_STORE/buildTrustStore/g`', 'commenter': 'dcapwell'}, {'comment': 'also only looks like its used in one place, so kinda prefer the code before that just added true into the method params', 'commenter': 'dcapwell'}, {'comment': ""it was a review comment from Dinesh that I agreed with, I think it's about conveying more meaning than a simple boolean rather than rely on the IDE to provide hints what they mean."", 'commenter': 'jonmeredith'}]"
763,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -114,19 +119,74 @@ public EncryptionOptions(EncryptionOptions options)
      * @return if the channel should be encrypted
      */
     public boolean isEnabled() {
-        return this.enabled;
+        return this.enabled != null && enabled;
     }
 
     /**
      * Sets if encryption should be enabled for this channel. Note that this should only be called by
      * the configuration parser or tests. It is public only for that purpose, mutating enabled state
      * is probably a bad idea.
-     * @param enabled
+     * @param enabled value to set
      */
     public void setEnabled(boolean enabled) {
         this.enabled = enabled;
     }
 
+    protected void unsetEnabled()
+    {
+        this.enabled = null;
+    }
+
+    /**
+     * Indicates if the channel may be encrypted (but is not requried to be).
+     * Explicitly providing a value in the configuration take precedent.
+     * If no optional value is set and !isEnabled(), then optional connections are allowed
+     * if a keystore exists. Without it, it would be impossible to establish the connections.
+     * @return if the channel may be encrypted
+     */
+    public boolean isOptional()
+    {
+        if (optional != null)
+            return optional;
+
+        // If someone is asking for an _insecure_ connection and not explicitly telling us to refuse
+        // encrypted connections AND they have a keystore file, we assume they would like to be able
+        // to transition to encrypted connections in the future.
+        if (new File(keystore).exists())","[{'comment': 'can we move this into the constructor?  would be good to avoid touching the filesystem every time a new connection is opened;  do we allow updating keystore at runtime, if so how do we detect this?', 'commenter': 'dcapwell'}]"
763,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -302,6 +363,30 @@ public boolean isEnabled() {
             return this.internode_encryption != InternodeEncryption.none;
         }
 
+        @Override
+        public TlsEncryptionPolicy tlsEncryptionPolicy()
+        {
+            if (this.enabled != null)
+            {
+                throw new ConfigurationException(""enabled should not be configured for server_encryption_policy, must use internode_encryption"");","[{'comment': 'do you mean `server_encryption_options` instead of `server_encryption_policy`?', 'commenter': 'dcapwell'}, {'comment': 'good eye, thanks.', 'commenter': 'jonmeredith'}]"
763,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -280,13 +340,14 @@ public int hashCode()
 
         public ServerEncryptionOptions()
         {
+            unsetEnabled(); // set to null DatabaseDescriptor can check if it was explicitly set.","[{'comment': ""is this needed?  the default is null so setting null shouldn't be required."", 'commenter': 'dcapwell'}, {'comment': 'Agreed -- not needed.', 'commenter': 'jonmeredith'}, {'comment': ""So removing in the current form causes test failures as the Yaml config loader I was originally using initializes the enabled field.\r\n\r\n```\r\nCaused by: org.apache.cassandra.exceptions.ConfigurationException: enabled should not be configured for server_encryption_options, must use internode_encryption\r\n\tat org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions.tlsEncryptionPolicy(EncryptionOptions.java:365)\r\n\tat org.apache.cassandra.security.SSLFactory.validateSslCerts(SSLFactory.java:409)\r\n\t... 12 more\r\n```\r\n\r\nI'll try with the improved version.\r\n"", 'commenter': 'jonmeredith'}]"
763,src/java/org/apache/cassandra/net/InboundConnectionInitiator.java,"@@ -460,14 +447,24 @@ void setupMessagingPipeline(InetAddressAndPort from, int useMessagingVersion, in
                         handler.id(true),
                         useMessagingVersion,
                         initiate.framing,
-                        pipeline.get(""ssl"") != null ? encryptionLogStatement(pipeline.channel(), settings.encryption) : ""disabled"");
+                        SocketFactory.encryptionConnectionSummary(pipeline.channel()));
 
             pipeline.addLast(""deserialize"", handler);
 
             pipeline.remove(this);
         }
     }
 
+    private static SslHandler getSslHandler(String description, Channel channel, EncryptionOptions.ServerEncryptionOptions encryptionOptions) throws IOException
+    {
+        final boolean BUILD_TRUST_STORE = true;
+        SslContext sslContext = SSLFactory.getOrCreateSslContext(encryptionOptions, BUILD_TRUST_STORE, SSLFactory.SocketType.SERVER);
+        InetSocketAddress peer = encryptionOptions.require_endpoint_verification ? (InetSocketAddress) channel.remoteAddress() : null;
+        SslHandler sslHandler = newSslHandler(channel, sslContext, peer);
+        logger.trace(description + "" inbound netty SslContext: context={}, engine={}"", sslContext.getClass().getName(), sslHandler.engine().getClass().getName());","[{'comment': 'nit, `logger.trace(""{} inbound netty..."", description, sslContext.getClass().getName(), sslHandler.engine().getClass().getName());`', 'commenter': 'dcapwell'}]"
763,src/java/org/apache/cassandra/net/SocketFactory.java,"@@ -228,39 +228,41 @@ static SslHandler newSslHandler(Channel channel, SslContext sslContext, @Nullabl
         return sslHandler;
     }
 
-    static String encryptionLogStatement(EncryptionOptions options)
+    /**
+     * Summarizes the intended encryption options, suitable for logging. Once a connection is established, use
+     * {@link SocketFactory#encryptionConnectionSummary} below.
+     * @param options options to summarize
+     * @return description of encryption options
+     */
+    static String encryptionOptionsSummary(EncryptionOptions options)
     {
-        if (options == null)
-            return ""disabled"";
+        if (options == null || options.tlsEncryptionPolicy() == EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
+            return EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED.description();
 
         String encryptionType = SSLFactory.openSslIsAvailable() ? ""openssl"" : ""jdk"";
-        return ""enabled ("" + encryptionType + ')';
+        return options.tlsEncryptionPolicy().description() + '(' + encryptionType + ')';
     }
 
-    static String encryptionLogStatement(Channel channel, EncryptionOptions options)
+    /**
+     * Summarizes the encryption status of a channel, suitable for logging.
+     * @return description of channel encryption
+     */
+    static String encryptionConnectionSummary(Channel channel)
     {
-        if (options == null || !options.isEnabled())
-            return ""disabled"";
-
-        StringBuilder sb = new StringBuilder(64);
-        if (options.optional)
-            sb.append(""optional (factory="");
-        else
-            sb.append(""enabled (factory="");
-        sb.append(SSLFactory.openSslIsAvailable() ? ""openssl"" : ""jdk"");
-
-        final SslHandler sslHandler = channel == null ? null : channel.pipeline().get(SslHandler.class);
-        if (sslHandler != null)
+        final SslHandler sslHandler = channel.pipeline().get(SslHandler.class);
+        if (sslHandler == null)
         {
-            SSLSession session = sslHandler.engine().getSession();
-            sb.append("";protocol="")
-              .append(session.getProtocol())
-              .append("";cipher="")
-              .append(session.getCipherSuite());
+            return EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED.description();
         }
-
-        sb.append(')');
-        return sb.toString();
+        SSLSession session = sslHandler.engine().getSession();
+
+        return  ""encrypted(factory="" +
+                (SSLFactory.openSslIsAvailable() ? ""openssl"" : ""jdk"") +
+                "";protocol="" +
+                (session != null ? session.getProtocol() : ""MISSING SESSION"") +","[{'comment': ""why is session nullable now but wasn't before the patch?  "", 'commenter': 'dcapwell'}]"
763,src/java/org/apache/cassandra/net/InboundConnectionSettings.java,"@@ -83,8 +83,8 @@ public boolean authenticate(InetAddress address, int port)
 
     public String toString()
     {
-        return format(""address: (%s), nic: %s, encryption: %s"",
-                      bindAddress, FBUtilities.getNetworkInterface(bindAddress.address), SocketFactory.encryptionLogStatement(null, encryption));
+        return format(""address: (%s), nic: %s, encrypted: %s"",","[{'comment': ""wasn't encryption correct before?  you are doing a summary and not a boolean; the following feels weird\r\n\r\n```\r\nencrypted: unencrypted\r\n```"", 'commenter': 'dcapwell'}]"
763,src/java/org/apache/cassandra/net/InboundSockets.java,"@@ -202,7 +203,8 @@ private static void addBindings(InboundConnectionSettings template, ImmutableLis
         InboundConnectionSettings       settings = template.withDefaults();
         InboundConnectionSettings legacySettings = template.withLegacyDefaults();
 
-        if (settings.encryption.enable_legacy_ssl_storage_port)
+        if (settings.encryption.enable_legacy_ssl_storage_port &&","[{'comment': 'I am wondering if it would be better to push this logic into `org.apache.cassandra.net.InboundConnectionSettings` rather than have here? or DatabaseDescriptor (only allow true if encryptions != none', 'commenter': 'dcapwell'}]"
763,src/java/org/apache/cassandra/transport/Server.java,"@@ -139,29 +137,27 @@ public synchronized void start()
         if (workerGroup != null)
             bootstrap = bootstrap.group(workerGroup);
 
-        if (this.useSSL)
-        {
-            final EncryptionOptions clientEnc = DatabaseDescriptor.getNativeProtocolEncryptionOptions();
+        final EncryptionOptions clientEnc = DatabaseDescriptor.getNativeProtocolEncryptionOptions();
 
-            if (clientEnc.optional)
-            {
-                logger.info(""Enabling optionally encrypted CQL connections between client and server"");
+        switch (this.tlsEncryptionPolicy)
+        {
+            case UNENCRYPTED:
+                bootstrap.childHandler(new Initializer(this));
+                break;
+            case OPTIONAL:
+                logger.debug(""Enabling optionally encrypted CQL connections between client and server"");
                 bootstrap.childHandler(new OptionalSecureInitializer(this, clientEnc));
-            }
-            else
-            {
-                logger.info(""Enabling encrypted CQL connections between client and server"");
+                break;
+            case ENCRYPTED:
+                logger.debug(""Enabling encrypted CQL connections between client and server"");
                 bootstrap.childHandler(new SecureInitializer(this, clientEnc));
-            }
-        }
-        else
-        {
-            bootstrap.childHandler(new Initializer(this));
+                break;
+            default:
+                throw new IllegalStateException(""Unrecognized TLS encryption policy: "" + this.tlsEncryptionPolicy);
         }
 
         // Bind and start to accept incoming connections.
-        logger.info(""Using Netty Version: {}"", Version.identify().entrySet());","[{'comment': 'why move this log to a different location?', 'commenter': 'dcapwell'}, {'comment': 'It prevents it being logged twice if there is a separate SSL port specified. We only need Netty version information once.', 'commenter': 'jonmeredith'}]"
770,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -204,6 +208,79 @@ public void setOptional(boolean optional) {
         this.optional = optional;
     }
 
+    /**
+     * Sets accepted TLS protocol for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea.
+     * @param protocol value to set
+     */
+    public void setProtocol(String protocol) {
+        this.protocol = protocol;
+    }
+
+    /**
+     * Sets accepted TLS protocols for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocl state
+     * is probably a bad idea.
+     * @param accepted_protocols value to set
+     */
+    public void setaccepted_protocols(List<String> accepted_protocols) {","[{'comment': 'Why are we using underscores? We use CamelCase convention. Likely typo? :)', 'commenter': 'dineshjoshi'}, {'comment': 'It\'s for dealing with snakeyaml.  After digging in a bit deeper for CASSANDRA-16144, I think it will be looking for a method named ""get"" + `String.capitalize(""accepted_protocols"")` so it wouldn\'t find this anyway.  Method name may have been in desperation on an earlier attempt. I\'ll see if I can improve it.\r\n', 'commenter': 'jonmeredith'}, {'comment': 'we should try to find if snakeyaml has something like `@JsonProperty(""accepted_protocols"")` like Jackson does...', 'commenter': 'dcapwell'}, {'comment': ""boo, it looks like there isn't a way, but we do own how yaml is loaded...  sadly Ekaterina's patch wasn't merged, as it would make it trivial to have yaml names != java names 😭 "", 'commenter': 'dcapwell'}]"
770,src/java/org/apache/cassandra/security/SSLFactory.java,"@@ -175,7 +173,7 @@ public static SSLContext createSSLContext(EncryptionOptions options, boolean bui
 
         try
         {
-            SSLContext ctx = SSLContext.getInstance(options.protocol);
+            SSLContext ctx = SSLContext.getInstance(""TLS""); //TODO: Make sure all uses restrict protocol to configured.","[{'comment': 'Is this TODO resolved? If so, please remove this comment?', 'commenter': 'dineshjoshi'}]"
770,src/java/org/apache/cassandra/security/SSLFactory.java,"@@ -397,41 +390,121 @@ public static synchronized void initHotReloading(EncryptionOptions.ServerEncrypt
         isHotReloadingInitialized = true;
     }
 
-
-    /**
-     * Sanity checks all certificates to ensure we can actually load them
+    // Non-logging
+    /*
+     * This class will filter all requested ciphers out that are not supported by the current {@link SSLEngine},
+     * logging messages for all dropped ciphers, and throws an exception if no ciphers are supported
      */
-    public static void validateSslCerts(EncryptionOptions.ServerEncryptionOptions serverOpts, EncryptionOptions clientOpts) throws IOException
+    public static final class LoggingCipherSuiteFilter implements CipherSuiteFilter
     {
-        try
+        // Version without logging the ciphers, make sure same filtering logic is used
+        // all the time, regardless of user output.
+        public static final CipherSuiteFilter QUIET_FILTER = new LoggingCipherSuiteFilter();
+        final String settingDescription;
+
+        private LoggingCipherSuiteFilter()
         {
-            // Ensure we're able to create both server & client SslContexts if they might ever be needed
-            if (serverOpts != null && serverOpts.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
-            {
-                createNettySslContext(serverOpts, true, SocketType.SERVER, openSslIsAvailable());
-                createNettySslContext(serverOpts, true, SocketType.CLIENT, openSslIsAvailable());
-            }
+            this.settingDescription = null;
         }
-        catch (Exception e)
+
+        public LoggingCipherSuiteFilter(String settingDescription)
         {
-            throw new IOException(""Failed to create SSL context using server_encryption_options!"", e);
+            this.settingDescription = settingDescription;
         }
 
-        try
+
+        @Override
+        public String[] filterCipherSuites(Iterable<String> ciphers, List<String> defaultCiphers,
+                                           Set<String> supportedCiphers)
         {
-            // Ensure we're able to create both server & client SslContexts if they might ever be needed
-            if (clientOpts != null && clientOpts.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
+            Objects.requireNonNull(defaultCiphers, ""defaultCiphers"");
+            Objects.requireNonNull(supportedCiphers, ""supportedCiphers"");
+
+            final List<String> newCiphers;
+            if (ciphers == null)
+            {
+                newCiphers = new ArrayList<>(defaultCiphers.size());
+                ciphers = defaultCiphers;
+            }
+            else
+            {
+                newCiphers = new ArrayList<>(supportedCiphers.size());
+            }
+            for (String c : ciphers)
             {
-                createNettySslContext(clientOpts, clientOpts.require_client_auth, SocketType.SERVER, openSslIsAvailable());
-                createNettySslContext(clientOpts, clientOpts.require_client_auth, SocketType.CLIENT, openSslIsAvailable());
+                if (c == null)
+                {
+                    break;
+                }
+                if (supportedCiphers.contains(c))
+                {
+                    newCiphers.add(c);
+                }
+                else
+                {
+                    if (settingDescription != null)
+                    {
+                        logger.warn(""Dropping unsupported cipher_suite {} from {} configuration"",
+                                    c, settingDescription.toLowerCase());
+                    }
+                }
             }
+            if (newCiphers.isEmpty())
+            {
+                throw new IllegalStateException(""No ciphers left after filtering supported cipher suite"");
+            }
+
+            return newCiphers.toArray(new String[0]);
         }
-        catch (Exception e)
+    }
+
+    public static void validateSslContext(String contextDescription, EncryptionOptions options, boolean buildTrustStore, boolean logProtocolAndCiphers) throws IOException
+    {
+        if (options != null && options.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
         {
-            throw new IOException(""Failed to create SSL context using client_encryption_options!"", e);
+            try
+            {
+                CipherSuiteFilter loggingCipherSuiteFilter =logProtocolAndCiphers ?  new LoggingCipherSuiteFilter(contextDescription)","[{'comment': 'nit: extra space : `?  new` and missing space between `=logProtocol...`.', 'commenter': 'dineshjoshi'}]"
770,src/java/org/apache/cassandra/security/SSLFactory.java,"@@ -397,41 +390,121 @@ public static synchronized void initHotReloading(EncryptionOptions.ServerEncrypt
         isHotReloadingInitialized = true;
     }
 
-
-    /**
-     * Sanity checks all certificates to ensure we can actually load them
+    // Non-logging
+    /*
+     * This class will filter all requested ciphers out that are not supported by the current {@link SSLEngine},
+     * logging messages for all dropped ciphers, and throws an exception if no ciphers are supported
      */
-    public static void validateSslCerts(EncryptionOptions.ServerEncryptionOptions serverOpts, EncryptionOptions clientOpts) throws IOException
+    public static final class LoggingCipherSuiteFilter implements CipherSuiteFilter
     {
-        try
+        // Version without logging the ciphers, make sure same filtering logic is used
+        // all the time, regardless of user output.
+        public static final CipherSuiteFilter QUIET_FILTER = new LoggingCipherSuiteFilter();
+        final String settingDescription;
+
+        private LoggingCipherSuiteFilter()
         {
-            // Ensure we're able to create both server & client SslContexts if they might ever be needed
-            if (serverOpts != null && serverOpts.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
-            {
-                createNettySslContext(serverOpts, true, SocketType.SERVER, openSslIsAvailable());
-                createNettySslContext(serverOpts, true, SocketType.CLIENT, openSslIsAvailable());
-            }
+            this.settingDescription = null;
         }
-        catch (Exception e)
+
+        public LoggingCipherSuiteFilter(String settingDescription)
         {
-            throw new IOException(""Failed to create SSL context using server_encryption_options!"", e);
+            this.settingDescription = settingDescription;
         }
 
-        try
+
+        @Override
+        public String[] filterCipherSuites(Iterable<String> ciphers, List<String> defaultCiphers,
+                                           Set<String> supportedCiphers)
         {
-            // Ensure we're able to create both server & client SslContexts if they might ever be needed
-            if (clientOpts != null && clientOpts.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
+            Objects.requireNonNull(defaultCiphers, ""defaultCiphers"");
+            Objects.requireNonNull(supportedCiphers, ""supportedCiphers"");
+
+            final List<String> newCiphers;
+            if (ciphers == null)
+            {
+                newCiphers = new ArrayList<>(defaultCiphers.size());
+                ciphers = defaultCiphers;
+            }
+            else
+            {
+                newCiphers = new ArrayList<>(supportedCiphers.size());
+            }
+            for (String c : ciphers)
             {
-                createNettySslContext(clientOpts, clientOpts.require_client_auth, SocketType.SERVER, openSslIsAvailable());
-                createNettySslContext(clientOpts, clientOpts.require_client_auth, SocketType.CLIENT, openSslIsAvailable());
+                if (c == null)
+                {
+                    break;
+                }
+                if (supportedCiphers.contains(c))
+                {
+                    newCiphers.add(c);
+                }
+                else
+                {
+                    if (settingDescription != null)
+                    {
+                        logger.warn(""Dropping unsupported cipher_suite {} from {} configuration"",
+                                    c, settingDescription.toLowerCase());
+                    }
+                }
             }
+            if (newCiphers.isEmpty())
+            {
+                throw new IllegalStateException(""No ciphers left after filtering supported cipher suite"");
+            }
+
+            return newCiphers.toArray(new String[0]);
         }
-        catch (Exception e)
+    }
+
+    public static void validateSslContext(String contextDescription, EncryptionOptions options, boolean buildTrustStore, boolean logProtocolAndCiphers) throws IOException
+    {
+        if (options != null && options.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
         {
-            throw new IOException(""Failed to create SSL context using client_encryption_options!"", e);
+            try
+            {
+                CipherSuiteFilter loggingCipherSuiteFilter =logProtocolAndCiphers ?  new LoggingCipherSuiteFilter(contextDescription)
+                                                                                  : LoggingCipherSuiteFilter.QUIET_FILTER;
+                SslContext sslContext = createNettySslContext(options, buildTrustStore, SocketType.SERVER, openSslIsAvailable(), loggingCipherSuiteFilter);
+                SSLEngine engine = sslContext.newEngine(ByteBufAllocator.DEFAULT);
+
+                if (logProtocolAndCiphers)
+                {
+                    String[] supportedProtocols = engine.getSupportedProtocols();
+                    String[] supportedCiphers = engine.getSupportedCipherSuites();
+                    String[] enabledProtocols = engine.getEnabledProtocols();
+                    String[] enabledCiphers = engine.getEnabledCipherSuites();
+
+                    logger.debug(""{} supported TLS protocols: {}"", contextDescription,
+                                 supportedProtocols == null ? ""system default"" : String.join("", "", supportedProtocols));
+                    logger.info(""{} enabled TLS protocols: {}"", contextDescription,
+                                enabledProtocols == null ? ""system default"" : String.join("", "", enabledProtocols));
+                    logger.debug(""{} supported cipher suites: {}"", contextDescription,
+                                 supportedCiphers == null ? ""system default"" : String.join("", "", supportedCiphers));
+                    logger.info(""{} enabled cipher suites: {}"", contextDescription,
+                                enabledCiphers == null ? ""system default"" : String.join("", "", enabledCiphers));
+                }
+
+                // Make sure it is possible to build the client context too
+                createNettySslContext(options, buildTrustStore, SocketType.CLIENT, openSslIsAvailable());
+            }
+            catch (Exception e)
+            {
+                throw new IOException(""Failed to create SSL context using "" + contextDescription, e);
+            }
         }
     }
 
+    /**
+     * Sanity checks all certificates to ensure we can actually load them
+     */
+    public static void validateSslCerts(EncryptionOptions.ServerEncryptionOptions serverOpts, EncryptionOptions clientOpts) throws IOException
+    {
+        validateSslContext(""server_encryption_options"", serverOpts, true, false);","[{'comment': 'Instead of true/false values, can we use meaningful constants?', 'commenter': 'dineshjoshi'}]"
770,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -209,6 +214,89 @@ public void setOptional(boolean optional) {
         this.optional = optional;
     }
 
+    /**
+     * Sets accepted TLS protocol for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea.
+     * @param protocol value to set
+     */
+    @VisibleForTesting
+    public void setProtocol(String protocol) {
+        this.protocol = protocol;
+    }
+
+    /**
+     * Sets accepted TLS protocols for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea. The function casing is required for snakeyaml to find this setter for the protected field.
+     * @param accepted_protocols value to set
+     */
+    public void setaccepted_protocols(List<String> accepted_protocols) {
+        this.accepted_protocols = ImmutableList.copyOf(accepted_protocols);
+    }
+
+    /* This list is substituted in configurations that have explicitly specified the original ""TLS"" default,
+     * it is not a 'default' list or 'support protocol versions' list.  It is just an attempt to preserve the
+     * original intent for the user configuration
+     */
+    private final List<String> tlsProtocolSubstitution = ImmutableList.of(""TLSv1.3"", ""TLSv1.2"", ""TLSv1.1"", ""TLSv1"");
+
+    /**
+     * Combine the pre-4.0 protocol field with the accepted_protocols list, substituting a list of
+     * explicit protocols for the previous catchall default of ""TLS""
+     * @return array of protocol names suitable for passing to SslContextBuilder.protocols, or null if the default
+     */
+    public List<String> acceptedProtocols()
+    {
+        if (accepted_protocols == null)
+        {
+            if (protocol == null)
+            {
+                return null;
+            }
+            // TLS is accepted by SSLContext.getInstance as a shorthand for give me an engine that
+            // can speak some of the TLS protocols.  It is not supported by SSLEngine.setAcceptedProtocols
+            // so substitute if the user hasn't provided an accepted protocol configuration
+            else if (protocol.equalsIgnoreCase(""TLS""))
+            {
+                return tlsProtocolSubstitution;
+            }
+            else // the user was trying to limit to a single specific protocol, so try that
+            {
+                return ImmutableList.of(protocol);
+            }
+        }
+
+        if (protocol != null && !protocol.equalsIgnoreCase(""TLS"") &&
+            accepted_protocols.stream().noneMatch(ap -> ap.equalsIgnoreCase(protocol)))
+        {
+            // If the user provided a non-generic default protocol, append it to accepted_protocols - they wanted
+            // it after all.
+            return ImmutableList.<String>builder().addAll(accepted_protocols).add(protocol).build();
+        }
+        else
+        {
+            return accepted_protocols;
+        }
+    }
+
+    public String[] acceptedProtocolsArray()
+    {
+        List<String> ap = acceptedProtocols();
+        if (ap == null)
+            return new String[]{};","[{'comment': 'nit: `new String[0];`', 'commenter': 'dcapwell'}, {'comment': 'ack', 'commenter': 'jonmeredith'}]"
770,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -209,6 +214,89 @@ public void setOptional(boolean optional) {
         this.optional = optional;
     }
 
+    /**
+     * Sets accepted TLS protocol for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea.
+     * @param protocol value to set
+     */
+    @VisibleForTesting
+    public void setProtocol(String protocol) {
+        this.protocol = protocol;
+    }
+
+    /**
+     * Sets accepted TLS protocols for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea. The function casing is required for snakeyaml to find this setter for the protected field.
+     * @param accepted_protocols value to set
+     */
+    public void setaccepted_protocols(List<String> accepted_protocols) {
+        this.accepted_protocols = ImmutableList.copyOf(accepted_protocols);
+    }
+
+    /* This list is substituted in configurations that have explicitly specified the original ""TLS"" default,
+     * it is not a 'default' list or 'support protocol versions' list.  It is just an attempt to preserve the
+     * original intent for the user configuration
+     */
+    private final List<String> tlsProtocolSubstitution = ImmutableList.of(""TLSv1.3"", ""TLSv1.2"", ""TLSv1.1"", ""TLSv1"");
+
+    /**
+     * Combine the pre-4.0 protocol field with the accepted_protocols list, substituting a list of
+     * explicit protocols for the previous catchall default of ""TLS""
+     * @return array of protocol names suitable for passing to SslContextBuilder.protocols, or null if the default
+     */
+    public List<String> acceptedProtocols()
+    {
+        if (accepted_protocols == null)
+        {
+            if (protocol == null)
+            {
+                return null;
+            }
+            // TLS is accepted by SSLContext.getInstance as a shorthand for give me an engine that
+            // can speak some of the TLS protocols.  It is not supported by SSLEngine.setAcceptedProtocols
+            // so substitute if the user hasn't provided an accepted protocol configuration
+            else if (protocol.equalsIgnoreCase(""TLS""))
+            {
+                return tlsProtocolSubstitution;
+            }
+            else // the user was trying to limit to a single specific protocol, so try that
+            {
+                return ImmutableList.of(protocol);
+            }
+        }
+
+        if (protocol != null && !protocol.equalsIgnoreCase(""TLS"") &&
+            accepted_protocols.stream().noneMatch(ap -> ap.equalsIgnoreCase(protocol)))
+        {
+            // If the user provided a non-generic default protocol, append it to accepted_protocols - they wanted
+            // it after all.
+            return ImmutableList.<String>builder().addAll(accepted_protocols).add(protocol).build();
+        }
+        else
+        {
+            return accepted_protocols;
+        }
+    }
+
+    public String[] acceptedProtocolsArray()
+    {
+        List<String> ap = acceptedProtocols();
+        if (ap == null)","[{'comment': 'personal preference:\r\n\r\n```\r\nreturn ap == null ? new String[0] : ap.toArray(new String[0]);\r\n```', 'commenter': 'dcapwell'}, {'comment': 'unlikely to be throwing stacktraces so happy to take your more concise suggestion.', 'commenter': 'jonmeredith'}]"
770,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -209,6 +214,89 @@ public void setOptional(boolean optional) {
         this.optional = optional;
     }
 
+    /**
+     * Sets accepted TLS protocol for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea.
+     * @param protocol value to set
+     */
+    @VisibleForTesting
+    public void setProtocol(String protocol) {
+        this.protocol = protocol;
+    }
+
+    /**
+     * Sets accepted TLS protocols for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea. The function casing is required for snakeyaml to find this setter for the protected field.
+     * @param accepted_protocols value to set
+     */
+    public void setaccepted_protocols(List<String> accepted_protocols) {
+        this.accepted_protocols = ImmutableList.copyOf(accepted_protocols);
+    }
+
+    /* This list is substituted in configurations that have explicitly specified the original ""TLS"" default,
+     * it is not a 'default' list or 'support protocol versions' list.  It is just an attempt to preserve the
+     * original intent for the user configuration
+     */
+    private final List<String> tlsProtocolSubstitution = ImmutableList.of(""TLSv1.3"", ""TLSv1.2"", ""TLSv1.1"", ""TLSv1"");
+
+    /**
+     * Combine the pre-4.0 protocol field with the accepted_protocols list, substituting a list of
+     * explicit protocols for the previous catchall default of ""TLS""
+     * @return array of protocol names suitable for passing to SslContextBuilder.protocols, or null if the default
+     */
+    public List<String> acceptedProtocols()
+    {
+        if (accepted_protocols == null)
+        {
+            if (protocol == null)
+            {
+                return null;
+            }
+            // TLS is accepted by SSLContext.getInstance as a shorthand for give me an engine that
+            // can speak some of the TLS protocols.  It is not supported by SSLEngine.setAcceptedProtocols
+            // so substitute if the user hasn't provided an accepted protocol configuration
+            else if (protocol.equalsIgnoreCase(""TLS""))
+            {
+                return tlsProtocolSubstitution;
+            }
+            else // the user was trying to limit to a single specific protocol, so try that
+            {
+                return ImmutableList.of(protocol);
+            }
+        }
+
+        if (protocol != null && !protocol.equalsIgnoreCase(""TLS"") &&
+            accepted_protocols.stream().noneMatch(ap -> ap.equalsIgnoreCase(protocol)))
+        {
+            // If the user provided a non-generic default protocol, append it to accepted_protocols - they wanted
+            // it after all.
+            return ImmutableList.<String>builder().addAll(accepted_protocols).add(protocol).build();
+        }
+        else
+        {
+            return accepted_protocols;
+        }
+    }
+
+    public String[] acceptedProtocolsArray()
+    {
+        List<String> ap = acceptedProtocols();
+        if (ap == null)
+            return new String[]{};
+        else
+            return ap.toArray(new String[0]);
+    }
+
+    public String[] cipherSuitesArray()
+    {
+        if (cipher_suites == null)
+            return new String[]{};","[{'comment': 'nit: `new String[0]`', 'commenter': 'dcapwell'}, {'comment': 'ack', 'commenter': 'jonmeredith'}]"
770,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -209,6 +214,89 @@ public void setOptional(boolean optional) {
         this.optional = optional;
     }
 
+    /**
+     * Sets accepted TLS protocol for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea.
+     * @param protocol value to set
+     */
+    @VisibleForTesting
+    public void setProtocol(String protocol) {
+        this.protocol = protocol;
+    }
+
+    /**
+     * Sets accepted TLS protocols for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea. The function casing is required for snakeyaml to find this setter for the protected field.
+     * @param accepted_protocols value to set
+     */
+    public void setaccepted_protocols(List<String> accepted_protocols) {
+        this.accepted_protocols = ImmutableList.copyOf(accepted_protocols);
+    }
+
+    /* This list is substituted in configurations that have explicitly specified the original ""TLS"" default,
+     * it is not a 'default' list or 'support protocol versions' list.  It is just an attempt to preserve the
+     * original intent for the user configuration
+     */
+    private final List<String> tlsProtocolSubstitution = ImmutableList.of(""TLSv1.3"", ""TLSv1.2"", ""TLSv1.1"", ""TLSv1"");
+
+    /**
+     * Combine the pre-4.0 protocol field with the accepted_protocols list, substituting a list of
+     * explicit protocols for the previous catchall default of ""TLS""
+     * @return array of protocol names suitable for passing to SslContextBuilder.protocols, or null if the default
+     */
+    public List<String> acceptedProtocols()
+    {
+        if (accepted_protocols == null)
+        {
+            if (protocol == null)
+            {
+                return null;
+            }
+            // TLS is accepted by SSLContext.getInstance as a shorthand for give me an engine that
+            // can speak some of the TLS protocols.  It is not supported by SSLEngine.setAcceptedProtocols
+            // so substitute if the user hasn't provided an accepted protocol configuration
+            else if (protocol.equalsIgnoreCase(""TLS""))
+            {
+                return tlsProtocolSubstitution;
+            }
+            else // the user was trying to limit to a single specific protocol, so try that
+            {
+                return ImmutableList.of(protocol);
+            }
+        }
+
+        if (protocol != null && !protocol.equalsIgnoreCase(""TLS"") &&
+            accepted_protocols.stream().noneMatch(ap -> ap.equalsIgnoreCase(protocol)))
+        {
+            // If the user provided a non-generic default protocol, append it to accepted_protocols - they wanted
+            // it after all.
+            return ImmutableList.<String>builder().addAll(accepted_protocols).add(protocol).build();
+        }
+        else
+        {
+            return accepted_protocols;
+        }
+    }
+
+    public String[] acceptedProtocolsArray()
+    {
+        List<String> ap = acceptedProtocols();
+        if (ap == null)
+            return new String[]{};
+        else
+            return ap.toArray(new String[0]);
+    }
+
+    public String[] cipherSuitesArray()
+    {
+        if (cipher_suites == null)","[{'comment': 'personal preference\r\n\r\n```\r\nreturn cipher_suites == null ? new String[0] : cipher_suites.toArray(new String[0])\r\n```', 'commenter': 'dcapwell'}, {'comment': 'ack', 'commenter': 'jonmeredith'}]"
770,src/java/org/apache/cassandra/db/virtual/SettingsTable.java,"@@ -20,7 +20,9 @@
 import java.lang.reflect.Field;
 import java.lang.reflect.Modifier;
 import java.util.Arrays;
+import java.util.List;","[{'comment': 'can you remove import?  not used.', 'commenter': 'dcapwell'}, {'comment': 'ack', 'commenter': 'jonmeredith'}]"
770,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -209,6 +214,89 @@ public void setOptional(boolean optional) {
         this.optional = optional;
     }
 
+    /**
+     * Sets accepted TLS protocol for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea.
+     * @param protocol value to set
+     */
+    @VisibleForTesting
+    public void setProtocol(String protocol) {
+        this.protocol = protocol;
+    }
+
+    /**
+     * Sets accepted TLS protocols for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea. The function casing is required for snakeyaml to find this setter for the protected field.
+     * @param accepted_protocols value to set
+     */
+    public void setaccepted_protocols(List<String> accepted_protocols) {
+        this.accepted_protocols = ImmutableList.copyOf(accepted_protocols);","[{'comment': ""The constructor now supports `null` but this doesn't, so should we do a null check to match?\r\n\r\n```\r\nthis.accepted_protocols = accepted_protocols == null ? null : ImmutableList.copyOf(accepted_protocols);\r\n```"", 'commenter': 'dcapwell'}, {'comment': 'ack', 'commenter': 'jonmeredith'}]"
770,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -209,6 +214,83 @@ public void setOptional(boolean optional) {
         this.optional = optional;
     }
 
+    /**
+     * Sets accepted TLS protocol for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea.
+     * @param protocol value to set
+     */
+    @VisibleForTesting
+    public void setProtocol(String protocol) {
+        this.protocol = protocol;
+    }
+
+    /**
+     * Sets accepted TLS protocols for this channel. Note that this should only be called by
+     * the configuration parser or tests. It is public only for that purpose, mutating protocol state
+     * is probably a bad idea. The function casing is required for snakeyaml to find this setter for the protected field.
+     * @param accepted_protocols value to set
+     */
+    public void setaccepted_protocols(List<String> accepted_protocols) {
+        this.accepted_protocols = accepted_protocols == null ? null : ImmutableList.copyOf(accepted_protocols);
+    }
+
+    /* This list is substituted in configurations that have explicitly specified the original ""TLS"" default,
+     * it is not a 'default' list or 'support protocol versions' list.  It is just an attempt to preserve the
+     * original intent for the user configuration
+     */
+    private final List<String> tlsProtocolSubstitution = ImmutableList.of(""TLSv1.3"", ""TLSv1.2"", ""TLSv1.1"", ""TLSv1"");","[{'comment': 'nit: should be static and named `TLS_PROTOCOL_SUBSTITUTION`', 'commenter': 'dcapwell'}]"
770,src/java/org/apache/cassandra/security/SSLFactory.java,"@@ -397,41 +392,121 @@ public static synchronized void initHotReloading(EncryptionOptions.ServerEncrypt
         isHotReloadingInitialized = true;
     }
 
-
-    /**
-     * Sanity checks all certificates to ensure we can actually load them
+    // Non-logging
+    /*
+     * This class will filter all requested ciphers out that are not supported by the current {@link SSLEngine},
+     * logging messages for all dropped ciphers, and throws an exception if no ciphers are supported
      */
-    public static void validateSslCerts(EncryptionOptions.ServerEncryptionOptions serverOpts, EncryptionOptions clientOpts) throws IOException
+    public static final class LoggingCipherSuiteFilter implements CipherSuiteFilter
     {
-        try
+        // Version without logging the ciphers, make sure same filtering logic is used
+        // all the time, regardless of user output.
+        public static final CipherSuiteFilter QUIET_FILTER = new LoggingCipherSuiteFilter();
+        final String settingDescription;
+
+        private LoggingCipherSuiteFilter()
         {
-            // Ensure we're able to create both server & client SslContexts if they might ever be needed
-            if (serverOpts != null && serverOpts.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
-            {
-                createNettySslContext(serverOpts, true, SocketType.SERVER, openSslIsAvailable());
-                createNettySslContext(serverOpts, true, SocketType.CLIENT, openSslIsAvailable());
-            }
+            this.settingDescription = null;
         }
-        catch (Exception e)
+
+        public LoggingCipherSuiteFilter(String settingDescription)
         {
-            throw new IOException(""Failed to create SSL context using server_encryption_options!"", e);
+            this.settingDescription = settingDescription;
         }
 
-        try
+
+        @Override
+        public String[] filterCipherSuites(Iterable<String> ciphers, List<String> defaultCiphers,
+                                           Set<String> supportedCiphers)
         {
-            // Ensure we're able to create both server & client SslContexts if they might ever be needed
-            if (clientOpts != null && clientOpts.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
+            Objects.requireNonNull(defaultCiphers, ""defaultCiphers"");
+            Objects.requireNonNull(supportedCiphers, ""supportedCiphers"");
+
+            final List<String> newCiphers;
+            if (ciphers == null)
+            {
+                newCiphers = new ArrayList<>(defaultCiphers.size());
+                ciphers = defaultCiphers;
+            }
+            else
+            {
+                newCiphers = new ArrayList<>(supportedCiphers.size());
+            }
+            for (String c : ciphers)
             {
-                createNettySslContext(clientOpts, clientOpts.require_client_auth, SocketType.SERVER, openSslIsAvailable());
-                createNettySslContext(clientOpts, clientOpts.require_client_auth, SocketType.CLIENT, openSslIsAvailable());
+                if (c == null)
+                {
+                    break;
+                }
+                if (supportedCiphers.contains(c))
+                {
+                    newCiphers.add(c);
+                }
+                else
+                {
+                    if (settingDescription != null)
+                    {
+                        logger.warn(""Dropping unsupported cipher_suite {} from {} configuration"",
+                                    c, settingDescription.toLowerCase());
+                    }
+                }
             }
+            if (newCiphers.isEmpty())
+            {
+                throw new IllegalStateException(""No ciphers left after filtering supported cipher suite"");
+            }
+
+            return newCiphers.toArray(new String[0]);
         }
-        catch (Exception e)
+    }
+
+    public static void validateSslContext(String contextDescription, EncryptionOptions options, boolean buildTrustStore, boolean logProtocolAndCiphers) throws IOException
+    {
+        if (options != null && options.tlsEncryptionPolicy() != EncryptionOptions.TlsEncryptionPolicy.UNENCRYPTED)
         {
-            throw new IOException(""Failed to create SSL context using client_encryption_options!"", e);
+            try
+            {
+                CipherSuiteFilter loggingCipherSuiteFilter = logProtocolAndCiphers ? new LoggingCipherSuiteFilter(contextDescription)
+                                                                                   : LoggingCipherSuiteFilter.QUIET_FILTER;
+                SslContext sslContext = createNettySslContext(options, buildTrustStore, SocketType.SERVER, openSslIsAvailable(), loggingCipherSuiteFilter);
+                SSLEngine engine = sslContext.newEngine(ByteBufAllocator.DEFAULT);","[{'comment': ""don't we need to release as you don't actually use this? its ref counted and `io.netty.handler.ssl.ReferenceCountedOpenSslEngine` looks like it should be cleaned up if allocated."", 'commenter': 'dcapwell'}, {'comment': 'Thanks for spotting this. Pushed up the fix we discussed.', 'commenter': 'jonmeredith'}]"
781,test/unit/org/apache/cassandra/io/util/DataOutputTest.java,"@@ -245,8 +246,8 @@ public void testDataOutputBufferBigReallocation() throws Exception
             Assert.assertEquals(DataOutputBuffer.MAX_ARRAY_SIZE, write.validateReallocation(DataOutputBuffer.MAX_ARRAY_SIZE + 1L));
             Assert.assertEquals(DataOutputBuffer.MAX_ARRAY_SIZE, write.validateReallocation(DataOutputBuffer.MAX_ARRAY_SIZE));
             Assert.assertEquals(DataOutputBuffer.MAX_ARRAY_SIZE - 1, write.validateReallocation(DataOutputBuffer.MAX_ARRAY_SIZE - 1));
-            checkThrowsRuntimeException(validateReallocationCallable( write, 0));
-            checkThrowsRuntimeException(validateReallocationCallable( write, 1));","[{'comment': '`checkThrowsRuntimeException()` is no longer referenced. We can remove it.  ', 'commenter': 'yifan-c'}]"
781,test/unit/org/apache/cassandra/io/util/DataOutputTest.java,"@@ -180,8 +181,9 @@ public void testDataOutputBufferMaxSizeFake() throws IOException
                 while (true)
                     write.publicFlush();
             }
-            catch (RuntimeException e) {
-                if (e.getClass() == RuntimeException.class)
+            catch (RuntimeException e)","[{'comment': 'nit: only catch `BufferOverflowException` so it does not swallow other RTE, if any. ', 'commenter': 'yifan-c'}]"
781,src/java/org/apache/cassandra/db/rows/PartitionSerializationException.java,"@@ -0,0 +1,37 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.rows;
+
+import org.apache.cassandra.schema.TableMetadata;
+
+public class PartitionSerializationException extends RuntimeException
+{
+    public PartitionSerializationException(UnfilteredRowIterator partition, Throwable cause)
+    {
+        super(buildMessage(partition), cause);
+    }
+
+    private static String buildMessage(UnfilteredRowIterator partition)
+    {
+        TableMetadata table = partition.metadata();
+        Object readablePartitionkey = table.partitionKeyType.compose(partition.partitionKey().getKey());","[{'comment': 'Looks like `partitionKeyType.getString()` is more commonly used in the code base to get the readable representation of partition key. \r\n\r\nBtw, both `getString` and `compose and toString` produce the same result. \r\nBut for the **composite partition key**, the output is not so nice. It contains utf8 characters. For the table \r\n```\r\nCREATE TABLE ks.tbl (key ascii, name ascii, val ascii, val1 ascii, PRIMARY KEY ((key, name), val))\r\n```\r\nThe partition key string for `key1, name1` is \r\n```\r\n\\x00\\x04key1\\x00\\x00\\x04name1\\x00\r\n```\r\n', 'commenter': 'yifan-c'}, {'comment': 'It would be nice to have a test to verify if the message matches expectation. ', 'commenter': 'yifan-c'}, {'comment': ""> But for the composite partition key, the output is not so nice. It contains utf8 characters.\r\n\r\nLooks like switching to `getString()` at minimum makes sense. I'm not sure if we should solve the composite problem. Will noodle on it...\r\n\r\n> a test to verify if the message matches expectation\r\n\r\nAgreed."", 'commenter': 'maedhroz'}]"
786,src/java/org/apache/cassandra/net/Message.java,"@@ -367,8 +368,13 @@ private Header(long id, Verb verb, InetAddressAndPort from, long createdAtNanos,
             this.id = id;
             this.verb = verb;
             this.from = from;
-            this.createdAtNanos = createdAtNanos;
             this.expiresAtNanos = expiresAtNanos;
+            if (verb.isResponse())
+                // Correct createdAtNanos to enforce the contraint, createdAtNanos <= expiresAtNanos
+                this.createdAtNanos = Math.min(createdAtNanos, expiresAtNanos);
+            else
+                this.createdAtNanos = createdAtNanos;
+            Preconditions.checkArgument(this.createdAtNanos <= this.expiresAtNanos, ""createdAtNanos cannot be more recent than expiresAtNanos"");","[{'comment': 'if this fails, would be good to also print the time.  To keep the string create lazy, may be best to do\r\n\r\n```\r\nif (this.createdAtNanos > this.expiresAtNanos)\r\n  throw new IllegalArgumentException(""createdAtNanos (""+this.createdAtNanos+"") cannot be more recent than expiresAtNanos (""+this.expiresAtNanos+"")"");\r\n```', 'commenter': 'dcapwell'}]"
786,src/java/org/apache/cassandra/net/Message.java,"@@ -367,8 +368,13 @@ private Header(long id, Verb verb, InetAddressAndPort from, long createdAtNanos,
             this.id = id;
             this.verb = verb;
             this.from = from;
-            this.createdAtNanos = createdAtNanos;
             this.expiresAtNanos = expiresAtNanos;
+            if (verb.isResponse())
+                // Correct createdAtNanos to enforce the contraint, createdAtNanos <= expiresAtNanos
+                this.createdAtNanos = Math.min(createdAtNanos, expiresAtNanos);","[{'comment': 'not sure how I feel about this... its no longer created at if we do this...  will need to think about this more.', 'commenter': 'dcapwell'}, {'comment': 'agree. will put more thoughts.', 'commenter': 'yifan-c'}, {'comment': 'Perhaps add a check that expiresAtNanos >= createdAtNanos and update `responseWith` and `failureResponse` to ensure the time is correct (or riskier make sure expiredTimeout() does the min check)?', 'commenter': 'jonmeredith'}, {'comment': ""At the high-level, the coordinator sends request messages to peers and waits for the response messages. The coordinator's waiting time is based on the original request messages' `createdAt` and `expiresAt` (AbstractReadExecutor and AbstractWriteResponseHandler for e.g.). The response message's `createdAt` is not important. \r\nIf looking at the `InboundMessageHandler`, the only field used for expiry check is `header.expiresAtNanos`. The `createdAt` time is not used. \r\n**Therefore, as long as the `expiresAt` is not changed across nodes, I think it makes no harm.** \r\n\r\nThe main benefit of correcting the `createdAt` in response message is that the serialized `VUnsignedInt` field for `createdAt` takes 1 byte instead of 9 bytes in the case of negative values.\r\n\r\nWe can also revert the constraint if not convinced, since the read/write unsigned vint can apply to negative values in fact with the cost of slightly larger message in some edge cases.  "", 'commenter': 'yifan-c'}, {'comment': ""I'm unconvinced by this change too - it's accepted and acceptable for rare edge cases to spend 9 bytes encoding a negative value with `writeUnsignedVInt`. The cost of those 9 bytes is negligible, and not worth trying to optimise in any way."", 'commenter': 'belliottsmith'}, {'comment': 'Reverted the correction to `createdAt`.', 'commenter': 'yifan-c'}]"
786,test/unit/org/apache/cassandra/net/FramingTest.java,"@@ -213,6 +217,37 @@ public void burnRandomLegacy()
         burnRandomLegacy(1000);
     }
 
+    @Test
+    public void testSerializeSizeMatchesEdgeCases() // See CASSANDRA-16103
+    {
+        int v40 = MessagingService.Version.VERSION_40.value;
+        Consumer<Long> subTest = timeGap ->
+        {
+            long createdAt = 0;
+            long expiresAt = createdAt + timeGap;
+            Message<NoPayload> message = Message.builder(Verb.READ_REPAIR_RSP, NoPayload.noPayload)
+                                                .from(FBUtilities.getBroadcastAddressAndPort())
+                                                .withCreatedAt(createdAt)
+                                                .withExpiresAt(TimeUnit.MILLISECONDS.toNanos(expiresAt))","[{'comment': ""withCreatedAt takes nanos, so `createdAt` has to be nanos, so timeGap must also be in nanos... which means `expiresAt` is also nanos.... so shouldn't have this conversion?"", 'commenter': 'dcapwell'}, {'comment': 'Good catch. The `timeGap` is in milliseconds. Fixed. ', 'commenter': 'yifan-c'}]"
786,test/unit/org/apache/cassandra/net/MessageTest.java,"@@ -39,6 +39,8 @@
 import org.apache.cassandra.tracing.Tracing.TraceType;
 import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.utils.UUIDGen;
+import org.assertj.core.api.Assertions;
+import org.openjdk.jmh.annotations.TearDown;","[{'comment': 'can you remove this import?  not used.', 'commenter': 'dcapwell'}]"
786,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -177,21 +178,28 @@
     // all time is boxed in the future around 50 years from today: Aug 20th, 2020 UTC
     public static final Gen<Timestamp> TIMESTAMP_GEN;
     public static final Gen<Date> DATE_GEN;
+    public static final Gen<Long> TIMESTAMP_NANOS;
+    public static final Gen<Long> SMALL_TIME_SPAN_GEN;
 
     static
     {
+        long secondInNanos = 1_000_000_000L;
         ZonedDateTime now = ZonedDateTime.of(2020, 8, 20,
                                              0, 0, 0, 0, ZoneOffset.UTC);
         ZonedDateTime startOfTime = now.minusYears(50);
         ZonedDateTime endOfDays = now.plusYears(50);
         Constraint millisConstraint = Constraint.between(startOfTime.toInstant().toEpochMilli(), endOfDays.toInstant().toEpochMilli());
-        Constraint nanosInSecondConstraint = Constraint.between(0, 999999999);
+        Constraint nanosInSecondConstraint = Constraint.between(0, secondInNanos - 1);
+        // Represents the timespan based on the most of the default request timeouts. See DatabaseDescriptor
+        Constraint smallTimeSpanNanosConstraint = Constraint.between(-1 * secondInNanos, 10 * secondInNanos);","[{'comment': 'I feel weird having negative time here, though in your test case it makes sense; should we move this generator there?', 'commenter': 'dcapwell'}]"
786,test/unit/org/apache/cassandra/utils/CassandraGenerators.java,"@@ -105,8 +114,26 @@
                                                                                        .<Message<? extends ReadCommand>>map(c -> Message.builder(Verb.READ_REQ, c).build())
                                                                                        .describedAs(CassandraGenerators::toStringRecursive);
 
+    private static Gen<Message<NoPayload>> responseGen(Verb verb)
+    {
+        return gen(rnd -> {
+            long timeSpan = SMALL_TIME_SPAN_GEN.generate(rnd);","[{'comment': ""I kinda feel its more clear if we do the following (assuming SMALL_TIME_SPAN_GEN doesn't return negative values, see other comment)\r\n\r\n```\r\nlong realCreatedAt = TIMESTAMP_NANOS.generate(rnd);\r\nlong networkDelay = SMALL_TIME_SPAN_GEN.generate(rnd);\r\nlong timeSpan = SMALL_TIME_SPAN_GEN.generate(rnd);\r\n\r\nlong createdAtNanos = realCreatedAt + networkDelay;\r\nlong expiresAtNanos = realCreatedAt + timeSpan;\r\n```\r\n\r\nmy thinking is, we keep track of the request create timestamp, and account for network delay.  This will get us into cases where createdAtNanos is > expiresAtNanos."", 'commenter': 'dcapwell'}]"
786,src/java/org/apache/cassandra/net/Message.java,"@@ -717,7 +725,7 @@ private int serializedHeaderSizePost40(Header header, int version)
             long size = 0;
             size += sizeofUnsignedVInt(header.id);
             size += CREATION_TIME_SIZE;
-            size += sizeofUnsignedVInt(1 + NANOSECONDS.toMillis(header.expiresAtNanos - header.createdAtNanos));
+            size += sizeofUnsignedVInt(NANOSECONDS.toMillis(header.expiresAtNanos - header.createdAtNanos));","[{'comment': ""Why aren't we adding the 1 to the serializeHeader instead?  This bug stems from missing the change in both places for  CASSANDRA-16064, and I can't remember why we though it was good. Pre-16064 this would have made sure the value was >=1 for a sane local clock.\r\n\r\nProbably more for @dcapwell to remember than @yifan-c "", 'commenter': 'jonmeredith'}, {'comment': 'Any value >= 0 for the first byte in vUnsignedInt can be correctly recognized that the vUnsignedInt contains only 1 byte. Not sure how the `+1` helps. And it does not help to prevent negative values too. (except -1)\r\nI would guess `+1` was written before \r\n```\r\n// in org.apache.cassandra.utils.vint.VIntCoding#computeUnsignedVIntSize(long)\r\nLong.numberOfLeadingZeros(value | 1); // | with 1 to ensure magntiude <= 63, so (63 - 1) / 7 <= 8\r\n```\r\nThe latter one (`value | 1`) is better and voids the need for `+1`. ', 'commenter': 'yifan-c'}, {'comment': 'I removed the +1 as it was missing on deserialize so caused `msg != deserialize(serialize(msg))`; it looks like I missed the removal of `+1` on size of.\r\n\r\nI talked to Aleksey and Benedict about why the `+1` was there and neither had an answer and said to remove it.', 'commenter': 'dcapwell'}]"
786,src/java/org/apache/cassandra/net/Message.java,"@@ -28,6 +28,7 @@
 import javax.annotation.Nullable;
 
 import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;","[{'comment': 'Do you still need this?', 'commenter': 'jonmeredith'}, {'comment': 'removed.', 'commenter': 'yifan-c'}]"
786,test/unit/org/apache/cassandra/utils/CassandraGenerators.java,"@@ -76,6 +81,11 @@
     private static final Gen<Integer> SMALL_POSITIVE_SIZE_GEN = SourceDSL.integers().between(1, 30);
     private static final Gen<Boolean> BOOLEAN_GEN = SourceDSL.booleans().all();
 
+    public static final Gen<InetAddressAndPort> INET_ADDRESS_AND_PORT_GEN = rnd -> {
+        InetAddress address = Generators.INET_ADDRESS_GEN.generate(rnd);
+        return InetAddressAndPort.getByAddressOverrideDefaults(address, SMALL_POSITIVE_SIZE_GEN.generate(rnd));","[{'comment': 'it would be better to use a port generator which returns values between `0-65,535`', 'commenter': 'dcapwell'}]"
875,src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java,"@@ -2276,8 +2276,13 @@ public static SSTableReader moveAndOpenSSTable(ColumnFamilyStore cfs, Descriptor
             throw new RuntimeException(msg);
         }
 
-        logger.info(""Renaming new SSTable {} to {}"", oldDescriptor, newDescriptor);
-        SSTableWriter.rename(oldDescriptor, newDescriptor, components);
+        if (copyData) {","[{'comment': 'brace on newline (same for the whole patch)', 'commenter': 'krummas'}]"
875,src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java,"@@ -2276,8 +2276,13 @@ public static SSTableReader moveAndOpenSSTable(ColumnFamilyStore cfs, Descriptor
             throw new RuntimeException(msg);
         }
 
-        logger.info(""Renaming new SSTable {} to {}"", oldDescriptor, newDescriptor);
-        SSTableWriter.rename(oldDescriptor, newDescriptor, components);
+        if (copyData) {
+            logger.info(""Copying new SSTable {} to {}"", oldDescriptor, newDescriptor);
+            SSTableWriter.copy(oldDescriptor, newDescriptor, components);","[{'comment': 'We should probably try to hard link the file here? If it fails we can fall back to copy', 'commenter': 'krummas'}]"
875,src/java/org/apache/cassandra/io/util/FileUtils.java,"@@ -240,6 +240,35 @@ public static void deleteWithConfirmWithThrottle(File file, RateLimiter rateLimi
         maybeFail(deleteWithConfirm(file, null, rateLimiter));
     }
 
+    public static void copyWithOutConfirm(String from, String to)
+    {
+        try {
+            Files.copy(new File(from).toPath(), new File(to).toPath());
+        } catch (IOException e) {
+            if (logger.isTraceEnabled())
+                logger.trace(""Could not copy file"" + from + "" to "" + to, e);
+        }
+    }
+
+    public static void copyWithConfirm(String from, String to)
+    {
+        copyWithConfirm(new File(from), new File(to));
+    }
+
+    public static void copyWithConfirm(File from, File to)
+    {
+        assert from.exists();
+        if (logger.isTraceEnabled())
+            logger.trace(""Copying {} to {}"", from.getPath(), to.getPath());
+
+        try {
+            Files.copy(from.toPath(), to.toPath());
+        } catch (IOException e) {","[{'comment': 'we should not ignore `IOException` here ', 'commenter': 'krummas'}]"
875,src/java/org/apache/cassandra/io/util/FileUtils.java,"@@ -240,6 +240,35 @@ public static void deleteWithConfirmWithThrottle(File file, RateLimiter rateLimi
         maybeFail(deleteWithConfirm(file, null, rateLimiter));
     }
 
+    public static void copyWithOutConfirm(String from, String to)
+    {
+        try {
+            Files.copy(new File(from).toPath(), new File(to).toPath());","[{'comment': 'nit: `Paths.get(from)`, `Paths.get(to)`', 'commenter': 'krummas'}]"
875,build.xml,"@@ -589,7 +589,8 @@
           <dependency groupId=""com.github.jbellis"" artifactId=""jamm"" version=""${jamm.version}""/>
           <dependency groupId=""org.yaml"" artifactId=""snakeyaml"" version=""1.26""/>
           <dependency groupId=""junit"" artifactId=""junit"" version=""4.12"" />
-          <dependency groupId=""org.mockito"" artifactId=""mockito-core"" version=""3.2.4"" />
+          <dependency groupId=""org.mockito"" artifactId=""mockito-core"" version=""3.7.7"" />","[{'comment': 'I had to increase the version as this version supports mocking of static methods (with use of Powermock I tried too but it was failing for various reasons), here we have static mocks available nicely. The first version supporting this is 3.4.0 but why not to bump it to the latest one if we are doing it anyway.\r\n\r\nhttps://wttech.blog/blog/2020/mocking-static-methods-made-possible-in-mockito-3.4.0/', 'commenter': 'smiklosovic'}]"
875,build.xml,"@@ -589,7 +589,8 @@
           <dependency groupId=""com.github.jbellis"" artifactId=""jamm"" version=""${jamm.version}""/>
           <dependency groupId=""org.yaml"" artifactId=""snakeyaml"" version=""1.26""/>
           <dependency groupId=""junit"" artifactId=""junit"" version=""4.12"" />
-          <dependency groupId=""org.mockito"" artifactId=""mockito-core"" version=""3.2.4"" />
+          <dependency groupId=""org.mockito"" artifactId=""mockito-core"" version=""3.7.7"" />
+          <dependency groupId=""org.mockito"" artifactId=""mockito-inline"" version=""3.7.7"" />","[{'comment': 'needed for statics', 'commenter': 'smiklosovic'}]"
875,src/java/org/apache/cassandra/io/util/FileUtils.java,"@@ -66,7 +66,7 @@
 import static org.apache.cassandra.utils.Throwables.maybeFail;
 import static org.apache.cassandra.utils.Throwables.merge;
 
-public final class FileUtils
+public class FileUtils","[{'comment': 'It does not matter to have it final as it already has private constructor so it can not be extended anyway as extender can not call super and it just blocks mocking as it can not deal with finals.', 'commenter': 'smiklosovic'}, {'comment': 'not relevant anymore', 'commenter': 'smiklosovic'}]"
875,src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java,"@@ -2276,8 +2276,24 @@ public static SSTableReader moveAndOpenSSTable(ColumnFamilyStore cfs, Descriptor
             throw new RuntimeException(msg);
         }
 
-        logger.info(""Renaming new SSTable {} to {}"", oldDescriptor, newDescriptor);
-        SSTableWriter.rename(oldDescriptor, newDescriptor, components);
+        if (copyData)
+        {
+            try
+            {
+                logger.info(""Hardlinking new SSTable {} to {}"", oldDescriptor, newDescriptor);
+                SSTableWriter.hardlink(oldDescriptor, newDescriptor, components);
+            }
+            catch (Throwable t)","[{'comment': 'we should not catch throwable here without rethrowing - we might hide OOM errors for example\r\n', 'commenter': 'krummas'}, {'comment': '@krummas but that is the point, because in that catch block I am copying SSTables instead of hardlinking them. If I rethrow, how will I fall back to copy then? ', 'commenter': 'smiklosovic'}, {'comment': 'catch `FSWriteError` or what is actually thrown by `SSTableWriter.hardlink`?', 'commenter': 'krummas'}]"
875,src/java/org/apache/cassandra/io/util/FileUtils.java,"@@ -193,6 +171,58 @@ public static File createDeletableTempFile(String prefix, String suffix)
         return f;
     }
 
+    public static void createHardLink(String from, String to)
+    {
+        createHardLink(new File(from), new File(to));
+    }
+
+    public static void createHardLink(File from, File to)
+    {
+        if (to.exists())
+            throw new RuntimeException(""Tried to create duplicate hard link to "" + to);
+        if (!from.exists())
+            throw new RuntimeException(""Tried to hard link to file that does not exist "" + from);
+
+        try
+        {
+            Files.createLink(to.toPath(), from.toPath());
+        }
+        catch (IOException e)
+        {
+            throw new FSWriteError(e, to);
+        }
+    }
+
+    public static void createHardLinkWithConfirm(File from, File to)
+    {
+        try
+        {
+            createHardLink(from, to);
+        }
+        catch (Throwable t)
+        {
+            throw new RuntimeException(String.format(""Unable to hardlink from %s to %s"", from, to), t);
+        }
+    }
+
+    public static void createHardLinkWithConfirm(String from, String to)
+    {
+        createHardLinkWithConfirm(new File(from), new File(to));
+    }
+
+    public static void createHardLinkWithoutConfirm(String from, String to)
+    {
+        try
+        {
+            createHardLink(new File(from), new File(to));
+        }
+        catch (Throwable t)","[{'comment': ""same as above - don't catch throwable"", 'commenter': 'krummas'}]"
875,src/java/org/apache/cassandra/tools/nodetool/Import.java,"@@ -75,6 +75,11 @@
             description = ""Run an extended verify, verifying all values in the new sstables"")
     private boolean extendedVerify = false;
 
+    @Option(title = ""copy_data"",
+            name = {""-p"", ""--copy-data""},","[{'comment': 'why `-p` ? ', 'commenter': 'michaelsembwever'}]"
898,src/java/org/apache/cassandra/db/Slice.java,"@@ -200,7 +200,12 @@ public Slice forPaging(ClusteringComparator comparator, Clustering<?> lastReturn
             if (cmp < 0 || (inclusive && cmp == 0))
                 return this;
 
-            return new Slice(start, inclusive ? ClusteringBound.inclusiveEndOf(lastReturned) : ClusteringBound.exclusiveEndOf(lastReturned));
+            ClusteringBound<?> nextEnd = inclusive ? ClusteringBound.inclusiveEndOf(lastReturned) : ClusteringBound.exclusiveEndOf(lastReturned);
+
+            if (isEmpty(comparator, start, nextEnd))","[{'comment': 'suggestion, not entirely sure it makes things clearer, but maybe bake this in the `cmp` check above - we can return null if `cmp == 0 && (!isInclusive || !start.isInclusive())` I think?', 'commenter': 'krummas'}]"
898,src/java/org/apache/cassandra/db/Slice.java,"@@ -212,7 +217,12 @@ public Slice forPaging(ClusteringComparator comparator, Clustering<?> lastReturn
             if (cmp < 0 || (inclusive && cmp == 0))
                 return this;
 
-            return new Slice(inclusive ? ClusteringBound.inclusiveStartOf(lastReturned) : ClusteringBound.exclusiveStartOf(lastReturned), end);
+            ClusteringBound<?> nextStart = inclusive ? ClusteringBound.inclusiveStartOf(lastReturned) : ClusteringBound.exclusiveStartOf(lastReturned);
+
+            if (isEmpty(comparator, nextStart, end))","[{'comment': 'same but with `end`^', 'commenter': 'krummas'}]"
898,src/java/org/apache/cassandra/db/filter/ClusteringIndexSliceFilter.java,"@@ -56,6 +56,20 @@ public boolean selectsAllPartition()
         return slices.size() == 1 && !slices.hasLowerBound() && !slices.hasUpperBound();
     }
 
+    public boolean isEmpty(ClusteringComparator comparator)
+    {
+        if (slices.isEmpty())
+            return true;
+
+        for (Slice slice : slices)
+        {
+            if (slice.isEmpty(comparator))
+                return true;","[{'comment': 'This is a bit wrong, if only one or some of the slices is empty, but not all.', 'commenter': 'iamaleksey'}]"
949,src/java/org/apache/cassandra/locator/Ec2MultiRegionSnitchIMDSv2.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.service.StorageService;
+
+/**
+ * Is exactly the same as the Ec2MultiRegionSnitch except it uses the Instance MetaData Service v2 (IMDSv2) which
+ * requires you to get a session token first.
+ */
+public class Ec2MultiRegionSnitchIMDSv2 extends Ec2SnitchIMDSv2
+{
+
+    private static final String PUBLIC_IP_QUERY_URL = ""http://169.254.169.254/latest/meta-data/public-ipv4"";
+    private static final String PRIVATE_IP_QUERY_URL = ""http://169.254.169.254/latest/meta-data/local-ipv4"";
+    private final String localPrivateAddress;
+
+    public Ec2MultiRegionSnitchIMDSv2() throws IOException, ConfigurationException
+    {
+        super();
+        InetAddress localPublicAddress = InetAddress.getByName(awsApiCall(PUBLIC_IP_QUERY_URL));
+        logger.info(""EC2Snitch using publicIP as identifier: {}"", localPublicAddress);
+        localPrivateAddress = awsApiCall(PRIVATE_IP_QUERY_URL);
+        // use the Public IP to broadcast Address to other nodes.
+        DatabaseDescriptor.setBroadcastAddress(localPublicAddress);","[{'comment': '@driftx this is the same as in the original snitch: https://github.com/blueconic/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/locator/Ec2MultiRegionSnitch.java#L53\r\n\r\nSo we left it the same.', 'commenter': 'paulrutter'}]"
949,src/java/org/apache/cassandra/locator/Ec2SnitchIMDSv2.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+import java.io.DataInputStream;
+import java.io.FilterInputStream;
+import java.io.IOException;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * An implementation of the Ec2Snitch which uses Instance Meta Data Service v2 (IMDSv2) which requires you
+ * to get a session token first before calling the metaData service
+ */
+public class Ec2SnitchIMDSv2 extends Ec2Snitch
+{
+    protected static final Logger logger = LoggerFactory.getLogger(Ec2SnitchIMDSv2.class);
+    private static final long REFRESH_TOKEN_TIME = 21600;
+    private static final String AWS_EC2_METADATA_HEADER_TTL = ""X-aws-ec2-metadata-token-ttl-seconds"";
+    private static final String TOKEN_TTL_SECONDS = String.valueOf(REFRESH_TOKEN_TIME);
+    private static final String AWS_EC2_METADATA_HEADER = ""X-aws-ec2-metadata-token"";
+    private static final String TOKEN_ENDPOINT = ""http://169.254.169.254/latest/api/token"";
+
+    private String myToken;
+    private Long myLastTokenTime;
+
+
+    public Ec2SnitchIMDSv2() throws IOException, ConfigurationException
+    {
+        super();
+    }
+
+    @Override
+    String awsApiCall(final String url) throws IOException, ConfigurationException
+    {
+        // Populate the region and zone by introspection, fail if 404 on metadata
+        if (myToken == null || myLastTokenTime == null
+            || System.currentTimeMillis() - myLastTokenTime > (REFRESH_TOKEN_TIME - 100))
+        {
+            getAndSetNewToken();
+        }
+        final HttpURLConnection conn = (HttpURLConnection) new URL(url).openConnection();
+        conn.setRequestProperty(AWS_EC2_METADATA_HEADER, myToken);
+        return getContent(conn);
+    }
+
+    /**
+     * Get a session token to enable requests to the meta data service.
+     * https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html
+     *
+     * @throws IOException
+     */
+    private void getAndSetNewToken() throws IOException
+    {
+        final URL url = new URL(TOKEN_ENDPOINT);
+        final HttpURLConnection http = (HttpURLConnection) url.openConnection();
+        http.setRequestProperty(AWS_EC2_METADATA_HEADER_TTL, TOKEN_TTL_SECONDS);
+        http.setRequestMethod(""PUT"");
+
+        myToken = getContent(http);
+        myLastTokenTime = System.currentTimeMillis();
+    }
+
+    private String getContent(final HttpURLConnection conn) throws IOException
+    {
+        DataInputStream d = null;
+        try
+        {
+            if (conn.getResponseCode() != 200)
+            {
+                throw new ConfigurationException(
+                ""Ec2SnitchIMDSv2 was unable to execute the API call. Not an ec2 node?"");
+            }
+            // Read the information. I wish I could say (String) conn.getContent() here...
+            final int cl = conn.getContentLength();
+            final byte[] b = new byte[cl];
+            d = new DataInputStream((FilterInputStream) conn.getContent());
+            d.readFully(b);
+            return new String(b, StandardCharsets.UTF_8);
+        }
+        finally","[{'comment': ""The original doesn't do this, so we left this the same: https://github.com/blueconic/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/Ec2Snitch.java#L104"", 'commenter': 'paulrutter'}]"
1071,build.xml,"@@ -528,7 +528,9 @@
           </dependency>
           <dependency groupId=""org.mockito"" artifactId=""mockito-core"" version=""3.2.4"" scope=""test""/>
           <dependency groupId=""org.quicktheories"" artifactId=""quicktheories"" version=""0.26"" scope=""test""/>
-          <dependency groupId=""com.google.code.java-allocation-instrumenter"" artifactId=""java-allocation-instrumenter"" version=""${allocation-instrumenter.version}"" scope=""test""/>
+          <dependency groupId=""com.google.code.java-allocation-instrumenter"" artifactId=""java-allocation-instrumenter"" version=""${allocation-instrumenter.version}"" scope=""test"">
+            <exclusion groupId=""com.google.guava"" artifactId=""guava""/>
+          </dependency>","[{'comment': 'interesting, `test-memory` target is broken.\r\n\r\nit throws `Error opening zip file or JAR manifest missing : build/lib/jars/java-allocation-instrumenter-3.1.0.jar`\r\nas that jar file is now test scope and only found under build/test/lib/jar/ now\r\nref: https://github.com/apache/cassandra/blob/cassandra-4.0/build.xml#L1609\r\n\r\n@jacek-lewandowski would you mind if we fix that in this PR?  \r\nAs I cannot see the use of `import com.google.monitoring.runtime.instrumentation` anywhere outside of tests, \r\nso line 1609 should be instead\r\n```\r\n<jvmarg value=""-javaagent:${build.dir}/test/lib/jars/java-allocation-instrumenter-${allocation-instrumenter.version}.jar""/>\r\n```', 'commenter': 'michaelsembwever'}]"
1184,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -1698,9 +1702,9 @@ public CompactionManager.AllSSTableOpStatus verify(Verifier.Options options) thr
     /**
      * Rewrites all SSTables according to specified parameters
      *
-     * @param skipIfCurrentVersion - if {@link true}, will rewrite only SSTables that have version older than the current one ({@link BigFormat#latestVersion})
+     * @param skipIfCurrentVersion - if {@link true}, will rewrite only SSTables that have version older than the current one ({@link org.apache.cassandra.io.sstable.format.big.BigFormat#latestVersion})","[{'comment': 'opportunistic bugfix, without this IDEA marks it errorneous hence all class', 'commenter': 'smiklosovic'}, {'comment': '👍 ', 'commenter': 'pauloricardomg'}]"
1184,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -1698,9 +1702,9 @@ public CompactionManager.AllSSTableOpStatus verify(Verifier.Options options) thr
     /**
      * Rewrites all SSTables according to specified parameters
      *
-     * @param skipIfCurrentVersion - if {@link true}, will rewrite only SSTables that have version older than the current one ({@link BigFormat#latestVersion})
+     * @param skipIfCurrentVersion - if {@link true}, will rewrite only SSTables that have version older than the current one ({@link org.apache.cassandra.io.sstable.format.big.BigFormat#latestVersion})
      * @param skipIfNewerThanTimestamp - max timestamp (local creation time) for SSTable; SSTables created _after_ this timestamp will be excluded from compaction
-     * @param skipIfCompressionMatches - if {@link true}, will rewrite only SSTables whose compression parameters are different from {@link CFMetaData#compressionParams()}
+     * @param skipIfCompressionMatches - if {@link true}, will rewrite only SSTables whose compression parameters are different from {@link TableMetadata#params#getCompressionParameters()} ()}","[{'comment': 'opportunistic bugfix, without this IDEA marks it errorneous hence all class', 'commenter': 'smiklosovic'}, {'comment': '👍 ', 'commenter': 'pauloricardomg'}]"
1184,src/java/org/apache/cassandra/service/snapshot/SnapshotManager.java,"@@ -80,43 +81,58 @@ public Collection<TableSnapshot> getExpiringSnapshots()
         return expiringSnapshots;
     }
 
+    public Collection<TableSnapshot> getEphemeralSnapshots()
+    {
+        return ephemeralSnapshots;
+    }
+
     public synchronized void start()
     {
-        loadSnapshots();
+        if (!snapshotsLoaded)
+            loadSnapshots();
+
         resumeSnapshotCleanup();
     }
 
     public synchronized void stop() throws InterruptedException, TimeoutException
     {
         expiringSnapshots.clear();
+        ephemeralSnapshots.clear();
         if (cleanupTaskFuture != null)
         {
             cleanupTaskFuture.cancel(false);
             cleanupTaskFuture = null;
         }
+        snapshotsLoaded = false;
     }
 
     public synchronized void addSnapshot(TableSnapshot snapshot)
     {
-        // We currently only care about expiring snapshots
-        if (snapshot.isExpiring())
+        // We currently care only about expiring and ephemeral snapshots
+        if (snapshot.isExpiring() && !expiringSnapshots.contains(snapshot))
         {
             logger.debug(""Adding expiring snapshot {}"", snapshot);
             expiringSnapshots.add(snapshot);
         }
+        // we are adding ephemeral snapshots only in case we are doing the initial loading
+        // as we are not interested in ephemeral snapshots in manager when node is running
+        else if (snapshot.isEphemeral() && !ephemeralSnapshots.contains(snapshot) && !snapshotsLoaded)
+        {
+            logger.debug(""Adding ephemeral snapshot {}"", snapshot);
+            ephemeralSnapshots.add(snapshot);
+        }
     }
 
-    @VisibleForTesting
-    protected synchronized void loadSnapshots()
+    public synchronized void loadSnapshots()
     {
         SnapshotLoader loader = new SnapshotLoader(DatabaseDescriptor.getAllDataFileLocations());
         addSnapshots(loader.loadSnapshots());
+        snapshotsLoaded = true;
     }
 
     @VisibleForTesting
     protected synchronized void addSnapshots(Collection<TableSnapshot> snapshots)
     {
-        logger.debug(""Adding snapshots: {}."", Joiner.on("", "").join(snapshots.stream().map(s -> s.getId()).collect(Collectors.toList())));","[{'comment': 'I moved this to addSnapshot method where I differ between expiring and ephemeral snapshot.', 'commenter': 'smiklosovic'}]"
1184,test/unit/org/apache/cassandra/db/DirectoriesTest.java,"@@ -105,7 +105,7 @@
     private static Set<TableMetadata> CFM;
     private static Map<String, List<File>> sstablesByTableName;
 
-    @Parameterized.Parameter(0)","[{'comment': '0 is default', 'commenter': 'smiklosovic'}]"
1184,test/unit/org/apache/cassandra/db/DirectoriesTest.java,"@@ -151,15 +151,15 @@ public void beforeTest() throws IOException
     @AfterClass
     public static void afterClass()
     {
-        FileUtils.deleteRecursive(tempDataDir);","[{'comment': 'opportunistic fix as the used method was marked as deprecated, similar cases in this PR at various places', 'commenter': 'smiklosovic'}, {'comment': 'sounds good.', 'commenter': 'pauloricardomg'}]"
1184,src/java/org/apache/cassandra/db/ColumnFamilyStore.java,"@@ -817,14 +829,6 @@ public static void  scrubDataDirectories(TableMetadata metadata) throws StartupE
                     if (!file.tryDelete())
                         logger.warn(""could not delete {}"", file.absolutePath());
         }
-","[{'comment': 'why is this being removed?', 'commenter': 'pauloricardomg'}, {'comment': 'can return back', 'commenter': 'smiklosovic'}, {'comment': 'ah right I see what you ask, like why I removed that block of the code? Because it is not necessary as I moved the logic up into scrubDataDirectories where I am fitlering indexes and we will scrub it with all tables as well. ', 'commenter': 'smiklosovic'}, {'comment': 'ok thanks for clarifying', 'commenter': 'pauloricardomg'}]"
1184,src/java/org/apache/cassandra/db/Directories.java,"@@ -977,24 +966,30 @@ public Map<String, TableSnapshot> listSnapshots()
             String tag = entry.getKey();
             Set<File> snapshotDirs = entry.getValue();
             SnapshotManifest manifest = maybeLoadManifest(metadata.keyspace, metadata.name, tag, snapshotDirs);
-            snapshots.put(tag, buildSnapshot(tag, manifest, snapshotDirs));
+            boolean ephemeral = manifest == null ? isEphemeral(snapshotDirs) : manifest.ephemeral;
+            snapshots.put(tag, buildSnapshot(tag, manifest, snapshotDirs, ephemeral));
         }
 
         return snapshots;
     }
 
-    protected TableSnapshot buildSnapshot(String tag, SnapshotManifest manifest, Set<File> snapshotDirs) {
+    protected TableSnapshot buildSnapshot(String tag, SnapshotManifest manifest, Set<File> snapshotDirs, boolean ephemeral) {
         Instant createdAt = manifest == null ? null : manifest.createdAt;
         Instant expiresAt = manifest == null ? null : manifest.expiresAt;
         return new TableSnapshot(metadata.keyspace, metadata.name, metadata.id.asUUID(), tag, createdAt, expiresAt,
-                                 snapshotDirs);
+                                 snapshotDirs, ephemeral);
+    }
+
+    protected static boolean isEphemeral(Set<File> snapshotDirs)","[{'comment': 'Can you add a short comment about why this method is needed - should we remove it at some point and just use the manifest to decide if a snapshot is ephemeral?', 'commenter': 'pauloricardomg'}, {'comment': 'ok i ll add comment', 'commenter': 'smiklosovic'}, {'comment': 'I rewrote it so that method is not needed anymore.', 'commenter': 'smiklosovic'}]"
1184,src/java/org/apache/cassandra/db/Directories.java,"@@ -1018,42 +1013,6 @@ protected static SnapshotManifest maybeLoadManifest(String keyspace, String tabl
         return null;
     }
 
-    public List<String> listEphemeralSnapshots()
-    {
-        final List<String> ephemeralSnapshots = new LinkedList<>();
-        for (File snapshot : listAllSnapshots())
-        {
-            if (getEphemeralSnapshotMarkerFile(snapshot).exists())
-                ephemeralSnapshots.add(snapshot.name());
-        }
-        return ephemeralSnapshots;
-    }
-
-    private List<File> listAllSnapshots()","[{'comment': 'why is this removed?', 'commenter': 'pauloricardomg'}, {'comment': 'I think it is because that method is not necessary anymore. That method was used in `listEphemeralSnapshots` but we do not need that method anymore.', 'commenter': 'smiklosovic'}, {'comment': 'ok thanks for clarifying.', 'commenter': 'pauloricardomg'}]"
1184,src/java/org/apache/cassandra/db/Directories.java,"@@ -983,18 +972,23 @@ public Map<String, TableSnapshot> listSnapshots()
         return snapshots;
     }
 
-    protected TableSnapshot buildSnapshot(String tag, SnapshotManifest manifest, Set<File> snapshotDirs) {
+    private TableSnapshot buildSnapshot(String tag, SnapshotManifest manifest, Set<File> snapshotDirs) {
+        boolean ephemeral;","[{'comment': ""I think a ternary if would be slightly more readable:\r\n`boolean ephemeral = manifest != null? manifest.isEphemeral() : isLegacyEphemeralSnapshot(snapshotDirs)`\r\n\r\nThis allow us to test the `isLegacyEphemeralSnapshot(snapshotDirs)` logic on a unit test more easily since it's a static method"", 'commenter': 'pauloricardomg'}]"
1184,src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java,"@@ -50,6 +55,7 @@ public void execute(NodeProbe probe)
 
             Map<String, String> options = new HashMap<>();
             options.put(""no_ttl"", Boolean.toString(noTTL));
+            options.put(""no_ephemeral"", Boolean.toString(!ephemeral));","[{'comment': ""It doesn't feel natural passing `!ephemeral`. Since the default is to **NOT** show snapshots, the property name should be called `include_ephemeral`, which defaults to false - and we can pass the `ephemeral` field without negating it."", 'commenter': 'pauloricardomg'}, {'comment': 'ok', 'commenter': 'smiklosovic'}]"
1184,src/java/org/apache/cassandra/service/CassandraDaemon.java,"@@ -373,6 +374,10 @@ protected void setup()
             throw new RuntimeException(e);
         }
 
+        // here we preload all snapshots so once we execute SnapshotManager#start we will have all snapshots scanned already
+        StorageService.instance.snapshotManager.loadSnapshots();
+        StorageService.instance.snapshotManager.clearSnapshots(StorageService.instance.snapshotManager.getEphemeralSnapshots());","[{'comment': ""I didn't understand why are we moving snapshot cleaning from `scrubDataDirectories`. I thought startup leftover cleanup logic was centralized there - this will make the logic be spread around multiple places. Not sure if it's a problem or not."", 'commenter': 'pauloricardomg'}, {'comment': 'Answered below.', 'commenter': 'smiklosovic'}]"
1184,src/java/org/apache/cassandra/service/StorageService.java,"@@ -4136,6 +4136,7 @@ public void clearSnapshot(String tag, String... keyspaceNames) throws IOExceptio
     public Map<String, TabularData> getSnapshotDetails(Map<String, String> options)
     {
         boolean skipExpiring = options != null && Boolean.parseBoolean(options.getOrDefault(""no_ttl"", ""false""));
+        boolean skipEphemeral = options != null && Boolean.parseBoolean(options.getOrDefault(""no_ephemeral"", ""true""));","[{'comment': 'this should probably be called `includeEphemeral` to be aligned with the nodetool parameter.', 'commenter': 'pauloricardomg'}, {'comment': 'no problem, I was trying to follow same logic we did in skipExpiring / ttl but if it reads better, why not.', 'commenter': 'smiklosovic'}]"
1184,src/java/org/apache/cassandra/service/snapshot/SnapshotManager.java,"@@ -80,43 +81,58 @@ public Collection<TableSnapshot> getExpiringSnapshots()
         return expiringSnapshots;
     }
 
+    public Collection<TableSnapshot> getEphemeralSnapshots()
+    {
+        return ephemeralSnapshots;
+    }
+
     public synchronized void start()
     {
-        loadSnapshots();
+        if (!snapshotsLoaded)
+            loadSnapshots();
+
         resumeSnapshotCleanup();
     }
 
     public synchronized void stop() throws InterruptedException, TimeoutException
     {
         expiringSnapshots.clear();
+        ephemeralSnapshots.clear();
         if (cleanupTaskFuture != null)
         {
             cleanupTaskFuture.cancel(false);
             cleanupTaskFuture = null;
         }
+        snapshotsLoaded = false;
     }
 
     public synchronized void addSnapshot(TableSnapshot snapshot)
     {
-        // We currently only care about expiring snapshots
-        if (snapshot.isExpiring())
+        // We currently care only about expiring and ephemeral snapshots
+        if (snapshot.isExpiring() && !expiringSnapshots.contains(snapshot))
         {
-            logger.debug(""Adding expiring snapshot {}"", snapshot);
+            logger.debug(""Adding expiring snapshot {}"", snapshot.getId());
             expiringSnapshots.add(snapshot);
         }
+        // we are adding ephemeral snapshots only in case we are doing the initial loading","[{'comment': ""I don't like this logic because it's inconsistent depending on the node state. We should either *always* include ephemeral, or never include ephemeral. Why do we need to load ephemeral snapshots in `SnapshotManager`? Can't we use `SnapshotLoader` to load ephemeral snapshots on `scrubDataDirectories` - no need to load on `SnapshotManager` ?"", 'commenter': 'pauloricardomg'}, {'comment': 'My logic behind that was that when we put this in scrub method, we walk the disk once and then we need to walk it for the second time when we are loading snapshots in manager. I do not know if this is a big problem to scan it twice or not. I was trying to do it in such a way that we need to walk it just once. If there is 10 000 snaphots I think it starts to make a difference if we walk once or twice.', 'commenter': 'smiklosovic'}, {'comment': 'To add to it - the problem is that if we are going to _always_ include ephemerals, you will have them during whole life of the node because they can be removed only on startup. So we will be basically only adding snapshots. Is not this a concern from memory point of view?', 'commenter': 'smiklosovic'}, {'comment': 'Well, I mean ... we can indeed always add it and then an operator can delete these ephemerals after he sees them via nodetool, yeah. I was not taking that possibility into account. The fact that they are _always removed on startup_ does not mean that we could not remove it while a node is up, right?', 'commenter': 'smiklosovic'}]"
1184,src/java/org/apache/cassandra/db/Directories.java,"@@ -973,17 +973,17 @@ public Map<String, TableSnapshot> listSnapshots()
     }
 
     private TableSnapshot buildSnapshot(String tag, SnapshotManifest manifest, Set<File> snapshotDirs) {
-        boolean ephemeral;
-        if (manifest == null)
-            ephemeral = snapshotDirs.stream().map(d -> new File(d, ""ephemeral.snapshot"")).anyMatch(File::exists);
-        else
-            ephemeral = manifest.isEphemeral();
+        boolean ephemeral = manifest != null ? manifest.isEphemeral() : isLegacyEphemeralSnapshot(snapshotDirs);
         Instant createdAt = manifest == null ? null : manifest.createdAt;
         Instant expiresAt = manifest == null ? null : manifest.expiresAt;
         return new TableSnapshot(metadata.keyspace, metadata.name, metadata.id.asUUID(), tag, createdAt, expiresAt,
                                  snapshotDirs, ephemeral);
     }
 ","[{'comment': 'TODO Ideally add VisibleForTesting and test it.', 'commenter': 'smiklosovic'}]"
1208,src/java/org/apache/cassandra/auth/Roles.java,"@@ -38,26 +35,8 @@
 
     private static final Role NO_ROLE = new Role("""", false, false, Collections.emptyMap(), Collections.emptySet());
 
-    private static RolesCache cache;
-    static
-    {
-        initRolesCache(DatabaseDescriptor.getRoleManager(),
-                       () -> DatabaseDescriptor.getAuthenticator().requireAuthentication());
-    }
-
-    @VisibleForTesting
-    public static void initRolesCache(IRoleManager roleManager, BooleanSupplier enableCache)
-    {
-        if (cache != null)
-            cache.unregisterMBean();
-        cache = new RolesCache(roleManager, enableCache);
-    }
-
-    @VisibleForTesting
-    public static void clearCache()
-    {
-        cache.invalidate();
-    }
+    public static final RolesCache cache = new RolesCache(DatabaseDescriptor.getRoleManager(),","[{'comment': 'Similarly to other caches it is setup in tests during initialization instead of changing the behavior via ""VisibleForTesting methods"". Here I ensured the cache instance is immutable (stored in a static final variable), so it is safe to expose.', 'commenter': 'azotcsit'}, {'comment': 'This seems to have overlapped with CASSANDRA-17016 being committed and which makes use of `initRolesCache`.', 'commenter': 'beobal'}, {'comment': ""Good catch! Again rebase did not highlight the issue, I'll make sure to run the build locally next time. Fixed now."", 'commenter': 'azotcsit'}]"
1208,src/java/org/apache/cassandra/auth/PasswordAuthenticator.java,"@@ -69,14 +69,18 @@
     static final byte NUL = 0;
     private SelectStatement authenticateStatement;
 
-    private CredentialsCache cache;
+    private static CredentialsCache cache;
 
     // No anonymous access.
     public boolean requireAuthentication()
     {
         return true;
     }
 
+    public static CredentialsCache getCredentialsCache() {","[{'comment': ""Need to discuss this and decided how we want to deal with this. From one hand, it exposes the cache outside and a kind of breaks encapsulation. From another hand, I really need the access to the cache from outside. Theoretically I could expose some particular methods, but:\r\n1. There are many methods to expose\r\n- invalidate()\r\n- invalidate(key)\r\n- getAll()\r\n- get(key)\r\n2. exposing some cache-related methods from PasswordAuthenticator/Roles/etc classes seems to be a bit confusing\r\n\r\nThe same problem is actual for ALL caches. For now, I made all of them public, so I could access them. But we need to decided what we want to do - keep it as is / expose methods partially / make some structural changes (not sure what exactly).\r\n\r\nThe current implementation with public access to the static caches still seems to be same since the caches are thread-safe and immutable (I mean the static instance variables - aka constants).\r\n\r\nI'd be happy to hear any suggestions.\r\n"", 'commenter': 'azotcsit'}, {'comment': ""I don't see a problem with making the caches public and breaking encapsulation/making them all first class citizens. In fact, they are intended to be independent of the underlying auth implementations as far as is possible, it's just that the nature of SASL authentication doesn't lend itself to a generic caching approach in the same way as simple user/pass credentials do."", 'commenter': 'beobal'}]"
1208,src/java/org/apache/cassandra/db/virtual/AbstractMutableVirtualTable.java,"@@ -145,7 +145,7 @@ protected void applyRangeTombstone(ColumnValues partitionKey, Range<ColumnValues
         throw invalidRequest(""Range deletion is not supported by table %s"", metadata);
     }
 
-    protected void applyRowDeletion(ColumnValues partitionKey, ColumnValues clusteringColumnValues)
+    protected void applyRowDeletion(ColumnValues partitionKey, ColumnValues clusteringColumns)","[{'comment': 'Missed that on review, so fixing now.', 'commenter': 'azotcsit'}]"
1208,test/unit/org/apache/cassandra/auth/RolesTest.java,"@@ -40,14 +46,12 @@
     public static void setupClass()
     {
         SchemaLoader.prepareServer();
-        // create the system_auth keyspace so the IRoleManager can function as normal
-        SchemaLoader.createKeyspace(SchemaConstants.AUTH_KEYSPACE_NAME,
-                                    KeyspaceParams.simple(1),
-                                    Iterables.toArray(AuthKeyspace.metadata().tables, TableMetadata.class));
-
-        IRoleManager roleManager = new LocalCassandraRoleManager();
-        roleManager.setup();
-        Roles.initRolesCache(roleManager, () -> true);
+        LocalCassandraRoleManager roleManager = new LocalCassandraRoleManager();","[{'comment': ""That's the right way to initialize this test, so there won't be a need for `Roles.initRolesCache` method."", 'commenter': 'azotcsit'}]"
1208,doc/source/new/virtualtables.rst,"@@ -308,6 +324,56 @@ As another example, to find how much time is remaining for SSTable tasks, use th
   SELECT total - progress AS remaining
   FROM system_views.sstable_tasks;
 
+Auth Caches Virtual Tables
+****************************
+
+Every authentication cache has a separate virtual table associated. The virtual tables show the data stored in caches
+and additionally support DELETE and TRUNCATE operations. In fact these operations just invalidate data in caches, no
+data is actually deleted from real tables.
+
+The tables show the following information:
+
+::
+
+  cqlsh:system_views> select * from credentials_cache;","[{'comment': ""I have a general concern on the current structure of this page - it is hard to maintain it (keep up-to-date). Maybe we should not list all table here? It won't be a part of this ticket for sure, I just want to raise the question..."", 'commenter': 'azotcsit'}]"
1208,src/java/org/apache/cassandra/db/virtual/JmxPermissionsCacheTable.java,"@@ -0,0 +1,94 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db.virtual;
+
+import java.util.Optional;
+import java.util.Set;
+
+import org.apache.cassandra.auth.IResource;
+import org.apache.cassandra.auth.Permission;
+import org.apache.cassandra.auth.PermissionDetails;
+import org.apache.cassandra.auth.Resources;
+import org.apache.cassandra.auth.RoleResource;
+import org.apache.cassandra.auth.jmx.AuthorizationProxy;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.schema.TableMetadata;
+
+final class JmxPermissionsCacheTable extends AbstractMutableVirtualTable
+{
+    private static final String ROLE = ""role"";
+    private static final String GRANTEE = ""grantee"";
+    private static final String RESOURCE = ""resource"";
+    private static final String PERMISSION = ""permission"";
+
+    JmxPermissionsCacheTable(String keyspace)
+    {
+        super(TableMetadata.builder(keyspace, ""jmx_permissions_cache"")
+                .comment(""JMX permissions cache"")
+                .kind(TableMetadata.Kind.VIRTUAL)
+                .partitioner(new LocalPartitioner(UTF8Type.instance))
+                .addPartitionKeyColumn(ROLE, UTF8Type.instance)
+                .addClusteringColumn(GRANTEE, UTF8Type.instance)
+                .addClusteringColumn(RESOURCE, UTF8Type.instance)
+                .addClusteringColumn(PERMISSION, UTF8Type.instance)","[{'comment': 'Same comment applies here as to the main (non-jmx) permissions cache wrt to granularity & clustering columns', 'commenter': 'beobal'}, {'comment': 'This class does not exist anymore. The latest version has `.addPartitionKeyColumn(ROLE, UTF8Type.instance)` only.\r\n\r\nIt seems to be an old comment that you added to review some time ago and posted now.', 'commenter': 'azotcsit'}, {'comment': ""hmm, sorry about that. I did start the review before your last update, but when I submitted it it didn't show me any of the outdated comments. Sorry for the noise!"", 'commenter': 'beobal'}]"
1208,src/java/org/apache/cassandra/tools/nodetool/InvalidatePermissionsCache.java,"@@ -38,7 +38,7 @@
 @Command(name = ""invalidatepermissionscache"", description = ""Invalidate the permissions cache"")
 public class InvalidatePermissionsCache extends NodeToolCmd
 {
-    @Arguments(usage = ""[<user>]"", description = ""A specific user for whom permissions need to be invalidated"")","[{'comment': 'I renamed user to role here to be coherent with the VT. Also ""user"" term seems to be outdated since everything relies onto roles now.', 'commenter': 'azotcsit'}]"
1208,test/unit/org/apache/cassandra/auth/CassandraNetworkAuthorizerTest.java,"@@ -63,13 +62,12 @@ public static void setupSuperUser()
     public static void defineSchema() throws ConfigurationException
     {
         SchemaLoader.prepareServer();
-        SchemaLoader.setupAuth(new LocalCassandraRoleManager(),
-                               new PasswordAuthenticator(),
-                               new AuthTestUtils.LocalCassandraAuthorizer(),
-                               new AuthTestUtils.LocalCassandraNetworkAuthorizer());
+        SchemaLoader.setupAuth(new AuthTestUtils.LocalCassandraRoleManager(),","[{'comment': ""nit: this is highly subjective, but I find the original alignment more readable. It wouldn't stop me +1'ing a patch, but I just wanted to mention in case it was done by an IDE unintentionally. (nb. applies in multiple locations in the PR)"", 'commenter': 'beobal'}, {'comment': ""Yes, it was automatically formatted by Intellij. I fixed it everywhere, but generally I'm not sure how to deal with this problem. We either need to document some project-wide stylistic preferences or define some checkstyle rules. Otherwise, people will keep changing formatting based on their subjective preferences."", 'commenter': 'azotcsit'}, {'comment': ""There is some documentation on this [here](https://cassandra.apache.org/_/development/code_style.html), although when I went to check it, it appears to have gotten a bit mangled over the course of a few site updates. \r\n\r\nThe alignment as illustrated in the style guide looks more like yours/Intellij's, but previously this was specified in the way it was initially [(see history)](https://web.archive.org/web/20191020015442/http://cassandra.apache.org/doc/latest/development/code_style.html)\r\n\r\nThe code style page does include links to style configs for Eclipse and Intellij, but I don't know whether these are compatible with the latest versions (the link to the Intellij jar is mostly broken anyway, requiring a few extra clicks). I'll open a Jira / PR to update the docs."", 'commenter': 'beobal'}, {'comment': ""Oh ok, by some reason I did not see the Intellij config gist. I'll definitely give it a try. Thanks for referencing!"", 'commenter': 'azotcsit'}]"
1208,test/unit/org/apache/cassandra/db/virtual/CredentialsCacheKeysTableTest.java,"@@ -0,0 +1,180 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import com.google.common.collect.ImmutableList;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.EndPoint;
+import com.datastax.driver.core.PlainTextAuthProvider;
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.auth.AuthTestUtils;
+import org.apache.cassandra.auth.AuthenticatedUser;
+import org.apache.cassandra.auth.IAuthenticator;
+import org.apache.cassandra.auth.RoleResource;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql3.CQLTester;
+import org.apache.cassandra.cql3.UntypedResultSet;
+
+import static org.apache.cassandra.auth.AuthTestUtils.ROLE_A;
+import static org.apache.cassandra.auth.AuthTestUtils.ROLE_B;
+
+public class CredentialsCacheKeysTableTest extends CQLTester
+{
+    private static final String KS_NAME = ""vts"";
+    private static AuthTestUtils.LocalPasswordAuthenticator passwordAuthenticator;
+
+    @SuppressWarnings(""FieldCanBeLocal"")
+    private CredentialsCacheKeysTable table;
+
+    @BeforeClass
+    public static void setUpClass()
+    {
+        // high value is used for convenient debugging
+        DatabaseDescriptor.setPermissionsValidity(20_000);
+
+        CQLTester.setUpClass();
+        AuthTestUtils.LocalCassandraRoleManager roleManager = new AuthTestUtils.LocalCassandraRoleManager();
+        passwordAuthenticator = new AuthTestUtils.LocalPasswordAuthenticator();
+        SchemaLoader.setupAuth(roleManager,
+                passwordAuthenticator,
+                new AuthTestUtils.LocalCassandraAuthorizer(),
+                new AuthTestUtils.LocalCassandraNetworkAuthorizer());
+
+        roleManager.createRole(AuthenticatedUser.SYSTEM_USER, ROLE_A, AuthTestUtils.getLoginRoleOprions());","[{'comment': 'typo here, which is common to all the new `*CacheKeysTableTest` and causes compilation failures. ', 'commenter': 'beobal'}, {'comment': 'Oh ok, it is a rebase issue. After rebasing onto the latest trunk the method name got fixed (CASSANDRA-16926), but since there was no conflict here, I missed it. Fixed everywhere.', 'commenter': 'azotcsit'}]"
1208,src/java/org/apache/cassandra/auth/PasswordAuthenticator.java,"@@ -78,6 +78,10 @@ public boolean requireAuthentication()
         return true;
     }
 
+    public CredentialsCache getCredentialsCache() {","[{'comment': 'nit: The brace should be on the next line ', 'commenter': 'blerer'}, {'comment': 'Fixed.', 'commenter': 'azotcsit'}]"
1208,src/java/org/apache/cassandra/db/virtual/CredentialsCacheKeysTable.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db.virtual;
+
+import org.apache.cassandra.auth.IAuthenticator;
+import org.apache.cassandra.auth.PasswordAuthenticator;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.schema.TableMetadata;
+
+final class CredentialsCacheKeysTable extends AbstractMutableVirtualTable
+{
+    private static final String ROLE = ""role"";
+
+    CredentialsCacheKeysTable(String keyspace)
+    {
+        super(TableMetadata.builder(keyspace, ""credentials_cache_keys"")
+                .comment(""keys in the credentials cache"")
+                .kind(TableMetadata.Kind.VIRTUAL)
+                .partitioner(new LocalPartitioner(UTF8Type.instance))
+                .addPartitionKeyColumn(ROLE, UTF8Type.instance)
+                .build());
+    }
+
+    public DataSet data()
+    {
+        SimpleDataSet result = new SimpleDataSet(metadata());
+
+        IAuthenticator authenticator = DatabaseDescriptor.getAuthenticator();
+        if (authenticator instanceof PasswordAuthenticator)
+            ((PasswordAuthenticator) authenticator).getCredentialsCache().getAll()
+                .forEach((roleName, ignored) -> result.row(roleName));","[{'comment': 'The logic to retrieve the credential cache could be extracted. Moreover, if I am not mistaken the IAuthenticator cannot be changed without restarting the server so we could retrieve the Credential cache only once in the constructor. ', 'commenter': 'blerer'}, {'comment': ""I double checked whether `IAuthenticator` is not supposed to be changed in runtime - that's correct. So I started setting up it in constructor."", 'commenter': 'azotcsit'}]"
1208,test/unit/org/apache/cassandra/service/ClientStateTest.java,"@@ -64,7 +62,7 @@ public static void afterClass()
     }
 
     @Test
-    public void permissionsCheckStartsAtHeadOfResourceChain() throws Exception {
+    public void permissionsCheckStartsAtHeadOfResourceChain() {","[{'comment': 'nit: The brace should be on the next line.', 'commenter': 'blerer'}, {'comment': 'Fixed.', 'commenter': 'azotcsit'}]"
1208,test/unit/org/apache/cassandra/db/virtual/PermissionsCacheKeysTableTest.java,"@@ -0,0 +1,196 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Set;
+
+import com.google.common.collect.ImmutableList;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.auth.AuthTestUtils;
+import org.apache.cassandra.auth.AuthenticatedUser;
+import org.apache.cassandra.auth.DataResource;
+import org.apache.cassandra.auth.IResource;
+import org.apache.cassandra.auth.Permission;
+import org.apache.cassandra.auth.RoleResource;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql3.CQLTester;
+import org.apache.cassandra.cql3.UntypedResultSet;
+
+import static org.apache.cassandra.auth.AuthTestUtils.ROLE_A;
+import static org.apache.cassandra.auth.AuthTestUtils.ROLE_B;
+
+public class PermissionsCacheKeysTableTest extends CQLTester
+{
+    private static final String KS_NAME = ""vts"";
+
+    @SuppressWarnings(""FieldCanBeLocal"")
+    private PermissionsCacheKeysTable table;
+
+    @BeforeClass
+    public static void setUpClass()
+    {
+        // high value is used for convenient debugging
+        DatabaseDescriptor.setPermissionsValidity(20_000);
+
+        CQLTester.setUpClass();
+        AuthTestUtils.LocalCassandraRoleManager roleManager = new AuthTestUtils.LocalCassandraRoleManager();","[{'comment': '`CQLTester.requireAuthentication` can be used to setup the authentication and automatically create the super user. After that it is possible to use CQL queries to create roles and grant/revoke permissions', 'commenter': 'blerer'}, {'comment': ""I changed tests to use `CQLTester.requireAuthentication` for local auth setup. However, I did not migrate them to use `useSuperUser`/`useUser` because these methods cannot be referred from static `@BeforeClass`. Theoretically I could put them into `@Before/@After` but I felt it is an overhead (not a problem though) to re-create resources for every test while it could be done in static `@BeforeClass` once and for all.\r\n\r\nAnother advantage in the current implementation is that it is possible to easily use `IResource.applicablePermissions`. For CQL version it would require more code to construct statements and look cumbersome.\r\n\r\nPlease, let me know your thoughts. If you believe that CQL is a better way, I'd like to understand the advantages. The only I can see is less dependencies on internal Java implementation (no need to changes tests if some Java classes are changed)."", 'commenter': 'azotcsit'}]"
1208,test/unit/org/apache/cassandra/db/virtual/CredentialsCacheKeysTableTest.java,"@@ -0,0 +1,180 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import com.google.common.collect.ImmutableList;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.EndPoint;
+import com.datastax.driver.core.PlainTextAuthProvider;
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.auth.AuthTestUtils;
+import org.apache.cassandra.auth.AuthenticatedUser;
+import org.apache.cassandra.auth.IAuthenticator;
+import org.apache.cassandra.auth.RoleResource;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql3.CQLTester;
+import org.apache.cassandra.cql3.UntypedResultSet;
+
+import static org.apache.cassandra.auth.AuthTestUtils.ROLE_A;
+import static org.apache.cassandra.auth.AuthTestUtils.ROLE_B;
+
+public class CredentialsCacheKeysTableTest extends CQLTester
+{
+    private static final String KS_NAME = ""vts"";
+    private static AuthTestUtils.LocalPasswordAuthenticator passwordAuthenticator;
+
+    @SuppressWarnings(""FieldCanBeLocal"")
+    private CredentialsCacheKeysTable table;
+
+    @BeforeClass
+    public static void setUpClass()
+    {
+        // high value is used for convenient debugging
+        DatabaseDescriptor.setPermissionsValidity(20_000);
+
+        CQLTester.setUpClass();
+        AuthTestUtils.LocalCassandraRoleManager roleManager = new AuthTestUtils.LocalCassandraRoleManager();
+        passwordAuthenticator = new AuthTestUtils.LocalPasswordAuthenticator();
+        SchemaLoader.setupAuth(roleManager,
+                               passwordAuthenticator,
+                               new AuthTestUtils.LocalCassandraAuthorizer(),
+                               new AuthTestUtils.LocalCassandraNetworkAuthorizer());
+
+        roleManager.createRole(AuthenticatedUser.SYSTEM_USER, ROLE_A, AuthTestUtils.getLoginRoleOptions());
+        roleManager.createRole(AuthenticatedUser.SYSTEM_USER, ROLE_B, AuthTestUtils.getLoginRoleOptions());
+    }
+
+    @Before
+    public void config()
+    {
+        table = new CredentialsCacheKeysTable(KS_NAME);
+        VirtualKeyspaceRegistry.instance.register(new VirtualKeyspace(KS_NAME, ImmutableList.of(table)));
+
+        // ensure nothing keeps cached between tests
+        passwordAuthenticator.getCredentialsCache().invalidate();
+    }
+
+    @AfterClass
+    public static void tearDownClass()
+    {
+        DatabaseDescriptor.setPermissionsValidity(DatabaseDescriptor.getRawConfig().permissions_validity_in_ms);
+    }
+
+    @Test
+    public void testSelectAllWhenPermissionsAreNotCached() throws Throwable
+    {
+        UntypedResultSet result = execute(""SELECT * FROM vts.credentials_cache_keys"");","[{'comment': 'The practice within the C* unit test is to inline the execute within the assertion. It is often easier to read to see the query next to the results. ', 'commenter': 'blerer'}, {'comment': 'Done for all tests.', 'commenter': 'azotcsit'}]"
1249,bin/cqlsh.py,"@@ -93,11 +93,6 @@
 # is a ../lib dir, use bundled libs there preferentially.
 ZIPLIB_DIRS = [os.path.join(CASSANDRA_PATH, 'lib')]
 myplatform = platform.system()","[{'comment': ""After removing the `is_win` part, the `myplatform` variable is only being used at `if myplatform == 'Linux':` below. It makes more sense to remove the global variable and change the line to `if platform.system() == 'Linux':`."", 'commenter': 'bowensong'}]"
1249,bin/cqlsh.py,"@@ -1894,7 +1878,7 @@ def do_clear(self, parsed):
         Clears the console.
         """"""
         import subprocess
-        subprocess.call(['clear', 'cls'][is_win], shell=True)
+        subprocess.call(['clear', 'cls'][False], shell=True)","[{'comment': ""That new code is equivalent to `subprocess.call('clear', shell=True)`, but less readable."", 'commenter': 'bowensong'}]"
1249,pylib/cqlshlib/test/run_cqlsh.py,"@@ -126,21 +118,12 @@ def timing_out_alarm(seconds):
         finally:
             signal.alarm(0)
 
-if is_win():
-    try:
-        import eventlet
-    except ImportError as e:
-        sys.exit(""evenlet library required to run cqlshlib tests on Windows"")
-
-    def timing_out(seconds):
-        return eventlet.Timeout(seconds, TimeoutError)
+# setitimer is new in 2.6, but it's still worth supporting, for potentially
+# faster tests because of sub-second resolution on timeouts.
+if hasattr(signal, 'setitimer'):","[{'comment': '`signal.setitimer()` is available on all Python versions (2.7 and 3) currently supported by cqlsh. The `if hasattr(...)` is redundant.', 'commenter': 'bowensong'}]"
1249,pylib/cqlshlib/test/run_cqlsh.py,"@@ -297,11 +277,6 @@ def __init__(self, path=None, host=None, port=None, keyspace=None, cqlver=None,
             args += ('--cqlversion', str(cqlver))
         if keyspace is not None:
             args += ('--keyspace', keyspace.lower())
-        if tty and is_win():
-            args += ('--tty',)
-            args += ('--encoding', 'utf-8')
-            if win_force_colors:","[{'comment': 'After removing this line, the `win_force_colors` parameter in the `__init__()` method is unused and should be removed too. The `test_no_color_output` test in the test_cqlsh_output.py file needs to be updated or removed.', 'commenter': 'bowensong'}]"
1249,pylib/cqlshlib/test/run_cqlsh.py,"@@ -30,15 +30,8 @@
 from os.path import join, normpath
 
 
-def is_win():
-    return sys.platform in (""cygwin"", ""win32"")
-
-if is_win():
-    from .winpty import WinPty","[{'comment': '`WinPty` is still being referenced in the `ProcRunner.start_proc()` method. Please double check and make sure all references to the `WinPty` module has been taken care of.', 'commenter': 'bowensong'}]"
1249,src/java/org/apache/cassandra/db/lifecycle/LogTransaction.java,"@@ -113,7 +112,6 @@ public CorruptTransactionLogException(String message, LogFile txnFile)
     private final Object lock;
     private final Ref<LogTransaction> selfRef;
     // Deleting sstables is tricky because the mmapping might not have been finalized yet,","[{'comment': 'nit: terminate with period.', 'commenter': 'jmckenzie-dev'}]"
1249,src/java/org/apache/cassandra/utils/NativeLibrary.java,"@@ -143,10 +140,8 @@ private static OSType getOsType()
             return LINUX;
         else if (osName.contains(""mac""))
             return MAC;
-        else if (osName.contains(""windows""))
-            return WINDOWS;
 
-        logger.warn(""the current operating system, {}, is unsupported by cassandra"", osName);
+        logger.warn(""the current operating system, {}, is unsupported by Cassandra"", osName);","[{'comment': 'Have we checked to confirm that, on windows under WSL, osType comes up as LINUX?', 'commenter': 'jmckenzie-dev'}, {'comment': 'I am not sure, I guess I have to check.', 'commenter': 'smiklosovic'}, {'comment': 'based on the information from https://stackoverflow.com/questions/66736736/what-is-the-os-name-system-property-value-in-java-on-windows-subsystem-for-linux I\'m pretty sure the `osName` is going to be `""linux""`. Some other StackOverflow posts also supports that. For example: https://stackoverflow.com/questions/68228844/maven-build-failed-due-to-permission-denied-exception-during-test-phase', 'commenter': 'bowensong'}]"
1249,doc/modules/cassandra/pages/troubleshooting/index.adoc,"@@ -8,10 +8,7 @@ isolate the problem using logs and tools. Luckily Cassandra had a great
 set of instrospection tools to help you.
 
 These pages include a number of command examples demonstrating various
-debugging and analysis techniques, mostly for Linux/Unix systems. If you
-don't have access to the machines running Cassandra, or are running on","[{'comment': 'Would keep the bit about ""If you don\'t have access to the machines running C*"" etc etc', 'commenter': 'jmckenzie-dev'}]"
1249,pylib/cqlshlib/test/run_cqlsh.py,"@@ -111,36 +103,6 @@ def timing_out_itimer(seconds):
         finally:
             signal.setitimer(signal.ITIMER_REAL, 0)
 
-@contextlib.contextmanager
-def timing_out_alarm(seconds):","[{'comment': 'Why did we remove the itimer post 2.6 stuff here?', 'commenter': 'jmckenzie-dev'}, {'comment': '@bowensong was this Windows specific?', 'commenter': 'smiklosovic'}, {'comment': ""@josh-mckenzie @smiklosovic\r\n\r\nNo, the removed method is not Windows specific.\r\n\r\nThe reason for removal:\r\n1. the only code referenced to this method was under the `if is_win(): ... else: ...` branch below, between line 129 and 143. That chunk of code is removed because they are Windows specific.\r\n2. Python 2.6 support was ended in 2013, nearly 9 years ago. This code has been practically sitting in an unreachable branch since then.\r\n3. the comment says it was kept for potentially faster tests, but the there's no test using it.\r\n\r\nAny reason to keep the unused and unreachable code in the there? I can add it back if this is a blocker."", 'commenter': 'bowensong'}, {'comment': 'thanks @bowensong , I think that the removal is justifiable.', 'commenter': 'smiklosovic'}]"
1249,pylib/cqlshlib/test/test_cqlsh_output.py,"@@ -587,10 +584,7 @@ def test_prompt(self):
             c.send('use NONEXISTENTKEYSPACE;\n')
             outputlines = c.read_to_next_prompt().splitlines()
 
-            start_index = 0
-            if c.realtty:
-                self.assertEqual(outputlines[start_index], 'use NONEXISTENTKEYSPACE;')","[{'comment': 'Why did we remove the assertion check for index 0?', 'commenter': 'jmckenzie-dev'}, {'comment': 'moved back.', 'commenter': 'smiklosovic'}]"
1249,pylib/cqlshlib/copyutil.py,"@@ -884,7 +884,7 @@ def __init__(self, fname, options):
         self.max_rows = options.copy['maxrows']
         self.skip_rows = options.copy['skiprows']
         self.fname = fname
-        self.sources = None  # must be created later due to pickle problems on Windows
+        self.sources = None","[{'comment': ""Without Windows' issues, should we instead initialize this at declaration time now?"", 'commenter': 'jmckenzie-dev'}, {'comment': 'looking into this, this one seems to be easy to move to constructor.', 'commenter': 'smiklosovic'}]"
1249,pylib/cqlshlib/copyutil.py,"@@ -1299,9 +1299,9 @@ def __init__(self, inpipe, outpipe, worker_pipes, fname, options):
         self.inpipe = inpipe
         self.outpipe = outpipe
         self.worker_pipes = worker_pipes
-        self.inmsg = None  # must be created after forking on Windows
-        self.outmsg = None  # must be created after forking on Windows
-        self.worker_channels = None  # must be created after forking on Windows
+        self.inmsg = None","[{'comment': ""Same here: without Windows' issues, should we instead initialize these at declaration time now?"", 'commenter': 'jmckenzie-dev'}]"
1249,pylib/cqlshlib/copyutil.py,"@@ -1405,8 +1405,8 @@ def __init__(self, params, target):
         super(ChildProcess, self).__init__(target=target)
         self.inpipe = params['inpipe']
         self.outpipe = params['outpipe']
-        self.inmsg = None  # must be initialized after fork on Windows
-        self.outmsg = None  # must be initialized after fork on Windows
+        self.inmsg = None","[{'comment': 'See above ☝️ . No reason not to init at decl?', 'commenter': 'jmckenzie-dev'}, {'comment': 'I am not sure about this, I admit I know 0 about threading in Python. But by mere looking at it, I do not think this is a big deal anyway. Maybe @bowensong  may shed some light on this? But I would personaly not touch it and just moved along.', 'commenter': 'smiklosovic'}, {'comment': ""I believe that we should move the code from `on_fork()` to `__init__()`, and may also remove the `on_fork()` method if it's no longer used.\r\n\r\nThe problem with Windows and Python `multiprocessing` is that Windows doesn't support `fork()`, therefore Python implemented a workaround. On Windows, Python `multiprocessing` library uses `pickle` to serialise everything in memory, spawn a new process, and then restores the memory content from the serialised data. The `ReceivingChannel` and `SendingChannel` objects are not serialisable because they have file descriptors (which I believe it's called a file handle on Windows) in them, therefore the code has to handle them after the fake `fork()`.\r\n\r\nHowever, I'm concerned that moving the code from `on_fork()` to `__init__()` may have other unintended side effects. For example, the file descriptors (FDs) will be opened before the fork, in some edge cases the fork may never happen (e.g.: an exception raised in or just after the init, but before the fork). Where's the code responsible for closing the FDs on the parent process side? Will this cause any FD leak? This clearly requires further work to find out.\r\n\r\nTo be honest, I don't think removing the comments without addressing the above is a wise move. Future developers wouldn't have the opportunity to understand why the code is written in this way if the comment is removed."", 'commenter': 'bowensong'}, {'comment': 'hi @bowensong , thanks for the very detailed answer. I can revert the removal of the comment or rather change it in such a way it would mention a ticket the work (or rather investigation into this matter) would be addressed in. I honestly think the refactoring of this would prolong / delay the merging of this branch unncessarilly.', 'commenter': 'smiklosovic'}, {'comment': ""@smiklosovic I agree. Refactoring this shouldn't be a part of this as it will unnecessarily prolong / delay this PR. The work is ideally done in a separate PR. I personally would prefer a comment added here and make it clear why the code was written in this way, and link to a JIRA ticket for refactoring this. Or, at minimal, a comment saying the code was written in this way for Windows `fork()` compatibility and is no longer necessary."", 'commenter': 'bowensong'}]"
1249,test/conf/cdc.yaml,"@@ -1,4 +1 @@
 cdc_enabled: true
-# Compression enabled since uncompressed + cdc isn't compatible w/Windows
-commitlog_compression:","[{'comment': ""What do we think is the most common config? I'd think compression on and probably lz4 because it's legit. If so, we should test cdc w/the expected common parameters. i.e. I'd leave this here on that grounds."", 'commenter': 'jmckenzie-dev'}, {'comment': '""default"" in cassandra.yaml is uncompressed. I doubt people are using compressed commit logs more often as it is not default.', 'commenter': 'smiklosovic'}]"
1249,test/unit/org/apache/cassandra/io/sstable/SSTableLoaderTest.java,"@@ -101,6 +101,8 @@ public void cleanup()
               We force a GC here to force buffer deallocation, and then try deleting the directory again.
               For more information, see: http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4715154
               If this is not the problem, the exception will be rethrown anyway.
+","[{'comment': 'I\'d massage this message a bit to remove the ""Windows does not allow"" etc. etc. completely.', 'commenter': 'jmckenzie-dev'}]"
1277,src/java/org/apache/cassandra/config/Schema.java,"@@ -17,16 +17,26 @@
  */
 package org.apache.cassandra.config;
 
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.Set;
+import java.util.UUID;","[{'comment': ""nit: If we're not actually making any functional changes to `Schema`, might as well leave this file alone entirely."", 'commenter': 'maedhroz'}]"
1277,test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java,"@@ -61,6 +66,7 @@
 import org.apache.cassandra.utils.ByteBufferUtil;
 import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.utils.Pair;
+import org.jboss.byteman.contrib.bmunit.BMRule;
 ","[{'comment': '@bereng I think we need to use `@RunWith(BMUnitRunner.class)` in this class for the rule to actually be installed. When I tried to run `testNoVisiblePartialSchemaUpdates()` without it, the sleep never happened.', 'commenter': 'maedhroz'}, {'comment': 'Ah I removed it by accident somehow apologies.', 'commenter': 'bereng'}]"
1277,test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java,"@@ -101,8 +107,68 @@ public static void defineSchema() throws ConfigurationException
     @Test
     public void testSchemaPullSynchoricity() throws Exception
     {
-        Method method = SchemaKeyspace.class.getDeclaredMethod(""convertSchemaToMutations"");
+        for (String methodName : Arrays.asList(""convertSchemaToMutations"",
+                                               ""truncate"",
+                                               ""saveSystemKeyspacesSchema""))
+        {
+            Method method = SchemaKeyspace.class.getDeclaredMethod(methodName);
+            assertTrue(methodName + "" is not thread-safe"", Modifier.isSynchronized(method.getModifiers()));
+        }
+
+        Method method = SchemaKeyspace.class.getDeclaredMethod(""calculateSchemaDigest"", Set.class);
+        assertTrue(Modifier.isSynchronized(method.getModifiers()));
+        method = SchemaKeyspace.class.getDeclaredMethod(""mergeSchemaAndAnnounceVersion"", Collection.class);
+        assertTrue(Modifier.isSynchronized(method.getModifiers()));
+        method = SchemaKeyspace.class.getDeclaredMethod(""mergeSchema"", Collection.class);
         assertTrue(Modifier.isSynchronized(method.getModifiers()));
+        method = SchemaKeyspace.class.getDeclaredMethod(""mergeSchema"", Keyspaces.class, Keyspaces.class);
+        assertTrue(Modifier.isSynchronized(method.getModifiers()));        
+    }
+
+    /** See CASSANDRA-16856/16996. Make sure schema pulls are synchronized to prevent concurrent schema pull/writes */
+    @Test
+    @BMRule(name = ""delay partition updates to schema tables"",
+            targetClass = ""ColumnFamilyStore"",
+            targetMethod = ""apply"",
+            action = ""Thread.sleep(5000);"",","[{'comment': 'If we make sure to use `BMUnitRunner.class` (see above), the sleep could probably be lowered again to what it is in 4.0/trunk.', 'commenter': 'maedhroz'}]"
1277,test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java,"@@ -101,8 +107,68 @@ public static void defineSchema() throws ConfigurationException
     @Test
     public void testSchemaPullSynchoricity() throws Exception
     {
-        Method method = SchemaKeyspace.class.getDeclaredMethod(""convertSchemaToMutations"");
+        for (String methodName : Arrays.asList(""convertSchemaToMutations"",
+                                               ""truncate"",
+                                               ""saveSystemKeyspacesSchema""))
+        {
+            Method method = SchemaKeyspace.class.getDeclaredMethod(methodName);
+            assertTrue(methodName + "" is not thread-safe"", Modifier.isSynchronized(method.getModifiers()));
+        }
+
+        Method method = SchemaKeyspace.class.getDeclaredMethod(""calculateSchemaDigest"", Set.class);
+        assertTrue(Modifier.isSynchronized(method.getModifiers()));
+        method = SchemaKeyspace.class.getDeclaredMethod(""mergeSchemaAndAnnounceVersion"", Collection.class);
+        assertTrue(Modifier.isSynchronized(method.getModifiers()));
+        method = SchemaKeyspace.class.getDeclaredMethod(""mergeSchema"", Collection.class);
         assertTrue(Modifier.isSynchronized(method.getModifiers()));
+        method = SchemaKeyspace.class.getDeclaredMethod(""mergeSchema"", Keyspaces.class, Keyspaces.class);
+        assertTrue(Modifier.isSynchronized(method.getModifiers()));        
+    }
+
+    /** See CASSANDRA-16856/16996. Make sure schema pulls are synchronized to prevent concurrent schema pull/writes */
+    @Test
+    @BMRule(name = ""delay partition updates to schema tables"",
+            targetClass = ""ColumnFamilyStore"",
+            targetMethod = ""apply"",
+            action = ""Thread.sleep(5000);"",
+            targetLocation = ""AT EXIT"")
+    public void testNoVisiblePartialSchemaUpdates() throws Exception
+    {
+        String keyspace = ""sandbox"";
+        ExecutorService pool = Executors.newFixedThreadPool(2);
+
+        SchemaKeyspace.truncate(); // Make sure there's nothing but the create we're about to do
+        CyclicBarrier barrier = new CyclicBarrier(2);
+
+        Future<Void> creation = pool.submit(() -> {
+            barrier.await();
+            createTable(keyspace, ""CREATE TABLE test (a text primary key, b int, c int)"");
+            return null;
+        });
+
+        Future<Collection<Mutation>> mutationsFromThread = pool.submit(() -> {
+            barrier.await();","[{'comment': ""The one way this test can still end up not testing what we want it to test is if the create gets delayed entirely by the time this runs.\r\n\r\nIf you change it to...\r\n\r\n```\r\nFuture<Collection<Mutation>> mutationsFromThread = pool.submit(() -> {\r\n    barrier.await();\r\n\r\n    Collection<Mutation> mutations = SchemaKeyspace.convertSchemaToMutations();\r\n    \r\n    // Make sure we actually have a mutation to check for partial modification.\r\n    while (mutations.size() == 0)\r\n        mutations = SchemaKeyspace.convertSchemaToMutations();\r\n    \r\n    return mutations;\r\n});\r\n```\r\n...it works and doesn't require any new sleeps. I'd probably change 4.0 and trunk to do that as well, just to make sure they actually hit the scenario we want. (Sorry, a bit of an oversight in my original test sketch.)"", 'commenter': 'maedhroz'}, {'comment': 'Which is what I was trying to explain about my test being deterministic and yours not. Either we added the tableId check to mine or added determinism to yours. Now I am happy with this test :-)', 'commenter': 'bereng'}]"
1278,src/java/org/apache/cassandra/schema/SchemaKeyspaceTables.java,"@@ -0,0 +1,52 @@
+","[{'comment': 'nit: random newline?', 'commenter': 'maedhroz'}]"
1278,src/java/org/apache/cassandra/schema/SchemaKeyspaceTables.java,"@@ -0,0 +1,52 @@
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.schema;
+
+import com.google.common.collect.ImmutableList;
+
+public class SchemaKeyspaceTables
+{
+    public static final String KEYSPACES = ""keyspaces"";
+    public static final String TABLES = ""tables"";
+    public static final String COLUMNS = ""columns"";
+    public static final String DROPPED_COLUMNS = ""dropped_columns"";
+    public static final String TRIGGERS = ""triggers"";
+    public static final String VIEWS = ""views"";
+    public static final String TYPES = ""types"";
+    public static final String FUNCTIONS = ""functions"";
+    public static final String AGGREGATES = ""aggregates"";
+    public static final String INDEXES = ""indexes"";
+ 
+    /**
+     * The order in this list matters.
+     *
+     * When flushing schema tables, we want to flush them in a way that mitigates the effects of an abrupt shutdown whilst
+     * the tables are being flushed. On startup, we load the schema from disk before replaying the CL, so we need to
+     * try to avoid problems like reading a table without columns or types, for example. So columns and types should be
+     * flushed before tables, which should be flushed before keyspaces.
+     *
+     * When truncating, the order should be reversed. For immutable lists this is an efficient operation that simply
+     * iterates in reverse order.
+     *
+     * See CASSANDRA-12213 for more details.
+     */
+    public static final ImmutableList<String> ALL =
+        ImmutableList.of(COLUMNS, DROPPED_COLUMNS, TRIGGERS, TYPES, FUNCTIONS, AGGREGATES, INDEXES, TABLES, VIEWS, KEYSPACES);","[{'comment': ""nit: seems like this has a newline for every table name in 4.0...don't really care which format we go with, but probably good to be consistent"", 'commenter': 'maedhroz'}, {'comment': 'It has to be the other one bc of long lines and the formatting rules. Solved.', 'commenter': 'bereng'}]"
1278,src/java/org/apache/cassandra/schema/SchemaKeyspace.java,"@@ -70,41 +75,14 @@ private SchemaKeyspace()
     private static final boolean FLUSH_SCHEMA_TABLES = Boolean.parseBoolean(System.getProperty(""cassandra.test.flush_local_schema_changes"", ""true""));
     private static final boolean IGNORE_CORRUPTED_SCHEMA_TABLES = Boolean.parseBoolean(System.getProperty(""cassandra.ignore_corrupted_schema_tables"", ""false""));
 
-    public static final String KEYSPACES = ""keyspaces"";
-    public static final String TABLES = ""tables"";
-    public static final String COLUMNS = ""columns"";
-    public static final String DROPPED_COLUMNS = ""dropped_columns"";
-    public static final String TRIGGERS = ""triggers"";
-    public static final String VIEWS = ""views"";
-    public static final String TYPES = ""types"";
-    public static final String FUNCTIONS = ""functions"";
-    public static final String AGGREGATES = ""aggregates"";
-    public static final String INDEXES = ""indexes"";
-
-    /**
-     * The order in this list matters.
-     *
-     * When flushing schema tables, we want to flush them in a way that mitigates the effects of an abrupt shutdown whilst
-     * the tables are being flushed. On startup, we load the schema from disk before replaying the CL, so we need to
-     * try to avoid problems like reading a table without columns or types, for example. So columns and types should be
-     * flushed before tables, which should be flushed before keyspaces.
-     *
-     * When truncating, the order should be reversed. For immutable lists this is an efficient operation that simply
-     * iterates in reverse order.
-     *
-     * See CASSANDRA-12213 for more details.
-     */
-    public static final ImmutableList<String> ALL =
-        ImmutableList.of(COLUMNS, DROPPED_COLUMNS, TRIGGERS, TYPES, FUNCTIONS, AGGREGATES, INDEXES, TABLES, VIEWS, KEYSPACES);
-
     /**
      * The tables to which we added the cdc column. This is used in {@link #makeUpdateForSchema} below to make sure we skip that
      * column is cdc is disabled as the columns breaks pre-cdc to post-cdc upgrades (typically, 3.0 -> 3.X).
      */
-    private static final Set<String> TABLES_WITH_CDC_ADDED = ImmutableSet.of(TABLES, VIEWS);
+    private static final Set<String> TABLES_WITH_CDC_ADDED = ImmutableSet.of(SchemaKeyspaceTables.TABLES, SchemaKeyspaceTables.VIEWS);
 
     private static final TableMetadata Keyspaces =
-        parse(KEYSPACES,
+        parse(SchemaKeyspaceTables.KEYSPACES,","[{'comment': ""@bereng Seems like in the 4.0 PR, we added a static import for the constants in `SchemaKeyspaceTables` to cut down on the size of the diff. Don't care a ton about which way we go, but let's be consistent."", 'commenter': 'maedhroz'}]"
1278,test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java,"@@ -63,16 +69,71 @@ public static void defineSchema() throws ConfigurationException
                                     SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1));
     }
 
-    /** See CASSANDRA-16856. Make sure schema pulls are synchronized to prevent concurrent schema pull/writes
-    *
-    * @throws Exception
-    */
-   @Test
-   public void testSchemaPullSynchoricity() throws Exception
-   {
-       Method method = SchemaKeyspace.class.getDeclaredMethod(""convertSchemaToMutations"");
-       assertTrue(Modifier.isSynchronized(method.getModifiers()));
-   }
+    /** See CASSANDRA-16856/16996. Make sure schema pulls are synchronized to prevent concurrent schema pull/writes */
+    @Test
+    public void testSchemaPullSynchronicity() throws Exception
+    {
+        for (String methodName : Arrays.asList(""schemaKeyspaceAsMutations"",
+                                               ""truncateSchemaKeyspace"",
+                                               ""saveSystemKeyspace"",
+                                               ""updateVersion""))
+        {
+            Method method = Schema.class.getDeclaredMethod(methodName);
+            assertTrue(Modifier.isSynchronized(method.getModifiers()));
+        }
+
+        Method method = Schema.class.getDeclaredMethod(""merge"", Collection.class);
+        assertTrue(Modifier.isSynchronized(method.getModifiers()));
+        method = Schema.class.getDeclaredMethod(""transform"", SchemaTransformation.class, boolean.class, long.class);
+        assertTrue(Modifier.isSynchronized(method.getModifiers()));
+    }
+
+    /** See CASSANDRA-16856/16996. Make sure schema pulls are synchronized to prevent concurrent schema pull/writes */
+    @Test
+    public void testNoVisiblePartialSchemaUpdates() throws Exception","[{'comment': ""@bereng So we still need the Byteman rule here, because otherwise there's nothing stopping the table creation from finishing completely before we call `Schema.instance.schemaKeyspaceAsMutations()`. In other words, we still need some arbitrarily large wait on `CFS#apply()` to make sure the second thread will see the mutation partially applied."", 'commenter': 'maedhroz'}]"
1311,src/java/org/apache/cassandra/config/Config.java,"@@ -276,6 +276,9 @@
 
     // Change-data-capture logs
     public boolean cdc_enabled = false;
+    // When true, new CDC mutations are rejected/blocked when reaching max CDC storage.
+    // When false, new CDC mutations can always be added. But it will remove the oldest CDC commit log segment on full.
+    public volatile boolean cdc_block_writes = true;","[{'comment': '(not strongly held opinion) - cdc_block_writes is ambiguous between cdc blocking write ack (like BatchCommitLogService) vs. blocking when space is full. Consider renaming to something like `cdc_block_when_full`', 'commenter': 'jmckenzie-dev'}]"
1311,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -3096,6 +3096,17 @@ public static void setCDCEnabled(boolean cdc_enabled)
         conf.cdc_enabled = cdc_enabled;
     }
 
+    public static boolean shouldCDCBlockWrites()","[{'comment': 'Rename to `getCDCBlockWrites` to be consistent w/the rest of the file (I went through _a lot_ of this on the Denylist patch :) )', 'commenter': 'jmckenzie-dev'}]"
1311,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -3096,6 +3096,17 @@ public static void setCDCEnabled(boolean cdc_enabled)
         conf.cdc_enabled = cdc_enabled;
     }
 
+    public static boolean shouldCDCBlockWrites()
+    {
+        return conf.cdc_block_writes;
+    }
+
+    public static void setCDCBlockWrites(boolean val)
+    {
+        Thread.dumpStack();","[{'comment': ""What's this dumpStack about? Left over from debugging?"", 'commenter': 'jmckenzie-dev'}]"
1311,src/java/org/apache/cassandra/db/commitlog/CommitLog.java,"@@ -420,6 +421,28 @@ public long getActiveOnDiskSize()
         return segmentRatios;
     }
 
+    @Override
+    public boolean getCDCBlockWrites()
+    {
+        return DatabaseDescriptor.shouldCDCBlockWrites();
+    }
+
+    @Override
+    public void setCDCBlockWrites(boolean val)
+    {
+        Preconditions.checkState(DatabaseDescriptor.isCDCEnabled(),
+                                 ""Unable to set block_writes (%s): CDC is not enabled."", val);
+        Preconditions.checkState(segmentManager instanceof CommitLogSegmentManagerCDC,
+                                 ""Unexpected commit log segment manager type: %s"", segmentManager.getClass().getName());","[{'comment': 'Consider making this error log more descriptive. Something like ""Logic error! CDC is enabled but we have the wrong CommitLogSegmentManager type. Please report this as a bug."" + the type.\r\n\r\nWhich is, admittedly, not the way we usually do error messages in our system, but IMO it\'d be much more useful for an operator. If we\'ve reached this point w/a disjoint between config param and the SegmentManager instantiated, something is _very very wrong_', 'commenter': 'jmckenzie-dev'}]"
1311,src/java/org/apache/cassandra/db/commitlog/CommitLogDescriptor.java,"@@ -220,7 +223,20 @@ public String fileName()
 
     public String cdcIndexFileName()
     {
-        return FILENAME_PREFIX + version + SEPARATOR + id + ""_cdc.idx"";
+        return FILENAME_PREFIX + version + SEPARATOR + id + INDEX_FILENAME_SUFFIX;
+    }
+
+    /**
+     * Infer the corresponding cdc index file using its cdc commitlog file
+     * @param cdcFile
+     * @return cdc index file
+     */
+    public static File inferCdcIndexFile(File cdcFile)
+    {
+        Preconditions.checkArgument(isValid(cdcFile.name()), ""Invalid commit log"");","[{'comment': 'It looks like this can throw an exception up stack and leave us with an inability to allocate new commit log files, repeatedly failing if the oldest CommitLogSegment is invalid.', 'commenter': 'jmckenzie-dev'}, {'comment': 'The check should not throw for the current usage in the codebase. The parameter has already been verified at the caller. \r\nSince the method is to infer, I think it would make sense to return `null` when failing to guess.', 'commenter': 'yifan-c'}]"
1311,src/java/org/apache/cassandra/db/commitlog/CommitLogDescriptor.java,"@@ -220,7 +223,20 @@ public String fileName()
 
     public String cdcIndexFileName()
     {
-        return FILENAME_PREFIX + version + SEPARATOR + id + ""_cdc.idx"";
+        return FILENAME_PREFIX + version + SEPARATOR + id + INDEX_FILENAME_SUFFIX;
+    }
+
+    /**
+     * Infer the corresponding cdc index file using its cdc commitlog file
+     * @param cdcFile
+     * @return cdc index file
+     */
+    public static File inferCdcIndexFile(File cdcFile)","[{'comment': 'nit: consider renaming the variable `cdcCommitLogSegment` or something to disambiguate in the body of the method.', 'commenter': 'jmckenzie-dev'}]"
1311,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -78,11 +81,45 @@ public void discard(CommitLogSegment segment, boolean delete)
                 FileUtils.deleteWithConfirm(cdcLink);
 
             File cdcIndexFile = segment.getCDCIndexFile();
-            if (cdcIndexFile.exists())
-                FileUtils.deleteWithConfirm(cdcIndexFile);
+            deleteCDCFiles(cdcLink, cdcIndexFile);
         }
     }
 
+    /**
+     * Delete the oldest CDC commit log segment to free up space.
+     * @return total deleted file size in bytes
+     */
+    public long deleteOldestCommitLogSegment()","[{'comment': 'Strongly recommend renaming this to `deleteOldestLinkedCDCSegmentFile` or something. It took me a double take of being Very Scared of this method before realizing it was deleting the oldest CDC segment and not just wildly deleting old CommitLogSegment files from the system. 😀 ', 'commenter': 'jmckenzie-dev'}, {'comment': '👍 Sure. Let\'s rename the method to add clarity. I probably omitted ""CDC"" for it is in the  `CommitLogSegmentManagerCDC` class. ', 'commenter': 'yifan-c'}]"
1311,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -78,11 +81,45 @@ public void discard(CommitLogSegment segment, boolean delete)
                 FileUtils.deleteWithConfirm(cdcLink);
 
             File cdcIndexFile = segment.getCDCIndexFile();
-            if (cdcIndexFile.exists())
-                FileUtils.deleteWithConfirm(cdcIndexFile);
+            deleteCDCFiles(cdcLink, cdcIndexFile);
         }
     }
 
+    /**
+     * Delete the oldest CDC commit log segment to free up space.
+     * @return total deleted file size in bytes
+     */
+    public long deleteOldestCommitLogSegment()
+    {
+        File cdcDir = new File(DatabaseDescriptor.getCDCLogLocation());
+        Preconditions.checkState(cdcDir.isDirectory(), ""The CDC directory does not exist."");
+        File[] files = cdcDir.tryList(f -> CommitLogDescriptor.isValid(f.name()));
+        Preconditions.checkState(files != null && files.length > 0, ""There should be at least 1 CDC commit log segment."");
+        List<File> sorted = Arrays.stream(files)
+                                  .sorted(Comparator.comparingLong(File::lastModified))
+                                  .collect(Collectors.toList());
+        File oldestCdcFile = sorted.get(0);
+        File cdcIndexFile = CommitLogDescriptor.inferCdcIndexFile(oldestCdcFile);
+        return deleteCDCFiles(oldestCdcFile, cdcIndexFile);
+    }
+
+    private long deleteCDCFiles(File cdcFile, File cdcIndexFile)
+    {
+        long total = 0;
+        if (cdcFile.exists())
+        {
+            total += cdcFile.length();
+            FileUtils.deleteWithConfirm(cdcFile);","[{'comment': ""Looks like this is deprecated as of CEP-10. Is there another call we should use here? (a cursory inspection didn't surface anything for me, but the method is definitely flagged as Deprecated...)"", 'commenter': 'jmckenzie-dev'}, {'comment': '👍 Now that there is a `File` wrapper class, `org.apache.cassandra.io.util.File`, instead of `java.io.File`. I think the intent of the deprecation is to call `delete` on the file wrapper directly.  ', 'commenter': 'yifan-c'}]"
1311,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -193,23 +230,23 @@ public void addCDCSize(long size)
     {
         private final RateLimiter rateLimiter = RateLimiter.create(1000.0 / DatabaseDescriptor.getCDCDiskCheckInterval());
         private ExecutorService cdcSizeCalculationExecutor;
-        private CommitLogSegmentManagerCDC segmentManager;
-
-        // Used instead of size during walk to remove chance of over-allocation
-        private volatile long sizeInProgress = 0;
+        private final CommitLogSegmentManagerCDC segmentManager;
+        // track the total size between two dictionary size calculations
+        private final AtomicLong sizeInProgress;","[{'comment': ""I keep racking my brain trying to remember why I went w/a volatile initially instead of an Atomic, and I can't remember. I think the Atomic's a cleaner API anyway, so :+1: to this change."", 'commenter': 'jmckenzie-dev'}]"
1311,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -250,9 +302,11 @@ void processDiscardedSegment(CommitLogSegment segment)
             {
                 // Add to flushed size before decrementing unflushed so we don't have a window of false generosity
                 if (segment.getCDCState() == CDCState.CONTAINS)
-                    size += segment.onDiskSize();
+                    sizeInProgress.getAndAdd(segment.onDiskSize());
+
+                // Subtract the (estimated) size of the segment from processNewSegment.
                 if (segment.getCDCState() != CDCState.FORBIDDEN)
-                    size -= defaultSegmentSize();
+                    sizeInProgress.getAndAdd(-defaultSegmentSize()); // todo: the defaultSegmentSize can be altered","[{'comment': 'It looks like this alteration only takes place in 4 unit tests at this point. Maybe we should take this opportunity to document the `DatabaseDescriptor.setCommitLogSegmentSize` method as being for testing only and point to its usage in `CommitLogSegmentManagerCDC` via javadoc link for future maintainers that may be thinking about changing that?', 'commenter': 'jmckenzie-dev'}, {'comment': 'Also, why is this `defaultSegmentSize` instead of just querying the `segment.onDiskSize()` like the call above? And yes, I know the simple answer is ""that\'s because that\'s how you originally wrote it Josh"", but I don\'t think this is... consistent?', 'commenter': 'jmckenzie-dev'}, {'comment': 'The reason to subtract `defaultSegmentSize` is to return the estimated segment size it has aggressively counted at `processNewSegment`. \r\nIn `processDiscardedSegment`, it counts the actual `onDiskSize` for the segment that `CONTAINS`. So it returns the estimated size. For the segment that remains in `PERMITTED`, it gets deleted, so it also need to subtract/return.\r\nI will update the comment to make it more clear. ', 'commenter': 'yifan-c'}]"
1311,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -2076,6 +2076,13 @@ public static int getCommitLogSegmentSize()
         return (int) ByteUnit.MEBI_BYTES.toBytes(conf.commitlog_segment_size_in_mb);
     }
 
+    /**
+     * Update commitlog_segment_size_in_mb in the tests.
+     * {@link CommitLogSegmentManagerCDC} uses the CommitLogSegmentSize to estimate the file size on allocation.
+     * It is important to keep the value unchanged for the estimation to be correct.
+     * @param sizeMegabytes
+     */
+    @VisibleForTesting /* Only for testing */","[{'comment': 'Just a thought - what if we added an `@OnlyForTesting` annotation for use in our code-base? I ended up doing this on another project I work on since it further communicates ""Hey, don\'t rely on this for stability or performance"" instead of being a statement about scope.\r\n\r\nCould also pursue that separately from this patch. 🤷 ', 'commenter': 'jmckenzie-dev'}, {'comment': 'yeah. I think we mostly use the annotation `@VisibleForTesting` for the purpose of OnlyForTesting. Since it covers a lot of the methods, creating a dedicated ticket for it feel better. I will make a new one. ', 'commenter': 'yifan-c'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V blockingGet(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.awaitUninterruptibly().getNow();
+        else
+            return null;
+    }
+
+    /**
+     * If the value for the given key is missing, execute a load function to obtain a value and put it into the map.
+     * It is guaranteed that the load function will be executed only once when the key is missing and mulitple threads
+     * called this method for the same key.
+     * <p>
+     * When the mapping function returns {@code null}, {@link NullPointerException} is thrown. When the mapping function
+     * throws exception, it is rethrown by this method. In both cases nothing gets added to the map.
+     */
+    public V blockingLoadIfAbsent(K key, Supplier<? extends V> loadFunction) throws RuntimeException
+    {
+        while (true)
+        {
+            Future<V> future = get(key);
+            AsyncPromise<V> newEntry = null;
+            if (future == null)
+            {
+                newEntry = new AsyncPromise<>();
+                future = putIfAbsent(key, newEntry);
+                if (future == null)
+                {
+                    // We managed to create an entry for the value. Now initialize it.
+                    future = newEntry;
+                    try
+                    {
+                        V v = loadFunction.get();
+                        if (v == null)
+                        {
+                            newEntry.setFailure(new NullPointerException(""The mapping function returned null""));","[{'comment': 'I would just throw here and let the exception handling deal with removal and passing on the failure.', 'commenter': 'blambov'}, {'comment': 'that will be simpler, agreed', 'commenter': 'jacek-lewandowski'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V blockingGet(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.awaitUninterruptibly().getNow();
+        else
+            return null;
+    }
+
+    /**
+     * If the value for the given key is missing, execute a load function to obtain a value and put it into the map.
+     * It is guaranteed that the load function will be executed only once when the key is missing and mulitple threads
+     * called this method for the same key.
+     * <p>
+     * When the mapping function returns {@code null}, {@link NullPointerException} is thrown. When the mapping function
+     * throws exception, it is rethrown by this method. In both cases nothing gets added to the map.
+     */
+    public V blockingLoadIfAbsent(K key, Supplier<? extends V> loadFunction) throws RuntimeException
+    {
+        while (true)
+        {
+            Future<V> future = get(key);
+            AsyncPromise<V> newEntry = null;
+            if (future == null)
+            {
+                newEntry = new AsyncPromise<>();
+                future = putIfAbsent(key, newEntry);
+                if (future == null)
+                {
+                    // We managed to create an entry for the value. Now initialize it.
+                    future = newEntry;
+                    try
+                    {
+                        V v = loadFunction.get();
+                        if (v == null)
+                        {
+                            newEntry.setFailure(new NullPointerException(""The mapping function returned null""));
+                            remove(key, future);
+                        }
+                        else
+                        {
+                            newEntry.setSuccess(v);
+                        }
+                    }
+                    catch (Throwable t)
+                    {
+                        newEntry.setFailure(t);
+                        // Remove future so that construction can be retried later
+                        remove(key, future);
+                    }
+                }
+                else
+                {
+                    newEntry = null;
+                }
+
+                // Else some other thread beat us to it, but we now have the reference to the future which we can wait for.
+            }
+
+            try
+            {
+                future.syncUninterruptibly();
+            }
+            catch (Throwable t)
+            {
+                // if blockingUnloadIfPresent was called in the meantime, we simply retry hoping that unloading gets
+                // finished soon
+                // also we retry if the concurrent attempt to load entry failed (but we do not retry if this attempt
+                // failed)
+                if (newEntry == null || Throwables.isCausedBy(t, ex -> ex instanceof KeyNotFoundException))
+                {
+                    Thread.yield();
+                    continue;
+                }
+            }
+
+            future.rethrowIfFailed();
+            return future.getNow();
+        }
+    }
+
+    /**
+     * If a value for the given key is present, unload function is run and the value is removed from the map.
+     * Similarly to {@link #blockingLoadIfAbsent(Object, Supplier)} at-most-once semantics is guaranteed for unload
+     * function.
+     * <p>
+     * When unload function fails, the value is removed from the map anyway and the failure is rethrown.
+     * <p>
+     * When the key was not found, the method returns {@code null}.
+     *
+     * @throws UnloadExecutionException when the unloading failed to complete - this is checked exception because
+     *                                  the value is removed from the map regardless of the result of unloading; therefore if the unloading failed, the
+     *                                  called is responsible for handling that; the {@link UnloadExecutionException} encapsulates the value which was
+     *                                  failed to unload.
+     */
+    public V blockingUnloadIfPresent(K key, Consumer<? super V> unloadFunction) throws UnloadExecutionException
+    {
+        Promise<V> droppedFuture = new AsyncPromise<V>().setFailure(new KeyNotFoundException());
+
+        Future<V> existingFuture;
+        do
+        {
+            existingFuture = get(key);
+            if (existingFuture == null || existingFuture.cause() != null)
+                return null;
+        } while (!replace(key, existingFuture, droppedFuture));
+
+        V v = existingFuture.awaitUninterruptibly().getNow();
+        if (v == null)
+        {
+            // which means that either the value failed to load or a concurrent attempt to unload already did the work
+            return null;","[{'comment': 'At this point we have replaced the entry with our `droppedFuture`, which will no longer be removed from the map.', 'commenter': 'blambov'}, {'comment': ""yes, thanks for this catch. I don't know how it happened. I must have changes something right before committing because even tests were not passing without fixing this :/"", 'commenter': 'jacek-lewandowski'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V blockingGet(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.awaitUninterruptibly().getNow();
+        else
+            return null;
+    }
+
+    /**
+     * If the value for the given key is missing, execute a load function to obtain a value and put it into the map.
+     * It is guaranteed that the load function will be executed only once when the key is missing and mulitple threads
+     * called this method for the same key.
+     * <p>
+     * When the mapping function returns {@code null}, {@link NullPointerException} is thrown. When the mapping function
+     * throws exception, it is rethrown by this method. In both cases nothing gets added to the map.
+     */
+    public V blockingLoadIfAbsent(K key, Supplier<? extends V> loadFunction) throws RuntimeException
+    {
+        while (true)
+        {
+            Future<V> future = get(key);
+            AsyncPromise<V> newEntry = null;
+            if (future == null)
+            {
+                newEntry = new AsyncPromise<>();
+                future = putIfAbsent(key, newEntry);
+                if (future == null)
+                {
+                    // We managed to create an entry for the value. Now initialize it.
+                    future = newEntry;
+                    try
+                    {
+                        V v = loadFunction.get();
+                        if (v == null)
+                        {
+                            newEntry.setFailure(new NullPointerException(""The mapping function returned null""));
+                            remove(key, future);
+                        }
+                        else
+                        {
+                            newEntry.setSuccess(v);
+                        }
+                    }
+                    catch (Throwable t)
+                    {
+                        newEntry.setFailure(t);
+                        // Remove future so that construction can be retried later
+                        remove(key, future);
+                    }
+                }
+                else
+                {
+                    newEntry = null;
+                }
+
+                // Else some other thread beat us to it, but we now have the reference to the future which we can wait for.
+            }
+
+            try
+            {
+                future.syncUninterruptibly();
+            }
+            catch (Throwable t)
+            {
+                // if blockingUnloadIfPresent was called in the meantime, we simply retry hoping that unloading gets
+                // finished soon
+                // also we retry if the concurrent attempt to load entry failed (but we do not retry if this attempt
+                // failed)
+                if (newEntry == null || Throwables.isCausedBy(t, ex -> ex instanceof KeyNotFoundException))
+                {
+                    Thread.yield();
+                    continue;
+                }
+            }
+
+            future.rethrowIfFailed();
+            return future.getNow();
+        }
+    }
+
+    /**
+     * If a value for the given key is present, unload function is run and the value is removed from the map.
+     * Similarly to {@link #blockingLoadIfAbsent(Object, Supplier)} at-most-once semantics is guaranteed for unload
+     * function.
+     * <p>
+     * When unload function fails, the value is removed from the map anyway and the failure is rethrown.
+     * <p>
+     * When the key was not found, the method returns {@code null}.
+     *
+     * @throws UnloadExecutionException when the unloading failed to complete - this is checked exception because
+     *                                  the value is removed from the map regardless of the result of unloading; therefore if the unloading failed, the
+     *                                  called is responsible for handling that; the {@link UnloadExecutionException} encapsulates the value which was
+     *                                  failed to unload.
+     */
+    public V blockingUnloadIfPresent(K key, Consumer<? super V> unloadFunction) throws UnloadExecutionException
+    {
+        Promise<V> droppedFuture = new AsyncPromise<V>().setFailure(new KeyNotFoundException());
+
+        Future<V> existingFuture;
+        do
+        {
+            existingFuture = get(key);
+            if (existingFuture == null || existingFuture.cause() != null)
+                return null;
+        } while (!replace(key, existingFuture, droppedFuture));
+
+        V v = existingFuture.awaitUninterruptibly().getNow();
+        if (v == null)
+        {
+            // which means that either the value failed to load or a concurrent attempt to unload already did the work
+            return null;
+        }
+
+        try
+        {
+            unloadFunction.accept(v);
+            return v;
+        }
+        catch (Throwable t)
+        {
+            throw new UnloadExecutionException(v, t);
+        }
+        finally
+        {
+            Future<V> future = remove(key);
+            assert future == droppedFuture;
+        }
+    }
+
+    private static class KeyNotFoundException extends IllegalStateException","[{'comment': 'I would name this `ConcurrentUnloadException`. Since you are catching and processing this in the loading method, though, it should not be necessary -- a completed `null` future would work just as well with slightly simpler code.', 'commenter': 'blambov'}, {'comment': 'replacing with null value sounds like a great idea', 'commenter': 'jacek-lewandowski'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V blockingGet(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.awaitUninterruptibly().getNow();
+        else
+            return null;
+    }
+
+    /**
+     * If the value for the given key is missing, execute a load function to obtain a value and put it into the map.
+     * It is guaranteed that the load function will be executed only once when the key is missing and mulitple threads
+     * called this method for the same key.
+     * <p>
+     * When the mapping function returns {@code null}, {@link NullPointerException} is thrown. When the mapping function
+     * throws exception, it is rethrown by this method. In both cases nothing gets added to the map.
+     */
+    public V blockingLoadIfAbsent(K key, Supplier<? extends V> loadFunction) throws RuntimeException
+    {
+        while (true)
+        {
+            Future<V> future = get(key);
+            AsyncPromise<V> newEntry = null;
+            if (future == null)
+            {
+                newEntry = new AsyncPromise<>();
+                future = putIfAbsent(key, newEntry);
+                if (future == null)
+                {
+                    // We managed to create an entry for the value. Now initialize it.
+                    future = newEntry;
+                    try
+                    {
+                        V v = loadFunction.get();
+                        if (v == null)
+                        {
+                            newEntry.setFailure(new NullPointerException(""The mapping function returned null""));
+                            remove(key, future);
+                        }
+                        else
+                        {
+                            newEntry.setSuccess(v);
+                        }
+                    }
+                    catch (Throwable t)
+                    {
+                        newEntry.setFailure(t);
+                        // Remove future so that construction can be retried later
+                        remove(key, future);
+                    }
+                }
+                else
+                {
+                    newEntry = null;
+                }
+
+                // Else some other thread beat us to it, but we now have the reference to the future which we can wait for.
+            }
+
+            try
+            {
+                future.syncUninterruptibly();
+            }
+            catch (Throwable t)
+            {
+                // if blockingUnloadIfPresent was called in the meantime, we simply retry hoping that unloading gets
+                // finished soon
+                // also we retry if the concurrent attempt to load entry failed (but we do not retry if this attempt
+                // failed)
+                if (newEntry == null || Throwables.isCausedBy(t, ex -> ex instanceof KeyNotFoundException))
+                {
+                    Thread.yield();
+                    continue;
+                }
+            }
+
+            future.rethrowIfFailed();","[{'comment': 'Why not do this by calling `throw t` at the end of the `catch` case above?', 'commenter': 'blambov'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>","[{'comment': ""This is not a very good name, especially if what we are doing is adding blocking methods that are the only way this class is meant to be used. It doesn't seem to make much sense to extend from the map either (yes, there's a performance hit, but this is not that hot a path)."", 'commenter': 'blambov'}, {'comment': ""Extending from makes it easier to test it. Obviously we can wrap a map as well. I agree that the name is unfortunate, just didn't come up with anything better - do you have some ideas?"", 'commenter': 'jacek-lewandowski'}, {'comment': ""Let's wrap the map for the extra safety (if the test really needs it, leave it package-private), and then we can call this something like `LoadingMap`."", 'commenter': 'blambov'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V blockingGet(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.awaitUninterruptibly().getNow();","[{'comment': ""Isn't the await superfluous?"", 'commenter': 'blambov'}, {'comment': 'yes, indeed, this is the result of the evolution of this class, I forgot to update this method', 'commenter': 'jacek-lewandowski'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V blockingGet(K key)","[{'comment': 'This is not really a blocking method. Perhaps `getIfPresent()`?', 'commenter': 'blambov'}, {'comment': 'replaced with `getIfReady`', 'commenter': 'jacek-lewandowski'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline","[{'comment': 'nit: unline -> unlike', 'commenter': 'blambov'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.","[{'comment': 'nit: sematnics -> semantics', 'commenter': 'blambov'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value","[{'comment': 'The comment appears to be outdated.', 'commenter': 'blambov'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V getIfReady(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.getNow();
+        else
+            return null;
+    }
+
+    /**
+     * If the value for the given key is missing, execute a load function to obtain a value and put it into the map.
+     * It is guaranteed that the load function will be executed only once when the key is missing and mulitple threads
+     * called this method for the same key.","[{'comment': 'I think we do need to mention the deadlock on nested reinitialization, and the behaviour on concurrent unload and failing load.', 'commenter': 'blambov'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V getIfReady(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.getNow();
+        else
+            return null;
+    }
+
+    /**
+     * If the value for the given key is missing, execute a load function to obtain a value and put it into the map.
+     * It is guaranteed that the load function will be executed only once when the key is missing and mulitple threads
+     * called this method for the same key.
+     * <p>
+     * When the mapping function returns {@code null}, {@link NullPointerException} is thrown. When the mapping function
+     * throws exception, it is rethrown by this method. In both cases nothing gets added to the map.
+     */
+    public V blockingLoadIfAbsent(K key, Supplier<? extends V> loadFunction) throws RuntimeException
+    {
+        while (true)
+        {
+            Future<V> future = get(key);
+            AsyncPromise<V> newEntry = null;
+            if (future == null)
+            {
+                newEntry = new AsyncPromise<>();
+                future = putIfAbsent(key, newEntry);
+                if (future == null)
+                {
+                    // We managed to create an entry for the value. Now initialize it.
+                    future = newEntry;
+                    try
+                    {
+                        V v = loadFunction.get();
+                        if (v == null)
+                            throw new NullPointerException(""The mapping function returned null"");
+                        else
+                            newEntry.setSuccess(v);
+                    }
+                    catch (Throwable t)
+                    {
+                        newEntry.setFailure(t);
+                        // Remove future so that construction can be retried later
+                        remove(key, future);
+                    }
+                }
+                else
+                {
+                    newEntry = null;
+                }
+
+                // Else some other thread beat us to it, but we now have the reference to the future which we can wait for.
+            }
+
+            future.awaitUninterruptibly();
+
+            if (future.getNow() != null) // implies success
+                return future.getNow();
+
+                // if blockingUnloadIfPresent was called in the meantime (success and getNow == null), we simply retry hoping that unloading gets finished soon
+                // also we retry if the concurrent attempt to load entry failed - but we do not retry if this attempt failed (failed and newEntry == null)","[{'comment': 'Maybe ""if the failing attempt was initiated by us"" instead of ""if this attempt failed""?', 'commenter': 'blambov'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V getIfReady(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.getNow();
+        else
+            return null;
+    }
+
+    /**
+     * If the value for the given key is missing, execute a load function to obtain a value and put it into the map.
+     * It is guaranteed that the load function will be executed only once when the key is missing and mulitple threads
+     * called this method for the same key.
+     * <p>
+     * When the mapping function returns {@code null}, {@link NullPointerException} is thrown. When the mapping function
+     * throws exception, it is rethrown by this method. In both cases nothing gets added to the map.
+     */
+    public V blockingLoadIfAbsent(K key, Supplier<? extends V> loadFunction) throws RuntimeException
+    {
+        while (true)
+        {
+            Future<V> future = get(key);
+            AsyncPromise<V> newEntry = null;
+            if (future == null)
+            {
+                newEntry = new AsyncPromise<>();
+                future = putIfAbsent(key, newEntry);
+                if (future == null)
+                {
+                    // We managed to create an entry for the value. Now initialize it.
+                    future = newEntry;
+                    try
+                    {
+                        V v = loadFunction.get();
+                        if (v == null)
+                            throw new NullPointerException(""The mapping function returned null"");
+                        else
+                            newEntry.setSuccess(v);
+                    }
+                    catch (Throwable t)
+                    {
+                        newEntry.setFailure(t);
+                        // Remove future so that construction can be retried later
+                        remove(key, future);
+                    }
+                }
+                else
+                {
+                    newEntry = null;
+                }
+
+                // Else some other thread beat us to it, but we now have the reference to the future which we can wait for.
+            }
+
+            future.awaitUninterruptibly();
+
+            if (future.getNow() != null) // implies success
+                return future.getNow();","[{'comment': ""Nit: the two separate calls make me wonder if the state couldn't change due to some race between them. That's not the case, but let's make it crystal clear by putting the value in a local variable."", 'commenter': 'blambov'}, {'comment': ""yes, it cannot change because it was awaited the result above, but ok, I'll make a local variable"", 'commenter': 'jacek-lewandowski'}]"
1331,src/java/org/apache/cassandra/utils/concurrent/ExtendedNonBlockingHashMap.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils.concurrent;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.function.BiFunction;
+import java.util.function.Consumer;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * An extension of {@link NonBlockingHashMap} where all values are wrapped by {@link Future}.
+ * <p>
+ * The main purpose of this class is to provide the functionality of concurrent hash map which may perform operations like
+ * {@link ConcurrentHashMap#computeIfAbsent(Object, Function)} and {@link ConcurrentHashMap#computeIfPresent(Object, BiFunction)}
+ * with synchronization scope reduced to the single key - that is, when dealing with a single key, unline
+ * {@link ConcurrentHashMap} we do not lock the whole map for the time the mapping function is running. This may help
+ * to avoid the case when we want to load/unload a value for a key K1 while loading/unloading a value for a key K2. Such
+ * scenario is forbidden in case of {@link ConcurrentHashMap} and leads to a deadlock. On the other hand, {@link NonBlockingHashMap}
+ * does not guarantee at-most-once sematnics of running the mapping function for a single key.
+ *
+ * @param <K>
+ * @param <V>
+ */
+public class ExtendedNonBlockingHashMap<K, V> extends NonBlockingHashMap<K, Future<V>>
+{
+    /**
+     * Get a value for a given key, waiting for initialization if necessary. This method does not initialize the value
+     * if missing. It returns {@code null} regardless the value is missing or failed to initialize.
+     */
+    public V getIfReady(K key)
+    {
+        Future<V> future = get(key);
+        if (future != null && future.isDone())
+            return future.getNow();
+        else
+            return null;
+    }
+
+    /**
+     * If the value for the given key is missing, execute a load function to obtain a value and put it into the map.
+     * It is guaranteed that the load function will be executed only once when the key is missing and mulitple threads
+     * called this method for the same key.
+     * <p>
+     * When the mapping function returns {@code null}, {@link NullPointerException} is thrown. When the mapping function
+     * throws exception, it is rethrown by this method. In both cases nothing gets added to the map.
+     */
+    public V blockingLoadIfAbsent(K key, Supplier<? extends V> loadFunction) throws RuntimeException
+    {
+        while (true)
+        {
+            Future<V> future = get(key);
+            AsyncPromise<V> newEntry = null;
+            if (future == null)
+            {
+                newEntry = new AsyncPromise<>();
+                future = putIfAbsent(key, newEntry);
+                if (future == null)
+                {
+                    // We managed to create an entry for the value. Now initialize it.
+                    future = newEntry;
+                    try
+                    {
+                        V v = loadFunction.get();
+                        if (v == null)
+                            throw new NullPointerException(""The mapping function returned null"");
+                        else
+                            newEntry.setSuccess(v);
+                    }
+                    catch (Throwable t)
+                    {
+                        newEntry.setFailure(t);
+                        // Remove future so that construction can be retried later
+                        remove(key, future);
+                    }
+                }
+                else
+                {
+                    newEntry = null;
+                }
+
+                // Else some other thread beat us to it, but we now have the reference to the future which we can wait for.
+            }
+
+            future.awaitUninterruptibly();
+
+            if (future.getNow() != null) // implies success
+                return future.getNow();
+
+                // if blockingUnloadIfPresent was called in the meantime (success and getNow == null), we simply retry hoping that unloading gets finished soon
+                // also we retry if the concurrent attempt to load entry failed - but we do not retry if this attempt failed (failed and newEntry == null)
+            else if (future.isSuccess() || newEntry == null)","[{'comment': 'I think this would be clearer as\r\n```\r\nif (newEntry != null)\r\n    future.rethrowIfFailed();\r\n\r\nThread.yield();\r\n```\r\nI would also consider using a boolean (named e.g. `usingOurFuture`) instead of `newEntry != null`.', 'commenter': 'blambov'}, {'comment': 'great idea!', 'commenter': 'jacek-lewandowski'}]"
1357,test/unit/org/apache/cassandra/net/WriteCallbackInfoTest.java,"@@ -32,19 +34,35 @@
 import org.apache.cassandra.locator.InetAddressAndPort;
 import org.apache.cassandra.schema.MockSchema;
 import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.service.paxos.Commit;
 import org.apache.cassandra.utils.ByteBufferUtil;
 
 import static org.apache.cassandra.locator.ReplicaUtils.full;
 
 public class WriteCallbackInfoTest
 {
+    private InetAddressAndPort testEp;
+
     @BeforeClass
     public static void initDD()
     {
         DatabaseDescriptor.daemonInitialization();
     }
 
+    @Before
+    public void setup() throws Exception
+    {
+        testEp = InetAddressAndPort.getByName(""192.168.1.1"");
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), testEp);","[{'comment': 'nice one', 'commenter': 'smiklosovic'}, {'comment': 'Thank you @smiklosovic. Are you reviewing the PR?', 'commenter': 'yifan-c'}]"
1357,src/java/org/apache/cassandra/config/Config.java,"@@ -76,6 +76,7 @@
     public volatile boolean hinted_handoff_enabled = true;
     public Set<String> hinted_handoff_disabled_datacenters = Sets.newConcurrentHashSet();
     public volatile int max_hint_window_in_ms = 3 * 3600 * 1000; // three hours
+    public volatile Integer max_hints_size_per_host_in_mb = 0; // non-positive means disabled","[{'comment': 'Can we use primitive here to avoid any boxing overhead?\r\n\r\n```suggestion\r\n    public volatile int max_hints_size_per_host_in_mb = 0; // non-positive means disabled\r\n```', 'commenter': 'frankgh'}]"
1357,src/java/org/apache/cassandra/hints/HintsStore.java,"@@ -218,7 +218,7 @@ else if (hintsFile.exists())
             logger.info(""Already deleted hint file {}"", descriptor.fileName());
 
         //noinspection ResultOfMethodCallIgnored
-        new File(hintsDirectory, descriptor.checksumFileName()).tryDelete();
+        descriptor.checksumFile(hintsDirectory).delete();","[{'comment': 'This changes behavior, an `FSError` might be thrown from the delete call. Whereas `tryDelete` does not ""ordinarily throw exceptions"".\r\n\r\nAdditionally, it looks like `delete` is `RateLimited`. So there might be blocking, which I\'m not sure if it\'s intended here.', 'commenter': 'frankgh'}]"
1357,src/java/org/apache/cassandra/hints/HintsStore.java,"@@ -236,6 +236,17 @@ void markDispatchOffset(HintsDescriptor descriptor, InputPosition inputPosition)
         dispatchPositions.put(descriptor, inputPosition);
     }
 
+    // returns total length of all files belonging to the hints store, in bytes.","[{'comment': 'can we make it a javadoc instead of a comment?', 'commenter': 'frankgh'}]"
1357,src/java/org/apache/cassandra/hints/HintsStore.java,"@@ -236,6 +236,17 @@ void markDispatchOffset(HintsDescriptor descriptor, InputPosition inputPosition)
         dispatchPositions.put(descriptor, inputPosition);
     }
 
+    // returns total length of all files belonging to the hints store, in bytes.
+    long getTotalFileSize()
+    {
+        long total = 0;
+        for (HintsDescriptor descriptor : Iterables.concat(dispatchDequeue, corruptedFiles))
+        {
+            total += descriptor.file(hintsDirectory).length();
+        }
+        return total;","[{'comment': 'we _could_ use streams here :)\r\n\r\n```java\r\nreturn Stream.concat(dispatchDequeue.stream(), corruptedFiles.stream())\r\n               .mapToLong(descriptor -> descriptor.file(hintsDirectory).length())\r\n               .sum();\r\n```', 'commenter': 'frankgh'}, {'comment': ""Yep. We could. I'd keep the current as it produces (negligible) less objects."", 'commenter': 'yifan-c'}]"
1357,src/java/org/apache/cassandra/service/StorageProxy.java,"@@ -2234,16 +2234,40 @@ public void setMaxHintWindow(int ms)
         DatabaseDescriptor.setMaxHintWindow(ms);
     }
 
+    public int getMaxHintsSizePerHostInMb()
+    {
+        return DatabaseDescriptor.getMaxHintsSizePerHostInMb();
+    }
+
+    public void setMaxHintsSizePerHostInMb(int value)
+    {
+        DatabaseDescriptor.setMaxHintsSizePerHostInMb(value);
+    }
+
     public static boolean shouldHint(Replica replica)
     {
         return shouldHint(replica, true);
     }
 
+    /**
+     * Determines whether a hint should be stored or not.","[{'comment': 'I guess a better name for this method would have been `shouldStoreHint()`', 'commenter': 'frankgh'}, {'comment': 'I agree with you that `shouldStoreHint` is a better name. The name has been there and it is a public function. So I am being conservative to not change it. At the end, `shouldHint` can describe the function too. ', 'commenter': 'yifan-c'}, {'comment': 'yeah, no need to change it', 'commenter': 'frankgh'}]"
1357,conf/cassandra.yaml,"@@ -63,6 +63,11 @@ hinted_handoff_enabled: true
 # created until it has been seen alive and gone down again.
 max_hint_window_in_ms: 10800000 # 3 hours
 
+# The file size limit to store hints for an unreachable host.
+# Once the local hints files have reached the limit, no more new hints will be created.
+# Set a non-positive value will disable the size limit.
+max_hints_size_per_host_in_mb: 0","[{'comment': 'should this be `max_hint_size_per_host_in_mb` instead, similar to `max_hint_window_in_ms`. So drop the plural to make it consistent?', 'commenter': 'frankgh'}, {'comment': 'There are other configurations like `max_hints_delivery_threads` and `max_hints_file_size_in_mb` that all use the plural form. Only `max_hint_window_in_ms` is using the singular form. I will keep the current one to be consistent with the majority. ', 'commenter': 'yifan-c'}, {'comment': ""yeah, let's make it consistent with the majority"", 'commenter': 'frankgh'}, {'comment': ""Agree with @yifan-c, 'hints' reading better"", 'commenter': 'skoppu22'}]"
1357,src/java/org/apache/cassandra/service/StorageProxy.java,"@@ -2262,26 +2286,38 @@ public static boolean shouldHint(Replica replica, boolean tryEnablePersistentWin
         long endpointDowntime = Gossiper.instance.getEndpointDowntime(endpoint);
         boolean hintWindowExpired = endpointDowntime > maxHintWindow;
 
-        if (tryEnablePersistentWindow && !hintWindowExpired && DatabaseDescriptor.hintWindowPersistentEnabled())
+        UUID hostIdForEndpoint = StorageService.instance.getHostIdForEndpoint(endpoint);
+        if (hostIdForEndpoint == null)
         {
-            UUID hostIdForEndpoint = StorageService.instance.getHostIdForEndpoint(endpoint);
-            if (hostIdForEndpoint != null)
-            {
-                long earliestHint = HintsService.instance.getEarliestHintForHost(hostIdForEndpoint);
-                hintWindowExpired = Clock.Global.currentTimeMillis() - maxHintWindow > earliestHint;
-                if (hintWindowExpired)
-                    Tracing.trace(""Not hinting {} for which there is the earliest hint stored at {}"", replica, earliestHint);
-            }
+            Tracing.trace(""Discarding hint for endpoint not part of ring: {}"", endpoint);
+            return false;
         }
-        else if (hintWindowExpired)
+
+        // if persisting hints window, hintWindowExpired might be updated according to the timestamp of the earliest hint
+        if (tryEnablePersistentWindow && !hintWindowExpired && DatabaseDescriptor.hintWindowPersistentEnabled())
         {
-            Tracing.trace(""Not hinting {} which has been down {} ms"", replica, endpointDowntime);
+            long earliestHint = HintsService.instance.getEarliestHintForHost(hostIdForEndpoint);
+            hintWindowExpired = Clock.Global.currentTimeMillis() - maxHintWindow > earliestHint;
+            if (hintWindowExpired)
+                Tracing.trace(""Not hinting {} for which there is the earliest hint stored at {}"", replica, earliestHint);
         }
 
         if (hintWindowExpired)
-            HintsService.instance.metrics.incrPastWindow(replica.endpoint());
+        {
+            HintsService.instance.metrics.incrPastWindow(endpoint);
+            Tracing.trace(""Not hinting {} which has been down {} ms"", endpoint, endpointDowntime);
+            return false;
+        }
+
+        long maxHintsSize = DatabaseDescriptor.getMaxHintsSizePerHost();
+        boolean hasHintsReachedMaxSize = maxHintsSize > 0 && HintsService.instance.getTotalHintsSize(hostIdForEndpoint) > maxHintsSize;
+        if (hasHintsReachedMaxSize)
+        {
+            Tracing.trace(""Not hinting {} which has reached to the max hints size {} bytes on disk"", endpoint, DatabaseDescriptor.getMaxHintsSizePerHost());","[{'comment': 'Maybe also log the total hint size value here: `HintsService.instance.getTotalHintsSize(hostIdForEndpoint)`. It might come in handy for debugging. ', 'commenter': 'frankgh'}]"
1357,test/unit/org/apache/cassandra/hints/HintsCatalogTest.java,"@@ -138,14 +136,27 @@ public void deleteHintsTest() throws IOException
     @Test
     public void exciseHintFiles() throws IOException
     {
-        File directory = new File(Files.createTempDirectory(null));
-        try
-        {
-            exciseHintFiles(directory);
-        }
-        finally
+        File directory = new File(testFolder.newFolder());
+        exciseHintFiles(directory);
+    }
+
+    @Test
+    public void hintsTotalSizeTest() throws IOException
+    {
+        File directory = new File(testFolder.newFolder());
+        UUID hostId = UUID.randomUUID();
+        long now = System.currentTimeMillis();","[{'comment': ""Maybe use `Clock.Global.currentTimeMillis()` here. I've seen tests being changed to use `Clock.Global.currentTimeMillis()`. See https://github.com/apache/cassandra/commit/599294c919df10a4aea592d78364af565d8ff1ed"", 'commenter': 'frankgh'}, {'comment': 'Good point!', 'commenter': 'yifan-c'}]"
1357,test/unit/org/apache/cassandra/service/StorageProxyTest.java,"@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.UUID;
+import java.util.concurrent.TimeUnit;
+import java.util.function.Consumer;
+
+import com.google.common.util.concurrent.Uninterruptibles;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.gms.EndpointState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.gms.HeartBeatState;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.jboss.byteman.contrib.bmunit.BMRule;
+import org.jboss.byteman.contrib.bmunit.BMUnitRunner;
+
+import static org.apache.cassandra.locator.ReplicaUtils.full;
+import static org.assertj.core.api.Assertions.assertThat;
+
+@RunWith(BMUnitRunner.class)
+public class StorageProxyTest
+{
+    @BeforeClass
+    public static void initDD()
+    {
+        DatabaseDescriptor.daemonInitialization();
+    }
+
+    @Test
+    public void testShouldHint() throws Exception
+    {
+        // HAPPY PATH with all defaults
+        shouldHintTest(replica -> {
+            assertThat(StorageProxy.shouldHint(replica)).isTrue();
+            assertThat(StorageProxy.shouldHint(replica, /* tryEnablePersistentWindow */ false)).isTrue();
+        });
+    }
+
+    @Test
+    public void testShouldHintOnWindowExpiry() throws Exception
+    {
+        shouldHintTest(replica -> {
+            // wait for 5 ms, we will shorten the hints window later
+            Uninterruptibles.sleepUninterruptibly(5, TimeUnit.MILLISECONDS);
+
+            final int originalHintWindow = DatabaseDescriptor.getMaxHintWindow();
+            try
+            {
+                DatabaseDescriptor.setMaxHintWindow(1); // 1 ms. It should not hint
+                assertThat(StorageProxy.shouldHint(replica)).isFalse();
+            }
+            finally
+            {
+                DatabaseDescriptor.setMaxHintWindow(originalHintWindow);
+            }
+        });
+    }
+
+    @Test
+    @BMRule(name = ""Hints size exceeded the limit"",
+            targetClass=""org.apache.cassandra.hints.HintsService"",
+            targetMethod=""getTotalHintsSize"",
+            action=""return 2097152;"") // 2MB
+    public void testShouldHintOnExceedingSize() throws Exception
+    {
+        shouldHintTest(replica -> {
+            final long orignialHintsSizeLimit = DatabaseDescriptor.getMaxHintsSizePerHost() / 1024L / 1024L;","[{'comment': '```suggestion\r\n            final long originalHintsSizeLimit = DatabaseDescriptor.getMaxHintsSizePerHost() / 1024L / 1024L;\r\n```', 'commenter': 'frankgh'}]"
1357,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -2606,6 +2606,21 @@ public static int getMaxHintWindow()
         return conf.max_hint_window_in_ms;
     }
 
+    public static void setMaxHintsSizePerHostInMb(int value)
+    {
+        conf.max_hints_size_per_host_in_mb = value;
+    }
+
+    public static int getMaxHintsSizePerHostInMb()
+    {
+        return conf.max_hints_size_per_host_in_mb;
+    }
+
+    public static long getMaxHintsSizePerHost()
+    {
+        return conf.max_hints_size_per_host_in_mb * 1024L * 1024L;","[{'comment': 'Use `ByteUnit.MEBI_BYTES.toBytes()` instead of manual conversion?', 'commenter': 'dineshjoshi'}]"
1357,src/java/org/apache/cassandra/config/Config.java,"@@ -81,6 +81,7 @@
     public boolean auto_bootstrap = true;
     public volatile boolean hinted_handoff_enabled = true;
     public Set<String> hinted_handoff_disabled_datacenters = Sets.newConcurrentHashSet();
+    public volatile int max_hint_window_in_ms = 3 * 3600 * 1000; // three hours","[{'comment': ""do we need this field here? Wouldn't the annotation in the next like cover this property?"", 'commenter': 'frankgh'}, {'comment': 'No. I think it was added during rebase accidentally. Good catch!', 'commenter': 'yifan-c'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -84,21 +84,46 @@ public void discard(CommitLogSegment segment, boolean delete)
 
     /**
      * Delete the oldest hard-linked CDC commit log segment to free up space.
+     * @param bytesToFree, the minimum space to free up
      * @return total deleted file size in bytes
      */
-    public long deleteOldestLinkedCDCCommitLogSegment()
+    public long deleteOldLinkedCDCCommitLogSegment(long bytesToFree)
     {
+        if (bytesToFree <= 0)
+            return 0;
+
         File cdcDir = new File(DatabaseDescriptor.getCDCLogLocation());
         Preconditions.checkState(cdcDir.isDirectory(), ""The CDC directory does not exist."");
         File[] files = cdcDir.tryList(f -> CommitLogDescriptor.isValid(f.name()));
-        Preconditions.checkState(files != null && files.length > 0,
-                                 ""There should be at least 1 CDC commit log segment."");
+        if (files == null || files.length == 0)
+        {
+            logger.warn(""Skip deleting due to no CDC commit log segments found."");
+            return 0;
+        }
         List<File> sorted = Arrays.stream(files)
-                                  .sorted(Comparator.comparingLong(File::lastModified))
+                                  // commit log file name (contains id) increases monotonically","[{'comment': ""While true today, I'm a little concerned about this undocumented (effectively; has a comment here but nothing to indicate dependency on other side) coupling. Could we formalize or comment in the CommitLogSegment naming / generation that we depend on that functionality here and {@link X} in the JavaDoc to tie them together for future maintainers?"", 'commenter': 'jmckenzie-dev'}, {'comment': 'Agree. I hesitated between comparing the file name and the `id` value before pushing the commit. \r\nHere, it only depends on the `id` field to sort. I will update the comparator to use id, so it is explicit. ', 'commenter': 'yifan-c'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -84,21 +84,46 @@ public void discard(CommitLogSegment segment, boolean delete)
 
     /**
      * Delete the oldest hard-linked CDC commit log segment to free up space.
+     * @param bytesToFree, the minimum space to free up
      * @return total deleted file size in bytes
      */
-    public long deleteOldestLinkedCDCCommitLogSegment()
+    public long deleteOldLinkedCDCCommitLogSegment(long bytesToFree)
     {
+        if (bytesToFree <= 0)
+            return 0;
+
         File cdcDir = new File(DatabaseDescriptor.getCDCLogLocation());
         Preconditions.checkState(cdcDir.isDirectory(), ""The CDC directory does not exist."");
         File[] files = cdcDir.tryList(f -> CommitLogDescriptor.isValid(f.name()));
-        Preconditions.checkState(files != null && files.length > 0,
-                                 ""There should be at least 1 CDC commit log segment."");
+        if (files == null || files.length == 0)
+        {
+            logger.warn(""Skip deleting due to no CDC commit log segments found."");
+            return 0;
+        }
         List<File> sorted = Arrays.stream(files)
-                                  .sorted(Comparator.comparingLong(File::lastModified))
+                                  // commit log file name (contains id) increases monotonically
+                                  .sorted(Comparator.comparing(File::name))
                                   .collect(Collectors.toList());
-        File oldestCdcFile = sorted.get(0);
-        File cdcIndexFile = CommitLogDescriptor.inferCdcIndexFile(oldestCdcFile);
-        return deleteCDCFiles(oldestCdcFile, cdcIndexFile);
+        long bytesDeleted = 0;
+        long bytesRemaining = 0;
+        boolean deletionCompleted = false;
+        // keep deleting from old to new until it reaches to the goal or the current writting segment","[{'comment': 'nit: writing', 'commenter': 'jmckenzie-dev'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -84,21 +84,46 @@ public void discard(CommitLogSegment segment, boolean delete)
 
     /**
      * Delete the oldest hard-linked CDC commit log segment to free up space.
+     * @param bytesToFree, the minimum space to free up
      * @return total deleted file size in bytes
      */
-    public long deleteOldestLinkedCDCCommitLogSegment()
+    public long deleteOldLinkedCDCCommitLogSegment(long bytesToFree)
     {
+        if (bytesToFree <= 0)
+            return 0;
+
         File cdcDir = new File(DatabaseDescriptor.getCDCLogLocation());
         Preconditions.checkState(cdcDir.isDirectory(), ""The CDC directory does not exist."");
         File[] files = cdcDir.tryList(f -> CommitLogDescriptor.isValid(f.name()));
-        Preconditions.checkState(files != null && files.length > 0,
-                                 ""There should be at least 1 CDC commit log segment."");
+        if (files == null || files.length == 0)
+        {
+            logger.warn(""Skip deleting due to no CDC commit log segments found."");
+            return 0;
+        }
         List<File> sorted = Arrays.stream(files)
-                                  .sorted(Comparator.comparingLong(File::lastModified))
+                                  // commit log file name (contains id) increases monotonically
+                                  .sorted(Comparator.comparing(File::name))
                                   .collect(Collectors.toList());
-        File oldestCdcFile = sorted.get(0);
-        File cdcIndexFile = CommitLogDescriptor.inferCdcIndexFile(oldestCdcFile);
-        return deleteCDCFiles(oldestCdcFile, cdcIndexFile);
+        long bytesDeleted = 0;
+        long bytesRemaining = 0;
+        boolean deletionCompleted = false;
+        // keep deleting from old to new until it reaches to the goal or the current writting segment
+        for (File linkedCdcFile : sorted)
+        {
+            // only evaluate/update when deletionCompleted is false
+            deletionCompleted = deletionCompleted
+                                || (bytesDeleted >= bytesToFree || linkedCdcFile.equals(allocatingFrom().getCDCFile()));","[{'comment': 'I found this conditional a little convoluted to parse. Maybe we do something like:\r\n ```\r\n           // only evaluate/update when deletionCompleted is false\r\n            if (!deletionCompleted)\r\n                deletionCompleted = bytesDeleted > bytesToFree || linkedCdcFile.equals(allocatingFrom().getCDCFile());\r\n```', 'commenter': 'jmckenzie-dev'}, {'comment': 'Yep. It does not worth it to trade readability with compactness.', 'commenter': 'yifan-c'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -158,6 +185,16 @@ public void shutdown()
         return alloc;
     }
 
+    // Non-blocking mode is just enabled for CDC. The segment is still marked as FORBIDDEN.","[{'comment': 'nit: rephrase to ""Non-blocking mode has just recently been enabled for CDC"" if that\'s the intent here. Reads like it\'s ""only"" set for CDC which is a little confusing.', 'commenter': 'jmckenzie-dev'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -264,29 +301,31 @@ public void start()
          */
         void processNewSegment(CommitLogSegment segment)
         {
-            // See synchronization in CommitLogSegment.setCDCState
-            synchronized(segment.cdcStateLock)
+            int segmentSize = defaultSegmentSize();
+            long allowance = allowableCDCBytes();
+            boolean blocking = DatabaseDescriptor.getCDCBlockWrites();
+
+            synchronized (segment.cdcStateLock)
             {
-                int segmentSize = defaultSegmentSize();
-                long allowance = allowableCDCBytes();
-                boolean blocking = DatabaseDescriptor.getCDCBlockWrites();
                 segment.setCDCState(blocking && segmentSize + sizeInProgress.get() > allowance
                                     ? CDCState.FORBIDDEN
                                     : CDCState.PERMITTED);
 
-                // Remove the oldest cdc segment file when exceeding the CDC storage allowance","[{'comment': 'Keep this bread crumb so people know where the other side of this synchronization is if they come back to modify later.', 'commenter': 'jmckenzie-dev'}, {'comment': 'Do you mean this comment line?\r\n```\r\n// See synchronization in CommitLogSegment.setCDCState```', 'commenter': 'yifan-c'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -264,29 +301,31 @@ public void start()
          */
         void processNewSegment(CommitLogSegment segment)
         {
-            // See synchronization in CommitLogSegment.setCDCState
-            synchronized(segment.cdcStateLock)
+            int segmentSize = defaultSegmentSize();
+            long allowance = allowableCDCBytes();
+            boolean blocking = DatabaseDescriptor.getCDCBlockWrites();
+
+            synchronized (segment.cdcStateLock)
             {
-                int segmentSize = defaultSegmentSize();
-                long allowance = allowableCDCBytes();
-                boolean blocking = DatabaseDescriptor.getCDCBlockWrites();
                 segment.setCDCState(blocking && segmentSize + sizeInProgress.get() > allowance
                                     ? CDCState.FORBIDDEN
                                     : CDCState.PERMITTED);
 
-                // Remove the oldest cdc segment file when exceeding the CDC storage allowance
-                while (!blocking && segmentSize + sizeInProgress.get() > allowance)
-                {
-                    long releasedSize = segmentManager.deleteOldestLinkedCDCCommitLogSegment();
-                    sizeInProgress.getAndAdd(-releasedSize);
-                    logger.debug(""Freed up {} bytes after deleting the oldest CDC commit log segment in non-blocking mode. "" +
-                                 ""Total on-disk CDC size: {}; allowed CDC size: {}"",
-                                 releasedSize, sizeInProgress.get() + segmentSize, allowance);
-                }
-
                 // Aggresively count in the (estimated) size of new segments.","[{'comment': 'nit: Spelling. Should be ""Aggressively"" (that\'s on me 😄 )', 'commenter': 'jmckenzie-dev'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -264,29 +301,31 @@ public void start()
          */
         void processNewSegment(CommitLogSegment segment)
         {
-            // See synchronization in CommitLogSegment.setCDCState
-            synchronized(segment.cdcStateLock)
+            int segmentSize = defaultSegmentSize();
+            long allowance = allowableCDCBytes();
+            boolean blocking = DatabaseDescriptor.getCDCBlockWrites();
+
+            synchronized (segment.cdcStateLock)
             {
-                int segmentSize = defaultSegmentSize();
-                long allowance = allowableCDCBytes();
-                boolean blocking = DatabaseDescriptor.getCDCBlockWrites();
                 segment.setCDCState(blocking && segmentSize + sizeInProgress.get() > allowance
                                     ? CDCState.FORBIDDEN
                                     : CDCState.PERMITTED);
 
-                // Remove the oldest cdc segment file when exceeding the CDC storage allowance
-                while (!blocking && segmentSize + sizeInProgress.get() > allowance)
-                {
-                    long releasedSize = segmentManager.deleteOldestLinkedCDCCommitLogSegment();
-                    sizeInProgress.getAndAdd(-releasedSize);
-                    logger.debug(""Freed up {} bytes after deleting the oldest CDC commit log segment in non-blocking mode. "" +
-                                 ""Total on-disk CDC size: {}; allowed CDC size: {}"",
-                                 releasedSize, sizeInProgress.get() + segmentSize, allowance);
-                }
-
                 // Aggresively count in the (estimated) size of new segments.
                 if (segment.getCDCState() == CDCState.PERMITTED)
-                    sizeInProgress.getAndAdd(segmentSize);
+                    addSize(segmentSize);
+            }
+
+            // Remove the oldest cdc segment file when exceeding the CDC storage allowance
+            if (!blocking && sizeInProgress.get() > allowance)
+            {
+                long bytesToFree = sizeInProgress.get() - allowance;
+                long remaningSize = segmentManager.deleteOldLinkedCDCCommitLogSegment(bytesToFree);","[{'comment': 'nit: spelling. `remainingSize`', 'commenter': 'jmckenzie-dev'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -84,21 +84,46 @@ public void discard(CommitLogSegment segment, boolean delete)
 
     /**
      * Delete the oldest hard-linked CDC commit log segment to free up space.
+     * @param bytesToFree, the minimum space to free up
      * @return total deleted file size in bytes","[{'comment': ""I think we either need to amend this javadoc @return to match that we're returning the bytes remaining or we need to juggle around variable names below. Currently this seems incorrect.\r\n\r\nIt looks like we're expecting CDC bytes remaining in CommitLogSegmentManagerCDC.processNewSegment, so I think this is probably just a vestigial javadoc entry."", 'commenter': 'jmckenzie-dev'}, {'comment': 'I forgot to address this one. I will update the doc soon', 'commenter': 'yifan-c'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -264,29 +301,31 @@ public void start()
          */
         void processNewSegment(CommitLogSegment segment)
         {
-            // See synchronization in CommitLogSegment.setCDCState
-            synchronized(segment.cdcStateLock)
+            int segmentSize = defaultSegmentSize();
+            long allowance = allowableCDCBytes();
+            boolean blocking = DatabaseDescriptor.getCDCBlockWrites();
+
+            synchronized (segment.cdcStateLock)
             {
-                int segmentSize = defaultSegmentSize();
-                long allowance = allowableCDCBytes();
-                boolean blocking = DatabaseDescriptor.getCDCBlockWrites();
                 segment.setCDCState(blocking && segmentSize + sizeInProgress.get() > allowance
                                     ? CDCState.FORBIDDEN
                                     : CDCState.PERMITTED);
 
-                // Remove the oldest cdc segment file when exceeding the CDC storage allowance
-                while (!blocking && segmentSize + sizeInProgress.get() > allowance)
-                {
-                    long releasedSize = segmentManager.deleteOldestLinkedCDCCommitLogSegment();
-                    sizeInProgress.getAndAdd(-releasedSize);
-                    logger.debug(""Freed up {} bytes after deleting the oldest CDC commit log segment in non-blocking mode. "" +
-                                 ""Total on-disk CDC size: {}; allowed CDC size: {}"",
-                                 releasedSize, sizeInProgress.get() + segmentSize, allowance);
-                }
-
                 // Aggresively count in the (estimated) size of new segments.
                 if (segment.getCDCState() == CDCState.PERMITTED)
-                    sizeInProgress.getAndAdd(segmentSize);
+                    addSize(segmentSize);
+            }
+
+            // Remove the oldest cdc segment file when exceeding the CDC storage allowance
+            if (!blocking && sizeInProgress.get() > allowance)
+            {
+                long bytesToFree = sizeInProgress.get() - allowance;
+                long remaningSize = segmentManager.deleteOldLinkedCDCCommitLogSegment(bytesToFree);
+                long releasedSize = sizeInProgress.get() - remaningSize;
+                sizeInProgress.getAndSet(remaningSize);
+                logger.debug(""Freed up {} ({}) bytes after deleting the oldest CDC commit log segments in non-blocking mode. "" +
+                             ""Total on-disk CDC size: {}; allowed CDC size: {}"",","[{'comment': 'Should we be passing `remainingSize` as the string param for our ""Total on-disk CDC size:""? Passing bytesToFree will be listing the sizeInProgress - allowance which _seems_ off?', 'commenter': 'jmckenzie-dev'}, {'comment': 'The value of the `remainingSize` is used in the log message. `sizeInProgress` is updated with `remainingSize` just before logging. I am OK to use `remainingSize` instead. \r\nIt is a debug message. I was using `bytesToFree` and `releasedSize` to inspect what is the size it expects to reclaim and what is the size it actually reclaimed. ', 'commenter': 'yifan-c'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -295,18 +334,23 @@ void processNewSegment(CommitLogSegment segment)
 
         void processDiscardedSegment(CommitLogSegment segment)
         {
-            // See synchronization in CommitLogSegment.setCDCState
-            synchronized(segment.cdcStateLock)
+            if (!segment.getCDCFile().exists())
+            {
+                logger.debug(""Skip updating size. The CDC commit log segement has been deleted already."");","[{'comment': 'Recommend revise this to something like ""Not processing discarded CommitLogSegment {}; this segment appears to have been deleted already."", segment)', 'commenter': 'jmckenzie-dev'}]"
1379,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -380,14 +430,12 @@ private void addSize(long toAdd)
     @VisibleForTesting
     public long updateCDCTotalSize()
     {
+        long sleepTime = DatabaseDescriptor.getCDCDiskCheckInterval() + 10L;","[{'comment': ""Discussed offline; up this to 50 and hold our noses and it should have enough buffer for really overloaded test environments, since it's ultimately a directory walk + atomic update."", 'commenter': 'jmckenzie-dev'}]"
1429,src/java/org/apache/cassandra/db/virtual/AbstractVirtualTable.java,"@@ -52,7 +53,9 @@ protected AbstractVirtualTable(TableMetadata metadata)
         if (!metadata.isVirtual())
             throw new IllegalArgumentException(""Cannot instantiate a non-virtual table"");
 
-        this.metadata = metadata;
+        this.metadata = metadata.unbuild()
+                                .id(TableId.forSystemTable(metadata.keyspace, metadata.name))","[{'comment': 'needed to make consistent cross-cluster', 'commenter': 'dcapwell'}, {'comment': 'Would probably be nicer if we gave virtual tables deterministic ids from the very beginning, via `TableMetadata.Builder#build()`?', 'commenter': 'iamaleksey'}]"
1429,src/java/org/apache/cassandra/cql3/QueryProcessor.java,"@@ -391,6 +397,61 @@ public static UntypedResultSet executeInternal(String query, Object... values)
             return null;
     }
 
+    public static Future<UntypedResultSet> execute(InetAddressAndPort address, String query, Object... values)","[{'comment': 'the reason for async is so repair can submit and deal with when complete; rather than block waiting.', 'commenter': 'dcapwell'}]"
1429,src/java/org/apache/cassandra/cql3/QueryProcessor.java,"@@ -391,6 +397,61 @@ public static UntypedResultSet executeInternal(String query, Object... values)
             return null;
     }
 
+    public static Future<UntypedResultSet> execute(InetAddressAndPort address, String query, Object... values)
+    {
+        Prepared prepared = prepareInternal(query);
+        QueryOptions options = makeInternalOptions(prepared.statement, values);
+        if (prepared.statement instanceof SelectStatement)
+        {
+            SelectStatement select = (SelectStatement) prepared.statement;
+            int nowInSec = FBUtilities.nowInSeconds();
+            ReadQuery readQuery = select.getQuery(options, nowInSec);
+            List<ReadCommand> commands;
+            if (readQuery instanceof ReadCommand)
+            {
+                commands = Collections.singletonList((ReadCommand) readQuery);
+            }
+            else if (readQuery instanceof SinglePartitionReadQuery.Group)
+            {
+                List<? extends SinglePartitionReadQuery> queries = ((SinglePartitionReadQuery.Group<? extends SinglePartitionReadQuery>) readQuery).queries;
+                queries.forEach(a -> {
+                    if (!(a instanceof ReadCommand))
+                        throw new IllegalArgumentException(""Queries found which are not ReadCommand: "" + a.getClass());
+                });
+                commands = (List<ReadCommand>) (List<?>) queries;
+            }
+            else
+            {
+                throw new IllegalArgumentException(""Unable to handle; only expected ReadCommands but given "" + readQuery.getClass());
+            }
+            Future<List<Message<ReadResponse>>> future = FutureCombiner.allOf(commands.stream()
+                                                                                      .map(rc -> Message.out(rc.verb(), rc))
+                                                                                      .map(m -> MessagingService.instance().<ReadResponse>sendWithResult(m, address))
+                                                                                      .collect(Collectors.toList()));
+
+            ResultSetBuilder result = new ResultSetBuilder(select.getResultMetadata(), select.getSelection().newSelectors(options), null);
+            return future.map(list -> {
+                int i = 0;
+                for (Message<ReadResponse> m : list)
+                {
+                    ReadResponse rsp = m.payload;
+                    try (PartitionIterator it = UnfilteredPartitionIterators.filter(rsp.makeIterator(commands.get(i++)), nowInSec))
+                    {
+                        while (it.hasNext())
+                        {
+                            try (RowIterator partition = it.next())
+                            {
+                                select.processPartition(partition, options, result, nowInSec);","[{'comment': 'was wondering if we should pull this into a util; PartitionIterator -> ResultSet is specific to SelectStatement', 'commenter': 'dcapwell'}]"
1429,src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java,"@@ -70,24 +72,61 @@ private PartitionRangeReadCommand(boolean isDigest,
         this.dataRange = dataRange;
     }
 
-    public static PartitionRangeReadCommand create(TableMetadata metadata,
-                                                   int nowInSec,
-                                                   ColumnFilter columnFilter,
-                                                   RowFilter rowFilter,
-                                                   DataLimits limits,
-                                                   DataRange dataRange)
+    private static PartitionRangeReadCommand create(boolean isDigest,
+                                                    int digestVersion,
+                                                    boolean acceptsTransient,
+                                                    TableMetadata metadata,
+                                                    int nowInSec,
+                                                    ColumnFilter columnFilter,
+                                                    RowFilter rowFilter,
+                                                    DataLimits limits,
+                                                    DataRange dataRange,
+                                                    IndexMetadata index,
+                                                    boolean trackWarnings)
     {
-        return new PartitionRangeReadCommand(false,
-                                             0,
-                                             false,
+        if (metadata.isVirtual())
+            return new VirtualTablePartitionRangeReadCommand(isDigest,
+                                                             digestVersion,
+                                                             acceptsTransient,
+                                                             metadata,
+                                                             nowInSec,
+                                                             columnFilter,
+                                                             rowFilter,
+                                                             limits,
+                                                             dataRange,
+                                                             index,
+                                                             trackWarnings);","[{'comment': ""I think we generally prefer to wrap multiline statement bodies like that in curly braces. I don't mind either way, but would prefer to change the style here and elsewhere across the PR (here and in `SinglePartitionReadCommand`)."", 'commenter': 'iamaleksey'}]"
1429,src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java,"@@ -449,7 +488,42 @@ public ReadCommand deserialize(DataInputPlus in,
         throws IOException
         {
             DataRange range = DataRange.serializer.deserialize(in, version, metadata);
-            return new PartitionRangeReadCommand(isDigest, digestVersion, acceptsTransient, metadata, nowInSec, columnFilter, rowFilter, limits, range, index, false);
+            return PartitionRangeReadCommand.create(isDigest, digestVersion, acceptsTransient, metadata, nowInSec, columnFilter, rowFilter, limits, range, index, false);
+        }
+    }
+
+    public static class VirtualTablePartitionRangeReadCommand extends PartitionRangeReadCommand
+    {
+        private VirtualTablePartitionRangeReadCommand(boolean isDigest, int digestVersion, boolean acceptsTransient, TableMetadata metadata, int nowInSec, ColumnFilter columnFilter, RowFilter rowFilter, DataLimits limits, DataRange dataRange, IndexMetadata index, boolean trackWarnings)","[{'comment': 'Formatting nit: gone a bit long here.', 'commenter': 'iamaleksey'}]"
1454,src/java/org/apache/cassandra/repair/LocalSyncTask.java,"@@ -93,7 +93,7 @@ StreamPlan createStreamPlan()
             plan.transferRanges(remote, desc.keyspace, RangesAtEndpoint.toDummyList(rangesToSync), desc.columnFamily);
         }
 
-        return plan;
+        return plan.withPlanId(desc.determanisticId());","[{'comment': 'the idea is that streaming done by repair will use the RepairJob id (as defined by CASSANDRA-15399), so if a repair exists with the same planId, then this stream can map to the repair without any networking changes', 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/repair/RepairJobDesc.java,"@@ -61,6 +68,17 @@ public RepairJobDesc(UUID parentSessionId, UUID sessionId, String keyspace, Stri
         this.ranges = ranges;
     }
 
+    public UUID determanisticId()","[{'comment': 'the idea is that streaming done by repair will use the RepairJob id (as defined by CASSANDRA-15399), so if a repair exists with the same planId, then this stream can map to the repair without any networking changes', 'commenter': 'dcapwell'}, {'comment': 'nit: `determanistic` - > `deterministic`', 'commenter': 'dineshjoshi'}]"
1454,src/java/org/apache/cassandra/repair/StreamingRepairTask.java,"@@ -88,7 +88,7 @@ StreamPlan createStreamPlan(InetAddressAndPort dest)
         if (!asymmetric)
             // see comment on RangesAtEndpoint.toDummyList for why we synthesize replicas here
             sp.transferRanges(dest, desc.keyspace, RangesAtEndpoint.toDummyList(ranges), desc.columnFamily); // send ranges to the remote node
-        return sp;
+        return sp.withPlanId(desc.determanisticId());","[{'comment': 'the idea is that streaming done by repair will use the RepairJob id (as defined by CASSANDRA-15399), so if a repair exists with the same planId, then this stream can map to the repair without any networking changes', 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/streaming/StreamManager.java,"@@ -213,6 +226,98 @@ private static double calculateEffectiveRateInBytes(double throughput)
     private final Map<UUID, StreamResultFuture> initiatorStreams = new NonBlockingHashMap<>();
     private final Map<UUID, StreamResultFuture> followerStreams = new NonBlockingHashMap<>();
 
+    private final Map<UUID, StreamingState> states = new NonBlockingHashMap<>();
+    private volatile ScheduledFuture<?> cleanup = null;
+
+    public StreamManager()
+    {
+        addListener(new StreamListener()
+        {
+            @Override
+            public void onRegister(StreamResultFuture result)
+            {
+                StreamingState state = new StreamingState(result);
+                StreamingState previous = states.putIfAbsent(state.getId(), state);
+                if (previous == null)
+                {
+                    state.phase.start();
+                    result.addEventListener(state);
+                }
+                else
+                {
+                    logger.warn(""Duplicate streaming states detected for id {}"", state.getId());
+                }
+            }
+        });
+    }
+
+    public void start()
+    {
+        this.cleanup = ScheduledExecutors.optionalTasks.scheduleAtFixedRate(this::cleanup, 0,
+                                                             DatabaseDescriptor.getStreamingStateCleanupInterval().toNanoseconds(),
+                                                             TimeUnit.NANOSECONDS);
+    }
+
+    public void stop()
+    {
+        ScheduledFuture<?> cleanup = this.cleanup;
+        if (cleanup != null)
+            cleanup.cancel(false);
+    }
+
+    public Collection<StreamingState> getStreamingStates()
+    {
+        return states.values();
+    }
+
+    public StreamingState getStreamingState(UUID id)
+    {
+        return states.get(id);
+    }
+
+    @VisibleForTesting
+    public void putStreamingState(StreamingState state)
+    {
+        StreamingState previous = states.putIfAbsent(state.getId(), state);
+        if (previous != null)
+            throw new AssertionError(""StreamPlan id "" + state.getId() + "" already exists"");
+    }
+
+    @VisibleForTesting
+    public void cleanup()
+    {
+        try
+        {
+            DurationSpec duration = DatabaseDescriptor.getStreamingStateExpires();
+            long durationNanos = duration.toNanoseconds();
+            long deadlineNanos = Clock.Global.nanoTime() - durationNanos;
+            for (StreamingState state : states.values())
+            {
+                if (state.getLastUpdatedAtNanos() < deadlineNanos)
+                {
+                    if (state.isComplete())
+                    {
+                        states.remove(state.getId());
+                    }
+                    else
+                    {
+                        logger.warn(""Stream {} has been running longer than state expires window {};\n{}"", state.getId(), duration, state);
+                    }
+                }
+            }
+        }
+        catch (Throwable t)
+        {
+            logger.warn(""Unexpected error cleaning up stream state"", t);","[{'comment': 'with scheduled tasks, if the task throws the scheduling stops, so using this to make sure we un again even if there is an error', 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/streaming/StreamPlan.java,"@@ -63,11 +63,28 @@ public StreamPlan(StreamOperation streamOperation, boolean connectSequentially)
     public StreamPlan(StreamOperation streamOperation, int connectionsPerHost,
                       boolean connectSequentially, UUID pendingRepair, PreviewKind previewKind)
     {
+        this.planId = UUIDGen.getTimeUUID();
         this.streamOperation = streamOperation;
         this.coordinator = new StreamCoordinator(streamOperation, connectionsPerHost, streamingFactory(),
                                                  false, connectSequentially, pendingRepair, previewKind);
     }
 
+    private StreamPlan(UUID planId, StreamOperation streamOperation, StreamCoordinator coordinator, boolean flushBeforeTransfer)
+    {
+        this.planId = planId;
+        this.streamOperation = streamOperation;
+        this.coordinator = coordinator;
+        this.flushBeforeTransfer = flushBeforeTransfer;
+    }
+
+    /**
+     * Allows to override the stream plan id; the main user is Repair so repair streams match job ids
+     */
+    public StreamPlan withPlanId(UUID planId)","[{'comment': 'the idea is that streaming done by repair will use the RepairJob id (as defined by CASSANDRA-15399), so if a repair exists with the same planId, then this stream can map to the repair without any networking changes', 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/config/Config.java,"@@ -742,6 +742,8 @@ public static void setClientMode(boolean clientMode)
     public volatile Set<String> table_properties_disallowed = Collections.emptySet();
     public volatile boolean user_timestamps_enabled = true;
     public volatile boolean read_before_write_list_operations_enabled = true;
+    public volatile DurationSpec streaming_state_expires = DurationSpec.inDays(3);","[{'comment': 'even though this is dynamic (could be updated and changes behavior), holding off adding JMX with the hope that CASSANDRA-15254 solves this generically...', 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/repair/LocalSyncTask.java,"@@ -75,7 +75,7 @@ StreamPlan createStreamPlan()
     {
         InetAddressAndPort remote =  nodePair.peer;
 
-        StreamPlan plan = new StreamPlan(StreamOperation.REPAIR, 1, false, pendingRepair, previewKind)
+        StreamPlan plan = new StreamPlan(desc.determanisticId(), StreamOperation.REPAIR, 1, false, pendingRepair, previewKind)","[{'comment': 'the idea is that streaming done by repair will use the RepairJob id (as defined by CASSANDRA-15399), so if a repair exists with the same planId, then this stream can map to the repair without any networking changes', 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/repair/StreamingRepairTask.java,"@@ -79,7 +79,7 @@ public void run()
     @VisibleForTesting
     StreamPlan createStreamPlan(InetAddressAndPort dest)
     {
-        StreamPlan sp = new StreamPlan(StreamOperation.REPAIR, 1, false, pendingRepair, previewKind)
+        StreamPlan sp = new StreamPlan(desc.determanisticId(), StreamOperation.REPAIR, 1, false, pendingRepair, previewKind)","[{'comment': 'the idea is that streaming done by repair will use the RepairJob id (as defined by CASSANDRA-15399), so if a repair exists with the same planId, then this stream can map to the repair without any networking changes', 'commenter': 'dcapwell'}, {'comment': 'Is there a 1-1 mapping between RepairJob and StreamSession? Is there going to be a possibility of Repair triggering multiple StreamSessions in the near future? It might be worth considering maintaining a mapping of RepairJob to StreamSessions if that might be the case in the future.', 'commenter': 'dineshjoshi'}, {'comment': ""ok, so this may not work as well as I wanted, and seems we don't actually test multiple streams well!\r\n\r\nin repair we get back N trees, and we check all pairs to see if there is a mismatch (coordinator may match peer1, but peer1 and peer2 may not match, so we ask them to fix themselves); so we can have multiple streams from a single RepairJob.  Now, for the coordinator to know what streams are used, it would make sense to have coordinator control the ID, but this trick won't work.\r\n\r\nI will revert these changes as they don't work as planned, need to also improve testing to trigger this case."", 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/streaming/StreamPlan.java,"@@ -63,6 +63,13 @@ public StreamPlan(StreamOperation streamOperation, boolean connectSequentially)
     public StreamPlan(StreamOperation streamOperation, int connectionsPerHost,
                       boolean connectSequentially, UUID pendingRepair, PreviewKind previewKind)
     {
+        this(UUIDGen.getTimeUUID(), streamOperation, connectionsPerHost, connectSequentially, pendingRepair, previewKind);
+    }
+
+    public StreamPlan(UUID planId, StreamOperation streamOperation, int connectionsPerHost,","[{'comment': 'the idea is that streaming done by repair will use the RepairJob id (as defined by CASSANDRA-15399), so if a repair exists with the same planId, then this stream can map to the repair without any networking changes', 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/streaming/StreamManager.java,"@@ -213,6 +226,98 @@ private static double calculateEffectiveRateInBytes(double throughput)
     private final Map<UUID, StreamResultFuture> initiatorStreams = new NonBlockingHashMap<>();
     private final Map<UUID, StreamResultFuture> followerStreams = new NonBlockingHashMap<>();
 
+    private final Map<UUID, StreamingState> states = new NonBlockingHashMap<>();
+    private volatile ScheduledFuture<?> cleanup = null;
+
+    public StreamManager()
+    {
+        addListener(new StreamListener()
+        {
+            @Override
+            public void onRegister(StreamResultFuture result)
+            {
+                StreamingState state = new StreamingState(result);
+                StreamingState previous = states.putIfAbsent(state.getId(), state);","[{'comment': ""still wondering if possible to have 1 node be a follower and not; doesn't make sense to stream to yourself, but not sure if that is actually blocked or not... wondering if I should make this `(id, follower)`"", 'commenter': 'dcapwell'}, {'comment': ""I don't think there is any check to avoid streaming data to yourself. I would double check to confirm. "", 'commenter': 'dineshjoshi'}]"
1454,test/unit/org/apache/cassandra/cql3/CQLTester.java,"@@ -1436,15 +1437,17 @@ public static void assertRows(UntypedResultSet result, Object[]... rows)
                 {
                     Object actualValueDecoded = actualValue == null ? null : column.type.getSerializer().deserialize(actualValue);
                     if (!Objects.equal(expected != null ? expected[j] : null, actualValueDecoded))
-                        Assert.fail(String.format(""Invalid value for row %d column %d (%s of type %s), expected <%s> but got <%s>"",
-                                                  i,
-                                                  j,
-                                                  column.name,
-                                                  column.type.asCQL3Type(),
-                                                  formatValue(expectedByteValue != null ? expectedByteValue.duplicate() : null, column.type),
-                                                  formatValue(actualValue, column.type)));
+                        error.append(String.format(""Invalid value for row %d column %d (%s of type %s), expected <%s> but got <%s>"",","[{'comment': ""I did this as I didn't want the first error found, I wanted all (as I had multiple errors in one query)"", 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/streaming/StreamManager.java,"@@ -213,6 +226,98 @@ private static double calculateEffectiveRateInBytes(double throughput)
     private final Map<UUID, StreamResultFuture> initiatorStreams = new NonBlockingHashMap<>();
     private final Map<UUID, StreamResultFuture> followerStreams = new NonBlockingHashMap<>();
 
+    private final Map<UUID, StreamingState> states = new NonBlockingHashMap<>();
+    private volatile ScheduledFuture<?> cleanup = null;
+
+    public StreamManager()
+    {
+        addListener(new StreamListener()","[{'comment': ""It's not a good idea to call an instance method in a constructor. At this point the object is partially constructed and you're calling `addListener` (instance method) on it. It's best to avoid this and have a separate method, say `init()`, that is explicitly called after the object is completely constructed. JCIP details `Safe Construction Practices` if you want to read more about it."", 'commenter': 'dineshjoshi'}, {'comment': 'ill move to start', 'commenter': 'dcapwell'}, {'comment': 'ok - make sure that you remove all listeners in `stop()` otherwise when you call `start()` after you call `stop()` the listeners will start piling up.', 'commenter': 'dineshjoshi'}]"
1454,src/java/org/apache/cassandra/streaming/StreamManager.java,"@@ -213,6 +226,98 @@ private static double calculateEffectiveRateInBytes(double throughput)
     private final Map<UUID, StreamResultFuture> initiatorStreams = new NonBlockingHashMap<>();
     private final Map<UUID, StreamResultFuture> followerStreams = new NonBlockingHashMap<>();
 
+    private final Map<UUID, StreamingState> states = new NonBlockingHashMap<>();
+    private volatile ScheduledFuture<?> cleanup = null;
+
+    public StreamManager()
+    {
+        addListener(new StreamListener()
+        {
+            @Override
+            public void onRegister(StreamResultFuture result)
+            {
+                StreamingState state = new StreamingState(result);
+                StreamingState previous = states.putIfAbsent(state.getId(), state);
+                if (previous == null)
+                {
+                    state.phase.start();
+                    result.addEventListener(state);
+                }
+                else
+                {
+                    logger.warn(""Duplicate streaming states detected for id {}"", state.getId());
+                }
+            }
+        });
+    }
+
+    public void start()
+    {
+        this.cleanup = ScheduledExecutors.optionalTasks.scheduleAtFixedRate(this::cleanup, 0,","[{'comment': 'So I am not sure if this will be an issue but avoid `scheduleAtFixedRate` and prefer `scheduleWithFixedDelay` just in case that  your `cleanup` method takes longer than the interval period to run. The latter would ensure that you would run at a fixed interval after completing the last execution.', 'commenter': 'dineshjoshi'}]"
1454,src/java/org/apache/cassandra/streaming/StreamingState.java,"@@ -0,0 +1,358 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.streaming;
+
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.net.InetSocketAddress;
+import java.util.Collection;
+import java.util.EnumMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.virtual.SimpleDataSet;
+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;
+import org.apache.cassandra.utils.Clock;
+import org.assertj.core.util.Throwables;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+public class StreamingState implements StreamEventHandler
+{
+    public enum State
+    {INIT, START, SUCCESS, FAILURE}
+
+    private final long createdAtMillis = Clock.Global.currentTimeMillis();
+
+    private final UUID id;
+    private final boolean follower;
+    private final StreamOperation operation;
+    // TODO is this changed after init?  looks like it is based off sessions which get added later?
+    private Set<InetSocketAddress> peers;
+    private Sessions sessions = Sessions.EMPTY;
+
+    private State state = State.INIT;
+    private String completeMessage = null;
+
+    private final long[] stateTimesNanos;
+    private volatile long lastUpdatedAtNanos;
+
+    // API for state changes
+    public final Phase phase = new Phase();
+
+    public StreamingState(StreamResultFuture result)
+    {
+        StreamCoordinator coordinator = result.getCoordinator();
+        this.id = result.planId;
+        this.operation = result.getCurrentState().streamOperation;
+        this.follower = coordinator.isFollower();
+        this.peers = coordinator.getPeers();
+        this.stateTimesNanos = new long[State.values().length];
+        stateTimesNanos[0] = Clock.Global.nanoTime();
+    }
+
+    public UUID getId()","[{'comment': ""So Cassandra doesn't use the `getXXX` naming. I'd rename this to `id()` instead of `getId()`"", 'commenter': 'dineshjoshi'}, {'comment': 'we are not actually consistent with this, and depends on the authors/reviewers involved', 'commenter': 'dcapwell'}]"
1454,src/java/org/apache/cassandra/streaming/StreamingState.java,"@@ -0,0 +1,358 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.streaming;
+
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.net.InetSocketAddress;
+import java.util.Collection;
+import java.util.EnumMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.virtual.SimpleDataSet;
+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;
+import org.apache.cassandra.utils.Clock;
+import org.assertj.core.util.Throwables;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+public class StreamingState implements StreamEventHandler
+{
+    public enum State
+    {INIT, START, SUCCESS, FAILURE}
+
+    private final long createdAtMillis = Clock.Global.currentTimeMillis();
+
+    private final UUID id;
+    private final boolean follower;
+    private final StreamOperation operation;
+    // TODO is this changed after init?  looks like it is based off sessions which get added later?
+    private Set<InetSocketAddress> peers;
+    private Sessions sessions = Sessions.EMPTY;
+
+    private State state = State.INIT;
+    private String completeMessage = null;
+
+    private final long[] stateTimesNanos;
+    private volatile long lastUpdatedAtNanos;
+
+    // API for state changes
+    public final Phase phase = new Phase();
+
+    public StreamingState(StreamResultFuture result)
+    {
+        StreamCoordinator coordinator = result.getCoordinator();
+        this.id = result.planId;
+        this.operation = result.getCurrentState().streamOperation;
+        this.follower = coordinator.isFollower();
+        this.peers = coordinator.getPeers();
+        this.stateTimesNanos = new long[State.values().length];
+        stateTimesNanos[0] = Clock.Global.nanoTime();
+    }
+
+    public UUID getId()
+    {
+        return id;
+    }
+
+    public boolean isFollower()
+    {
+        return follower;
+    }
+
+    public StreamOperation getOperation()","[{'comment': '`getOperation()` -> `operation()`', 'commenter': 'dineshjoshi'}]"
1454,src/java/org/apache/cassandra/streaming/StreamingState.java,"@@ -0,0 +1,358 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.streaming;
+
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.net.InetSocketAddress;
+import java.util.Collection;
+import java.util.EnumMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.virtual.SimpleDataSet;
+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;
+import org.apache.cassandra.utils.Clock;
+import org.assertj.core.util.Throwables;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+public class StreamingState implements StreamEventHandler
+{
+    public enum State
+    {INIT, START, SUCCESS, FAILURE}
+
+    private final long createdAtMillis = Clock.Global.currentTimeMillis();
+
+    private final UUID id;
+    private final boolean follower;
+    private final StreamOperation operation;
+    // TODO is this changed after init?  looks like it is based off sessions which get added later?
+    private Set<InetSocketAddress> peers;
+    private Sessions sessions = Sessions.EMPTY;
+
+    private State state = State.INIT;
+    private String completeMessage = null;
+
+    private final long[] stateTimesNanos;
+    private volatile long lastUpdatedAtNanos;
+
+    // API for state changes
+    public final Phase phase = new Phase();
+
+    public StreamingState(StreamResultFuture result)
+    {
+        StreamCoordinator coordinator = result.getCoordinator();
+        this.id = result.planId;
+        this.operation = result.getCurrentState().streamOperation;
+        this.follower = coordinator.isFollower();
+        this.peers = coordinator.getPeers();
+        this.stateTimesNanos = new long[State.values().length];
+        stateTimesNanos[0] = Clock.Global.nanoTime();
+    }
+
+    public UUID getId()
+    {
+        return id;
+    }
+
+    public boolean isFollower()
+    {
+        return follower;
+    }
+
+    public StreamOperation getOperation()
+    {
+        return operation;
+    }
+
+    public Set<InetSocketAddress> getPeers()","[{'comment': '`getPeers()` -> `peers()`', 'commenter': 'dineshjoshi'}]"
1454,src/java/org/apache/cassandra/streaming/StreamingState.java,"@@ -0,0 +1,358 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.streaming;
+
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.net.InetSocketAddress;
+import java.util.Collection;
+import java.util.EnumMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.virtual.SimpleDataSet;
+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;
+import org.apache.cassandra.utils.Clock;
+import org.assertj.core.util.Throwables;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+public class StreamingState implements StreamEventHandler
+{
+    public enum State
+    {INIT, START, SUCCESS, FAILURE}
+
+    private final long createdAtMillis = Clock.Global.currentTimeMillis();
+
+    private final UUID id;
+    private final boolean follower;
+    private final StreamOperation operation;
+    // TODO is this changed after init?  looks like it is based off sessions which get added later?
+    private Set<InetSocketAddress> peers;
+    private Sessions sessions = Sessions.EMPTY;
+
+    private State state = State.INIT;
+    private String completeMessage = null;
+
+    private final long[] stateTimesNanos;
+    private volatile long lastUpdatedAtNanos;
+
+    // API for state changes
+    public final Phase phase = new Phase();
+
+    public StreamingState(StreamResultFuture result)
+    {
+        StreamCoordinator coordinator = result.getCoordinator();
+        this.id = result.planId;
+        this.operation = result.getCurrentState().streamOperation;
+        this.follower = coordinator.isFollower();
+        this.peers = coordinator.getPeers();
+        this.stateTimesNanos = new long[State.values().length];
+        stateTimesNanos[0] = Clock.Global.nanoTime();
+    }
+
+    public UUID getId()
+    {
+        return id;
+    }
+
+    public boolean isFollower()
+    {
+        return follower;
+    }
+
+    public StreamOperation getOperation()
+    {
+        return operation;
+    }
+
+    public Set<InetSocketAddress> getPeers()
+    {
+        return peers;
+    }
+
+    public State getState()","[{'comment': '`getState()` -> `state()`', 'commenter': 'dineshjoshi'}]"
1454,src/java/org/apache/cassandra/streaming/StreamingState.java,"@@ -0,0 +1,358 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.streaming;
+
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.net.InetSocketAddress;
+import java.util.Collection;
+import java.util.EnumMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.virtual.SimpleDataSet;
+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;
+import org.apache.cassandra.utils.Clock;
+import org.assertj.core.util.Throwables;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+public class StreamingState implements StreamEventHandler
+{
+    public enum State
+    {INIT, START, SUCCESS, FAILURE}
+
+    private final long createdAtMillis = Clock.Global.currentTimeMillis();
+
+    private final UUID id;
+    private final boolean follower;
+    private final StreamOperation operation;
+    // TODO is this changed after init?  looks like it is based off sessions which get added later?
+    private Set<InetSocketAddress> peers;
+    private Sessions sessions = Sessions.EMPTY;
+
+    private State state = State.INIT;
+    private String completeMessage = null;
+
+    private final long[] stateTimesNanos;
+    private volatile long lastUpdatedAtNanos;
+
+    // API for state changes
+    public final Phase phase = new Phase();
+
+    public StreamingState(StreamResultFuture result)
+    {
+        StreamCoordinator coordinator = result.getCoordinator();
+        this.id = result.planId;
+        this.operation = result.getCurrentState().streamOperation;
+        this.follower = coordinator.isFollower();
+        this.peers = coordinator.getPeers();
+        this.stateTimesNanos = new long[State.values().length];
+        stateTimesNanos[0] = Clock.Global.nanoTime();
+    }
+
+    public UUID getId()
+    {
+        return id;
+    }
+
+    public boolean isFollower()
+    {
+        return follower;
+    }
+
+    public StreamOperation getOperation()
+    {
+        return operation;
+    }
+
+    public Set<InetSocketAddress> getPeers()
+    {
+        return peers;
+    }
+
+    public State getState()
+    {
+        return state;
+    }
+
+    public Sessions getSessions()
+    {
+        return sessions;
+    }
+
+    public boolean isComplete()
+    {
+        switch (state)
+        {
+            case SUCCESS:
+            case FAILURE:
+                return true;
+            default:
+                return false;
+        }
+    }
+
+    public StreamResultFuture getFuture()","[{'comment': '`getFuture()` -> `future()`', 'commenter': 'dineshjoshi'}]"
1454,src/java/org/apache/cassandra/streaming/StreamingState.java,"@@ -0,0 +1,358 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.streaming;
+
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.net.InetSocketAddress;
+import java.util.Collection;
+import java.util.EnumMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.virtual.SimpleDataSet;
+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;
+import org.apache.cassandra.utils.Clock;
+import org.assertj.core.util.Throwables;
+import org.checkerframework.checker.nullness.qual.Nullable;
+
+public class StreamingState implements StreamEventHandler
+{
+    public enum State
+    {INIT, START, SUCCESS, FAILURE}
+
+    private final long createdAtMillis = Clock.Global.currentTimeMillis();
+
+    private final UUID id;
+    private final boolean follower;
+    private final StreamOperation operation;
+    // TODO is this changed after init?  looks like it is based off sessions which get added later?
+    private Set<InetSocketAddress> peers;
+    private Sessions sessions = Sessions.EMPTY;
+
+    private State state = State.INIT;
+    private String completeMessage = null;
+
+    private final long[] stateTimesNanos;
+    private volatile long lastUpdatedAtNanos;
+
+    // API for state changes
+    public final Phase phase = new Phase();
+
+    public StreamingState(StreamResultFuture result)
+    {
+        StreamCoordinator coordinator = result.getCoordinator();
+        this.id = result.planId;
+        this.operation = result.getCurrentState().streamOperation;
+        this.follower = coordinator.isFollower();
+        this.peers = coordinator.getPeers();
+        this.stateTimesNanos = new long[State.values().length];
+        stateTimesNanos[0] = Clock.Global.nanoTime();
+    }
+
+    public UUID getId()
+    {
+        return id;
+    }
+
+    public boolean isFollower()
+    {
+        return follower;
+    }
+
+    public StreamOperation getOperation()
+    {
+        return operation;
+    }
+
+    public Set<InetSocketAddress> getPeers()
+    {
+        return peers;
+    }
+
+    public State getState()
+    {
+        return state;
+    }
+
+    public Sessions getSessions()
+    {
+        return sessions;
+    }
+
+    public boolean isComplete()
+    {
+        switch (state)
+        {
+            case SUCCESS:
+            case FAILURE:
+                return true;
+            default:
+                return false;
+        }
+    }
+
+    public StreamResultFuture getFuture()
+    {
+        if (follower)
+            return StreamManager.instance.getReceivingStream(id);
+        else
+            return StreamManager.instance.getInitiatorStream(id);
+    }
+
+    public float getProgress()","[{'comment': '`getProgress()` -> `progress()`. Please change this in the rest of the class.', 'commenter': 'dineshjoshi'}]"
1454,src/java/org/apache/cassandra/streaming/StreamManager.java,"@@ -213,6 +226,96 @@ private static double calculateEffectiveRateInBytes(double throughput)
     private final Map<UUID, StreamResultFuture> initiatorStreams = new NonBlockingHashMap<>();
     private final Map<UUID, StreamResultFuture> followerStreams = new NonBlockingHashMap<>();
 
+    private final Map<UUID, StreamingState> states = new NonBlockingHashMap<>();
+    private final StreamListener listener = new StreamListener()
+    {
+        @Override
+        public void onRegister(StreamResultFuture result)
+        {
+            StreamingState state = new StreamingState(result);
+            StreamingState previous = states.putIfAbsent(state.id(), state);
+            if (previous == null)
+            {
+                state.phase.start();
+                result.addEventListener(state);
+            }
+            else
+            {
+                logger.warn(""Duplicate streaming states detected for id {}"", state.id());
+            }
+        }
+    };
+    private volatile ScheduledFuture<?> cleanup = null;
+
+    public void start()
+    {
+        addListener(listener);
+        this.cleanup = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(this::cleanup, 0,
+                                                                               DatabaseDescriptor.getStreamingStateCleanupInterval().toNanoseconds(),
+                                                                               TimeUnit.NANOSECONDS);
+    }
+
+    public void stop()
+    {
+        removeListener(listener);
+        ScheduledFuture<?> cleanup = this.cleanup;
+        if (cleanup != null)
+            cleanup.cancel(false);
+    }
+
+    public Collection<StreamingState> getStreamingStates()
+    {
+        return states.values();
+    }
+
+    public StreamingState getStreamingState(UUID id)
+    {
+        return states.get(id);
+    }
+
+    @VisibleForTesting
+    public void putStreamingState(StreamingState state)
+    {
+        StreamingState previous = states.putIfAbsent(state.id(), state);
+        if (previous != null)
+            throw new AssertionError(""StreamPlan id "" + state.id() + "" already exists"");
+    }
+
+    @VisibleForTesting
+    public void cleanup()","[{'comment': 'Can you use an expiring cache (see `ActiveRepairService.repairStatusByCmd`) with an expiration of `streaming_state_expires`? I think this would be simpler then cleaning up entries periodically.', 'commenter': 'pauloricardomg'}]"
1488,test/unit/org/apache/cassandra/tools/ToolRunner.java,"@@ -216,6 +216,8 @@ public static ObservableTool invokeAsync(Map<String, String> env, InputStream st
         ProcessBuilder pb = new ProcessBuilder(args);
         if (env != null && !env.isEmpty())
             pb.environment().putAll(env);
+        String jvmOpts = pb.environment().getOrDefault(""JVM_OPTS"", """") + "" -Dcassandra.disable_tcactive_openssl=true"";","[{'comment': 'why also put `-Dcassandra.disable_tcactive_openssl=true` here, as well as into the build.xml ?\r\n e.g. see https://github.com/apache/cassandra/pull/1488/files#diff-766797f233c18114f9499750cf1ffbf3829aeea50283850619c01bd173132021R1488', 'commenter': 'michaelsembwever'}, {'comment': 'well, because we want the subprocess also use that property - it is not inherited automatically, right?', 'commenter': 'jacek-lewandowski'}]"
1488,src/java/org/apache/cassandra/io/util/PathUtils.java,"@@ -63,14 +63,16 @@
     private static final Set<StandardOpenOption> READ_WRITE_OPTIONS = unmodifiableSet(EnumSet.of(READ, WRITE, CREATE));
     private static final FileAttribute<?>[] NO_ATTRIBUTES = new FileAttribute[0];
 
+    private static final boolean USE_NIX_RECURSIVE_DELETE = CassandraRelevantProperties.USE_NIX_RECURSIVE_DELETE.getBoolean();","[{'comment': 'Could this condition be automatic on *nix, instead of explicit via a property flag?\r\n\r\nIs this applicable to runtime as well as tests?', 'commenter': 'michaelsembwever'}, {'comment': 'I wanted to make this a simple fix for testings - I guess changing that in general would require some extended verification', 'commenter': 'jacek-lewandowski'}, {'comment': 'fair enough. maybe a comment that we eventually want to hard code it to true? \r\n(and how we will encourage testing/verification of it?)', 'commenter': 'michaelsembwever'}, {'comment': 'Why alias this locally inside PathUtils.java instead of just calling the ``CassandraRelevantProperties.USE_NIX_RECURSIVE_DELETE.getBoolean()`` on each access?', 'commenter': 'jmckenzie-dev'}, {'comment': ""Looking into other classes I perceived it as a convention, but I'm not opposed to change that"", 'commenter': 'jacek-lewandowski'}, {'comment': 'Eh, either way is fine. Just curious.', 'commenter': 'jmckenzie-dev'}]"
1488,build.xml,"@@ -888,8 +888,7 @@
     <!--
         The build target builds all the .class files
     -->
-    <target name=""build"" depends=""resolver-retrieve-build,build-project,checkstyle"" description=""Compile Cassandra classes""/>
-    <target name=""_build_unsafe"" depends=""resolver-retrieve-build,build-project"" description=""Compile Cassandra classes without checks""/>
+    <target name=""build"" depends=""resolver-retrieve-build,build-project"" description=""Compile Cassandra classes""/>","[{'comment': 'a concern i have is that by removing `checkstyle` from `build`, `rat-check` from `build-project`, and `eclipse-warnings` from `jar`, we are less likely to be catching these failures before commits are pushed. because folk are not often running `artifacts` locally. even if CI catches it, it is still nice to have a faster dev feedback loop on such failures. also worth noting that circleci does not run artifacts.\r\n\r\nso the `_build_unsafe` and `*.skip` approach has appeal to me. leaning me towards instead adding the appropriate skip flags to all the ant calls in https://github.com/apache/cassandra-builds/blob/trunk/build-scripts/cassandra-test.sh and https://github.com/apache/cassandra-builds/blob/trunk/build-scripts/cassandra-dtest-pytest.sh', 'commenter': 'michaelsembwever'}, {'comment': 'Actually, they are explicitly added to `jar`: https://github.com/apache/cassandra/pull/1488/commits/04bc756409b99a34078f68c271df1f913098385b#diff-766797f233c18114f9499750cf1ffbf3829aeea50283850619c01bd173132021R1179\r\n\r\nSo the check will surely run in beginning of CI build\r\n\r\n', 'commenter': 'jacek-lewandowski'}]"
1488,ide/idea/workspace.xml,"@@ -353,4 +353,4 @@
       </provider>
     </entry>
   </component>
-</project>
+</project>","[{'comment': 'Why drop whitespace at end of file?', 'commenter': 'jmckenzie-dev'}]"
1488,src/java/org/apache/cassandra/config/CassandraRelevantProperties.java,"@@ -577,5 +581,4 @@ public boolean isPresent()
     {
         return System.getProperties().containsKey(key);
     }
-}
-
+}","[{'comment': 'Why the drop of whitespace at end of file?', 'commenter': 'jmckenzie-dev'}]"
1488,src/java/org/apache/cassandra/io/util/PathUtils.java,"@@ -64,14 +64,16 @@
     private static final Set<StandardOpenOption> READ_WRITE_OPTIONS = unmodifiableSet(EnumSet.of(READ, WRITE, CREATE));
     private static final FileAttribute<?>[] NO_ATTRIBUTES = new FileAttribute[0];
 
+    private static final boolean USE_NIX_RECURSIVE_DELETE = CassandraRelevantProperties.USE_NIX_RECURSIVE_DELETE.getBoolean();
+
     private static final Logger logger = LoggerFactory.getLogger(PathUtils.class);
     private static final NoSpamLogger nospam1m = NoSpamLogger.getLogger(logger, 1, TimeUnit.MINUTES);
 
     private static Consumer<Path> onDeletion = path -> {
         if (StorageService.instance.isDaemonSetupCompleted())
             setDeletionListener(ignore -> {});
-        else
-            logger.info(""Deleting file during startup: {}"", path);
+        else if (logger.isTraceEnabled())","[{'comment': 'Not for this patch, but we should create a follow up ticket: we should wrap this in some kind of util method instead of having this if / log at trace pattern peppered throughout the codebase. Something like `FBUtilities.logIfTrace(Logger logger, string msg)`.', 'commenter': 'jmckenzie-dev'}, {'comment': 'Obviously this is not needed in this case.\r\n\r\nHowever, in general case we should use `if` when:\r\n```java\r\nlog.trace(""message {}"", someHeavyToString());\r\n```\r\n\r\nif we go with a method like `logIfTrace` we would have to either evaluate that `someHeavyToString()` before going to that method regardless trace is enable or not (which undermine the existence of `if` at all) or we would have to pass a lambda, which is also much heavier than just checking `logger.isTraceEnabled`.\r\n\r\nI think the rules should be clear there - unless we do explicit string concatenation (which enforces calling `toString()` for each element) or we call something more than just trivial getters, we should have `if`, otherwise we can skip it because it brings nothing.\r\n\r\nOn the other hand, `Logger` has methods with signatures:\r\n```java\r\ntrace(String msg, Object ... paramas);\r\ntrace(String msg, Throwable t);\r\n```\r\nso basically if we want to pass a throwable we cannot make use of passing string format parameters as objects without the need to early call their `toString()` and actually doing manual string concatenation. \r\n\r\nin particular, there is lack of a method like:\r\n```java\r\ntrace(String msg, Throwable t, Object ... params)\r\n```\r\n\r\nSuch a method could be useful in that context.\r\n', 'commenter': 'jacek-lewandowski'}, {'comment': 'Really good point, and it\'d split the idiom if we had ""string interpolation goes this route, non goes that route"". I\'ve spent too much time working in C# w/Conditional runtime method attributes. :)\r\n\r\nCould _probably_ rely on hotspot to realize don\'t have tracing enabled and optimize that path away but not worth the UX / idiom split IMO.', 'commenter': 'jmckenzie-dev'}]"
1488,src/java/org/apache/cassandra/io/util/PathUtils.java,"@@ -310,13 +312,39 @@ public static Throwable delete(Path file, Throwable accumulate, @Nullable RateLi
         return accumulate;
     }
 
+    private static void deleteRecursiveUsingNixCommand(Path path, boolean quietly)
+    {
+        try
+        {
+            int result = Runtime.getRuntime().exec(new String[]{ ""rm"", quietly ? ""-rf"" : ""-r"", path.toAbsolutePath().toString() }).waitFor();","[{'comment': 'nit: build the command and cache in a string locally so you eliminate the potential defect / error of us logging a command that differs from the one we ran.', 'commenter': 'jmckenzie-dev'}]"
1488,src/java/org/apache/cassandra/io/util/PathUtils.java,"@@ -310,13 +312,39 @@ public static Throwable delete(Path file, Throwable accumulate, @Nullable RateLi
         return accumulate;
     }
 
+    private static void deleteRecursiveUsingNixCommand(Path path, boolean quietly)","[{'comment': ""May want to javadoc why this method exists and why we're using it for future maintainers. Also nit: consider renaming to `deleteRecursiveFast` which would leave us flexibility to switch on platform for faster deletes in the future if (for some bizarre reason) the rm approach ends up slower. Thinking OSX, WSL v2, etc."", 'commenter': 'jmckenzie-dev'}, {'comment': ""We always want to that fast don't we? 😄 \r\n\r\nBut this is a private method named after what it really does. And then we have a public remove method which decides which way to delete files should be chosen."", 'commenter': 'jacek-lewandowski'}, {'comment': ""Well, fair, nobody wants delete to be _slow_. But the reason this method exists is because on a specific platform it's significantly faster, and the only way a future maintainer would know that is an anthropological dig w/JIRA comments and descriptions to determine why something was changed. I'm fine leaving the name as is, but commenting why this seemingly redundant functionality was introduced would be nice."", 'commenter': 'jmckenzie-dev'}]"
1488,src/java/org/apache/cassandra/net/MessagingService.java,"@@ -233,6 +234,8 @@ public static int getVersionOrdinal(int version)
         return ordinal;
     }
 
+    public final static boolean NON_GRACEFUL_SHUTDOWN = Boolean.getBoolean(""cassandra.test.messagingService.nonGracefulShutdown"");","[{'comment': 'Should this be a `CassandraRelevantProperties`?', 'commenter': 'jmckenzie-dev'}, {'comment': '@josh-mckenzie great catch, yes, it definitely should be there. If we managed to have the second reviewer for CASSANDRA-17797 and we merged that work, this would fail the build :)', 'commenter': 'smiklosovic'}, {'comment': 'Quite interesting question. This is something to be used only in tests. Perhaps something overly contrived but maybe test properties should require something like `cassandra.test` property to be set in order to make them get assigned with a non-default value? (I mean, set `cassandra.test` to unlock those properties). Perhaps something for CASSANDRA-17797 @smiklosovic ', 'commenter': 'jacek-lewandowski'}, {'comment': 'The patch we work on is gathering all properties, irrelevant what they are for, in one place. I understand these are just test properties but having them somewhere separately, for example, in test source, would mean that we would start to reference test class in the prod code. Right? Not good. Having test code referencing prod code where these test properties are is better.\r\n\r\nI am not sure how to reply to your idea about `cassandra.test`. I think that in general it is good idea to have them with that prefix, yes. Otherwise we are not completely sure, at the first glance, if there is not any comment, what that property is actually used for / upon.\r\n\r\nIn the long run, we would like to have all test properties with such prefix (it is really messy right now) but the first task is to manage the current ""chaos"" with having them scattered across all the source code. As of now, nobody has a definitive answer what all properties we use ....\r\n\r\nI think test properties with some value should be set as the very first thing in the test class?  So the defaults would be overridden (if desirable) by a tester upon starting that test when this is `final static`. If the value of that property is meant to be changed during the test then this needs to be fixed on the code level to resolve that property dynamically as it is called.', 'commenter': 'smiklosovic'}, {'comment': 'I just meant to prevent setting those properties to some non-default values in production, that is, you can only override the defaults if some additional property like, `-Dcassandra.test=true` is set', 'commenter': 'jacek-lewandowski'}, {'comment': 'ah right ...  @Mmuzaf this is an interesting point to take into account, we might deal with this later I guess.', 'commenter': 'smiklosovic'}]"
1488,src/java/org/apache/cassandra/net/MessagingService.java,"@@ -527,11 +530,17 @@ public Future<Void> maybeReconnectWithNewIp(InetAddressAndPort address, InetAddr
      */
     public void shutdown()
     {
-        shutdown(1L, MINUTES, true, true);
+        if (NON_GRACEFUL_SHUTDOWN)
+            // this branch is used in unit-tests when we really never restart a node and shutting down means the end of test
+            shutdown(1L, MINUTES, false, true, false);
+        else
+            shutdown(1L, MINUTES, true, true, true);
     }
 
-    public void shutdown(long timeout, TimeUnit units, boolean shutdownGracefully, boolean shutdownExecutors)
+    public void shutdown(long timeout, TimeUnit units, boolean shutdownGracefully, boolean shutdownExecutors, boolean blocking)","[{'comment': 'I think this would be cleaner to break into 2 methods, `shutdownGracefully()` and `shutdownAbruptly()` or something, rather than going the multi-bool precondition most of the method is dead on various branches approach we have here.', 'commenter': 'jmckenzie-dev'}, {'comment': 'Yes, I agree', 'commenter': 'jacek-lewandowski'}]"
1488,test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java,"@@ -184,6 +184,12 @@
         private INodeProvisionStrategy.Strategy nodeProvisionStrategy = INodeProvisionStrategy.Strategy.MultipleNetworkInterfaces;
         private ShutdownExecutor shutdownExecutor = DEFAULT_SHUTDOWN_EXECUTOR;
 
+        {","[{'comment': ""Is there a reason you went w/the instance initializer vs. static initializer? I'd expect the latter to appropriately nuke the param for everything created inside the JVM context but I could be missing something."", 'commenter': 'jmckenzie-dev'}, {'comment': 'No particular reason AFAIR but the cluster initialization seemed more consistent to me that a static initializer because those properties are really relevant for the instance rather than static class. If we ever come up with the ability to pass individual system properties to node instances in JVM dtests, setting properties in a static initializer will become not relevant a all', 'commenter': 'jacek-lewandowski'}]"
1488,src/java/org/apache/cassandra/config/CassandraRelevantProperties.java,"@@ -584,4 +612,3 @@ public boolean isPresent()
         return System.getProperties().containsKey(key);
     }
 }
-","[{'comment': '@jacek-lewandowski this should be probably undeleted.', 'commenter': 'smiklosovic'}]"
1535,conf/cassandra.yaml,"@@ -1251,6 +1251,8 @@ server_encryption_options:
   # Set to a valid keystore if internode_encryption is dc, rack or all
   keystore: conf/.keystore
   keystore_password: cassandra
+  outbound_keystore: conf/.keystore
+  outbound_keystore_password: cassandra","[{'comment': 'Curious here. Do we plan to split those keystores? I see they are the same file for now. Is that expected?', 'commenter': 'bbotella'}, {'comment': '@bbotella  we do plan to split them. I just kept it as same in the template Cassandra.yaml file, but I can change it to a different dummy value if you think that is necessary.', 'commenter': 'jyothsnakonisa'}, {'comment': 'No need for a change. I was just curious.', 'commenter': 'bbotella'}]"
1535,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -120,7 +115,9 @@ public String description()
         REQUIRE_CLIENT_AUTH(""require_client_auth""),
         REQUIRE_ENDPOINT_VERIFICATION(""require_endpoint_verification""),
         ENABLED(""enabled""),
-        OPTIONAL(""optional"");
+        OPTIONAL(""optional""),
+        OUTBOUND_KEYSTORE(""outbound_keystore""),
+        OUTBOUND_KEYSTORE_PASSWORD(""outbound_keystore_password"");","[{'comment': 'Nit: Have we considered renaming the current KEYSTORE key in lines 106 and 107? as opposed to the newly created OUTBOND_KEYSTORE ones?', 'commenter': 'bbotella'}, {'comment': 'We want to make as minimal change as possible, renaming the current keystore would be a huge change.', 'commenter': 'jyothsnakonisa'}]"
1535,src/java/org/apache/cassandra/config/EncryptionOptions.java,"@@ -594,15 +595,20 @@ public int hashCode()
         public final InternodeEncryption internode_encryption;
         @Replaces(oldName = ""enable_legacy_ssl_storage_port"", deprecated = true)
         public final boolean legacy_ssl_storage_port_enabled;
+        public final String outbound_keystore;
+        public final String outbound_keystore_password;
 
         public ServerEncryptionOptions()
         {
             this.internode_encryption = InternodeEncryption.none;
             this.legacy_ssl_storage_port_enabled = false;
+            this.outbound_keystore = ""conf/.keystore"";
+            this.outbound_keystore_password = ""cassandra"";","[{'comment': ""Can't we get this from the config yaml file? Having them here hardcoded seems redundant."", 'commenter': 'bbotella'}, {'comment': 'I followed the existing pattern, may be its worth looking into making this change.', 'commenter': 'jyothsnakonisa'}]"
1535,src/java/org/apache/cassandra/security/DisableSslContextFactory.java,"@@ -36,6 +36,12 @@ protected TrustManagerFactory buildTrustManagerFactory() throws SSLException
         throw new UnsupportedOperationException();
     }
 
+    @Override
+    protected KeyManagerFactory buildOutboundKeyManagerFactory() throws SSLException
+    {
+        throw new UnsupportedOperationException();","[{'comment': 'Nit: What about adding a descriptive text to why it is unsupported, or providing some context?', 'commenter': 'bbotella'}, {'comment': '@bbotella I am not entirely sure why we have this class, but looks like this is used to disable SSL during Dtests. I added just a method in this class but this class is already present.', 'commenter': 'jyothsnakonisa'}]"
1535,src/java/org/apache/cassandra/security/FileBasedSslContextFactory.java,"@@ -174,6 +168,28 @@ protected TrustManagerFactory buildTrustManagerFactory() throws SSLException
         }
     }
 
+    private KeyManagerFactory getKeyManagerFactory(final String keystore, final String keystorePassword) throws SSLException
+    {
+        try (InputStream ksf = Files.newInputStream(Paths.get(keystore)))
+        {
+            final String algorithm = this.algorithm == null ? KeyManagerFactory.getDefaultAlgorithm() : this.algorithm;","[{'comment': 'Maybe a debug log statement mentioning that we are falling back to a default algorithm?', 'commenter': 'bbotella'}]"
1535,conf/cassandra.yaml,"@@ -1251,6 +1251,8 @@ server_encryption_options:
   # Set to a valid keystore if internode_encryption is dc, rack or all
   keystore: conf/.keystore
   keystore_password: cassandra
+  outbound_keystore: conf/.keystore
+  outbound_keystore_password: cassandra
   # Verify peer server certificates
   require_client_auth: false","[{'comment': '@jyothsnakonisa Do we have to make any changes to [OutboundConnectionInitiator](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundConnectionInitiator.java#L205) or [InboundConnectionInitiator](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/InboundConnectionInitiator.java#L534) or here [SSLFactory](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/security/SSLFactory.java#L415)? ', 'commenter': 'maulin-vasavada'}, {'comment': ""@jyothsnakonisa Have you thought of any other naming convention than `outbound`  here? `Outbound` is too contextual, meaning to whoever knows this code well it makes perfect sense but not otherwise - Ideally I wish we had named `server_encryption_options` as `internode_encryption_options` then we could have named this `client_keystore` and existing configuration as `server_keystore`. Just a thought based on my own confusion between Netty's Inbound/Outbound terminology +  Cassandra's Inbound/Outbound terminology."", 'commenter': 'maulin-vasavada'}, {'comment': 'Also, I had a question on whether we can put two private keys in the same keystore - one for the server mode and the other for the client mode for internode connection. Would java be able to use the correct certificate depending upon the context during the TLS handshake? \r\n\r\nUpdate: This concern is resolved as per the discussion in the jira ticket.', 'commenter': 'maulin-vasavada'}, {'comment': ""We don't have to make any change in `OutboundConnectionInitiator` or `InboundConnectionInitiator` or `SSLFactory`. `AbstractSslContextFactory` has the logic to create appropriate keystore based on server connection or client connection. "", 'commenter': 'jyothsnakonisa'}, {'comment': '@maulin-vasavada  I think renaming `server_encryption_options` would be quite a big change and might need major version change. We are planning to get this patch in 4.0.', 'commenter': 'jyothsnakonisa'}, {'comment': ""I experimented with having two private keys in same keystore but that did not work as the first key that is trusted by the other party is picked during SSL handshake and we don't have control over which key to be picked."", 'commenter': 'jyothsnakonisa'}, {'comment': ""Thanks @jyothsnakonisa for running an experiment with two private keys in the same keystore. As I noted in one of the updates above - I am convinced about having a separate keystore for outbound for client auth. Also, I agree that renaming server_encryption_options would be lot of changes, I was just providing reasoning on why 'outbound' word is too contextual and what could be `ideal`. No changes expected on that from this PR from my side.\r\n\r\nHowever, can you please take a look at one of the first comments in this series? - @jyothsnakonisa Do we have to make any changes to [OutboundConnectionInitiator](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundConnectionInitiator.java#L205) or [InboundConnectionInitiator](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/InboundConnectionInitiator.java#L534) or here [SSLFactory](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/security/SSLFactory.java#L415)?"", 'commenter': 'maulin-vasavada'}, {'comment': 'Sorry @jyothsnakonisa I missed to see your comment about Oubound/Inbound Connection Initator and SSLFactory changes comment. Please resolve this.', 'commenter': 'maulin-vasavada'}]"
1535,conf/cassandra.yaml,"@@ -1251,6 +1251,8 @@ server_encryption_options:
   # Set to a valid keystore if internode_encryption is dc, rack or all
   keystore: conf/.keystore
   keystore_password: cassandra
+  outbound_keystore: conf/.keystore","[{'comment': '@jyothsnakonisa We have to also update support for this configuration in [PEMBasedSslContextFactory](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/security/PEMBasedSslContextFactory.java), correct?', 'commenter': 'maulin-vasavada'}, {'comment': '@maulin-vasavada  thanks for pointing this out. I have updated the PR to make this change in `PEMBasedSslContextFactory`', 'commenter': 'jyothsnakonisa'}, {'comment': ""I don't have rights to resolve the discussion. Please do so for this."", 'commenter': 'maulin-vasavada'}]"
1535,src/java/org/apache/cassandra/security/PEMBasedSslContextFactory.java,"@@ -241,6 +262,39 @@ protected KeyManagerFactory buildKeyManagerFactory() throws SSLException
         }
     }
 
+    @Override
+    protected KeyManagerFactory buildOutboundKeyManagerFactory() throws SSLException {
+        try
+        {
+            if (hasOutboundKeystore())
+            {
+                if (maybeFileBasedOutboundPrivateKey)
+                {
+                    pemEncodedOutboundKey = readPEMFile(outboundKeystore); // read PEM from the file
+                }
+
+                KeyManagerFactory kmf = KeyManagerFactory.getInstance(
+                algorithm == null ? KeyManagerFactory.getDefaultAlgorithm() : algorithm);
+                KeyStore ks = buildKeyStore(pemEncodedOutboundKey, outboundKeyPassword);
+                if (!checkedExpiry)","[{'comment': '@jyothsnakonisa I think we should should separate the cert expiry checks for inbound vs outbound keys. According to this code once we verified client (or server) key certs it will skip the other. We should have `outboundCertExpiryCheck` sort of a flag on top of the existing one.', 'commenter': 'maulin-vasavada'}, {'comment': 'Thanks @maulin-vasavada, I addressed this issue in the following commit.', 'commenter': 'jyothsnakonisa'}]"
1535,src/java/org/apache/cassandra/security/ISslContextFactory.java,"@@ -99,6 +99,8 @@ default boolean hasKeystore()
         return true;
     }
 
+    boolean hasOutboundKeystore();","[{'comment': 'Can you please provide appropriate javadocs here?', 'commenter': 'maulin-vasavada'}]"
1535,test/conf/cassandra-sslcontextfactory.yaml,"@@ -67,6 +67,8 @@ server_encryption_options:
     internode_encryption: none
     keystore: conf/.keystore
     keystore_password: cassandra
+    outbound_keystore: conf/.keystore","[{'comment': 'Is the outbound keystore a required configuration? If not, can we have an example without it? We should make sure about the backwards compatibility.', 'commenter': 'maulin-vasavada'}, {'comment': '@maulin-vasavada We will maintain backward compatibility, the default value to the `outbound_keystore` is the `keystore` (like we have currently)', 'commenter': 'jyothsnakonisa'}]"
1535,test/unit/org/apache/cassandra/security/DefaultSslContextFactoryTest.java,"@@ -154,6 +162,54 @@ public void buildKeyManagerFactoryHappyPath() throws IOException
         Assert.assertTrue(defaultSslContextFactoryImpl3.checkedExpiry);
     }
 
+    @Test(expected = IOException.class)
+    public void buildOutboundKeyManagerFactoryWithInvalidKeystoreFile() throws IOException
+    {
+        Map<String, Object> config = new HashMap<>();
+        config.putAll(commonConfig);
+        config.put(""outbound_keystore"", ""/this/is/probably/not/a/file/on/your/test/machine"");
+
+        DefaultSslContextFactory defaultSslContextFactoryImpl = new DefaultSslContextFactory(config);
+        defaultSslContextFactoryImpl.checkedExpiry = false;
+        defaultSslContextFactoryImpl.buildKeyManagerFactory();
+    }
+
+    @Test(expected = IOException.class)
+    public void buildOutboundKeyManagerFactoryWithBadPassword() throws IOException
+    {
+        Map<String, Object> config = new HashMap<>();
+        config.putAll(commonConfig);
+        addOutboundKeystoreOptions(config);
+        config.put(""outbound_keystore_password"", ""HomeOfBadPasswords"");
+
+        DefaultSslContextFactory defaultSslContextFactoryImpl = new DefaultSslContextFactory(config);
+        defaultSslContextFactoryImpl.buildKeyManagerFactory();
+    }
+
+    @Test
+    public void buildOutboundKeyManagerFactoryHappyPath() throws IOException
+    {
+        Map<String, Object> config = new HashMap<>();
+        config.putAll(commonConfig);
+
+        DefaultSslContextFactory defaultSslContextFactoryImpl = new DefaultSslContextFactory(config);
+        // Make sure the exiry check didn't happen so far for the private key
+        Assert.assertFalse(defaultSslContextFactoryImpl.checkedExpiry);
+
+        addOutboundKeystoreOptions(config);
+        DefaultSslContextFactory defaultSslContextFactoryImpl2 = new DefaultSslContextFactory(config);
+        // Trigger the private key loading. That will also check for expired private key
+        defaultSslContextFactoryImpl2.buildOutboundKeyManagerFactory();
+        // Now we should have checked the private key's expiry
+        Assert.assertTrue(defaultSslContextFactoryImpl2.checkedExpiry);","[{'comment': 'This relates to my earlier comment on introducing a separate certExpiry check for outbound vs inbound. ', 'commenter': 'maulin-vasavada'}]"
1535,test/unit/org/apache/cassandra/security/DefaultSslContextFactoryTest.java,"@@ -154,6 +162,54 @@ public void buildKeyManagerFactoryHappyPath() throws IOException
         Assert.assertTrue(defaultSslContextFactoryImpl3.checkedExpiry);
     }
 
+    @Test(expected = IOException.class)
+    public void buildOutboundKeyManagerFactoryWithInvalidKeystoreFile() throws IOException
+    {
+        Map<String, Object> config = new HashMap<>();
+        config.putAll(commonConfig);
+        config.put(""outbound_keystore"", ""/this/is/probably/not/a/file/on/your/test/machine"");
+
+        DefaultSslContextFactory defaultSslContextFactoryImpl = new DefaultSslContextFactory(config);
+        defaultSslContextFactoryImpl.checkedExpiry = false;
+        defaultSslContextFactoryImpl.buildKeyManagerFactory();
+    }
+
+    @Test(expected = IOException.class)
+    public void buildOutboundKeyManagerFactoryWithBadPassword() throws IOException
+    {
+        Map<String, Object> config = new HashMap<>();
+        config.putAll(commonConfig);
+        addOutboundKeystoreOptions(config);
+        config.put(""outbound_keystore_password"", ""HomeOfBadPasswords"");
+
+        DefaultSslContextFactory defaultSslContextFactoryImpl = new DefaultSslContextFactory(config);
+        defaultSslContextFactoryImpl.buildKeyManagerFactory();
+    }
+
+    @Test
+    public void buildOutboundKeyManagerFactoryHappyPath() throws IOException","[{'comment': ""With these new tests we are testing outbound keymanager in isolation with inbound. However from the SslContextFactory class's perspective both (inbound and outbound) keystores exists together. Do you think it would be worth to add a test with both together? (Those tests can test for separate cert expiry checks etc)."", 'commenter': 'maulin-vasavada'}, {'comment': 'Added a check in the test case to check other flags. This will test the both the flags together.', 'commenter': 'jyothsnakonisa'}]"
1535,conf/cassandra.yaml,"@@ -1251,6 +1251,8 @@ server_encryption_options:
   # Set to a valid keystore if internode_encryption is dc, rack or all
   keystore: conf/.keystore
   keystore_password: cassandra
+#  outbound_keystore: conf/.keystore
+#  outbound_keystore_password: cassandra","[{'comment': 'Can we also add a comment describing these two properties, and maybe explain that when the `outbound_keystore` is not provided, it will default to `keystore` and when `outbound_keystore_password` is not provided it will default to `keystore_password`\r\n```suggestion\r\n#  outbound_keystore:\r\n#  outbound_keystore_password:\r\n```', 'commenter': 'frankgh'}]"
1537,src/java/org/apache/cassandra/cql3/statements/schema/AlterSchemaStatement.java,"@@ -46,7 +47,10 @@ protected AlterSchemaStatement(String keyspaceName)
 
     public void validate(ClientState state)
     {
-        // no-op; validation is performed while executing the statement, in apply()
+        // validation is performed while executing the statement, in apply()
+
+        // Cache our ClientState for use by guardrails
+        this.state = state;","[{'comment': 'this pattern keeps popping up... wondering if we should ""fix"" our apis to always provide this?', 'commenter': 'dcapwell'}]"
1537,src/java/org/apache/cassandra/db/guardrails/GuardrailsMBean.java,"@@ -211,6 +211,13 @@
      */
     void setUserTimestampsEnabled(boolean enabled);
 
+    /**
+     * Sets whether users can disable compression on tables
+     *
+     * @param enabled {@code true} if users can disable compression on a table, {@code false} otherwise.
+     */
+    void setUsersCanDisableCompression(boolean enabled);","[{'comment': 'no `get`/`is`?', 'commenter': 'dcapwell'}]"
1538,bin/cqlsh.py,"@@ -2384,7 +2441,8 @@ def main(options, hostname, port):
                       single_statement=options.execute,
                       request_timeout=options.request_timeout,
                       connect_timeout=options.connect_timeout,
-                      encoding=options.encoding)
+                      encoding=options.encoding,
+                      auth_provider=load_auth_provider(options))","[{'comment': 'Can you please provide any test cases and samples for these changes?', 'commenter': 'maulin-vasavada'}, {'comment': 'Will add tests in the vein of sslhandling.', 'commenter': 'bhouse99'}, {'comment': 'tests added', 'commenter': 'bhouse99'}, {'comment': 'thanks!', 'commenter': 'maulin-vasavada'}]"
1538,bin/cqlsh.py,"@@ -98,6 +99,33 @@
     ZIPLIB_DIRS = ()
 
 
+def parse_auth_provider_extended(option, opt_str, value, passed_parser):
+    splits = opt_str.split('.')
+    if len(splits) > 2:
+        raise optparse.OptionValueError(""Invalid --AuthProviderExtended argument only single period should be found: %s"" % opt_str)
+    if len(splits) < 2:
+        raise optparse.OptionValueError(""Invalid --AuthProviderExtended must provide a param using period: %s"" % opt_str)
+    key = splits[1]
+
+    if not hasattr(passed_parser.values, 'auth_provider_extended') or parser.values.auth_provider_extended is None:
+        passed_parser.values.auth_provider_extended = {}
+
+    passed_parser.values.auth_provider_extended[key] = value","[{'comment': 'The name `auth_provider_extended` is little confusing in my opinion. Since it is set of properties that the pluggable AuthProvider can use, would `auth_provider_properties` or `auth_provider_config` make more sense? ', 'commenter': 'maulin-vasavada'}, {'comment': 'Recently changed this to auth_provider_\r\nconfig.', 'commenter': 'bhouse99'}, {'comment': 'thanks! ', 'commenter': 'maulin-vasavada'}]"
1538,bin/cqlsh.py,"@@ -98,6 +99,33 @@
     ZIPLIB_DIRS = ()
 
 
+def parse_auth_provider_extended(option, opt_str, value, passed_parser):
+    splits = opt_str.split('.')
+    if len(splits) > 2:
+        raise optparse.OptionValueError(""Invalid --AuthProviderExtended argument only single period should be found: %s"" % opt_str)
+    if len(splits) < 2:
+        raise optparse.OptionValueError(""Invalid --AuthProviderExtended must provide a param using period: %s"" % opt_str)
+    key = splits[1]
+
+    if not hasattr(passed_parser.values, 'auth_provider_extended') or parser.values.auth_provider_extended is None:
+        passed_parser.values.auth_provider_extended = {}
+
+    passed_parser.values.auth_provider_extended[key] = value
+
+
+def add_auth_provider_extended_options(passed_parser, argv):
+    for arg in argv:
+        if arg.startswith('--AuthProviderExtended'):","[{'comment': 'Similar to my previous comment, would it make more sense to name this `AuthProviderProperty`/`AuthProviderConfig` (I believe this is a single property that can be overridden by the command line)?', 'commenter': 'maulin-vasavada'}, {'comment': 'Renamed to auth_provider_config.', 'commenter': 'bhouse99'}]"
1538,CHANGES.txt,"@@ -24,6 +24,7 @@
  * Add diagnostic events for guardrails (CASSANDRA-17197)
  * Pre hashed passwords in CQL (CASSANDRA-17334)
  * Increase cqlsh version (CASSANDRA-17432)
+ * Added cqlsh custom auth provider support (CASSANDRA-16456)","[{'comment': '@bhouse99 remove this line, I ll take care of that on the merge itself. No worries. We would have to rebase / fix it all over again as other people are adding stuff there in the meanwhile.', 'commenter': 'smiklosovic'}, {'comment': 'Kk, will do will avoid modifying in the future.', 'commenter': 'bhouse99'}]"
1538,bin/cqlsh.py,"@@ -405,7 +405,6 @@ def deserialize_date_fallback_int(byts, protocol_version):
     # Return cassandra.cqltypes.EMPTY instead of None for empty values
     cassandra.cqltypes.CassandraType.support_empty_values = True
 
-","[{'comment': 'should not be this kept?', 'commenter': 'smiklosovic'}]"
1538,pylib/cqlshlib/authproviderhandling.py,"@@ -0,0 +1,79 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import configparser
+from importlib import import_module
+
+def load_custom_auth_provider(config_file = None):
+    """"""
+    Function which loads a custom auth provider from available config.
+
+    Params:
+    * config_file ..: path to cqlsh config file (usually ~/.cqlshrc).
+
+    Will attempt to load a custom auth provider from available config file.
+    
+    Config file is expected to list module name /class in the *auth_provider*
+    section for dynamic loading (which is to be of type auth_provider)
+    
+    Additional params passed to the constructor of class should be specified
+    in the *auth_provider_config* section and can be freely named to match 
+    custom provider's expectation.
+
+    None is returned if no custom auth provider is found.
+
+    EXAMPLE  CQLSHRC: 
+    # .. inside cqlshrc file
+    
+    [auth_provider]
+    module = cassandra.auth
+    classname = PlainTextAuthProvider
+    
+    [auth_provider_config]
+    username = user1
+    password = password1
+    """"""
+
+    configs = configparser.ConfigParser()
+    configs.read(config_file)
+
+    def get_setting(section, option):
+        try:
+            return configs.get(section, option)
+        except configparser.Error:
+            return None
+    
+    def get_auth_provider_config_settings():
+        if 'auth_provider_config' in configs.sections():
+            return dict(configs.items(""auth_provider_config""))
+        else:
+            return None
+    
+    module_name = get_setting('auth_provider', 'module')
+    class_name = get_setting('auth_provider', 'classname')","[{'comment': 'what if there is no `class_name` but `module` is there? So `class_name` will be `None` but you are not checking it as you do upon `module_name`. Could you also add a test for that, please?', 'commenter': 'smiklosovic'}]"
1538,pylib/cqlshlib/authproviderhandling.py,"@@ -0,0 +1,79 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import configparser
+from importlib import import_module
+
+def load_custom_auth_provider(config_file = None):
+    """"""
+    Function which loads a custom auth provider from available config.
+
+    Params:
+    * config_file ..: path to cqlsh config file (usually ~/.cqlshrc).","[{'comment': 'the default path is actually `~/.cassandra/cqlshrc`', 'commenter': 'smiklosovic'}]"
1538,bin/cqlsh.py,"@@ -2091,7 +2095,6 @@ def is_file_secure(filename):
     # This is to allow ""sudo cqlsh"" to work with user owned credentials file.
     return (uid == 0 or st.st_uid == uid) and stat.S_IMODE(st.st_mode) & (stat.S_IRGRP | stat.S_IROTH) == 0
 
-","[{'comment': '@bhouse99 this line should not be removed. I think there is 2 lines policy in Python between methods.', 'commenter': 'smiklosovic'}]"
1538,bin/cqlsh.py,"@@ -2169,14 +2180,15 @@ def read_options(cmdlineargs, environment):
         credentials.read(options.credentials)
 
         # use the username from credentials file but fallback to cqlshrc if username is absent from the command line parameters
-        options.username = option_with_default(credentials.get, 'plain_text_auth', 'username', username_from_cqlshrc)
+        options.username = username_from_cqlshrc
 
     if not options.password:
         rawcredentials = configparser.RawConfigParser()
         rawcredentials.read(options.credentials)
 
         # handling password in the same way as username, priority cli > credentials > cqlshrc
         options.password = option_with_default(rawcredentials.get, 'plain_text_auth', 'password', password_from_cqlshrc)","[{'comment': '@bhouse99 any reason why you have not removed this line, similarly as it is done for username case above?', 'commenter': 'smiklosovic'}]"
1538,bin/cqlsh.py,"@@ -2169,14 +2180,15 @@ def read_options(cmdlineargs, environment):
         credentials.read(options.credentials)
 
         # use the username from credentials file but fallback to cqlshrc if username is absent from the command line parameters
-        options.username = option_with_default(credentials.get, 'plain_text_auth', 'username', username_from_cqlshrc)
+        options.username = username_from_cqlshrc","[{'comment': '@bhouse99 I do not get this change. So we are populating `options.username` with `username_from_cqlshrc`. So for what reason do we read `credentials.read(options.credentials)` 3 lines above? That seems to me like dead code?', 'commenter': 'smiklosovic'}]"
1555,test/distributed/org/apache/cassandra/distributed/action/GossipHelper.java,"@@ -222,7 +223,7 @@ public void accept(IInvokableInstance pullTo)
                 InetAddressAndPort endpoint = toCassandraInetAddressAndPort(pullFrom);
                 EndpointState state = Gossiper.instance.getEndpointStateForEndpoint(endpoint);
                 MigrationCoordinator.instance.reportEndpointVersion(endpoint, state);
-                MigrationCoordinator.instance.awaitSchemaRequests(TimeUnit.SECONDS.toMillis(10));
+                assertTrue(MigrationCoordinator.instance.awaitSchemaRequests(TimeUnit.SECONDS.toMillis(10)));","[{'comment': 'plz be kind and add a message ^_^', 'commenter': 'dcapwell'}]"
1555,test/distributed/org/apache/cassandra/distributed/impl/Instance.java,"@@ -541,7 +541,7 @@ public void startup(ICluster cluster)
                 StorageService.instance.registerDaemon(CassandraDaemon.getInstanceForTesting());
                 if (config.has(GOSSIP))
                 {
-                    MigrationManager.setUptimeFn(() -> TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startedAt));
+                    MigrationCoordinator.setUptimeFn(() -> TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startedAt));","[{'comment': ""spoke in slack, can you also fix the startedAt bug?  This is object init, which != uptime.\r\n\r\nWhen we run for 1m and restart, the restart should have uptime=0 but will be uptime=1m due to the fact we don't cleanup the object."", 'commenter': 'dcapwell'}]"
1556,src/java/org/apache/cassandra/schema/MigrationCoordinator.java,"@@ -90,15 +93,20 @@
     private static final Logger logger = LoggerFactory.getLogger(MigrationCoordinator.class);
     private static final Future<Void> FINISHED_FUTURE = ImmediateFuture.success(null);
 
+    private static LongSupplier getUptimeFn = () -> ManagementFactory.getRuntimeMXBean().getUptime();","[{'comment': 'this is dead code in your patch', 'commenter': 'dcapwell'}]"
1556,test/distributed/org/apache/cassandra/distributed/action/GossipHelper.java,"@@ -228,7 +229,7 @@ public void accept(IInvokableInstance pullTo)
                 InetAddressAndPort endpoint = toCassandraInetAddressAndPort(pullFrom);
                 EndpointState state = Gossiper.instance.getEndpointStateForEndpoint(endpoint);
                 Gossiper.instance.doOnChangeNotifications(endpoint, ApplicationState.SCHEMA, state.getApplicationState(ApplicationState.SCHEMA));
-                Schema.instance.waitUntilReady(Duration.ofSeconds(10));
+                assertTrue(""schema is ready"", Schema.instance.waitUntilReady(Duration.ofSeconds(10)));","[{'comment': 'you are a good person here, but not in 4.0?  why the hate of 4.0!?!?!!', 'commenter': 'dcapwell'}, {'comment': ""Puzzler. It was there, I'll put it back."", 'commenter': 'jonmeredith'}]"
1556,test/distributed/org/apache/cassandra/distributed/impl/Instance.java,"@@ -631,6 +633,7 @@ public void startup(ICluster cluster)
                 StorageService.instance.registerDaemon(CassandraDaemon.getInstanceForTesting());
                 if (config.has(GOSSIP))
                 {
+                    MigrationCoordinator.setUptimeFn(() -> TimeUnit.NANOSECONDS.toMillis(nanoTime() - startedAt));","[{'comment': ""spoke in slack, can you also fix the startedAt bug?  This is object init, which != uptime.\r\n\r\nWhen we run for 1m and restart, the restart should have uptime=0 but will be uptime=1m due to the fact we don't cleanup the object."", 'commenter': 'dcapwell'}]"
1556,src/java/org/apache/cassandra/schema/MigrationCoordinator.java,"@@ -467,16 +476,29 @@ void announce(UUID schemaVersion)
         SchemaDiagnostics.versionAnnounced(Schema.instance);
     }
 
-    private Future<Void> submitToMigrationIfNotShutdown(Runnable task)
+    private Future<?> submitToMigrationIfNotShutdown(Runnable task)
     {
-        if (executor.isShutdown() || executor.isTerminated())
+        boolean skipped = false;
+        try
         {
-            logger.info(""Skipped scheduled pulling schema from other nodes: the MIGRATION executor service has been shutdown."");
+            if (executor.isShutdown() || executor.isTerminated())
+            {
+                skipped = true;
+                return ImmediateFuture.success(null);
+            }
+            return executor.submit(task);
+        }
+        catch (RejectedExecutionException ex)
+        {
+            skipped = true;
             return ImmediateFuture.success(null);","[{'comment': ""shouldn't this return an failed future rather than success future?"", 'commenter': 'dcapwell'}, {'comment': '4.0 returned `null`.  The issue with this patch is that `executor.submit(task)` returns `Void`, which can only ever be `null`; so the caller has 0 way to know that the future was rejected; in 4.0 we could null check the future reference', 'commenter': 'dcapwell'}, {'comment': 'removing my comment due to the fact this looks to be the desired behavior of this function, so you are not changing anything, you are just being consistent with existing logic...', 'commenter': 'dcapwell'}]"
1567,src/java/org/apache/cassandra/db/compaction/CompactionController.java,"@@ -255,10 +255,7 @@ public LongPredicate getPurgeEvaluator(DecoratedKey key)
 
         for (SSTableReader sstable: filteredSSTables)
         {
-            // if we don't have bloom filter(bf_fp_chance=1.0 or filter file is missing),
-            // we check index file instead.
-            if (sstable.getBloomFilter() instanceof AlwaysPresentFilter && sstable.getPosition(key, SSTableReader.Operator.EQ, false) != null
-                || sstable.getBloomFilter().isPresent(key))
+            if (sstable.isPresent(key))","[{'comment': 'moved this logic into the sstable so it can be reused', 'commenter': 'dcapwell'}]"
1567,src/java/org/apache/cassandra/db/compaction/CompactionManager.java,"@@ -928,10 +929,10 @@ protected void runMayThrow()
         return futures;
     }
 
-    public void forceCompactionForTokenRange(ColumnFamilyStore cfStore, Collection<Range<Token>> ranges)
+    public void forceCompaction(ColumnFamilyStore cfStore, Supplier<Collection<SSTableReader>> sstablesFn, com.google.common.base.Predicate<SSTableReader> sstablesPredicate)","[{'comment': 'It is slightly weird to have both the `Supplier` and the `Predicate` here - but i guess its needed due to that `sstablesInBounds` method. Maybe we should add an assertion that `sstablesPredicate.test(..)` returns true for sstables from `sstablesFn` ', 'commenter': 'krummas'}, {'comment': 'not sure about this one, `sstablesFn` is called when compaction is paused, so if I call earlier than that could that not cause problems?  Unless I call early not sure how to get access to the SSTables', 'commenter': 'dcapwell'}, {'comment': 'I meant that after calling `sstablesFn.get()` in `taskCreator` we just `assert sstables.stream().allMatch(sstablesPredicate);`\r\n\r\nbut its fine leaving out as well', 'commenter': 'krummas'}, {'comment': 'added\r\n\r\n```\r\nList<SSTableReader> sstablesNotInPredicate = sstables.stream().filter(a -> ! sstablesPredicate.test(a)).collect(Collectors.toList());\r\n            assert sstablesNotInPredicate.isEmpty() : String.format(""Attempted to force compact %s, but predicate does not include"", sstablesNotInPredicate);\r\n```\r\n\r\nI prefer it show which sstables not included than existence', 'commenter': 'dcapwell'}, {'comment': 'this check broke some tests due to `(32, 31]` finding files that had bounds `(32, 32]`', 'commenter': 'dcapwell'}]"
1567,src/java/org/apache/cassandra/db/compaction/CompactionManager.java,"@@ -990,6 +996,23 @@ private static Collection<SSTableReader> sstablesInBounds(ColumnFamilyStore cfs,
         return sstables;
     }
 
+    public void forceCompactionForKey(ColumnFamilyStore cfStore, DecoratedKey key)
+    {
+        forceCompaction(cfStore, () -> sstablesWithKey(cfStore, key), sstable -> sstable.isPresent(key));
+    }
+
+    private static Collection<SSTableReader> sstablesWithKey(ColumnFamilyStore cfs, DecoratedKey key)
+    {
+        final Set<SSTableReader> sstables = new HashSet<>();
+        Iterable<SSTableReader> liveTables = cfs.getTracker().getView().select(SSTableSet.LIVE);","[{'comment': 'this could be\r\n```\r\nIterable<SSTableReader> liveTables = cfs.getTracker().getView().liveSSTablesInBounds(key.getToken().minKeyBound(), \r\n                                                                                     key.getToken().maxKeyBound());\r\n```\r\nto avoid false positives and to reduce the number of `.isPresent` checks below', 'commenter': 'krummas'}]"
1567,src/java/org/apache/cassandra/dht/Splitter.java,"@@ -92,6 +92,21 @@ else if (token.compareTo(unwrapped.left) < 0)
         return elapsedTokens;
     }
 
+    public Token next(Token token)","[{'comment': 'these are unused', 'commenter': 'krummas'}]"
1567,src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java,"@@ -2003,6 +2003,14 @@ public void addTo(Ref.IdentityCollection identities)
 
     }
 
+    public boolean isPresent(DecoratedKey key)","[{'comment': 'maybe rename this to indicate that there are no guarantees here - or invert it to say `isNotPresent` ?', 'commenter': 'krummas'}]"
1567,src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java,"@@ -2003,6 +2003,14 @@ public void addTo(Ref.IdentityCollection identities)
 
     }
 
+    public boolean isPresent(DecoratedKey key)
+    {
+        // if we don't have bloom filter(bf_fp_chance=1.0 or filter file is missing),
+        // we check index file instead.
+        IFilter bf = getBloomFilter();","[{'comment': ""```\r\n     [echo] Running Eclipse Code Analysis.  Output logged to /.../build/ecj/eclipse_compiler_checks.txt\r\n     [java] ----------\r\n     [java] 1. ERROR in /.../src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java (at line 2011)\r\n     [java]     return bf instanceof AlwaysPresentFilter && getPosition(key, Operator.EQ, false) != null || bf.isPresent(key);\r\n     [java]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n     [java] Potential resource leak: 'bf' may not be closed at this location\r\n```\r\n\r\ncan probably just avoid `getBloomFilter()` here"", 'commenter': 'krummas'}, {'comment': 'adding `eclipse-warnings` to my scripts so I run this more often!', 'commenter': 'dcapwell'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";","[{'comment': ""Unused, probably because, differently from `GuardrailMinimumReplicationFactorTest`, there isn't a `testConfigValidation` test method."", 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/db/guardrails/Guardrails.java,"@@ -333,6 +333,19 @@ public final class Guardrails implements GuardrailsMBean
                                : format(""The keyspace %s has a replication factor of %s, below the failure threshold of %s."",
                                         what, value, threshold));
 
+    /**
+     * Guardrail on the maximum replication factor.
+     */
+    public static final MaxThreshold maximumReplicationFactor =
+    new MaxThreshold(""maximum_replication_factor"",
+                  state -> CONFIG_PROVIDER.getOrCreate(state).getMaximumReplicationFactorWarnThreshold(),
+                  state -> CONFIG_PROVIDER.getOrCreate(state).getMaximumReplicationFactorFailThreshold(),
+                  (isWarning, what, value, threshold) ->
+                  isWarning ? format(""The keyspace %s has a replication factor of %s, above the warning threshold of %s."",
+                                     what, value, threshold)
+                            :format(""The keyspace %s has a replication factor of %s, above the failure threshold of %s."",
+                                    what, value, threshold));","[{'comment': 'Nit: misaligned\r\n```suggestion\r\n                            : format(""The keyspace %s has a replication factor of %s, above the failure threshold of %s."",\r\n                                     what, value, threshold));\r\n```', 'commenter': 'adelapena'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        assertFails(""CREATE KEYSPACE ks1 WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks1 has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        DatabaseDescriptor.setEndpointSnitch(snitch);
+        execute(""DROP KEYSPACE IF EXISTS ks1"");
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"",
+                    format(""The keyspace %s has a replication factor of 4, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertFails(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailAboveAlter() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        execute(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"",
+                    format(""The keyspace %s has a replication factor of 6, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+","[{'comment': 'Nit: unneeded blank line', 'commenter': 'adelapena'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        assertFails(""CREATE KEYSPACE ks1 WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks1 has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        DatabaseDescriptor.setEndpointSnitch(snitch);
+        execute(""DROP KEYSPACE IF EXISTS ks1"");
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"",
+                    format(""The keyspace %s has a replication factor of 4, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+","[{'comment': 'Nit: unneeded blank line.', 'commenter': 'adelapena'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        assertFails(""CREATE KEYSPACE ks1 WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks1 has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        DatabaseDescriptor.setEndpointSnitch(snitch);
+        execute(""DROP KEYSPACE IF EXISTS ks1"");
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"",
+                    format(""The keyspace %s has a replication factor of 4, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+","[{'comment': 'Nit: unneeded blank line.', 'commenter': 'adelapena'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        assertFails(""CREATE KEYSPACE ks1 WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks1 has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        DatabaseDescriptor.setEndpointSnitch(snitch);
+        execute(""DROP KEYSPACE IF EXISTS ks1"");
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+","[{'comment': 'Nit: unneeded blank line.', 'commenter': 'adelapena'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        assertFails(""CREATE KEYSPACE ks1 WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks1 has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        DatabaseDescriptor.setEndpointSnitch(snitch);
+        execute(""DROP KEYSPACE IF EXISTS ks1"");
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"",
+                    format(""The keyspace %s has a replication factor of 4, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertFails(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailAboveAlter() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        execute(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"",
+                    format(""The keyspace %s has a replication factor of 6, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFWarnFailBetween() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"",
+                    format(""The keyspace %s has a replication factor of 4, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFFailAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertFails(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 5}"",
+                    format(""he keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFFailAboveAlter() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        execute(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"");
+       assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 5}"",
+                   format(""he keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));","[{'comment': '```suggestion\r\n        assertFails(""ALTER KEYSPACE ks WITH replication = { \'class\': \'NetworkTopologyStrategy\', \'datacenter1\': 5}"",\r\n                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));\r\n```', 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/config/GuardrailsOptions.java,"@@ -82,6 +82,7 @@ public GuardrailsOptions(Config config)
         validatePercentageThreshold(config.data_disk_usage_percentage_warn_threshold, config.data_disk_usage_percentage_fail_threshold, ""data_disk_usage_percentage"");
         validateDataDiskUsageMaxDiskSize(config.data_disk_usage_max_disk_size);
         validateMinRFThreshold(config.minimum_replication_factor_warn_threshold, config.minimum_replication_factor_fail_threshold, ""minimum_replication_factor"");
+        validateMaxIntThreshold(config.maximum_replication_factor_warn_threshold, config.maximum_replication_factor_fail_threshold, ""maximum_replication_factor"");","[{'comment': 'Probably we should validate this against `default_keyspace_rf`, as we did with the guardrail for min RF.', 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/db/guardrails/Guardrails.java,"@@ -333,6 +333,19 @@ public final class Guardrails implements GuardrailsMBean
                                : format(""The keyspace %s has a replication factor of %s, below the failure threshold of %s."",
                                         what, value, threshold));
 
+    /**
+     * Guardrail on the maximum replication factor.
+     */
+    public static final MaxThreshold maximumReplicationFactor =
+    new MaxThreshold(""maximum_replication_factor"",
+                  state -> CONFIG_PROVIDER.getOrCreate(state).getMaximumReplicationFactorWarnThreshold(),
+                  state -> CONFIG_PROVIDER.getOrCreate(state).getMaximumReplicationFactorFailThreshold(),
+                  (isWarning, what, value, threshold) ->
+                  isWarning ? format(""The keyspace %s has a replication factor of %s, above the warning threshold of %s."",","[{'comment': 'May be able to tidy this up with something like:\r\n```\r\nformat(""The keyspace %s has a replication factor of %s, above the %s threshold of %s),\r\n             what, value, isWarning ? ""warning"" : ""failure"", threshold);\r\n```\r\nGets you down to one line with the ternary on the string argument only.', 'commenter': 'jmckenzie-dev'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";","[{'comment': ""You have inconsistent usage of this String throughout the tests. I'd either settle on using the KS string everywhere or just hardcode to ks everywhere."", 'commenter': 'jmckenzie-dev'}, {'comment': 'hardcoded the keyspace everywhere', 'commenter': 'thingtwin1'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&","[{'comment': ""It isn't immediately obvious to me why you're filtering out this set of warnings from your list. Maybe leave a quick comment to explain that these are expected warnings if that is indeed the case."", 'commenter': 'jmckenzie-dev'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable","[{'comment': ""nit: you're testing CREATE + warn, and ALTER + fail. Might be worth testing all four case combinations just for the sake of completeness. Really would only matter in some timeline where someone decouples the two in the future from each other."", 'commenter': 'jmckenzie-dev'}, {'comment': 'separated out the tests with CREATE(warn + fail) and ALTER(warn + fail) cases ', 'commenter': 'thingtwin1'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");","[{'comment': 'Same feedback here. Might want to have this test walk through the cases like:\r\n```\r\nassertValid(""CREATE... (with small #)\r\nassertWarns(""CREATE... (with medium #)\r\nassertFails(""CREATE... (with very big #)\r\n\r\nassertValid(""ALTER... (small)\r\nassertWarns(""ALTER... (medium)\r\nassertFails(""ALTER... (large)\r\n```\r\n\r\nThat way you have all your bases covered.', 'commenter': 'jmckenzie-dev'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        assertFails(""CREATE KEYSPACE ks1 WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks1 has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        DatabaseDescriptor.setEndpointSnitch(snitch);
+        execute(""DROP KEYSPACE IF EXISTS ks1"");
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"",
+                    format(""The keyspace %s has a replication factor of 4, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertFails(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailAboveAlter() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        execute(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"",
+                    format(""The keyspace %s has a replication factor of 6, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFWarnFailBetween() throws Throwable","[{'comment': 'Why is this named ""WarnFail""? Looks like it\'s a warn only test?', 'commenter': 'jmckenzie-dev'}, {'comment': 'it tests values that are in between warn and fail', 'commenter': 'thingtwin1'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+    private static final String WHAT = ""maximum_replication_factor"";
+    private static final String KS = ""ks"";
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategy() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s."",
+                           KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        assertFails(""CREATE KEYSPACE ks1 WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks1 has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+
+        DatabaseDescriptor.setEndpointSnitch(snitch);
+        execute(""DROP KEYSPACE IF EXISTS ks1"");
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyWarnAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, DISABLED_GUARDRAIL);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"",
+                    format(""The keyspace %s has a replication factor of 4, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertFails(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 5}"",
+                    format(""The keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFOnlyFailAboveAlter() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        execute(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"");
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"",
+                    format(""The keyspace %s has a replication factor of 6, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFWarnBelow() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 1}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 2}"");
+
+    }
+
+    @Test
+    public void testMaxKeyspaceRFWarnFailBetween() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 3}"",
+                    format(""The keyspace %s has a replication factor of 3, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"",
+                    format(""The keyspace %s has a replication factor of 4, above the warning threshold of %s."", KS, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFFailAbove() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertFails(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 5}"",
+                    format(""he keyspace %s has a replication factor of 5, above the failure threshold of %s"", KS, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMaxKeyspaceRFFailAboveAlter() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        execute(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 4}"");
+       assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 5}"",","[{'comment': 'nit: alignment on this line', 'commenter': 'jmckenzie-dev'}]"
1582,conf/cassandra.yaml,"@@ -1791,6 +1791,10 @@ drop_compact_storage_enabled: false
 # Suggested value for use in production: 2 or higher
 # minimum_replication_factor_warn_threshold: -1
 # minimum_replication_factor_fail_threshold: -1
+# Guardrail to warn or fail when the maximum replication factor is greater than threshold.","[{'comment': 'Recent changes have introduced a blank line separating the configs of every guardrail, we should also introduce that empty line here to keep the format:\r\n```suggestion\r\n#\r\n# Guardrail to warn or fail when the maximum replication factor is greater than threshold.\r\n```', 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/config/GuardrailsOptions.java,"@@ -648,6 +649,31 @@ public void setMinimumReplicationFactorThreshold(int warn, int fail)
                                   x -> config.minimum_replication_factor_fail_threshold = x);
     }
 
+    @Override
+    public int getMaximumReplicationFactorWarnThreshold()
+    {
+        return config.maximum_replication_factor_warn_threshold;
+    }
+
+    @Override
+    public int getMaximumReplicationFactorFailThreshold()
+    {
+        return config.maximum_replication_factor_fail_threshold;
+    }
+
+    public void setMaximumReplicationFactorThreshold(int warn, int fail)
+    {
+        validateMaxIntThreshold(warn, fail, ""maximum_replication_factor"");","[{'comment': ""This should be `validateMaxRFThreshold`. This would have been detected by a test equivalent to [`GuardrailMinimumReplicationFactorTest#testMinRFGreaterThanDefaultRF()`](https://github.com/apache/cassandra/blob/12751587818de4ae586a02c7d8568d815a4c0956/test/unit/org/apache/cassandra/db/guardrails/GuardrailMinimumReplicationFactorTest.java#L259-L276), but we don't have an equivalent test in `GuardrailMaximumReplicationFactorTest`."", 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/config/GuardrailsOptions.java,"@@ -738,6 +770,15 @@ private static void validateMinRFVersusDefaultRF(int fail, String name) throws I
         }
     }
 
+    private static void validateMaxRFVersusDefaultRF(int fail, String name) throws IllegalArgumentException","[{'comment': ""I think we don't need the explicit `throws IllegalArgumentException`."", 'commenter': 'adelapena'}, {'comment': ""The only caller of `validateMaxRFVersusDefaultRF` is `validateMaxRFThreshold`, and I don't think we are going to have any other callers. We might want to inline the contents of this method into `validateMaxRFThreshold`. Same for `validateMinRFVersusDefaultRF`."", 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/config/GuardrailsOptions.java,"@@ -738,6 +770,15 @@ private static void validateMinRFVersusDefaultRF(int fail, String name) throws I
         }
     }
 
+    private static void validateMaxRFVersusDefaultRF(int fail, String name) throws IllegalArgumentException
+    {
+        if (fail != -1 && fail < DatabaseDescriptor.getDefaultKeyspaceRF())
+        {
+            throw new IllegalArgumentException(String.format(""%s_fail_threshold to be set (%d) cannot be less than default_keyspace_rf (%d)"",","[{'comment': '```suggestion\r\n            throw new IllegalArgumentException(String.format(""%s_fail_threshold to be set (%d) cannot be lesser than default_keyspace_rf (%d)"",\r\n```', 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/db/guardrails/Guardrails.java,"@@ -341,6 +341,17 @@ public final class Guardrails implements GuardrailsMBean
                                : format(""The keyspace %s has a replication factor of %s, below the failure threshold of %s."",
                                         what, value, threshold));
 
+    /**
+     * Guardrail on the maximum replication factor.
+     */
+    public static final MaxThreshold maximumReplicationFactor =
+    new MaxThreshold(""maximum_replication_factor"",
+                  state -> CONFIG_PROVIDER.getOrCreate(state).getMaximumReplicationFactorWarnThreshold(),
+                  state -> CONFIG_PROVIDER.getOrCreate(state).getMaximumReplicationFactorFailThreshold(),
+                  (isWarning, what, value, threshold) ->
+                  format(""The keyspace %s has a replication factor of %s, above the %s threshold of %s"",
+                         what, value, isWarning ? ""warning"" : ""failure"", threshold));","[{'comment': 'We could use this same cleaner string formatting in the analogous `minimumReplicationFactor`.', 'commenter': 'adelapena'}, {'comment': 'This should probably end with a period:\r\n```suggestion\r\n                  format(""The keyspace %s has a replication factor of %s, above the %s threshold of %s."",\r\n                         what, value, isWarning ? ""warning"" : ""failure"", threshold));\r\n```\r\nThe new unit tests seem to expect that dot, and I think that\'s why some of them fail. Did you run them locally?', 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/db/guardrails/GuardrailsMBean.java,"@@ -553,4 +553,21 @@
      */
     void setMinimumReplicationFactorThreshold (int warn, int fail);
 
+    /**
+     * @return The threshold to fail when replication factor is greater threshold.","[{'comment': '```suggestion\r\n     * @return The threshold to fail when replication factor is greater than threshold.\r\n```\r\nThis also applies to the equivalent `getMinimumReplicationFactorWarnThreshold`', 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/db/guardrails/GuardrailsMBean.java,"@@ -553,4 +553,21 @@
      */
     void setMinimumReplicationFactorThreshold (int warn, int fail);
 
+    /**
+     * @return The threshold to fail when replication factor is greater threshold.
+     */
+    int getMaximumReplicationFactorWarnThreshold();
+
+    /**
+     * @return The threshold to fail when replication factor is greater threshold.","[{'comment': '```suggestion\r\n     * @return The threshold to fail when replication factor is greater than threshold.\r\n```\r\nThis also applies to the equivalent `getMinimumReplicationFactorFailThreshold`.', 'commenter': 'adelapena'}]"
1582,src/java/org/apache/cassandra/db/guardrails/GuardrailsMBean.java,"@@ -553,4 +553,21 @@
      */
     void setMinimumReplicationFactorThreshold (int warn, int fail);
 
+    /**
+     * @return The threshold to fail when replication factor is greater threshold.
+     */
+    int getMaximumReplicationFactorWarnThreshold();
+
+    /**
+     * @return The threshold to fail when replication factor is greater threshold.
+     */
+    int getMaximumReplicationFactorFailThreshold();
+
+    /**
+     * @param warn the threshold to warn when the maximum replication factor is greater than
+     *             threshold -1 means disabled.
+     * @param fail the threshold to fail when the maximum replication factor is greater than
+     *             threshold -1 means disabled.","[{'comment': '```suggestion\r\n     * @param warn The threshold to warn when the maximum replication factor is greater than\r\n     *             threshold. -1 means disabled.\r\n     * @param fail The threshold to fail when the maximum replication factor is greater than\r\n     *             threshold. -1 means disabled.\r\n```\r\nThis also applies to the equivalent `setMinimumReplicationFactorThreshold`.\r\n', 'commenter': 'adelapena'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,246 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester","[{'comment': 'As mentioned above, multiple tests in this class fail due to a missed period in the error message provider for the testes guardrail.\r\n\r\nAlso, we are missing tests equivalent to `GuardrailMinimumReplicationFactorTest#testConfigValidation` and `GuardrailMinimumReplicationFactorTest#testMinRFGreaterThanDefaultRF`. I think those tests are useful because they help us to detect things like the missed validation in  `GuardrailOptions#setMaximumReplicationFactorThreshold` that we mentioned above.', 'commenter': 'adelapena'}]"
1582,test/unit/org/apache/cassandra/db/guardrails/GuardrailMaximumReplicationFactorTest.java,"@@ -0,0 +1,246 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.guardrails;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+import org.junit.After;
+import org.junit.Test;
+
+import org.apache.cassandra.ServerTestUtils;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.locator.AbstractEndpointSnitch;
+import org.apache.cassandra.locator.IEndpointSnitch;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.service.ClientWarn;
+import org.apache.cassandra.service.StorageService;
+
+import static java.lang.String.format;
+
+public class GuardrailMaximumReplicationFactorTest extends ThresholdTester
+{
+    private static final int MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD = 2;
+    private static final int MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD = 4;
+    private static final int DISABLED_GUARDRAIL = -1;
+
+    public GuardrailMaximumReplicationFactorTest()
+    {
+        super(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD,
+              MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD,
+              Guardrails.maximumReplicationFactor,
+              Guardrails::setMaximumReplicationFactorThreshold,
+              Guardrails::getMaximumReplicationFactorWarnThreshold,
+              Guardrails::getMaximumReplicationFactorFailThreshold);
+    }
+
+    @After
+    public void cleanupTest() throws Throwable
+    {
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+    }
+
+    @Override
+    protected long currentValue()
+    {
+        return Long.parseLong((Keyspace.open(""ks"").getReplicationStrategy()).configOptions.get(""datacenter1""));
+    }
+
+    @Override
+    protected List<String> getWarnings()
+    {
+        List<String> warnings = ClientWarn.instance.getWarnings();
+
+        //filtering out nonguardrails produced warnings
+        return warnings == null
+               ? Collections.emptyList()
+               : warnings.stream()
+                         .filter(w -> !w.contains(""keyspace ks is higher than the number of nodes 1 for datacenter1"") &&
+                                      !w.contains(""When increasing replication factor you need to run a full (-full) repair to distribute the data"") &&
+                                      !w.contains(""keyspace ks is higher than the number of nodes"") &&
+                                      !w.contains(""Your replication factor 3 for keyspace ks is higher than the number of nodes 2 for datacenter datacenter2""))
+                         .collect(Collectors.toList());
+    }
+
+    @Test
+    public void testMaxKeyspaceRFDisabled() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(DISABLED_GUARDRAIL, DISABLED_GUARDRAIL);
+        assertMaxThresholdValid(""CREATE KEYSPACE ks WITH replication = { 'class': 'NetworkTopologyStrategy', 'datacenter1': 6}"");
+        assertMaxThresholdValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 10}"");
+    }
+
+    @Test
+    public void testSimpleStrategyCreate() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace ks has a replication factor of 3, above the warning threshold of %s."",
+                           MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""CREATE KEYSPACE ks1 WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace ks1 has a replication factor of 5, above the failure threshold of %s."",
+                           MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testSimpleStrategyAlter() throws Throwable
+    {
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        execute(""CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 2}"");
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3}"",
+                    format(""The keyspace ks has a replication factor of 3, above the warning threshold of %s."",
+                           MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 5}"",
+                    format(""The keyspace ks has a replication factor of 5, above the failure threshold of %s."",
+                           MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+    }
+
+    @Test
+    public void testMultipleDatacenter() throws Throwable
+    {
+        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
+        DatabaseDescriptor.setEndpointSnitch(new AbstractEndpointSnitch()
+        {
+            public static final String RACK1 = ServerTestUtils.RACK1;
+
+            @Override
+            public String getRack(InetAddressAndPort endpoint) { return RACK1; }
+
+            @Override
+            public String getDatacenter(InetAddressAndPort endpoint) { return ""datacenter2""; }
+
+            @Override
+            public int compareEndpoints(InetAddressAndPort target, Replica a1, Replica a2) { return 0; }
+        });
+
+        List<String> twoWarnings = Arrays.asList(format(""The keyspace ks has a replication factor of 3, above the warning threshold of %s."", MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD),
+                                                 format(""The keyspace ks has a replication factor of 3, above the warning threshold of %s."", MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+
+        StorageService.instance.getTokenMetadata().updateHostId(UUID.randomUUID(), InetAddressAndPort.getByName(""127.0.0.255""));
+        guardrails().setMaximumReplicationFactorThreshold(MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD);
+        assertValid(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace ks has a replication factor of 3, above the warning threshold of %s."", MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+        assertWarns(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+        assertFails(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+        assertFails(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+        execute(""DROP KEYSPACE IF EXISTS ks"");
+
+        execute(""CREATE KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 1, 'datacenter2' : 1 };"");
+        assertValid(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 2 };"");
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 3 };"",
+                    format(""The keyspace ks has a replication factor of 3, above the warning threshold of %s."", MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));
+        assertWarns(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 3, 'datacenter2' : 3 };"", twoWarnings);
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 2, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));
+        assertFails(""ALTER KEYSPACE ks WITH replication = { 'class' : 'NetworkTopologyStrategy', 'datacenter1': 5, 'datacenter2' : 5 };"",
+                    format(""The keyspace ks has a replication factor of 5, above the failure threshold of %s"", MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));","[{'comment': 'The phrases of the form `The keyspace ks has a replication factor...` are repeated multiple times across the test. We could try to reduce duplication and make the tests a bit more compact with a couple of utility methods like, for example:\r\n```java\r\nprivate void assertWarns(String query, int rf) throws Throwable\r\n{\r\n    assertWarns(query, format(""The keyspace ks has a replication factor of %d, above the warning threshold of %s."",\r\n                              rf, MAXIMUM_REPLICATION_FACTOR_WARN_THRESHOLD));\r\n}\r\n\r\nprivate void assertFails(String query, int rf) throws Throwable\r\n{\r\n    assertFails(query, format(""The keyspace ks has a replication factor of %d, above the failure threshold of %s."",\r\n                              rf, MAXIMUM_REPLICATION_FACTOR_FAIL_THRESHOLD));\r\n}\r\n```', 'commenter': 'adelapena'}]"
1587,src/java/org/apache/cassandra/locator/Ec2SnitchIMDSv2.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+import java.io.DataInputStream;
+import java.io.FilterInputStream;
+import java.io.IOException;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * An implementation of the Ec2Snitch which uses Instance Meta Data Service v2 (IMDSv2) which requires you
+ * to get a session token first before calling the metaData service
+ */
+public class Ec2SnitchIMDSv2 extends Ec2Snitch
+{
+    protected static final Logger logger = LoggerFactory.getLogger(Ec2SnitchIMDSv2.class);
+    private static final long REFRESH_TOKEN_TIME = 21600;
+    private static final String AWS_EC2_METADATA_HEADER_TTL = ""X-aws-ec2-metadata-token-ttl-seconds"";
+    private static final String TOKEN_TTL_SECONDS = String.valueOf(REFRESH_TOKEN_TIME);
+    private static final String AWS_EC2_METADATA_HEADER = ""X-aws-ec2-metadata-token"";
+    private static final String TOKEN_ENDPOINT = ""http://169.254.169.254/latest/api/token"";
+
+    private String myToken;
+    private Long myLastTokenTime;
+
+
+    public Ec2SnitchIMDSv2() throws IOException, ConfigurationException
+    {
+        super();
+    }
+
+    @Override
+    String awsApiCall(final String url) throws IOException, ConfigurationException
+    {
+        // Populate the region and zone by introspection, fail if 404 on metadata
+        if (myToken == null || myLastTokenTime == null
+            || System.currentTimeMillis() - myLastTokenTime > (REFRESH_TOKEN_TIME - 100))
+        {
+            getAndSetNewToken();
+        }
+        final HttpURLConnection conn = (HttpURLConnection) new URL(url).openConnection();
+        conn.setRequestProperty(AWS_EC2_METADATA_HEADER, myToken);
+        return getContent(conn);
+    }
+
+    /**
+     * Get a session token to enable requests to the meta data service.
+     * https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html
+     *
+     * @throws IOException
+     */
+    private void getAndSetNewToken() throws IOException
+    {
+        final URL url = new URL(TOKEN_ENDPOINT);
+        final HttpURLConnection http = (HttpURLConnection) url.openConnection();
+        http.setRequestProperty(AWS_EC2_METADATA_HEADER_TTL, TOKEN_TTL_SECONDS);
+        http.setRequestMethod(""PUT"");
+
+        myToken = getContent(http);
+        myLastTokenTime = System.currentTimeMillis();
+    }
+
+    private String getContent(final HttpURLConnection conn) throws IOException
+    {
+        DataInputStream d = null;
+        try
+        {
+            if (conn.getResponseCode() != 200)
+            {
+                throw new ConfigurationException(
+                ""Ec2SnitchIMDSv2 was unable to execute the API call. Not an ec2 node?"");
+            }
+            // Read the information. I wish I could say (String) conn.getContent() here...
+            final int cl = conn.getContentLength();","[{'comment': 'Could you extract this content reading code into some method maybe in `EC2Snitch` or somewhere else and then, use there and in the legacy EC2 snitch impl? (only for trunk)\r\n\r\nAlso, would be good to cover the case where content length is returned -1, and read the whole input stream then (or read the whole input stream always, for example using `ByteStreams.toByteArray`)', 'commenter': 'jacek-lewandowski'}]"
1587,src/java/org/apache/cassandra/locator/Ec2SnitchIMDSv2.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+import java.io.DataInputStream;
+import java.io.FilterInputStream;
+import java.io.IOException;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * An implementation of the Ec2Snitch which uses Instance Meta Data Service v2 (IMDSv2) which requires you
+ * to get a session token first before calling the metaData service
+ */
+public class Ec2SnitchIMDSv2 extends Ec2Snitch
+{
+    protected static final Logger logger = LoggerFactory.getLogger(Ec2SnitchIMDSv2.class);
+    private static final long REFRESH_TOKEN_TIME = 21600;","[{'comment': 'The documentation says that it is the maximum TTL for the token - would be good to let the user configure that time using snitch properties (please remember to include the unit suffix in property name)', 'commenter': 'jacek-lewandowski'}]"
1587,src/java/org/apache/cassandra/locator/Ec2SnitchIMDSv2.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+import java.io.DataInputStream;
+import java.io.FilterInputStream;
+import java.io.IOException;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * An implementation of the Ec2Snitch which uses Instance Meta Data Service v2 (IMDSv2) which requires you
+ * to get a session token first before calling the metaData service
+ */
+public class Ec2SnitchIMDSv2 extends Ec2Snitch
+{
+    protected static final Logger logger = LoggerFactory.getLogger(Ec2SnitchIMDSv2.class);
+    private static final long REFRESH_TOKEN_TIME = 21600;
+    private static final String AWS_EC2_METADATA_HEADER_TTL = ""X-aws-ec2-metadata-token-ttl-seconds"";
+    private static final String TOKEN_TTL_SECONDS = String.valueOf(REFRESH_TOKEN_TIME);
+    private static final String AWS_EC2_METADATA_HEADER = ""X-aws-ec2-metadata-token"";
+    private static final String TOKEN_ENDPOINT = ""http://169.254.169.254/latest/api/token"";
+
+    private String myToken;
+    private Long myLastTokenTime;
+
+
+    public Ec2SnitchIMDSv2() throws IOException, ConfigurationException
+    {
+        super();
+    }
+
+    @Override
+    String awsApiCall(final String url) throws IOException, ConfigurationException
+    {
+        // Populate the region and zone by introspection, fail if 404 on metadata
+        if (myToken == null || myLastTokenTime == null","[{'comment': 'nit: perhaps you can use `Suppliers.memoizeWithExpiration` to automatically control token lifetime', 'commenter': 'jacek-lewandowski'}]"
1587,src/java/org/apache/cassandra/locator/Ec2MultiRegionSnitchIMDSv2.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.service.StorageService;
+
+/**
+ * Is exactly the same as the Ec2MultiRegionSnitch except it uses the Instance MetaData Service v2 (IMDSv2) which
+ * requires you to get a session token first.
+ */
+public class Ec2MultiRegionSnitchIMDSv2 extends Ec2SnitchIMDSv2
+{
+
+    private static final String PUBLIC_IP_QUERY_URL = ""http://169.254.169.254/latest/meta-data/public-ipv4"";
+    private static final String PRIVATE_IP_QUERY_URL = ""http://169.254.169.254/latest/meta-data/local-ipv4"";
+    private final String localPrivateAddress;
+
+    public Ec2MultiRegionSnitchIMDSv2() throws IOException, ConfigurationException
+    {
+        super();","[{'comment': 'nit: redundant', 'commenter': 'jacek-lewandowski'}]"
1587,src/java/org/apache/cassandra/locator/Ec2SnitchIMDSv2.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+import java.io.DataInputStream;
+import java.io.FilterInputStream;
+import java.io.IOException;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * An implementation of the Ec2Snitch which uses Instance Meta Data Service v2 (IMDSv2) which requires you
+ * to get a session token first before calling the metaData service
+ */
+public class Ec2SnitchIMDSv2 extends Ec2Snitch
+{
+    protected static final Logger logger = LoggerFactory.getLogger(Ec2SnitchIMDSv2.class);
+    private static final long REFRESH_TOKEN_TIME = 21600;
+    private static final String AWS_EC2_METADATA_HEADER_TTL = ""X-aws-ec2-metadata-token-ttl-seconds"";
+    private static final String TOKEN_TTL_SECONDS = String.valueOf(REFRESH_TOKEN_TIME);
+    private static final String AWS_EC2_METADATA_HEADER = ""X-aws-ec2-metadata-token"";
+    private static final String TOKEN_ENDPOINT = ""http://169.254.169.254/latest/api/token"";
+
+    private String myToken;
+    private Long myLastTokenTime;
+
+
+    public Ec2SnitchIMDSv2() throws IOException, ConfigurationException","[{'comment': 'This constructor alone is redundant. You should add a constructor which takes `SnitchProperties` so that both are justified', 'commenter': 'jacek-lewandowski'}]"
1587,src/java/org/apache/cassandra/locator/Ec2SnitchIMDSv2.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+import java.io.DataInputStream;
+import java.io.FilterInputStream;
+import java.io.IOException;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * An implementation of the Ec2Snitch which uses Instance Meta Data Service v2 (IMDSv2) which requires you
+ * to get a session token first before calling the metaData service
+ */
+public class Ec2SnitchIMDSv2 extends Ec2Snitch
+{
+    protected static final Logger logger = LoggerFactory.getLogger(Ec2SnitchIMDSv2.class);
+    private static final long REFRESH_TOKEN_TIME = 21600;
+    private static final String AWS_EC2_METADATA_HEADER_TTL = ""X-aws-ec2-metadata-token-ttl-seconds"";
+    private static final String TOKEN_TTL_SECONDS = String.valueOf(REFRESH_TOKEN_TIME);
+    private static final String AWS_EC2_METADATA_HEADER = ""X-aws-ec2-metadata-token"";
+    private static final String TOKEN_ENDPOINT = ""http://169.254.169.254/latest/api/token"";","[{'comment': 'The endpoint IP is common for all of the addressed used here and in other EC2 classes. Would be good to extract it into a single field in `Ec2Snitch` class. If it was additionally configurable via `SnitchProperties` those classes would be more testable as we could simulate the EC2 endpoint (applies only for PR against trunk)', 'commenter': 'jacek-lewandowski'}]"
1587,src/java/org/apache/cassandra/locator/Ec2MultiRegionSnitchIMDSv2.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.service.StorageService;
+
+/**
+ * Is exactly the same as the Ec2MultiRegionSnitch except it uses the Instance MetaData Service v2 (IMDSv2) which
+ * requires you to get a session token first.
+ */
+public class Ec2MultiRegionSnitchIMDSv2 extends Ec2SnitchIMDSv2
+{
+
+    private static final String PUBLIC_IP_QUERY_URL = ""http://169.254.169.254/latest/meta-data/public-ipv4"";
+    private static final String PRIVATE_IP_QUERY_URL = ""http://169.254.169.254/latest/meta-data/local-ipv4"";
+    private final String localPrivateAddress;
+
+    public Ec2MultiRegionSnitchIMDSv2() throws IOException, ConfigurationException","[{'comment': 'Similar to `Ec2SnitchIMDSv2` - please add a constructor accepting `SnitchProperties`', 'commenter': 'jacek-lewandowski'}]"
1638,CHANGES.txt,"@@ -1,4 +1,5 @@
 4.1
+ * Fix OfflineTokenAllocatorTest timeouts (CASSANDRA-17291)","[{'comment': ""we don't need for tests changes I thought"", 'commenter': 'dcapwell'}]"
1638,test/unit/org/apache/cassandra/dht/tokenallocator/OfflineTokenAllocatorTestGenerations.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht.tokenallocator;
+
+import org.junit.Before;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Murmur3Partitioner;
+import org.apache.cassandra.dht.RandomPartitioner;
+import org.apache.cassandra.tools.Util;
+
+import static org.apache.cassandra.dht.tokenallocator.OfflineTokenAllocator.allocate;
+import static org.apache.cassandra.dht.tokenallocator.OfflineTokenAllocatorTestUtils.assertTokensAndNodeCount;
+import static org.apache.cassandra.dht.tokenallocator.OfflineTokenAllocatorTestUtils.makeRackCountArray;
+
+
+/**
+ * We break the testTokenGenerations test out as it runs long and pushes the rest of the test suite to timeout on both
+ * ci environments and local laptops.
+ */
+public class OfflineTokenAllocatorTestGenerations","[{'comment': ""doesn't end in Test so won't get picked up in CI"", 'commenter': 'dcapwell'}]"
1650,src/java/org/apache/cassandra/db/streaming/CassandraStreamReceiver.java,"@@ -23,9 +23,12 @@
 import java.util.List;
 import java.util.Set;
 
+import javax.xml.crypto.Data;","[{'comment': 'what is this import for?', 'commenter': 'smiklosovic'}, {'comment': 'Think this was a stray IDE autocompletion on an import probably; should be removed.', 'commenter': 'jmckenzie-dev'}]"
1650,conf/cassandra.yaml,"@@ -298,6 +298,16 @@ partitioner: org.apache.cassandra.dht.Murmur3Partitioner
 # containing a CDC-enabled table if at space limit in cdc_raw_directory).
 cdc_enabled: false
 
+# Specify whether writes to the CDC-enabled tables should be blocked when CDC data on disk has reached to the limit.
+# When setting to false, the writes will not be blocked and the oldest CDC data on disk will be deleted to
+# ensure the size constraint. The default is true.
+# cdc_block_writes: true","[{'comment': ""Wasn't this done as part of CASSANDRA-17004? https://github.com/apache/cassandra/pull/1311/files"", 'commenter': 'jmckenzie-dev'}, {'comment': 'Yes..', 'commenter': 'yifan-c'}]"
1650,conf/cassandra.yaml,"@@ -298,6 +298,16 @@ partitioner: org.apache.cassandra.dht.Murmur3Partitioner
 # containing a CDC-enabled table if at space limit in cdc_raw_directory).
 cdc_enabled: false
 
+# Specify whether writes to the CDC-enabled tables should be blocked when CDC data on disk has reached to the limit.
+# When setting to false, the writes will not be blocked and the oldest CDC data on disk will be deleted to
+# ensure the size constraint. The default is true.
+# cdc_block_writes: true
+
+# Specify whether enable write path for CDC during internodes streaming.","[{'comment': 'rephrase to `Specify whether CDC mutations are replayed through the write path on streaming`', 'commenter': 'jmckenzie-dev'}, {'comment': '👍 Yours reads better.', 'commenter': 'yifan-c'}]"
1650,conf/cassandra.yaml,"@@ -298,6 +298,16 @@ partitioner: org.apache.cassandra.dht.Murmur3Partitioner
 # containing a CDC-enabled table if at space limit in cdc_raw_directory).
 cdc_enabled: false
 
+# Specify whether writes to the CDC-enabled tables should be blocked when CDC data on disk has reached to the limit.
+# When setting to false, the writes will not be blocked and the oldest CDC data on disk will be deleted to
+# ensure the size constraint. The default is true.
+# cdc_block_writes: true
+
+# Specify whether enable write path for CDC during internodes streaming.
+# When enabled, CDC data streamed to the destination node will be written into commit log first. When setting to false,
+# the CDC data will be written into SSTables just the same as normal streaming. The default is true.
+# write_path_for_cdc_enabled: true","[{'comment': 'Recommend adding a bit as to ""why"" someone should care about this. For example: `If this is set to false, streaming will be considerably faster however it\'s possible that, in extreme situations (losing > RF # nodes in a replica set), you may have data in your SSTables that never makes it to the CDC log.`', 'commenter': 'jmckenzie-dev'}, {'comment': ""Noodling on this a bit; I think this name is confusing. By default, things on the write path w/CDC enabled will have their data in CDC. Maybe we should rename this something like `cdc_on_streaming_enabled` to clarify it? Or `use_write_path_for_cdc_while_streaming`, for example. I like the 1st though (cdc_on_streaming_enabled) since it's pretty straightforward as to what you do and don't get."", 'commenter': 'jmckenzie-dev'}, {'comment': 'Sounds good. I changed `RF # nodes` to `quorum # nodes`', 'commenter': 'yifan-c'}, {'comment': 'Good point. I see how it is confusing now, since the normal write path must produce CDC data when enabled. \r\nRegarding `cdc_on_streaming_enabled`, I am a bit worried that it could be confused with streaming CDC to downstream consumer... And the name of the second option is probably too long, preferably a short one.  \r\n\r\nEssentially, it is a repair mechanism for CDC data. How about `cdc_repair_enabled`? ', 'commenter': 'yifan-c'}, {'comment': 'That makes it sound like it repairs cdc data. What about `cdc_on_repair_enabled`?', 'commenter': 'jmckenzie-dev'}]"
1650,src/java/org/apache/cassandra/db/commitlog/CommitLog.java,"@@ -443,6 +439,29 @@ public void setCDCBlockWrites(boolean val)
         logger.info(""Updated CDC block_writes from {} to {}"", oldVal, val);
     }
 
+
+    @Override
+    public boolean isWritePathForCDCEnabled()
+    {
+        return DatabaseDescriptor.isWritePathForCDCEnabled();
+    }
+
+    @Override
+    public void setWritePathForCdcEnabled(boolean value)
+    {
+        ensureCDCEnabled();
+        DatabaseDescriptor.setWritePathForCDCEnabled(value);
+        logger.info(""Set write_path_for_cdc_enabled to {}"", value);
+    }
+
+    private void ensureCDCEnabled()","[{'comment': ""We lose the context (`unable to set block_writes`) or whatever the new usage is here. Maybe take in a string and prepend that as part of the message to indicate what the user was trying to do that's incompatible w/the currently disabled CDC state?"", 'commenter': 'jmckenzie-dev'}]"
1650,src/java/org/apache/cassandra/db/commitlog/CommitLogMBean.java,"@@ -88,4 +88,10 @@
     public boolean getCDCBlockWrites();
 
     public void setCDCBlockWrites(boolean val);
+
+    /** Returns true if internodes streaming of CDC data should go through write path */
+    boolean isWritePathForCDCEnabled();
+
+    /** Set whether enable write path for CDC data during internodes streaming, e.g. repair */
+    void setWritePathForCdcEnabled(boolean value);","[{'comment': 'nit: capitalization `CDC`', 'commenter': 'jmckenzie-dev'}]"
1650,src/java/org/apache/cassandra/db/streaming/CassandraStreamReceiver.java,"@@ -23,9 +23,12 @@
 import java.util.List;
 import java.util.Set;
 
+import javax.xml.crypto.Data;
+
 import com.google.common.base.Preconditions;
 import com.google.common.collect.Iterables;
 
+import org.apache.cassandra.config.DatabaseDescriptor;","[{'comment': 'import ordering (this should be down below w/the rest of the o.a.c, and the other 2 adjacent moved down)', 'commenter': 'jmckenzie-dev'}]"
1650,src/java/org/apache/cassandra/db/streaming/CassandraStreamReceiver.java,"@@ -172,23 +175,31 @@ private boolean hasCDC(ColumnFamilyStore cfs)
         return cfs.metadata().params.cdc;
     }
 
+    // returns true iif it is a cdc table and writepath is enabled for cdc.
+    private boolean shouldWriteCommitLog(ColumnFamilyStore cfs)
+    {
+        return DatabaseDescriptor.isWritePathForCDCEnabled() && hasCDC(cfs);
+    }
+
     /*
      * We have a special path for views and for CDC.
      *
      * For views, since the view requires cleaning up any pre-existing state, we must put all partitions
      * through the same write path as normal mutations. This also ensures any 2is are also updated.
      *
-     * For CDC-enabled tables, we want to ensure that the mutations are run through the CommitLog so they
-     * can be archived by the CDC process on discard.
+     * For CDC-enabled tables and write path for CDC is enabled, we want to ensure that the mutations are
+     * run through the CommitLog, so they can be archived by the CDC process on discard.
      */
     private boolean requiresWritePath(ColumnFamilyStore cfs)
     {
-        return hasCDC(cfs) || cfs.streamToMemtable() || (session.streamOperation().requiresViewBuild() && hasViews(cfs));
+        return shouldWriteCommitLog(cfs)","[{'comment': 'nit: consider renaming the method to something like ""cdcRequiresCommitLog"" or something to indicate that it\'s specific to CDC\'s needs rather than just ""writes for this CFS need the CommitLog"". Unless you plan on us extending `shouldWriteCommitLog` in the future w/other needs that aren\'t CDC specific... but I suppose we could do them at that time anyway.', 'commenter': 'jmckenzie-dev'}]"
1650,test/distributed/org/apache/cassandra/distributed/test/cdc/ToggleWritePathInStreamingForCDCTest.java,"@@ -0,0 +1,98 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.cdc;
+
+import java.util.function.Consumer;
+
+import org.junit.Test;
+
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.db.commitlog.CommitLogSegment;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+
+import static org.apache.cassandra.distributed.shared.AssertUtils.assertRows;
+import static org.apache.cassandra.distributed.shared.AssertUtils.assertTrue;
+import static org.apache.cassandra.distributed.shared.AssertUtils.row;
+
+public class ToggleWritePathInStreamingForCDCTest extends TestBaseImpl
+{
+    @Test
+    public void testWritePathForCDCEnabled() throws Exception
+    {
+        testWritePathForCDC(true, cluster -> {
+            cluster.get(2).runOnInstance(() -> {
+                boolean containCDCInLog = CommitLog.instance.segmentManager
+                                              .getActiveSegments()
+                                              .stream()
+                                              .anyMatch(s -> s.getCDCState() == CommitLogSegment.CDCState.CONTAINS);
+                assertTrue(""Mutaion should be added to commit log when write_path_for_cdc_enabled is true"",","[{'comment': 'not: `Mutation`', 'commenter': 'jmckenzie-dev'}]"
1650,test/distributed/org/apache/cassandra/distributed/test/cdc/ToggleWritePathInStreamingForCDCTest.java,"@@ -0,0 +1,98 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.cdc;
+
+import java.util.function.Consumer;
+
+import org.junit.Test;
+
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.db.commitlog.CommitLogSegment;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+
+import static org.apache.cassandra.distributed.shared.AssertUtils.assertRows;
+import static org.apache.cassandra.distributed.shared.AssertUtils.assertTrue;
+import static org.apache.cassandra.distributed.shared.AssertUtils.row;
+
+public class ToggleWritePathInStreamingForCDCTest extends TestBaseImpl
+{
+    @Test
+    public void testWritePathForCDCEnabled() throws Exception
+    {
+        testWritePathForCDC(true, cluster -> {
+            cluster.get(2).runOnInstance(() -> {
+                boolean containCDCInLog = CommitLog.instance.segmentManager
+                                              .getActiveSegments()
+                                              .stream()
+                                              .anyMatch(s -> s.getCDCState() == CommitLogSegment.CDCState.CONTAINS);
+                assertTrue(""Mutaion should be added to commit log when write_path_for_cdc_enabled is true"",
+                           containCDCInLog);
+            });
+        });
+    }
+
+    @Test
+    public void testWritePathForCDCDisabled() throws Exception
+    {
+        testWritePathForCDC(false, cluster -> {
+            cluster.get(2).runOnInstance(() -> {
+                boolean containCDCInLog = CommitLog.instance.segmentManager
+                                              .getActiveSegments()
+                                              .stream()
+                                              .allMatch(s -> s.getCDCState() != CommitLogSegment.CDCState.CONTAINS);
+                assertTrue(""No mutaion should be added to commit log when write_path_for_cdc_enabled is false"",","[{'comment': 'nit: `Mutation`', 'commenter': 'jmckenzie-dev'}]"
1650,test/distributed/org/apache/cassandra/distributed/test/cdc/ToggleWritePathInStreamingForCDCTest.java,"@@ -0,0 +1,98 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.cdc;
+
+import java.util.function.Consumer;
+
+import org.junit.Test;
+
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.db.commitlog.CommitLogSegment;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+
+import static org.apache.cassandra.distributed.shared.AssertUtils.assertRows;
+import static org.apache.cassandra.distributed.shared.AssertUtils.assertTrue;
+import static org.apache.cassandra.distributed.shared.AssertUtils.row;
+
+public class ToggleWritePathInStreamingForCDCTest extends TestBaseImpl
+{
+    @Test
+    public void testWritePathForCDCEnabled() throws Exception
+    {
+        testWritePathForCDC(true, cluster -> {
+            cluster.get(2).runOnInstance(() -> {
+                boolean containCDCInLog = CommitLog.instance.segmentManager
+                                              .getActiveSegments()
+                                              .stream()
+                                              .anyMatch(s -> s.getCDCState() == CommitLogSegment.CDCState.CONTAINS);
+                assertTrue(""Mutaion should be added to commit log when write_path_for_cdc_enabled is true"",
+                           containCDCInLog);
+            });
+        });
+    }
+
+    @Test
+    public void testWritePathForCDCDisabled() throws Exception
+    {
+        testWritePathForCDC(false, cluster -> {
+            cluster.get(2).runOnInstance(() -> {
+                boolean containCDCInLog = CommitLog.instance.segmentManager
+                                              .getActiveSegments()
+                                              .stream()
+                                              .allMatch(s -> s.getCDCState() != CommitLogSegment.CDCState.CONTAINS);
+                assertTrue(""No mutaion should be added to commit log when write_path_for_cdc_enabled is false"",
+                           containCDCInLog);
+            });
+        });
+    }
+
+    // test helper to repair data between nodes when write_path_for_cdc_enabled is on or off.
+    private void testWritePathForCDC(boolean enabled, Consumer<Cluster> assertion) throws Exception
+    {
+        try (Cluster cluster = init(Cluster.build(2)
+                                           .withConfig(c -> c.set(""cdc_enabled"", true)
+                                                             .set(""write_path_for_cdc_enabled"", enabled)
+                                                             .with(Feature.NETWORK)
+                                                             .with(Feature.GOSSIP))
+                                           .start()))
+        {
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (k INT PRIMARY KEY, v INT) WITH cdc=true""));
+
+            // Data only in node1
+            cluster.get(1).executeInternal(withKeyspace(""INSERT INTO %s.tbl (k, v) VALUES (1, 1)""));
+            Object[][] result = cluster.get(1).executeInternal(withKeyspace(""SELECT * FROM %s.tbl WHERE k = 1""));
+            assertRows(result, row(1, 1));
+            result = cluster.get(2).executeInternal(withKeyspace(""SELECT * FROM %s.tbl WHERE k = 1""));
+            assertRows(result);
+
+            // repair
+            cluster.get(1).flush(KEYSPACE);
+            cluster.get(2).nodetool(""repair"", KEYSPACE, ""tbl"");
+
+            // verify node2 now have data
+            result = cluster.get(2).executeInternal(withKeyspace(""SELECT * FROM %s.tbl WHERE k = 1""));
+            assertRows(result, row(1, 1));
+
+            assertion.accept(cluster);
+","[{'comment': 'ultra nit: whitespace', 'commenter': 'jmckenzie-dev'}]"
1757,bin/cassandra,"@@ -166,7 +166,7 @@ case ""`uname -s`"" in
                 dirs=""/lib64 /lib /usr/lib64 /usr/lib `cat /etc/ld.so.conf /etc/ld.so.conf.d/*.conf | grep '^/'`""
             fi
             dirs=`echo $dirs | tr "" "" "":""`
-            CASSANDRA_LIBJEMALLOC=$(find_library '.*/libjemalloc\.so\(\.1\)*' $dirs)
+            CASSANDRA_LIBJEMALLOC=$(find_library '.*/libjemalloc\.so\.*[1-9]*' $dirs)","[{'comment': 'This will cover all libraries which are ending on a number, like:\r\n````\r\n/usr/lib/x86_64-linux-gnu/libjemalloc.so.1\r\n/usr/lib/x86_64-linux-gnu/libjemalloc.so.2\r\n...\r\n````', 'commenter': 'smiklosovic'}]"
1757,bin/cassandra,"@@ -152,7 +152,7 @@ find_library()
     pattern=$1
     path=$(echo ${2} | tr "":"" "" "")
 
-    find $path -regex ""$pattern"" -print 2>/dev/null | head -n 1
+    find $path -regex ""$pattern"" -print 2>/dev/null | sort -r | head -n 1","[{'comment': 'If we have more than one libjemalloc package installed, even by accident, like, both 1 and 2 versions, `sort -r` will sort it in descending order and head will take the first one so we always use the newest libjemalloc library.', 'commenter': 'smiklosovic'}]"
1757,bin/cassandra,"@@ -132,21 +132,21 @@ find_library()
     pattern=$1
     path=$(echo ${2} | tr "":"" "" "")
 
-    find $path -regex ""$pattern"" -print 2>/dev/null | head -n 1
+    find $path -regex ""$pattern"" -print 2>/dev/null | sort -r | uniq | head -n 1","[{'comment': '“sort -u” makes things uniq, so don’t need that other pipe', 'commenter': 'dcapwell'}]"
1757,bin/cassandra,"@@ -132,21 +132,21 @@ find_library()
     pattern=$1
     path=$(echo ${2} | tr "":"" "" "")
 
-    find $path -regex ""$pattern"" -print 2>/dev/null | head -n 1
+    find $path -regex ""$pattern"" -print 2>/dev/null | sort -r | uniq | head -n 1
 }
 case ""`uname -s`"" in
     Linux)
         if [ -z $CASSANDRA_LIBJEMALLOC ] ; then
             which ldconfig > /dev/null 2>&1
-            if [ $? = 0 ] ; then
+            if [ ! -f /etc/lsb-release ] ; then","[{'comment': 'The above check makes sure the command exists, would keep it unless you found a bug. ', 'commenter': 'dcapwell'}]"
1770,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -190,14 +189,20 @@ public CommitLogSegment.Allocation allocate(Mutation mutation, int size) throws
         return alloc;
     }
 
-    // Non-blocking mode has just recently been enabled for CDC.
-    // The segment is still marked as FORBIDDEN. It should be set to PERMITTED.
-    private void ensureSegmentPermittedIfNotBlockWrites(CommitLogSegment segment)
+    // Permit a forbidden segment under the following conditions.
+    // - Non-blocking mode has just recently been enabled for CDC.
+    // - The CDC total space has droppped below the limit (e.g. CDC consumer cleans up).
+    private void permitSegmentMaybe(CommitLogSegment segment)
     {
-        if (!DatabaseDescriptor.getCDCBlockWrites() && segment.getCDCState() == CDCState.FORBIDDEN)
+        if (segment.getCDCState() != CDCState.FORBIDDEN)
+            return;
+
+        if (!DatabaseDescriptor.getCDCBlockWrites()
+            || cdcSizeTracker.sizeInProgress.get() < DatabaseDescriptor.getCDCTotalSpace())","[{'comment': 'We _may_ want to compare to DBD.getCDCTotalSpace - the size of a CL Segment. A hypothetical example of an edge case w/this logic here:\r\n1. CommitLogSegment size 128mb\r\n2. getCDCTotalSpace = 3.95G out of 4.0G\r\n3. we see that getCDCTotalSpace is < max, so we flip the bit\r\n4. Then we overflow by `CommitLogSegment size - (4.0 - 3.95)`\r\n\r\nOR - we just leave it as is because this is likely not a big problem and maybe document it. 🤷 ', 'commenter': 'jmckenzie-dev'}, {'comment': 'The `CDCSizeTracker` sums up the size of all commit logs, including CDC and non-CDC files, since it creates hard links for all newly created files. (A side question is.. maybe we want to created the link unless the state is `FORBIDDEN`?). So the size is often an over-estimate when the workload contains both CDC and non-CDC traffic. \r\nBack to the scenario in `allocate`... if the state was previously `FORBIDDEN`, it means the total size has exceeded the limit at the time of creation. Now, the total size drops below the limit. The `CDCSizeTracker` must have run at least once. In this case, the size of the file (`allocatingFrom`) is already counted. ', 'commenter': 'yifan-c'}, {'comment': ""`The CDCSizeTracker must have run at least once. In this case, the size of the file (allocatingFrom) is already counted.` <- I didn't connect that. You're right.\r\n\r\n`maybe we want to created the link unless the state is FORBIDDEN?`\r\n🤔 We shouldn't hard link the file if the cdcState in there is FORBIDDEN should we?\r\n```\r\n// Hard link file in cdc folder for realtime tracking\r\nFileUtils.createHardLink(segment.logFile, segment.getCDCFile());\r\n```"", 'commenter': 'jmckenzie-dev'}, {'comment': 'No. If the the state is FORBIDDEN, we do not create a link. The link is created only when the state is NOT FORBIDDEN (i.e. PERMITTED/CONTAINS). \r\nThe current implementation is that a hardlink is created for all new segments, regardless of the state. Since each FORBIDDEN segment has a change to revert back to PERMITTED, this handling is easy but at the cost of slightly over-counting (the size of one file). \r\nI think, and as you mentioned, it likely not a big problem. ', 'commenter': 'yifan-c'}, {'comment': ""I'm confused. Here's what I'm seeing in the code:\r\n```\r\n    public CommitLogSegment createSegment()\r\n    {\r\n        CommitLogSegment segment = CommitLogSegment.createSegment(commitLog, this);\r\n\r\n        // Hard link file in cdc folder for realtime tracking\r\n        FileUtils.createHardLink(segment.logFile, segment.getCDCFile());\r\n\r\n        cdcSizeTracker.processNewSegment(segment);\r\n        return segment;\r\n    }\r\n```\r\nI think the code would need to read something like this to prevent hard-linking files with cdc disabled:\r\n```\r\n    public CommitLogSegment createSegment()\r\n    {\r\n        CommitLogSegment segment = CommitLogSegment.createSegment(commitLog, this);\r\n\r\n        if (segment.getCDCState() != CDCState.FORBIDDEN)\r\n        {\r\n            // Hard link file in cdc folder for realtime tracking\r\n            FileUtils.createHardLink(segment.logFile, segment.getCDCFile());\r\n        }\r\n\r\n        cdcSizeTracker.processNewSegment(segment);\r\n        return segment;\r\n    }\r\n```\r\n\r\nFurther, we'd need to add logic to hard link the file on transition from FORBIDDEN to PERMITTED if that happened later in its lifecycle.\r\n\r\nThat make sense?"", 'commenter': 'jmckenzie-dev'}, {'comment': 'It makes sense. The if statement is what I proposed in the earlier comment. The ease of handling in the current implementation is what I guessed by reading the code.', 'commenter': 'yifan-c'}]"
1770,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -190,14 +189,20 @@ public CommitLogSegment.Allocation allocate(Mutation mutation, int size) throws
         return alloc;
     }
 
-    // Non-blocking mode has just recently been enabled for CDC.
-    // The segment is still marked as FORBIDDEN. It should be set to PERMITTED.
-    private void ensureSegmentPermittedIfNotBlockWrites(CommitLogSegment segment)
+    // Permit a forbidden segment under the following conditions.
+    // - Non-blocking mode has just recently been enabled for CDC.
+    // - The CDC total space has droppped below the limit (e.g. CDC consumer cleans up).
+    private void permitSegmentMaybe(CommitLogSegment segment)
     {
-        if (!DatabaseDescriptor.getCDCBlockWrites() && segment.getCDCState() == CDCState.FORBIDDEN)
+        if (segment.getCDCState() != CDCState.FORBIDDEN)
+            return;
+
+        if (!DatabaseDescriptor.getCDCBlockWrites()
+            || cdcSizeTracker.sizeInProgress.get() < DatabaseDescriptor.getCDCTotalSpace())
         {
             segment.setCDCState(CDCState.PERMITTED);
         }
+","[{'comment': 'nit: extra whitespace here', 'commenter': 'jmckenzie-dev'}]"
1770,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -219,6 +224,8 @@ private void throwIfForbidden(Mutation mutation, CommitLogSegment segment) throw
     /**
      * On segment creation, flag whether the segment should accept CDC mutations or not based on the total currently
      * allocated unflushed CDC segments and the contents of cdc_raw
+     *
+     * Synchronized on this","[{'comment': ""This is only called in one place; recommend we forklift this code out and above to where it's called so it's clear the lock is held while these operations take place."", 'commenter': 'jmckenzie-dev'}, {'comment': 'The method is an abstract one defined in the base class though. Alternatively, we can add `synchronized` to the method signature. Synchronize within a synchronized block should add trivial impact. ', 'commenter': 'yifan-c'}, {'comment': ""Ah, you're right. Let's add a `@Override` then and leave it as is. 😀 "", 'commenter': 'jmckenzie-dev'}]"
1770,src/java/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDC.java,"@@ -386,9 +388,6 @@ private void recalculateOverflowSize()
         {
             rateLimiter.acquire();
             calculateSize();","[{'comment': ""With this change we should probably go ahead and lift up the calculateSize() code body into the `submitOverflowSizeRecalculation` since it's only called in one place. Saves future maintainers the jump to trace operations."", 'commenter': 'jmckenzie-dev'}]"
1770,test/unit/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDCTest.java,"@@ -91,7 +91,7 @@ public void testCDCWriteFailure() throws Throwable
                 FileUtils.deleteWithConfirm(f);
 
             // Update size tracker to reflect deleted files. Should flip flag on current allocatingFrom to allow.
-            cdcMgr.updateCDCTotalSize();
+            long x = cdcMgr.updateCDCTotalSize();","[{'comment': 'unused variable declaration', 'commenter': 'jmckenzie-dev'}]"
1773,src/java/org/apache/cassandra/service/StorageService.java,"@@ -1855,14 +1855,21 @@ public Collection<InetAddressAndPort> prepareForBootstrap(long schemaTimeoutMill
                 }
 
                 // check for operator errors...
+                long nanoDelay = MILLISECONDS.toNanos(ringTimeoutMillis);","[{'comment': 'no need to recalculate this value in every iteration of the loop. So moved outside of the for loop', 'commenter': 'frankgh'}, {'comment': 'also using `MILLISECONDS.toNanos` here to avoid conversion errors', 'commenter': 'frankgh'}]"
1773,src/java/org/apache/cassandra/service/StorageService.java,"@@ -1855,14 +1855,21 @@ public Collection<InetAddressAndPort> prepareForBootstrap(long schemaTimeoutMill
                 }
 
                 // check for operator errors...
+                long nanoDelay = MILLISECONDS.toNanos(ringTimeoutMillis);
                 for (Token token : bootstrapTokens)
                 {
                     InetAddressAndPort existing = tokenMetadata.getEndpoint(token);
                     if (existing != null)
                     {
-                        long nanoDelay = ringTimeoutMillis * 1000000L;
-                        if (Gossiper.instance.getEndpointStateForEndpoint(existing).getUpdateTimestamp() > (nanoTime() - nanoDelay))
+                        EndpointState endpointStateForExisting = Gossiper.instance.getEndpointStateForEndpoint(existing);
+                        long updateTimestamp = endpointStateForExisting.getUpdateTimestamp();
+                        long allowedDelay = nanoTime() - nanoDelay;","[{'comment': ""don't need to save to a local variable, can keep in the if statement"", 'commenter': 'dcapwell'}, {'comment': 'we use it in the if statement as well as the log', 'commenter': 'frankgh'}]"
1773,src/java/org/apache/cassandra/service/StorageService.java,"@@ -1855,14 +1855,21 @@ public Collection<InetAddressAndPort> prepareForBootstrap(long schemaTimeoutMill
                 }
 
                 // check for operator errors...
+                long nanoDelay = MILLISECONDS.toNanos(ringTimeoutMillis);
                 for (Token token : bootstrapTokens)
                 {
                     InetAddressAndPort existing = tokenMetadata.getEndpoint(token);
                     if (existing != null)
                     {
-                        long nanoDelay = ringTimeoutMillis * 1000000L;
-                        if (Gossiper.instance.getEndpointStateForEndpoint(existing).getUpdateTimestamp() > (nanoTime() - nanoDelay))
+                        EndpointState endpointStateForExisting = Gossiper.instance.getEndpointStateForEndpoint(existing);
+                        long updateTimestamp = endpointStateForExisting.getUpdateTimestamp();
+                        long allowedDelay = nanoTime() - nanoDelay;
+                        if (updateTimestamp > allowedDelay && endpointStateForExisting.isAlive())","[{'comment': 'should be `||`. If it was updated within the last ring delay or we think its alive, then fail', 'commenter': 'dcapwell'}]"
1773,src/java/org/apache/cassandra/service/StorageService.java,"@@ -1855,14 +1855,23 @@ public Collection<InetAddressAndPort> prepareForBootstrap(long schemaTimeoutMill
                 }
 
                 // check for operator errors...
+                long nanoDelay = MILLISECONDS.toNanos(ringTimeoutMillis);
                 for (Token token : bootstrapTokens)
                 {
                     InetAddressAndPort existing = tokenMetadata.getEndpoint(token);
                     if (existing != null)
                     {
-                        long nanoDelay = ringTimeoutMillis * 1000000L;
-                        if (Gossiper.instance.getEndpointStateForEndpoint(existing).getUpdateTimestamp() > (nanoTime() - nanoDelay))
+                        EndpointState endpointStateForExisting = Gossiper.instance.getEndpointStateForEndpoint(existing);
+                        long updateTimestamp = endpointStateForExisting.getUpdateTimestamp();
+                        long allowedDelay = nanoTime() - nanoDelay;
+
+                        // if the node was updated within the ring delay or the node is alive, we should fail
+                        if (updateTimestamp > allowedDelay || endpointStateForExisting.isAlive())
+                        {
+                            logger.error(""Unable to replace node for token={}. The node is reporting as alive with updateTimestamp={} which exceeds the allowedDelay={}"",","[{'comment': '> The node is reporting as alive\r\n\r\nThis log can be confusing in the case that the endpoint is `isAlive` but the update <= allowedDelay, can you rework this to handle both cases?', 'commenter': 'dcapwell'}, {'comment': 'good catch, I have updated the log statement', 'commenter': 'frankgh'}]"
1773,src/java/org/apache/cassandra/service/StorageService.java,"@@ -1855,14 +1855,23 @@ public Collection<InetAddressAndPort> prepareForBootstrap(long schemaTimeoutMill
                 }
 
                 // check for operator errors...
+                long nanoDelay = MILLISECONDS.toNanos(ringTimeoutMillis);
                 for (Token token : bootstrapTokens)
                 {
                     InetAddressAndPort existing = tokenMetadata.getEndpoint(token);
                     if (existing != null)
                     {
-                        long nanoDelay = ringTimeoutMillis * 1000000L;
-                        if (Gossiper.instance.getEndpointStateForEndpoint(existing).getUpdateTimestamp() > (nanoTime() - nanoDelay))
+                        EndpointState endpointStateForExisting = Gossiper.instance.getEndpointStateForEndpoint(existing);
+                        long updateTimestamp = endpointStateForExisting.getUpdateTimestamp();
+                        long allowedDelay = nanoTime() - nanoDelay;
+
+                        // if the node was updated within the ring delay or the node is alive, we should fail
+                        if (updateTimestamp > allowedDelay || endpointStateForExisting.isAlive())
+                        {
+                            logger.error(""Unable to replace node for token={}. The node is reporting as {}alive with updateTimestamp={} which exceeds the allowedDelay={}"",","[{'comment': '> which exceeds the allowedDelay={}\r\n\r\nagain, this may not be true. you can keep it simple and just log the values', 'commenter': 'dcapwell'}, {'comment': 'spoke in slack `Unable to replace node for token={}. The node is reporting as {}alive with updateTimestamp={}, allowedDelay={}""` works for me!', 'commenter': 'dcapwell'}]"
1799,.github/pull_request_template.md,"@@ -0,0 +1,32 @@
+## Asscociated Issue
+ 
+Replace this text with the Jira issue resolved (e.g. CASSANDRA-####) text specifying why this is a trivial patch.
+
+## Author(s) and Reviewers
+
+Patch by <author>; reviewed by <reviewer[s]>
+
+Co-authored-by: author1 <author1@domain.com>, author2 <author2@domain.com>, ..., authorN <authorN@domain.com>
+
+## What the patch does
+
+Replace this text with a detailed commit paragraph of why and how the change was made and relevant context in a paragraph here
+
+##  Recommendations:
+
+- Commits should be squashed to remove intermediate development commit messages.
+- Key commit messages start with the issue number (CASSANDRA-xxxx)
+- if not a trivial change:","[{'comment': ""personally I would prefer we don't have this line and leave figuring out if trivial to the committers, so keep line 20 to 22, and remove the indent.  What is trivial for one may not be for another, so defaulting to best practice with committers being able to allow special cases for trivial work."", 'commenter': 'dcapwell'}, {'comment': '+1 to removing this.  What is and what is not trivial is up the committer ultimately.', 'commenter': 'driftx'}, {'comment': 'While committer is the ultimate authority, the author is making a request.  If the author believes it to be trivial the author should state why.  If committer agrees then it is trivial.  This is the opportunity for the author to make the case that the change is trivial.  If the change is trivial then there may not be a JRIA ticket, there should not be documentation changes (unless it is a documentation typo) and there should not be additional testing required.  If any of those exist then the change is probably not trivial.\r\n\r\nSee now that the squash comment and the commit messages start with are not appropriate and will remove them.\r\n', 'commenter': 'Claudenw'}]"
1799,.github/pull_request_template.md,"@@ -0,0 +1,32 @@
+## Asscociated Issue
+ 
+Replace this text with the Jira issue resolved (e.g. CASSANDRA-####) or text specifying why this is a trivial patch.
+
+## Author(s) and Reviewers
+
+Patch by &lt;author>; reviewed by <reviewer[s]>","[{'comment': ' needs to be correct to match that found in https://cassandra.apache.org/_/development/how_to_commit.html ', 'commenter': 'michaelsembwever'}, {'comment': 'So this should literally read:\r\n\r\n----\r\n<One sentence description, usually Jira title or CHANGES.txt summary>\r\n&lt;Optional lengthier description>\r\n\r\npatch by &lt;Authors>; reviewed by &lt;Reviewers> for CASSANDRA-#####\r\n\r\n\r\nCo-authored-by: Name1 &lt;email1>\r\nCo-authored-by: Name2 &lt;email2>\r\n\r\n----\r\n\r\n@michaelsembwever  What if there is no Jira ticket?  What do we put in for a trivial pull?', 'commenter': 'Claudenw'}]"
1799,.github/pull_request_template.md,"@@ -0,0 +1,32 @@
+## Asscociated Issue
+ 
+Replace this text with the Jira issue resolved (e.g. CASSANDRA-####) or text specifying why this is a trivial patch.
+
+## Author(s) and Reviewers
+
+Patch by &lt;author>; reviewed by <reviewer[s]>
+
+Co-authored-by: First Last <author1@domain.com><br/>
+Co-authored-by: First2 Last2 <author2@domain.com>
+
+
+## What the patch does
+
+Replace this text with a detailed commit paragraph of why and how the change was made and relevant context.","[{'comment': ""i say remove. it's in the commit message already."", 'commenter': 'michaelsembwever'}, {'comment': 'It is not in the commit if the pull request is not a patch. (like this one)', 'commenter': 'Claudenw'}, {'comment': ""i don't understand. how do you create a pull request without a commit? "", 'commenter': 'michaelsembwever'}]"
1799,.github/pull_request_template.md,"@@ -0,0 +1,32 @@
+## Asscociated Issue
+ 
+Replace this text with the Jira issue resolved (e.g. CASSANDRA-####) or text specifying why this is a trivial patch.
+
+## Author(s) and Reviewers
+
+Patch by &lt;author>; reviewed by <reviewer[s]>
+
+Co-authored-by: First Last <author1@domain.com><br/>
+Co-authored-by: First2 Last2 <author2@domain.com>
+
+
+## What the patch does
+
+Replace this text with a detailed commit paragraph of why and how the change was made and relevant context.
+
+##  Recommendations:
+
+if not a trivial change:
+  - Jira ticket contains a description of: what is fixed, why it is needed, and what branches to apply it to.
+  - Tests are included.
+  - Documentation changes and/or updates are included.
+  
+
+## Notice
+By submitting this pull request, I acknowledge that I am making a contribution to the Apache Software Foundation under the terms and conditions of the Contributor's Agreement.","[{'comment': 'i say remove this. it is not required.', 'commenter': 'michaelsembwever'}]"
1799,.github/pull_request_template.md,"@@ -0,0 +1,32 @@
+## Asscociated Issue
+ 
+Replace this text with the Jira issue resolved (e.g. CASSANDRA-####) or text specifying why this is a trivial patch.
+
+## Author(s) and Reviewers
+
+Patch by &lt;author>; reviewed by <reviewer[s]>
+
+Co-authored-by: First Last <author1@domain.com><br/>
+Co-authored-by: First2 Last2 <author2@domain.com>
+
+
+## What the patch does
+
+Replace this text with a detailed commit paragraph of why and how the change was made and relevant context.
+
+##  Recommendations:
+
+if not a trivial change:
+  - Jira ticket contains a description of: what is fixed, why it is needed, and what branches to apply it to.
+  - Tests are included.
+  - Documentation changes and/or updates are included.
+  
+
+## Notice
+By submitting this pull request, I acknowledge that I am making a contribution to the Apache Software Foundation under the terms and conditions of the Contributor's Agreement.
+
+###References:
+* The [Apache Cassandra ""Contributing to Cassandra"" guide](https://cassandra.apache.org/_/development/index.html) 
+* The [Apache Cassandra ""Working on Documentation"" guide](https://cassandra.apache.org/_/development/documentation.html)","[{'comment': ""remove this, it's already part of the previous link."", 'commenter': 'michaelsembwever'}, {'comment': 'I think we need to look at this as a possible first point of contact.  A new person grabs a ticket, clones the repository, makes a change, submits a pull request.  This is the first place the project has an opportunity to introduce the new person to the project standard practices.  Providing reference links is a way to do this.  Providing deeper links (like the documentation link) is a way to promote contributions in those spaces.\r\n\r\nFor these reasons I think that these 2 links should stay.', 'commenter': 'Claudenw'}, {'comment': ""Disagree, it's duplicating information. The documentation page is embedded in the first link."", 'commenter': 'michaelsembwever'}, {'comment': ""`duplicating information` <- duplication isn't always bad. The goal here is to provide context and framing for folks to get situated, and so long as we don't literally have redundancy on the front-facing store of information here, I think we should keep it."", 'commenter': 'jmckenzie-dev'}, {'comment': 'but why the documentation, instead of duplicating any of the other parts of that page? (i would have thought code style was far more valuable to the newcomer).\r\n\r\nthe very first thing on the index.html page is the list of sections, documentation being one of them. ', 'commenter': 'michaelsembwever'}]"
1799,.github/pull_request_template.md,"@@ -0,0 +1,32 @@
+## Asscociated Issue
+ 
+Replace this text with the Jira issue resolved (e.g. CASSANDRA-####) or text specifying why this is a trivial patch.
+
+## Author(s) and Reviewers
+
+Patch by &lt;author>; reviewed by <reviewer[s]>
+
+Co-authored-by: First Last <author1@domain.com><br/>
+Co-authored-by: First2 Last2 <author2@domain.com>
+
+
+## What the patch does
+
+Replace this text with a detailed commit paragraph of why and how the change was made and relevant context.
+
+##  Recommendations:
+
+if not a trivial change:
+  - Jira ticket contains a description of: what is fixed, why it is needed, and what branches to apply it to.
+  - Tests are included.
+  - Documentation changes and/or updates are included.
+  
+
+## Notice
+By submitting this pull request, I acknowledge that I am making a contribution to the Apache Software Foundation under the terms and conditions of the Contributor's Agreement.
+
+###References:
+* The [Apache Cassandra ""Contributing to Cassandra"" guide](https://cassandra.apache.org/_/development/index.html) 
+* The [Apache Cassandra ""Working on Documentation"" guide](https://cassandra.apache.org/_/development/documentation.html)
+* The [Apache Contributor's Agreement](https://www.apache.org/licenses/contributor-agreements.html).","[{'comment': 'i say remove this. it is not required and will only scare people off.', 'commenter': 'michaelsembwever'}, {'comment': 'I am not sure about scaring people off but, after checking on the legal stance, it seems that the person merging the pull request is required to have the ICLA and be responsible for the merge.  So I am happy to remove it.', 'commenter': 'Claudenw'}, {'comment': 'I disagree about this scaring people off. The second paragraph in the referenced page is:\r\n\r\n> These agreements help us achieve our goal of providing reliable and long-lived software products through collaborative, open-source software development. In all cases, contributors retain full rights to use their original contributions for any other purpose outside of Apache, while providing the ASF and its projects the right to distribute and build upon their work.\r\n\r\nWhich seems completely inclusive and reasonable to me.', 'commenter': 'jmckenzie-dev'}, {'comment': ""it's out of scope for the PR. we won't actually be asking the contributor for an ICLA. creates unnecessary confusion.\r\n\r\nspeaking from experience, i don't contribute simple one-liners PRs to other projects if i'm confronted with such a barrage of information. but that's just me."", 'commenter': 'michaelsembwever'}]"
1799,README.asc,"@@ -9,6 +9,8 @@ https://cwiki.apache.org/confluence/display/CASSANDRA2/DataModel[Row store] mean
 
 For more information, see http://cassandra.apache.org/[the Apache Cassandra web site].
 
+Issues should be reported on The https://issues.apache.org/jira/projects/CASSANDRA/issues/[The Cassandra Jira].","[{'comment': '@Claudenw there is double ""The"", one after ""on"" and one in the link.', 'commenter': 'smiklosovic'}, {'comment': 'fixed', 'commenter': 'Claudenw'}]"
1799,.github/pull_request_template.md,"@@ -0,0 +1,25 @@
+Thanks for sending a pull request! Here are some tips if you're new here:
+ 
+ * Ensure you have added or run the [appropriate tests](https://cassandra.apache.org/_/development/testing.html) for your PR","[{'comment': 'Add dot at the end please.', 'commenter': 'smiklosovic'}, {'comment': 'fixed', 'commenter': 'Claudenw'}]"
1817,src/java/org/apache/cassandra/service/StorageService.java,"@@ -3479,16 +3479,39 @@ public void forceTerminateAllRepairSessions() {
         ActiveRepairService.instance.terminateSessions();
     }
 
+    @Deprecated","[{'comment': '`@Deprecated` is very costly to users, so I am not in favor of asking users to migrate to a new method to fix a bug... we should just fix the bug and remove `@Deprecated`', 'commenter': 'dcapwell'}]"
1831,src/java/org/apache/cassandra/schema/SchemaKeyspace.java,"@@ -563,6 +565,11 @@ private static void addTableParamsToRowBuilder(TableParams params, Row.SimpleBui
         // in mixed operation with pre-4.1 versioned node during upgrades.
         if (params.memtable != MemtableParams.DEFAULT)
             builder.add(""memtable"", params.memtable.configurationKey());
+
+        // As above, only add the allow_auto_snapshot column if the value is not default (true) and
+        // auto-snapshotting is enabled, to avoid RTE in pre-4.2 versioned node during upgrades
+        if (DatabaseDescriptor.isAutoSnapshot() && !params.allowAutoSnapshot)","[{'comment': ""I would eliminate the `DatabaseDescriptor.isAutoSnapshot()` condition here - I'm unsure why it was added here? Only the value of the table param is important here."", 'commenter': 'iamaleksey'}]"
1831,src/java/org/apache/cassandra/schema/SchemaKeyspace.java,"@@ -955,6 +962,10 @@ private static TableMetadata fetchTable(String keyspaceName, String tableName, T
     static TableParams createTableParamsFromRow(UntypedResultSet.Row row)
     {
         return TableParams.builder()
+                          // allow_auto_snapshot column was introduced in 4.2
+                          .allowAutoSnapshot(row.has(""allow_auto_snapshot"") ?
+                                             row.getBoolean(""allow_auto_snapshot"", DatabaseDescriptor.isAutoSnapshot()) :
+                                             DatabaseDescriptor.isAutoSnapshot())","[{'comment': ""Again, when it comes to schema things, `DatabaseDescriptor.isAutoSnapshot()` shouldn't be relevant. Only set the value on the builder at all if the column is present in the row. Otherwise let it pick up the default builder value of `true` and later defer to `ColumnFamilyStore.isAutoSnapshot()` to see if both the per-table setting and the node-local one are true."", 'commenter': 'iamaleksey'}]"
1831,src/java/org/apache/cassandra/cql3/UntypedResultSet.java,"@@ -359,6 +359,12 @@ public boolean getBoolean(String column)
             return BooleanType.instance.compose(data.get(column));
         }
 
+        public boolean getBoolean(String column, boolean ifNull)
+        {
+            ByteBuffer bytes = data.get(column);
+            return bytes == null ? ifNull : BooleanType.instance.compose(bytes);
+        }
+","[{'comment': 'The method will no longer be needed if the other comment in this review is followed.', 'commenter': 'iamaleksey'}]"
1831,src/java/org/apache/cassandra/schema/SchemaKeyspace.java,"@@ -568,8 +568,7 @@ private static void addTableParamsToRowBuilder(TableParams params, Row.SimpleBui
 
         // As above, only add the allow_auto_snapshot column if the value is not default (true) and
         // auto-snapshotting is enabled, to avoid RTE in pre-4.2 versioned node during upgrades
-        if (DatabaseDescriptor.isAutoSnapshot() && !params.allowAutoSnapshot)
-            builder.add(""allow_auto_snapshot"", false);
+        builder.add(""allow_auto_snapshot"", params.allowAutoSnapshot);","[{'comment': ""You still want to gate it on `!params.allowAutoSnapshot`, so it's only persisted if explicitly set to `false`. Just don't involve `DatabaseDescriptor.isAutoSnapshot()`."", 'commenter': 'iamaleksey'}]"
1831,src/java/org/apache/cassandra/schema/SchemaKeyspace.java,"@@ -961,31 +960,33 @@ private static TableMetadata fetchTable(String keyspaceName, String tableName, T
     @VisibleForTesting
     static TableParams createTableParamsFromRow(UntypedResultSet.Row row)
     {
-        return TableParams.builder()
-                          // allow_auto_snapshot column was introduced in 4.2
-                          .allowAutoSnapshot(row.has(""allow_auto_snapshot"") ?
-                                             row.getBoolean(""allow_auto_snapshot"", DatabaseDescriptor.isAutoSnapshot()) :
-                                             DatabaseDescriptor.isAutoSnapshot())
-                          .bloomFilterFpChance(row.getDouble(""bloom_filter_fp_chance""))
-                          .caching(CachingParams.fromMap(row.getFrozenTextMap(""caching"")))
-                          .comment(row.getString(""comment""))
-                          .compaction(CompactionParams.fromMap(row.getFrozenTextMap(""compaction"")))
-                          .compression(CompressionParams.fromMap(row.getFrozenTextMap(""compression"")))
-                          .memtable(MemtableParams.get(row.has(""memtable"") ? row.getString(""memtable"") : null)) // memtable column was introduced in 4.1
-                          .defaultTimeToLive(row.getInt(""default_time_to_live""))
-                          .extensions(row.getFrozenMap(""extensions"", UTF8Type.instance, BytesType.instance))
-                          .gcGraceSeconds(row.getInt(""gc_grace_seconds""))
-                          .maxIndexInterval(row.getInt(""max_index_interval""))
-                          .memtableFlushPeriodInMs(row.getInt(""memtable_flush_period_in_ms""))
-                          .minIndexInterval(row.getInt(""min_index_interval""))
-                          .crcCheckChance(row.getDouble(""crc_check_chance""))
-                          .speculativeRetry(SpeculativeRetryPolicy.fromString(row.getString(""speculative_retry"")))
-                          .additionalWritePolicy(row.has(""additional_write_policy"") ?
-                                                     SpeculativeRetryPolicy.fromString(row.getString(""additional_write_policy"")) :
-                                                     SpeculativeRetryPolicy.fromString(""99PERCENTILE""))
-                          .cdc(row.has(""cdc"") && row.getBoolean(""cdc""))
-                          .readRepair(getReadRepairStrategy(row))
-                          .build();
+        TableParams.Builder builder = TableParams.builder()
+                                                 .bloomFilterFpChance(row.getDouble(""bloom_filter_fp_chance""))
+                                                 .caching(CachingParams.fromMap(row.getFrozenTextMap(""caching"")))
+                                                 .comment(row.getString(""comment""))
+                                                 .compaction(CompactionParams.fromMap(row.getFrozenTextMap(""compaction"")))
+                                                 .compression(CompressionParams.fromMap(row.getFrozenTextMap(""compression"")))
+                                                 .memtable(MemtableParams.get(row.has(""memtable"") ? row.getString(""memtable"") : null)) // memtable column was introduced in 4.1
+                                                 .defaultTimeToLive(row.getInt(""default_time_to_live""))
+                                                 .extensions(row.getFrozenMap(""extensions"", UTF8Type.instance, BytesType.instance))
+                                                 .gcGraceSeconds(row.getInt(""gc_grace_seconds""))
+                                                 .maxIndexInterval(row.getInt(""max_index_interval""))
+                                                 .memtableFlushPeriodInMs(row.getInt(""memtable_flush_period_in_ms""))
+                                                 .minIndexInterval(row.getInt(""min_index_interval""))
+                                                 .crcCheckChance(row.getDouble(""crc_check_chance""))
+                                                 .speculativeRetry(SpeculativeRetryPolicy.fromString(row.getString(""speculative_retry"")))
+                                                 .additionalWritePolicy(row.has(""additional_write_policy"") ?
+                                                                        SpeculativeRetryPolicy.fromString(row.getString(""additional_write_policy"")) :
+                                                                        SpeculativeRetryPolicy.fromString(""99PERCENTILE""))
+                                                 .cdc(row.has(""cdc"") && row.getBoolean(""cdc""))
+                                                 .readRepair(getReadRepairStrategy(row));
+
+        // allow_auto_snapshot column was introduced in 4.2
+        if (row.has(""allow_auto_snapshot"")) {
+            builder.allowAutoSnapshot(row.getBoolean(""allow_auto_snapshot""));
+        }","[{'comment': ""Either put braces on a new line or don't do braces at all? According to our style guide. I would prefer the latter, no braces."", 'commenter': 'iamaleksey'}]"
1831,src/java/org/apache/cassandra/cql3/statements/schema/TableAttributes.java,"@@ -98,6 +98,9 @@ public static Set<String> allKeywords()
 
     private TableParams build(TableParams.Builder builder)
     {
+        if (hasOption(Option.ALLOW_AUTO_SNAPSHOT))
+            builder.allowAutoSnapshot(getBoolean(Option.ALLOW_AUTO_SNAPSHOT.toString(), false));","[{'comment': ""I don't think this default value is correct. But more importantly, we know the option is present, so we shouldn't even be passing the default. Need to add a method that doesn't take a default value. And also do the same for `cdc` while here."", 'commenter': 'iamaleksey'}]"
1948,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -252,7 +255,7 @@ else if (hasStaticChanges)
             else if (hasRegularChanges)
                 return justRegular;
             else
-                throw new IllegalArgumentException();
+                throw new IllegalArgumentException(""CommandsForKey has_modifications="" + commandsForKey.hasModifications() + "", but no Static or Regular columns changed!"");","[{'comment': 'this was useful during debugging, but also has the side benefit if we start calling when hasModification=false we see clearly', 'commenter': 'dcapwell'}]"
1948,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -570,12 +553,12 @@ public static AccordCommand loadCommand(AccordCommandStore commandStore, TxnId t
         return command;
     }
 
-    private static <T> T deserializeWithVersionOr(UntypedResultSet.Row row, String dataColumn, String versionColumn, IVersionedSerializer<T> serializer, Supplier<T> defaultSupplier) throws IOException
+    private static <T> T deserializeWithVersionOr(UntypedResultSet.Row row, String dataColumn, LocalVersionedSerializer<T> serializer, Supplier<T> defaultSupplier) throws IOException
     {
-        if (!row.has(versionColumn))
+        if (!row.has(dataColumn))","[{'comment': ""This isn't a regression as `null` writes to data column are no longer a null but `version:null`, so the data column will exist where it didn't before."", 'commenter': 'dcapwell'}]"
1948,test/simulator/main/org/apache/cassandra/simulator/SimulationRunner.java,"@@ -376,6 +376,7 @@ protected void run(long seed, B builder) throws IOException
                 catch (Throwable t)
                 {
                     logger().error(""Failed on seed 0x{}"", Long.toHexString(seed), t);
+                    throw t;","[{'comment': 'this was added as the simulator JVM would always have rc=0... ', 'commenter': 'dcapwell'}]"
1948,test/simulator/main/org/apache/cassandra/simulator/SimulatorUtils.java,"@@ -40,9 +40,10 @@ public static RuntimeException failWithOOM()
     public static void dumpStackTraces(Logger logger)
     {
         Map<Thread, StackTraceElement[]> threadMap = Thread.getAllStackTraces();
-        threadMap.forEach((thread, ste) -> {
-            logger.error(""{}:\n   {}"", thread, Threads.prettyPrint(ste, false, ""   "", ""\n"", """"));
-        });
+        String prefix = ""   "";
+        String delimiter = ""\n"" + prefix;","[{'comment': ""`delimiter` doesn't include the `prefix` so to make the logs clean, I had to manually add"", 'commenter': 'dcapwell'}]"
1948,test/simulator/main/org/apache/cassandra/simulator/paxos/AccordSimulationRunner.java,"@@ -60,19 +60,27 @@ public static class Help extends HelpCommand<AccordClusterSimulation.Builder> {}
     /**
      * See {@link org.apache.cassandra.simulator} package info for execution tips
      */
-    public static void main(String[] args) throws IOException
+    public static void main(String... args) throws IOException
     {
         AccordClusterSimulation.Builder builder = new AccordClusterSimulation.Builder();
         builder.unique(uniqueNum.getAndIncrement());
 
-        Cli.<ICommand<AccordClusterSimulation.Builder>>builder(""accord"")
-           .withCommand(Run.class)
-           .withCommand(Reconcile.class)
-           .withCommand(Record.class)
-           .withCommand(Help.class)
-           .withDefaultCommand(Help.class)
-           .build()
-           .parse(args)
-           .run(builder);
+        try
+        {
+            Cli.<ICommand<AccordClusterSimulation.Builder>>builder(""accord"")
+               .withCommand(Run.class)
+               .withCommand(Reconcile.class)
+               .withCommand(Record.class)
+               .withCommand(Help.class)
+               .withDefaultCommand(Help.class)
+               .build()
+               .parse(args)
+               .run(builder);
+            System.exit(0);
+        }
+        catch (Throwable t)
+        {
+            System.exit(1);","[{'comment': 'feel that all simulator runners need this... when an error is throw the `main` thread dies, but not the background non-daemon threads... ', 'commenter': 'dcapwell'}, {'comment': 'Ok this is a problem for current tests: https://app.circleci.com/pipelines/github/dcapwell/cassandra/1666/workflows/7dd47b41-c2ed-4074-a9cd-6f9a8a72a4cc/jobs/13823\r\n\r\nRather than do system exit we need to throw... but at least when I was testing we had daemon threads which blocked the JVM from exiting when there was an issue...  ', 'commenter': 'dcapwell'}]"
1948,test/distributed/org/apache/cassandra/distributed/test/simulator/AccordSimulationTest.java,"@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.simulator;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+import com.google.common.base.StandardSystemProperty;
+import org.junit.Test;
+
+//REIVEW: totally fine dropping this as it is a dupe of org.apache.cassandra.simulator.test.ShortAccordSimulationTest
+// the goal of this test was being able to run in intellij.  This also has the side benefit that ""ant testclasslist"" can
+// run this test; which gives us parallel execution for free!
+public class AccordSimulationTest
+
+{
+    private static String getSeed()
+    {
+        String value = System.getProperty(""cassandra.test.simulation.seed"");
+        if (value != null)
+            return value;
+        long seed = System.currentTimeMillis();
+        return ""0x"" + Long.toHexString(seed);
+    }
+
+    @Test
+    public void test() throws IOException, InterruptedException, TimeoutException
+    {
+        String seedString = getSeed();
+
+        String classpath = StandardSystemProperty.JAVA_CLASS_PATH.value();
+        File exec = new File(StandardSystemProperty.JAVA_HOME.value(), ""bin/java"");
+
+        List<String> args = new ArrayList<>();
+        args.add(exec.getAbsolutePath());
+        args.addAll(setupJVM());
+        args.add(""org.apache.cassandra.simulator.paxos.AccordSimulationRunner"");
+        args.addAll(Arrays.asList(""run"",
+                                  ""--seed"", seedString,
+                                  ""-n"", ""3..6"",
+                                  ""-t"", ""1000"",
+                                  ""--cluster-action-limit"", ""-1"",
+                                  ""-c"", ""2"",
+                                  ""-s"", ""30""));
+        ProcessBuilder pb = new ProcessBuilder();
+        pb.environment().put(""CLASSPATH"", classpath);
+        pb.command(args);
+        Process p = pb.inheritIO().start();
+        boolean completed = p.waitFor(5, TimeUnit.MINUTES);
+        if (!completed)
+            throw new TimeoutException(""Timeout waiting on simulation"");
+        int rc = p.waitFor();
+        if (rc != 0)","[{'comment': 'heh, without the logs the CI output looks lame\r\n\r\nhttps://app.circleci.com/pipelines/github/dcapwell/cassandra/1665/workflows/65e737da-5269-40c5-a504-5bbd2701c6f1/jobs/13785', 'commenter': 'dcapwell'}]"
1948,src/java/org/apache/cassandra/service/accord/serializers/AcceptSerializers.java,"@@ -104,22 +104,29 @@ public long serializedSize(Accept.Invalidate invalidate, int version)
         public void serialize(AcceptOk acceptOk, DataOutputPlus out, int version) throws IOException
         {
             CommandSerializers.txnId.serialize(acceptOk.txnId, out, version);
-            CommandSerializers.deps.serialize(acceptOk.deps, out, version);
+            boolean hasDeps = acceptOk.deps != null;","[{'comment': 'can we use NullableSerializer here?', 'commenter': 'bdeggleston'}, {'comment': 'ill try to read that and see how to adapt', 'commenter': 'dcapwell'}]"
1948,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -115,12 +115,12 @@
     private static final Logger logger = LoggerFactory.getLogger(AccordKeyspace.class);
 
     public static final String COMMANDS = ""commands"";
-    public static final String COMMAND_SERIES = ""command_series"";
     public static final String COMMANDS_FOR_KEY = ""commands_for_key"";
 
     private static final String TIMESTAMP_TUPLE = ""tuple<bigint, bigint, int, bigint>"";
     private static final TupleType TIMESTAMP_TYPE = new TupleType(Lists.newArrayList(LongType.instance, LongType.instance, Int32Type.instance, LongType.instance));
     private static final String KEY_TUPLE = ""tuple<uuid, blob>"";
+    private static final int CURRENT_VERSION = 1;","[{'comment': 'this should be `MessagingService.current_version`', 'commenter': 'bdeggleston'}, {'comment': ""why?  The point was to decouple the storage version from the messaging version; if messaging bumps that doesn't mean storage did"", 'commenter': 'dcapwell'}, {'comment': ""the version used in IVersionedSerializer is intended to be the messaging service version, to support serializer changes in mixed mode clusters, so if we're going to be passing this directly into serializers, it does need to map to a current messaging version. That said, decoupling accord serializers from the messaging service _is_ a good idea. I'm already doing this in AccordPartialCommand.PartialCommandSerializer.Version. Why not promote that enum to AccordSerializerVersion and use it instead? That way you can still pass valid message versions into downstream serializers"", 'commenter': 'bdeggleston'}, {'comment': ""yeah good point... the serialized used in the end also work on network, so versions *has* to be network in order to avoid duplication...\r\n\r\n>  I'm already doing this in AccordPartialCommand.PartialCommandSerializer.Version. Why not promote that enum to AccordSerializerVersion and use it instead?\r\n\r\nso the current value is `VERSION_0(0, MessagingService.current_version);` which has the bug that we redefine if anyone bumps; so should become `VERSION_0(0, MessagingService.VERSION_40);`...\r\n\r\nNow, lets say we do promote, then we would need to have something like an AccordSerializer which is a `IVersionedSerializer` but that maps `int version` to `enum AccordSerializerVersion` and delegates to copies of the methods that take the `AccordSerializerVersion`...\r\n\r\nOne thing also being pointed out here is that `PartialCommandSerializer` and `LocalVersionedSerializer` duplicate logic, so we should unify this.\r\n\r\nIll take a stab at unifying this logic...\r\n\r\n"", 'commenter': 'dcapwell'}, {'comment': 'ok push new commit 39c27064f085371718495e6fa557ef0ae3ff3f14 which attempts to unify the 2 code paths', 'commenter': 'dcapwell'}]"
1948,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -433,31 +424,27 @@ public static Mutation getCommandMutation(AccordCommand command, long timestampM
             Row.Builder builder = BTreeRow.unsortedBuilder();
             builder.newRow(Clustering.EMPTY);
             int nowInSeconds = (int) TimeUnit.MICROSECONDS.toSeconds(timestampMicros);
-            int version = MessagingService.current_version;
-            ByteBuffer versionBytes = accessor.valueOf(version);
+
 
             if (command.status.hasModifications())
                 builder.addCell(live(CommandsColumns.status, timestampMicros, accessor.valueOf(command.status.get().ordinal())));
 
             if (command.homeKey.hasModifications())
             {","[{'comment': 'we can drop the braces for the blocks that have become single lines', 'commenter': 'bdeggleston'}, {'comment': 'pushed, left statements that are single statements but long and spread cross multiple lines; example\r\n\r\n```\r\nif (command.storedListeners.hasModifications())\r\n            {\r\n                addStoredSetChanges(builder, CommandsColumns.listeners,\r\n                                    timestampMicros, nowInSeconds, command.storedListeners,\r\n                                    ListenerProxy::identifier);\r\n            }\r\n```', 'commenter': 'dcapwell'}]"
1948,src/java/org/apache/cassandra/service/accord/async/AsyncWriter.java,"@@ -238,7 +240,7 @@ private void denormalize(AccordCommand command, AsyncContext context, Object cal
         }
 
         // There won't be a txn to denormalize against until the command has been preaccepted
-        if (command.status().hasBeen(Status.PreAccepted) && AccordPartialCommand.WithDeps.serializer.needsUpdate(command))
+        if (command.status().hasBeen(Status.PreAccepted) && AccordPartialCommand.WithDeps.serializer.needsUpdate(command) && !(command.txn() == null && command.status().isInvalidated()))","[{'comment': ""Not denormalizing an invalidated command may leave it in the uncommitted set, which could lead to incorrect deps being reported. That said, cfk denormalization may no longer be needed, and I'm going to try removing it in another branch, so hang tight"", 'commenter': 'bdeggleston'}, {'comment': ""if we do need to denormalize then we need these messages updated to pass along the `Keys`, as right now we don't have access"", 'commenter': 'dcapwell'}, {'comment': 'yep makes sense, nevermind', 'commenter': 'bdeggleston'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -64,6 +67,13 @@ public class Mutation implements IMutation, Supplier<Mutation>
 
     private final boolean cdcEnabled;
 
+    // Contains serialized representations of this mutation.
+    // Note: there is no functionality to clear/remove serialized instances, because a mutation must never
+    // be modified (e.g. calling add(PartitionUpdate)) when it's being serialized.
+    private static final int CACHED_SERIALIZATIONS = MessagingService.Version.values().length;
+    private static final int CACHEABLE_MUTATION_SIZE_LIMIT = Integer.getInteger(Config.PROPERTY_PREFIX + ""cacheable_mutation_size_limit_bytes"", 2 * 1024 * 1024) - 24;","[{'comment': ""What's the -24 here for? Maybe document it?"", 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't","[{'comment': 'nit: ""compute and cache""', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't
+         * exists yet. Note that this method is _not_ synchronized even though it may (and will often) be called","[{'comment': 'nit: ""exist yet""', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't
+         * exists yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionIndex = MessagingService.getVersionIndex(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.serializations[versionIndex];
+            if (serialization == null)
+            {
+                // We need to use a capacity-limited DOB here.
+                // If a mutation consists of one PartitionUpdate with one column that exceeds the
+                // ""cacheable-mutation-size-limit"", a capacity-limited DOB can handle that case and
+                // throw a BufferCapacityExceededException. ""Huge"" serialized mutations can have a
+                // bad impact to G1 GC, if the cached serialized mutation results in a
+                // ""humonguous object"" and also frequent re-allocations of the scratch buffer(s).
+                // I.e. large cached mutation objects cause GC pressure.
+                try (DataOutputBuffer dob = DataOutputBuffer.limitedScratchBuffer(CACHEABLE_MUTATION_SIZE_LIMIT))
+                {
+                    if (!serializeInternal(PartitionUpdate.serializer, mutation, dob, version,true))
+                    {
+                        serialization = new NonCacheableSerialization();
+                    }
+                    else
+                    {
+                        serialization = new CachedSerialization(dob.toByteArray());
+                    }
+                }
+                catch (DataOutputBuffer.BufferCapacityExceededException tooBig)","[{'comment': 'What are the implications of us using exception handling basically for control flow in the case of mutations that exceed our cacheable capacity vs. some other less invasive method? Wondering from a performance perspective as this is hot path and exception handling is notoriously Not Fast.', 'commenter': 'jmckenzie-dev'}, {'comment': ""Yeah, I'm reworking this, good call out"", 'commenter': 'tjake'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't
+         * exists yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionIndex = MessagingService.getVersionIndex(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.serializations[versionIndex];
+            if (serialization == null)
+            {
+                // We need to use a capacity-limited DOB here.
+                // If a mutation consists of one PartitionUpdate with one column that exceeds the
+                // ""cacheable-mutation-size-limit"", a capacity-limited DOB can handle that case and
+                // throw a BufferCapacityExceededException. ""Huge"" serialized mutations can have a
+                // bad impact to G1 GC, if the cached serialized mutation results in a
+                // ""humonguous object"" and also frequent re-allocations of the scratch buffer(s).
+                // I.e. large cached mutation objects cause GC pressure.
+                try (DataOutputBuffer dob = DataOutputBuffer.limitedScratchBuffer(CACHEABLE_MUTATION_SIZE_LIMIT))
+                {
+                    if (!serializeInternal(PartitionUpdate.serializer, mutation, dob, version,true))
+                    {
+                        serialization = new NonCacheableSerialization();
+                    }
+                    else
+                    {
+                        serialization = new CachedSerialization(dob.toByteArray());
+                    }
+                }
+                catch (DataOutputBuffer.BufferCapacityExceededException tooBig)
+                {
+                    serialization = new NonCacheableSerialization();
+                }
+                catch (IOException e)
+                {
+                    throw new RuntimeException(e);
+                }
+                mutation.serializations[versionIndex] = serialization;
+            }
+            return serialization;
+        }
+
+        static boolean serializeInternal(PartitionUpdate.PartitionUpdateSerializer serializer,
+                                         Mutation mutation,
+                                         DataOutputPlus out,
+                                         int version,
+                                         boolean isPrepare) throws IOException
+        {
+            Map<TableId, PartitionUpdate> modifications = mutation.modifications;
+
             /* serialize the modifications in the mutation */
-            int size = mutation.modifications.size();
+            int size = modifications.size();
             out.writeUnsignedVInt(size);
 
             assert size > 0;
-            for (Map.Entry<TableId, PartitionUpdate> entry : mutation.modifications.entrySet())
-                PartitionUpdate.serializer.serialize(entry.getValue(), out, version);
+            for (PartitionUpdate partitionUpdate : modifications.values())
+            {
+                serializer.serialize(partitionUpdate, out, version);
+                if (isCacheableMutationSizeLimit(out, isPrepare))
+                    return false;
+            }
+            return true;
         }
 
-        public Mutation deserialize(DataInputPlus in, int version, DeserializationHelper.Flag flag) throws IOException
+        private static boolean isCacheableMutationSizeLimit(DataOutputPlus out, boolean isPrepare)
         {
-            int size = (int)in.readUnsignedVInt();
-            assert size > 0;
-
-            PartitionUpdate update = PartitionUpdate.serializer.deserialize(in, version, flag);
-            if (size == 1)
-                return new Mutation(update);
-
-            ImmutableMap.Builder<TableId, PartitionUpdate> modifications = new ImmutableMap.Builder<>();
-            DecoratedKey dk = update.partitionKey();
+            return isPrepare && out.position() > CACHEABLE_MUTATION_SIZE_LIMIT;","[{'comment': 'Consider a `CassandraRelevantProperties` for this parameter.', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/io/util/TeeDataInputPlus.java,"@@ -0,0 +1,216 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.util;
+
+import java.io.EOFException;
+import java.io.IOException;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * DataInput that also stores the raw inputs into an output buffer
+ * This is useful for storing serialized buffers as they are deserialized
+ */
+public class TeeDataInputPlus implements DataInputPlus
+{
+    private final DataInputPlus source;
+    private final DataOutputPlus teeBuffer;
+
+    private final long limit;
+    private boolean limitReached = false;
+
+    public TeeDataInputPlus(DataInputPlus source, DataOutputPlus teeBuffer)
+    {
+        this(source, teeBuffer, -1);
+    }
+
+    public TeeDataInputPlus(DataInputPlus source, DataOutputPlus teeBuffer, long limit)
+    {
+        this.source = source;","[{'comment': 'We may want to assert that neither input is null here. May not; just a thought.', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -298,29 +308,14 @@ public String toString(boolean shallow)
         }
         return buff.append(""])"").toString();
     }
-    private int serializedSize30;
-    private int serializedSize3014;
-    private int serializedSize40;
+
+    private int serializedSize;
 
     public int serializedSize(int version)
     {
-        switch (version)
-        {
-            case VERSION_30:
-                if (serializedSize30 == 0)
-                    serializedSize30 = (int) serializer.serializedSize(this, VERSION_30);
-                return serializedSize30;
-            case VERSION_3014:
-                if (serializedSize3014 == 0)
-                    serializedSize3014 = (int) serializer.serializedSize(this, VERSION_3014);
-                return serializedSize3014;
-            case VERSION_40:
-                if (serializedSize40 == 0)
-                    serializedSize40 = (int) serializer.serializedSize(this, VERSION_40);
-                return serializedSize40;
-            default:
-                throw new IllegalStateException(""Unknown serialization version: "" + version);
-        }
+        if (serializedSize == 0)","[{'comment': ""Looks like we're losing our safety check against a bad version being passed in. If we have confidence this is the case we should document this method w/the invariant of the version being valid and/or Precondition check it. Or just restore some variation of the prior switch that throws an `IllegalStateException`"", 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.","[{'comment': ""This is an invalid javadoc target; doesn't resolve and I can't find it in the codebase."", 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't
+         * exists yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionIndex = MessagingService.getVersionIndex(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.serializations[versionIndex];
+            if (serialization == null)
+            {
+                // We need to use a capacity-limited DOB here.
+                // If a mutation consists of one PartitionUpdate with one column that exceeds the
+                // ""cacheable-mutation-size-limit"", a capacity-limited DOB can handle that case and
+                // throw a BufferCapacityExceededException. ""Huge"" serialized mutations can have a
+                // bad impact to G1 GC, if the cached serialized mutation results in a
+                // ""humonguous object"" and also frequent re-allocations of the scratch buffer(s).
+                // I.e. large cached mutation objects cause GC pressure.
+                try (DataOutputBuffer dob = DataOutputBuffer.limitedScratchBuffer(CACHEABLE_MUTATION_SIZE_LIMIT))
+                {
+                    if (!serializeInternal(PartitionUpdate.serializer, mutation, dob, version,true))","[{'comment': 'A ternary operator on the construction of the serialization variable here may help tidy up clarity a bit.', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't
+         * exists yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionIndex = MessagingService.getVersionIndex(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.serializations[versionIndex];
+            if (serialization == null)
+            {
+                // We need to use a capacity-limited DOB here.
+                // If a mutation consists of one PartitionUpdate with one column that exceeds the
+                // ""cacheable-mutation-size-limit"", a capacity-limited DOB can handle that case and
+                // throw a BufferCapacityExceededException. ""Huge"" serialized mutations can have a
+                // bad impact to G1 GC, if the cached serialized mutation results in a
+                // ""humonguous object"" and also frequent re-allocations of the scratch buffer(s).
+                // I.e. large cached mutation objects cause GC pressure.
+                try (DataOutputBuffer dob = DataOutputBuffer.limitedScratchBuffer(CACHEABLE_MUTATION_SIZE_LIMIT))
+                {
+                    if (!serializeInternal(PartitionUpdate.serializer, mutation, dob, version,true))
+                    {
+                        serialization = new NonCacheableSerialization();
+                    }
+                    else
+                    {
+                        serialization = new CachedSerialization(dob.toByteArray());
+                    }
+                }
+                catch (DataOutputBuffer.BufferCapacityExceededException tooBig)
+                {
+                    serialization = new NonCacheableSerialization();
+                }
+                catch (IOException e)
+                {
+                    throw new RuntimeException(e);
+                }
+                mutation.serializations[versionIndex] = serialization;","[{'comment': 'I was a little startled to see a `NonCacheableSerialization`... cached here. :) Might be worth renaming it to something like `SizeOnlyCachedSerialization` or something?', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't
+         * exists yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionIndex = MessagingService.getVersionIndex(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.serializations[versionIndex];
+            if (serialization == null)
+            {
+                // We need to use a capacity-limited DOB here.
+                // If a mutation consists of one PartitionUpdate with one column that exceeds the
+                // ""cacheable-mutation-size-limit"", a capacity-limited DOB can handle that case and
+                // throw a BufferCapacityExceededException. ""Huge"" serialized mutations can have a
+                // bad impact to G1 GC, if the cached serialized mutation results in a
+                // ""humonguous object"" and also frequent re-allocations of the scratch buffer(s).
+                // I.e. large cached mutation objects cause GC pressure.
+                try (DataOutputBuffer dob = DataOutputBuffer.limitedScratchBuffer(CACHEABLE_MUTATION_SIZE_LIMIT))
+                {
+                    if (!serializeInternal(PartitionUpdate.serializer, mutation, dob, version,true))
+                    {
+                        serialization = new NonCacheableSerialization();
+                    }
+                    else
+                    {
+                        serialization = new CachedSerialization(dob.toByteArray());
+                    }
+                }
+                catch (DataOutputBuffer.BufferCapacityExceededException tooBig)
+                {
+                    serialization = new NonCacheableSerialization();
+                }
+                catch (IOException e)
+                {
+                    throw new RuntimeException(e);
+                }
+                mutation.serializations[versionIndex] = serialization;
+            }
+            return serialization;
+        }
+
+        static boolean serializeInternal(PartitionUpdate.PartitionUpdateSerializer serializer,","[{'comment': ""Recommend we change this to `maybeSerializeInternal` to reflect that it might or might not actually complete and that's acceptable. The fact that we use it w/out parsing the return value and also w/parsing it implies that there's some flexibility in that usage."", 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't
+         * exists yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionIndex = MessagingService.getVersionIndex(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.serializations[versionIndex];
+            if (serialization == null)
+            {
+                // We need to use a capacity-limited DOB here.
+                // If a mutation consists of one PartitionUpdate with one column that exceeds the
+                // ""cacheable-mutation-size-limit"", a capacity-limited DOB can handle that case and
+                // throw a BufferCapacityExceededException. ""Huge"" serialized mutations can have a
+                // bad impact to G1 GC, if the cached serialized mutation results in a
+                // ""humonguous object"" and also frequent re-allocations of the scratch buffer(s).
+                // I.e. large cached mutation objects cause GC pressure.
+                try (DataOutputBuffer dob = DataOutputBuffer.limitedScratchBuffer(CACHEABLE_MUTATION_SIZE_LIMIT))
+                {
+                    if (!serializeInternal(PartitionUpdate.serializer, mutation, dob, version,true))
+                    {
+                        serialization = new NonCacheableSerialization();
+                    }
+                    else
+                    {
+                        serialization = new CachedSerialization(dob.toByteArray());
+                    }
+                }
+                catch (DataOutputBuffer.BufferCapacityExceededException tooBig)
+                {
+                    serialization = new NonCacheableSerialization();
+                }
+                catch (IOException e)
+                {
+                    throw new RuntimeException(e);
+                }
+                mutation.serializations[versionIndex] = serialization;
+            }
+            return serialization;
+        }
+
+        static boolean serializeInternal(PartitionUpdate.PartitionUpdateSerializer serializer,
+                                         Mutation mutation,
+                                         DataOutputPlus out,
+                                         int version,
+                                         boolean isPrepare) throws IOException
+        {
+            Map<TableId, PartitionUpdate> modifications = mutation.modifications;
+
             /* serialize the modifications in the mutation */
-            int size = mutation.modifications.size();
+            int size = modifications.size();
             out.writeUnsignedVInt(size);
 
             assert size > 0;
-            for (Map.Entry<TableId, PartitionUpdate> entry : mutation.modifications.entrySet())
-                PartitionUpdate.serializer.serialize(entry.getValue(), out, version);
+            for (PartitionUpdate partitionUpdate : modifications.values())
+            {
+                serializer.serialize(partitionUpdate, out, version);
+                if (isCacheableMutationSizeLimit(out, isPrepare))
+                    return false;
+            }
+            return true;
         }
 
-        public Mutation deserialize(DataInputPlus in, int version, DeserializationHelper.Flag flag) throws IOException
+        private static boolean isCacheableMutationSizeLimit(DataOutputPlus out, boolean isPrepare)","[{'comment': ""Super nit: maybe we rename to `atCacheableMutationSizeLimit`? The name kind of confused me as to what it was doing; kind of assumed it was checking to see if the limit _existed_ not that we'd hit it."", 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/net/MessagingService.java,"@@ -212,6 +215,23 @@ public class MessagingService extends MessagingServiceMBeanImpl
     public static final int current_version = VERSION_40;
     static AcceptVersions accept_messaging = new AcceptVersions(minimum_version, current_version);
     static AcceptVersions accept_streaming = new AcceptVersions(current_version, current_version);
+    static Map<Integer, Integer> versionIndexMap = Arrays.stream(Version.values()).collect(Collectors.toMap(v -> v.value, v -> v.ordinal()));
+
+    /**
+     * This is an optimisation to speed up the translation of the serialization
+     * version to the {@link Version} enum.
+     *
+     * @param version the serialization version
+     * @return a {@link Version}
+     */
+    public static int getVersionIndex(int version)
+    {
+        if (versionIndexMap.containsKey(version))","[{'comment': 'I think we could just .get on the map and throw on value == null. Save us the lookup then retrieve steps.', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/net/MessagingService.java,"@@ -212,6 +215,23 @@ public class MessagingService extends MessagingServiceMBeanImpl
     public static final int current_version = VERSION_40;
     static AcceptVersions accept_messaging = new AcceptVersions(minimum_version, current_version);
     static AcceptVersions accept_streaming = new AcceptVersions(current_version, current_version);
+    static Map<Integer, Integer> versionIndexMap = Arrays.stream(Version.values()).collect(Collectors.toMap(v -> v.value, v -> v.ordinal()));
+
+    /**
+     * This is an optimisation to speed up the translation of the serialization
+     * version to the {@link Version} enum.
+     *
+     * @param version the serialization version
+     * @return a {@link Version}
+     */
+    public static int getVersionIndex(int version)
+    {
+        if (versionIndexMap.containsKey(version))
+            return versionIndexMap.get(version);
+        throw new IllegalStateException(""Unkown serialization version: "" + version);
+    }
+
+    public final static boolean NON_GRACEFUL_SHUTDOWN = Boolean.getBoolean(""cassandra.test.messagingService.nonGracefulShutdown"");","[{'comment': 'This looks vestigial.', 'commenter': 'jmckenzie-dev'}, {'comment': 'oops', 'commenter': 'tjake'}]"
1954,src/java/org/apache/cassandra/io/util/TeeDataInputPlus.java,"@@ -0,0 +1,216 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.util;
+
+import java.io.EOFException;
+import java.io.IOException;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * DataInput that also stores the raw inputs into an output buffer
+ * This is useful for storing serialized buffers as they are deserialized
+ */
+public class TeeDataInputPlus implements DataInputPlus
+{
+    private final DataInputPlus source;
+    private final DataOutputPlus teeBuffer;
+
+    private final long limit;","[{'comment': 'Why do we use -1 to indicate no limit here vs. the 0 to mean no limit in `DataOutputBuffer.LimitingScratchBuffer`?', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/io/util/TeeDataInputPlus.java,"@@ -0,0 +1,216 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.util;
+
+import java.io.EOFException;
+import java.io.IOException;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * DataInput that also stores the raw inputs into an output buffer
+ * This is useful for storing serialized buffers as they are deserialized","[{'comment': 'Would add something to class level javadoc about the API -> that writes will stop buffering when you hit the limit and you can check that state with `isLimitReached`.', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +388,130 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or computed and cache said serialization if it doesn't
+         * exists yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedEndpoints(Mutation, WriteEndpoints, WriteHandler, Verb.AckedRequest)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionIndex = MessagingService.getVersionIndex(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.serializations[versionIndex];
+            if (serialization == null)
+            {
+                // We need to use a capacity-limited DOB here.
+                // If a mutation consists of one PartitionUpdate with one column that exceeds the
+                // ""cacheable-mutation-size-limit"", a capacity-limited DOB can handle that case and
+                // throw a BufferCapacityExceededException. ""Huge"" serialized mutations can have a
+                // bad impact to G1 GC, if the cached serialized mutation results in a
+                // ""humonguous object"" and also frequent re-allocations of the scratch buffer(s).
+                // I.e. large cached mutation objects cause GC pressure.
+                try (DataOutputBuffer dob = DataOutputBuffer.limitedScratchBuffer(CACHEABLE_MUTATION_SIZE_LIMIT))
+                {
+                    if (!serializeInternal(PartitionUpdate.serializer, mutation, dob, version,true))
+                    {
+                        serialization = new NonCacheableSerialization();
+                    }
+                    else
+                    {
+                        serialization = new CachedSerialization(dob.toByteArray());
+                    }
+                }
+                catch (DataOutputBuffer.BufferCapacityExceededException tooBig)
+                {
+                    serialization = new NonCacheableSerialization();
+                }
+                catch (IOException e)
+                {
+                    throw new RuntimeException(e);
+                }
+                mutation.serializations[versionIndex] = serialization;
+            }
+            return serialization;
+        }
+
+        static boolean serializeInternal(PartitionUpdate.PartitionUpdateSerializer serializer,
+                                         Mutation mutation,
+                                         DataOutputPlus out,
+                                         int version,
+                                         boolean isPrepare) throws IOException
+        {
+            Map<TableId, PartitionUpdate> modifications = mutation.modifications;
+
             /* serialize the modifications in the mutation */
-            int size = mutation.modifications.size();
+            int size = modifications.size();
             out.writeUnsignedVInt(size);
 
             assert size > 0;
-            for (Map.Entry<TableId, PartitionUpdate> entry : mutation.modifications.entrySet())
-                PartitionUpdate.serializer.serialize(entry.getValue(), out, version);
+            for (PartitionUpdate partitionUpdate : modifications.values())
+            {
+                serializer.serialize(partitionUpdate, out, version);
+                if (isCacheableMutationSizeLimit(out, isPrepare))
+                    return false;
+            }
+            return true;
         }
 
-        public Mutation deserialize(DataInputPlus in, int version, DeserializationHelper.Flag flag) throws IOException
+        private static boolean isCacheableMutationSizeLimit(DataOutputPlus out, boolean isPrepare)
         {
-            int size = (int)in.readUnsignedVInt();
-            assert size > 0;
-
-            PartitionUpdate update = PartitionUpdate.serializer.deserialize(in, version, flag);
-            if (size == 1)
-                return new Mutation(update);
-
-            ImmutableMap.Builder<TableId, PartitionUpdate> modifications = new ImmutableMap.Builder<>();
-            DecoratedKey dk = update.partitionKey();
+            return isPrepare && out.position() > CACHEABLE_MUTATION_SIZE_LIMIT;
+        }
 
-            modifications.put(update.metadata().id, update);
-            for (int i = 1; i < size; ++i)
+        public Mutation deserialize(DataInputPlus in, int version, DeserializationHelper.Flag flag) throws IOException
+        {
+            Mutation m;
+            try (DataOutputBuffer dob = DataOutputBuffer.limitedScratchBuffer(CACHEABLE_MUTATION_SIZE_LIMIT))
             {
-                update = PartitionUpdate.serializer.deserialize(in, version, flag);
-                modifications.put(update.metadata().id, update);
+                in = new TeeDataInputPlus(in, dob, CACHEABLE_MUTATION_SIZE_LIMIT);
+
+                int size = (int) in.readUnsignedVInt();
+                assert size > 0;
+
+                PartitionUpdate update = PartitionUpdate.serializer.deserialize(in, version, flag);
+                if (size == 1)
+                {
+                    m = new Mutation(update);
+                }
+                else
+                {
+                    ImmutableMap.Builder<TableId, PartitionUpdate> modifications = new ImmutableMap.Builder<>();
+                    DecoratedKey dk = update.partitionKey();
+
+                    modifications.put(update.metadata().id, update);
+                    for (int i = 1; i < size; ++i)
+                    {
+                        update = PartitionUpdate.serializer.deserialize(in, version, flag);
+                        modifications.put(update.metadata().id, update);
+                    }
+                    m = new Mutation(update.metadata().keyspace, dk, modifications.build(), approxTime.now());
+                }
+
+                if (!((TeeDataInputPlus)in).isLimitReached())","[{'comment': ""So higher level meta question: right now there's really no sign that the data in the `DataOutputBuffer` is incomplete except for checking the `isLimitReached` method inside `TeeDataInputPlus`. This leaves us with the case where a buffer could be floating around and accessed in an incomplete state (between write calls above and checking) - how could we better tie this together?\r\n\r\nFull disclosure - I can't think of a good answer. I don't think decorating `DataOutputBuffer` with a bool indicating if it's valid or not makes any sense for 99+% of the other use-cases. We could consider wrapping access to the nested `teeBuffer` inside `TeeDataInputPlus` to return null or some other sentinel on limit reached but at that point it's just a worse version of this current API.\r\n\r\nAnyway - this might be something worth calling out on the JavaDoc class level comment for TeeDataInputPlus and be clearly prescriptive about the intended use of the class."", 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/io/util/TeeDataInputPlus.java,"@@ -0,0 +1,216 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.util;
+
+import java.io.EOFException;
+import java.io.IOException;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.utils.Throwables;
+
+/**
+ * DataInput that also stores the raw inputs into an output buffer
+ * This is useful for storing serialized buffers as they are deserialized
+ */
+public class TeeDataInputPlus implements DataInputPlus
+{
+    private final DataInputPlus source;
+    private final DataOutputPlus teeBuffer;
+
+    private final long limit;
+    private boolean limitReached = false;
+
+    public TeeDataInputPlus(DataInputPlus source, DataOutputPlus teeBuffer)
+    {
+        this(source, teeBuffer, -1);
+    }
+
+    public TeeDataInputPlus(DataInputPlus source, DataOutputPlus teeBuffer, long limit)
+    {
+        this.source = source;
+        this.teeBuffer = teeBuffer;
+        this.limit = limit;
+    }
+
+    private void maybeWrite(int length, Throwables.DiscreteAction<IOException> writeAction) throws IOException
+    {
+        if (!limitReached && (limit <= 0 || teeBuffer.position() + length < limit))
+            writeAction.perform();
+        else
+            limitReached = true;
+    }
+
+    @Override
+    public void readFully(byte[] bytes) throws IOException
+    {
+        source.readFully(bytes);
+        maybeWrite(bytes.length, () -> teeBuffer.write(bytes));
+    }
+
+    @Override
+    public void readFully(byte[] bytes, int offset, int length) throws IOException
+    {
+        source.readFully(bytes, offset, length);
+        maybeWrite(length, () -> teeBuffer.write(bytes, offset, length));
+    }
+
+    @Override
+    public int skipBytes(int n) throws IOException
+    {
+        for (int i = 0; i < n; i++)
+        {
+            try
+            {
+                byte v = source.readByte();
+                maybeWrite(TypeSizes.BYTE_SIZE, () -> teeBuffer.writeByte(v));
+            }
+            catch (EOFException eof)
+            {
+                return i;
+            }
+        }
+        return n;
+    }
+
+    @Override
+    public boolean readBoolean() throws IOException
+    {
+        boolean v = source.readBoolean();
+        maybeWrite(TypeSizes.BOOL_SIZE, () -> teeBuffer.writeBoolean(v));
+        return v;
+    }
+
+    @Override
+    public byte readByte() throws IOException
+    {
+        byte v = source.readByte();
+        maybeWrite(TypeSizes.BYTE_SIZE, () -> teeBuffer.writeByte(v));
+        return v;
+    }
+
+    @Override
+    public int readUnsignedByte() throws IOException
+    {
+        int v = source.readUnsignedByte();
+        maybeWrite(TypeSizes.BYTE_SIZE, () -> teeBuffer.writeByte(v));
+        return v;
+    }
+
+    @Override
+    public short readShort() throws IOException
+    {
+        short v = source.readShort();
+        maybeWrite(TypeSizes.SHORT_SIZE, () -> teeBuffer.writeShort(v));
+        return v;
+    }
+
+    @Override
+    public int readUnsignedShort() throws IOException
+    {
+        int v = source.readUnsignedShort();
+        maybeWrite(TypeSizes.SHORT_SIZE, () -> teeBuffer.writeShort(v));
+        return v;
+    }
+
+    @Override
+    public char readChar() throws IOException
+    {
+        char v = source.readChar();
+        maybeWrite(TypeSizes.BYTE_SIZE, () -> teeBuffer.writeChar(v));
+        return v;
+    }
+
+    @Override
+    public int readInt() throws IOException
+    {
+        int v = source.readInt();
+        maybeWrite(TypeSizes.INT_SIZE, () -> teeBuffer.writeInt(v));
+        return v;
+    }
+
+    @Override
+    public long readLong() throws IOException
+    {
+        long v = source.readLong();
+        maybeWrite(TypeSizes.LONG_SIZE, () -> teeBuffer.writeLong(v));
+        return v;
+    }
+
+    @Override
+    public float readFloat() throws IOException
+    {
+        float v = source.readFloat();
+        maybeWrite(TypeSizes.FLOAT_SIZE, () -> teeBuffer.writeFloat(v));
+        return v;
+    }
+
+    @Override
+    public double readDouble() throws IOException
+    {
+        double v = source.readDouble();
+        maybeWrite(TypeSizes.DOUBLE_SIZE, () -> teeBuffer.writeDouble(v));
+        return v;
+    }
+
+    @Override
+    public String readLine() throws IOException
+    {
+        //This one isn't safe since we know the actual line termination type
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public String readUTF() throws IOException
+    {
+        String v = source.readUTF();
+        maybeWrite(TypeSizes.sizeof(v), () -> teeBuffer.writeUTF(v));
+        return v;
+    }
+
+    @Override
+    public long readVInt() throws IOException
+    {
+        long v = source.readVInt();
+        maybeWrite(TypeSizes.sizeofVInt(v), () -> teeBuffer.writeVInt(v));
+        return v;
+    }
+
+    @Override
+    public long readUnsignedVInt() throws IOException
+    {
+        long v = source.readUnsignedVInt();
+        maybeWrite(TypeSizes.sizeofUnsignedVInt(v), () -> teeBuffer.writeUnsignedVInt(v));
+        return v;
+    }
+
+    @Override
+    public void skipBytesFully(int n) throws IOException
+    {
+        source.skipBytesFully(n);
+        maybeWrite(n, () -> {
+            for (int i = 0; i < n; i++)
+                teeBuffer.writeByte(0);
+        });
+    }
+
+    public boolean isLimitReached()","[{'comment': 'JavaDoc this to indicate that a ""true"" return indicates the data in your `teeBuffer` can\'t be trusted.', 'commenter': 'jmckenzie-dev'}]"
1954,test/unit/org/apache/cassandra/utils/TeeDataInputPlusTest.java,"@@ -0,0 +1,141 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.util.Arrays;
+
+import org.junit.Test;
+
+import org.apache.cassandra.io.util.DataInputBuffer;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.io.util.TeeDataInputPlus;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+public class TeeDataInputPlusTest
+{
+    @Test
+    public void testTeeBuffer() throws Exception
+    {
+        DataOutputBuffer out = new DataOutputBuffer();
+        byte[] testData;
+
+        // boolean
+        out.writeBoolean(true);
+        // byte
+        out.writeByte(0x1);
+        // char
+        out.writeChar('a');
+        // short
+        out.writeShort(1);
+        // int
+        out.writeInt(1);
+        // long
+        out.writeLong(1L);
+        // float
+        out.writeFloat(1.0f);
+        // double
+        out.writeDouble(1.0d);
+        // vint
+        out.writeVInt(-1337L);
+        //unsigned vint
+        out.writeUnsignedVInt(1337L);
+        // String
+        out.writeUTF(""abc"");
+        //Another string to test skip
+        out.writeUTF(""garbagetoskipattheend"");
+        testData = out.toByteArray();
+
+        int LIMITED_SIZE = 40;
+        DataInputBuffer reader = new DataInputBuffer(testData);
+        DataInputBuffer reader2 = new DataInputBuffer(testData);
+        DataOutputBuffer teeOut = new DataOutputBuffer();
+        DataOutputBuffer limitedTeeOut = new DataOutputBuffer();
+        TeeDataInputPlus tee = new TeeDataInputPlus(reader, teeOut);
+        TeeDataInputPlus limitedTee = new TeeDataInputPlus(reader2, limitedTeeOut, LIMITED_SIZE);
+
+        // boolean = 1byte
+        boolean bool = tee.readBoolean();
+        assertTrue(bool);
+        bool = limitedTee.readBoolean();
+        assertTrue(bool);
+        // byte = 1byte
+        byte b = tee.readByte();
+        assertEquals(b, 0x1);
+        b = limitedTee.readByte();
+        assertEquals(b, 0x1);
+        // char = 2byte
+        char c = tee.readChar();
+        assertEquals('a', c);
+        c = limitedTee.readChar();
+        assertEquals('a', c);
+        // short = 2bytes
+        short s = tee.readShort();
+        assertEquals(1, s);
+        s = limitedTee.readShort();
+        assertEquals(1, s);
+        // int = 4bytes
+        int i = tee.readInt();
+        assertEquals(1, i);
+        i = limitedTee.readInt();
+        assertEquals(1, i);
+        // long = 8bytes
+        long l = tee.readLong();
+        assertEquals(1L, l);
+        l = limitedTee.readLong();
+        assertEquals(1L, l);
+        // float = 4bytes
+        float f = tee.readFloat();
+        assertEquals(1.0f, f, 0);
+        f = limitedTee.readFloat();
+        assertEquals(1.0f, f, 0);
+        // double = 8bytes
+        double d = tee.readDouble();
+        assertEquals(1.0d, d, 0);
+        d = limitedTee.readDouble();
+        assertEquals(1.0d, d, 0);
+        long vint = tee.readVInt();
+        assertEquals(-1337L, vint);
+        vint = limitedTee.readVInt();
+        assertEquals(-1337L, vint);
+        long uvint = tee.readUnsignedVInt();
+        assertEquals(1337L, uvint);
+        uvint = limitedTee.readUnsignedVInt();
+        assertEquals(1337L, uvint);
+        // String(""abc"") = 2(string size) + 3 = 5 bytes
+        String str = tee.readUTF();
+        assertEquals(""abc"", str);
+        str = limitedTee.readUTF();
+        assertEquals(""abc"", str);
+        int skipped = tee.skipBytes(100);
+        assertEquals(23, skipped);
+        skipped = limitedTee.skipBytes(100);
+        assertEquals(23, skipped);
+
+        byte[] teeData = teeOut.toByteArray();
+        assertFalse(tee.isLimitReached());
+        assertTrue(Arrays.equals(testData, teeData));
+
+        byte[] limitedTeeData = limitedTeeOut.toByteArray();
+        assertTrue(limitedTee.isLimitReached());
+        assertTrue(Arrays.equals(Arrays.copyOf(testData, LIMITED_SIZE -1 ), limitedTeeData));","[{'comment': ""... 2 integers together without a delimiter? I'm not familiar with this - what's this about?"", 'commenter': 'jmckenzie-dev'}, {'comment': ""I'm not sure I follow, do you mean the `LIMITED_SIZE-1`?  Since the limit is `LIMITED_SIZE` the buffer stopped copying at `LIMITED_SIZE-1` bytes"", 'commenter': 'tjake'}, {'comment': 'Oh. 🤦 This is a formatting issue - missing a whitespace between the - and the 1. I was literally reading\r\n`[int] [-int]` instead of `[int] - [int]`.\r\n\r\nSo this should be\r\n```\r\nassertTrue(Arrays.equals(Arrays.copyOf(testData, LIMITED_SIZE - 1 ), limitedTeeData));\r\n```', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/io/util/DataOutputBuffer.java,"@@ -61,32 +61,62 @@ private enum AllocationType { DIRECT, ONHEAP }
         @Override
         protected DataOutputBuffer initialValue()
         {
-            return new DataOutputBuffer()
-            {
-                @Override
-                public void close()
-                {
-                    if (buffer != null && buffer.capacity() <= MAX_RECYCLE_BUFFER_SIZE)
-                    {
-                        buffer.clear();
-                    }
-                    else
-                    {
-                        setBuffer(allocate(DEFAULT_INITIAL_BUFFER_SIZE));
-                    }
-                }
-
-                @Override
-                protected ByteBuffer allocate(int size)
-                {
-                    return ALLOCATION_TYPE == AllocationType.DIRECT ?
-                           ByteBuffer.allocateDirect(size) :
-                           ByteBuffer.allocate(size);
-                }
-            };
+            return new LimitingScratchBuffer();
         }
     };
 
+    /**
+     * Returns a {@link DataOutputBuffer} that is limited to a capacity of {@code limit} bytes.
+     * A {@code limit} of 0 means unlimited.
+     */
+    public static DataOutputBuffer limitedScratchBuffer(long limit)
+    {
+        LimitingScratchBuffer dob = (LimitingScratchBuffer) scratchBuffer.get();
+        dob.limit = limit;
+        return dob;
+    }
+
+    public static class BufferCapacityExceededException extends RuntimeException
+    {}
+
+    private static final class LimitingScratchBuffer extends DataOutputBuffer","[{'comment': 'Similar to how size limiting is a first class citizen in `TeeDataInputPlus`, I _think_ we could do the same thing for DataOutputPlus by promoting the `initialSizeBuffer` and `limit` to top level variables and just not use them in the case where the ""no limit"" constructor is used. That\'d give us consistency between the structures of the `DataOutputPlus` and `TeeDataInputPlus` and their treatment of limits and remove one layer of type indirection that exists here, as well as the casting on usage.', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/net/MessagingService.java,"@@ -212,6 +215,23 @@ public class MessagingService extends MessagingServiceMBeanImpl
     public static final int current_version = VERSION_40;
     static AcceptVersions accept_messaging = new AcceptVersions(minimum_version, current_version);
     static AcceptVersions accept_streaming = new AcceptVersions(current_version, current_version);
+    static Map<Integer, Integer> versionIndexMap = Arrays.stream(Version.values()).collect(Collectors.toMap(v -> v.value, v -> v.ordinal()));
+
+    /**
+     * This is an optimisation to speed up the translation of the serialization
+     * version to the {@link Version} enum.
+     *
+     * @param version the serialization version
+     * @return a {@link Version}
+     */
+    public static int getVersionIndex(int version)","[{'comment': ""Nit: Consider renaming to something very literal like `getVersionOrdinal` -> the current name (index) doesn't really clarify _what_ index it's retrieving for what context / what purpose / etc.\r\n\r\nI've spent some time chewing on this; I really don't like it from a meta / aesthetic perspective but I think we're backed into a corner w/the original enum implementation and it's rather strange values that we're using for `Version` right now."", 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -430,10 +521,71 @@ public Mutation deserialize(DataInputPlus in, int version) throws IOException
 
         public long serializedSize(Mutation mutation, int version)
         {
-            int size = TypeSizes.sizeofUnsignedVInt(mutation.modifications.size());
-            for (Map.Entry<TableId, PartitionUpdate> entry : mutation.modifications.entrySet())
-                size += PartitionUpdate.serializer.serializedSize(entry.getValue(), version);
+            return serialization(mutation, version).serializedSize(PartitionUpdate.serializer, mutation, version);
+        }
+    }
 
+    /**
+     * There are two implementations of this class. One that keeps the serialized representation on-heap for later
+     * reuse and one that doesn't. Keeping all sized mutations around may lead to ""bad"" GC pressure (G1 GC) due to humongous objects.","[{'comment': ""Think we should lift this up to the CACHEABLE_MUTATION_SIZE_LIMIT declaration as a javadoc so it's clear at declaration time why this limit exists."", 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/io/util/DataOutputBuffer.java,"@@ -61,32 +61,62 @@ private enum AllocationType { DIRECT, ONHEAP }
         @Override
         protected DataOutputBuffer initialValue()
         {
-            return new DataOutputBuffer()
-            {
-                @Override
-                public void close()
-                {
-                    if (buffer != null && buffer.capacity() <= MAX_RECYCLE_BUFFER_SIZE)
-                    {
-                        buffer.clear();
-                    }
-                    else
-                    {
-                        setBuffer(allocate(DEFAULT_INITIAL_BUFFER_SIZE));
-                    }
-                }
-
-                @Override
-                protected ByteBuffer allocate(int size)
-                {
-                    return ALLOCATION_TYPE == AllocationType.DIRECT ?
-                           ByteBuffer.allocateDirect(size) :
-                           ByteBuffer.allocate(size);
-                }
-            };
+            return new LimitingScratchBuffer();
         }
     };
 
+    /**
+     * Returns a {@link DataOutputBuffer} that is limited to a capacity of {@code limit} bytes.
+     * A {@code limit} of 0 means unlimited.
+     */
+    public static DataOutputBuffer limitedScratchBuffer(long limit)
+    {
+        LimitingScratchBuffer dob = (LimitingScratchBuffer) scratchBuffer.get();","[{'comment': ""Been chewing on this - given the advice is to close out this DOB quickly to prevent heap pressure, what's the point of using the FastThreadLocal scratchBuffer instead of just a new DOB?"", 'commenter': 'jmckenzie-dev'}, {'comment': 'A lot clearer now w/the new init + overrides on scratchBuffer init. 👍 ', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +411,115 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation, int)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedReplicas(Mutation, ReplicaPlan.ForWrite, AbstractWriteResponseHandler, String, Stage)}
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or compute and cache said serialization if it doesn't
+         * exist yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedReplicas(Mutation, ReplicaPlan.ForWrite, AbstractWriteResponseHandler, String, Stage)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionOrdinal = MessagingService.getVersionOrdinal(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.cachedSerializations[versionOrdinal];
+            if (serialization == null)
+            {
+                serialization = new SizeOnlyCacheableSerialization();
+                long sertializedSize = serialization.serializedSize(PartitionUpdate.serializer, mutation, version);
+
+                // Exessively large mutation objects cause GC pressure and huge allocations when serialized.","[{'comment': 'nit: Excessively', 'commenter': 'jmckenzie-dev'}]"
1954,src/java/org/apache/cassandra/db/Mutation.java,"@@ -393,34 +411,115 @@ public static class MutationSerializer implements IVersionedSerializer<Mutation>
     {
         public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException
         {
+            serialization(mutation, version).serialize(PartitionUpdate.serializer, mutation, out, version);
+        }
+
+        /**
+         * Called early during request processing to prevent that {@link #serialization(Mutation, int)} is
+         * called concurrently.
+         * See {@link org.apache.cassandra.service.StorageProxy#sendToHintedReplicas(Mutation, ReplicaPlan.ForWrite, AbstractWriteResponseHandler, String, Stage)}
+         */
+        @SuppressWarnings(""JavadocReference"")
+        public void prepareSerializedBuffer(Mutation mutation, int version)
+        {
+            serialization(mutation, version);
+        }
+
+        /**
+         * Retrieve the cached serialization of this mutation, or compute and cache said serialization if it doesn't
+         * exist yet. Note that this method is _not_ synchronized even though it may (and will often) be called
+         * concurrently. Concurrent calls are still safe however, the only risk is that the value is not cached yet,
+         * multiple concurrent calls may compute it multiple times instead of just once. This is ok as in practice
+         * as we make sure this doesn't happen in the hot path by forcing the initial caching in
+         * {@link org.apache.cassandra.service.StorageProxy#sendToHintedReplicas(Mutation, ReplicaPlan.ForWrite, AbstractWriteResponseHandler, String, Stage)}
+         * via {@link #prepareSerializedBuffer(Mutation)}, which is the only caller that passes
+         * {@code isPrepare==true}.
+         */
+        @SuppressWarnings(""JavadocReference"")
+        private Serialization serialization(Mutation mutation, int version)
+        {
+            int versionOrdinal = MessagingService.getVersionOrdinal(version);
+            // Retrieves the cached version, or build+cache it if it's not cached already.
+            Serialization serialization = mutation.cachedSerializations[versionOrdinal];
+            if (serialization == null)
+            {
+                serialization = new SizeOnlyCacheableSerialization();
+                long sertializedSize = serialization.serializedSize(PartitionUpdate.serializer, mutation, version);","[{'comment': 'nit: spelling. Should be ""serializedSize""', 'commenter': 'jmckenzie-dev'}]"
1962,src/java/org/apache/cassandra/cql3/statements/TransactionStatement.java,"@@ -0,0 +1,472 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql3.statements;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedSet;
+import java.util.TreeSet;
+import java.util.function.Consumer;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Iterables;
+
+import accord.api.Key;
+import accord.primitives.Keys;
+import accord.txn.Txn;
+import org.apache.cassandra.audit.AuditLogContext;
+import org.apache.cassandra.audit.AuditLogEntryType;
+import org.apache.cassandra.cql3.CQLStatement;
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.cql3.ColumnReference;
+import org.apache.cassandra.cql3.ColumnSpecification;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.cql3.ResultSet;
+import org.apache.cassandra.cql3.VariableSpecifications;
+import org.apache.cassandra.cql3.selection.ResultSetBuilder;
+import org.apache.cassandra.cql3.selection.Selection;
+import org.apache.cassandra.cql3.transactions.ConditionStatement;
+import org.apache.cassandra.cql3.transactions.ReferenceOperation;
+import org.apache.cassandra.cql3.transactions.SelectReferenceSource;
+import org.apache.cassandra.db.ReadQuery;
+import org.apache.cassandra.db.SinglePartitionReadCommand;
+import org.apache.cassandra.db.SinglePartitionReadQuery;
+import org.apache.cassandra.db.filter.DataLimits;
+import org.apache.cassandra.db.marshal.MapType;
+import org.apache.cassandra.db.marshal.SetType;
+import org.apache.cassandra.db.partitions.FilteredPartition;
+import org.apache.cassandra.db.rows.Cell;
+import org.apache.cassandra.db.rows.ColumnData;
+import org.apache.cassandra.schema.ColumnMetadata;
+import org.apache.cassandra.service.ClientState;
+import org.apache.cassandra.service.QueryState;
+import org.apache.cassandra.service.accord.AccordService;
+import org.apache.cassandra.service.accord.txn.TxnCondition;
+import org.apache.cassandra.service.accord.txn.TxnData;
+import org.apache.cassandra.service.accord.txn.TxnNamedRead;
+import org.apache.cassandra.service.accord.txn.TxnQuery;
+import org.apache.cassandra.service.accord.txn.TxnRead;
+import org.apache.cassandra.service.accord.txn.TxnReference;
+import org.apache.cassandra.service.accord.txn.TxnUpdate;
+import org.apache.cassandra.service.accord.txn.TxnWrite;
+import org.apache.cassandra.transport.messages.ResultMessage;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.LazyToString;
+
+import static org.apache.cassandra.cql3.statements.RequestValidations.checkFalse;
+import static org.apache.cassandra.cql3.statements.RequestValidations.checkNotNull;
+import static org.apache.cassandra.cql3.statements.RequestValidations.checkTrue;
+import static org.apache.cassandra.utils.Clock.Global.nanoTime;
+
+public class TransactionStatement implements CQLStatement
+{
+    public static final String DUPLICATE_TUPLE_NAME_MESSAGE = ""The name '%s' has already been used by a LET assignment."";
+    public static final String INCOMPLETE_PRIMARY_KEY_LET_MESSAGE = ""SELECT in LET assignment without LIMIT 1 must specify all primary key elements; CQL %s"";
+    public static final String INCOMPLETE_PRIMARY_KEY_SELECT_MESSAGE = ""Normal SELECT without LIMIT 1 must specify all primary key elements; CQL %s"";
+    public static final String NO_CONDITIONS_IN_UPDATES_MESSAGE = ""Updates within transactions may not specify their own conditions."";
+    public static final String NO_TIMESTAMPS_IN_UPDATES_MESSAGE = ""Updates within transactions may not specify custom timestamps."";
+    public static final String EMPTY_TRANSACTION_MESSAGE = ""Transaction contains no reads or writes"";
+    public static final String SELECT_REFS_NEED_COLUMN_MESSAGE = ""SELECT references must specify a column."";
+
+    static class NamedSelect
+    {
+        final TxnDataName name;
+        final SelectStatement select;
+
+        public NamedSelect(TxnDataName name, SelectStatement select)
+        {
+            this.name = name;
+            this.select = select;
+        }
+    }
+
+    private final List<NamedSelect> assignments;
+    private final NamedSelect returningSelect;
+    private final List<ColumnReference> returningReferences;
+    private final List<ModificationStatement> updates;
+    private final List<ConditionStatement> conditions;
+
+    private final VariableSpecifications bindVariables;
+
+    public TransactionStatement(List<NamedSelect> assignments,
+                                NamedSelect returningSelect,
+                                List<ColumnReference> returningReferences,
+                                List<ModificationStatement> updates,
+                                List<ConditionStatement> conditions,
+                                VariableSpecifications bindVariables)
+    {
+        this.assignments = assignments;
+        this.returningSelect = returningSelect;
+        this.returningReferences = returningReferences;
+        this.updates = updates;
+        this.conditions = conditions;
+        this.bindVariables = bindVariables;
+    }
+
+    @Override
+    public List<ColumnSpecification> getBindVariables()
+    {
+        return bindVariables.getBindVariables();
+    }
+
+    @Override
+    public void authorize(ClientState state)
+    {
+        // Assess read permissions for all data from both explicit LET statements and generated reads.
+        for (NamedSelect let : assignments)
+            let.select.authorize(state);
+
+        if (returningSelect != null)
+            returningSelect.select.authorize(state);
+
+        for (ModificationStatement update : updates)
+            update.authorize(state);
+    }
+
+    @Override
+    public void validate(ClientState state)
+    {
+        for (ModificationStatement statement : updates)
+            statement.validate(state);
+    }
+
+    @VisibleForTesting
+    public List<ColumnReference> getReturningReferences()
+    {
+        return returningReferences;
+    }
+
+    TxnNamedRead createNamedRead(NamedSelect namedSelect, QueryOptions options)
+    {
+        SelectStatement select = namedSelect.select;
+        ReadQuery readQuery = select.getQuery(options, 0);
+
+        // We reject reads from both LET and SELECT that do not specify a single row.
+        @SuppressWarnings(""unchecked"") 
+        SinglePartitionReadQuery.Group<SinglePartitionReadCommand> selectQuery = (SinglePartitionReadQuery.Group<SinglePartitionReadCommand>) readQuery;
+
+        return new TxnNamedRead(namedSelect.name, Iterables.getOnlyElement(selectQuery.queries));
+    }
+
+    TxnRead createRead(QueryOptions options, Consumer<Key> keyConsumer)
+    {
+        List<TxnNamedRead> reads = new ArrayList<>(assignments.size() + 1);
+
+        for (NamedSelect select : assignments)
+        {
+            TxnNamedRead read = createNamedRead(select, options);
+            keyConsumer.accept(read.key());
+            reads.add(read);
+        }
+
+        if (returningSelect != null)
+        {
+            TxnNamedRead read = createNamedRead(returningSelect, options);
+            keyConsumer.accept(read.key());
+            reads.add(read);
+        }
+
+        for (NamedSelect select : autoReads.values())
+            // don't need keyConsumer as the keys are known to exist due to Modification
+            reads.add(createNamedRead(select, options));
+        
+        return new TxnRead(reads);
+    }
+
+    TxnCondition createCondition(QueryOptions options)
+    {
+        if (conditions.isEmpty())
+            return TxnCondition.none();
+        if (conditions.size() == 1)
+            return conditions.get(0).createCondition(options);
+
+        List<TxnCondition> result = new ArrayList<>(conditions.size());
+        for (ConditionStatement condition : conditions)
+            result.add(condition.createCondition(options));
+
+        // TODO: OR support
+        return new TxnCondition.BooleanGroup(TxnCondition.Kind.AND, result);
+    }
+
+    private final Map<TxnDataName, NamedSelect> autoReads = new HashMap<>();
+
+    List<TxnWrite.Fragment> createWriteFragments(ClientState state, QueryOptions options, Consumer<Key> keyConsumer)
+    {
+        List<TxnWrite.Fragment> fragments = new ArrayList<>(updates.size());
+        int idx = 0;
+        for (ModificationStatement modification : updates)
+        {
+            TxnWrite.Fragment fragment = modification.getTxnWriteFragment(idx++, state, options);
+            keyConsumer.accept(fragment.key);
+            fragments.add(fragment);
+
+            if (modification.allReferenceOperations().stream().anyMatch(ReferenceOperation::requiresRead))
+            {
+                TxnDataName partitionName = TxnDataName.partitionRead(modification.metadata(), fragment.key.partitionKey());
+                if (!autoReads.containsKey(partitionName))
+                    autoReads.put(partitionName, new NamedSelect(partitionName, ((UpdateStatement) modification).createSelectForTxn()));
+            }
+        }
+        return fragments;
+    }
+
+    TxnUpdate createUpdate(ClientState state, QueryOptions options, Consumer<Key> keyConsumer)
+    {
+        return new TxnUpdate(createWriteFragments(state, options, keyConsumer), createCondition(options));
+    }
+
+    Keys toKeys(SortedSet<Key> keySet)
+    {
+        return new Keys(keySet);
+    }
+
+    @VisibleForTesting
+    public Txn createTxn(ClientState state, QueryOptions options)
+    {
+        SortedSet<Key> keySet = new TreeSet<>();
+        if (updates.isEmpty())
+        {
+            Preconditions.checkState(conditions.isEmpty());
+            TxnRead read = createRead(options, keySet::add);
+            return new Txn.InMemory(toKeys(keySet), read, TxnQuery.ALL);
+        }
+        else
+        {
+            TxnUpdate update = createUpdate(state, options, keySet::add);
+            TxnRead read = createRead(options, keySet::add);","[{'comment': 'Could `createRead` be pulled out of the two branches since it is done the same in both?', 'commenter': 'aweisberg'}]"
1962,src/java/org/apache/cassandra/service/accord/txn/TxnUpdate.java,"@@ -0,0 +1,157 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Objects;
+
+import accord.api.Data;
+import accord.api.Update;
+import accord.api.Write;
+import accord.primitives.Keys;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.service.accord.SerializationUtils;
+import org.apache.cassandra.transport.ProtocolVersion;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.ObjectSizes;
+
+import static org.apache.cassandra.service.accord.SerializationUtils.deserialize;
+import static org.apache.cassandra.service.accord.SerializationUtils.serialize;
+import static org.apache.cassandra.service.accord.SerializationUtils.serializeArray;
+import static org.apache.cassandra.utils.ByteBufferUtil.readWithVIntLength;
+import static org.apache.cassandra.utils.ByteBufferUtil.serializedSizeWithVIntLength;
+import static org.apache.cassandra.utils.ByteBufferUtil.writeWithVIntLength;
+
+public class TxnUpdate implements Update
+{
+    private static final long EMPTY_SIZE = ObjectSizes.measure(new TxnUpdate(new ByteBuffer[0], null));
+
+    private final ByteBuffer[] serializedUpdates;
+    private final ByteBuffer serializedCondition;
+
+    public TxnUpdate(List<TxnWrite.Fragment> updates, TxnCondition condition)
+    {
+        this.serializedUpdates = serialize(updates, TxnWrite.Fragment.serializer);
+        this.serializedCondition = serialize(condition, TxnCondition.serializer);
+    }
+
+    public TxnUpdate(ByteBuffer[] serializedUpdates, ByteBuffer serializedCondition)
+    {
+        this.serializedUpdates = serializedUpdates;
+        this.serializedCondition = serializedCondition;
+    }
+
+    public ByteBuffer serializedCondition()
+    {
+        return serializedCondition.duplicate();
+    }
+
+    public long estimatedSizeOnHeap()
+    {
+        long size = EMPTY_SIZE + ByteBufferUtil.estimatedSizeOnHeap(serializedCondition);
+        for (ByteBuffer update : serializedUpdates)
+            size += ByteBufferUtil.estimatedSizeOnHeap(update);
+        return size;
+    }
+
+    @Override
+    public String toString()
+    {
+        return ""TxnUpdate{"" +
+               ""updates="" + deserialize(serializedUpdates, TxnWrite.Fragment.serializer) +
+               "", condition="" + deserialize(serializedCondition, TxnCondition.serializer) +
+               '}';
+    }
+
+    @Override
+    public boolean equals(Object o)
+    {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+        TxnUpdate txnUpdate = (TxnUpdate) o;
+        return Arrays.equals(serializedUpdates, txnUpdate.serializedUpdates) && Objects.equals(serializedCondition, txnUpdate.serializedCondition);
+    }
+
+    @Override
+    public int hashCode()
+    {
+        int result = Objects.hash(serializedCondition);
+        result = 31 * result + Arrays.hashCode(serializedUpdates);
+        return result;
+    }
+
+    @Override
+    public Keys keys()
+    {
+        // TODO: Does this actually work?","[{'comment': ""It doesn't I think. I would expect no writes to be applied. This eventually filters to [here](https://github.com/apache/cassandra-accord/blob/197e4c740e4e755fe477ecc0b6f2284df253d37a/accord-core/src/main/java/accord/primitives/Writes.java#L79) and left folds over no keys not applying the writes. "", 'commenter': 'aweisberg'}]"
1962,src/antlr/Cql.g,"@@ -43,6 +43,7 @@ import Parser,Lexer;
     import org.apache.cassandra.cql3.selection.*;
     import org.apache.cassandra.cql3.statements.*;
     import org.apache.cassandra.cql3.statements.schema.*;
+    import org.apache.cassandra.cql3.transactions.*;","[{'comment': 'unused import?', 'commenter': 'dcapwell'}]"
1962,src/antlr/Parser.g,"@@ -73,6 +80,19 @@ options {
         return marker;
     }
 
+    public RowDataReference.Raw newRowDataReference(Selectable.RawIdentifier tuple, Selectable.Raw selectable)
+    {
+        if (!isParsingTxn)
+            throw new IllegalStateException();","[{'comment': 'can we get a useful error msg?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/audit/AuditLogEntryType.java,"@@ -60,6 +60,9 @@
     CREATE_ROLE(AuditLogEntryCategory.DCL),
     USE_KEYSPACE(AuditLogEntryCategory.OTHER),
     DESCRIBE(AuditLogEntryCategory.OTHER),
+    
+    // TODO: Is DML the most appropriate classification, given a transaction can read, write, or both?
+    TRANSACTION(AuditLogEntryCategory.DML),","[{'comment': 'Maybe best to add a new one?  Maybe `TRANSACTION` like we did for `WriteType`?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/config/Config.java,"@@ -1171,8 +1173,9 @@ public static void log(Config config)
             String value;
             try
             {
-                // Field.get() can throw NPE if the value of the field is null
-                value = field.get(config).toString();
+                // don't use exceptions for normal control flow!
+                Object obj = field.get(config);
+                value = obj != null ? obj.toString() : ""null"";","[{'comment': 'comment said that `field.get` throws NPE; is that true?  it should return null... should we also remove the `NPE` catch?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1855,6 +1855,11 @@ public static long getCasContentionTimeout(TimeUnit unit)
         return conf.cas_contention_timeout.to(unit);
     }
 
+    public static long getTransactionTimeout(TimeUnit unit)","[{'comment': 'no setter?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/utils/CollectionSerializers.java,"@@ -19,41 +19,43 @@
 package org.apache.cassandra.utils;
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.RandomAccess;
 import java.util.Set;
 import java.util.function.IntFunction;
 
 import com.google.common.collect.Maps;
 import com.google.common.collect.Sets;
 
-import org.apache.cassandra.db.TypeSizes;
 import org.apache.cassandra.io.IVersionedSerializer;
 import org.apache.cassandra.io.util.DataInputPlus;
 import org.apache.cassandra.io.util.DataOutputPlus;
 
-public class CollectionSerializer
+import static com.google.common.primitives.Ints.checkedCast;
+import static org.apache.cassandra.db.TypeSizes.sizeofUnsignedVInt;
+
+public class CollectionSerializers
 {
 
-    public static <V> void serializeCollection(IVersionedSerializer<V> valueSerializer, Collection<V> values, DataOutputPlus out, int version) throws IOException
+    public static <V> void serializeCollection(Collection<V> values, DataOutputPlus out, int version, IVersionedSerializer<V> valueSerializer) throws IOException
     {
         out.writeUnsignedVInt(values.size());
         for (V value : values)
             valueSerializer.serialize(value, out, version);
     }
 
-    public static <V, L extends List<V> & RandomAccess> void serializeList(IVersionedSerializer<V> valueSerializer, L values, DataOutputPlus out, int version) throws IOException","[{'comment': ""if you remove `& RandomAccess` shouldn't you just call `serializeCollection` then to avoid copy/paste?  `& RandomAccess` looks to be there to know its safe to do `.get` to avoid the memory overhead of iterators"", 'commenter': 'dcapwell'}]"
1962,test/unit/org/apache/cassandra/service/accord/AccordTestUtils.java,"@@ -154,20 +154,34 @@ public static void processCommandResult(AccordCommandStore commandStore, Command
                                         throw new RuntimeException(e);
                                     }
                                 })
-                                .reduce(null, AccordData::merge);
+                                .reduce(null, TxnData::merge);
             Write write = txn.update().apply(readData);
             ((AccordCommand)command).setWrites(new Writes(command.executeAt(), txn.keys(), write));
             ((AccordCommand)command).setResult(txn.query().compute(command.txnId(), readData, txn.read(), txn.update()));
         }).get();
     }
 
+    public static Txn createTxn(String query)","[{'comment': 'this is very useful for cases like Simulator which benefits from bypassing TransactionStatement to directly access `TxnData` and all ""hidden"" state', 'commenter': 'dcapwell'}]"
1962,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -60,6 +60,23 @@
     private static final Constraint DNS_DOMAIN_PART_CONSTRAINT = Constraint.between(0, DNS_DOMAIN_PART_DOMAIN.length - 1).withNoShrinkPoint();
 
     public static final Gen<String> IDENTIFIER_GEN = Generators.regexWord(SourceDSL.integers().between(1, 50));
+    public static final Gen<String> SYMBOL_GEN = filter(symbolGen(SourceDSL.integers().between(1, 48)), Generators::thisBugIsBroughtToYouByTheLetterP);
+    private static boolean thisBugIsBroughtToYouByTheLetterP(String value)
+    {
+        // In Lexer.g DURATION is before IDENT and Duration allows the following to parsse: P, and PT
+        // This causes an issue for cases that use IDENT as P and PT will not match as they matched DURATION already
+        // to avoid these cases, this function will be used to filter them out so only ""valid"" symbols are returned
+        // see CASSANDRA-17919
+        return !(""P"".equals(value) || ""PT"".equals(value));
+    }
+    private static final char CHAR_UNDERSCORE = 95;
+    public static Gen<String> symbolGen(Gen<Integer> size)
+    {
+        char[] domain = new char[LETTER_OR_DIGIT_DOMAIN.length + 1];
+        System.arraycopy(LETTER_OR_DIGIT_DOMAIN, 0, domain, 0, LETTER_OR_DIGIT_DOMAIN.length);
+        domain[domain.length - 1] = CHAR_UNDERSCORE;","[{'comment': ""nit: it would be good to save this outside this method as this is unchanged and doesn't require reallocating for each call"", 'commenter': 'dcapwell'}]"
1962,test/distributed/org/apache/cassandra/distributed/test/accord/AccordTestBase.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.accord;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import com.google.common.base.Throwables;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import net.bytebuddy.implementation.bind.annotation.This;
+import org.assertj.core.api.Assertions;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.slf4j.Logger;
+
+import accord.coordinate.Preempted;
+import org.apache.cassandra.cql3.statements.ModificationStatement;
+import org.apache.cassandra.cql3.statements.TransactionStatement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.service.accord.AccordService;
+import org.apache.cassandra.utils.FailingConsumer;
+
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static org.junit.Assert.assertArrayEquals;
+
+public abstract class AccordTestBase extends TestBaseImpl
+{
+    protected static final AtomicInteger COUNTER = new AtomicInteger(0);
+
+    protected static Cluster sharedCluster;","[{'comment': 'nit: all caps', 'commenter': 'dcapwell'}]"
1962,test/distributed/org/apache/cassandra/distributed/test/accord/AccordTestBase.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.accord;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import com.google.common.base.Throwables;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import net.bytebuddy.implementation.bind.annotation.This;
+import org.assertj.core.api.Assertions;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.slf4j.Logger;
+
+import accord.coordinate.Preempted;
+import org.apache.cassandra.cql3.statements.ModificationStatement;
+import org.apache.cassandra.cql3.statements.TransactionStatement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.service.accord.AccordService;
+import org.apache.cassandra.utils.FailingConsumer;
+
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static org.junit.Assert.assertArrayEquals;
+
+public abstract class AccordTestBase extends TestBaseImpl
+{
+    protected static final AtomicInteger COUNTER = new AtomicInteger(0);
+
+    protected static Cluster sharedCluster;
+    
+    protected String currentTable;
+
+    @BeforeClass
+    public static void setupClass() throws IOException
+    {
+        sharedCluster = createCluster();
+    }
+
+    @AfterClass
+    public static void teardown()
+    {
+        if (sharedCluster != null)
+            sharedCluster.close();
+    }
+    
+    @Before
+    public void setup()
+    {
+        currentTable = KEYSPACE + "".tbl"" + COUNTER.getAndIncrement();
+    }
+
+    protected static void assertRow(Cluster cluster, String query, int k, int c, int v)
+    {
+        Object[][] result = cluster.coordinator(1).execute(query, ConsistencyLevel.QUORUM);
+        assertArrayEquals(new Object[]{new Object[] {k, c, v}}, result);
+    }
+
+    protected void test(String tableDDL, FailingConsumer<Cluster> fn) throws Exception
+    {
+        sharedCluster.schemaChange(tableDDL);
+        sharedCluster.forEach(node -> node.runOnInstance(() -> AccordService.instance().createEpochFromConfigUnsafe()));
+
+        // Evict commands from the cache immediately to expose problems loading from disk.
+        sharedCluster.forEach(node -> node.runOnInstance(() -> AccordService.instance().setCacheSize(0)));
+
+        fn.accept(sharedCluster);
+
+        // Reset any messaging filters.
+        sharedCluster.filters().reset();","[{'comment': 'should this by try/finally?', 'commenter': 'dcapwell'}]"
1962,test/distributed/org/apache/cassandra/distributed/test/accord/AccordTestBase.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.accord;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import com.google.common.base.Throwables;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import net.bytebuddy.implementation.bind.annotation.This;
+import org.assertj.core.api.Assertions;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.slf4j.Logger;
+
+import accord.coordinate.Preempted;
+import org.apache.cassandra.cql3.statements.ModificationStatement;
+import org.apache.cassandra.cql3.statements.TransactionStatement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.service.accord.AccordService;
+import org.apache.cassandra.utils.FailingConsumer;
+
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static org.junit.Assert.assertArrayEquals;
+
+public abstract class AccordTestBase extends TestBaseImpl
+{
+    protected static final AtomicInteger COUNTER = new AtomicInteger(0);
+
+    protected static Cluster sharedCluster;
+    
+    protected String currentTable;
+
+    @BeforeClass
+    public static void setupClass() throws IOException
+    {
+        sharedCluster = createCluster();
+    }
+
+    @AfterClass
+    public static void teardown()
+    {
+        if (sharedCluster != null)
+            sharedCluster.close();
+    }
+    
+    @Before
+    public void setup()
+    {
+        currentTable = KEYSPACE + "".tbl"" + COUNTER.getAndIncrement();
+    }
+
+    protected static void assertRow(Cluster cluster, String query, int k, int c, int v)
+    {
+        Object[][] result = cluster.coordinator(1).execute(query, ConsistencyLevel.QUORUM);
+        assertArrayEquals(new Object[]{new Object[] {k, c, v}}, result);
+    }
+
+    protected void test(String tableDDL, FailingConsumer<Cluster> fn) throws Exception
+    {
+        sharedCluster.schemaChange(tableDDL);
+        sharedCluster.forEach(node -> node.runOnInstance(() -> AccordService.instance().createEpochFromConfigUnsafe()));
+
+        // Evict commands from the cache immediately to expose problems loading from disk.
+        sharedCluster.forEach(node -> node.runOnInstance(() -> AccordService.instance().setCacheSize(0)));
+
+        fn.accept(sharedCluster);
+
+        // Reset any messaging filters.
+        sharedCluster.filters().reset();
+    }
+
+    protected void test(FailingConsumer<Cluster> fn) throws Exception
+    {
+        test(""CREATE TABLE "" + currentTable + "" (k int, c int, v int, primary key (k, c))"", fn);
+    }
+
+    private static Cluster createCluster() throws IOException
+    {
+        // need to up the timeout else tests get flaky
+        // disable vnode for now, but should enable before trunk
+        return init(Cluster.build(2)
+                           .withoutVNodes()
+                           .withConfig(c -> c.with(Feature.NETWORK).set(""write_request_timeout"", ""10s"").set(""transaction_timeout"", ""15s""))
+                           .withInstanceInitializer(ByteBuddyHelper::install)
+                           .start());
+    }
+
+    // TODO: Retry on preemption may become unnecessary after the Unified Log is integrated.
+    protected static SimpleQueryResult assertRowEqualsWithPreemptedRetry(Cluster cluster, Object[] row, String check, Object... boundValues)
+    {
+        SimpleQueryResult result = executeWithRetry(cluster, check, boundValues);
+        Assertions.assertThat(result.toObjectArrays()).isEqualTo(row == null ? new Object[0] : new Object[] { row });
+        return result;
+    }
+
+    protected static SimpleQueryResult executeWithRetry(Cluster cluster, String check, Object... boundValues)
+    {
+        try
+        {
+            return cluster.coordinator(1).executeWithResult(check, ConsistencyLevel.ANY, boundValues);
+        }
+        catch (Throwable t)
+        {
+            if (Throwables.getRootCause(t).toString().contains(Preempted.class.getName()))
+                return executeWithRetry(cluster, check, boundValues);
+
+            throw t;
+        }
+    }
+
+    public static class ByteBuddyHelper","[{'comment': 'should this be renamed to be more explicit?  Maybe something like `EnforceUpdateDoesNotPerformRead`?', 'commenter': 'dcapwell'}]"
1962,test/distributed/org/apache/cassandra/distributed/test/accord/AccordTestBase.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.accord;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import com.google.common.base.Throwables;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import net.bytebuddy.implementation.bind.annotation.This;
+import org.assertj.core.api.Assertions;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.slf4j.Logger;
+
+import accord.coordinate.Preempted;
+import org.apache.cassandra.cql3.statements.ModificationStatement;
+import org.apache.cassandra.cql3.statements.TransactionStatement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.service.accord.AccordService;
+import org.apache.cassandra.utils.FailingConsumer;
+
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static org.junit.Assert.assertArrayEquals;
+
+public abstract class AccordTestBase extends TestBaseImpl
+{
+    protected static final AtomicInteger COUNTER = new AtomicInteger(0);
+
+    protected static Cluster sharedCluster;
+    
+    protected String currentTable;
+
+    @BeforeClass
+    public static void setupClass() throws IOException
+    {
+        sharedCluster = createCluster();
+    }
+
+    @AfterClass
+    public static void teardown()
+    {
+        if (sharedCluster != null)
+            sharedCluster.close();
+    }
+    
+    @Before
+    public void setup()
+    {
+        currentTable = KEYSPACE + "".tbl"" + COUNTER.getAndIncrement();
+    }
+
+    protected static void assertRow(Cluster cluster, String query, int k, int c, int v)
+    {
+        Object[][] result = cluster.coordinator(1).execute(query, ConsistencyLevel.QUORUM);
+        assertArrayEquals(new Object[]{new Object[] {k, c, v}}, result);
+    }
+
+    protected void test(String tableDDL, FailingConsumer<Cluster> fn) throws Exception
+    {
+        sharedCluster.schemaChange(tableDDL);
+        sharedCluster.forEach(node -> node.runOnInstance(() -> AccordService.instance().createEpochFromConfigUnsafe()));
+
+        // Evict commands from the cache immediately to expose problems loading from disk.
+        sharedCluster.forEach(node -> node.runOnInstance(() -> AccordService.instance().setCacheSize(0)));
+
+        fn.accept(sharedCluster);
+
+        // Reset any messaging filters.
+        sharedCluster.filters().reset();
+    }
+
+    protected void test(FailingConsumer<Cluster> fn) throws Exception
+    {
+        test(""CREATE TABLE "" + currentTable + "" (k int, c int, v int, primary key (k, c))"", fn);
+    }
+
+    private static Cluster createCluster() throws IOException
+    {
+        // need to up the timeout else tests get flaky
+        // disable vnode for now, but should enable before trunk
+        return init(Cluster.build(2)
+                           .withoutVNodes()
+                           .withConfig(c -> c.with(Feature.NETWORK).set(""write_request_timeout"", ""10s"").set(""transaction_timeout"", ""15s""))
+                           .withInstanceInitializer(ByteBuddyHelper::install)
+                           .start());
+    }
+
+    // TODO: Retry on preemption may become unnecessary after the Unified Log is integrated.
+    protected static SimpleQueryResult assertRowEqualsWithPreemptedRetry(Cluster cluster, Object[] row, String check, Object... boundValues)
+    {
+        SimpleQueryResult result = executeWithRetry(cluster, check, boundValues);
+        Assertions.assertThat(result.toObjectArrays()).isEqualTo(row == null ? new Object[0] : new Object[] { row });
+        return result;
+    }
+
+    protected static SimpleQueryResult executeWithRetry(Cluster cluster, String check, Object... boundValues)
+    {
+        try
+        {
+            return cluster.coordinator(1).executeWithResult(check, ConsistencyLevel.ANY, boundValues);
+        }
+        catch (Throwable t)
+        {
+            if (Throwables.getRootCause(t).toString().contains(Preempted.class.getName()))","[{'comment': 'FYI there is a `AssertionUtils` class to make working with this easier...\r\n\r\n```\r\nif (AssertionUtils.rootCauseIs(Preempted.class).matches(t))\r\n```\r\nThis was written to work with Assert4j but can directly call `matches`\r\n\r\nThe main reason to prefer these methods is that they try to do the type checks and not just the name check, so `class FluffyKitten extends Preempted` will be detected as well.\r\n', 'commenter': 'dcapwell'}]"
1962,test/distributed/org/apache/cassandra/distributed/test/accord/AccordTestBase.java,"@@ -0,0 +1,166 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.accord;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import com.google.common.base.Throwables;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import net.bytebuddy.implementation.bind.annotation.This;
+import org.assertj.core.api.Assertions;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.slf4j.Logger;
+
+import accord.coordinate.Preempted;
+import org.apache.cassandra.cql3.statements.ModificationStatement;
+import org.apache.cassandra.cql3.statements.TransactionStatement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.service.accord.AccordService;
+import org.apache.cassandra.utils.FailingConsumer;
+
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static org.junit.Assert.assertArrayEquals;
+
+public abstract class AccordTestBase extends TestBaseImpl
+{
+    protected static final AtomicInteger COUNTER = new AtomicInteger(0);
+
+    protected static Cluster sharedCluster;
+    
+    protected String currentTable;
+
+    @BeforeClass
+    public static void setupClass() throws IOException
+    {
+        sharedCluster = createCluster();
+    }
+
+    @AfterClass
+    public static void teardown()
+    {
+        if (sharedCluster != null)
+            sharedCluster.close();
+    }
+    
+    @Before
+    public void setup()
+    {
+        currentTable = KEYSPACE + "".tbl"" + COUNTER.getAndIncrement();
+    }
+
+    protected static void assertRow(Cluster cluster, String query, int k, int c, int v)
+    {
+        Object[][] result = cluster.coordinator(1).execute(query, ConsistencyLevel.QUORUM);
+        assertArrayEquals(new Object[]{new Object[] {k, c, v}}, result);
+    }
+
+    protected void test(String tableDDL, FailingConsumer<Cluster> fn) throws Exception
+    {
+        sharedCluster.schemaChange(tableDDL);
+        sharedCluster.forEach(node -> node.runOnInstance(() -> AccordService.instance().createEpochFromConfigUnsafe()));
+
+        // Evict commands from the cache immediately to expose problems loading from disk.
+        sharedCluster.forEach(node -> node.runOnInstance(() -> AccordService.instance().setCacheSize(0)));
+
+        fn.accept(sharedCluster);
+
+        // Reset any messaging filters.
+        sharedCluster.filters().reset();
+    }
+
+    protected void test(FailingConsumer<Cluster> fn) throws Exception
+    {
+        test(""CREATE TABLE "" + currentTable + "" (k int, c int, v int, primary key (k, c))"", fn);
+    }
+
+    private static Cluster createCluster() throws IOException
+    {
+        // need to up the timeout else tests get flaky
+        // disable vnode for now, but should enable before trunk
+        return init(Cluster.build(2)
+                           .withoutVNodes()
+                           .withConfig(c -> c.with(Feature.NETWORK).set(""write_request_timeout"", ""10s"").set(""transaction_timeout"", ""15s""))
+                           .withInstanceInitializer(ByteBuddyHelper::install)
+                           .start());
+    }
+
+    // TODO: Retry on preemption may become unnecessary after the Unified Log is integrated.
+    protected static SimpleQueryResult assertRowEqualsWithPreemptedRetry(Cluster cluster, Object[] row, String check, Object... boundValues)
+    {
+        SimpleQueryResult result = executeWithRetry(cluster, check, boundValues);
+        Assertions.assertThat(result.toObjectArrays()).isEqualTo(row == null ? new Object[0] : new Object[] { row });
+        return result;
+    }
+
+    protected static SimpleQueryResult executeWithRetry(Cluster cluster, String check, Object... boundValues)
+    {
+        try
+        {
+            return cluster.coordinator(1).executeWithResult(check, ConsistencyLevel.ANY, boundValues);
+        }
+        catch (Throwable t)
+        {
+            if (Throwables.getRootCause(t).toString().contains(Preempted.class.getName()))
+                return executeWithRetry(cluster, check, boundValues);","[{'comment': 'should we add delays?  If so can use `org.apache.cassandra.utils.Retry#retryWithBackoffBlocking(int, java.util.function.Supplier<A>)`', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/utils/ByteBufferUtil.java,"@@ -533,6 +551,35 @@ else if (obj instanceof byte[])
             return ByteBuffer.wrap((byte[]) obj);
         else if (obj instanceof ByteBuffer)
             return (ByteBuffer) obj;
+        else if (obj instanceof List)
+        {
+            List<?> list = (List<?>) obj;
+            // convert subtypes to BB
+            List<ByteBuffer> bbs = list.stream().map(ByteBufferUtil::objectToBytes).collect(Collectors.toList());
+            // decompose/serializer doesn't use the isMultiCell, so safe to do this
+            return ListType.getInstance(BytesType.instance, false).decompose(bbs);
+        }
+        else if (obj instanceof Map)
+        {
+            Map<?, ?> map = (Map<?, ?>) obj;
+            // convert subtypes to BB
+            Map<ByteBuffer, ByteBuffer> bbs = new LinkedHashMap<>();
+            for (Map.Entry<?, ?> e : map.entrySet())
+                bbs.put(objectToBytes(e.getKey()), objectToBytes(e.getValue()));","[{'comment': 'for safety should we validate `put` returns `null`?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/utils/ByteBufferUtil.java,"@@ -533,6 +551,35 @@ else if (obj instanceof byte[])
             return ByteBuffer.wrap((byte[]) obj);
         else if (obj instanceof ByteBuffer)
             return (ByteBuffer) obj;
+        else if (obj instanceof List)
+        {
+            List<?> list = (List<?>) obj;
+            // convert subtypes to BB
+            List<ByteBuffer> bbs = list.stream().map(ByteBufferUtil::objectToBytes).collect(Collectors.toList());
+            // decompose/serializer doesn't use the isMultiCell, so safe to do this
+            return ListType.getInstance(BytesType.instance, false).decompose(bbs);
+        }
+        else if (obj instanceof Map)
+        {
+            Map<?, ?> map = (Map<?, ?>) obj;
+            // convert subtypes to BB
+            Map<ByteBuffer, ByteBuffer> bbs = new LinkedHashMap<>();
+            for (Map.Entry<?, ?> e : map.entrySet())
+                bbs.put(objectToBytes(e.getKey()), objectToBytes(e.getValue()));
+            // decompose/serializer doesn't use the isMultiCell, so safe to do this
+            return MapType.getInstance(BytesType.instance, BytesType.instance, false).decompose(bbs);
+        }
+        else if (obj instanceof Set)
+        {
+            Set<?> set = (Set<?>) obj;
+            // convert subtypes to BB
+            Set<ByteBuffer> bbs = new LinkedHashSet<>();
+            set.forEach(o -> bbs.add(objectToBytes(o)));","[{'comment': 'for safety should we validate `add` returns `true`?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/utils/ArraySerializers.java,"@@ -0,0 +1,56 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.IOException;
+import java.util.function.IntFunction;
+
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+
+import static com.google.common.primitives.Ints.checkedCast;
+import static org.apache.cassandra.db.TypeSizes.sizeofUnsignedVInt;
+
+public class ArraySerializers
+{
+    public static <T> void serializeArray(T[] items, DataOutputPlus out, int version, IVersionedSerializer<T> serializer) throws IOException
+    {
+        out.writeUnsignedVInt(items.length);
+        for (T item : items)
+            serializer.serialize(item, out, version);
+    }
+
+    public static <T> T[] deserializeArray(DataInputPlus in, int version, IVersionedSerializer<T> serializer, IntFunction<T[]> arrayFactory) throws IOException
+    {
+        int size = checkedCast(in.readUnsignedVInt());","[{'comment': 'since `checkedCast(in.readUnsignedVInt())` is the common case, should we add this to `DataInputPlus`?  `readCheckedUnsignedVInt`?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/service/accord/txn/TxnDataName.java,"@@ -0,0 +1,225 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class TxnDataName implements Comparable<TxnDataName>
+{
+    private static final TxnDataName RETURNING = new TxnDataName(Kind.RETURNING);
+
+    public enum Kind
+    {
+        USER((byte) 1),
+        RETURNING((byte) 2),
+        AUTO_READ((byte) 3);
+
+        private final byte value;
+
+        Kind(byte value)
+        {
+            this.value = value;
+        }
+
+        public static Kind from(byte b)
+        {
+            switch (b)
+            {
+                case 1:
+                    return USER;
+                case 2:
+                    return RETURNING;
+                case 3:
+                    return AUTO_READ;
+                default:
+                    throw new IllegalArgumentException(""Unknown kind: "" + b);
+            }
+        }
+    }
+
+    private final Kind kind;
+    private final String[] parts;
+
+    public TxnDataName(Kind kind, String... parts)
+    {
+        this.kind = kind;
+        this.parts = parts;
+    }
+
+    public static TxnDataName user(String name)
+    {
+        return new TxnDataName(Kind.USER, name);
+    }
+
+    public static TxnDataName returning()
+    {
+        return RETURNING;
+    }
+
+    public static TxnDataName partitionRead(TableMetadata metadata, DecoratedKey key, int index)","[{'comment': ""TODO: honestly I don't remember why `int index`..."", 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/service/accord/txn/TxnDataName.java,"@@ -0,0 +1,225 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class TxnDataName implements Comparable<TxnDataName>
+{
+    private static final TxnDataName RETURNING = new TxnDataName(Kind.RETURNING);
+
+    public enum Kind
+    {
+        USER((byte) 1),
+        RETURNING((byte) 2),
+        AUTO_READ((byte) 3);
+
+        private final byte value;
+
+        Kind(byte value)
+        {
+            this.value = value;
+        }
+
+        public static Kind from(byte b)
+        {
+            switch (b)
+            {
+                case 1:
+                    return USER;
+                case 2:
+                    return RETURNING;
+                case 3:
+                    return AUTO_READ;
+                default:
+                    throw new IllegalArgumentException(""Unknown kind: "" + b);
+            }
+        }
+    }
+
+    private final Kind kind;
+    private final String[] parts;
+
+    public TxnDataName(Kind kind, String... parts)
+    {
+        this.kind = kind;
+        this.parts = parts;
+    }
+
+    public static TxnDataName user(String name)
+    {
+        return new TxnDataName(Kind.USER, name);
+    }
+
+    public static TxnDataName returning()
+    {
+        return RETURNING;
+    }
+
+    public static TxnDataName partitionRead(TableMetadata metadata, DecoratedKey key, int index)
+    {
+        return new TxnDataName(Kind.AUTO_READ, metadata.keyspace, metadata.name, bytesToString(key.getKey()), String.valueOf(index));
+    }
+
+    private static String bytesToString(ByteBuffer bytes)
+    {
+        return ByteBufferUtil.bytesToHex(bytes);
+    }
+
+    private static ByteBuffer stringToBytes(String string)
+    {
+        return ByteBufferUtil.hexToBytes(string);
+    }","[{'comment': 'wondering if it would be best to switch from `String[]` to `ByteBuffer[]` as this adds extra memory cost... this is a hidden internal detail so we can change w/e we want; just something to think about.', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/service/accord/txn/TxnDataName.java,"@@ -0,0 +1,225 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class TxnDataName implements Comparable<TxnDataName>
+{
+    private static final TxnDataName RETURNING = new TxnDataName(Kind.RETURNING);
+
+    public enum Kind
+    {
+        USER((byte) 1),
+        RETURNING((byte) 2),
+        AUTO_READ((byte) 3);
+
+        private final byte value;
+
+        Kind(byte value)
+        {
+            this.value = value;
+        }
+
+        public static Kind from(byte b)
+        {
+            switch (b)
+            {
+                case 1:
+                    return USER;
+                case 2:
+                    return RETURNING;
+                case 3:
+                    return AUTO_READ;
+                default:
+                    throw new IllegalArgumentException(""Unknown kind: "" + b);
+            }
+        }
+    }
+
+    private final Kind kind;
+    private final String[] parts;
+
+    public TxnDataName(Kind kind, String... parts)
+    {
+        this.kind = kind;
+        this.parts = parts;
+    }
+
+    public static TxnDataName user(String name)
+    {
+        return new TxnDataName(Kind.USER, name);
+    }
+
+    public static TxnDataName returning()
+    {
+        return RETURNING;
+    }
+
+    public static TxnDataName partitionRead(TableMetadata metadata, DecoratedKey key, int index)
+    {
+        return new TxnDataName(Kind.AUTO_READ, metadata.keyspace, metadata.name, bytesToString(key.getKey()), String.valueOf(index));
+    }
+
+    private static String bytesToString(ByteBuffer bytes)
+    {
+        return ByteBufferUtil.bytesToHex(bytes);
+    }
+
+    private static ByteBuffer stringToBytes(String string)
+    {
+        return ByteBufferUtil.hexToBytes(string);
+    }
+
+    public Kind getKind()
+    {
+        return kind;
+    }
+
+    public List<String> getParts()
+    {
+        return Collections.unmodifiableList(Arrays.asList(parts));
+    }
+
+    public boolean isFor(TableMetadata metadata)
+    {
+        if (kind != Kind.AUTO_READ)
+            return false;
+        return metadata.keyspace.equals(parts[0])
+               && metadata.name.equals(parts[1]);
+    }
+
+    public DecoratedKey getDecoratedKey(TableMetadata metadata)
+    {
+        checkKind(Kind.AUTO_READ);","[{'comment': 'should we also check `isFor`?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/service/accord/txn/TxnDataName.java,"@@ -0,0 +1,225 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class TxnDataName implements Comparable<TxnDataName>
+{
+    private static final TxnDataName RETURNING = new TxnDataName(Kind.RETURNING);
+
+    public enum Kind
+    {
+        USER((byte) 1),
+        RETURNING((byte) 2),
+        AUTO_READ((byte) 3);
+
+        private final byte value;
+
+        Kind(byte value)
+        {
+            this.value = value;
+        }
+
+        public static Kind from(byte b)
+        {
+            switch (b)
+            {
+                case 1:
+                    return USER;
+                case 2:
+                    return RETURNING;
+                case 3:
+                    return AUTO_READ;
+                default:
+                    throw new IllegalArgumentException(""Unknown kind: "" + b);
+            }
+        }
+    }
+
+    private final Kind kind;
+    private final String[] parts;
+
+    public TxnDataName(Kind kind, String... parts)
+    {
+        this.kind = kind;
+        this.parts = parts;
+    }
+
+    public static TxnDataName user(String name)
+    {
+        return new TxnDataName(Kind.USER, name);
+    }
+
+    public static TxnDataName returning()
+    {
+        return RETURNING;
+    }
+
+    public static TxnDataName partitionRead(TableMetadata metadata, DecoratedKey key, int index)
+    {
+        return new TxnDataName(Kind.AUTO_READ, metadata.keyspace, metadata.name, bytesToString(key.getKey()), String.valueOf(index));
+    }
+
+    private static String bytesToString(ByteBuffer bytes)
+    {
+        return ByteBufferUtil.bytesToHex(bytes);
+    }
+
+    private static ByteBuffer stringToBytes(String string)
+    {
+        return ByteBufferUtil.hexToBytes(string);
+    }
+
+    public Kind getKind()
+    {
+        return kind;
+    }
+
+    public List<String> getParts()
+    {
+        return Collections.unmodifiableList(Arrays.asList(parts));
+    }
+
+    public boolean isFor(TableMetadata metadata)
+    {
+        if (kind != Kind.AUTO_READ)
+            return false;
+        return metadata.keyspace.equals(parts[0])
+               && metadata.name.equals(parts[1]);
+    }
+
+    public DecoratedKey getDecoratedKey(TableMetadata metadata)
+    {
+        checkKind(Kind.AUTO_READ);
+        ByteBuffer data = stringToBytes(parts[2]);
+        return metadata.partitioner.decorateKey(data);
+    }
+
+    public boolean atIndex(int index)
+    {
+        checkKind(Kind.AUTO_READ);
+        return Integer.parseInt(parts[3]) == index;
+    }
+
+    private void checkKind(Kind expected)
+    {
+        if (kind != expected)
+            throw new IllegalStateException(""Expected kind "" + expected + "" but is "" + kind);
+    }
+
+    public long estimatedSizeOnHeap()
+    {
+        long size = 0;","[{'comment': 'we should include `EMPTY` that we learn from JAMM...', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/service/accord/txn/TxnDataName.java,"@@ -0,0 +1,225 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class TxnDataName implements Comparable<TxnDataName>
+{
+    private static final TxnDataName RETURNING = new TxnDataName(Kind.RETURNING);
+
+    public enum Kind
+    {
+        USER((byte) 1),
+        RETURNING((byte) 2),
+        AUTO_READ((byte) 3);
+
+        private final byte value;
+
+        Kind(byte value)
+        {
+            this.value = value;
+        }
+
+        public static Kind from(byte b)
+        {
+            switch (b)
+            {
+                case 1:
+                    return USER;
+                case 2:
+                    return RETURNING;
+                case 3:
+                    return AUTO_READ;
+                default:
+                    throw new IllegalArgumentException(""Unknown kind: "" + b);
+            }
+        }
+    }
+
+    private final Kind kind;
+    private final String[] parts;
+
+    public TxnDataName(Kind kind, String... parts)
+    {
+        this.kind = kind;
+        this.parts = parts;
+    }
+
+    public static TxnDataName user(String name)
+    {
+        return new TxnDataName(Kind.USER, name);
+    }
+
+    public static TxnDataName returning()
+    {
+        return RETURNING;
+    }
+
+    public static TxnDataName partitionRead(TableMetadata metadata, DecoratedKey key, int index)
+    {
+        return new TxnDataName(Kind.AUTO_READ, metadata.keyspace, metadata.name, bytesToString(key.getKey()), String.valueOf(index));
+    }
+
+    private static String bytesToString(ByteBuffer bytes)
+    {
+        return ByteBufferUtil.bytesToHex(bytes);
+    }
+
+    private static ByteBuffer stringToBytes(String string)
+    {
+        return ByteBufferUtil.hexToBytes(string);
+    }
+
+    public Kind getKind()
+    {
+        return kind;
+    }
+
+    public List<String> getParts()
+    {
+        return Collections.unmodifiableList(Arrays.asList(parts));
+    }
+
+    public boolean isFor(TableMetadata metadata)
+    {
+        if (kind != Kind.AUTO_READ)
+            return false;
+        return metadata.keyspace.equals(parts[0])
+               && metadata.name.equals(parts[1]);
+    }
+
+    public DecoratedKey getDecoratedKey(TableMetadata metadata)
+    {
+        checkKind(Kind.AUTO_READ);
+        ByteBuffer data = stringToBytes(parts[2]);
+        return metadata.partitioner.decorateKey(data);
+    }
+
+    public boolean atIndex(int index)
+    {
+        checkKind(Kind.AUTO_READ);
+        return Integer.parseInt(parts[3]) == index;
+    }
+
+    private void checkKind(Kind expected)
+    {
+        if (kind != expected)
+            throw new IllegalStateException(""Expected kind "" + expected + "" but is "" + kind);
+    }
+
+    public long estimatedSizeOnHeap()
+    {
+        long size = 0;
+        for (String part : parts)
+            size += part.length();","[{'comment': 'should include `String` object overhead right?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/service/accord/txn/TxnDataName.java,"@@ -0,0 +1,225 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class TxnDataName implements Comparable<TxnDataName>
+{
+    private static final TxnDataName RETURNING = new TxnDataName(Kind.RETURNING);
+
+    public enum Kind
+    {
+        USER((byte) 1),
+        RETURNING((byte) 2),
+        AUTO_READ((byte) 3);
+
+        private final byte value;
+
+        Kind(byte value)
+        {
+            this.value = value;
+        }
+
+        public static Kind from(byte b)
+        {
+            switch (b)
+            {
+                case 1:
+                    return USER;
+                case 2:
+                    return RETURNING;
+                case 3:
+                    return AUTO_READ;
+                default:
+                    throw new IllegalArgumentException(""Unknown kind: "" + b);
+            }
+        }
+    }
+
+    private final Kind kind;
+    private final String[] parts;
+
+    public TxnDataName(Kind kind, String... parts)
+    {
+        this.kind = kind;
+        this.parts = parts;
+    }
+
+    public static TxnDataName user(String name)
+    {
+        return new TxnDataName(Kind.USER, name);
+    }
+
+    public static TxnDataName returning()
+    {
+        return RETURNING;
+    }
+
+    public static TxnDataName partitionRead(TableMetadata metadata, DecoratedKey key, int index)
+    {
+        return new TxnDataName(Kind.AUTO_READ, metadata.keyspace, metadata.name, bytesToString(key.getKey()), String.valueOf(index));
+    }
+
+    private static String bytesToString(ByteBuffer bytes)
+    {
+        return ByteBufferUtil.bytesToHex(bytes);
+    }
+
+    private static ByteBuffer stringToBytes(String string)
+    {
+        return ByteBufferUtil.hexToBytes(string);
+    }
+
+    public Kind getKind()
+    {
+        return kind;
+    }
+
+    public List<String> getParts()
+    {
+        return Collections.unmodifiableList(Arrays.asList(parts));
+    }
+
+    public boolean isFor(TableMetadata metadata)
+    {
+        if (kind != Kind.AUTO_READ)
+            return false;
+        return metadata.keyspace.equals(parts[0])
+               && metadata.name.equals(parts[1]);
+    }
+
+    public DecoratedKey getDecoratedKey(TableMetadata metadata)
+    {
+        checkKind(Kind.AUTO_READ);
+        ByteBuffer data = stringToBytes(parts[2]);
+        return metadata.partitioner.decorateKey(data);
+    }
+
+    public boolean atIndex(int index)
+    {
+        checkKind(Kind.AUTO_READ);
+        return Integer.parseInt(parts[3]) == index;
+    }
+
+    private void checkKind(Kind expected)
+    {
+        if (kind != expected)
+            throw new IllegalStateException(""Expected kind "" + expected + "" but is "" + kind);
+    }
+
+    public long estimatedSizeOnHeap()
+    {
+        long size = 0;
+        for (String part : parts)
+            size += part.length();
+        return size;
+    }
+
+    @Override
+    public int compareTo(TxnDataName o)
+    {
+        int rc = kind.compareTo(o.kind);
+        if (rc != 0)
+            return rc;
+        // same kind has same length
+        int size = parts.length;
+        assert o.parts.length == size : String.format(""Expected other.parts.length == %d but was %d"", size, o.parts.length);
+        for (int i = 0; i < size; i++)
+        {
+            rc = parts[i].compareTo(o.parts[i]);
+            if (rc != 0)
+                return rc;
+        }
+        return 0;
+    }
+
+    @Override
+    public boolean equals(Object o)
+    {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+        TxnDataName that = (TxnDataName) o;
+        return kind == that.kind && Arrays.equals(parts, that.parts);
+    }
+
+    @Override
+    public int hashCode()
+    {
+        int result = Objects.hash(kind);
+        result = 31 * result + Arrays.hashCode(parts);
+        return result;
+    }
+
+    public String name()
+    {
+        return String.join("":"", parts);
+    }
+
+    @Override
+    public String toString()
+    {
+        return kind.name() + "":"" + name();
+    }
+
+    public static final IVersionedSerializer<TxnDataName> serializer = new IVersionedSerializer<TxnDataName>()
+    {
+        @Override
+        public void serialize(TxnDataName t, DataOutputPlus out, int version) throws IOException
+        {
+            out.writeByte(t.kind.value);
+            out.writeInt(t.parts.length);","[{'comment': 'you have ArraySerializer now, should we delegate to that?  that also does `vint` rather than `int` so may also be smaller?', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/service/accord/AccordService.java,"@@ -99,6 +110,39 @@ public static long nowInMicros()
         return TimeUnit.MILLISECONDS.toMicros(Clock.Global.currentTimeMillis());
     }
 
+    public TxnData coordinate(Txn txn)
+    {
+        try
+        {
+            Future<Result> future = node.coordinate(txn);
+            Result result = future.get(DatabaseDescriptor.getTransactionTimeout(TimeUnit.SECONDS), TimeUnit.SECONDS);","[{'comment': 'nit: if someone does `500ms` this will be incorrect, we should do `TimeUnit.NANOSECONDS` to avoid this issue', 'commenter': 'dcapwell'}]"
1962,src/java/org/apache/cassandra/service/accord/AccordService.java,"@@ -99,6 +110,39 @@ public static long nowInMicros()
         return TimeUnit.MILLISECONDS.toMicros(Clock.Global.currentTimeMillis());
     }
 
+    public TxnData coordinate(Txn txn)
+    {
+        try
+        {
+            Future<Result> future = node.coordinate(txn);
+            Result result = future.get(DatabaseDescriptor.getTransactionTimeout(TimeUnit.SECONDS), TimeUnit.SECONDS);
+            return (TxnData) result;
+        }
+        catch (ExecutionException e)
+        {
+            Throwable cause = e.getCause();
+            if (cause instanceof Timeout)
+                throw throwTimeout(txn);
+            throw new RuntimeException(e);","[{'comment': 'this should throw `cause`?', 'commenter': 'dcapwell'}]"
1962,test/unit/org/apache/cassandra/service/accord/txn/TxnBuilder.java,"@@ -0,0 +1,191 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.SortedSet;
+import java.util.TreeSet;
+
+import accord.primitives.Keys;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Iterables;
+
+import accord.api.Key;
+import accord.primitives.Txn;
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.cql3.QueryProcessor;
+import org.apache.cassandra.cql3.VariableSpecifications;
+import org.apache.cassandra.cql3.statements.ModificationStatement;
+import org.apache.cassandra.cql3.statements.SelectStatement;
+import org.apache.cassandra.db.ReadQuery;
+import org.apache.cassandra.db.SinglePartitionReadCommand;
+import org.apache.cassandra.db.SinglePartitionReadQuery;
+import org.apache.cassandra.db.partitions.PartitionUpdate;
+import org.apache.cassandra.schema.ColumnMetadata;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.ClientState;
+import org.apache.cassandra.service.accord.api.AccordKey;
+
+public class TxnBuilder
+{
+    private final List<TxnNamedRead> reads = new ArrayList<>();
+    private final List<TxnWrite.Fragment> writes = new ArrayList<>();
+    private final List<TxnCondition> conditions = new ArrayList<>();
+
+    public static TxnBuilder builder()
+    {
+        return new TxnBuilder();
+    }
+
+    public TxnBuilder withRead(String name, String query)
+    {
+        return withRead(TxnDataName.user(name), query, VariableSpecifications.empty());
+    }
+
+    public TxnBuilder withRead(TxnDataName name, String query)
+    {
+        return withRead(name, query, VariableSpecifications.empty());
+    }
+
+    public TxnBuilder withRead(TxnDataName name, String query, VariableSpecifications bindVariables, Object... values)
+    {
+        SelectStatement.RawStatement parsed = (SelectStatement.RawStatement) QueryProcessor.parseStatement(query);","[{'comment': 'Just tested this... you can tell parser we are doing a txn!\r\n\r\n```\r\npublic static CQLStatement.Raw parseStatement(String queryStr) throws SyntaxException\r\n    {\r\n        try\r\n        {\r\n            return CQLFragmentParser.parseAnyUnhandled(parser -> {\r\n                parser.gParser.isParsingTxn = true;\r\n                try\r\n                {\r\n                    return parser.query();\r\n                }\r\n                finally\r\n                {\r\n                    parser.gParser.isParsingTxn = false;\r\n                }\r\n            }, queryStr);\r\n        }\r\n```', 'commenter': 'dcapwell'}, {'comment': ""I do wonder if its best to change this class though to create a CQL and parse that... we have logic in `TransactionStatement` to better understand the semantics, but this doesn't do that and is very raw... "", 'commenter': 'dcapwell'}]"
1962,test/unit/org/apache/cassandra/service/accord/txn/TxnBuilder.java,"@@ -0,0 +1,191 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.SortedSet;
+import java.util.TreeSet;
+
+import accord.primitives.Keys;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Iterables;
+
+import accord.api.Key;
+import accord.primitives.Txn;
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.cql3.QueryProcessor;
+import org.apache.cassandra.cql3.VariableSpecifications;
+import org.apache.cassandra.cql3.statements.ModificationStatement;
+import org.apache.cassandra.cql3.statements.SelectStatement;
+import org.apache.cassandra.db.ReadQuery;
+import org.apache.cassandra.db.SinglePartitionReadCommand;
+import org.apache.cassandra.db.SinglePartitionReadQuery;
+import org.apache.cassandra.db.partitions.PartitionUpdate;
+import org.apache.cassandra.schema.ColumnMetadata;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.ClientState;
+import org.apache.cassandra.service.accord.api.AccordKey;
+
+public class TxnBuilder
+{
+    private final List<TxnNamedRead> reads = new ArrayList<>();
+    private final List<TxnWrite.Fragment> writes = new ArrayList<>();
+    private final List<TxnCondition> conditions = new ArrayList<>();
+
+    public static TxnBuilder builder()
+    {
+        return new TxnBuilder();
+    }
+
+    public TxnBuilder withRead(String name, String query)
+    {
+        return withRead(TxnDataName.user(name), query, VariableSpecifications.empty());
+    }
+
+    public TxnBuilder withRead(TxnDataName name, String query)
+    {
+        return withRead(name, query, VariableSpecifications.empty());
+    }
+
+    public TxnBuilder withRead(TxnDataName name, String query, VariableSpecifications bindVariables, Object... values)","[{'comment': ""would be good to make `private`; can increase the visibility later, but nice to hide when we don't use externally"", 'commenter': 'dcapwell'}]"
1962,test/unit/org/apache/cassandra/service/accord/txn/TxnBuilder.java,"@@ -0,0 +1,191 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.txn;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.SortedSet;
+import java.util.TreeSet;
+
+import accord.primitives.Keys;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Iterables;
+
+import accord.api.Key;
+import accord.primitives.Txn;
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.cql3.QueryProcessor;
+import org.apache.cassandra.cql3.VariableSpecifications;
+import org.apache.cassandra.cql3.statements.ModificationStatement;
+import org.apache.cassandra.cql3.statements.SelectStatement;
+import org.apache.cassandra.db.ReadQuery;
+import org.apache.cassandra.db.SinglePartitionReadCommand;
+import org.apache.cassandra.db.SinglePartitionReadQuery;
+import org.apache.cassandra.db.partitions.PartitionUpdate;
+import org.apache.cassandra.schema.ColumnMetadata;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.ClientState;
+import org.apache.cassandra.service.accord.api.AccordKey;
+
+public class TxnBuilder
+{
+    private final List<TxnNamedRead> reads = new ArrayList<>();
+    private final List<TxnWrite.Fragment> writes = new ArrayList<>();
+    private final List<TxnCondition> conditions = new ArrayList<>();
+
+    public static TxnBuilder builder()
+    {
+        return new TxnBuilder();
+    }
+
+    public TxnBuilder withRead(String name, String query)
+    {
+        return withRead(TxnDataName.user(name), query, VariableSpecifications.empty());
+    }
+
+    public TxnBuilder withRead(TxnDataName name, String query)
+    {
+        return withRead(name, query, VariableSpecifications.empty());
+    }
+
+    public TxnBuilder withRead(TxnDataName name, String query, VariableSpecifications bindVariables, Object... values)
+    {
+        SelectStatement.RawStatement parsed = (SelectStatement.RawStatement) QueryProcessor.parseStatement(query);
+        // the parser will only let us define a ref name if we're parsing a transaction, which we're not
+        // so we need to manually add it in the call, and confirm nothing got parsed
+        Preconditions.checkState(parsed.parameters.refName == null);
+
+        SelectStatement statement = parsed.prepare(bindVariables);
+        QueryOptions queryOptions = QueryProcessor.makeInternalOptions(statement, values);
+        ReadQuery readQuery = statement.getQuery(queryOptions, 0);
+        SinglePartitionReadQuery.Group<SinglePartitionReadCommand> selectQuery = (SinglePartitionReadQuery.Group<SinglePartitionReadCommand>) readQuery;
+        reads.add(new TxnNamedRead(name, Iterables.getOnlyElement(selectQuery.queries)));
+        return this;
+    }
+
+    public TxnBuilder withWrite(PartitionUpdate update, TxnReferenceOperations referenceOps)
+    {
+        int index = writes.size();
+        writes.add(new TxnWrite.Fragment(AccordKey.of(update), index, update, referenceOps));
+        return this;
+    }
+
+    public TxnBuilder withWrite(String query, TxnReferenceOperations referenceOps, VariableSpecifications variables, Object... values)
+    {
+        ModificationStatement.Parsed parsed = (ModificationStatement.Parsed) QueryProcessor.parseStatement(query);
+        ModificationStatement prepared = parsed.prepare(variables);
+        QueryOptions options = QueryProcessor.makeInternalOptions(prepared, values);
+        return withWrite(prepared.getTxnUpdate(ClientState.forInternalCalls(), options), referenceOps);
+    }
+
+    public TxnBuilder withWrite(String query)
+    {
+        return withWrite(query, TxnReferenceOperations.empty(), VariableSpecifications.empty());
+    }
+
+    static TxnReference reference(TxnDataName name, String column)
+    {
+        ColumnMetadata metadata = null;
+        if (column != null)
+        {
+            String[] parts = column.split(""\\."");
+            Preconditions.checkArgument(parts.length == 3);","[{'comment': 'so `keyspace.table.name`?  what happens if you do `a.b`?  that would be size 4.\r\n\r\nAlso, can we get a useful error msg?', 'commenter': 'dcapwell'}]"
1965,test/distributed/org/apache/cassandra/distributed/test/ring/BootstrapTest.java,"@@ -196,6 +197,12 @@ public void bootstrapUnspecifiedFailsOnResumeTest() throws Throwable
                                bootstrap()),
                         newInstance.config().num());
         }
+        catch (AssumptionViolatedException ave)
+        {
+            // We get an AssumptionViolatedException if we're in a test job configured w/vnodes
+            if (ave.getMessage().contains(""vnode is requested but not supported""))","[{'comment': 'remove this check, the type is very explicit ""junit ignore this test"", you trying to ignore that isn\'t good and will be brittle.\r\n\r\nAlso, add `throw ave` so it actually is skipped and not passing.', 'commenter': 'dcapwell'}]"
1991,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -321,7 +315,7 @@ public long latestEpoch()
     }
 
     @Override
-    public Timestamp maxConflict(Keys keys)
+    public Timestamp preaccept(TxnId txnId, Keys keys)","[{'comment': 'Why does `preaccept` not include the same logic as the implementation in `InMemoryCommandStore` around sometimes returning a new `Timestamp`?', 'commenter': 'aweisberg'}, {'comment': 'Good point. This was a (very) bad rebase.', 'commenter': 'belliottsmith'}]"
1991,src/java/org/apache/cassandra/service/accord/api/AccordAgent.java,"@@ -49,4 +50,10 @@ public void onHandledException(Throwable throwable)
     {
         // TODO: this
     }
+
+    @Override
+    public boolean isExpired(TxnId initiated, long now)
+    {
+        return false;","[{'comment': 'No check at all? This is TBD until it can be configured?', 'commenter': 'aweisberg'}, {'comment': ""Yeah, I think we also want to discriminate between write and read transactions here. I don't mind plumbing it in for now, but it's probably better to leave it until later - it's also not essential for correctness, it only applies to long running original coordinators. I vacillated over whether we should keep it at all, but decided it was probably better."", 'commenter': 'belliottsmith'}, {'comment': 'As it happens, it looks like the whole Agent is to-be-implemented anyway', 'commenter': 'belliottsmith'}]"
1998,src/java/org/apache/cassandra/db/marshal/TypeParser.java,"@@ -130,6 +138,73 @@ public AbstractType<?> parse() throws SyntaxException, ConfigurationException
             return getAbstractType(name);
     }
 
+
+    /**
+     * parse PartitionOrdering from old version of PartitionOrdering' string format 
+     * */
+    private static  AbstractType<?> defaultParsePartitionOrdering(TypeParser typeParser)
+    {
+        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();
+        Iterator<String> argIterator = typeParser.getKeyValueParameters().keySet().iterator();
+        if (argIterator.hasNext()) {
+            partitioner = FBUtilities.newPartitioner(argIterator.next());
+            assert !argIterator.hasNext();
+        }
+        return partitioner.partitionOrdering();
+    }
+    
+    //the format is (partitioner:type)
+    public AbstractType<?> getPartitionerDefinedOrder()
+    {
+        if (isEOS())
+            return defaultParsePartitionOrdering(this);
+        skipBlank();","[{'comment': 'I would move this before the `isEOS` check.', 'commenter': 'blambov'}, {'comment': 'ok', 'commenter': 'Maxwell-Guo'}]"
1998,src/java/org/apache/cassandra/db/marshal/TypeParser.java,"@@ -130,6 +138,73 @@ public AbstractType<?> parse() throws SyntaxException, ConfigurationException
             return getAbstractType(name);
     }
 
+
+    /**
+     * parse PartitionOrdering from old version of PartitionOrdering' string format 
+     * */
+    private static  AbstractType<?> defaultParsePartitionOrdering(TypeParser typeParser)
+    {
+        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();
+        Iterator<String> argIterator = typeParser.getKeyValueParameters().keySet().iterator();
+        if (argIterator.hasNext()) {
+            partitioner = FBUtilities.newPartitioner(argIterator.next());
+            assert !argIterator.hasNext();
+        }
+        return partitioner.partitionOrdering();
+    }
+    
+    //the format is (partitioner:type)
+    public AbstractType<?> getPartitionerDefinedOrder()
+    {
+        if (isEOS())
+            return defaultParsePartitionOrdering(this);
+        skipBlank();
+        if (str.charAt(idx) != '(')
+            throw new IllegalStateException();
+        Pair<Boolean, AbstractType<?>>  result = null;
+        ++idx; // skipping '('
+
+        if (str.charAt(idx) == ')')
+        {
+            ++idx;
+            return  defaultParsePartitionOrdering(this);
+        }
+        skipBlank();","[{'comment': 'This also makes better sense before the check above.', 'commenter': 'blambov'}, {'comment': 'ok', 'commenter': 'Maxwell-Guo'}]"
1998,src/java/org/apache/cassandra/db/marshal/PartitionerDefinedOrder.java,"@@ -28,29 +29,38 @@
 import org.apache.cassandra.dht.IPartitioner;
 import org.apache.cassandra.utils.ByteBufferUtil;
 import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.Pair;
 
 /** for sorting columns representing row keys in the row ordering as determined by a partitioner.
  * Not intended for user-defined CFs, and will in fact error out if used with such. */
 public class PartitionerDefinedOrder extends AbstractType<ByteBuffer>
 {
     private final IPartitioner partitioner;
-
+    private final AbstractType<?> baseType;
+    
     public PartitionerDefinedOrder(IPartitioner partitioner)
     {
         super(ComparisonType.CUSTOM);
         this.partitioner = partitioner;
+        this.baseType = null;
+    }
+
+    public PartitionerDefinedOrder(IPartitioner partitioner, AbstractType<?> baseType)
+    {
+        super(ComparisonType.CUSTOM);
+        this.partitioner = partitioner;
+        this.baseType = baseType;
     }
 
     public static AbstractType<?> getInstance(TypeParser parser)
     {
-        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();
-        Iterator<String> argIterator = parser.getKeyValueParameters().keySet().iterator();
-        if (argIterator.hasNext())
-        {
-            partitioner = FBUtilities.newPartitioner(argIterator.next());
-            assert !argIterator.hasNext();
-        }
-        return partitioner.partitionOrdering();
+        TypeParser clone = parser.clone();
+        return clone.getPartitionerDefinedOrder();","[{'comment': ""There shouldn't be a need for `clone` here. (The clone might even break something, in the unlikely case of this being part of a composite/tuple.)"", 'commenter': 'blambov'}, {'comment': '+1', 'commenter': 'Maxwell-Guo'}]"
1998,src/java/org/apache/cassandra/db/marshal/TypeParser.java,"@@ -130,6 +138,73 @@ public AbstractType<?> parse() throws SyntaxException, ConfigurationException
             return getAbstractType(name);
     }
 
+
+    /**
+     * parse PartitionOrdering from old version of PartitionOrdering' string format 
+     * */
+    private static  AbstractType<?> defaultParsePartitionOrdering(TypeParser typeParser)
+    {
+        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();
+        Iterator<String> argIterator = typeParser.getKeyValueParameters().keySet().iterator();
+        if (argIterator.hasNext()) {
+            partitioner = FBUtilities.newPartitioner(argIterator.next());
+            assert !argIterator.hasNext();
+        }
+        return partitioner.partitionOrdering();
+    }
+    
+    //the format is (partitioner:type)
+    public AbstractType<?> getPartitionerDefinedOrder()
+    {
+        if (isEOS())
+            return defaultParsePartitionOrdering(this);
+        skipBlank();
+        if (str.charAt(idx) != '(')
+            throw new IllegalStateException();
+        Pair<Boolean, AbstractType<?>>  result = null;
+        ++idx; // skipping '('
+
+        if (str.charAt(idx) == ')')
+        {
+            ++idx;
+            return  defaultParsePartitionOrdering(this);
+        }
+        skipBlank();
+        String k = readNextIdentifier();
+        skipBlank();
+        if (str.charAt(idx) == ':')
+        {
+            ++idx;
+            skipBlank();
+        }
+        else if (str.charAt(idx) != ',' && str.charAt(idx) != ')')","[{'comment': ""Shouldn't we throw an error if the next part is neither a separator nor a closing bracket?"", 'commenter': 'blambov'}, {'comment': 'yes ,throw an error may be more efficient', 'commenter': 'Maxwell-Guo'}, {'comment': 'Why do we accept a comma?', 'commenter': 'blambov'}, {'comment': 'Yes, a comma is not need here, remove it now .', 'commenter': 'Maxwell-Guo'}]"
1998,src/java/org/apache/cassandra/db/marshal/TypeParser.java,"@@ -130,6 +138,73 @@ public AbstractType<?> parse() throws SyntaxException, ConfigurationException
             return getAbstractType(name);
     }
 
+
+    /**
+     * parse PartitionOrdering from old version of PartitionOrdering' string format 
+     * */
+    private static  AbstractType<?> defaultParsePartitionOrdering(TypeParser typeParser)
+    {
+        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();
+        Iterator<String> argIterator = typeParser.getKeyValueParameters().keySet().iterator();
+        if (argIterator.hasNext()) {
+            partitioner = FBUtilities.newPartitioner(argIterator.next());
+            assert !argIterator.hasNext();
+        }
+        return partitioner.partitionOrdering();
+    }
+    
+    //the format is (partitioner:type)","[{'comment': 'Could you expand on this? The actual format appears to permit more, at least `PartitionerDefinedOrder`, `PartitionerDefinedOrder(<partitioner>)` and `PartitionerDefinedOrder(<partitioner>:<baseType>)`.', 'commenter': 'blambov'}, {'comment': '+1', 'commenter': 'Maxwell-Guo'}]"
1998,src/java/org/apache/cassandra/db/marshal/TypeParser.java,"@@ -130,6 +138,73 @@ public AbstractType<?> parse() throws SyntaxException, ConfigurationException
             return getAbstractType(name);
     }
 
+
+    /**
+     * parse PartitionOrdering from old version of PartitionOrdering' string format 
+     * */
+    private static  AbstractType<?> defaultParsePartitionOrdering(TypeParser typeParser)
+    {
+        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();
+        Iterator<String> argIterator = typeParser.getKeyValueParameters().keySet().iterator();
+        if (argIterator.hasNext()) {
+            partitioner = FBUtilities.newPartitioner(argIterator.next());
+            assert !argIterator.hasNext();
+        }
+        return partitioner.partitionOrdering();
+    }
+    
+    //the format is (partitioner:type)
+    public AbstractType<?> getPartitionerDefinedOrder()
+    {
+        if (isEOS())
+            return defaultParsePartitionOrdering(this);
+        skipBlank();
+        if (str.charAt(idx) != '(')
+            throw new IllegalStateException();
+        Pair<Boolean, AbstractType<?>>  result = null;
+        ++idx; // skipping '('
+
+        if (str.charAt(idx) == ')')
+        {
+            ++idx;
+            return  defaultParsePartitionOrdering(this);","[{'comment': ""Couldn't we just use `DatabaseDescriptor.getPartitioner().partitionerOrdering()` here?"", 'commenter': 'blambov'}, {'comment': 'just want to using the old format type parse of PartitionerDefinedOrder to do a double check parse. \r\nif the string of type is %s(%s) ', 'commenter': 'Maxwell-Guo'}, {'comment': 'In this case we have processed ""PartitionerDefinedOrder()"". Do we expect other parameters after the closing bracket?', 'commenter': 'blambov'}, {'comment': 'for line 176 we processed case: PartitionerDefinedOrder(<partitioner>) and We do not expect other parameters after closing bracket. \r\n', 'commenter': 'Maxwell-Guo'}]"
1998,src/java/org/apache/cassandra/db/marshal/TypeParser.java,"@@ -130,6 +138,73 @@ public AbstractType<?> parse() throws SyntaxException, ConfigurationException
             return getAbstractType(name);
     }
 
+
+    /**
+     * parse PartitionOrdering from old version of PartitionOrdering' string format 
+     * */
+    private static  AbstractType<?> defaultParsePartitionOrdering(TypeParser typeParser)
+    {
+        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();
+        Iterator<String> argIterator = typeParser.getKeyValueParameters().keySet().iterator();
+        if (argIterator.hasNext()) {
+            partitioner = FBUtilities.newPartitioner(argIterator.next());
+            assert !argIterator.hasNext();
+        }
+        return partitioner.partitionOrdering();
+    }
+    
+    //the format is (partitioner:type)
+    public AbstractType<?> getPartitionerDefinedOrder()
+    {
+        if (isEOS())
+            return defaultParsePartitionOrdering(this);
+        skipBlank();
+        if (str.charAt(idx) != '(')
+            throw new IllegalStateException();
+        Pair<Boolean, AbstractType<?>>  result = null;
+        ++idx; // skipping '('
+
+        if (str.charAt(idx) == ')')
+        {
+            ++idx;
+            return  defaultParsePartitionOrdering(this);
+        }
+        skipBlank();
+        String k = readNextIdentifier();
+        skipBlank();
+        if (str.charAt(idx) == ':')
+        {
+            ++idx;
+            skipBlank();
+        }
+        else if (str.charAt(idx) != ',' && str.charAt(idx) != ')')
+        {
+            return defaultParsePartitionOrdering(this);
+        }
+        IPartitioner partitioner = FBUtilities.newPartitioner(k);
+        AbstractType<?> type = partitioner.partitionOrdering();
+        if (partitioner.partitionOrdering() instanceof PartitionerDefinedOrder)
+        {
+            PartitionerDefinedOrder tmp = (PartitionerDefinedOrder) partitioner.partitionOrdering();
+            ++idx;
+            try
+            {
+                type = tmp.withBaseType(parse());
+            }
+            catch (Throwable throwable)
+            {
+                Iterator<String> argIterator = this.getKeyValueParameters().keySet().iterator();","[{'comment': ""Shouldn't we just throw an error in this case?"", 'commenter': 'blambov'}, {'comment': 'just want to using the old format type parse of PartitionerDefinedOrder to do a double check parse.\r\nif the string of type is %s(%s)', 'commenter': 'Maxwell-Guo'}, {'comment': '""PartitionerDefinedOrder(%s)"" can be handled more easily by just checking if `:` was seen. If not, just return `type`. If yes, then `((PartitionerDefinedOrder)type).withBaseType(parse())` must succeed.\r\n\r\nAre you trying to preserve existing bad behavior of the parser (e.g. accepting string such as ""PartitionerDefinedOrder(Murmur3Partitioner=something)"")?', 'commenter': 'blambov'}]"
1998,src/java/org/apache/cassandra/db/marshal/TypeParser.java,"@@ -130,6 +138,73 @@ public AbstractType<?> parse() throws SyntaxException, ConfigurationException
             return getAbstractType(name);
     }
 
+
+    /**
+     * parse PartitionOrdering from old version of PartitionOrdering' string format 
+     * */
+    private static  AbstractType<?> defaultParsePartitionOrdering(TypeParser typeParser)
+    {
+        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();
+        Iterator<String> argIterator = typeParser.getKeyValueParameters().keySet().iterator();
+        if (argIterator.hasNext()) {
+            partitioner = FBUtilities.newPartitioner(argIterator.next());
+            assert !argIterator.hasNext();
+        }
+        return partitioner.partitionOrdering();
+    }
+    
+    //the format is (partitioner:type)
+    public AbstractType<?> getPartitionerDefinedOrder()
+    {
+        if (isEOS())
+            return defaultParsePartitionOrdering(this);
+        skipBlank();
+        if (str.charAt(idx) != '(')
+            throw new IllegalStateException();
+        Pair<Boolean, AbstractType<?>>  result = null;
+        ++idx; // skipping '('
+
+        if (str.charAt(idx) == ')')
+        {
+            ++idx;
+            return  defaultParsePartitionOrdering(this);
+        }
+        skipBlank();
+        String k = readNextIdentifier();
+        skipBlank();
+        if (str.charAt(idx) == ':')
+        {
+            ++idx;
+            skipBlank();
+        }
+        else if (str.charAt(idx) != ',' && str.charAt(idx) != ')')
+        {
+            return defaultParsePartitionOrdering(this);
+        }
+        IPartitioner partitioner = FBUtilities.newPartitioner(k);
+        AbstractType<?> type = partitioner.partitionOrdering();
+        if (partitioner.partitionOrdering() instanceof PartitionerDefinedOrder)","[{'comment': 'Looks like this test should be checking if a `:` was seen instead.', 'commenter': 'blambov'}, {'comment': '@blambov \r\n It may no need to do checking.\r\nThere will be two  possibilities when arrive here,1.PartitionerDefinedOrder(<partitioner>), then the final results will returned at the catch section from line 194 to line 205 .the partitioner type is get from k\r\n2.PartitionerDefinedOrder(<partitioner>:<baseType>),when arriving here the base type can be parsed with function parse() at line 192 ,also the partitioner type is get with k.\r\nFor ByteOrderedPartitioner AND OrderPreservingPartitioner ,their partitionOrdering() will return ByteType.Instance and UTF8Type.Instance so we should only do for Random and mum3', 'commenter': 'Maxwell-Guo'}, {'comment': ""I see what you mean, and it makes sense if we want to preserve existing poor parsing behavior.\r\n\r\nHowever, I would treat an appearance of e.g. `PartitionerDefinedOrder(ByteOrderedPartitioner)` as an error, because in that case there exists an underlying type that can and will be used directly. If the type is specified as `PartitionerDefinedOrder`, the result of this construction must be an instance of that.\r\n\r\nThe strings we are parsing here are generated internally; I don't believe the user has any control over them and thus can't construct invalid-but-acceptable definitions such as this."", 'commenter': 'blambov'}]"
1998,test/unit/org/apache/cassandra/db/marshal/TypeParserTest.java,"@@ -98,4 +98,24 @@ public void testParsePartitionerOrder() throws ConfigurationException, SyntaxExc
         }
         assertSame(DatabaseDescriptor.getPartitioner().partitionOrdering(), TypeParser.parse(""PartitionerDefinedOrder""));
     }
+
+    @Test
+    public void testParsePartitionerOrderWithBaseType() throws ConfigurationException, SyntaxException
+    {
+        for (IPartitioner partitioner: new IPartitioner[] { Murmur3Partitioner.instance,
+                                                            ByteOrderedPartitioner.instance,
+                                                            RandomPartitioner.instance,
+                                                            OrderPreservingPartitioner.instance })
+        {
+            AbstractType<?> type = partitioner.partitionOrdering();
+            if (type instanceof PartitionerDefinedOrder)
+            {
+                PartitionerDefinedOrder tmp = (PartitionerDefinedOrder) type;
+                tmp.withBaseType(Int32Type.instance);","[{'comment': 'I believe this should be `type = ...`.', 'commenter': 'blambov'}, {'comment': 'yes, it is a bug I have write ,fix it now.', 'commenter': 'Maxwell-Guo'}, {'comment': 'yes', 'commenter': 'Maxwell-Guo'}]"
1998,test/unit/org/apache/cassandra/db/marshal/TypeParserTest.java,"@@ -98,4 +98,24 @@ public void testParsePartitionerOrder() throws ConfigurationException, SyntaxExc
         }
         assertSame(DatabaseDescriptor.getPartitioner().partitionOrdering(), TypeParser.parse(""PartitionerDefinedOrder""));
     }
+
+    @Test
+    public void testParsePartitionerOrderWithBaseType() throws ConfigurationException, SyntaxException
+    {
+        for (IPartitioner partitioner: new IPartitioner[] { Murmur3Partitioner.instance,
+                                                            ByteOrderedPartitioner.instance,
+                                                            RandomPartitioner.instance,
+                                                            OrderPreservingPartitioner.instance })
+        {
+            AbstractType<?> type = partitioner.partitionOrdering();
+            if (type instanceof PartitionerDefinedOrder)
+            {
+                PartitionerDefinedOrder tmp = (PartitionerDefinedOrder) type;
+                tmp.withBaseType(Int32Type.instance);
+            }
+            System.out.println(type.toString());
+            assertSame(type, TypeParser.parse(type.toString()));
+        }
+        assertSame(DatabaseDescriptor.getPartitioner().partitionOrdering(), TypeParser.parse(""PartitionerDefinedOrder""));
+    }","[{'comment': 'It would be helpful to test some explicit definitions to try all acceptable variations (default partitioner, partitioner without base type, partitioner with base type).', 'commenter': 'blambov'}, {'comment': '@blambov besides ,I have a question here ,why we use assertSame here ? for this patch if it is a PartitionerDefinedOrder  with a base type there will return a new PartitionerDefinedOrder object here and the memory address must be different . \r\nSo I want to know if the place in the codebase that invoked PartitionerDefinedOrder class must keep the memory address be the same ?', 'commenter': 'Maxwell-Guo'}, {'comment': ""Many implementations of `AbstractType` keep a map of already created types to ensure that they don't create the same object more than once. See e.g. `SetType`. It makes some sense to do the same for `PartitionerDefinedOrder` as well, but it is not something I personally insist on -- either change the tests to check equality, or add a static map of instances and consult it when constructing a type."", 'commenter': 'blambov'}]"
1998,src/java/org/apache/cassandra/db/marshal/PartitionerDefinedOrder.java,"@@ -107,6 +117,30 @@ public TypeSerializer<ByteBuffer> getSerializer()
     @Override
     public String toString()
     {
+        if(baseType != null)
+        {
+            return String.format(""%s(%s:%s)"", getClass().getName(), partitioner.getClass().getName(), baseType.toString());
+        }
         return String.format(""%s(%s)"", getClass().getName(), partitioner.getClass().getName());
     }
+    
+    public AbstractType<?>  getBaseType()
+    {
+        return baseType; 
+    }
+
+    @Override
+    public boolean equals(Object obj)
+    {
+        if (this == obj)
+        {
+            return true;
+        }
+        if (obj instanceof PartitionerDefinedOrder)
+        {
+            PartitionerDefinedOrder other = (PartitionerDefinedOrder) obj;
+            return this.baseType.equals(other.baseType) && this.partitioner.equals(other.partitioner);","[{'comment': 'Does this not throw when `baseType` is null?', 'commenter': 'blambov'}, {'comment': 'yes, null should be added', 'commenter': 'Maxwell-Guo'}]"
2000,src/java/org/apache/cassandra/cql3/statements/PropertyDefinitions.java,"@@ -84,17 +85,29 @@ public Boolean hasProperty(String name)
         return properties.containsKey(name);
     }
 
-    public String getString(String key, String defaultValue) throws SyntaxException
+    public static boolean parseBoolean(String key, String value) throws SyntaxException
     {
-        String value = getSimple(key);
-        return value != null ? value : defaultValue;
+        if (null == value)
+            throw new IllegalArgumentException(""value argument can't be null"");","[{'comment': ""Shouldn't the key name be referenced in the exception."", 'commenter': 'Claudenw'}, {'comment': 'Probably, but this goes just to 4.0 and 4.1, this preserves what was there as closely as possible, not important here. We may fix it in trunk if you are up to.', 'commenter': 'smiklosovic'}]"
2001,src/java/org/apache/cassandra/cql3/statements/PropertyDefinitions.java,"@@ -97,17 +98,29 @@ public Boolean hasProperty(String name)
         return properties.containsKey(name);
     }
 
-    public String getString(String key, String defaultValue) throws SyntaxException
+    public static boolean parseBoolean(String key, String value) throws SyntaxException
     {
-        String value = getSimple(key);
-        return value != null ? value : defaultValue;
+        if (null == value)
+            throw new IllegalArgumentException(""value argument can't be null"");","[{'comment': ""Shouldn't this exception mention the key name?"", 'commenter': 'Claudenw'}]"
2004,src/java/org/apache/cassandra/index/internal/CassandraIndexFunctions.java,"@@ -51,6 +59,8 @@ default AbstractType<?> getIndexedValueType(ColumnMetadata indexedColumn)
         return indexedColumn.type;
     }
 
+    public AbstractType<?> getIndexedPartitionKeyType(ColumnMetadata indexedColumn);","[{'comment': 'If this was defined as \r\n\r\npublic default Abstract<?> getIndexPartitionKeyType(ColumnMetadat indexedColumn) {\r\n    return indexedColumn.type;\r\n}\r\n\r\nMost of the overides provided below would not be needed.', 'commenter': 'Claudenw'}]"
2005,src/java/org/apache/cassandra/index/internal/composites/PartitionKeyIndex.java,"@@ -94,4 +94,5 @@ public boolean isStale(Row data, ByteBuffer indexValue, int nowInSec)
     {
         return !data.hasLiveData(nowInSec, enforceStrictLiveness);
     }
+","[{'comment': 'remove the blank line', 'commenter': 'Claudenw'}]"
2005,src/java/org/apache/cassandra/index/internal/composites/RegularColumnIndex.java,"@@ -76,7 +76,7 @@ public <T> CBuilder buildIndexClusteringPrefix(ByteBuffer partitionKey,
         // base table partition should be returned for any mathching index entry.
         return builder;
     }
-
+    ","[{'comment': 'remove the spaces from the blank line', 'commenter': 'Claudenw'}]"
2005,src/java/org/apache/cassandra/tools/SSTableExport.java,"@@ -71,8 +72,8 @@
 
     static
     {
-        DatabaseDescriptor.clientInitialization();
-
+        DatabaseDescriptor.clientInitialization(false);
+        ","[{'comment': 'remove the spaces', 'commenter': 'Claudenw'}]"
2005,src/java/org/apache/cassandra/tools/Util.java,"@@ -331,10 +334,18 @@ public static TableMetadata metadataFromSSTable(Descriptor desc) throws IOExcept
                     builder.addRegularColumn(ident, entry.getValue());
                 });
         builder.addPartitionKeyColumn(""PartitionKey"", header.getKeyType());
-        for (int i = 0; i < header.getClusteringTypes().size(); i++)
+        for (int i = 0; i < header.getClusteringTypes().size() ; i++)","[{'comment': 'remove the space before the semi-colon', 'commenter': 'Claudenw'}]"
2008,src/java/org/apache/cassandra/net/CustomParamsSerializer.java,"@@ -0,0 +1,72 @@
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+class CustomParamsSerializer implements IVersionedSerializer<Map<String,ByteBuffer>>
+{
+    public static final CustomParamsSerializer serializer = new CustomParamsSerializer();
+
+    @Override
+    public void serialize(Map<String, ByteBuffer> t, DataOutputPlus out, int version) throws IOException
+    {
+        out.writeInt(t.size());
+        for (Map.Entry<String, ByteBuffer> e : t.entrySet())
+        {
+            out.writeUTF(e.getKey());
+            ByteBufferUtil.writeWithLength(e.getValue(), out);
+        }
+    }
+
+    @Override
+    public Map<String, ByteBuffer> deserialize(DataInputPlus in, int version) throws IOException
+    {
+        int entries = in.readInt();
+        Map<String, ByteBuffer> customParams = new HashMap<>(entries);","[{'comment': 'You want to use `Maps.newHashMapWithExpectedSize()` here, to take load factor into account.', 'commenter': 'iamaleksey'}, {'comment': 'yup, good catch.', 'commenter': 'michaelsembwever'}]"
2008,src/java/org/apache/cassandra/net/CustomParamsSerializer.java,"@@ -0,0 +1,72 @@
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+class CustomParamsSerializer implements IVersionedSerializer<Map<String,ByteBuffer>>
+{
+    public static final CustomParamsSerializer serializer = new CustomParamsSerializer();
+
+    @Override
+    public void serialize(Map<String, ByteBuffer> t, DataOutputPlus out, int version) throws IOException
+    {
+        out.writeInt(t.size());
+        for (Map.Entry<String, ByteBuffer> e : t.entrySet())
+        {
+            out.writeUTF(e.getKey());
+            ByteBufferUtil.writeWithLength(e.getValue(), out);
+        }
+    }
+
+    @Override
+    public Map<String, ByteBuffer> deserialize(DataInputPlus in, int version) throws IOException
+    {
+        int entries = in.readInt();
+        Map<String, ByteBuffer> customParams = new HashMap<>(entries);
+        for (int i = 0 ; i < entries ; ++i)
+        {
+            String name = in.readUTF();
+            ByteBuffer bb = ByteBufferUtil.readWithLength(in);
+            customParams.put(name, bb);
+        }
+        return customParams;
+    }
+
+    @Override
+    public long serializedSize(Map<String, ByteBuffer> t, int version)
+    {
+        return TypeSizes.INT_SIZE
+                + t.keySet().stream().collect(Collectors.summingInt((k) -> TypeSizes.sizeof(k)))
+                + t.values().stream().collect(Collectors.summingInt((v) -> TypeSizes.sizeofWithLength(v)));","[{'comment': 'No need for two streams here. One regular for-loop over the entry set will do better and read no less clearly, *plus* mimic `serialize()` more closely, which is a good practice for the method couple that needs to be kept in-sync.', 'commenter': 'iamaleksey'}, {'comment': 'too obvious. thanks!', 'commenter': 'michaelsembwever'}]"
2008,src/java/org/apache/cassandra/net/CustomParamsSerializer.java,"@@ -0,0 +1,72 @@
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+class CustomParamsSerializer implements IVersionedSerializer<Map<String,ByteBuffer>>","[{'comment': 'Would *probably* consider something less heavy than `ByteBuffer` for values.', 'commenter': 'iamaleksey'}, {'comment': ""It looks to me like in the case of jaeger-tracing it's a String to String map, under the hood? Could use that, or byte[]."", 'commenter': 'iamaleksey'}, {'comment': ""Zipkin uses byte[]. This isn't meant to be tracing specific either. \r\n\r\n"", 'commenter': 'michaelsembwever'}, {'comment': 'changed to `byte[]`', 'commenter': 'michaelsembwever'}]"
2008,src/java/org/apache/cassandra/net/CustomParamsSerializer.java,"@@ -0,0 +1,77 @@
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+import com.google.common.collect.Maps;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+class CustomParamsSerializer implements IVersionedSerializer<Map<String,byte[]>>
+{
+    public static final CustomParamsSerializer serializer = new CustomParamsSerializer();
+
+    @Override
+    public void serialize(Map<String, byte[]> t, DataOutputPlus out, int version) throws IOException
+    {
+        out.writeInt(t.size());
+        for (Map.Entry<String, byte[]> e : t.entrySet())
+        {
+            out.writeUTF(e.getKey());
+            out.writeInt(e.getValue().length);
+            out.write(e.getValue());
+        }
+    }
+
+    @Override
+    public Map<String, byte[]> deserialize(DataInputPlus in, int version) throws IOException
+    {
+        int entries = in.readInt();
+        Map<String, byte[]> customParams = Maps.newHashMapWithExpectedSize(entries);
+        for (int i = 0 ; i < entries ; ++i)
+        {
+            String name = in.readUTF();
+            int len = in.readInt();
+            byte[] bytes = new byte[len];
+            in.readFully(bytes);
+            customParams.put(name, bytes);
+        }
+        return customParams;
+    }
+
+    @Override
+    public long serializedSize(Map<String, byte[]> t, int version)
+    {
+        return TypeSizes.INT_SIZE
+                + t.entrySet().stream().collect(Collectors.summingInt(","[{'comment': 'FWIW I meant not using streams at all here. No need to create avoidable garbage here. What we tend to do in absolute most of our serializers (open any) is to have `serialize()` and `serializedSize()` look almost the same, visually, except in the latter you just sum up the sizes instead of writing out.', 'commenter': 'iamaleksey'}]"
2008,src/java/org/apache/cassandra/net/CustomParamsSerializer.java,"@@ -0,0 +1,80 @@
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+import com.google.common.collect.Maps;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+class CustomParamsSerializer implements IVersionedSerializer<Map<String,byte[]>>
+{
+    public static final CustomParamsSerializer serializer = new CustomParamsSerializer();
+
+    @Override
+    public void serialize(Map<String, byte[]> t, DataOutputPlus out, int version) throws IOException
+    {
+        out.writeInt(t.size());
+        for (Map.Entry<String, byte[]> e : t.entrySet())
+        {
+            out.writeUTF(e.getKey());
+            out.writeInt(e.getValue().length);","[{'comment': 'I would use a varint here, as four bytes per value is going to be mostly wasteful in this context.', 'commenter': 'iamaleksey'}, {'comment': 'Would throw the helper methods `writeWithVIntLength()`, `readWithVIntLength()`, and `sizeWithVIntLength()` into `ByteArrayUtil`, next to the existing ones for short/int. We have the equivalents for byte buffers in `ByteBufferUtil`, might as well add them for byte arrays.', 'commenter': 'iamaleksey'}]"
2008,src/java/org/apache/cassandra/net/Message.java,"@@ -473,6 +482,13 @@ public Builder<T> withParam(ParamType type, Object value)
             return this;
         }
 
+        public Builder<T> withCustomParam(String name, byte[] value)","[{'comment': 'May want to provide a method that takes a prebuilt map in its entirety for those users of the API who would prefer that to doing some unnecessary look-ups per param.', 'commenter': 'iamaleksey'}, {'comment': ""At the moment there's not a clear use case for this method either.\r\n\r\nTracing implementations will override `Tracing. addTraceHeaders(..)` which `Message.buildParams(..)` calls.\r\ne.g. https://github.com/thelastpickle/cassandra-zipkin-tracing/blob/ffb9afb9bc876ac212d51305ac7594843d661d2a/src/main/java/com/thelastpickle/cassandra/tracing/ZipkinTracing.java#L208-L210"", 'commenter': 'michaelsembwever'}]"
2008,src/java/org/apache/cassandra/net/Message.java,"@@ -473,6 +482,13 @@ public Builder<T> withParam(ParamType type, Object value)
             return this;
         }
 
+        public Builder<T> withCustomParam(String name, byte[] value)
+        {
+            params.putIfAbsent(ParamType.CUSTOM_MAP, new HashMap<String,byte[]>());
+            ((Map<String,byte[]>) params.get(ParamType.CUSTOM_MAP)).put(name, value);","[{'comment': 'On that note, you are wasting work here by doing the lookup twice - once for `putIfAbsent()`, then again for `get()`.\r\n\r\nNot only that, but you are also allocating an entire `HashMap` every time, even if it already exists in `params` map.\r\n\r\nCould either use `computeIfAbsent()` and hold on to the return value, or write this as a manual `get()` + `put()` if null.', 'commenter': 'iamaleksey'}]"
2008,src/java/org/apache/cassandra/net/CustomParamsSerializer.java,"@@ -0,0 +1,76 @@
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+import com.google.common.collect.Maps;
+
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+class CustomParamsSerializer implements IVersionedSerializer<Map<String,byte[]>>
+{
+    public static final CustomParamsSerializer serializer = new CustomParamsSerializer();
+
+    @Override
+    public void serialize(Map<String, byte[]> t, DataOutputPlus out, int version) throws IOException
+    {
+        out.writeInt(t.size());","[{'comment': 'Can/should also be a vint (unsigned).', 'commenter': 'iamaleksey'}]"
2008,src/java/org/apache/cassandra/utils/ByteArrayUtil.java,"@@ -230,6 +232,17 @@ public static void writeWithShortLength(byte[] buffer, DataOutput out) throws IO
         out.write(buffer);
     }
 
+    public static void writeWithVIntLength(byte[] bytes, DataOutputPlus out) throws IOException
+    {
+        out.writeVInt(bytes.length);","[{'comment': '`writeUnsignedVInt()`', 'commenter': 'iamaleksey'}]"
2008,src/java/org/apache/cassandra/utils/ByteArrayUtil.java,"@@ -230,6 +232,17 @@ public static void writeWithShortLength(byte[] buffer, DataOutput out) throws IO
         out.write(buffer);
     }
 
+    public static void writeWithVIntLength(byte[] bytes, DataOutputPlus out) throws IOException
+    {
+        out.writeVInt(bytes.length);
+        out.write(bytes);
+    }
+
+    public static int serializedSizeWithVIntLength(byte[] bytes)
+    {
+        return TypeSizes.sizeofVInt(bytes.length) + bytes.length;","[{'comment': '`sizeofUnsignedVInt()`', 'commenter': 'iamaleksey'}]"
2026,src/java/org/apache/cassandra/service/accord/AccordCommand.java,"@@ -802,10 +803,21 @@ public void removeWaitingOn(TxnId txnId, Timestamp executeAt)
     }
 
     @Override
-    public TxnId firstWaitingOnApply()
+    public boolean isWaitingOnDependency()","[{'comment': ""These two changes are cut and pasting from Accord. Do they need to be duplicated? They reference different maps, so maybe that's not reasonable, but the less duplication the better."", 'commenter': 'aweisberg'}, {'comment': ""I'm not settled on the final API for this, but I'm also not sure we should force implementations to use separate maps - that's a detail that just happens to be shared between implementations today. It might be that a single array of TxnId is used with the status being stored in an adjacent array. I'd prefer to have value types and be able to ask for the earliest dependency of either kind with both its Timestamp and TxnId. It might be, as we improve efficiency C* side, we can improve this API further.\r\n\r\nWhich is all to say I don't mind all that much either way if you feel strongly about this at present."", 'commenter': 'belliottsmith'}]"
2026,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -325,6 +325,12 @@ public Timestamp preaccept(TxnId txnId, Keys keys)
         return time.uniqueNow(max);
     }
 
+    @Override
+    public Timestamp uniqueNow(TxnId greaterThan)
+    {
+        return time.uniqueNow(greaterThan);","[{'comment': 'If node time service already factors this out why does it need to be in both the command store implementations?', 'commenter': 'aweisberg'}, {'comment': ""Fair. I'll expose the time service instead."", 'commenter': 'belliottsmith'}]"
2028,doc/modules/cassandra/pages/operating/denylisting_partitions.adoc,"@@ -0,0 +1,141 @@
+= Denylisting Partitions
+
+Due to access patterns and data modeling, sometimes there are specific partitions
+that are ""hot"" and can cause instability in a Cassandra cluster. This often occurs
+when your data model includes many update or insert operations on a single partition,
+causing the partition to grow very large over time and in turn making it very expensive
+to read and maintain.
+
+Cassandra supports ""denylisting"" these problematic partitions so that when clients
+issue point reads (`SELECT` statements with the partition key specified) or range
+reads (`SELECT *`, etc that pull a range of data) that intersect with a blocked
+partition key, the query will be immediately rejected with an `InvalidQueryException`.
+
+== How to denylist a partition key
+
+The ``system_distributed.denylisted_partitions`` table can be used to denylist partitions.
+There are a couple of ways to interact with and mutate this data. First: directly
+via CQL by inserting a record with the following details:
+
+- Keyspace name (ks_name)
+- Table name (table_name)
+- Partition Key (partition_key)
+
+The partition key format needs to be in the same form required by ``nodetool getendpoints``.
+
+Following are several examples for denylisting partition keys in keyspace `ks` and
+table `table1` for different data types on the primary key `Id`:
+
+ - Id is a simple type - INSERT INTO system_distributed.denylisted_partitions (ks_name, table_name, partition_key) VALUES ('ks','table1','1');
+ - Id is a blob        - INSERT INTO system_distributed.denylisted_partitions (ks_name, table_name, partition_key) VALUES ('ks','table1','12345f');
+ - Id has a colon      - INSERT INTO system_distributed.denylisted_partitions (ks_name, table_name, partition_key) VALUES ('ks','table1','1\:2');
+
+In the case of composite column partition keys (Key1, Key2):
+
+ - INSERT INTO system_distributed.denylisted_partitions (ks_name, table_name, partition_key) VALUES ('ks', 'table1', 'k11:k21')
+
+
+ === Special considerations","[{'comment': 'The leading whitespace on this section is leading to things being formatted incorrectly (at least in intellij preview; assuming that will hold true for generated pages?)', 'commenter': 'jmckenzie-dev'}]"
2047,src/java/org/apache/cassandra/tools/nodetool/stats/CompactionHistoryPrinter.java,"@@ -29,6 +29,8 @@
 
 public class CompactionHistoryPrinter
 {
+    public static final String UNKNOW_TYPE = ""UNKNOW"";","[{'comment': '`UNKNOWN_TYPE = ""UNKNOWN"";` - Missing `N` at the end of `UNKNOW`.\r\n\r\nThis constant should be moved to `CompactionHistoryTabularData`', 'commenter': 'smiklosovic'}, {'comment': 'ok', 'commenter': 'Maxwell-Guo'}]"
2047,src/java/org/apache/cassandra/cql3/UntypedResultSet.java,"@@ -351,7 +351,11 @@ public ByteBuffer getBlob(String column)
 
         public String getString(String column)
         {
-            return UTF8Type.instance.compose(data.get(column));","[{'comment': 'this change is not necessary, leave that method as it was before', 'commenter': 'smiklosovic'}, {'comment': 'I modify all other comment, but for this ,I should left it unchangede. When we upgrade from previous version that have not got the new added compaction_type column , the data.get(column) will return null ,and so a npe will appears, we can see the getInt and getUUID method below ,they have consider the case of null value.\r\n\r\n\r\nFirstly , I considered changing all the methods of the corresponding type of get method  to consider the case where value is null, but I gave up later. I thought it would be more appropriate to open a new jira.', 'commenter': 'Maxwell-Guo'}, {'comment': 'besides I have also tried upgrade my cluster from a previous version from this version for Cassandra-18062\r\nif  leave that method as it was before a npe appears,', 'commenter': 'Maxwell-Guo'}, {'comment': '@Maxwell-Guo You can use `public boolean has(String column)` in that class to check if it is null and if it is not then you call `getString` otherwise you return null. It is not necessary to change this method.\r\n\r\nIt is ok to open new ticket to investigate if we should not solve this in more robust manner but that is not objective of this ticket.', 'commenter': 'smiklosovic'}]"
2047,src/java/org/apache/cassandra/db/compaction/CompactionHistoryTabularData.java,"@@ -74,10 +74,11 @@ public static TabularData from(UntypedResultSet resultSet) throws OpenDataExcept
             long bytesIn = row.getLong(ITEM_NAMES[4]);
             long bytesOut = row.getLong(ITEM_NAMES[5]);
             Map<Integer, Long> rowMerged = row.getMap(ITEM_NAMES[6], Int32Type.instance, LongType.instance);
-
+            String compactionType = row.getString(ITEM_NAMES[7]);","[{'comment': 'if `row.getString` returns `null` here, `compactionType` should be set to `UNKNOWN` here.', 'commenter': 'smiklosovic'}, {'comment': 'ok', 'commenter': 'Maxwell-Guo'}]"
2047,src/java/org/apache/cassandra/tools/nodetool/stats/CompactionHistoryPrinter.java,"@@ -65,14 +67,15 @@ public void print(CompactionHistoryHolder data, PrintStream out)
             for (Object chr : compactionHistories)
             {
                 Map value = chr instanceof Map<?, ?> ? (Map)chr : Collections.emptyMap();
-                String[] obj = new String[7];
+                String[] obj = new String[8];
                 obj[0] = (String)value.get(""id"");
                 obj[1] = (String)value.get(""keyspace_name"");
                 obj[2] = (String)value.get(""columnfamily_name"");
                 obj[3] = (String)value.get(""compacted_at"");
                 obj[4] = value.get(""bytes_in"").toString();
                 obj[5] = value.get(""bytes_out"").toString();
                 obj[6] = (String)value.get(""rows_merged"");
+                obj[7] = value.get(""compaction_type"") == null ?  UNKNOW_TYPE : (String)value.get(""compaction_type"");","[{'comment': 'this should be `obj[7] = value.get(""compaction_type"");` because it already contains type or `UNKNOWN`.', 'commenter': 'smiklosovic'}, {'comment': '+1', 'commenter': 'Maxwell-Guo'}]"
2047,test/distributed/org/apache/cassandra/distributed/upgrade/UpgradeTestBase.java,"@@ -90,7 +90,7 @@ public UpgradeableCluster.Builder builder()
     public static final Semver v30 = new Semver(""3.0.0-alpha1"", SemverType.LOOSE);
     public static final Semver v3X = new Semver(""3.11.0"", SemverType.LOOSE);
     public static final Semver v40 = new Semver(""4.0-alpha1"", SemverType.LOOSE);
-    public static final Semver v41 = new Semver(""4.1-alpha1"", SemverType.LOOSE);
+    public static final Semver v41 = new Semver(""4.1"", SemverType.LOOSE);","[{'comment': '@Maxwell-Guo why have you changed this?', 'commenter': 'smiklosovic'}, {'comment': '@smiklosovic  Thank you for you review .There was something urgent yesterday at that time , so I seems that I have forgot to check this place when submitting.\r\nI have changed it back.', 'commenter': 'Maxwell-Guo'}]"
2047,test/distributed/org/apache/cassandra/distributed/upgrade/CompactionHistorySystemTableUpgradeTest.java,"@@ -39,26 +39,22 @@ public void compactionHistorySystemTableTest() throws Throwable
             .nodes(2)
             .nodesToUpgrade(2)
             .upgradesToCurrentFrom(v40).setup((cluster) -> {
-              //create table    
+              //create table
             cluster.schemaChange(""CREATE TABLE "" + KEYSPACE + "".tb ("" +
                 ""pk text PRIMARY KEY,"" +
                 ""c1 text,"" +
                 ""c2 int,"" +
                 ""c3 int)"");
-          
              // disable auto compaction
             cluster.stream().forEach(node -> node.nodetool(""disableautocompaction""));
-          
             // generate sstables
-            for (int i = 0; i != 10; ++ i) 
+            for (int i = 0; i != 10; ++ i)
             {
                 cluster.coordinator(1).execute(""INSERT INTO "" + KEYSPACE + "".tb (pk, c1, c2, c3) VALUES ('pk"" + i + ""', 'c1"" + i + ""', "" + i + ',' + i + ')'  , ConsistencyLevel.ALL);
                 cluster.stream().forEach(node -> node.flush(KEYSPACE));
             }
-          
             // force compact
             cluster.stream().forEach(node -> node.forceCompact(KEYSPACE, ""tb""));
-          
         }).runAfterNodeUpgrade((cluster, node) -> {
           String query = ""SELECT compaction_type FROM "" + KEYSPACE + "".tb where keyspace_name = '"" + KEYSPACE + ""' AND columnfamily_name = 'tb' ALLOW FILTERING"";","[{'comment': '@Maxwell-Guo I do not think this is right. You are selecting from keyspace `KEYSPACE` and table `tb`. Should not that be `system.compaction_history`? Have you tried to execute this test?', 'commenter': 'smiklosovic'}]"
2047,test/distributed/org/apache/cassandra/distributed/upgrade/CompactionHistorySystemTableUpgradeTest.java,"@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.upgrade;
+
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.tools.ToolRunner;
+import org.junit.Test;
+
+import java.util.Arrays;
+
+import static org.apache.cassandra.db.compaction.CompactionHistoryTabularData.UNKNOWN_TYPE;
+import static org.apache.cassandra.distributed.shared.AssertUtils.assertRows;
+import static org.apache.cassandra.distributed.shared.AssertUtils.row;
+import static org.apache.cassandra.tools.ToolRunner.invokeNodetool;
+import static org.junit.Assert.assertTrue;
+
+public class CompactionHistorySystemTableUpgradeTest extends UpgradeTestBase
+{
+    @Test
+    public void compactionHistorySystemTableTest() throws Throwable
+    {
+        new TestCase()
+            .nodes(2)
+            .nodesToUpgrade(2)
+            .upgradesToCurrentFrom(v40).setup((cluster) -> {
+              //create table
+            cluster.schemaChange(""CREATE TABLE "" + KEYSPACE + "".tb ("" +
+                ""pk text PRIMARY KEY,"" +
+                ""c1 text,"" +
+                ""c2 int,"" +
+                ""c3 int)"");
+             // disable auto compaction
+            cluster.stream().forEach(node -> node.nodetool(""disableautocompaction""));
+            // generate sstables
+            for (int i = 0; i != 10; ++ i)
+            {
+                cluster.coordinator(1).execute(""INSERT INTO "" + KEYSPACE + "".tb (pk, c1, c2, c3) VALUES ('pk"" + i + ""', 'c1"" + i + ""', "" + i + ',' + i + ')'  , ConsistencyLevel.ALL);
+                cluster.stream().forEach(node -> node.flush(KEYSPACE));
+            }
+            // force compact
+            cluster.stream().forEach(node -> node.forceCompact(KEYSPACE, ""tb""));
+        }).runAfterNodeUpgrade((cluster, node) -> {
+          String query = ""SELECT compaction_type FROM system.compaction_history where keyspace_name = '"" + KEYSPACE + ""' AND columnfamily_name = 'tb' ALLOW FILTERING"";
+          Object[][] expectedResult = {
+              row(null)
+          };
+          assertRows(cluster.coordinator(1).execute(query, ConsistencyLevel.ALL), expectedResult);
+          assertRows(cluster.coordinator(2).execute(query, ConsistencyLevel.ALL), expectedResult);
+
+          ToolRunner.ToolResult toolHistory = invokeNodetool(""compactionhistory"");
+          toolHistory.assertOnCleanExit();
+          String stdout = toolHistory.getStdout();
+          String [] resultArray = stdout.split(""\n"");
+          assertTrue(Arrays.stream(resultArray)
+              .anyMatch(result -> result.contains(UNKNOWN_TYPE)","[{'comment': '@Maxwell-Guo why do we expect UNKNOWN_TYPE here? Should not that be some concrete type?', 'commenter': 'smiklosovic'}, {'comment': 'aha I see, we are testing the output from times it was not there yet.', 'commenter': 'smiklosovic'}]"
2047,src/java/org/apache/cassandra/db/compaction/CompactionHistoryTabularData.java,"@@ -74,10 +74,14 @@ public static TabularData from(UntypedResultSet resultSet) throws OpenDataExcept
             long bytesIn = row.getLong(ITEM_NAMES[4]);
             long bytesOut = row.getLong(ITEM_NAMES[5]);
             Map<Integer, Long> rowMerged = row.getMap(ITEM_NAMES[6], Int32Type.instance, LongType.instance);
-
+            String compactionType = OperationType.UNKNOWN.type;
+            if (row.has(ITEM_NAMES[7]))","[{'comment': 'it does not seem like this check is really needed since the system table migrator should add that column before we ever execute the code here', 'commenter': 'jacek-lewandowski'}, {'comment': 'Thanks @jacek-lewandowski \r\nYes, if a migration for this system table  is added , This method is no longer need. ', 'commenter': 'Maxwell-Guo'}, {'comment': 'I think it should be, otherwise when this column is added?', 'commenter': 'jacek-lewandowski'}]"
2047,test/distributed/org/apache/cassandra/distributed/upgrade/CompactionHistorySystemTableUpgradeTest.java,"@@ -0,0 +1,92 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.upgrade;
+
+import com.google.common.collect.Lists;
+import com.vdurmont.semver4j.Semver;
+import org.apache.cassandra.db.compaction.OperationType;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.tools.ToolRunner;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+
+import static org.apache.cassandra.distributed.shared.AssertUtils.assertRows;
+import static org.apache.cassandra.distributed.shared.AssertUtils.row;
+import static org.apache.cassandra.tools.ToolRunner.invokeNodetoolJvmDtest;
+import static org.junit.Assert.assertTrue;
+
+@RunWith(Parameterized.class)
+public class CompactionHistorySystemTableUpgradeTest extends UpgradeTestBase","[{'comment': ""I'm not sure, but such things are tested in system migrator test"", 'commenter': 'jacek-lewandowski'}, {'comment': 'If a migrate for compation_history is added in SystemKeyspaceMigrator , this method should be changed. \r\nI just want to doing some test on the result of upgrate ,So I think we can keep this test . right ? ', 'commenter': 'Maxwell-Guo'}, {'comment': 'Depending on whether it provides some value of the test in migrator. If it provides some better coverage, then ok. But in general upgrade tests are quite heavy and if we can verify the same in a pretty simple unit/simple CQLTester based integration test I would go that way', 'commenter': 'jacek-lewandowski'}, {'comment': 'Hi @jacek-lewandowski , I keep this test case  with the test of nodetool compactionhistory. I think the test case can not be covered by SystemKeyspaceMigratorTest', 'commenter': 'Maxwell-Guo'}]"
2047,src/java/org/apache/cassandra/db/compaction/CompactionHistoryProperty.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.compaction;
+
+import com.google.common.collect.Maps;
+
+import java.util.Arrays;
+import java.util.Map;
+import java.util.TreeMap;
+
+/**
+ * properties that for system.compaction_history, and more properties can be add in the future
+ * */
+public class CompactionHistoryProperty
+{
+    // define the map key for the compaction_properties column in system.compaction_history
+    public static final String[] COMPACTION_PROPERTIES_KEYS = { ""compaction_type"" };
+    // define the map value for the compaction_properties column in system.compaction_history
+    public static final String[] DEFAULT_COMPACTION_PROPERTIES_VALUES = { OperationType.UNKNOWN.type };
+    
+    /**
+     * get the compaction history properties , if none value exists  a default value will return
+     * @param propertiesColumnValueInput input properties map
+     * @return the properties
+     * */
+    public static Map<String, String> getCompactionHistroyProperties(Map<String, String> propertiesColumnValueInput)","[{'comment': 'This method seems to be too complex to what it really does. What do you think about simply declaring an immutable map with defaults, and then in the method pre-populate the map you want to return with those defaults? ', 'commenter': 'jacek-lewandowski'}, {'comment': '+1 on declaring an immutable map with defaults, I can change the code\r\n\r\nI think there may be two case :\r\n1. when upgrate from lower version that do not contains the compaction_properties column , then all the default value can just return.\r\n\r\n2.If some new properties was added in the compaction_properties column, such as for 4.2 we have compaction_type , but we may add new property for example compaction_time or resouce cost at 5.0 , then compaction_type is already exits but ""compaction_time or resouce cost"" may use a default value. \r\n', 'commenter': 'Maxwell-Guo'}]"
2047,src/java/org/apache/cassandra/db/compaction/CompactionHistoryProperty.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.compaction;
+
+import com.google.common.collect.Maps;
+
+import java.util.Arrays;
+import java.util.Map;
+import java.util.TreeMap;
+
+/**
+ * properties that for system.compaction_history, and more properties can be add in the future
+ * */
+public class CompactionHistoryProperty
+{
+    // define the map key for the compaction_properties column in system.compaction_history
+    public static final String[] COMPACTION_PROPERTIES_KEYS = { ""compaction_type"" };","[{'comment': 'would be good to have constants for certain property names', 'commenter': 'jacek-lewandowski'}, {'comment': '+1', 'commenter': 'Maxwell-Guo'}]"
2047,src/java/org/apache/cassandra/db/compaction/CompactionHistoryTabularData.java,"@@ -24,16 +24,17 @@
 import org.apache.cassandra.cql3.UntypedResultSet;
 import org.apache.cassandra.db.marshal.Int32Type;
 import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.db.marshal.UTF8Type;
 import org.apache.cassandra.utils.FBUtilities;
 
 public class CompactionHistoryTabularData
 {
     private static final String[] ITEM_NAMES = new String[]{ ""id"", ""keyspace_name"", ""columnfamily_name"", ""compacted_at"",
-                                                             ""bytes_in"", ""bytes_out"", ""rows_merged"", ""compaction_type"" };
+                                                             ""bytes_in"", ""bytes_out"", ""rows_merged"", ""compaction_properties"" };","[{'comment': 'what do you think about including all the keys defined in CompactionHstoryProperty?', 'commenter': 'jacek-lewandowski'}, {'comment': ""1. I just put all the properties together in a map as you said. The reason why I do not including all keys defined in CompactionHstoryProperty is that the keys may be so much , and for nodetool compactionhistory 's output it may be not easy to see. \r\n2. Besides, in the future if some one add new properties , he may mdofiy more than one place to suceesfully add a property, define the key and default value in CompactionHistoryProperty, and add property key here for ITEM_NAMES and ITEM_DESCS, and the ITEM_TYPES should be added with new property type.\r\nAnd the out put result should also be modified to get the value from the map. \r\nBut if use this compaction_properties , none of the modification described in part 2 is needed. And the out put result is just like rows_merged column ,which is also a map type , and the output is also looks better\r\n"", 'commenter': 'Maxwell-Guo'}]"
2047,src/java/org/apache/cassandra/db/compaction/CompactionTask.java,"@@ -244,7 +247,8 @@ public boolean apply(SSTableReader sstable)
             for (int i = 0; i < mergedRowCounts.length; i++)
                 totalSourceRows += mergedRowCounts[i] * (i + 1);
 
-            String mergeSummary = updateCompactionHistory(cfs.keyspace.getName(), cfs.getTableName(), mergedRowCounts, startsize, endsize, compactionType.type);
+            String mergeSummary = updateCompactionHistory(cfs.keyspace.getName(), cfs.getTableName(), mergedRowCounts, startsize, endsize,
+                ImmutableMap.of(CompactionHistoryProperty.COMPACTION_PROPERTIES_KEYS[0], compactionType.type));","[{'comment': 'This is a good place to use a constant for compaction type property name', 'commenter': 'jacek-lewandowski'}, {'comment': '+1', 'commenter': 'Maxwell-Guo'}]"
2056,build.xml,"@@ -1063,7 +1063,7 @@
         <jvmarg value=""-ea""/>
         <jvmarg value=""-Djava.io.tmpdir=${tmp.dir}""/>
         <jvmarg value=""-Dcassandra.debugrefcount=true""/>
-        <jvmarg value=""-Xss256k""/>
+        <jvmarg value=""-Xss384k""/>","[{'comment': 'this is a weaker version of \r\n\r\n```\r\ncommit 4941b279bceb3a0fbe388be4305d25ec771f7b78\r\nAuthor: David Capwell <dcapwell@apache.org>\r\nDate:   Tue Dec 13 13:37:40 2022 -0800\r\n\r\n    CEP-10: Simulator Java11 Support\r\n\r\n    patch by David Capwell; reviewed by Benedict Elliott Smith, Ekaterina Dimitrova for CASSANDRA-17178\r\n```\r\n\r\nI needed as I test on M1 Mac', 'commenter': 'dcapwell'}, {'comment': 'I can remove, or we can fix-up when we rebase against trunk', 'commenter': 'dcapwell'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/ActionList.java,"@@ -41,6 +41,13 @@ public class ActionList extends AbstractCollection<Action>
     public static ActionList empty() { return EMPTY; }
     public static ActionList of(Action action) { return new ActionList(new Action[] { action }); }
     public static ActionList of(Stream<Action> action) { return new ActionList(action.toArray(Action[]::new)); }
+    public static ActionList of(Stream<Action> action, Stream<Action>... actions)
+    {
+        Stream<Action> accm = action;
+        for (Stream<Action> a : actions)","[{'comment': 'probably cleaner (and more efficient) to use `Stream.of(actions).flatMap(a->a)`', 'commenter': 'belliottsmith'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/AbstractPairOfSequencesPaxosSimulation.java,"@@ -227,7 +149,7 @@ public Action get()
                     {
                         if (simulated.time.nanoTime() >= untilNanos)
                         {
-                            available.add(next);
+                            IntStream.of(partitions).mapToObj(Integer::valueOf).forEach(available::add);","[{'comment': '`.boxed()?`', 'commenter': 'belliottsmith'}]"
2056,test/distributed/org/apache/cassandra/distributed/api/Row.java,"@@ -170,26 +282,75 @@ public UUID getUUID(String name)
         return (UUID) uuid;
     }
 
+    public UUID getUUID(String name, UUID defaultValue)
+    {
+        Object uuid = get(name, defaultValue);
+        if (uuid instanceof TimeUUID)
+            return ((TimeUUID) uuid).asUUID();
+        return (UUID) uuid;
+    }
+
     public Date getTimestamp(int index)
     {
         return get(index);
     }
 
+    public Date getTimestamp(int index, Date defaultValue)
+    {
+        return get(index, defaultValue);
+    }
+
     public Date getTimestamp(String name)
     {
         return get(name);
     }
 
+    public Date getTimestamp(String name, Date defaultValue)
+    {
+        return get(name, defaultValue);
+    }
+
     public <T> Set<T> getSet(int index)
     {
         return get(index);
     }
 
+    public <T> Set<T> getSet(int index, Set<T> defaultValue)
+    {
+        return get(index, defaultValue);
+    }
+
     public <T> Set<T> getSet(String name)
     {
         return get(name);
     }
 
+    public <T> Set<T> getSet(String name, Set<T> defaultValue)
+    {
+        return get(name, defaultValue);
+    }
+
+    // HERE","[{'comment': '?', 'commenter': 'belliottsmith'}, {'comment': ""I honestly don't remember; removed"", 'commenter': 'dcapwell'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/AbstractPairOfSequencesPaxosSimulation.java,"@@ -124,101 +122,25 @@ public ActionPlan plan()
 
         plan = plan.encapsulate(ActionPlan.setUpTearDown(
             ActionList.of(
-                cluster.stream().map(i -> simulated.run(""Insert Partitions"", i, executeForPrimaryKeys(preInsertStmt(), primaryKeys)))
-            ).andThen(
-                // TODO (now): this is temporary until we have correct epoch handling
-                ActionList.of(
-                    cluster.stream().map(i -> simulated.run(""Create Accord Epoch"", i, () -> AccordService.instance().createEpochFromConfigUnsafe()))
-                )
-//            ).andThen(
-//                // TODO (now): this is temporary until we have parameterisation of simulation
-//                ActionList.of(
-//                    cluster.stream().map(i -> simulated.run(""Disable Accord Cache"", i, () -> AccordService.instance.setCacheSize(0)))
-//                )
+                cluster.stream().map(i -> simulated.run(""Insert Partitions"", i, executeForPrimaryKeys(preInsertStmt(), primaryKeys))),
+                cluster.stream().map(i -> simulated.run(""Create Accord Epoch"", i, () -> AccordService.instance().createEpochFromConfigUnsafe()))","[{'comment': 'The TODO is still valid?', 'commenter': 'belliottsmith'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/AbstractPairOfSequencesPaxosSimulation.java,"@@ -124,101 +122,25 @@ public ActionPlan plan()
 
         plan = plan.encapsulate(ActionPlan.setUpTearDown(
             ActionList.of(
-                cluster.stream().map(i -> simulated.run(""Insert Partitions"", i, executeForPrimaryKeys(preInsertStmt(), primaryKeys)))
-            ).andThen(
-                // TODO (now): this is temporary until we have correct epoch handling
-                ActionList.of(
-                    cluster.stream().map(i -> simulated.run(""Create Accord Epoch"", i, () -> AccordService.instance().createEpochFromConfigUnsafe()))
-                )
-//            ).andThen(
-//                // TODO (now): this is temporary until we have parameterisation of simulation
-//                ActionList.of(
-//                    cluster.stream().map(i -> simulated.run(""Disable Accord Cache"", i, () -> AccordService.instance.setCacheSize(0)))
-//                )
+                cluster.stream().map(i -> simulated.run(""Insert Partitions"", i, executeForPrimaryKeys(preInsertStmt(), primaryKeys))),
+                cluster.stream().map(i -> simulated.run(""Create Accord Epoch"", i, () -> AccordService.instance().createEpochFromConfigUnsafe()))
             ),
             ActionList.of(
                 cluster.stream().map(i -> SimulatedActionTask.unsafeTask(""Shutdown "" + i.broadcastAddress(), RELIABLE, RELIABLE_NO_TIMEOUTS, simulated, i, i::shutdown))
             )
         ));
 
-        final int nodes = cluster.size();
-        for (int primaryKey : primaryKeys)
-            historyCheckers.add(new HistoryChecker(primaryKey));
-
-        List<Supplier<Action>> primaryKeyActions = new ArrayList<>();
-        for (int pki = 0 ; pki < primaryKeys.length ; ++pki)
-        {
-            int primaryKey = primaryKeys[pki];
-            HistoryChecker historyChecker = historyCheckers.get(pki);
-            Supplier<Action> supplier = new Supplier<Action>()
-            {
-                int i = 0;
-
-                @Override
-                public Action get()
-                {
-                    int node = simulated.random.uniform(1, nodes + 1);
-                    IInvokableInstance instance = cluster.get(node);
-                    switch (serialConsistency)
-                    {
-                        default: throw new AssertionError();
-                        case LOCAL_SERIAL:
-                            if (simulated.snitch.dcOf(node) > 0)
-                            {
-                                // perform some queries against these nodes but don't expect them to be linearizable
-                                return nonVerifying(i++, instance, primaryKey, historyChecker);
-                            }
-                        case SERIAL:
-                            return simulated.random.decide(readRatio)
-                                   ? verifying(i++, instance, primaryKey, historyChecker)
-                                   : modifying(i++, instance, primaryKey, historyChecker);
-                    }
-                }
-
-                @Override
-                public String toString()
-                {
-                    return Integer.toString(primaryKey);
-                }
-            };
-
-            final ActionListener listener = debug.debug(PARTITION, simulated.time, cluster, KEYSPACE, primaryKey);
-            if (listener != null)
-            {
-                Supplier<Action> wrap = supplier;
-                supplier = new Supplier<Action>()
-                {
-                    @Override
-                    public Action get()
-                    {
-                        Action action = wrap.get();
-                        action.register(listener);
-                        return action;
-                    }
-
-                    @Override
-                    public String toString()
-                    {
-                        return wrap.toString();
-                    }
-                };
-            }
-
-            primaryKeyActions.add(supplier);
-        }
+        BiFunction<SimulatedSystems, int[], Supplier<Action>> factory = actionFactory();
 
         List<Integer> available = IntStream.range(0, primaryKeys.length).boxed().collect(Collectors.toList());
         Action stream = Actions.infiniteStream(concurrency, new Supplier<Action>() {
             @Override
             public Action get()
             {
-                int i = simulated.random.uniform(0, available.size());
-                int next = available.get(i);
-                available.set(i, available.get(available.size() - 1));
-                available.remove(available.size() - 1);
+                int[] partitions = consume(simulated.random, available);","[{'comment': '`primaryKeys`? or `selectedPrimaryKeys`? To keep nomenclature consistent', 'commenter': 'belliottsmith'}, {'comment': 'This confused me when first working on it as its not the keys but the index of the keys.  The usage uses `primaryKeyIndex`, how about that?  \r\n\r\nThis is what the accord side does\r\n\r\n```\r\nreturn (simulated, primaryKeyIndex) -> {\r\n  int[] primaryKeys = IntStream.of(primaryKeyIndex).map(i -> this.primaryKeys[i]).toArray();\r\n...\r\n```', 'commenter': 'dcapwell'}, {'comment': 'Sure, sounds good', 'commenter': 'belliottsmith'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/HistoryChecker.java,"@@ -127,16 +127,22 @@ Event setById(int id, Event event)
         return byId[id] = event;
     }
 
+    private static int eventId(int[] witnessSequence, int eventPosition)
+    {
+        return eventPosition == 0 ? -1 : witnessSequence[eventPosition - 1];
+    }
+
     void witness(Observation witness, int[] witnessSequence, int start, int end)
     {
         int eventPosition = witnessSequence.length;
-        int eventId = eventPosition == 0 ? -1 : witnessSequence[eventPosition - 1];
+        int eventId = eventId(witnessSequence, eventPosition);
         setById(witness.id, new Event(witness.id)).log.add(new VerboseWitness(witness.id, start, end, witnessSequence));
         Event event = get(eventPosition, eventId);
         recordWitness(event, witness, witnessSequence);
         recordVisibleBy(event, end);
         recordVisibleUntil(event, start);
 
+","[{'comment': '?', 'commenter': 'belliottsmith'}, {'comment': 'the extra space?  I believe I made a change that I later removed, removing this extra space', 'commenter': 'dcapwell'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/Observation.java,"@@ -18,21 +18,38 @@
 
 package org.apache.cassandra.simulator.paxos;
 
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+
 class Observation implements Comparable<Observation>
 {
     final int id;
-    final Object[][] result;
+    private final SimpleQueryResult result;","[{'comment': ""Either make them all private or none? It's package private, so doesn't seem necessary to encapsulate, but don't mind which you pick."", 'commenter': 'belliottsmith'}, {'comment': 'removed `private` and removed the getter for it; usage accesses the field directly', 'commenter': 'dcapwell'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/PairOfSequencesAccordSimulation.java,"@@ -47,90 +56,34 @@
 import org.apache.cassandra.distributed.api.IInvokableInstance;
 import org.apache.cassandra.distributed.api.IIsolatedExecutor;
 import org.apache.cassandra.distributed.api.QueryResults;
-import org.apache.cassandra.exceptions.RequestTimeoutException;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
 import org.apache.cassandra.schema.ColumnMetadata;
 import org.apache.cassandra.schema.TableMetadata;
 import org.apache.cassandra.service.accord.AccordService;
 import org.apache.cassandra.service.accord.AccordTestUtils;
 import org.apache.cassandra.service.accord.txn.TxnData;
-import org.apache.cassandra.service.accord.txn.TxnDataName;
+import org.apache.cassandra.simulator.Action;
 import org.apache.cassandra.simulator.Debug;
 import org.apache.cassandra.simulator.RunnableActionScheduler;
 import org.apache.cassandra.simulator.cluster.ClusterActions;
 import org.apache.cassandra.simulator.systems.SimulatedSystems;
 import org.apache.cassandra.simulator.utils.IntRange;
+import org.apache.cassandra.utils.concurrent.UncheckedInterruptedException;
 
-import static org.apache.cassandra.distributed.api.ConsistencyLevel.ANY;
 import static org.apache.cassandra.simulator.paxos.HistoryChecker.fail;
-import static org.apache.cassandra.utils.ByteBufferUtil.bytes;
+import static org.apache.cassandra.utils.AssertionUtils.hasCause;
+import static org.apache.cassandra.utils.AssertionUtils.hasCauseAnyOf;
+import static org.assertj.core.api.Assertions.anyOf;","[{'comment': 'unused', 'commenter': 'belliottsmith'}, {'comment': 'when we rebase against trunk this will become a check style error... looking forward to that!', 'commenter': 'dcapwell'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/HistoryValidator.java,"@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.simulator.paxos;
+
+import javax.annotation.Nullable;
+
+public interface HistoryValidator
+{
+    Checker start(Observation observation);","[{'comment': ""`start` is perhaps a confusing name when the operation has completed, and we refer to start/end times of an observation. `begin` seems less confusing, but perhaps `log` or `enter` or something else to distinguish it would be better.\r\n\r\nI also don't think we should be saving these as registers inside the checkers themselves - if we want this API (and I'm fine with it) we should allocate a temporary object. Efficiency here isn't important, and it is much clearer and less prone to misuse."", 'commenter': 'belliottsmith'}, {'comment': '> begin seems less confusing, but perhaps log or enter or something else to distinguish it would be better.\r\n\r\nI like `log` so went with that.\r\n\r\n> we should allocate a temporary object\r\n\r\ndone.', 'commenter': 'dcapwell'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/HistoryValidator.java,"@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.simulator.paxos;
+
+import javax.annotation.Nullable;
+
+public interface HistoryValidator
+{
+    Checker start(Observation observation);
+
+    void log(@Nullable Integer pk);
+
+    interface Checker extends AutoCloseable
+    {
+        void witnessRead(int pk, int count, int[] seq);
+        void witnessWrite(int pk);","[{'comment': ""I'm not 100% keen on this, as it disguises the fact that the `id` is what we're logging here. I think perhaps(?) I would rather accept the `id` redundantly and verify it is the same, or some other change I haven't thought of yet."", 'commenter': 'belliottsmith'}, {'comment': 'Not a fan as that is captured by `log(Observation)`, so would allow users to provide the wrong thing and make usage more verbose.', 'commenter': 'dcapwell'}, {'comment': ""They couldn't provide the wrong thing if you validate it?\r\n\r\nAs it stands, it is unclear what it is we are logging as written, which wasn't the case before. It's implicit knowledge that it is the observation id we are writing, which is especially obtuse since the id exists whether or not we write. \r\n\r\nI'd be inclined then to just have `LinearizabilityValidator` implement an API similar to the one provided by the StrictSerializabilityVerifier, and introduce a common interface that we call directly, as it was IMO clearer what was being reported (but not prescriptive - if you have a better idea by all means)"", 'commenter': 'belliottsmith'}, {'comment': ""> As it stands, it is unclear what it is we are logging as written, which wasn't the case before. It's implicit knowledge that it is the observation id we are writing, which is especially obtuse since the id exists whether or not we write.\r\n\r\nI understand your point.  If we add the id then I question why `log` should see `Observation`?  Maybe the following is better?\r\n\r\n```\r\npublic interface HistoryValidator {\r\n  Checker log(int start, int end);\r\n  void log(@Nullable Integer pk);\r\n\r\n  interface Checker extends AutoCloseable {\r\n    void witnessRead(int pk, int count, int[] sequence);\r\n    void witnessWrite(int pk, int id);\r\n  }\r\n}\r\n```\r\n\r\nThe main issue with this interface is that `LinearizabilityValidator` doesn't see `Observation` so would need a refactor in `HistoryChecker`; not a large one but would be needed (issue is in `org.apache.cassandra.simulator.paxos.HistoryChecker#witness`), we did speak about dropping `LinearizabilityValidator`, maybe this is better motivation?\r\n\r\nI personally prefer the existing interface as you can't mess up and put event id in pk and pk in event id, but feel the above interface is a compromise?  Thoughts?\r\n\r\n"", 'commenter': 'dcapwell'}, {'comment': ""yeah, that works for me. Though looking at it written there, the two different `log` methods probably aren't ideal. Perhaps rename the prior `log` method to `print` to match the underlying impls?"", 'commenter': 'belliottsmith'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/StrictSerializabilityValidator.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.simulator.paxos;
+
+import javax.annotation.Nullable;
+
+import accord.verify.StrictSerializabilityVerifier;
+import com.carrotsearch.hppc.IntIntHashMap;
+import com.carrotsearch.hppc.IntIntMap;
+
+public class StrictSerializabilityValidator implements HistoryValidator, HistoryValidator.Checker
+{
+    private final StrictSerializabilityVerifier verifier;
+    private final IntIntMap index;
+    private Observation observation;
+
+    public StrictSerializabilityValidator(int[] primaryKeys)
+    {
+        this.verifier = new StrictSerializabilityVerifier(primaryKeys.length);
+        index = new IntIntHashMap(primaryKeys.length);
+        for (int i = 0; i < primaryKeys.length; i++)
+            index.put(primaryKeys[i], i);
+    }
+
+    @Override
+    public Checker start(Observation observation)
+    {
+        this.observation = observation;
+        verifier.begin();
+        return this;
+    }
+
+    @Override
+    public void log(@Nullable Integer pk)
+    {
+        if (pk == null) verifier.print();
+        else verifier.print(get(pk));
+    }
+
+    private int get(int pk)
+    {
+        if (index.containsKey(pk))
+            return index.get(pk);
+        throw new IllegalArgumentException(""Unknown pk="" + pk);
+    }
+
+    @Override
+    public void witnessRead(int pk, int count, int[] seq)
+    {
+        convertHistoryViolation(() -> verifier.witnessRead(get(pk), seq));","[{'comment': ""I don't believe this call can throw a violation? Nor the write one."", 'commenter': 'belliottsmith'}, {'comment': 'it doesn\'t now, but figured it ""may"" in the future, so safer to guard against', 'commenter': 'dcapwell'}]"
2056,test/simulator/main/org/apache/cassandra/simulator/paxos/PairOfSequencesAccordSimulation.java,"@@ -281,21 +211,151 @@ protected String preInsertStmt()
     }
 
     @Override
-    Operation verifying(int operationId, IInvokableInstance instance, int primaryKey, HistoryChecker historyChecker)
+    boolean allowMultiplePartitions() { return true; }
+
+    @Override
+    BiFunction<SimulatedSystems, int[], Supplier<Action>> actionFactory()
     {
-        return new VerifyingOperation(operationId, instance, serialConsistency, primaryKey, historyChecker);
+        AtomicInteger id = new AtomicInteger(0);
+
+        return (simulated, primaryKeyIndex) -> {
+            int[] partitions = IntStream.of(primaryKeyIndex).map(i -> primaryKeys[i]).toArray();
+            return () -> accordAction(id.getAndIncrement(), simulated, partitions);
+        };
     }
 
-    @Override
-    Operation nonVerifying(int operationId, IInvokableInstance instance, int primaryKey, HistoryChecker historyChecker)
+    private static IIsolatedExecutor.SerializableCallable<SimpleQueryResult> query(int id, int[] partitions, int[] readOnly)
     {
-        return new NonVerifyingOperation(operationId, instance, serialConsistency, primaryKey, historyChecker);
+        return () -> execute(createAccordTxn(id, partitions, readOnly), ""pk"", ""count"", ""seq"");
     }
 
-    @Override
-    Operation modifying(int operationId, IInvokableInstance instance, int primaryKey, HistoryChecker historyChecker)
+    public class ReadWriteOperation extends Operation
     {
-        return new ModifyingOperation(operationId, instance, ANY, serialConsistency, primaryKey, historyChecker);
+        private final IntSet allPartitions;
+        private final IntSet readOnlySet;","[{'comment': 'we should also have `writeOnly`?', 'commenter': 'belliottsmith'}, {'comment': ""`allPartitions` contains all read/write partitions.  `readOnlySet` is the set that only did a read. Because we have access to the read accord did, all write operations have a read as well, so currently we do not get write only partitions.\r\n\r\nI am not 100% sure how to do a write only txn as the validation logic tracks history; so if a txn doesn't load that history then its doing a blind write and could trigger a violation that wasn't a real violation.  My first thought would be that we periodically truncate the history, but do wonder if that would just make things more complex (if we have not observed all writes and did a truncate, may miss a violation)"", 'commenter': 'dcapwell'}, {'comment': 'Well, the list append can be truly blind (and should be), so we’re testing additional functionality. If it isn’t today, that’s something we should address (it is for Paxos, and we shouldn’t regress).\r\n\r\nWe shouldn’t also witness any reads performed internally, and if we do that’s also a bug - and for now at least we should filter them if it isn’t trivial to fix properly.\r\n\r\nI’m not sure what you mean about false violations but the verifier supports blind writes just fine?', 'commenter': 'belliottsmith'}, {'comment': '> Well, the list append can be truly blind (and should be), so we’re testing additional functionality. If it isn’t today, that’s something we should address (it is for Paxos, and we shouldn’t regress).\r\n\r\nMakes sense, we can\'t do for `String` but *should* be able to do for `list`, so could leverage that.\r\n\r\n> We shouldn’t also witness any reads performed internally, and if we do that’s also a bug \r\n\r\nIts an internal detail, normal CQL is unable to do this.  If you work at the `Txn` level you see the reads, if you work at the CQL level you only see what you asked for.\r\n\r\n> I’m not sure what you mean about false violations but the verifier supports blind writes just fine?\r\n\r\nits more if you are working with a type that requires read to update, then to avoid a read you need to ignore the history; simple example\r\n\r\n```\r\ncreate table foo (pk int PRIMARY KEY, seq text)\r\n\r\ntxn(INSERT INTO foo(pk, seq) VALUES (?, ""lacking history..."")\r\n```', 'commenter': 'dcapwell'}, {'comment': 'Have you looked at how the Paxos one works? It uses `+=`, and can manage blind writes just fine for both registers. Obviously under the hood the implementation has to perform a read for the string based `+=`, so it is not a blind write in implementation. But at the CQL level it is a blind write, and at the implementation level these are treated slightly differently.\r\n\r\nSaying that, it would probably also be good to randomise whether `seq1` as maintained/unmaintained so we can separately track the correctness of truly blind writes, as with Accord they are treated differently.', 'commenter': 'belliottsmith'}]"
2062,conf/cqlshrc.sample,"@@ -109,6 +112,9 @@ port = 9042
 
 
 ;[ssl]
+;; Version of TLS to be used. Required to be set for cassandra version > 3.11.8","[{'comment': '@neshkeev I do not think this is necessary to add, there is this later in `ssl` section:\r\n\r\n````\r\n; this is effectively ignored from 4.1 included as TLS protocol is auto-negotiated and will\r\n; be removed in the next major version of Cassandra, possible values were TLSv1, TLSv1_1 or TLSv1_2\r\n;version =\r\n````', 'commenter': 'smiklosovic'}, {'comment': 'I removed it', 'commenter': 'neshkeev'}, {'comment': 'https://issues.apache.org/jira/browse/CASSANDRA-17452', 'commenter': 'smiklosovic'}]"
2062,conf/cqlshrc.sample,"@@ -33,6 +32,10 @@
 ; classname = PlainTextAuthProvider
 ; username = user1
 
+[protocol]
+;; Specify a specific protcol version otherwise the client will default and downgrade as necessary","[{'comment': '@neshkeev is this really relevant? can you point me to the PR suggesting to add this?', 'commenter': 'smiklosovic'}, {'comment': '@smiklosovic , [here](https://github.com/apache/cassandra/pull/366/files#diff-f55a268d347d301121adb03b1d33958837ed984295b83fbe474dde2781fe809fR26) it is', 'commenter': 'neshkeev'}]"
2062,doc/modules/cassandra/pages/architecture/guarantees.adoc,"@@ -1,17 +1,21 @@
 = Guarantees
 
 Apache Cassandra is a highly scalable and reliable database. Cassandra
-is used in web based applications that serve large number of clients and
+<<<<<<< HEAD","[{'comment': '@neshkeev this seems like conflicts were not resolved properly.', 'commenter': 'smiklosovic'}, {'comment': ""It's fixed"", 'commenter': 'neshkeev'}]"
2062,doc/modules/cassandra/pages/architecture/guarantees.adoc,"@@ -47,46 +51,50 @@ Cassandra makes the following guarantees.
 * Batched writes across multiple tables are guaranteed to succeed
 completely or not at all
 * Secondary indexes are guaranteed to be consistent with their local
-replicas data
+replicas' data
 
 == High Scalability
 
 Cassandra is a highly scalable storage system in which nodes may be
-added/removed as needed. Using gossip-based protocol a unified and
+added/removed as needed. Using gossip-based protocol, a unified and
 consistent membership list is kept at each node.
 
 == High Availability
 
 Cassandra guarantees high availability of data by implementing a
-fault-tolerant storage system. Failure detection in a node is detected
-using a gossip-based protocol.
+fault-tolerant storage system. Failure of a node is detected using
+a gossip-based protocol.
 
 == Durability
 
 Cassandra guarantees data durability by using replicas. Replicas are
 multiple copies of a data stored on different nodes in a cluster. In a
 multi-datacenter environment the replicas may be stored on different
 datacenters. If one replica is lost due to unrecoverable node/datacenter
-failure the data is not completely lost as replicas are still available.
+<<<<<<< HEAD","[{'comment': '@neshkeev not resolved conflicts', 'commenter': 'smiklosovic'}, {'comment': ""It's fixed"", 'commenter': 'neshkeev'}]"
2062,doc/modules/cassandra/pages/faq/index.adoc,"@@ -79,7 +79,10 @@ intensive process that may result in adverse cluster performance. It's
 highly recommended to do rolling repairs, as an attempt to repair the
 entire cluster at once will most likely swamp it. Note that you will
 need to run a full repair (`-full`) to make sure that already repaired
-sstables are not skipped.
+sstables are not skipped. You should use ConsistencyLevel.QUORUM or ALL","[{'comment': 'may be ... `ConsistencyLevel.QUORUM` or `ALL` ...', 'commenter': 'smiklosovic'}, {'comment': ""It's fixed"", 'commenter': 'neshkeev'}]"
2062,doc/modules/cassandra/pages/tools/cqlsh.adoc,"@@ -38,7 +38,7 @@ modules that are central to the performance of `COPY`.
 == cqlshrc
 
 The `cqlshrc` file holds configuration options for `cqlsh`. 
-By default, the file is locagted the user's home directory at `~/.cassandra/cqlsh`, but a
+By default, the file is locagted the user's home directory at `~/.cassandra/cqlshrc`, but a","[{'comment': '`locagted` -> `located`', 'commenter': 'smiklosovic'}, {'comment': 'Good catch! I fixed it', 'commenter': 'neshkeev'}]"
2062,src/java/org/apache/cassandra/service/StartupChecks.java,"@@ -226,7 +226,7 @@ public void execute(StartupChecksOptions options)
     public static final StartupCheck checkValidLaunchDate = new StartupCheck()
     {
         /**
-         * The earliest legit timestamp a casandra instance could have ever launched.
+         * The earliest legit timestamp a cassandra instance could have ever launched.","[{'comment': '`cassandra` -> `Cassandra`', 'commenter': 'smiklosovic'}, {'comment': 'Fixed', 'commenter': 'neshkeev'}]"
2062,doc/modules/cassandra/pages/architecture/dynamo.adoc,"@@ -256,7 +255,7 @@ secondary indices with them.
 Transient replication is an experimental feature that is not ready
 for production use. The expected audience is experienced users of
 Cassandra capable of fully validating a deployment of their particular
-application. That means being able check that operations like reads,
+application. That means being able to check that operations like reads,","[{'comment': '```suggestion\r\napplication. That means you have the experience to check that operations like reads,\r\n```', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/architecture/guarantees.adoc,"@@ -97,12 +97,12 @@ uncommitted, without making a new addition or update.
 The guarantee for batched writes across multiple tables is that they
 will eventually succeed, or none will. Batch data is first written to
 batchlog system data, and when the batch data has been successfully
-stored in the cluster the batchlog data is removed. The batch is
-replicated to another node to ensure the full batch completes in the
-event the coordinator node fails.
+stored in the cluster, the batchlog data is removed. The batch is
+replicated to another node to ensure that the full batch completes in
+the event if the coordinator node fails.","[{'comment': '```suggestion\r\nthe event the coordinator node fails.\r\n```', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/architecture/overview.adoc,"@@ -59,19 +59,19 @@ keys.
 CQL supports numerous advanced features over a partitioned dataset such
 as:
 
-* Single partition lightweight transactions with atomic compare and set
-semantics.
+* Single-partition lightweight transactions with atomic compare and set
+semantics
 * User-defined types, functions and aggregates
-* Collection types including sets, maps, and lists.
+* Collection types including sets, maps, and lists
 * Local secondary indices
-* (Experimental) materialized views
+* (Experimental) materialized views.","[{'comment': '```suggestion\r\n* (Experimental) materialized views\r\n```', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/architecture/snitch.adoc,"@@ -2,17 +2,17 @@
 
 In cassandra, the snitch has two functions:
 
-* it teaches Cassandra enough about your network topology to route
+* It teaches Cassandra enough about your network topology to route","[{'comment': 'Generally, with a colon in the previous sentence, you use lowercase to start the bullet item.', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/architecture/snitch.adoc,"@@ -2,17 +2,17 @@
 
 In cassandra, the snitch has two functions:
 
-* it teaches Cassandra enough about your network topology to route
+* It teaches Cassandra enough about your network topology to route
 requests efficiently.
-* it allows Cassandra to spread replicas around your cluster to avoid
+* It allows Cassandra to spread replicas around your cluster to avoid","[{'comment': 'same comment', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/architecture/storage_engine.adoc,"@@ -3,17 +3,17 @@
 [[commit-log]]
 == CommitLog
 
-Commitlogs are an append only log of all mutations local to a Cassandra
+Commitlogs are an append-only log of all mutations local to a Cassandra
 node. Any data written to Cassandra will first be written to a commit
 log before being written to a memtable. This provides durability in the
 case of unexpected shutdown. On startup, any mutations in the commit log
 will be applied to memtables.
 
-All mutations write optimized by storing in commitlog segments, reducing
-the number of seeks needed to write to disk. Commitlog Segments are
-limited by the `commitlog_segment_size` option, once the size is
+All mutations are write-optimized by storing in commitlog segments, reducing
+the number of seeks needed to write to disk. Commitlog segments are
+limited by the `commitlog_segment_size` option. Once the size is
 reached, a new commitlog segment is created. Commitlog segments can be
-archived, deleted, or recycled once all its data has been flushed to
+archived, deleted, or recycled once all their data has been flushed to","[{'comment': '```suggestion\r\narchived, deleted, or recycled once all the data has been flushed to\r\n```', 'commenter': 'polandll'}, {'comment': ""Don't anthropomorphize. a commit log is not a person. :-)"", 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/architecture/storage_engine.adoc,"@@ -221,5 +224,5 @@ match the ""ib"" SSTable version
 
 [source,bash]
 ----
-include:example$find_sstables.sh[]
+include::../../examples/BASH/find_sstables.sh[]","[{'comment': '```suggestion\r\ninclude::example$BASH/find_sstables.sh[]\r\n```', 'commenter': 'polandll'}, {'comment': 'Never use a path in includes in asciidoc/antora. https://docs.antora.org/antora/latest/page/include-an-example/', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/cql/types.adoc,"@@ -338,13 +338,13 @@ include::example$CQL/update_list.cql[]
 .Warning
 ====
 The append and prepend operations are not idempotent by nature. So in
-particular, if one of these operation timeout, then retrying the
+particular, if one of these operations timeout, then retrying the","[{'comment': '```suggestion\r\nparticular, if one of these operations times out, then retrying the\r\n```', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/operating/auditlogging.adoc,"@@ -88,10 +88,23 @@ Common audit log entry types are one of the following:
 | ERROR | REQUEST_FAILURE
 |===
 
+== Availability and durability
+
+NOTE: Unlike data, audit log entries are not replicated
+
+For a given query, the corresponding audit entry is only stored on the coordinator node.
+For example, an ``INSERT`` in a keyspace with replication factor of 3 will produce an audit entry on one node, the coordinator who handled the request, and not on the two other nodes.
+For this reason, and depending on compliance requirements you must meet, you have to make sure that audig logs are stored on a non-ephemeral storage.
+
+You can achieve custom needs with <<archive_command>> option.","[{'comment': '```suggestion\r\nYou can achieve custom needs with the <<archive_command>> option.\r\n```', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/operating/auditlogging.adoc,"@@ -88,10 +88,23 @@ Common audit log entry types are one of the following:
 | ERROR | REQUEST_FAILURE
 |===
 
+== Availability and durability
+
+NOTE: Unlike data, audit log entries are not replicated
+
+For a given query, the corresponding audit entry is only stored on the coordinator node.
+For example, an ``INSERT`` in a keyspace with replication factor of 3 will produce an audit entry on one node, the coordinator who handled the request, and not on the two other nodes.
+For this reason, and depending on compliance requirements you must meet, you have to make sure that audig logs are stored on a non-ephemeral storage.","[{'comment': '```suggestion\r\nFor this reason, and depending on compliance requirements you must meet, make sure that audit logs are stored on a non-ephemeral storage.\r\n```', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/operating/auditlogging.adoc,"@@ -123,16 +136,25 @@ audit_logging_options:
 
 === enabled
 
-Audit logging is enabled by setting the `enabled` option to `true` in
-the `audit_logging_options` setting. 
+Control wether audit logging is enabled or disabled (default).","[{'comment': '```suggestion\r\nControl whether audit logging is enabled or disabled (default).\r\n```', 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/operating/auditlogging.adoc,"@@ -186,52 +210,101 @@ excluded_categories: DDL, DML, QUERY, PREPARE
 Users to audit log are set with the `included_users` and `excluded_users` options. 
 The `included_users` option specifies a comma-separated list of users to include explicitly.
 The `excluded_users` option specifies a comma-separated list of users to exclude explicitly.
-By default all users are included, and no users are excluded. 
+By default, all users are included, and no users are excluded.
 
 [source, yaml]
 ----
 included_users: 
 excluded_users: john, mary
 ----
 
+[[roll_cycle]]
 === roll_cycle
 
 The ``roll_cycle`` defines the frequency with which the audit log segments are rolled.
-Supported values are ``HOURLY`` (default), ``MINUTELY``, and ``DAILY``.
+Supported values are:
+
+- ``MINUTELY``
+- ``FIVE_MINUTELY``
+- ``TEN_MINUTELY``
+- ``TWENTY_MINUTELY``
+- ``HALF_HOURLY``
+- ``HOURLY`` (default)
+- ``TWO_HOURLY``
+- ``FOUR_HOURLY``
+- ``SIX_HOURLY``
+- ``DAILY``
+
 For example: ``roll_cycle: DAILY``
 
+WARNING: Read the following paragraph when changing ``roll_cycle`` on a production node.
+
+With the `BinLogger` implementation, any attempt to modify the roll cycle on a node where audit logging was previously enabled will fail silentely due to https://github.com/OpenHFT/Chronicle-Queue[Chronicle Queue] roll cycle inference mechanism (even if you delete the ``metadata.cq4t`` file).
+
+Here is an example of such an override visible in Cassandra logs:
+----
+INFO  [main] <DATE TIME> BinLog.java:420 - Attempting to configure bin log: Path: /path/to/audit Roll cycle: TWO_HOURLY [...]
+WARN  [main] <DATE TIME> SingleChronicleQueueBuilder.java:477 - Overriding roll cycle from TWO_HOURLY to FIVE_MINUTE
+----
+
+In order to change ``roll_cycle`` on a node, you have to:
+
+1. Stop Cassandra
+2. Move or offload all audit logs somewhere else (in a safe and durable location)
+3. Restart Cassandra.
+4. Check Cassandra logs
+5. Make sure that audit log filenames under ``audit_logs_dir`` correspond to the new roll cycle.
+
 === block
 
 The ``block`` option specifies whether audit logging should block writing or drop log records if the audit logging falls behind. Supported boolean values are ``true`` (default) or ``false``.
-For example: ``block: false`` to drop records
+
+For example: ``block: false`` to drop records (e.g. if audit is used for troobleshooting)
+
+For regulatory compliance purpose, it's a good practice to explicitely set ``block: true`` to prevent any regression in case of future default value change.","[{'comment': ""```suggestion\r\nFor regulatory compliance purposes, it's a good practice to explicitly set ``block: true`` to prevent any regression in case of future default value change.\r\n```"", 'commenter': 'polandll'}]"
2062,doc/modules/cassandra/pages/operating/auditlogging.adoc,"@@ -186,52 +210,101 @@ excluded_categories: DDL, DML, QUERY, PREPARE
 Users to audit log are set with the `included_users` and `excluded_users` options. 
 The `included_users` option specifies a comma-separated list of users to include explicitly.
 The `excluded_users` option specifies a comma-separated list of users to exclude explicitly.
-By default all users are included, and no users are excluded. 
+By default, all users are included, and no users are excluded.
 
 [source, yaml]
 ----
 included_users: 
 excluded_users: john, mary
 ----
 
+[[roll_cycle]]
 === roll_cycle
 
 The ``roll_cycle`` defines the frequency with which the audit log segments are rolled.
-Supported values are ``HOURLY`` (default), ``MINUTELY``, and ``DAILY``.
+Supported values are:
+
+- ``MINUTELY``
+- ``FIVE_MINUTELY``
+- ``TEN_MINUTELY``
+- ``TWENTY_MINUTELY``
+- ``HALF_HOURLY``
+- ``HOURLY`` (default)
+- ``TWO_HOURLY``
+- ``FOUR_HOURLY``
+- ``SIX_HOURLY``
+- ``DAILY``
+
 For example: ``roll_cycle: DAILY``
 
+WARNING: Read the following paragraph when changing ``roll_cycle`` on a production node.
+
+With the `BinLogger` implementation, any attempt to modify the roll cycle on a node where audit logging was previously enabled will fail silentely due to https://github.com/OpenHFT/Chronicle-Queue[Chronicle Queue] roll cycle inference mechanism (even if you delete the ``metadata.cq4t`` file).
+
+Here is an example of such an override visible in Cassandra logs:
+----
+INFO  [main] <DATE TIME> BinLog.java:420 - Attempting to configure bin log: Path: /path/to/audit Roll cycle: TWO_HOURLY [...]
+WARN  [main] <DATE TIME> SingleChronicleQueueBuilder.java:477 - Overriding roll cycle from TWO_HOURLY to FIVE_MINUTE
+----
+
+In order to change ``roll_cycle`` on a node, you have to:
+
+1. Stop Cassandra
+2. Move or offload all audit logs somewhere else (in a safe and durable location)
+3. Restart Cassandra.
+4. Check Cassandra logs
+5. Make sure that audit log filenames under ``audit_logs_dir`` correspond to the new roll cycle.
+
 === block
 
 The ``block`` option specifies whether audit logging should block writing or drop log records if the audit logging falls behind. Supported boolean values are ``true`` (default) or ``false``.
-For example: ``block: false`` to drop records
+
+For example: ``block: false`` to drop records (e.g. if audit is used for troobleshooting)
+
+For regulatory compliance purpose, it's a good practice to explicitely set ``block: true`` to prevent any regression in case of future default value change.
 
 === max_queue_weight
 
 The ``max_queue_weight`` option sets the maximum weight of in-memory queue for records waiting to be written to the file before blocking or dropping.  The option must be set to a positive value. The default value is 268435456, or 256 MiB.
+
 For example, to change the default: ``max_queue_weight: 134217728 # 128 MiB``
 
 === max_log_size
 
 The ``max_log_size`` option sets the maximum size of the rolled files to retain on disk before deleting the oldest file.  The option must be set to a positive value. The default is 17179869184, or 16 GiB.
 For example, to change the default: ``max_log_size: 34359738368 # 32 GiB``
 
+WARNING: ``max_log_size`` is ignored if ``archive_command`` option is set.
+
+[[archive_command]]
 === archive_command
 
+NOTE: If ``archive_command`` option is empty or unset (default), Cassandra uses a built-in DeletingArchiver that deletes the oldest files if ``max_log_size`` is reached.
+
 The ``archive_command`` option sets the user-defined archive script to execute on rolled log files.
-For example: ``archive_command: /usr/local/bin/archiveit.sh %path # %path is the file being rolled``
+For example: ``archive_command: ""/usr/local/bin/archiveit.sh %path""``
 
-=== max_archive_retries
+``%path`` is replaced with the absolute file path of the file being rolled.
 
-The ``max_archive_retries`` option sets the max number of retries of failed archive commands. The default is 10.
-For example: ``max_archive_retries: 10``
+When using a user-defined script, Cassandra do **not** use the DeletingArchiver, so it's the responsability of the script to make any required cleanup.","[{'comment': ""```suggestion\r\nWhen using a user-defined script, Cassandra does **not** use the DeletingArchiver, so it's the responsibility of the script to make any required cleanup.\r\n```"", 'commenter': 'polandll'}]"
2073,src/java/org/apache/cassandra/dht/IPartitioner.java,"@@ -24,6 +24,7 @@
 import java.util.Optional;
 import java.util.Random;
 
+import accord.local.ShardDistributor;","[{'comment': 'Redundant import', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/service/accord/AccordService.java,"@@ -31,6 +31,8 @@
 import accord.impl.SimpleProgressLog;
 import accord.impl.SizeOfIntersectionSorter;
 import accord.local.Node;
+import accord.local.ShardDistributor;","[{'comment': 'Dead import', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/service/accord/AccordService.java,"@@ -41,6 +43,8 @@
 import org.apache.cassandra.exceptions.WriteTimeoutException;
 import org.apache.cassandra.net.IVerbHandler;
 import org.apache.cassandra.service.accord.api.AccordAgent;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey;","[{'comment': 'Dead import', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/db/rows/AbstractCell.java,"@@ -224,7 +224,7 @@ public String toString()
         if (isTombstone())
             return String.format(""[%s=<tombstone> %s]"", column().name, livenessInfoString());
         else
-            return String.format(""[%s=%s %s]"", column().name, safeToString(type), livenessInfoString());
+            return String.format(""[%s=%s %s]"", column().name, safeToString(column.type), livenessInfoString());","[{'comment': 'Why?', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/db/marshal/ByteArrayAccessor.java,"@@ -107,6 +108,7 @@ public byte[] read(DataInputPlus in, int length) throws IOException
     @Override
     public byte[] slice(byte[] input, int offset, int length)
     {
+        Invariants.checkArgument(offset + length <= input.length);","[{'comment': 'Maybe more appropriate and consistent to use `Preconditions.checkArgument()` in a super-generic utility class like this? ', 'commenter': 'iamaleksey'}, {'comment': ""`Invariants` is a replacement for `Preconditions` and is otherwise largely equivalent. Though it is brought in from accord-core rather than Guava. I don't mind sticking to `Preconditions` in C*, but the switch in Accord was to make it easier to catch application-level faults in the debugger as a lot of libraries have failed `Preconditions` trigger repeatedly. Also, it's a better name, and we have flexibility to modify/expand it as necessary."", 'commenter': 'belliottsmith'}]"
2073,src/java/org/apache/cassandra/service/accord/TokenRange.java,"@@ -36,9 +36,9 @@ public TokenRange(AccordRoutingKey start, AccordRoutingKey end)
         super(start, end);
     }
 
-    public static TokenRange fullRange(TableId tableId)
+    public static TokenRange fullRange(String keyspace, TableId tableId)","[{'comment': '`TableId` arg now unused', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/service/accord/async/AsyncOperation.java,"@@ -65,6 +66,7 @@
 
     private State state = State.INITIALIZED;
     private final AccordCommandStore commandStore;
+    private SafeAccordCommandStore safeStore;","[{'comment': ""I don't think we need this field? Should be fine with just a local variable under `case LOADING`? Am I missing something important?"", 'commenter': 'iamaleksey'}]"
2073,test/unit/org/apache/cassandra/service/accord/api/AccordKeyTest.java,"@@ -59,26 +59,26 @@ public static IPartitioner partitioner(TableId tableId)
     public void partitionKeyTest()
     {
         DecoratedKey dk = partitioner(TABLE1).decorateKey(ByteBufferUtil.bytes(5));
-        PartitionKey pk = new PartitionKey(TABLE1, dk);
+        PartitionKey pk = new PartitionKey(""ks"", TABLE1, dk);","[{'comment': 'Completely insignificant, but is there a reason to use `""ks""` here (that exists) vs `""""` elsewhere below? ', 'commenter': 'iamaleksey'}, {'comment': 'Yes, TABLE1 has a keyspace of ""ks"" and we serialise only the tableId and grab the keyspace from its metadata, so we deserialise ks.', 'commenter': 'belliottsmith'}]"
2073,test/unit/org/apache/cassandra/service/accord/api/AccordKeyTest.java,"@@ -91,10 +91,22 @@ public void tableComparisonTest()
         Assert.assertTrue(TABLE1.compareTo(TABLE2) < 0);
 
         DecoratedKey dk1 = partitioner(TABLE1).decorateKey(ByteBufferUtil.bytes(5));
-        PartitionKey pk1 = new PartitionKey(TABLE1, dk1);
+        PartitionKey pk1 = new PartitionKey("""", TABLE1, dk1);
 
         DecoratedKey dk2 = partitioner(TABLE2).decorateKey(ByteBufferUtil.bytes(5));
-        PartitionKey pk2 = new PartitionKey(TABLE2, dk2);
+        PartitionKey pk2 = new PartitionKey("""", TABLE2, dk2);
+
+        Assert.assertTrue(pk1.compareTo(pk2) == 0);","[{'comment': 'Nope', 'commenter': 'iamaleksey'}]"
2073,test/unit/org/apache/cassandra/service/accord/api/AccordKeyTest.java,"@@ -59,26 +59,26 @@ public static IPartitioner partitioner(TableId tableId)
     public void partitionKeyTest()
     {
         DecoratedKey dk = partitioner(TABLE1).decorateKey(ByteBufferUtil.bytes(5));
-        PartitionKey pk = new PartitionKey(TABLE1, dk);
+        PartitionKey pk = new PartitionKey(""ks"", TABLE1, dk);
         SerializerTestUtils.assertSerializerIOEquality(pk, PartitionKey.serializer);
     }
 
     @Test
     public void tokenKeyTest()
     {
         DecoratedKey dk = partitioner(TABLE1).decorateKey(ByteBufferUtil.bytes(5));
-        TokenKey pk = new TokenKey(TABLE1, dk.getToken());
+        TokenKey pk = new TokenKey("""", dk.getToken());
         SerializerTestUtils.assertSerializerIOEquality(pk, TokenKey.serializer);
     }
 
     @Test
     public void comparisonTest()
     {
         DecoratedKey dk = partitioner(TABLE1).decorateKey(ByteBufferUtil.bytes(5));
-        PartitionKey pk = new PartitionKey(TABLE1, dk);
-        TokenKey tk = new TokenKey(TABLE1, dk.getToken());
-        TokenKey tkLow = new TokenKey(TABLE1, dk.getToken().decreaseSlightly());
-        TokenKey tkHigh = new TokenKey(TABLE1, dk.getToken().increaseSlightly());
+        PartitionKey pk = new PartitionKey("""", TABLE1, dk);
+        TokenKey tk = new TokenKey("""", dk.getToken());
+        TokenKey tkLow = new TokenKey("""", dk.getToken().decreaseSlightly());
+        TokenKey tkHigh = new TokenKey("""", dk.getToken().increaseSlightly());
 
         Assert.assertTrue(tk.compareTo(pk) == 0);","[{'comment': 'No longer yep', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/service/accord/AccordTopologyUtils.java,"@@ -23,6 +23,7 @@
 import java.util.HashSet;","[{'comment': 'A bunch of unused imports (4)', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/service/accord/AccordTopologyUtils.java,"@@ -114,21 +112,15 @@ public static Topology createTopology(long epoch)
             if (SchemaConstants.REPLICATED_SYSTEM_KEYSPACE_NAMES.contains(ksname))","[{'comment': 'Pre-existing, but `SchemaConstants#isSystemKeyspace()` is what you want instead of two `contains()` checks.', 'commenter': 'iamaleksey'}, {'comment': 'Better yet, only iterate over `Schema.instance.distributedKeyspaces()` (`.names()`) and filter out on `SchemaConstants#isReplicatedSystemKeyspace()`.', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/service/accord/api/AccordRoutableKey.java,"@@ -22,30 +22,25 @@
 
 import accord.primitives.RoutableKey;
 import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.schema.TableId;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey.SentinelKey;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey.TokenKey;
 
 public abstract class AccordRoutableKey implements RoutableKey
 {
-    final TableId tableId;
+    final String keyspace; // TODO (desired): use an id (TrM)
 
-    protected AccordRoutableKey(TableId tableId)
+    protected AccordRoutableKey(String keyspace)
     {
-        this.tableId = tableId;
+        this.keyspace = keyspace;
     }
 
-    public final TableId tableId() { return tableId; }
+    public final String keyspace() { return keyspace; }
     public abstract Token token();
 
-    @Override
-    public final int routingHash()
-    {
-        return token().tokenHash();
-    }
-
     @Override
     public int hashCode()
     {
-        return Objects.hash(tableId, routingHash());
+        return Objects.hash(keyspace, token().tokenHash());
     }
 
     public final int compareTo(RoutableKey that)","[{'comment': '`@Override`? (pre-existing)', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -91,29 +342,25 @@ private static long getThreadId(ExecutorService executor)
     private final Agent agent;
     private final DataStore dataStore;
     private final ProgressLog progressLog;
-    private final RangesForEpoch rangesForEpoch;
+    private final RangesForEpochHolder rangesForEpochHolder;
 
     public AccordCommandStore(int id,
-                              int generation,
-                              int index,
-                              int numShards,
                               NodeTimeService time,
                               Agent agent,
                               DataStore dataStore,
                               ProgressLog.Factory progressLogFactory,
-                              RangesForEpoch rangesForEpoch,
-                              ExecutorService executor)
+                              RangesForEpochHolder rangesForEpoch)
     {
-        super(id, generation, index, numShards);
+        super(id);
         this.time = time;
         this.agent = agent;
         this.dataStore = dataStore;
         this.progressLog = progressLogFactory.create(this);
-        this.rangesForEpoch = rangesForEpoch;
-        this.loggingId = String.format(""[%s:%s]"", generation, index);
-        this.executor = executor;
+        this.rangesForEpochHolder = rangesForEpoch;
+        this.loggingId = String.format(""[%s]"", id);
+        this.executor = executorFactory().sequential(CommandStore.class.getSimpleName() + '[' + id + ']');;","[{'comment': 'Redundant semicolon', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/AccordBytesSplitter.java,"@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.math.BigInteger;
+
+import accord.utils.Invariants;
+
+public abstract class AccordBytesSplitter extends AccordSplitter
+{
+    @Override
+    BigInteger minimumValue()
+    {
+        return BigInteger.ZERO;
+    }
+
+    @Override
+    BigInteger maximumValue(BigInteger start)
+    {
+        return BigInteger.ONE.shiftLeft(1 + start.bitLength());
+    }
+
+    @Override
+    BigInteger valueForToken(Token token)
+    {
+        byte[] bytes = ((ByteOrderedPartitioner.BytesToken) token).token;
+        int bitLength = bytes.length * 8;
+        BigInteger value = BigInteger.ZERO;
+        for (int i = 0 ; i < Math.min(8, bytes.length) ; ++i)
+            value = value.add(BigInteger.valueOf(bytes[i] & 0xffL).shiftLeft(bitLength - (i + 1) * 8));
+        return value;
+    }
+
+    @Override
+    Token tokenForValue(BigInteger value)
+    {
+        // TODO (required): test
+        Invariants.checkArgument(value.compareTo(BigInteger.ZERO) >= 0);
+        int bitLength = value.bitLength();
+        byte[] bytes = new byte[(bitLength + 7) / 8];
+        for (int i = 0 ; i < bitLength ; i += 8)
+            bytes[i/8] = value.shiftRight(bitLength - (i+1)*8).byteValue();
+        return new ByteOrderedPartitioner.BytesToken(bytes);
+    }
+}","[{'comment': 'I think these are scuffed, both. `tokenForValue()` should shift right by entire bytes only, for one. And `valueForToken()` is off as well, in one of two ways depending on the goal.', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/RandomPartitioner.java,"@@ -92,6 +92,18 @@ public BigInteger valueForToken(Token token)
         {
             return ((BigIntegerToken)token).getTokenValue();
         }
+
+        @Override
+        BigInteger minimumValue()
+        {
+            return ZERO;","[{'comment': 'Should be `MINIMUM`?', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java,"@@ -276,4 +277,55 @@ public AbstractType<?> partitionOrdering()
     {
         return UTF8Type.instance;
     }
+
+    @Override
+    public AccordSplitter accordSplitter()
+    {
+        return this;
+    }
+
+    @Override
+    BigInteger valueForToken(Token token)
+    {
+        String chars = ((StringToken) token).token;
+        int charLength = 8;
+        BigInteger value = BigInteger.ZERO;
+        for (int i = 0 ; i < Math.min(8, chars.length()) ; ++i)
+            value = value.add(BigInteger.valueOf(chars.charAt(i) & 0xffffL).shiftLeft((charLength - 1 - i) * 16));
+        return value;
+    }
+
+    @Override
+    Token tokenForValue(BigInteger value)
+    {
+        // TODO (required): test
+        Invariants.checkArgument(value.compareTo(BigInteger.ZERO) >= 0);
+        int charLength = (value.bitLength() + 15) / 16;
+        char[] chars = new char[charLength];
+        for (int i = 0 ; i < charLength ; ++i)
+            chars[i] = (char) value.shiftRight((charLength - 1 - i) * 16).shortValue();
+        return new StringToken(new String(chars));
+    }
+
+    @Override
+    BigInteger minimumValue()
+    {
+        return BigInteger.ZERO;
+    }
+
+    @Override
+    BigInteger maximumValue(BigInteger start)
+    {
+        return BigInteger.ONE.shiftLeft((start.bitLength() + 15) / 16);","[{'comment': '`* 16`?', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java,"@@ -276,4 +277,55 @@ public AbstractType<?> partitionOrdering()
     {
         return UTF8Type.instance;
     }
+
+    @Override
+    public AccordSplitter accordSplitter()
+    {
+        return this;
+    }
+
+    @Override
+    BigInteger valueForToken(Token token)
+    {
+        String chars = ((StringToken) token).token;
+        int charLength = 8;
+        BigInteger value = BigInteger.ZERO;
+        for (int i = 0 ; i < Math.min(8, chars.length()) ; ++i)
+            value = value.add(BigInteger.valueOf(chars.charAt(i) & 0xffffL).shiftLeft((charLength - 1 - i) * 16));
+        return value;
+    }
+
+    @Override
+    Token tokenForValue(BigInteger value)
+    {
+        // TODO (required): test
+        Invariants.checkArgument(value.compareTo(BigInteger.ZERO) >= 0);
+        int charLength = (value.bitLength() + 15) / 16;
+        char[] chars = new char[charLength];
+        for (int i = 0 ; i < charLength ; ++i)
+            chars[i] = (char) value.shiftRight((charLength - 1 - i) * 16).shortValue();
+        return new StringToken(new String(chars));
+    }
+
+    @Override
+    BigInteger minimumValue()
+    {
+        return BigInteger.ZERO;
+    }
+
+    @Override
+    BigInteger maximumValue(BigInteger start)
+    {
+        return BigInteger.ONE.shiftLeft((start.bitLength() + 15) / 16);
+    }
+
+    @Override
+    BigInteger normalize(BigInteger normalize, BigInteger with)
+    {
+        int normalizeByteCount = (normalize.bitLength() + 15) / 16;","[{'comment': '`normalizeCharCount`?', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java,"@@ -276,4 +277,55 @@ public AbstractType<?> partitionOrdering()
     {
         return UTF8Type.instance;
     }
+
+    @Override
+    public AccordSplitter accordSplitter()
+    {
+        return this;
+    }
+
+    @Override
+    BigInteger valueForToken(Token token)
+    {
+        String chars = ((StringToken) token).token;
+        int charLength = 8;
+        BigInteger value = BigInteger.ZERO;
+        for (int i = 0 ; i < Math.min(8, chars.length()) ; ++i)
+            value = value.add(BigInteger.valueOf(chars.charAt(i) & 0xffffL).shiftLeft((charLength - 1 - i) * 16));
+        return value;
+    }
+
+    @Override
+    Token tokenForValue(BigInteger value)
+    {
+        // TODO (required): test
+        Invariants.checkArgument(value.compareTo(BigInteger.ZERO) >= 0);
+        int charLength = (value.bitLength() + 15) / 16;
+        char[] chars = new char[charLength];
+        for (int i = 0 ; i < charLength ; ++i)
+            chars[i] = (char) value.shiftRight((charLength - 1 - i) * 16).shortValue();
+        return new StringToken(new String(chars));
+    }
+
+    @Override
+    BigInteger minimumValue()
+    {
+        return BigInteger.ZERO;
+    }
+
+    @Override
+    BigInteger maximumValue(BigInteger start)
+    {
+        return BigInteger.ONE.shiftLeft((start.bitLength() + 15) / 16);
+    }
+
+    @Override
+    BigInteger normalize(BigInteger normalize, BigInteger with)
+    {
+        int normalizeByteCount = (normalize.bitLength() + 15) / 16;
+        int withByteCount = (with.bitLength() + 15) / 16;","[{'comment': '`withCharCount`?', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/ByteOrderedPartitioner.java,"@@ -386,4 +386,20 @@ public AbstractType<?> partitionOrdering()
     {
         return BytesType.instance;
     }
+
+    @Override
+    public AccordSplitter accordSplitter()
+    {
+        return this;
+    }
+
+    @Override
+    BigInteger normalize(BigInteger normalize, BigInteger with)","[{'comment': 'Overrides an identical implementation in base `AccordBytesSplitter`', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java,"@@ -276,4 +277,55 @@ public AbstractType<?> partitionOrdering()
     {
         return UTF8Type.instance;
     }
+
+    @Override
+    public AccordSplitter accordSplitter()
+    {
+        return this;
+    }
+
+    @Override
+    BigInteger valueForToken(Token token)
+    {
+        String chars = ((StringToken) token).token;
+        int charLength = 8;
+        BigInteger value = BigInteger.ZERO;
+        for (int i = 0 ; i < Math.min(8, chars.length()) ; ++i)
+            value = value.add(BigInteger.valueOf(chars.charAt(i) & 0xffffL).shiftLeft((charLength - 1 - i) * 16));
+        return value;
+    }
+
+    @Override
+    Token tokenForValue(BigInteger value)
+    {
+        // TODO (required): test
+        Invariants.checkArgument(value.compareTo(BigInteger.ZERO) >= 0);
+        int charLength = (value.bitLength() + 15) / 16;
+        char[] chars = new char[charLength];
+        for (int i = 0 ; i < charLength ; ++i)
+            chars[i] = (char) value.shiftRight((charLength - 1 - i) * 16).shortValue();
+        return new StringToken(new String(chars));
+    }
+
+    @Override
+    BigInteger minimumValue()
+    {
+        return BigInteger.ZERO;
+    }
+
+    @Override
+    BigInteger maximumValue(BigInteger start)
+    {
+        return BigInteger.ONE.shiftLeft(16 * ((start.bitLength() + 15) / 16));
+    }
+
+    @Override
+    BigInteger normalize(BigInteger normalize, BigInteger with)
+    {
+        int normalizeCharCount = (normalize.bitLength() + 15) / 16;
+        int withCharCount = (with.bitLength() + 15) / 16;
+        if (normalizeCharCount >= withCharCount)","[{'comment': '`>=` here, `<=` in `AccordBytesSplitter`', 'commenter': 'iamaleksey'}, {'comment': '(So gotta fix the other one)', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java,"@@ -22,7 +22,10 @@
 import java.nio.charset.CharacterCodingException;
 import java.util.*;
 import java.util.concurrent.ThreadLocalRandom;
+import java.util.function.Function;
 
+import accord.api.RoutingKey;
+import accord.primitives.Ranges;
 import accord.utils.Invariants;","[{'comment': 'Dead import now', 'commenter': 'iamaleksey'}]"
2073,src/java/org/apache/cassandra/dht/AccordBytesSplitter.java,"@@ -20,51 +20,80 @@
 
 import java.math.BigInteger;
 
+import accord.api.RoutingKey;
+import accord.primitives.Ranges;
 import accord.utils.Invariants;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey;
 
-public abstract class AccordBytesSplitter extends AccordSplitter
+import static accord.utils.Invariants.checkArgument;
+import static java.math.BigInteger.ONE;
+import static java.math.BigInteger.ZERO;
+
+public class AccordBytesSplitter extends AccordSplitter
 {
+    final int byteLength;
+
+    protected AccordBytesSplitter(Ranges ranges)
+    {
+        int bytesLength = 0;
+        for (accord.primitives.Range range : ranges)
+        {
+            bytesLength = Integer.max(bytesLength, byteLength(range.start()));
+            bytesLength = Integer.max(bytesLength, byteLength(range.end()));
+        }
+        this.byteLength = bytesLength;
+    }
+
     @Override
     BigInteger minimumValue()
     {
-        return BigInteger.ZERO;
+        return ZERO;
     }
 
     @Override
-    BigInteger maximumValue(BigInteger start)
+    BigInteger maximumValue()
     {
-        return BigInteger.ONE.shiftLeft(8 * ((start.bitLength() + 7) / 8));
+        return ONE.shiftLeft(8 * byteLength).subtract(ONE);
     }
 
     @Override
     BigInteger valueForToken(Token token)
     {
         byte[] bytes = ((ByteOrderedPartitioner.BytesToken) token).token;
-        int byteLength = 8;
-        BigInteger value = BigInteger.ZERO;
-        for (int i = 0 ; i < Math.min(8, bytes.length) ; ++i)
+        checkArgument(bytes.length <= byteLength);
+        int byteLength = byteLength(token);","[{'comment': 'This is just `bytes.length` with some extra indirection?', 'commenter': 'iamaleksey'}, {'comment': 'Might as well just fold `normalize()` into here to be honest by simply not shadowing `byteLength` field.', 'commenter': 'iamaleksey'}]"
2088,src/java/org/apache/cassandra/cql3/statements/TransactionStatement.java,"@@ -177,9 +178,16 @@ TxnNamedRead createNamedRead(NamedSelect namedSelect, QueryOptions options)
         SinglePartitionReadQuery.Group<SinglePartitionReadCommand> selectQuery = (SinglePartitionReadQuery.Group<SinglePartitionReadCommand>) readQuery;
 
         if (selectQuery.queries.size() != 1)
-            throw new IllegalArgumentException(""Within a transaction, SELECT statements must select a single partition; found "" + selectQuery.queries.size() + "" partitions"");
+        {
+            if (!TxnDataName.returning().equals(namedSelect.name))
+                throw new IllegalArgumentException(""Within a transaction, SELECT statements must select a single partition; found "" + selectQuery.queries.size() + "" partitions"");","[{'comment': '```suggestion\r\n                throw new IllegalArgumentException(""Within a transaction, implicit reads and reads within LET statements must select a single partition; found "" + selectQuery.queries.size() + "" partitions"");\r\n```', 'commenter': 'maedhroz'}]"
2088,src/java/org/apache/cassandra/cql3/statements/TransactionStatement.java,"@@ -177,9 +178,16 @@ TxnNamedRead createNamedRead(NamedSelect namedSelect, QueryOptions options)
         SinglePartitionReadQuery.Group<SinglePartitionReadCommand> selectQuery = (SinglePartitionReadQuery.Group<SinglePartitionReadCommand>) readQuery;
 
         if (selectQuery.queries.size() != 1)
-            throw new IllegalArgumentException(""Within a transaction, SELECT statements must select a single partition; found "" + selectQuery.queries.size() + "" partitions"");
+        {
+            if (!TxnDataName.returning().equals(namedSelect.name))
+                throw new IllegalArgumentException(""Within a transaction, SELECT statements must select a single partition; found "" + selectQuery.queries.size() + "" partitions"");
+            // multi partitions on the same table are only allowed for the returning clause
+            return IntStream.range(0, selectQuery.queries.size())
+                            .mapToObj(i -> new TxnNamedRead(TxnDataName.returning(i), selectQuery.queries.get(i)))
+                            .collect(Collectors.toList());","[{'comment': ""This should be a pretty hot path, so I'd have two potential concerns.\r\n\r\n1.) I think we'll create less overall garbage w/ something like...\r\n\r\n```\r\nList<TxnNamedRead> list = new ArrayList<>(selectQuery.queries.size());\r\nfor (int i = 0; i < selectQuery.queries.size(); i++)\r\n    list.add(new TxnNamedRead(TxnDataName.returning(i), selectQuery.queries.get(i)));\r\n```\r\n\r\n2.) Given how many times we'll only have one query, I'd leave `createNamedRead()` alone and create a new `createNamedReads()` to house the new logic here. No point in creating even a singleton list if we don't have to."", 'commenter': 'maedhroz'}]"
2088,src/java/org/apache/cassandra/cql3/statements/TransactionStatement.java,"@@ -182,6 +182,24 @@ TxnNamedRead createNamedRead(NamedSelect namedSelect, QueryOptions options)
         return new TxnNamedRead(namedSelect.name, Iterables.getOnlyElement(selectQuery.queries));
     }
 
+    List<TxnNamedRead> createNamedReads(NamedSelect namedSelect, QueryOptions options)
+    {
+        SelectStatement select = namedSelect.select;
+        ReadQuery readQuery = select.getQuery(options, 0);
+
+        // We reject reads from both LET and SELECT that do not specify a single row.","[{'comment': 'nit: can remove now', 'commenter': 'maedhroz'}]"
2088,src/java/org/apache/cassandra/cql3/statements/TransactionStatement.java,"@@ -195,9 +213,10 @@ private List<TxnNamedRead> createNamedReads(QueryOptions options, Consumer<Key>
 
         if (returningSelect != null)
         {
-            TxnNamedRead read = createNamedRead(returningSelect, options);
-            keyConsumer.accept(read.key());
-            reads.add(read);
+            for (TxnNamedRead read : createNamedReads(returningSelect, options)) {","[{'comment': 'nit: newline before brace?', 'commenter': 'maedhroz'}]"
2104,src/java/org/apache/cassandra/streaming/SessionInfo.java,"@@ -47,24 +47,28 @@ public final class SessionInfo implements Serializable
     private final Map<String, ProgressInfo> receivingFiles = new ConcurrentHashMap<>();
     private final Map<String, ProgressInfo> sendingFiles = new ConcurrentHashMap<>();
 
+    public final String failurereason;","[{'comment': 'nit: `failureReason` camel-case for members/arguments.', 'commenter': 'jonmeredith'}]"
2104,src/java/org/apache/cassandra/streaming/StreamResultFuture.java,"@@ -239,8 +239,14 @@ private synchronized void maybeComplete()
             StreamState finalState = getCurrentState();
             if (finalState.hasFailedSession())
             {
-                logger.warn(""[Stream #{}] Stream failed"", planId);
-                tryFailure(new StreamException(finalState, ""Stream failed""));
+                StringBuilder stringBuilder = new StringBuilder();
+                for (SessionInfo info : finalState.getAllSessionInfo())
+                {
+                    if (info.isFailed())
+                        stringBuilder.append(""\nSession peer "").append(info.peer).append(info.failurereason);
+                }
+                logger.warn(""[Stream #{}] Stream failed: {}"", planId, stringBuilder);
+                tryFailure(new StreamException(finalState, ""Stream failed: "" + stringBuilder));","[{'comment': 'nit: Doesn\'t really matter for performance as not called frequently, but you could include the common `""String failed:""` in the string builder. Could also convert to something like `String message = stringBuilder.toString();`, rather than calling toString twice and allocating two String objects.', 'commenter': 'jonmeredith'}, {'comment': '> but you dould include the common ""String failed:"" in the string builder.\r\n\r\nI don\'t see it added to the string builder, but do think for getting rid of copy/paste it should be added.\r\n\r\nI would do the following at line 243\r\n\r\n```\r\nstringBuilder.append(""Stream failed:"");\r\n```\r\n\r\nthen get rid of ""Stream failed:"" in logger and exception.', 'commenter': 'dcapwell'}, {'comment': '> Could also convert to something like String message = stringBuilder.toString();\r\n\r\nYes please!', 'commenter': 'dcapwell'}]"
2104,src/java/org/apache/cassandra/streaming/StreamSession.java,"@@ -514,13 +517,20 @@ synchronized void addTransferStreams(Collection<OutgoingStream> streams)
         }
     }
 
-    private synchronized Future<?> closeSession(State finalState)
+    private Future<?> closeSession(State finalState)
+    {
+        return closeSession(finalState, null);
+    }
+
+    private synchronized Future<?> closeSession(State finalState, String reason)","[{'comment': 'nit: maybe use `failureReason` if only used for failures, not other messaging.', 'commenter': 'jonmeredith'}, {'comment': ""agree, also this is a single failure and not plural, so the variable shouldn't be plural"", 'commenter': 'dcapwell'}]"
2104,src/java/org/apache/cassandra/streaming/StreamSession.java,"@@ -694,8 +702,7 @@ public Future<?> onError(Throwable e)
             state(State.FAILED); // make sure subsequent error handling sees the session in a final state 
             channel.sendControlMessage(new SessionFailedMessage()).awaitUninterruptibly();
         }
-
-        return closeSession(State.FAILED);
+        return closeSession(State.FAILED, ""Failed because of an unkown exception;\n"" + Throwables.getStackTraceAsString(e));","[{'comment': ""nit: typo `unkown` - the exception isn't really unknown, it's just not EOF.\r\n\r\nThe details of the exception aren't provided, only the stack trace which might lose info. We've already logged the main error details including the stacktrace, so a summary with just the name/cause may be more useful when debugging.\r\n\r\nThe summary message above is also going to say there was a failure, so maybe it's good enough to just convert the exception to a String which would include any message with it? "", 'commenter': 'jonmeredith'}, {'comment': ""> The details of the exception aren't provided, only the stack trace which might lose info. We've already logged the main error details including the stacktrace, so a summary with just the name/cause may be more useful when debugging.\r\n\r\nwhat details are you looking for?  What info is lost?\r\n\r\nWe may have logged the error, but that can be hard in debugging as we need to link 2 logs or a log with a vtable... IMO its best to include the exception so while debugging everything is in one place"", 'commenter': 'dcapwell'}, {'comment': ""I think it's just printing the stack trace, not any message reason. Including the stacktrace in the message seems like it would generate a lot of output and be hard to read, but perhaps that's worth it."", 'commenter': 'jonmeredith'}]"
2104,src/java/org/apache/cassandra/streaming/StreamSession.java,"@@ -1107,7 +1114,7 @@ private void initiatorCompleteOrWait()
     public synchronized void sessionFailed()
     {
         logger.error(""[Stream #{}] Remote peer {} failed stream session."", planId(), peer.toString());
-        closeSession(State.FAILED);
+        closeSession(State.FAILED,"" Remote peer "" + peer + "" failed stream session"");","[{'comment': 'nit: formatting\r\n```suggestion\r\n        closeSession(State.FAILED, ""Remote peer "" + peer + "" failed stream session"");\r\n```\r\n\r\nLooks like there\'s some inconsistent formatting of the whitespace at the start of `failureReasons` - it\'s probably better to have the string builder add it between peers as it may be forgotten when adding new messages.', 'commenter': 'jonmeredith'}]"
2104,src/java/org/apache/cassandra/streaming/StreamSession.java,"@@ -1116,7 +1123,7 @@ public synchronized void sessionFailed()
     public synchronized void sessionTimeout()
     {
         logger.error(""[Stream #{}] timeout with {}."", planId(), peer.toString());
-        closeSession(State.FAILED);
+        closeSession(State.FAILED, "" Failed because the session timed out"");","[{'comment': '```suggestion\r\n        closeSession(State.FAILED, ""Session timed out"");\r\n```', 'commenter': 'jonmeredith'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException","[{'comment': '```suggestion\r\n    private void streamTimeoutTest(String reason) throws IOException\r\n```', 'commenter': 'jonmeredith'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BB::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""timeout_delay"", ""1ms""))
+                                      .start())
+        {
+
+            init(cluster);
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            ForkJoinPool.commonPool().execute(() -> triggerStreaming(cluster, true));
+            State.STREAM_IS_RUNNING.await();
+            logger.info(""Streaming is running... time to wake it up"");
+            State.UNBLOCK_STREAM.signal();
+
+            IInvokableInstance failingNode = cluster.get(1);
+
+            searchForLog(failingNode, reason);
+
+","[{'comment': 'nit: extra blank lines', 'commenter': 'jonmeredith'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;","[{'comment': 'nit:  snake-case for constants may make it a little easier to read - `FAILING_NODE`', 'commenter': 'jonmeredith'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BB::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""timeout_delay"", ""1ms""))
+                                      .start())
+        {
+
+            init(cluster);
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            ForkJoinPool.commonPool().execute(() -> triggerStreaming(cluster, true));
+            State.STREAM_IS_RUNNING.await();
+            logger.info(""Streaming is running... time to wake it up"");
+            State.UNBLOCK_STREAM.signal();
+
+            IInvokableInstance failingNode = cluster.get(1);
+
+            searchForLog(failingNode, reason);
+
+
+        }
+    }
+
+
+    private void triggerStreaming(Cluster cluster, boolean expectedEntireSSTable)
+    {
+        IInvokableInstance node1 = cluster.get(1);
+        IInvokableInstance node2 = cluster.get(2);
+
+        // repair will do streaming IFF there is a mismatch; so cause one
+        for (int i = 0; i < 10; i++)
+            node1.executeInternal(withKeyspace(""INSERT INTO %s.tbl (pk) VALUES (?)""), i); // timestamp won't match, causing a mismatch
+
+        // trigger streaming; expected to fail as streaming socket closed in the middle (currently this is an unrecoverable event)
+        //Blocks until the stream is complete
+        node2.nodetoolResult(""repair"", ""-full"", KEYSPACE, ""tbl"").asserts().failure();
+    }
+
+    private void searchForLog(IInvokableInstance failingNode, String reason)
+    {
+        LogResult<List<String>> result = failingNode.logs().grepForErrors(-1, Pattern.compile(""Stream failed:""));
+        // grepForErrors will include all ERROR logs even if they don't match the pattern; for this reason need to filter after the fact
+        List<String> matches = result.getResult();
+
+        matches = matches.stream().filter(s -> s.startsWith(""WARN"") && s.contains(""Stream failed"")).collect(Collectors.toList());","[{'comment': 'How do you know this is your `Stream failed` message rather than something else just containing the same string?', 'commenter': 'jonmeredith'}, {'comment': ""wouldn't `Stream failed` match everything?  We explicitly searched for that... so everything *must* contain that string."", 'commenter': 'dcapwell'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BB::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""timeout_delay"", ""1ms""))
+                                      .start())
+        {
+
+            init(cluster);
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            ForkJoinPool.commonPool().execute(() -> triggerStreaming(cluster, true));
+            State.STREAM_IS_RUNNING.await();
+            logger.info(""Streaming is running... time to wake it up"");
+            State.UNBLOCK_STREAM.signal();
+
+            IInvokableInstance failingNode = cluster.get(1);
+
+            searchForLog(failingNode, reason);
+
+
+        }
+    }
+
+
+    private void triggerStreaming(Cluster cluster, boolean expectedEntireSSTable)
+    {
+        IInvokableInstance node1 = cluster.get(1);
+        IInvokableInstance node2 = cluster.get(2);
+
+        // repair will do streaming IFF there is a mismatch; so cause one
+        for (int i = 0; i < 10; i++)
+            node1.executeInternal(withKeyspace(""INSERT INTO %s.tbl (pk) VALUES (?)""), i); // timestamp won't match, causing a mismatch
+
+        // trigger streaming; expected to fail as streaming socket closed in the middle (currently this is an unrecoverable event)
+        //Blocks until the stream is complete
+        node2.nodetoolResult(""repair"", ""-full"", KEYSPACE, ""tbl"").asserts().failure();
+    }
+
+    private void searchForLog(IInvokableInstance failingNode, String reason)
+    {
+        LogResult<List<String>> result = failingNode.logs().grepForErrors(-1, Pattern.compile(""Stream failed:""));
+        // grepForErrors will include all ERROR logs even if they don't match the pattern; for this reason need to filter after the fact
+        List<String> matches = result.getResult();
+
+        matches = matches.stream().filter(s -> s.startsWith(""WARN"") && s.contains(""Stream failed"")).collect(Collectors.toList());
+        logger.info(""Stream failed logs found: {}"", String.join(""\n"", matches));
+
+        Assertions.assertThat(matches)
+                  .describedAs(""node%d expected 1 element but was not true"", failingNode.config().num()).hasSize(1);
+        String logLine = matches.get(0);
+        Assertions.assertThat(logLine).contains(reason);
+
+        Matcher match = Pattern.compile("".*\\[Stream #(.*)\\]"").matcher(logLine);
+        if (!match.find()) throw new AssertionError(""Unable to parse: "" + logLine);
+        UUID planId = UUID.fromString(match.group(1));
+        SimpleQueryResult qr = failingNode.executeInternalWithResult(""SELECT * FROM system_views.streaming WHERE id=?"", planId);
+        Assertions.assertThat(qr.hasNext()).isTrue();
+        Assertions.assertThat(qr.next().getString(""failure_cause"")).contains(reason);
+    }
+
+    @Shared
+    public static class TestCondition
+    {
+        private volatile boolean signaled = false;
+
+        public void await()
+        {
+            while (!signaled)
+            {
+                synchronized (this)
+                {
+                    try
+                    {
+                        this.wait();
+                    }
+                    catch (InterruptedException e)
+                    {
+                        throw new AssertionError(e);
+                    }
+                }
+            }
+        }
+
+        public void signal()
+        {
+            signaled = true;
+            synchronized (this)
+            {
+                this.notify();
+            }
+        }
+    }
+
+    @Shared
+    public static class State
+    {
+        public static final TestCondition STREAM_IS_RUNNING = new TestCondition();
+        public static final TestCondition UNBLOCK_STREAM = new TestCondition();
+
+    }
+
+    public static class BB
+    {
+        @SuppressWarnings(""unused"")
+        public static int writeDirectlyToChannel(ByteBuffer buf, @SuperCall Callable<Integer> zuper) throws Exception
+        {
+            if (isCaller(BigTableZeroCopyWriter.class.getName(), ""write""))
+            {
+                State.STREAM_IS_RUNNING.signal();
+                State.UNBLOCK_STREAM.await();
+            }
+            // different context; pass through
+            return zuper.call();
+        }
+
+        @SuppressWarnings(""unused"")
+        public static boolean append(UnfilteredRowIterator partition, @SuperCall Callable<Boolean> zuper) throws Exception
+        {
+            if (isCaller(CassandraIncomingFile.class.getName(), ""read"")) // handles compressed and non-compressed
+                throw new java.nio.channels.ClosedChannelException();
+            // different context; pass through
+            return zuper.call();
+        }
+
+        private static boolean isCaller(String klass, String method)
+        {
+            //TODO is there a cleaner way to check this?
+            StackTraceElement[] stack = Thread.currentThread().getStackTrace();
+            System.out.println(""CURRENTTTT THREAD: "" + Thread.currentThread().getName());","[{'comment': ""debug msg? We don't normally output logging to stdout - not sure if any of the linters will complain."", 'commenter': 'jonmeredith'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BB::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""timeout_delay"", ""1ms""))
+                                      .start())
+        {
+
+            init(cluster);
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            ForkJoinPool.commonPool().execute(() -> triggerStreaming(cluster, true));
+            State.STREAM_IS_RUNNING.await();
+            logger.info(""Streaming is running... time to wake it up"");
+            State.UNBLOCK_STREAM.signal();
+
+            IInvokableInstance failingNode = cluster.get(1);
+
+            searchForLog(failingNode, reason);
+
+
+        }
+    }
+
+
+    private void triggerStreaming(Cluster cluster, boolean expectedEntireSSTable)
+    {
+        IInvokableInstance node1 = cluster.get(1);
+        IInvokableInstance node2 = cluster.get(2);
+
+        // repair will do streaming IFF there is a mismatch; so cause one
+        for (int i = 0; i < 10; i++)
+            node1.executeInternal(withKeyspace(""INSERT INTO %s.tbl (pk) VALUES (?)""), i); // timestamp won't match, causing a mismatch
+
+        // trigger streaming; expected to fail as streaming socket closed in the middle (currently this is an unrecoverable event)
+        //Blocks until the stream is complete
+        node2.nodetoolResult(""repair"", ""-full"", KEYSPACE, ""tbl"").asserts().failure();
+    }
+
+    private void searchForLog(IInvokableInstance failingNode, String reason)
+    {
+        LogResult<List<String>> result = failingNode.logs().grepForErrors(-1, Pattern.compile(""Stream failed:""));
+        // grepForErrors will include all ERROR logs even if they don't match the pattern; for this reason need to filter after the fact
+        List<String> matches = result.getResult();
+
+        matches = matches.stream().filter(s -> s.startsWith(""WARN"") && s.contains(""Stream failed"")).collect(Collectors.toList());
+        logger.info(""Stream failed logs found: {}"", String.join(""\n"", matches));
+
+        Assertions.assertThat(matches)
+                  .describedAs(""node%d expected 1 element but was not true"", failingNode.config().num()).hasSize(1);
+        String logLine = matches.get(0);
+        Assertions.assertThat(logLine).contains(reason);
+
+        Matcher match = Pattern.compile("".*\\[Stream #(.*)\\]"").matcher(logLine);
+        if (!match.find()) throw new AssertionError(""Unable to parse: "" + logLine);
+        UUID planId = UUID.fromString(match.group(1));
+        SimpleQueryResult qr = failingNode.executeInternalWithResult(""SELECT * FROM system_views.streaming WHERE id=?"", planId);
+        Assertions.assertThat(qr.hasNext()).isTrue();
+        Assertions.assertThat(qr.next().getString(""failure_cause"")).contains(reason);
+    }
+
+    @Shared
+    public static class TestCondition
+    {
+        private volatile boolean signaled = false;
+
+        public void await()
+        {
+            while (!signaled)
+            {
+                synchronized (this)
+                {
+                    try
+                    {
+                        this.wait();
+                    }
+                    catch (InterruptedException e)
+                    {
+                        throw new AssertionError(e);
+                    }
+                }
+            }
+        }
+
+        public void signal()
+        {
+            signaled = true;
+            synchronized (this)
+            {
+                this.notify();
+            }
+        }
+    }
+
+    @Shared
+    public static class State
+    {
+        public static final TestCondition STREAM_IS_RUNNING = new TestCondition();
+        public static final TestCondition UNBLOCK_STREAM = new TestCondition();
+
+    }
+
+    public static class BB
+    {
+        @SuppressWarnings(""unused"")
+        public static int writeDirectlyToChannel(ByteBuffer buf, @SuperCall Callable<Integer> zuper) throws Exception
+        {
+            if (isCaller(BigTableZeroCopyWriter.class.getName(), ""write""))
+            {
+                State.STREAM_IS_RUNNING.signal();
+                State.UNBLOCK_STREAM.await();
+            }
+            // different context; pass through
+            return zuper.call();
+        }
+
+        @SuppressWarnings(""unused"")
+        public static boolean append(UnfilteredRowIterator partition, @SuperCall Callable<Boolean> zuper) throws Exception
+        {
+            if (isCaller(CassandraIncomingFile.class.getName(), ""read"")) // handles compressed and non-compressed
+                throw new java.nio.channels.ClosedChannelException();
+            // different context; pass through
+            return zuper.call();
+        }
+
+        private static boolean isCaller(String klass, String method)
+        {
+            //TODO is there a cleaner way to check this?
+            StackTraceElement[] stack = Thread.currentThread().getStackTrace();
+            System.out.println(""CURRENTTTT THREAD: "" + Thread.currentThread().getName());
+            for (int i = 0; i < stack.length; i++)
+            {
+                StackTraceElement e = stack[i];
+                System.out.println(""KLASS: "" + klass + "" e.getClassName "" + e.getClassName() + "" e.getMethodName "" + e.getMethodName());
+                if (klass.equals(e.getClassName()) && method.equals(e.getMethodName()))
+                    return true;
+            }
+            return false;
+        }
+
+        public static void install(ClassLoader classLoader, Integer num)
+        {
+            if (num != FAILINGNODE)
+                return;
+            new ByteBuddy().rebase(SequentialWriter.class)
+                           .method(named(""writeDirectlyToChannel"").and(takesArguments(1)))
+                           .intercept(MethodDelegation.to(BB.class))
+                           .make()
+                           .load(classLoader, ClassLoadingStrategy.Default.INJECTION);
+
+            new ByteBuddy().rebase(RangeAwareSSTableWriter.class)
+                           .method(named(""append"").and(takesArguments(1)))
+                           .intercept(MethodDelegation.to(BB.class))
+                           .make()
+                           .load(classLoader, ClassLoadingStrategy.Default.INJECTION);
+        }
+
+    }
+
+    public static class BBHelper
+    {
+        @SuppressWarnings(""unused"")
+        public static int writeDirectlyToChannel(ByteBuffer buf, @SuperCall Callable<Integer> zuper) throws Exception
+        {
+            if (isCaller(BigTableZeroCopyWriter.class.getName(), ""write""))
+                //throw new java.nio.channels.ClosedChannelException();
+                throw new RuntimeException(""TEST"");
+            // different context; pass through
+            return zuper.call();
+        }
+
+        @SuppressWarnings(""unused"")
+        public static boolean append(UnfilteredRowIterator partition, @SuperCall Callable<Boolean> zuper) throws Exception
+        {
+            if (isCaller(CassandraIncomingFile.class.getName(), ""read"")) // handles compressed and non-compressed
+                throw new java.nio.channels.ClosedChannelException();
+            // different context; pass through
+            return zuper.call();
+        }
+
+        private static boolean isCaller(String klass, String method)
+        {
+            //TODO is there a cleaner way to check this?
+            StackTraceElement[] stack = Thread.currentThread().getStackTrace();
+            System.out.println(""CURRENT THREAD: "" + Thread.currentThread().getName());","[{'comment': 'debug msg?', 'commenter': 'jonmeredith'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BB::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""timeout_delay"", ""1ms""))
+                                      .start())
+        {
+
+            init(cluster);
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            ForkJoinPool.commonPool().execute(() -> triggerStreaming(cluster, true));
+            State.STREAM_IS_RUNNING.await();
+            logger.info(""Streaming is running... time to wake it up"");
+            State.UNBLOCK_STREAM.signal();
+
+            IInvokableInstance failingNode = cluster.get(1);
+
+            searchForLog(failingNode, reason);
+
+
+        }
+    }
+
+
+    private void triggerStreaming(Cluster cluster, boolean expectedEntireSSTable)
+    {
+        IInvokableInstance node1 = cluster.get(1);
+        IInvokableInstance node2 = cluster.get(2);
+
+        // repair will do streaming IFF there is a mismatch; so cause one
+        for (int i = 0; i < 10; i++)
+            node1.executeInternal(withKeyspace(""INSERT INTO %s.tbl (pk) VALUES (?)""), i); // timestamp won't match, causing a mismatch
+
+        // trigger streaming; expected to fail as streaming socket closed in the middle (currently this is an unrecoverable event)
+        //Blocks until the stream is complete
+        node2.nodetoolResult(""repair"", ""-full"", KEYSPACE, ""tbl"").asserts().failure();
+    }
+
+    private void searchForLog(IInvokableInstance failingNode, String reason)
+    {
+        LogResult<List<String>> result = failingNode.logs().grepForErrors(-1, Pattern.compile(""Stream failed:""));
+        // grepForErrors will include all ERROR logs even if they don't match the pattern; for this reason need to filter after the fact
+        List<String> matches = result.getResult();
+
+        matches = matches.stream().filter(s -> s.startsWith(""WARN"") && s.contains(""Stream failed"")).collect(Collectors.toList());
+        logger.info(""Stream failed logs found: {}"", String.join(""\n"", matches));
+
+        Assertions.assertThat(matches)
+                  .describedAs(""node%d expected 1 element but was not true"", failingNode.config().num()).hasSize(1);
+        String logLine = matches.get(0);
+        Assertions.assertThat(logLine).contains(reason);
+
+        Matcher match = Pattern.compile("".*\\[Stream #(.*)\\]"").matcher(logLine);
+        if (!match.find()) throw new AssertionError(""Unable to parse: "" + logLine);
+        UUID planId = UUID.fromString(match.group(1));
+        SimpleQueryResult qr = failingNode.executeInternalWithResult(""SELECT * FROM system_views.streaming WHERE id=?"", planId);
+        Assertions.assertThat(qr.hasNext()).isTrue();
+        Assertions.assertThat(qr.next().getString(""failure_cause"")).contains(reason);
+    }
+
+    @Shared
+    public static class TestCondition
+    {
+        private volatile boolean signaled = false;
+
+        public void await()
+        {
+            while (!signaled)
+            {
+                synchronized (this)
+                {
+                    try
+                    {
+                        this.wait();
+                    }
+                    catch (InterruptedException e)
+                    {
+                        throw new AssertionError(e);
+                    }
+                }
+            }
+        }
+
+        public void signal()
+        {
+            signaled = true;
+            synchronized (this)
+            {
+                this.notify();
+            }
+        }
+    }
+
+    @Shared
+    public static class State
+    {
+        public static final TestCondition STREAM_IS_RUNNING = new TestCondition();
+        public static final TestCondition UNBLOCK_STREAM = new TestCondition();
+
+    }
+
+    public static class BB
+    {
+        @SuppressWarnings(""unused"")
+        public static int writeDirectlyToChannel(ByteBuffer buf, @SuperCall Callable<Integer> zuper) throws Exception
+        {
+            if (isCaller(BigTableZeroCopyWriter.class.getName(), ""write""))
+            {
+                State.STREAM_IS_RUNNING.signal();
+                State.UNBLOCK_STREAM.await();
+            }
+            // different context; pass through
+            return zuper.call();
+        }
+
+        @SuppressWarnings(""unused"")
+        public static boolean append(UnfilteredRowIterator partition, @SuperCall Callable<Boolean> zuper) throws Exception
+        {
+            if (isCaller(CassandraIncomingFile.class.getName(), ""read"")) // handles compressed and non-compressed
+                throw new java.nio.channels.ClosedChannelException();
+            // different context; pass through
+            return zuper.call();
+        }
+
+        private static boolean isCaller(String klass, String method)
+        {
+            //TODO is there a cleaner way to check this?
+            StackTraceElement[] stack = Thread.currentThread().getStackTrace();
+            System.out.println(""CURRENTTTT THREAD: "" + Thread.currentThread().getName());
+            for (int i = 0; i < stack.length; i++)
+            {
+                StackTraceElement e = stack[i];
+                System.out.println(""KLASS: "" + klass + "" e.getClassName "" + e.getClassName() + "" e.getMethodName "" + e.getMethodName());
+                if (klass.equals(e.getClassName()) && method.equals(e.getMethodName()))
+                    return true;
+            }
+            return false;
+        }
+
+        public static void install(ClassLoader classLoader, Integer num)
+        {
+            if (num != FAILINGNODE)
+                return;
+            new ByteBuddy().rebase(SequentialWriter.class)
+                           .method(named(""writeDirectlyToChannel"").and(takesArguments(1)))
+                           .intercept(MethodDelegation.to(BB.class))
+                           .make()
+                           .load(classLoader, ClassLoadingStrategy.Default.INJECTION);
+
+            new ByteBuddy().rebase(RangeAwareSSTableWriter.class)
+                           .method(named(""append"").and(takesArguments(1)))
+                           .intercept(MethodDelegation.to(BB.class))
+                           .make()
+                           .load(classLoader, ClassLoadingStrategy.Default.INJECTION);
+        }
+
+    }
+
+    public static class BBHelper
+    {
+        @SuppressWarnings(""unused"")
+        public static int writeDirectlyToChannel(ByteBuffer buf, @SuperCall Callable<Integer> zuper) throws Exception
+        {
+            if (isCaller(BigTableZeroCopyWriter.class.getName(), ""write""))
+                //throw new java.nio.channels.ClosedChannelException();
+                throw new RuntimeException(""TEST"");
+            // different context; pass through
+            return zuper.call();
+        }
+
+        @SuppressWarnings(""unused"")
+        public static boolean append(UnfilteredRowIterator partition, @SuperCall Callable<Boolean> zuper) throws Exception
+        {
+            if (isCaller(CassandraIncomingFile.class.getName(), ""read"")) // handles compressed and non-compressed
+                throw new java.nio.channels.ClosedChannelException();
+            // different context; pass through
+            return zuper.call();
+        }
+
+        private static boolean isCaller(String klass, String method)
+        {
+            //TODO is there a cleaner way to check this?
+            StackTraceElement[] stack = Thread.currentThread().getStackTrace();
+            System.out.println(""CURRENT THREAD: "" + Thread.currentThread().getName());
+            for (int i = 0; i < stack.length; i++)
+            {
+                StackTraceElement e = stack[i];
+                System.out.println(""KLASS: "" + klass + "" e.getClassName "" + e.getClassName() + "" e.getMethodName "" + e.getMethodName());","[{'comment': 'debug msg?', 'commenter': 'jonmeredith'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BB::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""timeout_delay"", ""1ms""))
+                                      .start())
+        {
+
+            init(cluster);
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            ForkJoinPool.commonPool().execute(() -> triggerStreaming(cluster, true));
+            State.STREAM_IS_RUNNING.await();
+            logger.info(""Streaming is running... time to wake it up"");
+            State.UNBLOCK_STREAM.signal();
+
+            IInvokableInstance failingNode = cluster.get(1);
+
+            searchForLog(failingNode, reason);
+
+
+        }
+    }
+
+
+    private void triggerStreaming(Cluster cluster, boolean expectedEntireSSTable)
+    {
+        IInvokableInstance node1 = cluster.get(1);
+        IInvokableInstance node2 = cluster.get(2);
+
+        // repair will do streaming IFF there is a mismatch; so cause one
+        for (int i = 0; i < 10; i++)
+            node1.executeInternal(withKeyspace(""INSERT INTO %s.tbl (pk) VALUES (?)""), i); // timestamp won't match, causing a mismatch
+
+        // trigger streaming; expected to fail as streaming socket closed in the middle (currently this is an unrecoverable event)
+        //Blocks until the stream is complete
+        node2.nodetoolResult(""repair"", ""-full"", KEYSPACE, ""tbl"").asserts().failure();
+    }
+
+    private void searchForLog(IInvokableInstance failingNode, String reason)
+    {
+        LogResult<List<String>> result = failingNode.logs().grepForErrors(-1, Pattern.compile(""Stream failed:""));
+        // grepForErrors will include all ERROR logs even if they don't match the pattern; for this reason need to filter after the fact
+        List<String> matches = result.getResult();
+
+        matches = matches.stream().filter(s -> s.startsWith(""WARN"") && s.contains(""Stream failed"")).collect(Collectors.toList());
+        logger.info(""Stream failed logs found: {}"", String.join(""\n"", matches));
+
+        Assertions.assertThat(matches)
+                  .describedAs(""node%d expected 1 element but was not true"", failingNode.config().num()).hasSize(1);
+        String logLine = matches.get(0);
+        Assertions.assertThat(logLine).contains(reason);
+
+        Matcher match = Pattern.compile("".*\\[Stream #(.*)\\]"").matcher(logLine);
+        if (!match.find()) throw new AssertionError(""Unable to parse: "" + logLine);
+        UUID planId = UUID.fromString(match.group(1));
+        SimpleQueryResult qr = failingNode.executeInternalWithResult(""SELECT * FROM system_views.streaming WHERE id=?"", planId);
+        Assertions.assertThat(qr.hasNext()).isTrue();
+        Assertions.assertThat(qr.next().getString(""failure_cause"")).contains(reason);
+    }
+
+    @Shared
+    public static class TestCondition
+    {
+        private volatile boolean signaled = false;
+
+        public void await()
+        {
+            while (!signaled)
+            {
+                synchronized (this)
+                {
+                    try
+                    {
+                        this.wait();
+                    }
+                    catch (InterruptedException e)
+                    {
+                        throw new AssertionError(e);
+                    }
+                }
+            }
+        }
+
+        public void signal()
+        {
+            signaled = true;
+            synchronized (this)
+            {
+                this.notify();
+            }
+        }
+    }
+
+    @Shared
+    public static class State
+    {
+        public static final TestCondition STREAM_IS_RUNNING = new TestCondition();
+        public static final TestCondition UNBLOCK_STREAM = new TestCondition();
+
+    }
+
+    public static class BB
+    {
+        @SuppressWarnings(""unused"")
+        public static int writeDirectlyToChannel(ByteBuffer buf, @SuperCall Callable<Integer> zuper) throws Exception
+        {
+            if (isCaller(BigTableZeroCopyWriter.class.getName(), ""write""))
+            {
+                State.STREAM_IS_RUNNING.signal();
+                State.UNBLOCK_STREAM.await();
+            }
+            // different context; pass through
+            return zuper.call();
+        }
+
+        @SuppressWarnings(""unused"")
+        public static boolean append(UnfilteredRowIterator partition, @SuperCall Callable<Boolean> zuper) throws Exception
+        {
+            if (isCaller(CassandraIncomingFile.class.getName(), ""read"")) // handles compressed and non-compressed
+                throw new java.nio.channels.ClosedChannelException();
+            // different context; pass through
+            return zuper.call();
+        }
+
+        private static boolean isCaller(String klass, String method)
+        {
+            //TODO is there a cleaner way to check this?
+            StackTraceElement[] stack = Thread.currentThread().getStackTrace();
+            System.out.println(""CURRENTTTT THREAD: "" + Thread.currentThread().getName());
+            for (int i = 0; i < stack.length; i++)
+            {
+                StackTraceElement e = stack[i];
+                System.out.println(""KLASS: "" + klass + "" e.getClassName "" + e.getClassName() + "" e.getMethodName "" + e.getMethodName());","[{'comment': 'debug msg?', 'commenter': 'jonmeredith'}]"
2104,src/java/org/apache/cassandra/config/Config.java,"@@ -160,6 +160,8 @@ public static Set<String> splitCommaDelimited(String src)
     @Replaces(oldName = ""slow_query_log_timeout_in_ms"", converter = Converters.MILLIS_DURATION_LONG, deprecated = true)
     public volatile DurationSpec.LongMillisecondsBound slow_query_log_timeout = new DurationSpec.LongMillisecondsBound(""500ms"");
 
+    public volatile DurationSpec.LongMillisecondsBound timeout_delay = new DurationSpec.LongMillisecondsBound(""43000000ms"");","[{'comment': 'Can we name this more specifically as there are many timeouts in Cassandra - e.g. `stream_transfer_task_timeout` and maybe the timeout could be a more readable `12h`? ', 'commenter': 'jonmeredith'}]"
2104,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -3416,6 +3416,11 @@ public static boolean streamEntireSSTables()
         return conf.stream_entire_sstables;
     }
 
+    public static long timeoutDelay()","[{'comment': 'the pattern is to be `getTimeoutDelay`, should follow the pattern', 'commenter': 'dcapwell'}]"
2104,src/java/org/apache/cassandra/streaming/SessionInfo.java,"@@ -205,4 +209,8 @@ public SessionSummary createSummary()
     {
         return new SessionSummary(FBUtilities.getBroadcastAddressAndPort(), peer, receivingSummaries, sendingSummaries);
     }
+
+    public String getFailurereason() {","[{'comment': 'should be `getFailureReason()`', 'commenter': 'dcapwell'}]"
2104,src/java/org/apache/cassandra/streaming/StreamSession.java,"@@ -154,7 +155,7 @@
  * (via {@link org.apache.cassandra.net.MessagingService}, while the actual files themselves are sent by a special
  * ""streaming"" connection type. See {@link StreamingMultiplexedChannel} for details. Because of the asynchronous
  */
-public class StreamSession implements IEndpointStateChangeSubscriber","[{'comment': ""can you cleanup imports?  We shouldn't have imports that are not used above"", 'commenter': 'dcapwell'}]"
2104,src/java/org/apache/cassandra/streaming/StreamSession.java,"@@ -682,8 +691,7 @@ public Future<?> onError(Throwable e)
                              planId(),
                              peer.getHostAddressAndPort(),
                              e);
-
-                return closeSession(State.FAILED);
+                return closeSession(State.FAILED, "" Failed because there was an "" + e.getClass().getCanonicalName() + "" with state="" + state.name());","[{'comment': 'can you remove the leading space?', 'commenter': 'dcapwell'}]"
2104,src/java/org/apache/cassandra/streaming/StreamSession.java,"@@ -981,7 +988,7 @@ public void streamSent(OutgoingStreamMessage message)
         StreamTransferTask task = transfers.get(message.header.tableId);
         if (task != null)
         {
-            task.scheduleTimeout(message.header.sequenceNumber, 12, TimeUnit.HOURS);
+            task.scheduleTimeout(message.header.sequenceNumber, DatabaseDescriptor.timeoutDelay(), TimeUnit.MILLISECONDS);","[{'comment': 'this is brittle, rather than expecting the config to be MILLISECONDS, we should have the DD return the Duration and convert to nanos; this allows users to define any value they want.', 'commenter': 'dcapwell'}]"
2104,src/java/org/apache/cassandra/streaming/management/SessionInfoCompositeData.java,"@@ -151,7 +151,7 @@ public StreamSummary apply(CompositeData input)
                                            connecting,
                                            fromArrayOfCompositeData((CompositeData[]) values[5], toStreamSummary),
                                            fromArrayOfCompositeData((CompositeData[]) values[6], toStreamSummary),
-                                           StreamSession.State.valueOf((String) values[7]));
+                                           StreamSession.State.valueOf((String) values[7]), null);","[{'comment': ""please document why you are using `null` here.  Its fine to say its null to maintain backwards compatibility, but its still best to say this so the next reader doesn't question why its null"", 'commenter': 'dcapwell'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))","[{'comment': 'why are you setting 3 tokens for a 2 node cluster?', 'commenter': 'dcapwell'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))","[{'comment': 'why 3 tokens for a 2 node cluster?', 'commenter': 'dcapwell'}]"
2104,test/distributed/org/apache/cassandra/distributed/test/streaming/StreamFailureTest.java,"@@ -0,0 +1,327 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.streaming;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ForkJoinPool;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import net.bytebuddy.ByteBuddy;
+import net.bytebuddy.dynamic.loading.ClassLoadingStrategy;
+import net.bytebuddy.implementation.MethodDelegation;
+import net.bytebuddy.implementation.bind.annotation.SuperCall;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.streaming.CassandraIncomingFile;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.LogResult;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter;
+import org.apache.cassandra.io.sstable.format.big.BigTableZeroCopyWriter;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.Shared;
+import org.assertj.core.api.Assertions;
+import static net.bytebuddy.matcher.ElementMatchers.named;
+import static net.bytebuddy.matcher.ElementMatchers.takesArguments;
+
+public class StreamFailureTest extends TestBaseImpl
+{
+
+    private static final int FAILINGNODE = 2;
+
+    private static final Logger logger = LoggerFactory.getLogger(StreamFailureTest.class);
+
+
+    @Test
+    public void failureInTheMiddleWithUnknown() throws IOException
+    {
+        streamTest(true, ""java.lang.RuntimeException: TEST"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureInTheMiddleWithEOF() throws IOException
+    {
+        streamTest(false, ""Session peer /127.0.0.1:7012 Failed because there was an java.nio.channels.ClosedChannelException with state=STREAMING"", FAILINGNODE);
+    }
+
+    @Test
+    public void failureDueToSessionFailed() throws IOException
+    {
+        streamTest(true,""Remote peer /127.0.0.2:7012 failed stream session"", 1);
+    }
+
+    @Test
+    public void failureDueToSessionTimeout() throws IOException
+    {
+        streamTest2(""Failed because the session timed out"");
+    }
+
+    private void streamTest(boolean zeroCopyStreaming, String reason, Integer failedNode) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BBHelper::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        .set(""stream_entire_sstables"", zeroCopyStreaming)
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""disk_failure_policy"", ""die""))
+                                      .start())
+        {
+            init(cluster);
+
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            triggerStreaming(cluster, zeroCopyStreaming);
+            // make sure disk failure policy is not triggered
+
+            IInvokableInstance failingNode = cluster.get(failedNode);
+
+            searchForLog(failingNode, reason);
+        }
+    }
+
+    private void streamTest2(String reason) throws IOException
+    {
+        try (Cluster cluster = Cluster.build(2)
+                                      .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(3))
+                                      .withInstanceInitializer(BB::install)
+                                      .withConfig(c -> c.with(Feature.values())
+                                                        // when die, this will try to halt JVM, which is easier to validate in the test
+                                                        // other levels require checking state of the subsystems
+                                                        .set(""timeout_delay"", ""1ms""))
+                                      .start())
+        {
+
+            init(cluster);
+            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int PRIMARY KEY)""));
+
+            ForkJoinPool.commonPool().execute(() -> triggerStreaming(cluster, true));
+            State.STREAM_IS_RUNNING.await();
+            logger.info(""Streaming is running... time to wake it up"");
+            State.UNBLOCK_STREAM.signal();
+
+            IInvokableInstance failingNode = cluster.get(1);
+
+            searchForLog(failingNode, reason);
+
+
+        }
+    }
+
+
+    private void triggerStreaming(Cluster cluster, boolean expectedEntireSSTable)
+    {
+        IInvokableInstance node1 = cluster.get(1);
+        IInvokableInstance node2 = cluster.get(2);
+
+        // repair will do streaming IFF there is a mismatch; so cause one
+        for (int i = 0; i < 10; i++)
+            node1.executeInternal(withKeyspace(""INSERT INTO %s.tbl (pk) VALUES (?)""), i); // timestamp won't match, causing a mismatch
+
+        // trigger streaming; expected to fail as streaming socket closed in the middle (currently this is an unrecoverable event)
+        //Blocks until the stream is complete
+        node2.nodetoolResult(""repair"", ""-full"", KEYSPACE, ""tbl"").asserts().failure();
+    }
+
+    private void searchForLog(IInvokableInstance failingNode, String reason)
+    {
+        LogResult<List<String>> result = failingNode.logs().grepForErrors(-1, Pattern.compile(""Stream failed:""));
+        // grepForErrors will include all ERROR logs even if they don't match the pattern; for this reason need to filter after the fact
+        List<String> matches = result.getResult();
+
+        matches = matches.stream().filter(s -> s.startsWith(""WARN"") && s.contains(""Stream failed"")).collect(Collectors.toList());
+        logger.info(""Stream failed logs found: {}"", String.join(""\n"", matches));
+
+        Assertions.assertThat(matches)
+                  .describedAs(""node%d expected 1 element but was not true"", failingNode.config().num()).hasSize(1);","[{'comment': 'nit: `.hasSize` should be next line', 'commenter': 'dcapwell'}]"
2109,src/java/org/apache/cassandra/io/sstable/CQLSSTableWriter.java,"@@ -247,7 +246,7 @@ public CQLSSTableWriter rawAddRow(List<ByteBuffer> values)
         ClientState state = ClientState.forInternalCalls();
         List<ByteBuffer> keys = modificationStatement.buildPartitionKeyNames(options, state);
 
-        long now = currentTimeMillis();
+        long now = Clock.Global.currentTimeMillis();","[{'comment': 'please revert this change, the common style is to static import `Global`, so should remove this style change', 'commenter': 'dcapwell'}, {'comment': '👍', 'commenter': 'JeetKunDoug'}]"
2109,test/unit/org/apache/cassandra/io/sstable/CQLSSTableWriterTest.java,"@@ -1116,6 +1118,122 @@ public void testFrozenSetTypeCustomOrdered() throws Exception
         assertEquals(0, filtered.size());
     }
 
+    @Test
+    public void testWriteWithTimestamps() throws Exception
+    {
+        long now = currentTimeMillis();
+        long then = now - 1000;
+        final String schema = ""CREATE TABLE "" + qualifiedTable + "" (""
+                              + ""  k int,""
+                              + ""  c1 int,""
+                              + ""  c2 int,""
+                              + ""  v text,""
+                              + ""  PRIMARY KEY (k)""","[{'comment': 'this is confusing, `c1` and `c2` normally means ""clustering column"" when you read the code, but this does not include it; can you rename these to v1, v2, and v3?  or change the test to make these clustering columns?', 'commenter': 'dcapwell'}]"
2109,test/unit/org/apache/cassandra/io/sstable/CQLSSTableWriterTest.java,"@@ -1116,6 +1118,122 @@ public void testFrozenSetTypeCustomOrdered() throws Exception
         assertEquals(0, filtered.size());
     }
 
+    @Test
+    public void testWriteWithTimestamps() throws Exception
+    {
+        long now = currentTimeMillis();
+        long then = now - 1000;
+        final String schema = ""CREATE TABLE "" + qualifiedTable + "" (""
+                              + ""  k int,""
+                              + ""  c1 int,""
+                              + ""  c2 int,""
+                              + ""  v text,""
+                              + ""  PRIMARY KEY (k)""
+                              + "")"";
+
+        CQLSSTableWriter writer = CQLSSTableWriter.builder()
+                                                  .inDirectory(dataDir)
+                                                  .forTable(schema)
+                                                  .using(""INSERT INTO "" + qualifiedTable +
+                                                         "" (k, c1, c2, v) VALUES (?,?,?,?) using timestamp ?"" )
+                                                  .build();
+
+        // Note that, all other things being equal, Cassandra will sort these rows lexicographically, so we use ""higher"" values in the
+        // row we expect to ""win"" so that we're sure that it isn't just accidentally picked due to the row sorting.
+        writer.addRow( 1, 4, 5, ""b"", now); // This write should be the one found at the end because it has a higher timestamp
+        writer.addRow( 1, 2, 3, ""a"", then);
+        writer.close();
+        loadSSTables(dataDir, keyspace);
+
+        UntypedResultSet resultSet = QueryProcessor.executeInternal(""SELECT * FROM "" + qualifiedTable);
+        assertEquals(1, resultSet.size());
+
+        Iterator<UntypedResultSet.Row> iter = resultSet.iterator();
+        UntypedResultSet.Row r1 = iter.next();
+        assertEquals(1, r1.getInt(""k""));
+        assertEquals(4, r1.getInt(""c1""));
+        assertEquals(5, r1.getInt(""c2""));
+        assertEquals(""b"", r1.getString(""v""));
+        assertFalse(iter.hasNext());
+    }
+    @Test
+    public void testWriteWithTtl() throws Exception
+    {
+        final String schema = ""CREATE TABLE "" + qualifiedTable + "" (""
+                              + ""  k int,""
+                              + ""  c1 int,""
+                              + ""  c2 int,""
+                              + ""  v text,""
+                              + ""  PRIMARY KEY (k)""","[{'comment': 'this is confusing, `c1` and `c2` normally means ""clustering column"" when you read the code, but this does not include it; can you rename these to v1, v2, and v3?  or change the test to make these clustering columns?', 'commenter': 'dcapwell'}]"
2109,test/unit/org/apache/cassandra/io/sstable/CQLSSTableWriterTest.java,"@@ -1116,6 +1118,122 @@ public void testFrozenSetTypeCustomOrdered() throws Exception
         assertEquals(0, filtered.size());
     }
 
+    @Test
+    public void testWriteWithTimestamps() throws Exception
+    {
+        long now = currentTimeMillis();
+        long then = now - 1000;
+        final String schema = ""CREATE TABLE "" + qualifiedTable + "" (""
+                              + ""  k int,""
+                              + ""  c1 int,""
+                              + ""  c2 int,""
+                              + ""  v text,""
+                              + ""  PRIMARY KEY (k)""
+                              + "")"";
+
+        CQLSSTableWriter writer = CQLSSTableWriter.builder()
+                                                  .inDirectory(dataDir)
+                                                  .forTable(schema)
+                                                  .using(""INSERT INTO "" + qualifiedTable +
+                                                         "" (k, c1, c2, v) VALUES (?,?,?,?) using timestamp ?"" )
+                                                  .build();
+
+        // Note that, all other things being equal, Cassandra will sort these rows lexicographically, so we use ""higher"" values in the
+        // row we expect to ""win"" so that we're sure that it isn't just accidentally picked due to the row sorting.
+        writer.addRow( 1, 4, 5, ""b"", now); // This write should be the one found at the end because it has a higher timestamp
+        writer.addRow( 1, 2, 3, ""a"", then);
+        writer.close();
+        loadSSTables(dataDir, keyspace);
+
+        UntypedResultSet resultSet = QueryProcessor.executeInternal(""SELECT * FROM "" + qualifiedTable);
+        assertEquals(1, resultSet.size());
+
+        Iterator<UntypedResultSet.Row> iter = resultSet.iterator();
+        UntypedResultSet.Row r1 = iter.next();
+        assertEquals(1, r1.getInt(""k""));
+        assertEquals(4, r1.getInt(""c1""));
+        assertEquals(5, r1.getInt(""c2""));
+        assertEquals(""b"", r1.getString(""v""));
+        assertFalse(iter.hasNext());
+    }
+    @Test
+    public void testWriteWithTtl() throws Exception
+    {
+        final String schema = ""CREATE TABLE "" + qualifiedTable + "" (""
+                              + ""  k int,""
+                              + ""  c1 int,""
+                              + ""  c2 int,""
+                              + ""  v text,""
+                              + ""  PRIMARY KEY (k)""
+                              + "")"";
+
+        CQLSSTableWriter.Builder builder = CQLSSTableWriter.builder()
+                                                         .inDirectory(dataDir)
+                                                         .forTable(schema)
+                                                         .using(""INSERT INTO "" + qualifiedTable +
+                                                                "" (k, c1, c2, v) VALUES (?,?,?,?) using TTL ?"");
+        CQLSSTableWriter writer = builder.build();
+        // add a row that _should_ show up - 1 hour TTL
+        writer.addRow( 1, 2, 3, ""a"", 3600);
+        // Insert a row with a TTL of 1 second - should not appear in results once we sleep
+        writer.addRow( 2, 4, 5, ""b"", 1);
+        writer.close();
+        Thread.sleep(1200); // Slightly over 1 second, just to make sure
+        loadSSTables(dataDir, keyspace);
+
+        UntypedResultSet resultSet = QueryProcessor.executeInternal(""SELECT * FROM "" + qualifiedTable);
+        assertEquals(1, resultSet.size());
+
+        Iterator<UntypedResultSet.Row> iter = resultSet.iterator();
+        UntypedResultSet.Row r1 = iter.next();
+        assertEquals(1, r1.getInt(""k""));
+        assertEquals(2, r1.getInt(""c1""));
+        assertEquals(3, r1.getInt(""c2""));
+        assertEquals(""a"", r1.getString(""v""));
+        assertFalse(iter.hasNext());
+    }
+    @Test
+    public void testWriteWithTimestampsAndTtl() throws Exception
+    {
+        final String schema = ""CREATE TABLE "" + qualifiedTable + "" (""
+                              + ""  k int,""
+                              + ""  c1 int,""
+                              + ""  c2 int,""
+                              + ""  v text,""
+                              + ""  PRIMARY KEY (k)""","[{'comment': 'this is confusing, `c1` and `c2` normally means ""clustering column"" when you read the code, but this does not include it; can you rename these to v1, v2, and v3?  or change the test to make these clustering columns?', 'commenter': 'dcapwell'}]"
2117,src/java/org/apache/cassandra/db/compaction/CompactionManager.java,"@@ -1848,7 +1848,7 @@ public void run()
                 }
             }
         };
-
+        ","[{'comment': 'this is not necessary, please return it back as it was.', 'commenter': 'smiklosovic'}]"
2117,src/java/org/apache/cassandra/db/virtual/SnapshotsTable.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import javax.management.openmbean.TabularData;
+import javax.management.openmbean.TabularDataSupport;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.db.SnapshotDetailsTabularData;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageService;
+
+public class SnapshotsTable extends AbstractVirtualTable
+{
+    private static final String SNAPSHOT_NAME = ""snapshot_name"";
+    private static final String KEYSPACE_NAME = ""keyspace_name"";
+    private static final String COLUMNFAMILY_NAME = ""columnfamily_name"";
+    private static final String TRUE_SIZE = ""true_size"";
+    private static final String SIZE_ON_DISK = ""size_on_disk"";
+    private static final String CREATE_TIME = ""created_at"";
+    private static final String EXPIRATION_TIME = ""expires_at"";
+    private static final String EPHEMERAL = ""ephemeral"";
+    
+    SnapshotsTable(String keyspace)
+    {
+        super(TableMetadata.builder(keyspace, ""snapshots"")
+                           .comment(""tables snapshots"")","[{'comment': 'I would change this to ""available snapshots"" or ""current snapshots"" or ""present snapshots""', 'commenter': 'smiklosovic'}]"
2117,src/java/org/apache/cassandra/db/virtual/SnapshotsTable.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import javax.management.openmbean.TabularData;
+import javax.management.openmbean.TabularDataSupport;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.db.SnapshotDetailsTabularData;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageService;
+
+public class SnapshotsTable extends AbstractVirtualTable
+{
+    private static final String SNAPSHOT_NAME = ""snapshot_name"";
+    private static final String KEYSPACE_NAME = ""keyspace_name"";
+    private static final String COLUMNFAMILY_NAME = ""columnfamily_name"";
+    private static final String TRUE_SIZE = ""true_size"";
+    private static final String SIZE_ON_DISK = ""size_on_disk"";
+    private static final String CREATE_TIME = ""created_at"";
+    private static final String EXPIRATION_TIME = ""expires_at"";
+    private static final String EPHEMERAL = ""ephemeral"";
+    
+    SnapshotsTable(String keyspace)
+    {
+        super(TableMetadata.builder(keyspace, ""snapshots"")
+                           .comment(""tables snapshots"")
+                           .kind(TableMetadata.Kind.VIRTUAL)
+                           .partitioner(new LocalPartitioner(UTF8Type.instance))
+                           .addPartitionKeyColumn(SNAPSHOT_NAME, UTF8Type.instance)
+                           .addClusteringColumn(KEYSPACE_NAME, UTF8Type.instance)
+                           .addClusteringColumn(COLUMNFAMILY_NAME, UTF8Type.instance)
+                           .addRegularColumn(TRUE_SIZE, LongType.instance)
+                           .addRegularColumn(SIZE_ON_DISK, LongType.instance)
+                           .addRegularColumn(CREATE_TIME, UTF8Type.instance)
+                           .addRegularColumn(EXPIRATION_TIME, UTF8Type.instance)
+                           .addRegularColumn(EPHEMERAL, BooleanType.instance)
+                           .build());
+    }
+  
+    @Override
+    public DataSet data() 
+    {
+        SimpleDataSet result = new SimpleDataSet(metadata());
+        
+        // include snapshots with ttl and ephemeral
+        Map<String, TabularData> snapshotDetails = StorageService.instance.getSnapshotDetails(ImmutableMap.of(""no_ttl"", ""false"", ""include_ephemeral"", ""true""));
+        for(Map.Entry<String, TabularData> entry : snapshotDetails.entrySet())","[{'comment': '`for (`', 'commenter': 'smiklosovic'}]"
2117,src/java/org/apache/cassandra/db/virtual/SnapshotsTable.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import javax.management.openmbean.TabularData;
+import javax.management.openmbean.TabularDataSupport;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.db.SnapshotDetailsTabularData;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageService;
+
+public class SnapshotsTable extends AbstractVirtualTable
+{
+    private static final String SNAPSHOT_NAME = ""snapshot_name"";
+    private static final String KEYSPACE_NAME = ""keyspace_name"";
+    private static final String COLUMNFAMILY_NAME = ""columnfamily_name"";
+    private static final String TRUE_SIZE = ""true_size"";
+    private static final String SIZE_ON_DISK = ""size_on_disk"";
+    private static final String CREATE_TIME = ""created_at"";
+    private static final String EXPIRATION_TIME = ""expires_at"";
+    private static final String EPHEMERAL = ""ephemeral"";
+    
+    SnapshotsTable(String keyspace)
+    {
+        super(TableMetadata.builder(keyspace, ""snapshots"")
+                           .comment(""tables snapshots"")
+                           .kind(TableMetadata.Kind.VIRTUAL)
+                           .partitioner(new LocalPartitioner(UTF8Type.instance))
+                           .addPartitionKeyColumn(SNAPSHOT_NAME, UTF8Type.instance)","[{'comment': 'I would change the key. Partition key would be keyspace and clustering column would be table and name of the snapshot. That way, you can ask:\r\n\r\n````\r\nuse system_views;\r\nselect * from snapshots;\r\nselect * from snapshots where keyspace = \'ks1\';\r\nselect * from snapshots where keyspace = \'ks1\' and table = \'tb1\';\r\nselect * from snapshots where keyspace = \'ks1\' and table = \'tb1\' and name = ""mysnapshot"";\r\nselect * from snapshots where name = ""mysnapshot"" allow filtering;\r\n````\r\n\r\nIn practice, I think it is more common to ask questions like ""what snapshots this keyspace / table"" has? Instead of querying that table by name of the particular snapshot. What if I do not know the name of the snapshot in advance?\r\n\r\n@pauloricardomg what do you think? I think this is the most important aspect of this PR.\r\n', 'commenter': 'smiklosovic'}, {'comment': '+1  on what you said ', 'commenter': 'Maxwell-Guo'}, {'comment': 'hi @Maxwell-Guo , any chance to rework this as discussed above?', 'commenter': 'smiklosovic'}, {'comment': 'hi @smiklosovic ，thanks so much. Busy these days so I feel sorry for the delay in replying to the message.\r\nI have already finished the above work.\r\nBesides I hope you can take a look at the reply of naming below.Thanks', 'commenter': 'Maxwell-Guo'}, {'comment': ""@pauloricardomg what do you think about column names? Should we follow the naming in nodetool's output? I think we should not be tied by it."", 'commenter': 'smiklosovic'}, {'comment': 'Sorry for the delay - have been busy in the past couple of weeks but will try to be faster on next iterations so we can wrap this up soon.\r\n\r\nI agree with with @smiklosovic comment about using more succint column names (ie. `keyspace` vs `keyspace_name` and `table` vs `columnfamily_name`), but I think we should use the column name ""tag"" and not ""name"" to refer to a snapshot identifier, and keep this as table partition key. We should enable allow filtering on this virtual table (with [CASSANDRA-18271](https://issues.apache.org/jira/browse/CASSANDRA-18271)) to allow querying by non partition keys (ie. `keyspace` or `table`). This would allow the following queries:\r\n\r\n```sql\r\nuse system_views;\r\nselect * from snapshots;\r\nselect * from snapshots where tag = ""mysnapshot"" # query by partition key\r\nselect * from snapshots where keyspace = \'ks1\'; # query by non-partition keys\r\nselect * from snapshots where keyspace = \'ks1\' and table = \'tb1\';\r\nselect * from snapshots where keyspace = \'ks1\' and table = \'tb1\' and tag = ""mysnapshot"";\r\n```\r\n\r\nWhat do you think?\r\n', 'commenter': 'pauloricardomg'}, {'comment': '> I think we should use the column name ""tag"" and not ""name"" to refer to a snapshot identifier\r\n\r\nEven though we have been using `tag` to refer to snapshot ids, this name is a bit misleading since a resource could in theory have multiple tags (but just one ID). @smiklosovic I wonder if it would make sense to just call it snapshot `id` and update internal usages to refer to snapshots by `id` and not `name/tag` ?\r\n\r\nUpdated queries with `id` colum name for snapshot identifier:\r\n```sql\r\nuse system_views;\r\nselect * from snapshots;\r\nselect * from snapshots where id = ""mysnapshot"" # query by partition key\r\nselect * from snapshots where keyspace = \'ks1\'; # query by non-partition keys\r\nselect * from snapshots where keyspace = \'ks1\' and table = \'tb1\';\r\nselect * from snapshots where keyspace = \'ks1\' and table = \'tb1\' and id = ""mysnapshot"";\r\n```', 'commenter': 'pauloricardomg'}, {'comment': ""Hi @pauloricardomg  and @smiklosovic \r\nI found that the word keyspace is a reserved word for cql \r\nhttps://github.com/apache/cassandra/blob/trunk/src/antlr/Parser.g#L1898\r\nSo and error will occurs when doing select when specified keyspace = xxx or select keyspace  where xxx\r\nI think that's why other system table using keyspace_name instand of keyspace\r\nhttps://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SystemKeyspace.java#L238\r\n\r\nAnd I am ok if we change snapshot_name from name to tag, And i have already finish this.\r\n"", 'commenter': 'Maxwell-Guo'}, {'comment': 'yes it is very unfortunate that keyspace is a reserved word. It may be workarounded by \'keyspace\' (putting that to quotes) but that is quite offputting and not intuitive for users to deal with this ... So next best thing is ""keyspace_name"".\r\n\r\nOr we fix cql grammar to not fail on that.\r\n\r\nI dont object using \'id\' but doing that internally might be dedicated to a separate ticket for the sake of simplicity in this ticket.\r\n\r\nFiltering will be enabled by default on vtables when not explicitly disallowed so nothing to cover there.', 'commenter': 'smiklosovic'}, {'comment': 'Thanks @smiklosovic , So what about using keyspace_name and table_name ? Because I found that some of the system tables just using keyspace_name and table_name , although virtual table is not a real system table, it is similar in a sense. \r\n\r\nI think open a new ticket for change the tag/name to id internally is ok, if we all agree on this I can open a ticket for this and finish the task.\r\n@pauloricardomg  WDYT?', 'commenter': 'Maxwell-Guo'}, {'comment': 'I think it is fine to have keyspace_name and table_name.', 'commenter': 'smiklosovic'}, {'comment': 'Please change the code to include my suggestion I posted above:\r\n\r\n_Partition key would be `keyspace_name` and clustering columns would be `table_name` and `id` of the snapshot._', 'commenter': 'smiklosovic'}, {'comment': ""> Partition key would be keyspace_name and clustering columns would be table_name and id of the snapshot.\r\n\r\nWhy do you think they should be primary keys?  I think primary key of snapshot should be `id`, since this is the primary snapshot identifier (ie. different keyspaces cannot have the same snapshot id). It's still possible to query by `keyspace_name` and `table_name` if needed after CASSANDRA-18238."", 'commenter': 'pauloricardomg'}, {'comment': ""In my mind ,I think the scope of snapshot name is bigger than keyspace as different keyspace may have same snapshot name, so first time I treate id as partition key and the output format is same with snapshot detail information.But latter I am +1 on @smiklosovic , for the first perspective of a user may be the keyspace, and they want to know a certain snapshot under the keyspace when writing a cql.I want to make sure it's what you think。 @smiklosovic "", 'commenter': 'Maxwell-Guo'}, {'comment': '@pauloricardomg \r\n\r\nRegardless of whether we can query without allow filtering after 18238 I think it is still a good practice to act as we would normally do when modeling the schema.\r\n\r\nAs explained above and what Maxwell just explained, I think keyspace > table > snapshot id.\r\n\r\nWe can have same snapshot names after all, no? So if keyspace and table are clustering columns, with two snapshots of the same name, we would have this partition:\r\n\r\n    snapshotName1 | keyspace1 | table1\r\n    snapshotName1 | keyspace2 | table2\r\n\r\nprimary key would be `((snapshotName), keyspaceName, tableName)`\r\n\r\nIs not this counter-intuitive to have a partition which is logically coupling different keyspaces and names under the same snapshot name for logically different snapshots? That does not make sense to me. \r\n\r\nHowever, on the other hand, it is possible to do this:\r\n\r\n    ./bin/nodetool snapshot --kt-list ks.tb,system.local -t mysnapshot\r\n\r\nSo `listsnapshots` will do this:\r\n\r\n    mysnapshot  system  local  1.16 KiB  21.47 KiB    2023-03-02T13:19:13.757Z\r\n    mysnapshot  ks      tb     1.02 KiB   6.08 KiB    2023-03-02T13:19:13.757Z\r\n\r\nBut then I can do this as well, again:\r\n\r\n    ./bin/nodetool snapshot --kt-list ks.tb2 -t mysnapshot\r\n\r\nWhich would print it like:\r\n\r\n    mysnapshot     ks       tb2        1.16 KiB  21.47 KiB    2023-03-02T13:19:42.140Z                \r\n    mysnapshot     ks       tb         1.02 KiB  6.08 KiB     2023-03-02T13:19:13.757Z                \r\n    mysnapshot     system   local      107 bytes 6.98 KiB     2023-03-02T13:19:13.757Z \r\n\r\nSo, with the primary key `((id), keyspace, table)`, the advantage is that we would be able to visually see what all tables were snapshotted in that one logical snapshot based on the same timestamp. Here, we see, from the timestamp, that ks.tb and system.local were snapshotted ""together"".\r\n\r\nSo from this perspective it is better if snapshot id is partition key.\r\n\r\nI would go so far to include timestamp into primary key as well: `((id), keyspace, table, timestamp)`\r\n\r\nThis way we would have them ordered too and it does not need to be specified when querying.\r\n\r\nIf we made it like already suggested: (keyspace, table, snapshotid), we would lose the information, it might be like:\r\n\r\n    ks1 tb1 snapshot1 2023-03-02T13:19:42.757Z        \r\n    ks3 tb2 snapshot1 2023-03-02T13:19:13.757Z \r\n    ks3 tb3 snapshot1 2023-03-02T13:19:42.757Z \r\n    ks4 tb4 snapshot1 2023-03-02T13:19:13.757Z \r\n\r\nBut here it is not so simple to see that ""ks4.tb4"" and ""ks3.tb2"" are forming one logical snapshot and then ""ks1.tb1"" and ""ks3.tb3"" another.\r\n\r\nSo, utimately, it should be like this:\r\n\r\n((id), ts, ks, tb))\r\n\r\nTimestamp has to go first to have it ordered, same timestamp for some snapshot id would mean that all keyspaces and tables belong to that particular snapshot name which was taken at that exact time.\r\n\r\n    cqlsh> select * from ks.test ;\r\n     id | ts | ks | tb\r\n    ----+----+----+----\r\n      1 |  2 |  3 |  4      first snapshot\r\n      1 |  2 |  3 |  5      first snapshot\r\n      1 |  2 |  3 |  6      first snapshot\r\n      1 |  3 |  1 |  2      second snapshot\r\n      1 |  3 |  1 |  3      second snapshot\r\n      2 |  5 |  2 |  6      third snapshot\r\n', 'commenter': 'smiklosovic'}, {'comment': '> Is not this counter-intuitive to have a partition which is logically coupling different keyspaces and names under the same snapshot name for logically different snapshots? That does not make sense to me.\r\n\r\nAgreed, thanks for the clarification.\r\n\r\n> But then I can do this as well, again: `./bin/nodetool snapshot --kt-list ks.tb2 -t mysnapshot`\r\n\r\nEven though this is allowed I think this is an inconsistency between [takeSnapshot](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L4162) and [takeMultipleTableSnapshot](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L4208). In the first method [it\'s not possible](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L4183) to create a snapshot if a snapshot with the same id/tag already exists in any keyspace, while in the second method it\'s only checked if a snapshot with the same name does not exist [in the same table](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L4235).\r\n\r\nI believe the behavior of `takeSnapshot` is the correct one, because it was implemented first and avoids confusion if there are multiple snapshots with the same name taken at different times for different tables. I think we should unify these methods and make this consistent on [CASSANDRA-18271](https://issues.apache.org/jira/browse/CASSANDRA-18271), so it\'s only possible to have a single logical snapshot with the same id.\r\n\r\nIf we do that, we should make the partition key `((snapshotId), keyspaceName, tableName)` and not include the timestamp in the primary key, since logical snapshots taken at the same time will be grouped together by id - in this case only ""legacy"" snapshots (taken before  [CASSANDRA-18271](https://issues.apache.org/jira/browse/CASSANDRA-18271)) will be unordered which is not a big deal. WDYT?', 'commenter': 'pauloricardomg'}, {'comment': 'If so,It seems that there is no need for me to change the code logic ?', 'commenter': 'Maxwell-Guo'}, {'comment': '@pauloricardomg I think we should bring this to ML as we are changing the behavior suddenly and some snapshots will not be possible anymore (snapshots with same name).', 'commenter': 'smiklosovic'}, {'comment': 'ok I will send a message shortly to the ML to see what others think. In any case we should make the behavior consistent whatever we agree to (either allow `takeSnapshot` to take snapshot with existing id on different table, or restrict `takeMultipleTableSnapshot` to not allow snapshot with existing id).', 'commenter': 'pauloricardomg'}]"
2117,src/java/org/apache/cassandra/db/virtual/SnapshotsTable.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import javax.management.openmbean.TabularData;
+import javax.management.openmbean.TabularDataSupport;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.db.SnapshotDetailsTabularData;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageService;
+
+public class SnapshotsTable extends AbstractVirtualTable
+{
+    private static final String SNAPSHOT_NAME = ""snapshot_name"";","[{'comment': 'isnt ""name"" enough?', 'commenter': 'smiklosovic'}, {'comment': 'I hope that the  selectstatement\' output result of snapshot virtual table is similar to that of listsnapshot, otherwise the user may have some confusion.\r\nThe description of snapshot name is ""snapshot_name"", ""keyspace_name"", ""columnfamilt_name"" and so on.\r\nThough I think table or table name is better than ""columnfamily_name"" as what you said below.', 'commenter': 'Maxwell-Guo'}]"
2117,src/java/org/apache/cassandra/db/virtual/SnapshotsTable.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import javax.management.openmbean.TabularData;
+import javax.management.openmbean.TabularDataSupport;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.db.SnapshotDetailsTabularData;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageService;
+
+public class SnapshotsTable extends AbstractVirtualTable
+{
+    private static final String SNAPSHOT_NAME = ""snapshot_name"";
+    private static final String KEYSPACE_NAME = ""keyspace_name"";","[{'comment': 'isnt ""keyspace"" enough?', 'commenter': 'smiklosovic'}, {'comment': 'same with the upper comment', 'commenter': 'Maxwell-Guo'}]"
2117,src/java/org/apache/cassandra/db/virtual/SnapshotsTable.java,"@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import javax.management.openmbean.TabularData;
+import javax.management.openmbean.TabularDataSupport;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.db.SnapshotDetailsTabularData;
+import org.apache.cassandra.db.marshal.BooleanType;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.StorageService;
+
+public class SnapshotsTable extends AbstractVirtualTable
+{
+    private static final String SNAPSHOT_NAME = ""snapshot_name"";
+    private static final String KEYSPACE_NAME = ""keyspace_name"";
+    private static final String COLUMNFAMILY_NAME = ""columnfamily_name"";","[{'comment': 'I would rename ""columnfamily"" to ""table"" so ""columnfamily_name"" to ""table"" only?', 'commenter': 'smiklosovic'}, {'comment': 'same with the upper comment', 'commenter': 'Maxwell-Guo'}]"
2117,test/unit/org/apache/cassandra/db/virtual/SnapshotsTableTest.java,"@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.time.Instant;
+
+import com.google.common.collect.ImmutableList;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DurationSpec;
+import org.apache.cassandra.cql3.CQLTester;
+import org.apache.cassandra.cql3.UntypedResultSet;
+
+public class SnapshotsTableTest extends CQLTester
+{
+    private static final String KS_NAME = ""vts"";
+    private final String SNAPSHOT_TTL = ""snapshotTtl"";
+    private final String SNAPSHOT_NO_TTL = ""snapshotNoTtl"";
+    private final String SNAPSHOT_EPHEMERAL = ""snapshotEphemeral"";
+    private final DurationSpec.IntSecondsBound ttl = new DurationSpec.IntSecondsBound (""4h"");
+    
+    @BeforeClass
+    public static void setUpClass()
+    {
+        CQLTester.setUpClass();
+    }
+
+    @Before
+    public void before() throws Throwable
+    {
+        SnapshotsTable table = new SnapshotsTable(KS_NAME);
+        VirtualKeyspaceRegistry.instance.register(new VirtualKeyspace(KS_NAME, ImmutableList.of(table)));
+        
+        createTable(""CREATE TABLE %s (pk int, ck int, PRIMARY KEY (pk, ck))"");
+        for (int i = 0; i != 10; ++i)
+        {
+            execute(""INSERT INTO %s (pk, ck) VALUES (?, ?)"", i, i);
+        }
+        flush();
+    }
+    
+    @After
+    public void after()
+    {
+        clearSnapshot(SNAPSHOT_NO_TTL, currentKeyspace());
+        clearSnapshot(SNAPSHOT_TTL, currentKeyspace());
+        clearSnapshot(SNAPSHOT_EPHEMERAL, currentKeyspace());
+        dropTable(""DROP TABLE %s"");
+    }
+   
+    @Test
+    public void testSnapshots() throws Throwable
+    {
+        Instant createTime = Instant.now();
+        String createTimeStr = createTime.toString();
+        snapshot(SNAPSHOT_NO_TTL, createTime);
+        snapshot(SNAPSHOT_TTL, ttl, createTime);
+        snapshot(SNAPSHOT_EPHEMERAL, true, null, createTime);
+
+        // query all from snapshots virtual table
+        UntypedResultSet result = execute(""SELECT id, keyspace_name, table_name, created_at, expires_at, ephemeral FROM vts.snapshots"");
+        assertRows(result,
+                row(SNAPSHOT_EPHEMERAL, CQLTester.KEYSPACE, currentTable(), createTimeStr, null, true),
+                row(SNAPSHOT_NO_TTL, CQLTester.KEYSPACE, currentTable(), createTimeStr, null, false),
+                row(SNAPSHOT_TTL, CQLTester.KEYSPACE, currentTable(), createTimeStr, createTime.plusSeconds(ttl.toSeconds()).toString(), false));
+
+        // query with conditions
+        result = execute(""SELECT id, keyspace_name, table_name, created_at, expires_at, ephemeral FROM vts.snapshots where ephemeral = true allow filtering"");","[{'comment': '`allow filtering` is not necessary', 'commenter': 'smiklosovic'}]"
2117,test/unit/org/apache/cassandra/db/virtual/SnapshotsTableTest.java,"@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.virtual;
+
+import java.time.Instant;
+
+import com.google.common.collect.ImmutableList;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DurationSpec;
+import org.apache.cassandra.cql3.CQLTester;
+import org.apache.cassandra.cql3.UntypedResultSet;
+
+public class SnapshotsTableTest extends CQLTester
+{
+    private static final String KS_NAME = ""vts"";
+    private final String SNAPSHOT_TTL = ""snapshotTtl"";
+    private final String SNAPSHOT_NO_TTL = ""snapshotNoTtl"";
+    private final String SNAPSHOT_EPHEMERAL = ""snapshotEphemeral"";
+    private final DurationSpec.IntSecondsBound ttl = new DurationSpec.IntSecondsBound (""4h"");
+    
+    @BeforeClass
+    public static void setUpClass()
+    {
+        CQLTester.setUpClass();
+    }
+
+    @Before
+    public void before() throws Throwable
+    {
+        SnapshotsTable table = new SnapshotsTable(KS_NAME);
+        VirtualKeyspaceRegistry.instance.register(new VirtualKeyspace(KS_NAME, ImmutableList.of(table)));
+        
+        createTable(""CREATE TABLE %s (pk int, ck int, PRIMARY KEY (pk, ck))"");
+        for (int i = 0; i != 10; ++i)
+        {
+            execute(""INSERT INTO %s (pk, ck) VALUES (?, ?)"", i, i);
+        }
+        flush();
+    }
+    
+    @After
+    public void after()
+    {
+        clearSnapshot(SNAPSHOT_NO_TTL, currentKeyspace());
+        clearSnapshot(SNAPSHOT_TTL, currentKeyspace());
+        clearSnapshot(SNAPSHOT_EPHEMERAL, currentKeyspace());
+        dropTable(""DROP TABLE %s"");
+    }
+   
+    @Test
+    public void testSnapshots() throws Throwable
+    {
+        Instant createTime = Instant.now();
+        String createTimeStr = createTime.toString();
+        snapshot(SNAPSHOT_NO_TTL, createTime);
+        snapshot(SNAPSHOT_TTL, ttl, createTime);
+        snapshot(SNAPSHOT_EPHEMERAL, true, null, createTime);
+
+        // query all from snapshots virtual table
+        UntypedResultSet result = execute(""SELECT id, keyspace_name, table_name, created_at, expires_at, ephemeral FROM vts.snapshots"");
+        assertRows(result,
+                row(SNAPSHOT_EPHEMERAL, CQLTester.KEYSPACE, currentTable(), createTimeStr, null, true),
+                row(SNAPSHOT_NO_TTL, CQLTester.KEYSPACE, currentTable(), createTimeStr, null, false),
+                row(SNAPSHOT_TTL, CQLTester.KEYSPACE, currentTable(), createTimeStr, createTime.plusSeconds(ttl.toSeconds()).toString(), false));
+
+        // query with conditions
+        result = execute(""SELECT id, keyspace_name, table_name, created_at, expires_at, ephemeral FROM vts.snapshots where ephemeral = true allow filtering"");
+        assertRows(result,
+                row(SNAPSHOT_EPHEMERAL, CQLTester.KEYSPACE, currentTable(), createTimeStr, null, true));
+
+        result = execute(""SELECT id, keyspace_name, table_name, created_at, expires_at, ephemeral FROM vts.snapshots where id = ? allow filtering"", SNAPSHOT_TTL);","[{'comment': '`allow filtering` is not necessary', 'commenter': 'smiklosovic'}]"
2119,src/java/org/apache/cassandra/repair/consistent/SyncStatSummary.java,"@@ -147,21 +152,36 @@ public String toString()
             }
             StringBuilder output = new StringBuilder();
 
-            output.append(String.format(""%s.%s - %s ranges, %s sstables, %s bytes\n"", keyspace, table, ranges, files, FBUtilities.prettyPrintMemory(bytes)));
+            output.append(String.format(""%s.%s - %s ranges, %s sstables, %s bytes\n"", keyspace, table, ranges.size(), files, FBUtilities.prettyPrintMemory(bytes)));","[{'comment': ""I'm unclear what the process is for modifying output of nodetool subcommands, but this will potentially change the console output of `nodetool repair`. IMO external tooling shouldn't be depending on the exact strings that nodetool emits, so this is fine with me but are we ok with having no option to force it to the old output?"", 'commenter': 'beobal'}, {'comment': '@beobal good point in general but I think this change is justifiable. It seems to be like overkill to me to offer the old format too.', 'commenter': 'smiklosovic'}, {'comment': 'I agree, just wanted to mention it in case we are in the minority on this.', 'commenter': 'beobal'}]"
2119,src/java/org/apache/cassandra/utils/RangesSerializer.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.net.MessagingService;
+
+public class RangesSerializer implements IVersionedSerializer<Collection<Range<Token>>>
+{
+    public static final RangesSerializer serializer = new RangesSerializer();
+
+    @Override
+    public void serialize(Collection<Range<Token>> ranges, DataOutputPlus out, int version) throws IOException
+    {
+        out.writeInt(ranges.size());
+        for (Range<Token> r : ranges)
+        {
+            Token.serializer.serialize(r.left, out, version);
+            Token.serializer.serialize(r.right, out, MessagingService.current_version);","[{'comment': 'Not that it currently matters as the token serializer disregards `version` but this looks like a mistake?', 'commenter': 'beobal'}, {'comment': '@beobal you mean like this line is redundant? There is the very same thing on line 46 so this seems to be a leftover from coding ...\r\n \r\nedit: ah sorry, there is `r.right`, so you say it should be like this?\r\n\r\n    Token.serializer.serialize(r.right, out, version);\r\n\r\n', 'commenter': 'smiklosovic'}, {'comment': ""yep, I mean I don't see a reason to hardcode the version here."", 'commenter': 'beobal'}, {'comment': 'done.', 'commenter': 'smiklosovic'}]"
2120,src/java/org/apache/cassandra/service/accord/IAccordService.java,"@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord;
+
+import accord.messages.Request;
+import accord.primitives.Txn;
+import accord.topology.TopologyManager;
+import org.apache.cassandra.db.ConsistencyLevel;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.service.accord.txn.TxnData;
+
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+public interface IAccordService
+{
+    IVerbHandler<? extends Request> verbHandler();
+
+    void createEpochFromConfigUnsafe();
+
+    TxnData coordinate(Txn txn, ConsistencyLevel consistencyLevel);
+
+    long currentEpoch();
+
+    void setCacheSize(long kb);
+
+    TopologyManager topology();","[{'comment': 'I kinda wish we could just return `Node`, but since that is what owns the threads... I doubt we can =(', 'commenter': 'dcapwell'}]"
2120,test/conf/cassandra.yaml,"@@ -53,6 +53,7 @@ materialized_views_enabled: true
 drop_compact_storage_enabled: true
 file_cache_enabled: true
 auto_hints_cleanup_enabled: true
+accord_transactions_enabled: true","[{'comment': ""don't you need to change this config for JVM dtest as well?"", 'commenter': 'dcapwell'}, {'comment': 'I changed it in `AccordTestBase`, of course, but outside that, my goal was to keep the rest of the in-JVM test landscape running w/ the default, which right now means `accord_transactions_enabled: false`. Right trade-off, or am I thinking about it wrong?', 'commenter': 'maedhroz'}, {'comment': ""You know what...I think you're right. It's probably better to have all of this active in a testing context so we ferret out problems earlier. Changing..."", 'commenter': 'maedhroz'}]"
2122,test/unit/org/apache/cassandra/transport/DuplicateHeapBufferTest.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.transport;
+
+import java.nio.ByteBuffer;
+import java.nio.ReadOnlyBufferException;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufAllocator;
+import io.netty.buffer.PooledByteBufAllocator;
+import org.apache.cassandra.utils.memory.MemoryUtil;
+
+public class DuplicateHeapBufferTest
+{
+    private static final ByteBufAllocator allocator = PooledByteBufAllocator.DEFAULT;
+    private ByteBuf buf;
+
+    @Test
+    public void heapByteBuffer()
+    {
+        heapTest(50, false);
+    }
+    @Test
+    public void readOnlyHeapByteBuffer()
+    {
+        heapTest(50, true);
+    }
+
+    @Test
+    public void directByteBuffer()
+    {
+        directTest(50, false);
+    }
+    @Test
+    public void readOnlyDirectByteBuffer()
+    {
+        directTest(50, true);
+    }
+
+    @Test
+    public void CBUtilWriteValueTest()
+    {
+        ByteBuffer bb = ByteBuffer.allocate(50);
+        int size = bb.capacity();
+        buf = allocator.heapBuffer(size);
+
+        CBUtil.writeValue(bb, buf);
+        Assert.assertEquals(0, buf.readerIndex());
+        Assert.assertEquals(bb, CBUtil.readValue(buf));
+        Assert.assertEquals(buf.writerIndex(), buf.readerIndex());
+    }
+
+    private void directTest(int capacity, boolean readOnly)
+    {
+        ByteBuffer hollowBuffer = CBUtil.getLocalBuffer();
+        ByteBuffer bb = readOnly ? ByteBuffer.allocateDirect(capacity).asReadOnlyBuffer() : ByteBuffer.allocateDirect(capacity);
+
+        int size = bb.capacity();
+        buf = allocator.directBuffer(size);
+
+        try {","[{'comment': 'braces on new line in this whole try catch', 'commenter': 'smiklosovic'}]"
2122,test/unit/org/apache/cassandra/transport/DuplicateHeapBufferTest.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.transport;
+
+import java.nio.ByteBuffer;
+import java.nio.ReadOnlyBufferException;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufAllocator;
+import io.netty.buffer.PooledByteBufAllocator;
+import org.apache.cassandra.utils.memory.MemoryUtil;
+
+public class DuplicateHeapBufferTest
+{
+    private static final ByteBufAllocator allocator = PooledByteBufAllocator.DEFAULT;
+    private ByteBuf buf;
+
+    @Test
+    public void heapByteBuffer()
+    {
+        heapTest(50, false);
+    }
+    @Test
+    public void readOnlyHeapByteBuffer()
+    {
+        heapTest(50, true);
+    }
+
+    @Test
+    public void directByteBuffer()
+    {
+        directTest(50, false);
+    }
+    @Test
+    public void readOnlyDirectByteBuffer()
+    {
+        directTest(50, true);
+    }
+
+    @Test
+    public void CBUtilWriteValueTest()
+    {
+        ByteBuffer bb = ByteBuffer.allocate(50);
+        int size = bb.capacity();
+        buf = allocator.heapBuffer(size);
+
+        CBUtil.writeValue(bb, buf);
+        Assert.assertEquals(0, buf.readerIndex());
+        Assert.assertEquals(bb, CBUtil.readValue(buf));
+        Assert.assertEquals(buf.writerIndex(), buf.readerIndex());
+    }
+
+    private void directTest(int capacity, boolean readOnly)
+    {
+        ByteBuffer hollowBuffer = CBUtil.getLocalBuffer();
+        ByteBuffer bb = readOnly ? ByteBuffer.allocateDirect(capacity).asReadOnlyBuffer() : ByteBuffer.allocateDirect(capacity);
+
+        int size = bb.capacity();
+        buf = allocator.directBuffer(size);
+
+        try {
+            MemoryUtil.duplicateHeapByteBuffer(bb, hollowBuffer);
+        }
+        catch (Error e) {
+            Assert.assertEquals(new AssertionError().getClass(), e.getClass());
+        }
+    }
+
+    private void heapTest(int capacity, boolean readOnly)
+    {
+        ByteBuffer hollowBuffer = CBUtil.getLocalBuffer();
+        ByteBuffer bb = readOnly ? ByteBuffer.allocate(capacity).asReadOnlyBuffer() : ByteBuffer.allocate(capacity);
+
+        int size = bb.capacity();
+        buf = allocator.heapBuffer(size);
+
+        try {","[{'comment': 'same', 'commenter': 'smiklosovic'}]"
2122,test/unit/org/apache/cassandra/transport/DuplicateHeapBufferTest.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.transport;
+
+import java.nio.ByteBuffer;
+import java.nio.ReadOnlyBufferException;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufAllocator;
+import io.netty.buffer.PooledByteBufAllocator;
+import org.apache.cassandra.utils.memory.MemoryUtil;
+
+public class DuplicateHeapBufferTest
+{
+    private static final ByteBufAllocator allocator = PooledByteBufAllocator.DEFAULT;
+    private ByteBuf buf;
+
+    @Test
+    public void heapByteBuffer()
+    {
+        heapTest(50, false);
+    }
+    @Test
+    public void readOnlyHeapByteBuffer()
+    {
+        heapTest(50, true);
+    }
+
+    @Test
+    public void directByteBuffer()
+    {
+        directTest(50, false);
+    }
+    @Test
+    public void readOnlyDirectByteBuffer()
+    {
+        directTest(50, true);
+    }
+
+    @Test
+    public void CBUtilWriteValueTest()
+    {
+        ByteBuffer bb = ByteBuffer.allocate(50);
+        int size = bb.capacity();
+        buf = allocator.heapBuffer(size);
+
+        CBUtil.writeValue(bb, buf);
+        Assert.assertEquals(0, buf.readerIndex());","[{'comment': 'cant you import it statically? saves some time to read it ', 'commenter': 'smiklosovic'}]"
2122,test/unit/org/apache/cassandra/transport/DuplicateHeapBufferTest.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.transport;
+
+import java.nio.ByteBuffer;
+import java.nio.ReadOnlyBufferException;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufAllocator;
+import io.netty.buffer.PooledByteBufAllocator;
+import org.apache.cassandra.utils.memory.MemoryUtil;
+
+public class DuplicateHeapBufferTest
+{
+    private static final ByteBufAllocator allocator = PooledByteBufAllocator.DEFAULT;
+    private ByteBuf buf;
+
+    @Test
+    public void heapByteBuffer()
+    {
+        heapTest(50, false);
+    }
+    @Test
+    public void readOnlyHeapByteBuffer()
+    {
+        heapTest(50, true);
+    }
+
+    @Test
+    public void directByteBuffer()
+    {
+        directTest(50, false);
+    }
+    @Test
+    public void readOnlyDirectByteBuffer()
+    {
+        directTest(50, true);
+    }
+
+    @Test
+    public void CBUtilWriteValueTest()
+    {
+        ByteBuffer bb = ByteBuffer.allocate(50);
+        int size = bb.capacity();
+        buf = allocator.heapBuffer(size);
+
+        CBUtil.writeValue(bb, buf);
+        Assert.assertEquals(0, buf.readerIndex());
+        Assert.assertEquals(bb, CBUtil.readValue(buf));
+        Assert.assertEquals(buf.writerIndex(), buf.readerIndex());
+    }
+
+    private void directTest(int capacity, boolean readOnly)
+    {
+        ByteBuffer hollowBuffer = CBUtil.getLocalBuffer();
+        ByteBuffer bb = readOnly ? ByteBuffer.allocateDirect(capacity).asReadOnlyBuffer() : ByteBuffer.allocateDirect(capacity);
+
+        int size = bb.capacity();
+        buf = allocator.directBuffer(size);
+
+        try {
+            MemoryUtil.duplicateHeapByteBuffer(bb, hollowBuffer);
+        }
+        catch (Error e) {
+            Assert.assertEquals(new AssertionError().getClass(), e.getClass());","[{'comment': 'why do you need to create an instance of AssertionError to call getClass() on it? Does not `AssertionError.class.getClass()` do the job?', 'commenter': 'smiklosovic'}]"
2122,test/unit/org/apache/cassandra/transport/DuplicateHeapBufferTest.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.transport;
+
+import java.nio.ByteBuffer;
+import java.nio.ReadOnlyBufferException;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufAllocator;
+import io.netty.buffer.PooledByteBufAllocator;
+import org.apache.cassandra.utils.memory.MemoryUtil;
+
+public class DuplicateHeapBufferTest
+{
+    private static final ByteBufAllocator allocator = PooledByteBufAllocator.DEFAULT;
+    private ByteBuf buf;
+
+    @Test
+    public void heapByteBuffer()
+    {
+        heapTest(50, false);
+    }
+    @Test
+    public void readOnlyHeapByteBuffer()
+    {
+        heapTest(50, true);
+    }
+
+    @Test
+    public void directByteBuffer()
+    {
+        directTest(50, false);
+    }
+    @Test
+    public void readOnlyDirectByteBuffer()
+    {
+        directTest(50, true);
+    }
+
+    @Test
+    public void CBUtilWriteValueTest()
+    {
+        ByteBuffer bb = ByteBuffer.allocate(50);
+        int size = bb.capacity();
+        buf = allocator.heapBuffer(size);
+
+        CBUtil.writeValue(bb, buf);
+        Assert.assertEquals(0, buf.readerIndex());
+        Assert.assertEquals(bb, CBUtil.readValue(buf));
+        Assert.assertEquals(buf.writerIndex(), buf.readerIndex());
+    }
+
+    private void directTest(int capacity, boolean readOnly)
+    {
+        ByteBuffer hollowBuffer = CBUtil.getLocalBuffer();
+        ByteBuffer bb = readOnly ? ByteBuffer.allocateDirect(capacity).asReadOnlyBuffer() : ByteBuffer.allocateDirect(capacity);
+
+        int size = bb.capacity();
+        buf = allocator.directBuffer(size);
+
+        try {
+            MemoryUtil.duplicateHeapByteBuffer(bb, hollowBuffer);
+        }
+        catch (Error e) {
+            Assert.assertEquals(new AssertionError().getClass(), e.getClass());
+        }
+    }
+
+    private void heapTest(int capacity, boolean readOnly)
+    {
+        ByteBuffer hollowBuffer = CBUtil.getLocalBuffer();
+        ByteBuffer bb = readOnly ? ByteBuffer.allocate(capacity).asReadOnlyBuffer() : ByteBuffer.allocate(capacity);
+
+        int size = bb.capacity();
+        buf = allocator.heapBuffer(size);
+
+        try {
+            ByteBuffer temp = MemoryUtil.duplicateHeapByteBuffer(bb, hollowBuffer);
+            Assert.assertEquals(bb.position(), temp.position());
+            Assert.assertEquals(bb.limit(), temp.limit());
+            Assert.assertEquals(bb.capacity(), temp.capacity());
+            Assert.assertEquals(bb.array(), temp.array());
+        }
+        catch (Exception e){
+            Assert.assertEquals(new ReadOnlyBufferException().getClass(), e.getClass());","[{'comment': '`ReadOnlyBufferException.class.getClass()` maybe?', 'commenter': 'smiklosovic'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -46,7 +46,7 @@
 import org.apache.cassandra.utils.Pair;
 import org.apache.cassandra.utils.TimeUUID;
 import org.apache.cassandra.utils.UUIDGen;
-
+import org.apache.cassandra.utils.memory.MemoryUtil;","[{'comment': 'should not be there some free line after imports and before the beginning of comments? This feels strange.', 'commenter': 'smiklosovic'}, {'comment': '@NateAdere  why was this comment resolved?', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -614,4 +623,9 @@ private static byte[] readRawBytes(ByteBuf cb, int length)
         return bytes;
     }
 
+    public static ByteBuffer getLocalBuffer()
+    {
+        return localBuffer.get();
+    }
+","[{'comment': 'empty line probably redundant?', 'commenter': 'smiklosovic'}]"
2122,test/unit/org/apache/cassandra/transport/DuplicateHeapBufferTest.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.transport;
+
+import java.nio.ByteBuffer;
+import java.nio.ReadOnlyBufferException;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufAllocator;
+import io.netty.buffer.PooledByteBufAllocator;
+import org.apache.cassandra.utils.memory.MemoryUtil;
+
+public class DuplicateHeapBufferTest","[{'comment': 'spoke already, we should remove as you moved away from heap buffer copying', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/utils/memory/MemoryUtil.java,"@@ -34,19 +34,20 @@
 
     private static final Unsafe unsafe;
     private static final Class<?> DIRECT_BYTE_BUFFER_CLASS, RO_DIRECT_BYTE_BUFFER_CLASS;
-    private static final long DIRECT_BYTE_BUFFER_ADDRESS_OFFSET;","[{'comment': 'can you revert all changes?  as you migrated away from the heap buffer duplicate logic this logic is no longer needed to be changed', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -614,4 +653,9 @@ private static byte[] readRawBytes(ByteBuf cb, int length)
         return bytes;
     }
 
+    public static ByteBuffer getLocalBuffer()","[{'comment': 'I would leave private for now', 'commenter': 'dcapwell'}, {'comment': 'also can you rename to `getLocalDirectBuffer`?  so callers are more clear ', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -478,7 +487,37 @@ public static void writeValue(ByteBuffer bytes, ByteBuf cb)
         cb.writeInt(remaining);
 
         if (remaining > 0)
-            cb.writeBytes(bytes.duplicate());
+        {
+            cb.writeBytes(MemoryUtil.duplicateHeapByteBuffer(bytes, getLocalBuffer()));
+        }
+    }
+
+    public static void writeValue(ByteBuffer src, ByteBuf dest)","[{'comment': 'should not copy/paste an existing method...\r\n\r\nYou ""should"" add a new method that can write a `ByteBuffer` to a `ByteBuf` without allocating, then have the one line `cb.writeBytes(bytes.duplicate());` call that method instead.', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -466,7 +475,7 @@ public static void writeValue(byte[] bytes, ByteBuf cb)
         cb.writeBytes(bytes);
     }
 
-    public static void writeValue(ByteBuffer bytes, ByteBuf cb)
+    public static void writeValu(ByteBuffer bytes, ByteBuf cb)","[{'comment': 'please revert', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -69,6 +69,15 @@ protected CharsetDecoder initialValue()
         }
     };
 
+    private final static FastThreadLocal<ByteBuffer> localBuffer = new FastThreadLocal<ByteBuffer>()","[{'comment': 'would be best to call it `localDirectBuffer`', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -478,7 +487,37 @@ public static void writeValue(ByteBuffer bytes, ByteBuf cb)
         cb.writeInt(remaining);
 
         if (remaining > 0)
-            cb.writeBytes(bytes.duplicate());
+        {
+            cb.writeBytes(MemoryUtil.duplicateHeapByteBuffer(bytes, getLocalBuffer()));
+        }
+    }
+
+    public static void writeValue(ByteBuffer src, ByteBuf dest)
+    {
+        if (src == null)
+        {
+            dest.writeInt(-1);
+            return;
+        }
+
+        int length = src.remaining();
+        dest.writeInt(length);
+
+        if (src.hasArray())
+        {
+            byte[] array = src.array();","[{'comment': 'can you add a comment explaining why we do this when direct duplicates the buffer?  You found a JDK bug, we should expose that to anyone reading so they know that there may be dragons to replicate', 'commenter': 'dcapwell'}, {'comment': 'Is this comment descriptive enough?', 'commenter': 'NateAdere'}, {'comment': 'can you flesh this out more?  Scott showed you the bug.\r\n\r\nit is also not an ""improper"" way, it was a bug in CMS', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -478,7 +487,35 @@ public static void writeValue(ByteBuffer bytes, ByteBuf cb)
         cb.writeInt(remaining);
 
         if (remaining > 0)
-            cb.writeBytes(bytes.duplicate());
+        {","[{'comment': 'remove the `{}`', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -478,7 +487,35 @@ public static void writeValue(ByteBuffer bytes, ByteBuf cb)
         cb.writeInt(remaining);
 
         if (remaining > 0)
-            cb.writeBytes(bytes.duplicate());
+        {
+            addBytes(bytes, cb);
+        }
+    }
+
+    public static void addBytes(ByteBuffer src, ByteBuf dest)
+    {
+        if (src == null)","[{'comment': ""why the null check?  at least in the single usage this doesn't allow null... I prefer if we remove and allow the NPE if a user provides null"", 'commenter': 'dcapwell'}]"
2122,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -299,6 +299,16 @@ private static char[] createDNSDomainPartDomain()
     }
 
     public static Gen<ByteBuffer> bytes(int min, int max)
+    {
+        return bytes(min, max, SourceDSL.arbitrary().constant(BBCases.HEAP));
+    }
+
+    public static Gen<ByteBuffer> bytesAnyType(int min, int max)
+    {
+        return bytes(min, max, SourceDSL.arbitrary().enumValues(BBCases.class));
+    }
+
+    public static Gen<ByteBuffer> bytes(int min, int max, Gen<BBCases> cases)","[{'comment': ""have to make this private, or fix `BBCases`.  You can't have a public method expose a private class (the build should be failing right now...).\r\n\r\nIf you choose to keep this method (I would not) then you should rename `BBCases` to be more clear to readers..."", 'commenter': 'dcapwell'}]"
2122,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -314,11 +324,32 @@ public static Gen<ByteBuffer> bytes(int min, int max)
             // to add more randomness, also shift offset in the array so the same size doesn't yield the same bytes
             int offset = (int) rnd.next(Constraint.between(0, MAX_BLOB_LENGTH - size));
 
-            return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            switch (cases.generate(rnd))
+            {
+                case HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+                case READ_ONLY_HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size).asReadOnlyBuffer();
+                case DIRECT:
+                {
+                    ByteBuffer bb = ByteBuffer.allocateDirect(size);
+                    bb.put(LazySharedBlob.SHARED_BYTES, offset, size);
+                    bb.flip();
+                    return bb;","[{'comment': 'can you make this a function?  best not to copy/paste', 'commenter': 'dcapwell'}]"
2122,test/unit/org/apache/cassandra/transport/WriteBytesDuplicateTest.java,"@@ -0,0 +1,56 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.transport;
+
+import org.junit.Test;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.Unpooled;
+import org.apache.cassandra.utils.Generators;
+import org.assertj.core.api.Assertions;
+
+import static org.quicktheories.QuickTheory.qt;
+
+
+public class WriteBytesDuplicateTest","[{'comment': 'why `Duplicate`?  You are ""writing""', 'commenter': 'dcapwell'}]"
2122,src/java/org/apache/cassandra/transport/CBUtil.java,"@@ -478,7 +487,32 @@ public static void writeValue(ByteBuffer bytes, ByteBuf cb)
         cb.writeInt(remaining);
 
         if (remaining > 0)
-            cb.writeBytes(bytes.duplicate());
+            addBytes(bytes, cb);
+    }
+
+    public static void addBytes(ByteBuffer src, ByteBuf dest)
+    {
+        if (src.remaining() == 0)
+            return;
+
+        int length = src.remaining();
+
+        //heap buffers are copied this way in order to avoid JVM crashing","[{'comment': ""please give more detail, this isn't enough for anyone to understand why this code is this way.  You should also move it within the `src.hasArray` block as that comment only makes sense there."", 'commenter': 'dcapwell'}]"
2122,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -314,11 +324,36 @@ public static Gen<ByteBuffer> bytes(int min, int max)
             // to add more randomness, also shift offset in the array so the same size doesn't yield the same bytes
             int offset = (int) rnd.next(Constraint.between(0, MAX_BLOB_LENGTH - size));
 
-            return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            return handleCases(cases, rnd, offset, size);
         };
+    };
+
+    private enum BBCases { HEAP, READ_ONLY_HEAP, DIRECT, READ_ONLY_DIRECT }
+
+    private static ByteBuffer handleCases(Gen<BBCases> cases, RandomnessSource rnd, int offset, int size) {
+        switch (cases.generate(rnd))
+        {
+            case HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            case READ_ONLY_HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size).asReadOnlyBuffer();
+            case DIRECT:
+            {
+                ByteBuffer bb = ByteBuffer.allocateDirect(size);
+                bb.put(LazySharedBlob.SHARED_BYTES, offset, size);
+                bb.flip();
+                return bb;","[{'comment': 'when I said you should avoid copy/paste and make this a method, I mean \r\n\r\n```\r\nByteBuffer bb = ByteBuffer.allocateDirect(size);\r\n                 bb.put(LazySharedBlob.SHARED_BYTES, offset, size);\r\n                 bb.flip();\r\n```\r\n\r\nand not this new `handleCases`...', 'commenter': 'dcapwell'}, {'comment': 'the new handleCases helps with readability so my plan is to keep that method and add ""ByteBuffer bb = directBufferFromSharedBlob(offset, size);"" to address the copy/paste code. Any opinions on removing or keeping the method?', 'commenter': 'NateAdere'}]"
2122,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -314,11 +324,36 @@ public static Gen<ByteBuffer> bytes(int min, int max)
             // to add more randomness, also shift offset in the array so the same size doesn't yield the same bytes
             int offset = (int) rnd.next(Constraint.between(0, MAX_BLOB_LENGTH - size));
 
-            return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            return handleCases(cases, rnd, offset, size);
         };
+    };
+
+    private enum BBCases { HEAP, READ_ONLY_HEAP, DIRECT, READ_ONLY_DIRECT }
+
+    private static ByteBuffer handleCases(Gen<BBCases> cases, RandomnessSource rnd, int offset, int size) {
+        switch (cases.generate(rnd))
+        {
+            case HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            case READ_ONLY_HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size).asReadOnlyBuffer();
+            case DIRECT:
+            {
+                ByteBuffer bb = ByteBuffer.allocateDirect(size);
+                bb.put(LazySharedBlob.SHARED_BYTES, offset, size);
+                bb.flip();
+                return bb;
+            }
+            case READ_ONLY_DIRECT:
+            {
+                ByteBuffer bb = ByteBuffer.allocateDirect(size);
+                bb.put(LazySharedBlob.SHARED_BYTES, offset, size);
+                bb.flip();
+                return bb.asReadOnlyBuffer();
+            }
+            default: throw new AssertionError(""cann't wait for jdk 17!"");
+        }
     }
 
-    /**
+     /**","[{'comment': 'please revert', 'commenter': 'dcapwell'}]"
2122,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -314,11 +324,39 @@ public static Gen<ByteBuffer> bytes(int min, int max)
             // to add more randomness, also shift offset in the array so the same size doesn't yield the same bytes
             int offset = (int) rnd.next(Constraint.between(0, MAX_BLOB_LENGTH - size));
 
-            return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            return handleCases(cases, rnd, offset, size);
         };
+    };
+
+    private enum BBCases { HEAP, READ_ONLY_HEAP, DIRECT, READ_ONLY_DIRECT }
+
+    private static ByteBuffer handleCases(Gen<BBCases> cases, RandomnessSource rnd, int offset, int size) {
+        switch (cases.generate(rnd))
+        {
+            case HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            case READ_ONLY_HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size).asReadOnlyBuffer();
+            case DIRECT:
+            {
+                ByteBuffer bb = directBufferFromSharedBlob(offset, size);
+                return bb;
+            }","[{'comment': 'nit: cleanup, you can just do `return directBufferFromSharedBlob(offset, size);` which lets you get rid of 3 lines of code', 'commenter': 'dcapwell'}]"
2122,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -314,11 +324,39 @@ public static Gen<ByteBuffer> bytes(int min, int max)
             // to add more randomness, also shift offset in the array so the same size doesn't yield the same bytes
             int offset = (int) rnd.next(Constraint.between(0, MAX_BLOB_LENGTH - size));
 
-            return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            return handleCases(cases, rnd, offset, size);
         };
+    };
+
+    private enum BBCases { HEAP, READ_ONLY_HEAP, DIRECT, READ_ONLY_DIRECT }
+
+    private static ByteBuffer handleCases(Gen<BBCases> cases, RandomnessSource rnd, int offset, int size) {
+        switch (cases.generate(rnd))
+        {
+            case HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            case READ_ONLY_HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size).asReadOnlyBuffer();
+            case DIRECT:
+            {
+                ByteBuffer bb = directBufferFromSharedBlob(offset, size);
+                return bb;
+            }
+            case READ_ONLY_DIRECT:
+            {
+                ByteBuffer bb = directBufferFromSharedBlob(offset, size);
+                return bb.asReadOnlyBuffer();","[{'comment': 'nit: cleanup, you can just do `return directBufferFromSharedBlob(offset, size);` which lets you get rid of 3 lines of code', 'commenter': 'dcapwell'}]"
2122,test/unit/org/apache/cassandra/utils/Generators.java,"@@ -314,11 +324,39 @@ public static Gen<ByteBuffer> bytes(int min, int max)
             // to add more randomness, also shift offset in the array so the same size doesn't yield the same bytes
             int offset = (int) rnd.next(Constraint.between(0, MAX_BLOB_LENGTH - size));
 
-            return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            return handleCases(cases, rnd, offset, size);
         };
+    };
+
+    private enum BBCases { HEAP, READ_ONLY_HEAP, DIRECT, READ_ONLY_DIRECT }
+
+    private static ByteBuffer handleCases(Gen<BBCases> cases, RandomnessSource rnd, int offset, int size) {
+        switch (cases.generate(rnd))
+        {
+            case HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size);
+            case READ_ONLY_HEAP: return ByteBuffer.wrap(LazySharedBlob.SHARED_BYTES, offset, size).asReadOnlyBuffer();
+            case DIRECT:
+            {
+                ByteBuffer bb = directBufferFromSharedBlob(offset, size);
+                return bb;
+            }
+            case READ_ONLY_DIRECT:
+            {
+                ByteBuffer bb = directBufferFromSharedBlob(offset, size);
+                return bb.asReadOnlyBuffer();
+            }
+            default: throw new AssertionError(""cann't wait for jdk 17!"");","[{'comment': 'fix typo plz', 'commenter': 'dcapwell'}]"
2151,ide/idea/workspace.xml,"@@ -173,7 +183,28 @@
       <option name=""MAIN_CLASS_NAME"" value="""" />
       <option name=""METHOD_NAME"" value="""" />
       <option name=""TEST_OBJECT"" value=""class"" />
-      <option name=""VM_PARAMETERS"" value=""-Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml -Dlogback.configurationFile=file://$PROJECT_DIR$/test/conf/logback-test.xml -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.skip_sync=true -ea -XX:MaxMetaspaceSize=1G -XX:SoftRefLRUPolicyMSPerMB=0 -XX:HeapDumpPath=build/test -XX:ActiveProcessorCount=2 -Dcassandra.strict.runtime.checks=true -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dmigration-sstable-root=$PROJECT_DIR$/test/data/migration-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.tolerate_sstable_size=true -Dcassandra.skip_sync=true -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true -Dcassandra.use_nix_recursive_delete=true -Dcassandra.test.messagingService.nonGracefulShutdown=true -Dcassandra.test.flush_local_schema_changes=false "" />
+      <option name=""VM_PARAMETERS"" value=""
+                                          -Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml
+                                          -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs
+                                          -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true
+                                          -Dcassandra.ring_delay_ms=1000","[{'comment': 'thanks for dedupping =)', 'commenter': 'dcapwell'}]"
2151,ide/idea/workspace.xml,"@@ -173,7 +183,28 @@
       <option name=""MAIN_CLASS_NAME"" value="""" />
       <option name=""METHOD_NAME"" value="""" />
       <option name=""TEST_OBJECT"" value=""class"" />
-      <option name=""VM_PARAMETERS"" value=""-Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml -Dlogback.configurationFile=file://$PROJECT_DIR$/test/conf/logback-test.xml -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.skip_sync=true -ea -XX:MaxMetaspaceSize=1G -XX:SoftRefLRUPolicyMSPerMB=0 -XX:HeapDumpPath=build/test -XX:ActiveProcessorCount=2 -Dcassandra.strict.runtime.checks=true -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dmigration-sstable-root=$PROJECT_DIR$/test/data/migration-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.tolerate_sstable_size=true -Dcassandra.skip_sync=true -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true -Dcassandra.use_nix_recursive_delete=true -Dcassandra.test.messagingService.nonGracefulShutdown=true -Dcassandra.test.flush_local_schema_changes=false "" />
+      <option name=""VM_PARAMETERS"" value=""
+                                          -Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml
+                                          -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs
+                                          -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true
+                                          -Dcassandra.ring_delay_ms=1000
+                                          -Dcassandra.skip_sync=true","[{'comment': 'thanks for dedupping =)', 'commenter': 'dcapwell'}]"
2151,ide/idea/workspace.xml,"@@ -173,7 +183,28 @@
       <option name=""MAIN_CLASS_NAME"" value="""" />
       <option name=""METHOD_NAME"" value="""" />
       <option name=""TEST_OBJECT"" value=""class"" />
-      <option name=""VM_PARAMETERS"" value=""-Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml -Dlogback.configurationFile=file://$PROJECT_DIR$/test/conf/logback-test.xml -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.skip_sync=true -ea -XX:MaxMetaspaceSize=1G -XX:SoftRefLRUPolicyMSPerMB=0 -XX:HeapDumpPath=build/test -XX:ActiveProcessorCount=2 -Dcassandra.strict.runtime.checks=true -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dmigration-sstable-root=$PROJECT_DIR$/test/data/migration-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.tolerate_sstable_size=true -Dcassandra.skip_sync=true -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true -Dcassandra.use_nix_recursive_delete=true -Dcassandra.test.messagingService.nonGracefulShutdown=true -Dcassandra.test.flush_local_schema_changes=false "" />
+      <option name=""VM_PARAMETERS"" value=""
+                                          -Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml
+                                          -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs
+                                          -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true
+                                          -Dcassandra.ring_delay_ms=1000
+                                          -Dcassandra.skip_sync=true
+                                          -Dcassandra.strict.runtime.checks=true
+                                          -Dcassandra.test.flush_local_schema_changes=false
+                                          -Dcassandra.test.messagingService.nonGracefulShutdown=true
+                                          -Dcassandra.tolerate_sstable_size=true
+                                          -Dcassandra.use_nix_recursive_delete=true
+                                          -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables","[{'comment': 'thanks for dedupping =)', 'commenter': 'dcapwell'}]"
2151,ide/idea/workspace.xml,"@@ -173,7 +183,28 @@
       <option name=""MAIN_CLASS_NAME"" value="""" />
       <option name=""METHOD_NAME"" value="""" />
       <option name=""TEST_OBJECT"" value=""class"" />
-      <option name=""VM_PARAMETERS"" value=""-Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml -Dlogback.configurationFile=file://$PROJECT_DIR$/test/conf/logback-test.xml -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.skip_sync=true -ea -XX:MaxMetaspaceSize=1G -XX:SoftRefLRUPolicyMSPerMB=0 -XX:HeapDumpPath=build/test -XX:ActiveProcessorCount=2 -Dcassandra.strict.runtime.checks=true -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dmigration-sstable-root=$PROJECT_DIR$/test/data/migration-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.tolerate_sstable_size=true -Dcassandra.skip_sync=true -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true -Dcassandra.use_nix_recursive_delete=true -Dcassandra.test.messagingService.nonGracefulShutdown=true -Dcassandra.test.flush_local_schema_changes=false "" />
+      <option name=""VM_PARAMETERS"" value=""
+                                          -Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml
+                                          -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs
+                                          -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true
+                                          -Dcassandra.ring_delay_ms=1000
+                                          -Dcassandra.skip_sync=true
+                                          -Dcassandra.strict.runtime.checks=true
+                                          -Dcassandra.test.flush_local_schema_changes=false
+                                          -Dcassandra.test.messagingService.nonGracefulShutdown=true
+                                          -Dcassandra.tolerate_sstable_size=true
+                                          -Dcassandra.use_nix_recursive_delete=true
+                                          -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables
+                                          -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin
+                                          -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables","[{'comment': 'thanks for dedupping =)', 'commenter': 'dcapwell'}]"
2151,ide/idea/workspace.xml,"@@ -173,7 +183,28 @@
       <option name=""MAIN_CLASS_NAME"" value="""" />
       <option name=""METHOD_NAME"" value="""" />
       <option name=""TEST_OBJECT"" value=""class"" />
-      <option name=""VM_PARAMETERS"" value=""-Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml -Dlogback.configurationFile=file://$PROJECT_DIR$/test/conf/logback-test.xml -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.skip_sync=true -ea -XX:MaxMetaspaceSize=1G -XX:SoftRefLRUPolicyMSPerMB=0 -XX:HeapDumpPath=build/test -XX:ActiveProcessorCount=2 -Dcassandra.strict.runtime.checks=true -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables -Dmigration-sstable-root=$PROJECT_DIR$/test/data/migration-sstables -Dcassandra.ring_delay_ms=1000 -Dcassandra.tolerate_sstable_size=true -Dcassandra.skip_sync=true -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true -Dcassandra.use_nix_recursive_delete=true -Dcassandra.test.messagingService.nonGracefulShutdown=true -Dcassandra.test.flush_local_schema_changes=false "" />
+      <option name=""VM_PARAMETERS"" value=""
+                                          -Dcassandra.config=file://$PROJECT_DIR$/test/conf/cassandra.yaml
+                                          -Dcassandra.logdir=$PROJECT_DIR$/build/test/logs
+                                          -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true
+                                          -Dcassandra.ring_delay_ms=1000
+                                          -Dcassandra.skip_sync=true
+                                          -Dcassandra.strict.runtime.checks=true
+                                          -Dcassandra.test.flush_local_schema_changes=false
+                                          -Dcassandra.test.messagingService.nonGracefulShutdown=true
+                                          -Dcassandra.tolerate_sstable_size=true
+                                          -Dcassandra.use_nix_recursive_delete=true
+                                          -Dinvalid-legacy-sstable-root=$PROJECT_DIR$/test/data/invalid-legacy-sstables
+                                          -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin
+                                          -Dlegacy-sstable-root=$PROJECT_DIR$/test/data/legacy-sstables
+                                          -Dlogback.configurationFile=file://$PROJECT_DIR$/test/conf/logback-test.xml
+                                          -Dmigration-sstable-root=$PROJECT_DIR$/test/data/migration-sstables
+                                          -XX:ActiveProcessorCount=2
+                                          -XX:HeapDumpPath=build/test
+                                          -XX:MaxMetaspaceSize=1G
+                                          -XX:SoftRefLRUPolicyMSPerMB=0
+                                          -ea
+                                          -Didea.click.expand.for.rest.of.options"" />","[{'comment': ""this one is new... quick google and didn't find results, what is this?"", 'commenter': 'dcapwell'}, {'comment': ""It was an attempt to remind somebody looking at the IDEA GUI that there were options that weren't visible due to all the whitespace. As it caused confusion in review I'll just remove it."", 'commenter': 'jonmeredith'}]"
2151,ide/idea/workspace.xml,"@@ -143,7 +143,17 @@
     <configuration default=""true"" type=""Application"" factoryName=""Application"">
       <extension name=""coverage"" enabled=""false"" merge=""false"" sample_coverage=""true"" runner=""idea"" />
       <option name=""MAIN_CLASS_NAME"" value="""" />
-      <option name=""VM_PARAMETERS"" value=""-Dcassandra.config=file://$PROJECT_DIR$/conf/cassandra.yaml -Dcassandra.storagedir=$PROJECT_DIR$/data -Dlogback.configurationFile=file://$PROJECT_DIR$/conf/logback.xml -Dcassandra.logdir=$PROJECT_DIR$/data/logs -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin -DQT_SHRINKS=0 -ea -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true -XX:HeapDumpPath=build/test"" />
+      <option name=""VM_PARAMETERS"" value=""
+                                          -DQT_SHRINKS=0
+                                          -Dcassandra.config=file://$PROJECT_DIR$/conf/cassandra.yaml
+                                          -Dcassandra.logdir=$PROJECT_DIR$/data/logs
+                                          -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true
+                                          -Dcassandra.storagedir=$PROJECT_DIR$/data
+                                          -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin
+                                          -Dlogback.configurationFile=file://$PROJECT_DIR$/conf/logback.xml
+                                          -ea
+                                          -XX:HeapDumpPath=build/test
+                                          -Didea.click.expand.for.rest.of.options"" />","[{'comment': 'this is new', 'commenter': 'dcapwell'}]"
2151,ide/idea/workspace.xml,"@@ -193,7 +224,18 @@
     <configuration default=""false"" name=""Cassandra"" type=""Application"" factoryName=""Application"">
       <extension name=""coverage"" enabled=""false"" merge=""false"" sample_coverage=""true"" runner=""idea"" />
       <option name=""MAIN_CLASS_NAME"" value=""org.apache.cassandra.service.CassandraDaemon"" />
-      <option name=""VM_PARAMETERS"" value=""-Dcassandra-foreground=yes -Dcassandra.config=file://$PROJECT_DIR$/conf/cassandra.yaml -Dcassandra.storagedir=$PROJECT_DIR$/data -Dlogback.configurationFile=file://$PROJECT_DIR$/conf/logback.xml -Dcassandra.logdir=$PROJECT_DIR$/data/logs -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin -Dcassandra.jmx.local.port=7199 -ea -Xmx1G -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true -XX:HeapDumpPath=build/test"" />
+      <option name=""VM_PARAMETERS"" value=""
+                                          -Dcassandra-foreground=yes
+                                          -Dcassandra.config=file://$PROJECT_DIR$/conf/cassandra.yaml
+                                          -Dcassandra.jmx.local.port=7199
+                                          -Dcassandra.logdir=$PROJECT_DIR$/data/logs
+                                          -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true
+                                          -Dcassandra.storagedir=$PROJECT_DIR$/data
+                                          -Djava.library.path=$PROJECT_DIR$/lib/sigar-bin
+                                          -Dlogback.configurationFile=file://$PROJECT_DIR$/conf/logback.xml
+                                          -Xmx1G
+                                          -ea","[{'comment': 'why did you remove `-XX:HeapDumpPath=build/test`?', 'commenter': 'dcapwell'}, {'comment': 'excellent eyes - mistake, will add back in.', 'commenter': 'jonmeredith'}]"
2220,src/java/org/apache/cassandra/cql3/Operations.java,"@@ -42,29 +44,46 @@ public final class Operations implements Iterable<Operation>
     /**
      * The operations on regular columns.
      */
-    private final List<Operation> regularOperations = new ArrayList<>();
+    private final List<Operation> regularOperations;
 
     /**
      * The operations on static columns.
      */
-    private final List<Operation> staticOperations = new ArrayList<>();
+    private final List<Operation> staticOperations;
 
-    private final List<ReferenceOperation> regularSubstitutions = new ArrayList<>();
-    private final List<ReferenceOperation> staticSubstitutions = new ArrayList<>();
+    private final List<ReferenceOperation> regularSubstitutions;
+    private final List<ReferenceOperation> staticSubstitutions;
 
     public Operations(StatementType type)
     {
         this.type = type;
+        regularOperations = new ArrayList<>();
+        staticOperations = new ArrayList<>();
+        regularSubstitutions = new ArrayList<>();
+        staticSubstitutions = new ArrayList<>();
+    }
+
+    private Operations(Operations other)
+    {
+        type = other.type;
+        regularOperations = new ArrayList<>(other.regularOperations);
+        staticOperations = new ArrayList<>(other.staticOperations);
+        regularSubstitutions = new ArrayList<>(other.regularSubstitutions);
+        staticSubstitutions = new ArrayList<>(other.staticSubstitutions);
     }
 
-    public void migrateReadRequiredOperations()
+    @Nullable
+    public Operations migrateReadRequiredOperations()
     {
-        migrateReadRequiredOperations(staticOperations, staticSubstitutions);
-        migrateReadRequiredOperations(regularOperations, regularSubstitutions);
+        Operations other = new Operations(this);","[{'comment': ""one way to lower memory in the case of no migration is to do 2 passes, first to find if any need to migrate, and the second to migrate them.  This would lower the memory costs in the cases where `requiresRead` isn't found and only adds extra cycles to the cases where it is found....  It was hard for me to say which way to go as I don't really know how common both sides are, so not sure which to bias towards...  so I went with the less-work route and did a tiny change to existing `migrateReadRequiredOperations` at the cost of extra memory."", 'commenter': 'dcapwell'}, {'comment': ""I guess you could do something like this...\r\n\r\n```\r\n@Nullable\r\npublic Operations migrateReadRequiredOperations()\r\n{\r\n    Operations other = null;\r\n\r\n    if (regularOperations.stream().anyMatch(Operation::requiresRead))\r\n    {\r\n        other = new Operations(this);\r\n        migrateReadRequiredOperations(other.regularOperations, other.regularSubstitutions);\r\n        migrateReadRequiredOperations(other.staticOperations, other.staticSubstitutions);\r\n    }\r\n    else if (staticOperations.stream().anyMatch(Operation::requiresRead))\r\n    {\r\n        other = new Operations(this);\r\n        migrateReadRequiredOperations(other.staticOperations, other.staticSubstitutions);\r\n    }\r\n    \r\n    return other;\r\n}\r\n```\r\nThis is cheaper in the case where we don't need any migrations, and operations on static columns are probably the exceptional case...just not sure what the probabilities are, as you say."", 'commenter': 'maedhroz'}, {'comment': ""One more idea...\r\n\r\nWe create `Operations` and add items to it in the `ModificationStatement.Parsed#prepareInternal()` implementations. Can we just add the `Operation` instances requiring reads to the correct list there and avoid later migrations altogether?\r\n\r\nex. In `ParsedUpdate#prepareInternal()`, we have `isForTxn` telling us whether or not we're processing a transaction. We could add operations that need a read via `Operations#add(ColumnMetadata, ReferenceOperation)` there."", 'commenter': 'maedhroz'}, {'comment': 'had so many back and forth on this due to CAS.... I simplified it all by having `Operations` take a `boolean isForTxn` and it properly places operations where they belong, and added new constructor that does the migration in a single path', 'commenter': 'dcapwell'}]"
2220,src/java/org/apache/cassandra/cql3/statements/UpdateStatement.java,"@@ -237,12 +246,14 @@ public static class ParsedInsertJson extends ModificationStatement.Parsed
     {
         private final Json.Raw jsonValue;
         private final boolean defaultUnset;
+        private final boolean isForTxn;
 
-        public ParsedInsertJson(QualifiedName name, Attributes.Raw attrs, Json.Raw jsonValue, boolean defaultUnset, boolean ifNotExists, StatementSource source)
+        public ParsedInsertJson(QualifiedName name, Attributes.Raw attrs, Json.Raw jsonValue, boolean defaultUnset, boolean ifNotExists, StatementSource source, boolean isForTxn)
         {
             super(name, StatementType.INSERT, attrs, null, ifNotExists, false, source);
             this.jsonValue = jsonValue;
             this.defaultUnset = defaultUnset;
+            this.isForTxn = isForTxn;","[{'comment': ""I think isForTxn is doesn't quite communicate that this is really for Accord so even CAS is included not just `TransactionStatement`. In some contexts we use lightweight transaction to describe CAS, but we don't always use this version for the modification statement for CAS just when it is on Accord."", 'commenter': 'aweisberg'}, {'comment': '> so even CAS is included\r\nCAS isn\'t included, it\'s `false` there.  When CAS uses accord we ""migrate"" and create a Accord transaction version of these statements', 'commenter': 'dcapwell'}]"
2237,src/java/org/apache/cassandra/concurrent/SyncFutureTask.java,"@@ -71,7 +71,11 @@ public void run()
         catch (Throwable t)
         {
             tryFailure(t);
-            ExecutionFailure.handle(t);
+            // A lot of exceptions are expected and will be handled by Cassandra
+            // by consuming the result of the future task so only treat Error
+            // as uncaught
+            if (t instanceof Error)","[{'comment': 'What problem is this solving here? Not sure if only handling `Error` as an unhandled exception is a good idea. We also want to handle things like IOExceptions a certain way, and this relies on a registered callback doing the right thing.', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -2957,6 +2974,10 @@ public static boolean paxoTopologyRepairStrictEachQuorum()
         return conf.paxos_topology_repair_strict_each_quorum;
     }
 
+    // TODO imperative that at startup we check that the legacy paxos strategy is compatible
+    // with cluster metadata for consensus migration
+    // If another node updates the CM with consensus migration and we don't support it that is a serious issue as well
+    // Ideally this config could only ever exist in CM so that the cluster always agrees on what it is","[{'comment': '`LegacyPaxosStrategy` this seems unintuitive as a config parameter since user facing docs typically refer to paxos operations  as light weight transactions. Something like `LightweightTransactionProtocol{paxos, accord}` would make it clearer I think.\r\n\r\nAlso, from a configuration perspective, it seems like this is more of a starting state, with the actual state being determined by cluster metadata and any migration history it contains.', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/cql3/Lists.java,"@@ -52,6 +59,8 @@
  */
 public abstract class Lists
 {
+    private static final Logger logger = LoggerFactory.getLogger(Lists.class);","[{'comment': 'unused', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/cql3/Lists.java,"@@ -524,11 +533,18 @@ static void doAppend(Term.Terminal value, ColumnMetadata column, UpdateParameter
                 // during SSTable write.
                 Guardrails.itemsPerCollection.guard(elements.size(), column.name.toString(), false, params.clientState);
 
+                long cellIndex = 0;
                 int dataSize = 0;
                 for (ByteBuffer buffer : elements)
                 {
-                    ByteBuffer uuid = ByteBuffer.wrap(params.nextTimeUUIDAsBytes());
-                    Cell<?> cell = params.addCell(column, CellPath.create(uuid), buffer);
+                    ByteBuffer cellPath;
+                    // Accord will need to replace this value later once it knows the executeAt timestamp
+                    // so just put a TimeUUID with MSB centinel for now
+                    if (params.constructingAccordBaseUpdate)
+                        cellPath = TimeUUID.atUnixMicrosWithLsb(0, cellIndex++).toBytes();","[{'comment': 'we should consolidate accord cell path logic in the CellPath class and use something like `CellPath#cellPathForAccordPlaceholder` here. It could also go in this class. Either way, the logic for producing and reading/replacing special values should be in the same place', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/cql3/statements/CQL3CasRequest.java,"@@ -65,20 +67,26 @@
 import org.apache.cassandra.service.accord.txn.TxnQuery;
 import org.apache.cassandra.service.accord.txn.TxnRead;
 import org.apache.cassandra.service.accord.txn.TxnReference;
+import org.apache.cassandra.service.accord.txn.TxnResult;
 import org.apache.cassandra.service.accord.txn.TxnUpdate;
 import org.apache.cassandra.service.accord.txn.TxnWrite;
 import org.apache.cassandra.service.paxos.Ballot;
 import org.apache.cassandra.utils.Pair;
 import org.apache.cassandra.utils.TimeUUID;
 
 import static com.google.common.base.Preconditions.checkState;
-import static org.apache.cassandra.service.accord.txn.TxnDataName.Kind.USER;
+import static org.apache.cassandra.service.StorageProxy.ConsensusAttemptResult;
+import static org.apache.cassandra.service.accord.txn.TxnDataName.Kind.CAS_READ;
+import static org.apache.cassandra.service.accord.txn.TxnResult.Kind.retry_new_protocol;
+
 
 /**
  * Processed CAS conditions and update on potentially multiple rows of the same partition.
  */
 public class CQL3CasRequest implements CASRequest
 {
+    private static final Logger logger = LoggerFactory.getLogger(CQL3CasRequest.class);","[{'comment': 'unused', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/db/rows/Cell.java,"@@ -41,6 +46,8 @@
  */
 public abstract class Cell<V> extends ColumnData
 {
+    private static final Logger logger = LoggerFactory.getLogger(Cell.class);","[{'comment': 'unused', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/db/rows/Cell.java,"@@ -282,7 +289,8 @@ public <V> Cell<V> deserialize(DataInputPlus in, LivenessInfo rowLiveness, Colum
                 }
             }
 
-            return accessor.factory().cell(column, timestamp, ttl, localDeletionTime, value, path);
+            Cell cell = accessor.factory().cell(column, timestamp, ttl, localDeletionTime, value, path);","[{'comment': 'unnecessary assignment here, you can just return from the factory', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/db/rows/ColumnData.java,"@@ -284,7 +287,19 @@ public static void digest(Digest digest, ColumnData cd)
      * This exists for the Paxos path, see {@link PartitionUpdate#updateAllTimestamp} for additional details.
      */
     public abstract ColumnData updateAllTimestamp(long newTimestamp);
-    public abstract ColumnData updateAllTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime);
+
+    /**
+     * @param cellToMaybeNewListPath If the cell is a list append cell a new cell path is returned generated based on the Accord executeAt timestamp
+     */
+    public abstract ColumnData updateAllTimesForAccord(@Nonnull Function<Cell, CellPath> cellToMaybeNewListPath, long newTimestamp, int newLocalDeletionTime);","[{'comment': 'could this be renamed to `updateTimesAndPathsForAccord`?', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/dht/Range.java,"@@ -40,6 +60,34 @@
 {
     public static final long serialVersionUID = 1L;
 
+    public static final boolean EXPENSIVE_CHECKS = Boolean.getBoolean(""org.apache.cassandra.dht.Range.expensive_checks"") | true;","[{'comment': ""this is OR'd to true"", 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/metrics/TableMetrics.java,"@@ -80,6 +80,11 @@
     public final static LatencyMetrics GLOBAL_WRITE_LATENCY = new LatencyMetrics(GLOBAL_FACTORY, GLOBAL_ALIAS_FACTORY, ""Write"");
     public final static LatencyMetrics GLOBAL_RANGE_LATENCY = new LatencyMetrics(GLOBAL_FACTORY, GLOBAL_ALIAS_FACTORY, ""Range"");
 
+    // TODO is the overhead of tracking latency of key and range migration too at the leel of detail of reads and writes too much?","[{'comment': ""if the overhead of tracking reads and writes is acceptable, I'd imagine tracking migration latency is as well. It will also be useful when understanding what's happening in a cluster."", 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/net/Verb.java,"@@ -259,6 +288,13 @@
     ACCORD_GET_DEPS_RSP         (148, P2, writeTimeout, REQUEST_RESPONSE, () -> GetDepsSerializers.reply, RESPONSE_HANDLER),
     ACCORD_GET_DEPS_REQ         (147, P2, writeTimeout, ACCORD,               () -> GetDepsSerializers.request,       () -> AccordService.instance().verbHandler(), ACCORD_GET_DEPS_RSP),
 
+    CONSENSUS_KEY_MIGRATION_FINISHED(149, P1, writeTimeout, MUTATION, () -> ConsensusKeyMigrationFinished.serializer, () -> ConsensusKeyMigrationState.consensusKeyMigrationFinishedHandler),","[{'comment': 'can you match the format of the other verb declarations?', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/net/Verb.java,"@@ -259,6 +288,13 @@
     ACCORD_GET_DEPS_RSP         (148, P2, writeTimeout, REQUEST_RESPONSE, () -> GetDepsSerializers.reply, RESPONSE_HANDLER),
     ACCORD_GET_DEPS_REQ         (147, P2, writeTimeout, ACCORD,               () -> GetDepsSerializers.request,       () -> AccordService.instance().verbHandler(), ACCORD_GET_DEPS_RSP),
 
+    CONSENSUS_KEY_MIGRATION_FINISHED(149, P1, writeTimeout, MUTATION, () -> ConsensusKeyMigrationFinished.serializer, () -> ConsensusKeyMigrationState.consensusKeyMigrationFinishedHandler),
+
+    UPDATE_CM_RSP(151, P1, writeTimeout, REQUEST_RESPONSE, () -> NoPayload.serializer, () -> ResponseVerbHandler.instance),","[{'comment': 'Can the CM abbreviation be expanded?', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/repair/AccordRepairJob.java,"@@ -0,0 +1,165 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair;
+
+import java.math.BigInteger;
+import java.util.List;
+
+import accord.api.RoutingKey;
+import accord.primitives.Ranges;
+import org.apache.cassandra.dht.AccordSplitter;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.service.ConsensusTableMigrationState.ConsensusMigrationRepairResult;
+import org.apache.cassandra.service.accord.TokenRange;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey.TokenKey;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.tcm.Epoch;
+
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Collections.emptyList;
+import static org.apache.cassandra.utils.Clock.Global.nanoTime;
+
+/*
+ * Accord repair consists of creating a barrier transaction for all the ranges which ensure that all Accord transactions
+ * before the Epoch and point in time at which the repair started have their side effects visible to Paxos and regular quorum reads.
+ */
+public class AccordRepairJob extends AbstractRepairJob
+{
+    public static final BigInteger TWO = BigInteger.valueOf(2);
+
+    private final Ranges ranges;
+
+    private final AccordSplitter splitter;
+
+    private BigInteger rangeStep;
+
+    private Epoch minEpoch = ClusterMetadata.current().epoch;
+
+    public AccordRepairJob(RepairSession repairSession, String cfname)
+    {
+        super(repairSession, cfname);
+        List<Range<Token>> normalizedRanges = Range.normalize(desc.ranges);
+        IPartitioner partitioner = normalizedRanges.get(0).left.getPartitioner();
+        TokenRange[] tokenRanges = new TokenRange[normalizedRanges.size()];
+        for (int i = 0; i < normalizedRanges.size(); i++)
+            tokenRanges[i] = new TokenRange(new TokenKey(ks.getName(), normalizedRanges.get(i).left), new TokenKey(ks.getName(), normalizedRanges.get(i).right));
+        this.ranges = Ranges.of(tokenRanges);
+        this.splitter = partitioner.accordSplitter().apply(Ranges.of(tokenRanges));
+    }
+
+    @Override
+    protected void runRepair()
+    {
+        try
+        {
+            for (accord.primitives.Range range : ranges)
+                repairRange((TokenRange)range);
+            state.phase.success();
+            cfs.metric.repairsCompleted.inc();
+            trySuccess(new RepairResult(desc, emptyList(), ConsensusMigrationRepairResult.fromAccordRepair(minEpoch)));
+        }
+        catch (Throwable t)
+        {
+            state.phase.fail(t);
+            cfs.metric.repairsCompleted.inc();
+            tryFailure(t);
+        }
+    }
+
+    private void repairRange(TokenRange range)
+    {
+        RoutingKey remainingStart = range.start();
+        // TODO validate wrap around is handled correctly, off by one and inclusive/exclusive
+        BigInteger rangeSize = splitter.sizeOf(range);
+        if (rangeStep == null)
+            rangeStep = BigInteger.ONE.max(splitter.divide(rangeSize, 1000));
+
+        BigInteger offset = BigInteger.ZERO;
+
+        TokenRange lastRepaired = null;
+        int iteration = 0;
+        while (true)
+        {
+            iteration++;
+            if (iteration % 100 == 0)
+                rangeStep = rangeStep.multiply(TWO);
+
+            BigInteger remaining = rangeSize.subtract(offset);
+            BigInteger length = remaining.min(rangeStep);
+
+            long start = nanoTime();
+            boolean dependencyOverflow = false;
+            try
+            {
+                // Splitter is approximate so it can't work right up to the end
+                TokenRange toRepair;
+                if (splitter.compare(offset, rangeSize) >= 0)
+                {
+                    if (remainingStart.equals(range.end()))
+                        return;
+
+                    // Final repair is whatever remains
+                    toRepair = range.newRange(remainingStart, range.end());
+                }
+                else
+                {
+                    toRepair = splitter.subRange(range, offset, splitter.add(offset, length));
+                    checkState(iteration > 1 || toRepair.start().equals(range.start()));
+                }
+                checkState(!toRepair.equals(lastRepaired), ""Shouldn't repair the same range twice"");
+                checkState(lastRepaired == null || toRepair.start().equals(lastRepaired.end()), ""Next range should directly follow previous range"");
+                lastRepaired = toRepair;
+                // TODO Won't work until range transactions work
+//                AccordService.instance().barrier(toRepair, minEpoch.getEpoch(), nanoTime(), BarrierType.global_sync, false);","[{'comment': 'Since regular repairs are incompatible with accord, accord repair should ensure that _all_ replicas are up to date to remain consistent with data replication guarantees, not just a quorum. The exception here would be for repairs run with --force, and possibly for Accord->Paxos migrations. \r\n', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/repair/CassandraRepairJob.java,"@@ -86,13 +86,10 @@ public class RepairJob extends AsyncFuture<RepairResult> implements Runnable
      *  @param session RepairSession that this RepairJob belongs
      * @param columnFamily name of the ColumnFamily to repair
      */
-    public RepairJob(RepairSession session, String columnFamily)
+    public CassandraRepairJob(RepairSession session, String columnFamily)
     {
-        this.session = session;
-        this.taskExecutor = session.taskExecutor;
+        super(session, columnFamily);","[{'comment': ""standard repair isn't going to be compatible with tables actively using accord. We should have a method, probably in AccordService, that will indicate if there is any accord history for the table and refuse to run a normal repair if there is. "", 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/repair/messages/RepairOption.java,"@@ -164,6 +172,12 @@ public static Set<Range<Token>> parseRanges(String rangesStr, IPartitioner parti
      *             ranges to the same host multiple times</td>
      *             <td>false</td>
      *         </tr>
+     *         <tr>
+     *             <td>accordRepair</td>
+     *             <td>""true"" if the repair should be of Accord in flight transactions. Will ensure
+     *             that once repair completes all Accord transactions are replicated at quorum</td>
+     *             <td>false</td>","[{'comment': 'assuming we adjust accord repair guarantees, the description for the accord option will need adjusting', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/ActiveRepairService.java,"@@ -664,6 +679,7 @@ public boolean invokeOnFailure()
             {
                 // we pre-filter the endpoints we want to repair for forced incremental repairs. So if any of the
                 // remaining ones go down, we still want to fail so we don't create repair sessions that can't complete
+                // TODO should there be a different default/behavior for Accord repairs?","[{'comment': ""for AccordRepair, I think we should fail if we're not able to get a barrier accepted by a quorum if it's a force repair, and fail on any down nodes if not a forced repair."", 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/ConsensusKeyMigrationState.java,"@@ -0,0 +1,359 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.UUID;
+import java.util.function.Function;
+import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.primitives.Ints;
+
+import accord.api.BarrierType;
+import com.github.benmanes.caffeine.cache.Cache;
+import com.github.benmanes.caffeine.cache.Caffeine;
+import org.apache.cassandra.concurrent.ImmediateExecutor;
+import org.apache.cassandra.concurrent.Stage;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ConsistencyLevel;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.SystemKeyspace;
+import org.apache.cassandra.db.WriteType;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.exceptions.CasWriteTimeoutException;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.locator.EndpointsForToken;
+import org.apache.cassandra.locator.Replica;
+import org.apache.cassandra.metrics.ClientRequestsMetricsHolder;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.schema.TableId;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.ConsensusTableMigrationState.ConsensusMigratedAt;
+import org.apache.cassandra.service.accord.AccordService;
+import org.apache.cassandra.service.accord.api.PartitionKey;
+import org.apache.cassandra.service.paxos.AbstractPaxosRepair.Failure;
+import org.apache.cassandra.service.paxos.AbstractPaxosRepair.Result;
+import org.apache.cassandra.service.paxos.PaxosRepair;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.tcm.Epoch;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.ObjectSizes;
+import org.apache.cassandra.utils.Pair;
+import org.apache.cassandra.utils.UUIDSerializer;
+
+import static org.apache.cassandra.net.Verb.CONSENSUS_KEY_MIGRATION_FINISHED;
+import static org.apache.cassandra.service.ConsensusTableMigrationState.ConsensusMigrationTarget;
+import static org.apache.cassandra.service.ConsensusTableMigrationState.ConsensusMigrationTarget.paxos;
+import static org.apache.cassandra.service.ConsensusTableMigrationState.TableMigrationState;
+import static org.apache.cassandra.utils.Clock.Global.nanoTime;
+
+/**
+ * Tracks the migration state of individual keys storing the migration (or not) in system.consensus_migration_state
+ * with an in-memory cache in front. Only locally replicated keys are tracked here to avoid storing too much
+ * state when token aware routing is not used.
+ *
+ * It is safe to migrate keys multiple times so no effort is made to ensure exactly once behavior and the system table
+ * expires key migration state after 7 days.
+ */
+public abstract class ConsensusKeyMigrationState
+{
+    /*
+     * Used to notify other replicas when key migration has occurred so they can
+     * also cache that the key migration was done
+     */
+    public static class ConsensusKeyMigrationFinished
+    {
+        @Nonnull
+        private final UUID tableId;
+        @Nonnull
+        private final ByteBuffer partitionKey;
+        @Nonnull
+        private final ConsensusMigratedAt consensusMigratedAt;
+
+        private ConsensusKeyMigrationFinished(@Nonnull UUID tableId,
+                                              @Nonnull ByteBuffer partitionKey,
+                                              @Nonnull ConsensusMigratedAt consensusMigratedAt)
+        {
+            this.tableId = tableId;
+            this.partitionKey = partitionKey;
+            this.consensusMigratedAt = consensusMigratedAt;
+        }
+
+        public static final IVersionedSerializer<ConsensusKeyMigrationFinished> serializer = new IVersionedSerializer<ConsensusKeyMigrationFinished>()
+        {
+            @Override
+            public void serialize(ConsensusKeyMigrationFinished t, DataOutputPlus out, int version) throws IOException
+            {
+                UUIDSerializer.serializer.serialize(t.tableId, out, version);
+                ByteBufferUtil.writeWithVIntLength(t.partitionKey, out);
+                ConsensusMigratedAt.serializer.serialize(t.consensusMigratedAt, out, version);
+            }
+
+            @Override
+            public ConsensusKeyMigrationFinished deserialize(DataInputPlus in, int version) throws IOException
+            {
+                UUID tableId = UUIDSerializer.serializer.deserialize(in, version);
+                ByteBuffer partitionKey = ByteBufferUtil.readWithVIntLength(in);
+                ConsensusMigratedAt consensusMigratedAt = ConsensusMigratedAt.serializer.deserialize(in, version);
+                return new ConsensusKeyMigrationFinished(tableId, partitionKey, consensusMigratedAt);
+            }
+
+            @Override
+            public long serializedSize(ConsensusKeyMigrationFinished t, int version)
+            {
+                return UUIDSerializer.serializer.serializedSize(t.tableId, version)
+                       + ByteBufferUtil.serializedSizeWithVIntLength(t.partitionKey)
+                       + ConsensusMigratedAt.serializer.serializedSize(t.consensusMigratedAt, version);
+            }
+        };
+    }
+
+    /*
+     * Bundles various aspects of key migration state together to avoid multiple lookups
+     * and to communicate multiple result values and state
+     */
+    public static class KeyMigrationState
+    {
+        static final KeyMigrationState MIGRATION_NOT_NEEDED = new KeyMigrationState(null, null, null, null);
+
+        public final ConsensusMigratedAt consensusMigratedAt;
+
+        public final Epoch currentEpoch;
+
+        public final TableMigrationState tableMigrationState;
+
+        public final DecoratedKey key;
+
+        private KeyMigrationState(ConsensusMigratedAt consensusMigratedAt, Epoch currentEpoch,
+                                  TableMigrationState tableMigrationState, DecoratedKey key)
+        {
+            this.consensusMigratedAt = consensusMigratedAt;
+            this.currentEpoch = currentEpoch;
+            this.tableMigrationState = tableMigrationState;
+            this.key = key;
+        }
+
+        /*
+         * This will trigger a distributed migration for the key, but will only block on local completion
+         * so Paxos reads can return a result as soon as the local state is ready
+         */
+        public void maybePerformAccordToPaxosKeyMigration(boolean isForWrite)
+        {
+            if (paxosReadSatisfiedByKeyMigration())
+                return;
+
+            // TODO better query start time?
+            TableMigrationState tms = tableMigrationState;
+            repairKeyAccord(key, tms.keyspaceName, tms.tableId, tms.minMigrationEpoch(key.getToken()).getEpoch(), nanoTime(), false, isForWrite);
+        }
+
+        private boolean paxosReadSatisfiedByKeyMigration()
+        {
+            // No migration in progress, it's safe
+            if (tableMigrationState == null)
+                return true;
+
+            return tableMigrationState.paxosReadSatisfiedByKeyMigrationAtEpoch(key, consensusMigratedAt);
+        }
+    }
+
+    private static final int EMPTY_KEY_SIZE = Ints.checkedCast(ObjectSizes.measureDeep(Pair.create(null, UUID.randomUUID())));
+    private static final int VALUE_SIZE = Ints.checkedCast(ObjectSizes.measureDeep(new ConsensusMigratedAt(Epoch.EMPTY, ConsensusMigrationTarget.accord)));
+    private static final Cache<Pair<ByteBuffer, UUID>, ConsensusMigratedAt> MIGRATION_STATE_CACHE =
+        Caffeine.newBuilder()
+                .maximumWeight(DatabaseDescriptor.getConsensusMigrationCacheSizeInMiB() << 20)
+                .<Pair<ByteBuffer, UUID>, ConsensusMigratedAt>weigher((k, v) -> EMPTY_KEY_SIZE + Ints.checkedCast(ByteBufferUtil.estimatedSizeOnHeap(k.left)) + VALUE_SIZE)
+                .executor(ImmediateExecutor.INSTANCE)
+                .build();
+    private static final Function<Pair<ByteBuffer, UUID>, ConsensusMigratedAt> LOADING_FUNCTION = k -> SystemKeyspace.loadConsensusKeyMigrationState(k.left, k.right);","[{'comment': 'can this be moved into the cache `get` that uses it?', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/ConsensusTableMigrationState.java,"@@ -0,0 +1,697 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.AbstractMap.SimpleEntry;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.stream.Collectors;
+import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableMap.Builder;
+import com.google.common.collect.ImmutableSortedMap;
+import com.google.common.primitives.SignedBytes;
+import com.google.common.util.concurrent.FutureCallback;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.config.Config.LegacyPaxosStrategy;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.IPartitionerDependentSerializer;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.repair.RepairJobDesc;
+import org.apache.cassandra.repair.RepairResult;
+import org.apache.cassandra.repair.messages.RepairOption;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableId;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.paxos.Paxos;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.tcm.ClusterMetadataService;
+import org.apache.cassandra.tcm.Epoch;
+import org.apache.cassandra.tcm.transformations.BeginConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.MaybeFinishConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.SetConsensusMigrationTargetProtocol;
+import org.apache.cassandra.utils.NullableSerializer;
+import org.apache.cassandra.utils.PojoToString;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+import static com.google.common.collect.ImmutableList.toImmutableList;
+import static org.apache.cassandra.db.TypeSizes.sizeof;
+import static org.apache.cassandra.dht.Range.intersectionOfNormalizedRanges;
+import static org.apache.cassandra.dht.Range.normalize;
+import static org.apache.cassandra.dht.Range.rangeSerializer;
+import static org.apache.cassandra.dht.Range.subtract;
+import static org.apache.cassandra.dht.Range.subtractNormalizedRanges;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeSet;
+import static org.apache.cassandra.utils.CollectionSerializers.newCollectionSerializer;
+import static org.apache.cassandra.utils.CollectionSerializers.newHashMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeCollection;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedCollectionSize;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedMapSize;
+
+/**
+ * Track and update the migration state of individual table and ranges within those tables
+ */
+public abstract class ConsensusTableMigrationState
+{
+    private static final Logger logger = LoggerFactory.getLogger(ConsensusTableMigrationState.class);
+
+    public static final IPartitionerDependentSerializer<List<Range<Token>>> rangesSerializer = newCollectionSerializer(rangeSerializer);
+
+    public static final FutureCallback<RepairResult> completedRepairJobHandler = new FutureCallback<RepairResult>()
+    {
+        @Override
+        public void onSuccess(@Nullable RepairResult repairResult)
+        {
+            checkNotNull(repairResult, ""repairResult should not be null"");
+            ConsensusMigrationRepairResult migrationResult = repairResult.consensusMigrationRepairResult;
+
+            // Need to repair both Paxos and base table state
+            // Could track them separately, but doesn't seem worth the effort
+            if (migrationResult.consensusMigrationRepairType == ConsensusMigrationRepairType.ineligible)
+                return;
+
+            RepairJobDesc desc = repairResult.desc;
+            ClusterMetadataService.instance.commit(
+                new MaybeFinishConsensusMigrationForTableAndRange(
+                    desc.keyspace, desc.columnFamily, ImmutableList.copyOf(desc.ranges),
+                    migrationResult.minEpoch, migrationResult.consensusMigrationRepairType));
+        }
+
+        @Override
+        public void onFailure(Throwable throwable)
+        {
+            // Only successes drive forward progress
+        }
+    };
+
+    @VisibleForTesting
+    public static void reset()
+    {
+        ClusterMetadataService.reset();
+    }
+
+    public enum ConsensusMigrationRepairType
+    {
+        ineligible(0),
+        paxos(1),
+        accord(2);
+
+        public final byte value;
+
+        ConsensusMigrationRepairType(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationRepairType fromString(String repairType)
+        {
+            return ConsensusMigrationRepairType.valueOf(repairType.toLowerCase());
+        }
+
+        public static ConsensusMigrationRepairType fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return ConsensusMigrationRepairType.paxos;
+                case 1:
+                    return ConsensusMigrationRepairType.accord;
+            }
+        }
+    }
+
+    public enum ConsensusMigrationTarget {","[{'comment': 'brace formatting', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/ConsensusTableMigrationState.java,"@@ -0,0 +1,697 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.AbstractMap.SimpleEntry;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.stream.Collectors;
+import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableMap.Builder;
+import com.google.common.collect.ImmutableSortedMap;
+import com.google.common.primitives.SignedBytes;
+import com.google.common.util.concurrent.FutureCallback;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.config.Config.LegacyPaxosStrategy;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.IPartitionerDependentSerializer;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.repair.RepairJobDesc;
+import org.apache.cassandra.repair.RepairResult;
+import org.apache.cassandra.repair.messages.RepairOption;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableId;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.paxos.Paxos;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.tcm.ClusterMetadataService;
+import org.apache.cassandra.tcm.Epoch;
+import org.apache.cassandra.tcm.transformations.BeginConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.MaybeFinishConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.SetConsensusMigrationTargetProtocol;
+import org.apache.cassandra.utils.NullableSerializer;
+import org.apache.cassandra.utils.PojoToString;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+import static com.google.common.collect.ImmutableList.toImmutableList;
+import static org.apache.cassandra.db.TypeSizes.sizeof;
+import static org.apache.cassandra.dht.Range.intersectionOfNormalizedRanges;
+import static org.apache.cassandra.dht.Range.normalize;
+import static org.apache.cassandra.dht.Range.rangeSerializer;
+import static org.apache.cassandra.dht.Range.subtract;
+import static org.apache.cassandra.dht.Range.subtractNormalizedRanges;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeSet;
+import static org.apache.cassandra.utils.CollectionSerializers.newCollectionSerializer;
+import static org.apache.cassandra.utils.CollectionSerializers.newHashMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeCollection;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedCollectionSize;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedMapSize;
+
+/**
+ * Track and update the migration state of individual table and ranges within those tables
+ */
+public abstract class ConsensusTableMigrationState
+{
+    private static final Logger logger = LoggerFactory.getLogger(ConsensusTableMigrationState.class);
+
+    public static final IPartitionerDependentSerializer<List<Range<Token>>> rangesSerializer = newCollectionSerializer(rangeSerializer);
+
+    public static final FutureCallback<RepairResult> completedRepairJobHandler = new FutureCallback<RepairResult>()
+    {
+        @Override
+        public void onSuccess(@Nullable RepairResult repairResult)
+        {
+            checkNotNull(repairResult, ""repairResult should not be null"");
+            ConsensusMigrationRepairResult migrationResult = repairResult.consensusMigrationRepairResult;
+
+            // Need to repair both Paxos and base table state
+            // Could track them separately, but doesn't seem worth the effort
+            if (migrationResult.consensusMigrationRepairType == ConsensusMigrationRepairType.ineligible)
+                return;
+
+            RepairJobDesc desc = repairResult.desc;
+            ClusterMetadataService.instance.commit(
+                new MaybeFinishConsensusMigrationForTableAndRange(
+                    desc.keyspace, desc.columnFamily, ImmutableList.copyOf(desc.ranges),
+                    migrationResult.minEpoch, migrationResult.consensusMigrationRepairType));
+        }
+
+        @Override
+        public void onFailure(Throwable throwable)
+        {
+            // Only successes drive forward progress
+        }
+    };
+
+    @VisibleForTesting
+    public static void reset()
+    {
+        ClusterMetadataService.reset();
+    }
+
+    public enum ConsensusMigrationRepairType
+    {
+        ineligible(0),
+        paxos(1),
+        accord(2);
+
+        public final byte value;
+
+        ConsensusMigrationRepairType(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationRepairType fromString(String repairType)
+        {
+            return ConsensusMigrationRepairType.valueOf(repairType.toLowerCase());
+        }
+
+        public static ConsensusMigrationRepairType fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return ConsensusMigrationRepairType.paxos;
+                case 1:
+                    return ConsensusMigrationRepairType.accord;
+            }
+        }
+    }
+
+    public enum ConsensusMigrationTarget {
+        paxos(0),
+        accord(1);
+
+        public final byte value;
+
+        ConsensusMigrationTarget(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationTarget fromString(String targetProtocol)
+        {
+            return ConsensusMigrationTarget.valueOf(targetProtocol.toLowerCase());
+        }
+
+        public static ConsensusMigrationTarget fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return paxos;
+                case 1:
+                    return accord;
+            }
+        }
+    }
+
+    public static class ConsensusMigrationRepairResult
+    {
+        private final ConsensusMigrationRepairType consensusMigrationRepairType;","[{'comment': 'can we just name this `type`?', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/ConsensusTableMigrationState.java,"@@ -0,0 +1,697 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.AbstractMap.SimpleEntry;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.stream.Collectors;
+import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableMap.Builder;
+import com.google.common.collect.ImmutableSortedMap;
+import com.google.common.primitives.SignedBytes;
+import com.google.common.util.concurrent.FutureCallback;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.config.Config.LegacyPaxosStrategy;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.IPartitionerDependentSerializer;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.repair.RepairJobDesc;
+import org.apache.cassandra.repair.RepairResult;
+import org.apache.cassandra.repair.messages.RepairOption;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableId;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.paxos.Paxos;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.tcm.ClusterMetadataService;
+import org.apache.cassandra.tcm.Epoch;
+import org.apache.cassandra.tcm.transformations.BeginConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.MaybeFinishConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.SetConsensusMigrationTargetProtocol;
+import org.apache.cassandra.utils.NullableSerializer;
+import org.apache.cassandra.utils.PojoToString;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+import static com.google.common.collect.ImmutableList.toImmutableList;
+import static org.apache.cassandra.db.TypeSizes.sizeof;
+import static org.apache.cassandra.dht.Range.intersectionOfNormalizedRanges;
+import static org.apache.cassandra.dht.Range.normalize;
+import static org.apache.cassandra.dht.Range.rangeSerializer;
+import static org.apache.cassandra.dht.Range.subtract;
+import static org.apache.cassandra.dht.Range.subtractNormalizedRanges;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeSet;
+import static org.apache.cassandra.utils.CollectionSerializers.newCollectionSerializer;
+import static org.apache.cassandra.utils.CollectionSerializers.newHashMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeCollection;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedCollectionSize;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedMapSize;
+
+/**
+ * Track and update the migration state of individual table and ranges within those tables
+ */
+public abstract class ConsensusTableMigrationState
+{
+    private static final Logger logger = LoggerFactory.getLogger(ConsensusTableMigrationState.class);
+
+    public static final IPartitionerDependentSerializer<List<Range<Token>>> rangesSerializer = newCollectionSerializer(rangeSerializer);
+
+    public static final FutureCallback<RepairResult> completedRepairJobHandler = new FutureCallback<RepairResult>()
+    {
+        @Override
+        public void onSuccess(@Nullable RepairResult repairResult)
+        {
+            checkNotNull(repairResult, ""repairResult should not be null"");
+            ConsensusMigrationRepairResult migrationResult = repairResult.consensusMigrationRepairResult;
+
+            // Need to repair both Paxos and base table state
+            // Could track them separately, but doesn't seem worth the effort
+            if (migrationResult.consensusMigrationRepairType == ConsensusMigrationRepairType.ineligible)
+                return;
+
+            RepairJobDesc desc = repairResult.desc;
+            ClusterMetadataService.instance.commit(
+                new MaybeFinishConsensusMigrationForTableAndRange(
+                    desc.keyspace, desc.columnFamily, ImmutableList.copyOf(desc.ranges),
+                    migrationResult.minEpoch, migrationResult.consensusMigrationRepairType));
+        }
+
+        @Override
+        public void onFailure(Throwable throwable)
+        {
+            // Only successes drive forward progress
+        }
+    };
+
+    @VisibleForTesting
+    public static void reset()
+    {
+        ClusterMetadataService.reset();
+    }
+
+    public enum ConsensusMigrationRepairType
+    {
+        ineligible(0),
+        paxos(1),
+        accord(2);
+
+        public final byte value;
+
+        ConsensusMigrationRepairType(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationRepairType fromString(String repairType)
+        {
+            return ConsensusMigrationRepairType.valueOf(repairType.toLowerCase());
+        }
+
+        public static ConsensusMigrationRepairType fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return ConsensusMigrationRepairType.paxos;
+                case 1:
+                    return ConsensusMigrationRepairType.accord;
+            }
+        }
+    }
+
+    public enum ConsensusMigrationTarget {
+        paxos(0),
+        accord(1);
+
+        public final byte value;
+
+        ConsensusMigrationTarget(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationTarget fromString(String targetProtocol)
+        {
+            return ConsensusMigrationTarget.valueOf(targetProtocol.toLowerCase());
+        }
+
+        public static ConsensusMigrationTarget fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return paxos;
+                case 1:
+                    return accord;
+            }
+        }
+    }
+
+    public static class ConsensusMigrationRepairResult
+    {
+        private final ConsensusMigrationRepairType consensusMigrationRepairType;
+        private final Epoch minEpoch;
+
+        private ConsensusMigrationRepairResult(ConsensusMigrationRepairType consensusMigrationRepairType, Epoch minEpoch)
+        {
+            this.consensusMigrationRepairType = consensusMigrationRepairType;
+            this.minEpoch = minEpoch;
+        }
+
+        public static ConsensusMigrationRepairResult fromCassandraRepair(Epoch minEpoch, boolean didPaxosAndRegularRepair)
+        {
+            checkArgument(!didPaxosAndRegularRepair || minEpoch.isAfter(Epoch.EMPTY), ""Epoch should not be empty if Paxos and regular repairs were performed"");
+            if (didPaxosAndRegularRepair)
+                return new ConsensusMigrationRepairResult(ConsensusMigrationRepairType.paxos, minEpoch);
+            else
+                return new ConsensusMigrationRepairResult(ConsensusMigrationRepairType.ineligible, Epoch.EMPTY);","[{'comment': 'a repair should also be ineligible if it was run with `--force` and repair then skipped one or more nodes', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/ConsensusTableMigrationState.java,"@@ -0,0 +1,697 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.AbstractMap.SimpleEntry;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.stream.Collectors;
+import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableMap.Builder;
+import com.google.common.collect.ImmutableSortedMap;
+import com.google.common.primitives.SignedBytes;
+import com.google.common.util.concurrent.FutureCallback;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.config.Config.LegacyPaxosStrategy;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.IPartitionerDependentSerializer;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.repair.RepairJobDesc;
+import org.apache.cassandra.repair.RepairResult;
+import org.apache.cassandra.repair.messages.RepairOption;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableId;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.paxos.Paxos;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.tcm.ClusterMetadataService;
+import org.apache.cassandra.tcm.Epoch;
+import org.apache.cassandra.tcm.transformations.BeginConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.MaybeFinishConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.SetConsensusMigrationTargetProtocol;
+import org.apache.cassandra.utils.NullableSerializer;
+import org.apache.cassandra.utils.PojoToString;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+import static com.google.common.collect.ImmutableList.toImmutableList;
+import static org.apache.cassandra.db.TypeSizes.sizeof;
+import static org.apache.cassandra.dht.Range.intersectionOfNormalizedRanges;
+import static org.apache.cassandra.dht.Range.normalize;
+import static org.apache.cassandra.dht.Range.rangeSerializer;
+import static org.apache.cassandra.dht.Range.subtract;
+import static org.apache.cassandra.dht.Range.subtractNormalizedRanges;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeSet;
+import static org.apache.cassandra.utils.CollectionSerializers.newCollectionSerializer;
+import static org.apache.cassandra.utils.CollectionSerializers.newHashMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeCollection;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedCollectionSize;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedMapSize;
+
+/**
+ * Track and update the migration state of individual table and ranges within those tables
+ */
+public abstract class ConsensusTableMigrationState","[{'comment': ""Let's make an `o.a.c.service.consensus` package, and possibly an `o.a.c.service.consensus.migration` package, and move some of this stuff in there"", 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/ConsensusTableMigrationState.java,"@@ -0,0 +1,697 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.AbstractMap.SimpleEntry;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.stream.Collectors;
+import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableMap.Builder;
+import com.google.common.collect.ImmutableSortedMap;
+import com.google.common.primitives.SignedBytes;
+import com.google.common.util.concurrent.FutureCallback;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.config.Config.LegacyPaxosStrategy;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.IPartitionerDependentSerializer;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.repair.RepairJobDesc;
+import org.apache.cassandra.repair.RepairResult;
+import org.apache.cassandra.repair.messages.RepairOption;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableId;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.paxos.Paxos;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.tcm.ClusterMetadataService;
+import org.apache.cassandra.tcm.Epoch;
+import org.apache.cassandra.tcm.transformations.BeginConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.MaybeFinishConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.SetConsensusMigrationTargetProtocol;
+import org.apache.cassandra.utils.NullableSerializer;
+import org.apache.cassandra.utils.PojoToString;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+import static com.google.common.collect.ImmutableList.toImmutableList;
+import static org.apache.cassandra.db.TypeSizes.sizeof;
+import static org.apache.cassandra.dht.Range.intersectionOfNormalizedRanges;
+import static org.apache.cassandra.dht.Range.normalize;
+import static org.apache.cassandra.dht.Range.rangeSerializer;
+import static org.apache.cassandra.dht.Range.subtract;
+import static org.apache.cassandra.dht.Range.subtractNormalizedRanges;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeSet;
+import static org.apache.cassandra.utils.CollectionSerializers.newCollectionSerializer;
+import static org.apache.cassandra.utils.CollectionSerializers.newHashMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeCollection;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedCollectionSize;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedMapSize;
+
+/**
+ * Track and update the migration state of individual table and ranges within those tables
+ */
+public abstract class ConsensusTableMigrationState
+{
+    private static final Logger logger = LoggerFactory.getLogger(ConsensusTableMigrationState.class);
+
+    public static final IPartitionerDependentSerializer<List<Range<Token>>> rangesSerializer = newCollectionSerializer(rangeSerializer);
+
+    public static final FutureCallback<RepairResult> completedRepairJobHandler = new FutureCallback<RepairResult>()
+    {
+        @Override
+        public void onSuccess(@Nullable RepairResult repairResult)
+        {
+            checkNotNull(repairResult, ""repairResult should not be null"");
+            ConsensusMigrationRepairResult migrationResult = repairResult.consensusMigrationRepairResult;
+
+            // Need to repair both Paxos and base table state
+            // Could track them separately, but doesn't seem worth the effort
+            if (migrationResult.consensusMigrationRepairType == ConsensusMigrationRepairType.ineligible)
+                return;
+
+            RepairJobDesc desc = repairResult.desc;
+            ClusterMetadataService.instance.commit(
+                new MaybeFinishConsensusMigrationForTableAndRange(
+                    desc.keyspace, desc.columnFamily, ImmutableList.copyOf(desc.ranges),
+                    migrationResult.minEpoch, migrationResult.consensusMigrationRepairType));
+        }
+
+        @Override
+        public void onFailure(Throwable throwable)
+        {
+            // Only successes drive forward progress
+        }
+    };
+
+    @VisibleForTesting
+    public static void reset()
+    {
+        ClusterMetadataService.reset();
+    }
+
+    public enum ConsensusMigrationRepairType
+    {
+        ineligible(0),
+        paxos(1),
+        accord(2);
+
+        public final byte value;
+
+        ConsensusMigrationRepairType(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationRepairType fromString(String repairType)
+        {
+            return ConsensusMigrationRepairType.valueOf(repairType.toLowerCase());
+        }
+
+        public static ConsensusMigrationRepairType fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return ConsensusMigrationRepairType.paxos;
+                case 1:
+                    return ConsensusMigrationRepairType.accord;
+            }
+        }
+    }
+
+    public enum ConsensusMigrationTarget {
+        paxos(0),
+        accord(1);
+
+        public final byte value;
+
+        ConsensusMigrationTarget(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationTarget fromString(String targetProtocol)
+        {
+            return ConsensusMigrationTarget.valueOf(targetProtocol.toLowerCase());
+        }
+
+        public static ConsensusMigrationTarget fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return paxos;
+                case 1:
+                    return accord;
+            }
+        }
+    }
+
+    public static class ConsensusMigrationRepairResult
+    {
+        private final ConsensusMigrationRepairType consensusMigrationRepairType;
+        private final Epoch minEpoch;
+
+        private ConsensusMigrationRepairResult(ConsensusMigrationRepairType consensusMigrationRepairType, Epoch minEpoch)
+        {
+            this.consensusMigrationRepairType = consensusMigrationRepairType;
+            this.minEpoch = minEpoch;
+        }
+
+        public static ConsensusMigrationRepairResult fromCassandraRepair(Epoch minEpoch, boolean didPaxosAndRegularRepair)
+        {
+            checkArgument(!didPaxosAndRegularRepair || minEpoch.isAfter(Epoch.EMPTY), ""Epoch should not be empty if Paxos and regular repairs were performed"");
+            if (didPaxosAndRegularRepair)
+                return new ConsensusMigrationRepairResult(ConsensusMigrationRepairType.paxos, minEpoch);
+            else
+                return new ConsensusMigrationRepairResult(ConsensusMigrationRepairType.ineligible, Epoch.EMPTY);
+        }
+
+        public static ConsensusMigrationRepairResult fromAccordRepair(Epoch minEpoch)
+        {
+            checkArgument(minEpoch.isAfter(Epoch.EMPTY), ""Accord repairs should always occur at an Epoch"");
+            return new ConsensusMigrationRepairResult(ConsensusMigrationRepairType.accord, minEpoch);
+        }
+    }
+
+    public static class ConsensusMigratedAt
+    {
+        public static final IVersionedSerializer<ConsensusMigratedAt> serializer = NullableSerializer.wrap(new IVersionedSerializer<ConsensusMigratedAt>()
+        {
+            @Override
+            public void serialize(ConsensusMigratedAt t, DataOutputPlus out, int version) throws IOException
+            {
+                Epoch.messageSerializer.serialize(t.migratedAtEpoch, out, version);
+                out.writeByte(t.migratedAtTarget.value);
+            }
+
+            @Override
+            public ConsensusMigratedAt deserialize(DataInputPlus in, int version) throws IOException
+            {
+                Epoch migratedAtEpoch = Epoch.messageSerializer.deserialize(in, version);
+                ConsensusMigrationTarget target = ConsensusMigrationTarget.fromValue(in.readByte());
+                return new ConsensusMigratedAt(migratedAtEpoch, target);
+            }
+
+            @Override
+            public long serializedSize(ConsensusMigratedAt t, int version)
+            {
+                return TypeSizes.sizeof(ConsensusMigrationTarget.accord.value)
+                       + Epoch.messageSerializer.serializedSize(t.migratedAtEpoch, version);
+            }
+        });
+
+        // Fields are not nullable when used for messaging
+        @Nullable
+        public final Epoch migratedAtEpoch;
+
+        @Nullable
+        public final ConsensusMigrationTarget migratedAtTarget;
+
+        public ConsensusMigratedAt(Epoch migratedAtEpoch, ConsensusMigrationTarget migratedAtTarget)
+        {
+            this.migratedAtEpoch = migratedAtEpoch;
+            this.migratedAtTarget = migratedAtTarget;
+        }
+    }
+
+    // TODO Move this into the schema for the table once this is based off of TrM
+    public static class TableMigrationState
+    {
+        @Nonnull
+        public final String keyspaceName;
+
+        @Nonnull
+        public final String tableName;
+
+        @Nonnull
+        public final TableId tableId;
+
+        @Nonnull
+        public final ConsensusMigrationTarget targetProtocol;
+
+        @Nonnull
+        public final List<Range<Token>> migratedRanges;
+
+        /*
+         * Necessary to track which ranges started migrating at which epoch
+         * in order to know whether a repair qualifies in terms of finishing
+         * migration of the range.
+         */
+        @Nonnull
+        public final NavigableMap<Epoch, List<Range<Token>>> migratingRangesByEpoch;
+
+        public static final IPartitionerDependentSerializer<TableMigrationState> serializer = new IPartitionerDependentSerializer<TableMigrationState>()
+        {
+            @Override
+            public void serialize(TableMigrationState t, DataOutputPlus out, int version) throws IOException
+            {
+                out.write(t.targetProtocol.value);
+                out.writeUTF(t.keyspaceName);
+                out.writeUTF(t.tableName);
+                t.tableId.serialize(out);
+                serializeCollection(t.migratedRanges, out, version, rangeSerializer);
+                serializeMap(t.migratingRangesByEpoch, out, version, Epoch.messageSerializer, rangesSerializer);
+            }
+
+            @Override
+            public TableMigrationState deserialize(DataInputPlus in, IPartitioner partitioner, int version) throws IOException
+            {
+                ConsensusMigrationTarget targetProtocol = ConsensusMigrationTarget.fromValue(in.readByte());
+                String keyspaceName = in.readUTF();
+                String tableName = in.readUTF();
+                TableId tableId = TableId.deserialize(in);
+                Set<Range<Token>> migratedRanges = deserializeSet(in, partitioner, version, rangeSerializer);
+                Map<Epoch, List<Range<Token>>> migratingRangesByEpoch = deserializeMap(in, partitioner, version, Epoch.messageSerializer, rangesSerializer, newHashMap());
+                return new TableMigrationState(keyspaceName, tableName, tableId, targetProtocol, migratedRanges, migratingRangesByEpoch);
+            }
+
+            @Override
+            public long serializedSize(TableMigrationState t, int version)
+            {
+                return sizeof(t.targetProtocol.value)
+                        + sizeof(t.keyspaceName)
+                        + sizeof(t.tableName)
+                        + t.tableId.serializedSize()
+                        + serializedCollectionSize(t.migratedRanges, version, rangeSerializer)
+                        + serializedMapSize(t.migratingRangesByEpoch, version, Epoch.messageSerializer, rangesSerializer);
+            }
+        };
+
+        @Nonnull
+        public final List<Range<Token>> migratingRanges;
+
+        @Nonnull
+        public final List<Range<Token>> migratingAndMigratedRanges;
+
+        public TableMigrationState(@Nonnull String keyspaceName,
+                                   @Nonnull String tableName,
+                                   @Nonnull TableId tableId,
+                                   @Nonnull ConsensusMigrationTarget targetProtocol,
+                                   @Nonnull Collection<Range<Token>> migratedRanges,
+                                   @Nonnull Map<Epoch, List<Range<Token>>> migratingRangesByEpoch)
+        {
+            this.keyspaceName = keyspaceName;
+            this.tableName = tableName;
+            this.tableId = tableId;
+            this.targetProtocol = targetProtocol;
+            this.migratedRanges = ImmutableList.copyOf(normalize(migratedRanges));
+            this.migratingRangesByEpoch = ImmutableSortedMap.copyOf(
+            migratingRangesByEpoch.entrySet()
+                                  .stream()
+                                  .map( entry -> new SimpleEntry<>(entry.getKey(), ImmutableList.copyOf(normalize(entry.getValue()))))
+                                  .collect(Collectors.toList()));
+            this.migratingRanges = ImmutableList.copyOf(normalize(migratingRangesByEpoch.values().stream().flatMap(Collection::stream).collect(Collectors.toList())));
+            this.migratingAndMigratedRanges = ImmutableList.copyOf(normalize(ImmutableList.<Range<Token>>builder().addAll(migratedRanges).addAll(migratingRanges).build()));
+        }
+
+        public TableMigrationState withRangesMigrating(@Nonnull Collection<Range<Token>> ranges,
+                                                       @Nonnull ConsensusMigrationTarget target,
+                                                       @Nonnull Epoch epoch)
+        {
+            checkArgument(epoch.isAfter(Epoch.EMPTY), ""Epoch shouldn't be empty"");
+
+            // Doesn't matter which epoch the range started migrating in for this context so merge them all
+            Collection<Range<Token>> migratingRanges = normalize(migratingRangesByEpoch.values().stream().flatMap(Collection::stream).collect(Collectors.toList()));
+            checkArgument(target == targetProtocol, ""Requested migration to target protocol "" + target + "" conflicts with in progress migration to protocol "" + targetProtocol);
+            List<Range<Token>> normalizedRanges = normalize(ranges);
+            checkArgument(!subtract(normalizedRanges, migratingRanges).isEmpty(), ""Range "" + ranges + "" is already being migrated"");
+            Set<Range<Token>> withoutAlreadyMigrated = subtract(normalizedRanges, migratedRanges);
+            checkArgument(!withoutAlreadyMigrated.isEmpty(), ""Range "" + ranges + "" is already migrated"");
+            Set<Range<Token>> withoutBoth = subtract(withoutAlreadyMigrated, migratingRanges);
+            checkArgument(!withoutBoth.isEmpty(), ""Range "" + ranges + "" is already migrating/migrated"");
+
+            if (!Range.equals(normalizedRanges, withoutBoth))
+                logger.warn(""Ranges "" + normalizedRanges + "" to start migrating is already partially migrating/migrated "" + withoutBoth);
+
+            Map<Epoch, List<Range<Token>>> newMigratingRanges = new HashMap<>(migratingRangesByEpoch.size() + 1);
+            newMigratingRanges.putAll(migratingRangesByEpoch);
+            newMigratingRanges.put(epoch, normalizedRanges);
+
+            return new TableMigrationState(keyspaceName, tableName, tableId, targetProtocol, migratedRanges, newMigratingRanges);
+        }
+
+        public TableMigrationState withRangesRepairedAtEpoch(@Nonnull Collection<Range<Token>> ranges,
+                                                             @Nonnull Epoch epoch)
+        {
+            checkArgument(epoch.isAfter(Epoch.EMPTY), ""Epoch shouldn't be empty"");
+
+            List<Range<Token>> normalizedRepairedRanges = normalize(ranges);
+            // This should be inclusive because the epoch we store in the map is the epoch in which the range has been marked migrating
+            // in startMigrationToConsensusProtocol
+            NavigableMap<Epoch, List<Range<Token>>> coveredEpochs = migratingRangesByEpoch.headMap(epoch, true);
+            List<Range<Token>> normalizedMigratingRanges = normalize(coveredEpochs.values().stream().flatMap(Collection::stream).collect(Collectors.toList()));
+            List<Range<Token>> normalizedRepairedIntersection = intersectionOfNormalizedRanges(normalizedRepairedRanges, normalizedMigratingRanges);
+            checkState(!normalizedRepairedIntersection.isEmpty(), ""None of Ranges "" + ranges + "" were being migrated"");
+
+            Map<Epoch, List<Range<Token>>> newMigratingRangesByEpoch = new HashMap<>();
+
+            // Everything in this epoch or later can't have been migrated so re-add all of them
+            newMigratingRangesByEpoch.putAll(migratingRangesByEpoch.tailMap(epoch, false));
+
+            // Include anything still remaining to be migrated after subtracting what was repaired
+            for (Map.Entry<Epoch, List<Range<Token>>> e : coveredEpochs.entrySet())
+            {
+                // Epoch when these ranges started migrating
+                Epoch rangesEpoch = e.getKey();
+                List<Range<Token>> epochMigratingRanges = e.getValue();
+                List<Range<Token>> remainingRanges = subtractNormalizedRanges(epochMigratingRanges, normalizedRepairedIntersection);
+                if (!remainingRanges.isEmpty())
+                    newMigratingRangesByEpoch.put(rangesEpoch, remainingRanges);
+            }
+
+            List<Range<Token>> newMigratedRanges = new ArrayList<>(normalizedMigratingRanges.size() + ranges.size());
+            newMigratedRanges.addAll(migratedRanges);
+            newMigratedRanges.addAll(normalizedRepairedIntersection);
+            return new TableMigrationState(keyspaceName, tableName, tableId, targetProtocol, newMigratedRanges, newMigratingRangesByEpoch);
+        }
+
+        public boolean paxosReadSatisfiedByKeyMigrationAtEpoch(DecoratedKey key, ConsensusMigratedAt consensusMigratedAt)
+        {
+            // This check is being done from a Paxos read attempt which needs to
+            // check if Accord needs to resolve any in flight accord transactions
+            // if the migration target is Accord then nothing needs to be done
+            if (targetProtocol != ConsensusMigrationTarget.paxos)
+                return true;
+
+            return satisfiedByKeyMigrationAtEpoch(key, consensusMigratedAt);
+        }
+
+        public boolean satisfiedByKeyMigrationAtEpoch(@Nonnull DecoratedKey key, @Nullable ConsensusMigratedAt consensusMigratedAt)
+        {
+            if (consensusMigratedAt == null)
+            {
+                // It hasn't been migrated and needs migration if it is in a migrating range
+                return Range.isInNormalizedRanges(key.getToken(), migratingRanges);
+            }
+            else
+            {
+                // It has been migrated and might be from a late enough epoch to satisfy this migration
+                return consensusMigratedAt.migratedAtTarget == targetProtocol
+                       && migratingRangesByEpoch.headMap(consensusMigratedAt.migratedAtEpoch, true).values()
+                                                .stream()
+                                                .flatMap(List::stream)
+                                                .anyMatch(range -> range.contains(key.getToken()));
+            }
+        }
+
+        public Epoch minMigrationEpoch(Token token)
+        {
+            // TODO should there be an index to make this more efficient?
+            for (Map.Entry<Epoch, List<Range<Token>>> e : migratingRangesByEpoch.entrySet())
+            {
+                if (Range.isInNormalizedRanges(token, e.getValue()))
+                    return e.getKey();
+            }
+            return Epoch.EMPTY;
+        }
+
+
+        public @Nonnull TableId getTableId()
+        {
+            return tableId;
+        }
+
+        public TableMigrationState withMigrationTarget(ConsensusMigrationTarget targetProtocol, Epoch epoch)
+        {
+            if (this.targetProtocol == targetProtocol)
+                return this;
+
+            // Migrating ranges remain migrating because individual keys may have already been migrated
+            // So for correctness we need to perform key migration
+            // We do need to update the epoch so that a new repair is required to drive the migration
+            Map<Epoch, List<Range<Token>>> migratingRangesByEpoch = ImmutableMap.of(epoch, migratingRanges);
+
+            Token minToken = ColumnFamilyStore.getIfExists(tableId).getPartitioner().getMinimumToken();
+            Range<Token> fullRange = new Range(minToken, minToken);
+            // What is migrated already is anything that was never migrated/migrating before (untouched)
+            List<Range<Token>> migratedRanges = ImmutableList.copyOf(normalize(fullRange.subtractAll(migratingAndMigratedRanges)));
+
+            return new TableMigrationState(keyspaceName, tableName, tableId, targetProtocol, migratedRanges, migratingRangesByEpoch);
+        }
+
+        public Map<String, Object> toMap()
+        {
+            Builder<String, Object> builder = ImmutableMap.builder();
+            builder.put(""keyspace"", keyspaceName);
+            builder.put(""table"", tableName);
+            builder.put(""tableId"", tableId.toString());
+            builder.put(""targetProtocol"", targetProtocol.toString());
+            builder.put(""migratedRanges"", migratedRanges.stream().map(Objects::toString).collect(toImmutableList()));
+            Map<Long, List<String>> rangesByEpoch = new LinkedHashMap<>();
+            for (Map.Entry<Epoch, List<Range<Token>>> entry : migratingRangesByEpoch.entrySet())
+            {
+                rangesByEpoch.put(entry.getKey().getEpoch(), entry.getValue().stream().map(Objects::toString).collect(toImmutableList()));
+            }
+            builder.put(""migratingRangesByEpoch"", rangesByEpoch);
+            return builder.build();
+        }
+
+        @Override
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+            TableMigrationState that = (TableMigrationState) o;
+            return keyspaceName.equals(that.keyspaceName) && tableName.equals(that.tableName) && tableId.equals(that.tableId) && targetProtocol == that.targetProtocol && migratedRanges.equals(that.migratedRanges) && migratingRangesByEpoch.equals(that.migratingRangesByEpoch) && migratingRanges.equals(that.migratingRanges) && migratingAndMigratedRanges.equals(that.migratingAndMigratedRanges);
+        }
+
+        @Override
+        public int hashCode()
+        {
+            return Objects.hash(keyspaceName, tableName, tableId, targetProtocol, migratedRanges, migratingRangesByEpoch, migratingRanges, migratingAndMigratedRanges);
+        }
+    }
+
+    // TODO this will mostly go away once we can move TableMigrationState into the table schema
+    public static class MigrationStateSnapshot
+    {
+        @Nonnull
+        public final Map<TableId, TableMigrationState> tableStates;
+
+        public final Epoch epoch;
+
+        public static final IVersionedSerializer<MigrationStateSnapshot> messagingSerializer = new IVersionedSerializer<MigrationStateSnapshot>()
+        {
+            @Override
+            public void serialize(MigrationStateSnapshot t, DataOutputPlus out, int version) throws IOException
+            {
+                serializer.serialize(t, out, version);
+            }
+
+            @Override
+            public MigrationStateSnapshot deserialize(DataInputPlus in, int version) throws IOException
+            {
+                return serializer.deserialize(in, DatabaseDescriptor.getPartitioner(), version);
+            }
+
+            @Override
+            public long serializedSize(MigrationStateSnapshot t, int version)
+            {
+                return serializer.serializedSize(t, version);
+            }
+        };
+
+        public static final IPartitionerDependentSerializer<MigrationStateSnapshot> serializer = new IPartitionerDependentSerializer<MigrationStateSnapshot>()
+        {
+            @Override
+            public void serialize(MigrationStateSnapshot migrationStateSnapshot, DataOutputPlus out, int version) throws IOException
+            {
+                Epoch.messageSerializer.serialize(migrationStateSnapshot.epoch, out, version);
+                serializeMap(migrationStateSnapshot.tableStates, out, version, TableId.serializer, TableMigrationState.serializer);
+            }
+
+            @Override
+            public MigrationStateSnapshot deserialize(DataInputPlus in, IPartitioner p, int version) throws IOException
+            {
+                Epoch epoch = Epoch.messageSerializer.deserialize(in, version);
+                Map<TableId, TableMigrationState> tableMigrationStates = deserializeMap(in, p, version, TableId.serializer, TableMigrationState.serializer, newHashMap());
+                return new MigrationStateSnapshot(tableMigrationStates, epoch);
+            }
+
+            @Override
+            public long serializedSize(MigrationStateSnapshot migrationStateSnapshot, int version)
+            {
+                return Epoch.messageSerializer.serializedSize(migrationStateSnapshot.epoch, version) + serializedMapSize(migrationStateSnapshot.tableStates, version, TableId.serializer, TableMigrationState.serializer);
+            }
+        };
+
+        public MigrationStateSnapshot(@Nonnull Map<TableId, TableMigrationState> tableStates, @Nonnull Epoch epoch)
+        {
+            checkNotNull(tableStates, ""tableStates is null"");
+            checkNotNull(epoch, ""epoch is null"");
+            this.tableStates = ImmutableMap.copyOf(tableStates);
+            this.epoch = epoch;
+        }
+
+        public Map<String, Object> toMap(@Nullable Set<String> keyspaceNames, @Nullable Set<String> tableNames)
+        {
+            return ImmutableMap.of(""epoch"", epoch.getEpoch(),
+                                   ""tableStates"", tableStatesAsMaps(keyspaceNames, tableNames),
+                                   ""version"", PojoToString.CURRENT_VERSION);
+        }
+
+        private List<Map<String, Object>> tableStatesAsMaps(@Nullable Set<String> keyspaceNames,
+                                                            @Nullable Set<String> tableNames)
+        {
+            ImmutableList.Builder<Map<String, Object>> builder = ImmutableList.builder();
+            for (TableMigrationState tms : tableStates.values())
+            {
+                if (keyspaceNames != null && !keyspaceNames.contains(tms.keyspaceName))
+                    continue;
+                if (tableNames != null && !tableNames.contains(tms.tableName))
+                    continue;
+                builder.add(tms.toMap());
+            }
+            return builder.build();
+        }
+
+        @Override
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+            MigrationStateSnapshot that = (MigrationStateSnapshot) o;
+            return tableStates.equals(that.tableStates) && epoch.equals(that.epoch);
+        }
+
+        @Override
+        public int hashCode()
+        {
+            return Objects.hash(tableStates, epoch);
+        }
+    }
+
+    private ConsensusTableMigrationState() {}
+
+    // Used by callers to avoid looking up the TMS multiple times
+    public static @Nullable TableMigrationState getTableMigrationState(long epoch, TableId tableId)
+    {
+        ClusterMetadata cm = ClusterMetadataService.instance.maybeCatchup(Epoch.create(0, epoch));
+        TableMigrationState tms = cm.migrationStateSnapshot.tableStates.get(tableId);
+        return tms;
+    }
+
+    /*
+     * Set or change the migration target for the keyspaces and tables. Can be used to reverse the direction of a migration
+     * or instantly migrate a table to a new protocol.
+     */
+    public static void setConsensusMigrationTargetProtocol(@Nonnull String targetProtocolName,
+                                                           @Nonnull List<String> keyspaceNames,
+                                                           @Nonnull Optional<List<String>> maybeTables)
+    {
+        checkArgument(!keyspaceNames.isEmpty(), ""At least one keyspace must be specified"");
+        checkArgument(keyspaceNames.size() == 1 || !maybeTables.isPresent(), ""Can't specify tables with multiple keyspaces"");
+        checkArgument(!maybeTables.isPresent() || !maybeTables.get().isEmpty(), ""Must provide at least 1 table if Optional is not empty"");
+        ConsensusMigrationTarget targetProtocol = ConsensusMigrationTarget.fromString(targetProtocolName);
+
+        if (DatabaseDescriptor.getLegacyPaxosStrategy() == LegacyPaxosStrategy.accord)
+            throw new IllegalStateException(""Mixing a hard coded strategy with migration is unsupported"");
+
+        if (!Paxos.useV2())
+            throw new IllegalStateException(""Can't do any consensus migrations from/to PaxosV1, switch to V2 first"");
+
+        List<TableId> tableIds = keyspacesAndTablesToTableIds(keyspaceNames, maybeTables);
+        ClusterMetadataService.instance.commit(new SetConsensusMigrationTargetProtocol(targetProtocol, tableIds));
+    }
+
+    public static void startMigrationToConsensusProtocol(@Nonnull String targetProtocolName,
+                                                         @Nonnull List<String> keyspaceNames,
+                                                         @Nonnull Optional<List<String>> maybeTables,
+                                                         @Nonnull Optional<String> maybeRangesStr)
+    {
+        checkArgument(!keyspaceNames.isEmpty(), ""At least one keyspace must be specified"");
+        checkArgument(keyspaceNames.size() == 1 || !maybeTables.isPresent(), ""Can't specify tables with multiple keyspaces"");","[{'comment': 'could we interpret empty keyspaces as a command to start migration of all key spaces?', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/ConsensusTableMigrationState.java,"@@ -0,0 +1,697 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.AbstractMap.SimpleEntry;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.stream.Collectors;
+import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableMap.Builder;
+import com.google.common.collect.ImmutableSortedMap;
+import com.google.common.primitives.SignedBytes;
+import com.google.common.util.concurrent.FutureCallback;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.config.Config.LegacyPaxosStrategy;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.IPartitionerDependentSerializer;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.repair.RepairJobDesc;
+import org.apache.cassandra.repair.RepairResult;
+import org.apache.cassandra.repair.messages.RepairOption;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.schema.TableId;
+import org.apache.cassandra.schema.TableMetadata;
+import org.apache.cassandra.service.paxos.Paxos;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.tcm.ClusterMetadataService;
+import org.apache.cassandra.tcm.Epoch;
+import org.apache.cassandra.tcm.transformations.BeginConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.MaybeFinishConsensusMigrationForTableAndRange;
+import org.apache.cassandra.tcm.transformations.SetConsensusMigrationTargetProtocol;
+import org.apache.cassandra.utils.NullableSerializer;
+import org.apache.cassandra.utils.PojoToString;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+import static com.google.common.collect.ImmutableList.toImmutableList;
+import static org.apache.cassandra.db.TypeSizes.sizeof;
+import static org.apache.cassandra.dht.Range.intersectionOfNormalizedRanges;
+import static org.apache.cassandra.dht.Range.normalize;
+import static org.apache.cassandra.dht.Range.rangeSerializer;
+import static org.apache.cassandra.dht.Range.subtract;
+import static org.apache.cassandra.dht.Range.subtractNormalizedRanges;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.deserializeSet;
+import static org.apache.cassandra.utils.CollectionSerializers.newCollectionSerializer;
+import static org.apache.cassandra.utils.CollectionSerializers.newHashMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeCollection;
+import static org.apache.cassandra.utils.CollectionSerializers.serializeMap;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedCollectionSize;
+import static org.apache.cassandra.utils.CollectionSerializers.serializedMapSize;
+
+/**
+ * Track and update the migration state of individual table and ranges within those tables
+ */
+public abstract class ConsensusTableMigrationState
+{
+    private static final Logger logger = LoggerFactory.getLogger(ConsensusTableMigrationState.class);
+
+    public static final IPartitionerDependentSerializer<List<Range<Token>>> rangesSerializer = newCollectionSerializer(rangeSerializer);
+
+    public static final FutureCallback<RepairResult> completedRepairJobHandler = new FutureCallback<RepairResult>()
+    {
+        @Override
+        public void onSuccess(@Nullable RepairResult repairResult)
+        {
+            checkNotNull(repairResult, ""repairResult should not be null"");
+            ConsensusMigrationRepairResult migrationResult = repairResult.consensusMigrationRepairResult;
+
+            // Need to repair both Paxos and base table state
+            // Could track them separately, but doesn't seem worth the effort
+            if (migrationResult.consensusMigrationRepairType == ConsensusMigrationRepairType.ineligible)
+                return;
+
+            RepairJobDesc desc = repairResult.desc;
+            ClusterMetadataService.instance.commit(
+                new MaybeFinishConsensusMigrationForTableAndRange(
+                    desc.keyspace, desc.columnFamily, ImmutableList.copyOf(desc.ranges),
+                    migrationResult.minEpoch, migrationResult.consensusMigrationRepairType));
+        }
+
+        @Override
+        public void onFailure(Throwable throwable)
+        {
+            // Only successes drive forward progress
+        }
+    };
+
+    @VisibleForTesting
+    public static void reset()
+    {
+        ClusterMetadataService.reset();
+    }
+
+    public enum ConsensusMigrationRepairType
+    {
+        ineligible(0),
+        paxos(1),
+        accord(2);
+
+        public final byte value;
+
+        ConsensusMigrationRepairType(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationRepairType fromString(String repairType)
+        {
+            return ConsensusMigrationRepairType.valueOf(repairType.toLowerCase());
+        }
+
+        public static ConsensusMigrationRepairType fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return ConsensusMigrationRepairType.paxos;
+                case 1:
+                    return ConsensusMigrationRepairType.accord;
+            }
+        }
+    }
+
+    public enum ConsensusMigrationTarget {
+        paxos(0),
+        accord(1);
+
+        public final byte value;
+
+        ConsensusMigrationTarget(int value)
+        {
+            this.value = SignedBytes.checkedCast(value);
+        }
+
+        public static ConsensusMigrationTarget fromString(String targetProtocol)
+        {
+            return ConsensusMigrationTarget.valueOf(targetProtocol.toLowerCase());
+        }
+
+        public static ConsensusMigrationTarget fromValue(byte value)
+        {
+            switch (value)
+            {
+                default:
+                    throw new IllegalArgumentException(value + "" is not recognized"");
+                case 0:
+                    return paxos;
+                case 1:
+                    return accord;
+            }
+        }
+    }
+
+    public static class ConsensusMigrationRepairResult
+    {
+        private final ConsensusMigrationRepairType consensusMigrationRepairType;
+        private final Epoch minEpoch;
+
+        private ConsensusMigrationRepairResult(ConsensusMigrationRepairType consensusMigrationRepairType, Epoch minEpoch)
+        {
+            this.consensusMigrationRepairType = consensusMigrationRepairType;
+            this.minEpoch = minEpoch;
+        }
+
+        public static ConsensusMigrationRepairResult fromCassandraRepair(Epoch minEpoch, boolean didPaxosAndRegularRepair)
+        {
+            checkArgument(!didPaxosAndRegularRepair || minEpoch.isAfter(Epoch.EMPTY), ""Epoch should not be empty if Paxos and regular repairs were performed"");
+            if (didPaxosAndRegularRepair)
+                return new ConsensusMigrationRepairResult(ConsensusMigrationRepairType.paxos, minEpoch);
+            else
+                return new ConsensusMigrationRepairResult(ConsensusMigrationRepairType.ineligible, Epoch.EMPTY);
+        }
+
+        public static ConsensusMigrationRepairResult fromAccordRepair(Epoch minEpoch)
+        {
+            checkArgument(minEpoch.isAfter(Epoch.EMPTY), ""Accord repairs should always occur at an Epoch"");
+            return new ConsensusMigrationRepairResult(ConsensusMigrationRepairType.accord, minEpoch);
+        }
+    }
+
+    public static class ConsensusMigratedAt
+    {
+        public static final IVersionedSerializer<ConsensusMigratedAt> serializer = NullableSerializer.wrap(new IVersionedSerializer<ConsensusMigratedAt>()
+        {
+            @Override
+            public void serialize(ConsensusMigratedAt t, DataOutputPlus out, int version) throws IOException
+            {
+                Epoch.messageSerializer.serialize(t.migratedAtEpoch, out, version);
+                out.writeByte(t.migratedAtTarget.value);
+            }
+
+            @Override
+            public ConsensusMigratedAt deserialize(DataInputPlus in, int version) throws IOException
+            {
+                Epoch migratedAtEpoch = Epoch.messageSerializer.deserialize(in, version);
+                ConsensusMigrationTarget target = ConsensusMigrationTarget.fromValue(in.readByte());
+                return new ConsensusMigratedAt(migratedAtEpoch, target);
+            }
+
+            @Override
+            public long serializedSize(ConsensusMigratedAt t, int version)
+            {
+                return TypeSizes.sizeof(ConsensusMigrationTarget.accord.value)
+                       + Epoch.messageSerializer.serializedSize(t.migratedAtEpoch, version);
+            }
+        });
+
+        // Fields are not nullable when used for messaging
+        @Nullable
+        public final Epoch migratedAtEpoch;
+
+        @Nullable
+        public final ConsensusMigrationTarget migratedAtTarget;
+
+        public ConsensusMigratedAt(Epoch migratedAtEpoch, ConsensusMigrationTarget migratedAtTarget)
+        {
+            this.migratedAtEpoch = migratedAtEpoch;
+            this.migratedAtTarget = migratedAtTarget;
+        }
+    }
+
+    // TODO Move this into the schema for the table once this is based off of TrM
+    public static class TableMigrationState
+    {
+        @Nonnull
+        public final String keyspaceName;
+
+        @Nonnull
+        public final String tableName;
+
+        @Nonnull
+        public final TableId tableId;
+
+        @Nonnull
+        public final ConsensusMigrationTarget targetProtocol;
+
+        @Nonnull
+        public final List<Range<Token>> migratedRanges;
+
+        /*
+         * Necessary to track which ranges started migrating at which epoch
+         * in order to know whether a repair qualifies in terms of finishing
+         * migration of the range.
+         */
+        @Nonnull
+        public final NavigableMap<Epoch, List<Range<Token>>> migratingRangesByEpoch;
+
+        public static final IPartitionerDependentSerializer<TableMigrationState> serializer = new IPartitionerDependentSerializer<TableMigrationState>()
+        {
+            @Override
+            public void serialize(TableMigrationState t, DataOutputPlus out, int version) throws IOException
+            {
+                out.write(t.targetProtocol.value);
+                out.writeUTF(t.keyspaceName);
+                out.writeUTF(t.tableName);
+                t.tableId.serialize(out);
+                serializeCollection(t.migratedRanges, out, version, rangeSerializer);
+                serializeMap(t.migratingRangesByEpoch, out, version, Epoch.messageSerializer, rangesSerializer);
+            }
+
+            @Override
+            public TableMigrationState deserialize(DataInputPlus in, IPartitioner partitioner, int version) throws IOException
+            {
+                ConsensusMigrationTarget targetProtocol = ConsensusMigrationTarget.fromValue(in.readByte());
+                String keyspaceName = in.readUTF();
+                String tableName = in.readUTF();
+                TableId tableId = TableId.deserialize(in);
+                Set<Range<Token>> migratedRanges = deserializeSet(in, partitioner, version, rangeSerializer);
+                Map<Epoch, List<Range<Token>>> migratingRangesByEpoch = deserializeMap(in, partitioner, version, Epoch.messageSerializer, rangesSerializer, newHashMap());
+                return new TableMigrationState(keyspaceName, tableName, tableId, targetProtocol, migratedRanges, migratingRangesByEpoch);
+            }
+
+            @Override
+            public long serializedSize(TableMigrationState t, int version)
+            {
+                return sizeof(t.targetProtocol.value)
+                        + sizeof(t.keyspaceName)
+                        + sizeof(t.tableName)
+                        + t.tableId.serializedSize()
+                        + serializedCollectionSize(t.migratedRanges, version, rangeSerializer)
+                        + serializedMapSize(t.migratingRangesByEpoch, version, Epoch.messageSerializer, rangesSerializer);
+            }
+        };
+
+        @Nonnull
+        public final List<Range<Token>> migratingRanges;
+
+        @Nonnull
+        public final List<Range<Token>> migratingAndMigratedRanges;
+
+        public TableMigrationState(@Nonnull String keyspaceName,
+                                   @Nonnull String tableName,
+                                   @Nonnull TableId tableId,
+                                   @Nonnull ConsensusMigrationTarget targetProtocol,
+                                   @Nonnull Collection<Range<Token>> migratedRanges,
+                                   @Nonnull Map<Epoch, List<Range<Token>>> migratingRangesByEpoch)
+        {
+            this.keyspaceName = keyspaceName;
+            this.tableName = tableName;
+            this.tableId = tableId;
+            this.targetProtocol = targetProtocol;
+            this.migratedRanges = ImmutableList.copyOf(normalize(migratedRanges));
+            this.migratingRangesByEpoch = ImmutableSortedMap.copyOf(
+            migratingRangesByEpoch.entrySet()
+                                  .stream()
+                                  .map( entry -> new SimpleEntry<>(entry.getKey(), ImmutableList.copyOf(normalize(entry.getValue()))))
+                                  .collect(Collectors.toList()));
+            this.migratingRanges = ImmutableList.copyOf(normalize(migratingRangesByEpoch.values().stream().flatMap(Collection::stream).collect(Collectors.toList())));
+            this.migratingAndMigratedRanges = ImmutableList.copyOf(normalize(ImmutableList.<Range<Token>>builder().addAll(migratedRanges).addAll(migratingRanges).build()));
+        }
+
+        public TableMigrationState withRangesMigrating(@Nonnull Collection<Range<Token>> ranges,
+                                                       @Nonnull ConsensusMigrationTarget target,
+                                                       @Nonnull Epoch epoch)
+        {
+            checkArgument(epoch.isAfter(Epoch.EMPTY), ""Epoch shouldn't be empty"");","[{'comment': ""should validation be more tolerant of overlapping/duplicate begin migration commands? I don't think there's any real downside to logging that the range has already been migrated or is migrating so the command will be either be a noop or will affect a subset of the specified ranges. I'd imagine this will make life easier for operators at least."", 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/StorageProxy.java,"@@ -2995,6 +3048,39 @@ public final boolean equals(Object o)
         }
     }
 
+    public static class ConsensusAttemptResult","[{'comment': ""wdyt about having a single private ctor, and having 3 static construction methods that named something like `txnResult`, `paxosResult`, and `switchProtocol`, or something like that? At least in the case of `retry_new_protocol`, it would make it a bit easier to understand what's happening without having to remember how the various ctor invocations affect it's state"", 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -83,6 +90,10 @@ private static long getThreadId(ExecutorService executor)
     private final ProgressLog progressLog;
     private final RangesForEpochHolder rangesForEpochHolder;
 
+    // TODO (expected): schedule regular pruning of this collection, and this is not persistent!
+    @Nullable
+    ReducingRangeMap<Timestamp> rejectBefore;","[{'comment': 'should this be addressed? I think persistence could be mostly copied from paxos repair history as well, and scheduling a periodic command store maintenance task is pretty reasonable.', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -110,6 +121,38 @@ public int id()
         return id;
     }
 
+    @Override
+    public void setRejectBefore(ReducingRangeMap<Timestamp> newRejectBefore)
+    {
+        this.rejectBefore = newRejectBefore;
+    }
+
+    @Override
+    public Timestamp preaccept(TxnId txnId, Seekables<?, ?> keys, SafeCommandStore safeStore)
+    {
+        NodeTimeService time = safeStore.time();","[{'comment': 'see my accord review comment about generalizing this on `InMemoryCommandStore`', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/accord/serializers/RecoverySerializers.java,"@@ -90,11 +91,12 @@ void serializeOk(RecoverOk recoverOk, DataOutputPlus out, int version) throws IO
             CommandSerializers.ballot.serialize(recoverOk.accepted, out, version);
             serializeNullable(recoverOk.executeAt, out, version, CommandSerializers.timestamp);
             DepsSerializer.partialDeps.serialize(recoverOk.deps, out, version);
+            DepsSerializer.partialDeps.serialize(recoverOk.acceptedDeps, out, version);","[{'comment': ""in many cases, I'd expect these to be identical to `deps`. Can we add a flags field, and use it to indicate `acceptDeps == deps`, and skip serialization/deserialization?"", 'commenter': 'bdeggleston'}, {'comment': ""in many cases, I'd expect these to be identical to `deps`. Can we add a flags field, and use it to indicate `acceptDeps == deps`, and skip serialization/deserialization?"", 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/accord/txn/TxnWrite.java,"@@ -48,24 +52,28 @@
 import org.apache.cassandra.db.RegularAndStaticColumns;
 import org.apache.cassandra.db.TypeSizes;
 import org.apache.cassandra.db.partitions.PartitionUpdate;
+import org.apache.cassandra.db.rows.Cell;
+import org.apache.cassandra.db.rows.CellPath;
 import org.apache.cassandra.db.rows.Row;
 import org.apache.cassandra.io.IVersionedSerializer;
 import org.apache.cassandra.io.util.DataInputPlus;
 import org.apache.cassandra.io.util.DataOutputPlus;
 import org.apache.cassandra.schema.ColumnMetadata;
-import org.apache.cassandra.service.accord.AccordSafeCommandsForKey;
 import org.apache.cassandra.service.accord.AccordSafeCommandStore;
+import org.apache.cassandra.service.accord.AccordSafeCommandsForKey;
 import org.apache.cassandra.service.accord.api.PartitionKey;
 import org.apache.cassandra.utils.ByteBufferUtil;
 import org.apache.cassandra.utils.ObjectSizes;
 
-import static org.apache.cassandra.utils.ArraySerializers.deserializeArray;
 import static org.apache.cassandra.service.accord.AccordSerializers.partitionUpdateSerializer;
+import static org.apache.cassandra.utils.ArraySerializers.deserializeArray;
 import static org.apache.cassandra.utils.ArraySerializers.serializeArray;
 import static org.apache.cassandra.utils.ArraySerializers.serializedArraySize;
 
 public class TxnWrite extends AbstractKeySorted<TxnWrite.Update> implements Write
 {
+    private static final Logger logger = LoggerFactory.getLogger(TxnWrite.class);","[{'comment': 'unused', 'commenter': 'bdeggleston'}]"
2237,src/java/org/apache/cassandra/service/accord/txn/TxnNamedRead.java,"@@ -122,24 +149,68 @@ public AsyncChain<Data> read(boolean isForWriteTxn, SafeCommandStore safeStore,
         // immediately after the transaction executed, and this simplifies things a great deal
         int nowInSeconds = (int) TimeUnit.MICROSECONDS.toSeconds(executeAt.hlc());
 
+        if (ConsensusRequestRouter.instance.isKeyInMigratingRangeFromPaxos(tms, key))
+            return performCoordinatedRead(consistencyLevel, command, nowInSeconds, metrics);
+        else
+            return performLocalRead(command, nowInSeconds);
+    }
+
+    private AsyncChain<Data> performCoordinatedRead(ConsistencyLevel consistencyLevel, SinglePartitionReadCommand command, int nowInSeconds, AccordClientRequestMetrics metrics)","[{'comment': ""I think it would be possible to read data from 'the future'. If we read from a replica that has already applied this operation, we will break correctness. This is especially possible in recovery."", 'commenter': 'bdeggleston'}]"
2251,src/java/org/apache/cassandra/db/SSTableImporter.java,"@@ -130,6 +131,8 @@ synchronized List<String> importNewSSTables(Options options)
             {
                 try
                 {
+                    if (abort)
+                        throw new InterruptedException(""SSTables import has been aborted"");","[{'comment': ""If the only reason for aborting is because of draining. Would using `operationMode` simplify the implementation? The changes in CFS and StorageService can be dropped. \r\n\r\n```\r\nif (StorageService.instance.getOperationMode().equalsIgnoreCase(StorageService.Mode.DRAINING.toString()))\r\n``` \r\n\r\nAnother question is that is it the only place to abort the import? The sstables could be opening already before invoking `drain`. Those sstables won't be aborted. \r\nIt is not possible/clean to add the check everywhere. It might make sense to add one extra check before line#183 to avoid loading and building SI of the newly opened sstables. "", 'commenter': 'yifan-c'}, {'comment': 'Done.', 'commenter': '5'}]"
2251,test/unit/org/apache/cassandra/service/StorageServiceDrainTest.java,"@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.List;","[{'comment': 'unused import', 'commenter': 'yifan-c'}, {'comment': 'Removed.', 'commenter': '5'}]"
2251,src/java/org/apache/cassandra/db/SSTableImporter.java,"@@ -182,19 +185,35 @@ synchronized List<String> importNewSSTables(Options options)
 
         try (Refs<SSTableReader> refs = Refs.ref(newSSTables))
         {
+            abortIfDraining();
             cfs.getTracker().addSSTables(newSSTables);
             for (SSTableReader reader : newSSTables)
             {
                 if (options.invalidateCaches && cfs.isRowCacheEnabled())
                     invalidateCachesForSSTable(reader);
             }
-
+        }
+        catch (Throwable t)
+        {
+            logger.error(""[{}] FaiFailed adding sstables"", importID, t);","[{'comment': 'Typo? `FaiFailed`?', 'commenter': 'dineshjoshi'}, {'comment': 'Fixed.', 'commenter': '5'}]"
2251,src/java/org/apache/cassandra/db/SSTableImporter.java,"@@ -182,19 +185,35 @@ synchronized List<String> importNewSSTables(Options options)
 
         try (Refs<SSTableReader> refs = Refs.ref(newSSTables))
         {
+            abortIfDraining();
             cfs.getTracker().addSSTables(newSSTables);
             for (SSTableReader reader : newSSTables)
             {
                 if (options.invalidateCaches && cfs.isRowCacheEnabled())
                     invalidateCachesForSSTable(reader);
             }
-
+        }
+        catch (Throwable t)
+        {
+            logger.error(""[{}] FaiFailed adding sstables"", importID, t);
+            throw new RuntimeException(""Failed adding sstables"", t);","[{'comment': 'Nit: `sstables` -> `SSTables`', 'commenter': 'dineshjoshi'}, {'comment': 'Fixed here and in six other places within this method.', 'commenter': '5'}]"
2256,src/java/org/apache/cassandra/io/util/TrackedDataOutputPlus.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.util;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.apache.cassandra.utils.vint.VIntCoding;
+
+public class TrackedDataOutputPlus implements DataOutputPlus
+{
+    private final DataOutputPlus out;
+    private int position = 0;
+
+    private TrackedDataOutputPlus(DataOutputPlus out)
+    {
+        this.out = out;
+    }
+
+    public static TrackedDataOutputPlus wrap(DataOutputPlus out)
+    {
+        return new TrackedDataOutputPlus(out);
+    }
+
+    @Override
+    public void write(int b) throws IOException
+    {
+        out.write(b);
+        position += 1;
+    }
+
+    @Override
+    public void write(byte[] b) throws IOException
+    {
+        out.write(b);
+        position += b.length;
+    }
+
+    @Override
+    public void write(byte[] b, int off, int len) throws IOException
+    {
+        out.write(b, off, len);
+        position += len;
+    }
+
+    @Override
+    public void writeBoolean(boolean v) throws IOException
+    {
+        out.writeBoolean(v);
+        position += 1;
+    }
+
+    @Override
+    public void writeByte(int v) throws IOException
+    {
+        out.writeByte(v);
+        position += 1;
+    }
+
+    @Override
+    public void writeShort(int v) throws IOException
+    {
+        out.writeShort(v);
+        position += 2;
+    }
+
+    @Override
+    public void writeChar(int v) throws IOException
+    {
+        out.writeChar(v);
+        position += 2;
+    }
+
+    @Override
+    public void writeInt(int v) throws IOException
+    {
+        out.writeInt(v);
+        position += 4;
+    }
+
+    @Override
+    public void writeLong(long v) throws IOException
+    {
+        out.writeLong(v);
+        position += 8;
+    }
+
+    @Override
+    public void writeFloat(float v) throws IOException
+    {
+        out.writeFloat(v);
+        position += 4;
+    }
+
+    @Override
+    public void writeDouble(double v) throws IOException
+    {
+        out.writeDouble(v);
+        position += 8;
+    }
+
+    @Override
+    public void writeBytes(String s) throws IOException
+    {
+        out.writeBytes(s);
+        position += s.length();
+    }
+
+    @Override
+    public void writeChars(String s) throws IOException
+    {
+        out.writeChars(s);
+        position += s.length() * 2;
+    }
+
+    @Override
+    public void writeUTF(String s) throws IOException
+    {
+        UnbufferedDataOutputStreamPlus.writeUTF(s, out);","[{'comment': 'missing `position`', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/io/util/TrackedDataOutputPlus.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.util;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.apache.cassandra.utils.vint.VIntCoding;
+
+public class TrackedDataOutputPlus implements DataOutputPlus
+{
+    private final DataOutputPlus out;
+    private int position = 0;
+
+    private TrackedDataOutputPlus(DataOutputPlus out)
+    {
+        this.out = out;
+    }
+
+    public static TrackedDataOutputPlus wrap(DataOutputPlus out)
+    {
+        return new TrackedDataOutputPlus(out);
+    }
+
+    @Override
+    public void write(int b) throws IOException
+    {
+        out.write(b);
+        position += 1;
+    }
+
+    @Override
+    public void write(byte[] b) throws IOException
+    {
+        out.write(b);
+        position += b.length;
+    }
+
+    @Override
+    public void write(byte[] b, int off, int len) throws IOException
+    {
+        out.write(b, off, len);
+        position += len;
+    }
+
+    @Override
+    public void writeBoolean(boolean v) throws IOException
+    {
+        out.writeBoolean(v);
+        position += 1;
+    }
+
+    @Override
+    public void writeByte(int v) throws IOException
+    {
+        out.writeByte(v);
+        position += 1;
+    }
+
+    @Override
+    public void writeShort(int v) throws IOException
+    {
+        out.writeShort(v);
+        position += 2;
+    }
+
+    @Override
+    public void writeChar(int v) throws IOException
+    {
+        out.writeChar(v);
+        position += 2;
+    }
+
+    @Override
+    public void writeInt(int v) throws IOException
+    {
+        out.writeInt(v);
+        position += 4;
+    }
+
+    @Override
+    public void writeLong(long v) throws IOException
+    {
+        out.writeLong(v);
+        position += 8;
+    }
+
+    @Override
+    public void writeFloat(float v) throws IOException
+    {
+        out.writeFloat(v);
+        position += 4;
+    }
+
+    @Override
+    public void writeDouble(double v) throws IOException
+    {
+        out.writeDouble(v);
+        position += 8;
+    }
+
+    @Override
+    public void writeBytes(String s) throws IOException
+    {
+        out.writeBytes(s);
+        position += s.length();
+    }
+
+    @Override
+    public void writeChars(String s) throws IOException
+    {
+        out.writeChars(s);
+        position += s.length() * 2;
+    }
+
+    @Override
+    public void writeUTF(String s) throws IOException
+    {
+        UnbufferedDataOutputStreamPlus.writeUTF(s, out);
+    }
+
+    @Override
+    public void write(ByteBuffer buffer) throws IOException
+    {
+        out.write(buffer);
+        position += buffer.remaining();","[{'comment': ""to be safe shouldn't this be first?  just incase `out.write` consumes?"", 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/io/util/TrackedDataOutputPlus.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.util;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.apache.cassandra.utils.vint.VIntCoding;
+
+public class TrackedDataOutputPlus implements DataOutputPlus
+{
+    private final DataOutputPlus out;
+    private int position = 0;
+
+    private TrackedDataOutputPlus(DataOutputPlus out)
+    {
+        this.out = out;
+    }
+
+    public static TrackedDataOutputPlus wrap(DataOutputPlus out)
+    {
+        return new TrackedDataOutputPlus(out);
+    }
+
+    @Override
+    public void write(int b) throws IOException
+    {
+        out.write(b);
+        position += 1;
+    }
+
+    @Override
+    public void write(byte[] b) throws IOException
+    {
+        out.write(b);
+        position += b.length;
+    }
+
+    @Override
+    public void write(byte[] b, int off, int len) throws IOException
+    {
+        out.write(b, off, len);
+        position += len;
+    }
+
+    @Override
+    public void writeBoolean(boolean v) throws IOException
+    {
+        out.writeBoolean(v);
+        position += 1;
+    }
+
+    @Override
+    public void writeByte(int v) throws IOException
+    {
+        out.writeByte(v);
+        position += 1;
+    }
+
+    @Override
+    public void writeShort(int v) throws IOException
+    {
+        out.writeShort(v);
+        position += 2;
+    }
+
+    @Override
+    public void writeChar(int v) throws IOException
+    {
+        out.writeChar(v);
+        position += 2;
+    }
+
+    @Override
+    public void writeInt(int v) throws IOException
+    {
+        out.writeInt(v);
+        position += 4;
+    }
+
+    @Override
+    public void writeLong(long v) throws IOException
+    {
+        out.writeLong(v);
+        position += 8;
+    }
+
+    @Override
+    public void writeFloat(float v) throws IOException
+    {
+        out.writeFloat(v);
+        position += 4;
+    }
+
+    @Override
+    public void writeDouble(double v) throws IOException
+    {
+        out.writeDouble(v);
+        position += 8;
+    }
+
+    @Override
+    public void writeBytes(String s) throws IOException
+    {
+        out.writeBytes(s);
+        position += s.length();
+    }
+
+    @Override
+    public void writeChars(String s) throws IOException
+    {
+        out.writeChars(s);
+        position += s.length() * 2;
+    }
+
+    @Override
+    public void writeUTF(String s) throws IOException
+    {
+        UnbufferedDataOutputStreamPlus.writeUTF(s, out);
+    }
+
+    @Override
+    public void write(ByteBuffer buffer) throws IOException
+    {
+        out.write(buffer);
+        position += buffer.remaining();
+    }
+
+    @Override
+    public void write(ReadableMemory memory, long offset, long length) throws IOException
+    {
+        out.write(memory, offset, length);
+        position += length;
+    }
+
+    @Override
+    public void writeVInt(long i) throws IOException
+    {
+        VIntCoding.writeVInt(i, this);","[{'comment': 'missing `position`', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/io/util/TrackedDataOutputPlus.java,"@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.util;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.apache.cassandra.utils.vint.VIntCoding;
+
+public class TrackedDataOutputPlus implements DataOutputPlus
+{
+    private final DataOutputPlus out;
+    private int position = 0;
+
+    private TrackedDataOutputPlus(DataOutputPlus out)
+    {
+        this.out = out;
+    }
+
+    public static TrackedDataOutputPlus wrap(DataOutputPlus out)
+    {
+        return new TrackedDataOutputPlus(out);
+    }
+
+    @Override
+    public void write(int b) throws IOException
+    {
+        out.write(b);
+        position += 1;
+    }
+
+    @Override
+    public void write(byte[] b) throws IOException
+    {
+        out.write(b);
+        position += b.length;
+    }
+
+    @Override
+    public void write(byte[] b, int off, int len) throws IOException
+    {
+        out.write(b, off, len);
+        position += len;
+    }
+
+    @Override
+    public void writeBoolean(boolean v) throws IOException
+    {
+        out.writeBoolean(v);
+        position += 1;
+    }
+
+    @Override
+    public void writeByte(int v) throws IOException
+    {
+        out.writeByte(v);
+        position += 1;
+    }
+
+    @Override
+    public void writeShort(int v) throws IOException
+    {
+        out.writeShort(v);
+        position += 2;
+    }
+
+    @Override
+    public void writeChar(int v) throws IOException
+    {
+        out.writeChar(v);
+        position += 2;
+    }
+
+    @Override
+    public void writeInt(int v) throws IOException
+    {
+        out.writeInt(v);
+        position += 4;
+    }
+
+    @Override
+    public void writeLong(long v) throws IOException
+    {
+        out.writeLong(v);
+        position += 8;
+    }
+
+    @Override
+    public void writeFloat(float v) throws IOException
+    {
+        out.writeFloat(v);
+        position += 4;
+    }
+
+    @Override
+    public void writeDouble(double v) throws IOException
+    {
+        out.writeDouble(v);
+        position += 8;
+    }
+
+    @Override
+    public void writeBytes(String s) throws IOException
+    {
+        out.writeBytes(s);
+        position += s.length();
+    }
+
+    @Override
+    public void writeChars(String s) throws IOException
+    {
+        out.writeChars(s);
+        position += s.length() * 2;
+    }
+
+    @Override
+    public void writeUTF(String s) throws IOException
+    {
+        UnbufferedDataOutputStreamPlus.writeUTF(s, out);
+    }
+
+    @Override
+    public void write(ByteBuffer buffer) throws IOException
+    {
+        out.write(buffer);
+        position += buffer.remaining();
+    }
+
+    @Override
+    public void write(ReadableMemory memory, long offset, long length) throws IOException
+    {
+        out.write(memory, offset, length);
+        position += length;
+    }
+
+    @Override
+    public void writeVInt(long i) throws IOException
+    {
+        VIntCoding.writeVInt(i, this);
+    }
+
+    @Override
+    public void writeUnsignedVInt(long i) throws IOException
+    {
+        VIntCoding.writeUnsignedVInt(i, this);","[{'comment': 'missing `position`', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/utils/NoSpamLogger.java,"@@ -218,6 +218,11 @@ private NoSpamLogger(Logger wrapped, long minInterval, TimeUnit timeUnit)
         minIntervalNanos = timeUnit.toNanos(minInterval);
     }
 
+    public static NoSpamLogger wrap(Logger wrapped, long minInterval, TimeUnit timeUnit)","[{'comment': 'any reason to use this vs `org.apache.cassandra.utils.NoSpamLogger#getLogger`?', 'commenter': 'dcapwell'}]"
2256,test/unit/org/apache/cassandra/service/accord/AccordTestUtils.java,"@@ -331,7 +336,8 @@ public static AccordCommandStore createAccordCommandStore(Node.Id node, LongSupp
                                       new AccordAgent(),
                                       null,
                                       cs -> NOOP_PROGRESS_LOG,
-                                      new SingleEpochRanges(topology.rangesForNode(node)));
+                                      new SingleEpochRanges(topology.rangesForNode(node)),
+                                      NOOP_ACCORD_JOURNAL);","[{'comment': 'kinda feel we might want to always run, but may want to run with an in-memory file system (like what simulator does)', 'commenter': 'dcapwell'}]"
2256,test/unit/org/apache/cassandra/journal/DescriptorTest.java,"@@ -0,0 +1,24 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import org.junit.Test;
+
+public class DescriptorTest","[{'comment': ""can you remove or add tests?  This class causes the build to fail due to the import that isn't used"", 'commenter': 'dcapwell'}]"
2256,test/unit/org/apache/cassandra/journal/JournalTest.java,"@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.Collections;
+
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.utils.TimeUUID;
+
+import static junit.framework.TestCase.assertEquals;","[{'comment': 'this causes the build to fail, I believe you want `import static org.junit.Assert.assertEquals;`', 'commenter': 'dcapwell'}]"
2256,test/unit/org/apache/cassandra/journal/SyncedOffsetsTest.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.file.Files;
+
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.util.File;
+
+import static junit.framework.TestCase.assertEquals;","[{'comment': 'this causes the build to fail, believe you want `import static org.junit.Assert.assertEquals;`', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/InMemoryIndex.java,"@@ -0,0 +1,136 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.NavigableMap;
+import java.util.TreeMap;
+import java.util.concurrent.ConcurrentSkipListMap;
+import java.util.concurrent.atomic.AtomicReference;
+
+import javax.annotation.Nullable;
+
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.io.util.FileOutputStreamPlus;
+
+/**
+ * An index for a segment that's still being updated by journal writers concurrently.
+ */
+final class InMemoryIndex<K> implements Index<K>
+{
+    private static final int[] EMPTY = new int[0];
+
+    private final KeySupport<K> keySupport;
+    private final NavigableMap<K, int[]> index;
+    private final AtomicReference<K> lastId;","[{'comment': 'it would be good for maintenance to explain why this exists.  As far as I can tell reading the code `return index.isEmpty() ? null : index.lastKey();` is `O(N)` complexity, so this only exists to make that `O(1)`, which is leveraged in `org.apache.cassandra.journal.Index#mayContainId`', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Index.java,"@@ -0,0 +1,79 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import javax.annotation.Nullable;
+
+import org.apache.cassandra.utils.Closeable;
+
+/**
+ * Mapping of client supplied ids to in-segment offsets
+ */
+interface Index<K> extends Closeable
+{
+    /**
+     * Update the index with a new entry with id and offset
+     */
+    void update(K id, int offset);
+
+    /**
+     * Look up offsets by id. It's possible, due to retries, for a segment
+     * to contain the same record with the same id more than once, at
+     * different offsets.
+     *
+     * @return the found offsets into the segment, if any; can be empty
+     */
+    int[] lookUp(K id);
+
+    /**
+     * Look up offsets by id. It's possible, due to retries, for a segment
+     * to contain the same record with the same id more than once, at
+     * different offsets. Return the first offset for provided record id, or -1 if none.
+     *
+     * @return the first offset into the segment, or -1 is none were found
+     */
+    int lookUpFirst(K id);
+
+    /**
+     * @return the first (smallest) id in the index
+     */
+    @Nullable
+    K firstId();
+
+    /**
+     * @return the last (largest) id in the index
+     */
+    @Nullable
+    K lastId();
+
+    /**
+     * Persist the index on disk to the file matching the desrcriptor.
+     */
+    void persist(Descriptor descriptor);
+
+    /**
+     * @return whether the id falls within lower/upper bounds of the index
+     */
+    default boolean mayContainId(K id, KeySupport<K> keySupport)","[{'comment': 'I feel like `KeySupport` should be removed from this method and a new `KeySupport<K> support()` get added (or implement in an abstract class), that way its not possible for the ""wrong"" logic to get used', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/ActiveSegment.java,"@@ -0,0 +1,394 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.StandardOpenOption;
+import java.util.*;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.LockSupport;
+
+import com.codahale.metrics.Timer;
+import org.apache.cassandra.io.util.*;
+import org.apache.cassandra.utils.*;
+import org.apache.cassandra.utils.concurrent.OpOrder;
+import org.apache.cassandra.utils.concurrent.Ref;
+import org.apache.cassandra.utils.concurrent.WaitQueue;
+
+final class ActiveSegment<K> extends Segment<K>
+{
+    final FileChannel channel;
+
+    // OpOrder used to order appends wrt flush
+    private final OpOrder appendOrder = new OpOrder();
+
+    // position in the buffer we are allocating from
+    private final AtomicInteger allocatePosition = new AtomicInteger(0);
+
+    /*
+     * Everything before this offset has been written and flushed.
+     */
+    private volatile int lastFlushedOffset = 0;
+
+    /*
+     * End position of the buffer; initially set to its capacity and
+     * updated to point to the last written position as the segment is being closed
+     * no need to be volatile as writes are protected by appendOrder barrier.
+     */
+    private int endOfBuffer;
+
+    // a signal that writers can wait on to be notified of a completed flush in BATCH and GROUP FlushMode
+    private final WaitQueue flushComplete = WaitQueue.newWaitQueue();
+
+    private final Ref<Segment<K>> selfRef;
+
+    private ActiveSegment(
+        Descriptor descriptor, Params params, SyncedOffsets syncedOffsets, Index<K> index, Metadata metadata, KeySupport<K> keySupport)
+    {
+        super(descriptor, syncedOffsets, index, metadata, keySupport);
+
+        try
+        {
+            channel = FileChannel.open(file.toPath(), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE);
+            buffer = channel.map(FileChannel.MapMode.READ_WRITE, 0, params.segmentSize());
+            endOfBuffer = buffer.capacity();
+            selfRef = new Ref<>(this, new Tidier(descriptor, channel, buffer, syncedOffsets));
+        }
+        catch (IOException e)
+        {
+            throw new JournalWriteError(descriptor, file, e);
+        }
+    }
+
+    static <K> ActiveSegment<K> create(Descriptor descriptor, Params params, KeySupport<K> keySupport)
+    {
+        SyncedOffsets syncedOffsets = SyncedOffsets.active(descriptor, true);
+        Index<K> index = InMemoryIndex.create(keySupport);
+        Metadata metadata = Metadata.create();
+        return new ActiveSegment<>(descriptor, params, syncedOffsets, index, metadata, keySupport);
+    }
+
+    /**
+     * Read the entry and specified offset into the entry holder.
+     * Expects the caller to acquire the ref to the segment and the record to exist.
+     */
+    @Override
+    boolean read(int offset, EntrySerializer.EntryHolder<K> into)
+    {
+        ByteBuffer duplicate = buffer.duplicate().position(offset).limit(buffer.capacity());
+        try
+        {
+            EntrySerializer.read(into, keySupport, duplicate, descriptor.userVersion);
+        }
+        catch (IOException e)
+        {
+            throw new JournalReadError(descriptor, file, e);
+        }
+        return true;
+    }
+
+    /**
+     * Stop writing to this file, flush and close it. Does nothing if the file is already closed.
+     */
+    @Override
+    public synchronized void close()
+    {
+        close(true);
+    }
+
+    /**
+     * @return true if the closed segment was definitely empty, false otherwise
+     */
+    private synchronized boolean close(boolean persistComponents)
+    {
+        boolean isEmpty = discardUnusedTail();
+        if (!isEmpty)
+        {
+            flush();
+            if (persistComponents) persistComponents();
+        }
+        release();
+        return isEmpty;
+    }
+
+    /**
+     * Close and discard a pre-allocated, available segment, that's never been exposed
+     */
+    void closeAndDiscard()
+    {
+        boolean isEmpty = close(false);
+        if (!isEmpty) throw new IllegalStateException();
+        discard();
+    }
+
+    void closeAndIfEmptyDiscard()
+    {
+        boolean isEmpty = close(true);
+        if (isEmpty) discard();
+    }
+
+    void persistComponents()
+    {
+        index.persist(descriptor);
+        metadata.persist(descriptor);","[{'comment': 'think we need `org.apache.cassandra.utils.SyncUtil#trySyncDir` after', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/KeySupport.java,"@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Comparator;
+import java.util.zip.Checksum;
+
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+
+/**
+ * Record keys must satisfy two properties:
+ * <p>
+ * 1. Must have a fixed serialized size
+ * 2. Must be byte-order comparable
+ */
+public interface KeySupport<K> extends Comparator<K>
+{
+    int serializedSize(int userVersion);
+
+    void serialize(K key, DataOutputPlus out, int userVersion) throws IOException;
+
+    K deserialize(DataInputPlus in, int userVersion) throws IOException;
+
+    K deserialize(ByteBuffer buffer, int position, int userVersion);
+
+    void updateChecksum(Checksum crc, K key, int userVersion);","[{'comment': 'nit: for consistency with `serialize` would be good to be `K key, Checksum crc, int userVersion`', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/OnDiskIndex.java,"@@ -0,0 +1,306 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.StandardOpenOption;
+import java.util.Arrays;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.zip.CRC32;
+
+import javax.annotation.Nullable;
+
+import org.apache.cassandra.io.util.DataInputBuffer;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.utils.Crc;
+
+import static org.apache.cassandra.journal.Journal.validateCRC;
+import static org.apache.cassandra.utils.FBUtilities.updateChecksumInt;
+
+/**
+ * An on-disk (memory-mapped) index for a completed flushed segment.
+ * <p/>
+ * TODO (expected): block-level CRC
+ */
+final class OnDiskIndex<K> implements Index<K>
+{
+    private static final int[] EMPTY = new int[0];
+
+    private static final int FILE_PREFIX_SIZE = 4 + 4; // count of entries, CRC
+    private static final int VALUE_SIZE = 4;           // int offset
+
+    private final int KEY_SIZE;
+    private final int ENTRY_SIZE;
+
+    private final Descriptor descriptor;
+    private final KeySupport<K> keySupport;
+
+    private final FileChannel channel;
+    private volatile MappedByteBuffer buffer;
+    private final int entryCount;
+
+    private volatile K firstId, lastId;
+
+    private OnDiskIndex(
+        Descriptor descriptor, KeySupport<K> keySupport, FileChannel channel, MappedByteBuffer buffer, int entryCount)
+    {
+        this.descriptor = descriptor;
+        this.keySupport = keySupport;
+        this.channel = channel;
+        this.buffer = buffer;
+        this.entryCount = entryCount;
+
+        KEY_SIZE = keySupport.serializedSize(descriptor.userVersion);
+        ENTRY_SIZE = KEY_SIZE + VALUE_SIZE;
+    }
+
+    /**
+     * Open the index for reading, validate CRC
+     */
+    @SuppressWarnings({ ""resource"", ""RedundantSuppression"" })
+    static <K> OnDiskIndex<K> open(Descriptor descriptor, KeySupport<K> keySupport)
+    {
+        File file = descriptor.fileFor(Component.INDEX);
+        FileChannel channel = null;
+        MappedByteBuffer buffer = null;
+        try
+        {
+            channel = FileChannel.open(file.toPath(), StandardOpenOption.READ);
+            buffer = channel.map(FileChannel.MapMode.READ_ONLY, 0, channel.size());
+
+            int entryCount = buffer.getInt(0);
+            OnDiskIndex<K> index = new OnDiskIndex<>(descriptor, keySupport, channel, buffer, entryCount);
+            index.validate();
+            index.init();
+            return index;
+        }
+        catch (Throwable e)
+        {
+            FileUtils.clean(buffer);
+            FileUtils.closeQuietly(channel);
+            throw new JournalReadError(descriptor, file, e);
+        }
+    }
+
+    private void init()
+    {
+        if (entryCount > 0)
+        {
+            firstId = keyAtIndex(0);
+             lastId = keyAtIndex(entryCount - 1);
+        }
+    }
+
+    @Override
+    public void close()
+    {
+        try
+        {
+            FileUtils.clean(buffer);
+            buffer = null;
+            channel.close();
+        }
+        catch (IOException e)
+        {
+            throw new JournalWriteError(descriptor, Component.INDEX, e);
+        }
+    }
+
+    void validate() throws IOException
+    {
+        CRC32 crc = Crc.crc32();
+
+        try (DataInputBuffer in = new DataInputBuffer(buffer, true))
+        {
+            int entryCount = in.readInt();
+            updateChecksumInt(crc, entryCount);
+            validateCRC(crc, in.readInt());","[{'comment': 'should also verify that the provided `entryCount` to constructor matches\r\n\r\n```\r\nif (entryCount != this.entryCount)\r\n  throw new IOException(String.format(""Read entry count %d != %d provided on init"", this.entryCount, entryCount));\r\n```', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/OnDiskIndex.java,"@@ -0,0 +1,306 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.StandardOpenOption;
+import java.util.Arrays;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.zip.CRC32;
+
+import javax.annotation.Nullable;
+
+import org.apache.cassandra.io.util.DataInputBuffer;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.utils.Crc;
+
+import static org.apache.cassandra.journal.Journal.validateCRC;
+import static org.apache.cassandra.utils.FBUtilities.updateChecksumInt;
+
+/**
+ * An on-disk (memory-mapped) index for a completed flushed segment.
+ * <p/>
+ * TODO (expected): block-level CRC
+ */
+final class OnDiskIndex<K> implements Index<K>","[{'comment': 'wondering if using the term index in CASSANDRA-16052 would be useful once that lands and we rebase again.', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/OnDiskIndex.java,"@@ -0,0 +1,306 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.StandardOpenOption;
+import java.util.Arrays;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.zip.CRC32;
+
+import javax.annotation.Nullable;
+
+import org.apache.cassandra.io.util.DataInputBuffer;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.utils.Crc;
+
+import static org.apache.cassandra.journal.Journal.validateCRC;
+import static org.apache.cassandra.utils.FBUtilities.updateChecksumInt;
+
+/**
+ * An on-disk (memory-mapped) index for a completed flushed segment.
+ * <p/>
+ * TODO (expected): block-level CRC
+ */
+final class OnDiskIndex<K> implements Index<K>
+{
+    private static final int[] EMPTY = new int[0];
+
+    private static final int FILE_PREFIX_SIZE = 4 + 4; // count of entries, CRC
+    private static final int VALUE_SIZE = 4;           // int offset
+
+    private final int KEY_SIZE;
+    private final int ENTRY_SIZE;
+
+    private final Descriptor descriptor;
+    private final KeySupport<K> keySupport;
+
+    private final FileChannel channel;
+    private volatile MappedByteBuffer buffer;
+    private final int entryCount;
+
+    private volatile K firstId, lastId;
+
+    private OnDiskIndex(
+        Descriptor descriptor, KeySupport<K> keySupport, FileChannel channel, MappedByteBuffer buffer, int entryCount)
+    {
+        this.descriptor = descriptor;
+        this.keySupport = keySupport;
+        this.channel = channel;
+        this.buffer = buffer;
+        this.entryCount = entryCount;
+
+        KEY_SIZE = keySupport.serializedSize(descriptor.userVersion);
+        ENTRY_SIZE = KEY_SIZE + VALUE_SIZE;
+    }
+
+    /**
+     * Open the index for reading, validate CRC
+     */
+    @SuppressWarnings({ ""resource"", ""RedundantSuppression"" })
+    static <K> OnDiskIndex<K> open(Descriptor descriptor, KeySupport<K> keySupport)
+    {
+        File file = descriptor.fileFor(Component.INDEX);
+        FileChannel channel = null;
+        MappedByteBuffer buffer = null;
+        try
+        {
+            channel = FileChannel.open(file.toPath(), StandardOpenOption.READ);
+            buffer = channel.map(FileChannel.MapMode.READ_ONLY, 0, channel.size());
+
+            int entryCount = buffer.getInt(0);
+            OnDiskIndex<K> index = new OnDiskIndex<>(descriptor, keySupport, channel, buffer, entryCount);
+            index.validate();
+            index.init();
+            return index;
+        }
+        catch (Throwable e)
+        {
+            FileUtils.clean(buffer);
+            FileUtils.closeQuietly(channel);
+            throw new JournalReadError(descriptor, file, e);
+        }
+    }
+
+    private void init()
+    {
+        if (entryCount > 0)
+        {
+            firstId = keyAtIndex(0);
+             lastId = keyAtIndex(entryCount - 1);
+        }
+    }
+
+    @Override
+    public void close()
+    {
+        try
+        {
+            FileUtils.clean(buffer);
+            buffer = null;
+            channel.close();
+        }
+        catch (IOException e)
+        {
+            throw new JournalWriteError(descriptor, Component.INDEX, e);
+        }
+    }
+
+    void validate() throws IOException
+    {
+        CRC32 crc = Crc.crc32();
+
+        try (DataInputBuffer in = new DataInputBuffer(buffer, true))
+        {
+            int entryCount = in.readInt();
+            updateChecksumInt(crc, entryCount);
+            validateCRC(crc, in.readInt());
+
+            Crc.updateCrc32(crc, buffer, FILE_PREFIX_SIZE, FILE_PREFIX_SIZE + entryCount * ENTRY_SIZE);
+            in.skipBytesFully(entryCount * ENTRY_SIZE);
+            validateCRC(crc, in.readInt());
+
+            if (in.available() != 0)
+                throw new IOException(""Trailing data encountered in segment index "" + descriptor.fileFor(Component.INDEX));
+        }
+    }
+
+    static <K> void write(
+        NavigableMap<K, int[]> entries, KeySupport<K> keySupport, DataOutputPlus out, int userVersion) throws IOException
+    {
+        CRC32 crc = Crc.crc32();
+
+        int size = entries.values()
+                          .stream()
+                          .mapToInt(offsets -> offsets.length)
+                          .sum();
+        out.writeInt(size);
+        updateChecksumInt(crc, size);
+        out.writeInt((int) crc.getValue());
+
+        for (Map.Entry<K, int[]> entry : entries.entrySet())
+        {
+            for (int offset : entry.getValue())
+            {
+                K key = entry.getKey();
+                keySupport.serialize(key, out, userVersion);
+                keySupport.updateChecksum(crc, key, userVersion);
+
+                out.writeInt(offset);
+                updateChecksumInt(crc, offset);
+            }
+        }
+
+        out.writeInt((int) crc.getValue());
+    }
+
+    @Override
+    @Nullable
+    public K firstId()
+    {
+        return firstId;
+    }
+
+    @Override
+    @Nullable
+    public K lastId()
+    {
+        return lastId;
+    }
+
+    @Override
+    public int[] lookUp(K id)
+    {
+        if (!mayContainId(id, keySupport))
+            return EMPTY;
+
+        int keyIndex = binarySearch(id);
+        if (keyIndex < 0)
+            return EMPTY;
+
+        int[] offsets = new int[] { offsetAtIndex(keyIndex) };
+
+        /*
+         * Duplicate entries are possible within one segment (but should be rare).
+         * Check and add entries before and after the found result (not guaranteed to be first).
+         */
+
+        for (int i = keyIndex - 1; i >= 0 && id.equals(keyAtIndex(i)); i--)
+        {
+            int length = offsets.length;
+            offsets = Arrays.copyOf(offsets, length + 1);
+            offsets[length] = offsetAtIndex(i);
+        }
+
+        for (int i = keyIndex + 1; i < entryCount && id.equals(keyAtIndex(i)); i++)
+        {
+            int length = offsets.length;
+            offsets = Arrays.copyOf(offsets, length + 1);
+            offsets[length] = offsetAtIndex(i);
+        }
+
+        Arrays.sort(offsets);
+        return offsets;
+    }
+
+    @Override
+    public int lookUpFirst(K id)
+    {
+        if (!mayContainId(id, keySupport))
+            return -1;
+
+        int keyIndex = binarySearch(id);
+
+        /*
+         * Duplicate entries are possible within one segment (but should be rare).
+         * Check and add entries before until we find the first occurrence of key.
+         */
+        for (int i = keyIndex - 1; i >= 0 && id.equals(keyAtIndex(i)); i--)
+            keyIndex = i;
+
+        return keyIndex < 0 ? -1 : offsetAtIndex(keyIndex);
+    }
+
+    private K keyAtIndex(int index)
+    {
+        return keySupport.deserialize(buffer, FILE_PREFIX_SIZE + index * ENTRY_SIZE, descriptor.userVersion);
+    }
+
+    private int offsetAtIndex(int index)
+    {
+        return buffer.getInt(FILE_PREFIX_SIZE + index * ENTRY_SIZE + KEY_SIZE);
+    }
+
+    /*
+     * This has been lifted from {@see IndexSummary}'s implementation,
+     * which itself was lifted from Harmony's Collections implementation.
+     */
+    private int binarySearch(K key)
+    {
+        int low = 0, mid = entryCount, high = mid - 1, result = -1;
+        while (low <= high)
+        {
+            mid = (low + high) >> 1;
+            result = -compareWithKeyAt(key, mid);
+            if (result > 0)
+            {
+                low = mid + 1;
+            }
+            else if (result == 0)
+            {
+                return mid;
+            }
+            else
+            {
+                high = mid - 1;
+            }
+        }
+        return -mid - (result < 0 ? 1 : 2);
+    }
+
+    private int compareWithKeyAt(K key, int keyIndex)
+    {
+        int offset = FILE_PREFIX_SIZE + ENTRY_SIZE * keyIndex;
+        return keySupport.compareWithKeyAt(key, buffer, offset, descriptor.userVersion);
+    }
+
+    @Override
+    public void update(K id, int offset)
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public void persist(Descriptor descriptor)
+    {
+        throw new UnsupportedOperationException();
+    }","[{'comment': 'might be good to have `Index` and `MutableIndex` that way this is solved at the type level', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/StaticSegment.java,"@@ -0,0 +1,342 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.NoSuchFileException;
+import java.nio.file.StandardOpenOption;
+import java.util.*;
+
+import org.agrona.collections.IntHashSet;
+import org.apache.cassandra.io.util.DataInputBuffer;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.utils.Closeable;
+import org.apache.cassandra.utils.concurrent.Ref;
+
+/**
+ * An immutable data segment that is no longer written to.
+ * <p>
+ * Can be compacted with input from {@code PersistedInvalidations} into a new smaller segment,
+ * with invalidated entries removed.
+ */
+final class StaticSegment<K> extends Segment<K>
+{
+    final FileChannel channel;
+
+    private final Ref<Segment<K>> selfRef;
+
+    private StaticSegment(Descriptor descriptor,
+                          FileChannel channel,
+                          MappedByteBuffer buffer,
+                          SyncedOffsets syncedOffsets,
+                          Index<K> index,
+                          Metadata metadata,
+                          KeySupport<K> keySupport)
+    {
+        super(descriptor, syncedOffsets, index, metadata, keySupport);
+
+        this.channel = channel;
+        this.buffer = buffer;
+
+        selfRef = new Ref<>(this, new Tidier<>(descriptor, channel, buffer, index));
+    }
+
+    /**
+     * Loads all segments matching the supplied desctiptors
+     *
+     * @param descriptors descriptors of the segments to load
+     * @return list of the loaded segments
+     */
+    static <K> List<StaticSegment<K>> open(Collection<Descriptor> descriptors, KeySupport<K> keySupport)
+    {
+        List<StaticSegment<K>> segments = new ArrayList<>(descriptors.size());
+        for (Descriptor descriptor : descriptors)
+            segments.add(open(descriptor, keySupport));
+        return segments;
+    }
+
+    /**
+     * Load the segment corresponding to the provided desrciptor
+     *
+     * @param descriptor descriptor of the segment to load
+     * @return the loaded segment
+     */
+    @SuppressWarnings({ ""resource"", ""RedundantSuppression"" })
+    static <K> StaticSegment<K> open(Descriptor descriptor, KeySupport<K> keySupport)
+    {
+        if (!Component.DATA.existsFor(descriptor))
+            throw new IllegalArgumentException(""Data file for segment "" + descriptor + "" doesn't exist"");
+
+        SyncedOffsets syncedOffsets = Component.SYNCED_OFFSETS.existsFor(descriptor)
+                                    ? SyncedOffsets.load(descriptor)
+                                    : SyncedOffsets.absent();
+
+        Metadata metadata = Component.INDEX.existsFor(descriptor)
+                          ? Metadata.load(descriptor)
+                          : Metadata.rebuildAndPersist(descriptor, keySupport, syncedOffsets.syncedOffset());
+
+        OnDiskIndex<K> index = Component.METADATA.existsFor(descriptor)
+                             ? OnDiskIndex.open(descriptor, keySupport)
+                             : OnDiskIndex.rebuildAndPersist(descriptor, keySupport, syncedOffsets.syncedOffset());","[{'comment': 'I think this is backwards, if `INDEX` exists or not we load `Metadata`, and when `METADATA` exists or not we load `Index`?', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/service/accord/AccordJournal.java,"@@ -0,0 +1,407 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.service.accord;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.*;
+import java.util.concurrent.Executor;
+import java.util.zip.Checksum;
+
+import com.google.common.primitives.Ints;
+
+import accord.local.Node.Id;
+import accord.local.PreLoadContext;
+import accord.messages.Accept;
+import accord.messages.Apply;
+import accord.messages.Commit;
+import accord.messages.MessageType;
+import accord.messages.PreAccept;
+import accord.messages.TxnRequest;
+import accord.primitives.*;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.journal.Journal;
+import org.apache.cassandra.journal.KeySupport;
+import org.apache.cassandra.journal.Params;
+import org.apache.cassandra.journal.ValueSerializer;
+import org.apache.cassandra.service.accord.serializers.AcceptSerializers;
+import org.apache.cassandra.service.accord.serializers.ApplySerializers;
+import org.apache.cassandra.service.accord.serializers.CommitSerializers;
+import org.apache.cassandra.service.accord.serializers.EnumSerializer;
+import org.apache.cassandra.service.accord.serializers.PreacceptSerializers;
+
+import static org.apache.cassandra.db.TypeSizes.BYTE_SIZE;
+import static org.apache.cassandra.db.TypeSizes.INT_SIZE;
+import static org.apache.cassandra.db.TypeSizes.LONG_SIZE;
+import static org.apache.cassandra.utils.FBUtilities.updateChecksumInt;
+import static org.apache.cassandra.utils.FBUtilities.updateChecksumLong;
+
+/*
+ *  TODO: expose more journal params via Config
+ */
+class AccordJournal
+{
+    private static final Set<Integer> SENTINEL_HOSTS = Collections.singleton(0);
+
+    final File directory;
+    final Journal<Key, TxnRequest<?>> journal;
+
+    AccordJournal()
+    {
+        directory = new File(DatabaseDescriptor.getAccordJournalDirectory());
+        journal = new Journal<>(""AccordJournal"", directory, Params.DEFAULT, Key.SUPPORT, MESSAGE_SERIALIZER);
+    }
+
+    AccordJournal start()
+    {
+        journal.start();
+        return this;
+    }
+
+    void shutdown()
+    {
+        journal.shutdown();
+    }
+
+    boolean mustAppend(PreLoadContext context)
+    {
+        return context instanceof TxnRequest && Type.mustAppend((TxnRequest<?>) context);
+    }
+
+    void append(int storeId, PreLoadContext context, Executor executor, Runnable onDurable)
+    {
+        append(storeId, (TxnRequest<?>) context, executor, onDurable);
+    }
+
+    void append(int storeId, TxnRequest<?> message, Executor executor, Runnable onDurable)
+    {
+        Key key = new Key(message.txnId, Type.fromMsgType(message.type()), storeId);
+        journal.asyncWrite(key, message, SENTINEL_HOSTS, executor, onDurable);","[{'comment': ""`org.apache.cassandra.journal.Metadata` whole existence is to track hosts, but our requests don't contain a host and we currently use a fixed host at the write side; what is this host tracking supposed to be for?"", 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Descriptor.java,"@@ -0,0 +1,167 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.List;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.cassandra.io.util.File;
+
+import static java.lang.String.format;
+import static java.util.stream.Collectors.toList;
+
+/**
+ * Timestamp and version encoded in the file name, e.g.
+ * log-1637159888484-2-1-1.data
+ * log-1637159888484-2-1-1.indx
+ * log-1637159888484-2-1-1.meta
+ * log-1637159888484-2-1-1.sync
+ */
+final class Descriptor implements Comparable<Descriptor>
+{
+    private static final String SEPARATOR = ""-"";
+    private static final String PREFIX = ""log"" + SEPARATOR;
+    private static final String TMP_SUFFIX = ""tmp"";
+
+    private static final Pattern DATA_FILE_PATTERN =
+        Pattern.compile(   PREFIX + ""(\\d+)"" // timestamp
+                      + SEPARATOR + ""(\\d+)"" // generation
+                      + SEPARATOR + ""(\\d+)"" // journal version
+                      + SEPARATOR + ""(\\d+)"" // user version
+                      + ""\\.""     + Component.DATA.extension);
+
+    private static final Pattern TMP_FILE_PATTERN =
+        Pattern.compile(   PREFIX + ""\\d+""   // timestamp
+                      + SEPARATOR + ""\\d+""   // generation
+                      + SEPARATOR + ""\\d+""   // journal version
+                      + SEPARATOR + ""\\d+""   // user version
+                      + ""\\.""     + ""[a-z]+"" // component extension
+                      + ""\\.""     + TMP_SUFFIX);
+
+
+    static final int JOURNAL_VERSION_1 = 1;
+    static final int CURRENT_JOURNAL_VERSION = JOURNAL_VERSION_1;
+
+    final File directory;
+    final long timestamp;
+    final int generation;
+
+    /**
+     * Serialization version for journal components; bumped as journal
+     * implementation evolves over time.
+     */
+    final int journalVersion;
+
+    /**
+     * Serialization version for user content - specifically journal keys
+     * and journal values; bumped when user logic evolves.
+     */
+    final int userVersion;
+
+    private Descriptor(File directory, long timestamp, int generation, int journalVersion, int userVersion)
+    {
+        this.directory = directory;
+        this.timestamp = timestamp;
+        this.generation = generation;
+        this.journalVersion = journalVersion;
+        this.userVersion = userVersion;
+    }
+
+    static Descriptor create(File directory, long timestamp, int userVersion)
+    {
+        return new Descriptor(directory, timestamp, 1, CURRENT_JOURNAL_VERSION, userVersion);
+    }
+
+    static Descriptor fromName(File directory, String name)
+    {
+        Matcher matcher = DATA_FILE_PATTERN.matcher(name);
+        if (!matcher.matches())
+            throw new IllegalArgumentException(""Provided filename is not valid for a data segment file"");
+
+        long timestamp = Long.parseLong(matcher.group(1));
+        int generation = Integer.parseInt(matcher.group(2));
+        int journalVersion = Integer.parseInt(matcher.group(3));
+        int userVersion = Integer.parseInt(matcher.group(4));
+
+        return new Descriptor(directory, timestamp, generation, journalVersion, userVersion);
+    }
+
+    Descriptor withIncrementedGeneration()","[{'comment': 'dead code', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Descriptor.java,"@@ -0,0 +1,167 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.List;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.cassandra.io.util.File;
+
+import static java.lang.String.format;
+import static java.util.stream.Collectors.toList;
+
+/**
+ * Timestamp and version encoded in the file name, e.g.
+ * log-1637159888484-2-1-1.data
+ * log-1637159888484-2-1-1.indx
+ * log-1637159888484-2-1-1.meta
+ * log-1637159888484-2-1-1.sync
+ */
+final class Descriptor implements Comparable<Descriptor>
+{
+    private static final String SEPARATOR = ""-"";
+    private static final String PREFIX = ""log"" + SEPARATOR;
+    private static final String TMP_SUFFIX = ""tmp"";
+
+    private static final Pattern DATA_FILE_PATTERN =
+        Pattern.compile(   PREFIX + ""(\\d+)"" // timestamp
+                      + SEPARATOR + ""(\\d+)"" // generation
+                      + SEPARATOR + ""(\\d+)"" // journal version
+                      + SEPARATOR + ""(\\d+)"" // user version
+                      + ""\\.""     + Component.DATA.extension);
+
+    private static final Pattern TMP_FILE_PATTERN =
+        Pattern.compile(   PREFIX + ""\\d+""   // timestamp
+                      + SEPARATOR + ""\\d+""   // generation
+                      + SEPARATOR + ""\\d+""   // journal version
+                      + SEPARATOR + ""\\d+""   // user version
+                      + ""\\.""     + ""[a-z]+"" // component extension
+                      + ""\\.""     + TMP_SUFFIX);
+
+
+    static final int JOURNAL_VERSION_1 = 1;
+    static final int CURRENT_JOURNAL_VERSION = JOURNAL_VERSION_1;
+
+    final File directory;
+    final long timestamp;
+    final int generation;
+
+    /**
+     * Serialization version for journal components; bumped as journal
+     * implementation evolves over time.
+     */
+    final int journalVersion;
+
+    /**
+     * Serialization version for user content - specifically journal keys
+     * and journal values; bumped when user logic evolves.
+     */
+    final int userVersion;
+
+    private Descriptor(File directory, long timestamp, int generation, int journalVersion, int userVersion)
+    {
+        this.directory = directory;
+        this.timestamp = timestamp;
+        this.generation = generation;
+        this.journalVersion = journalVersion;
+        this.userVersion = userVersion;
+    }
+
+    static Descriptor create(File directory, long timestamp, int userVersion)
+    {
+        return new Descriptor(directory, timestamp, 1, CURRENT_JOURNAL_VERSION, userVersion);
+    }
+
+    static Descriptor fromName(File directory, String name)
+    {
+        Matcher matcher = DATA_FILE_PATTERN.matcher(name);
+        if (!matcher.matches())
+            throw new IllegalArgumentException(""Provided filename is not valid for a data segment file"");
+
+        long timestamp = Long.parseLong(matcher.group(1));
+        int generation = Integer.parseInt(matcher.group(2));
+        int journalVersion = Integer.parseInt(matcher.group(3));
+        int userVersion = Integer.parseInt(matcher.group(4));
+
+        return new Descriptor(directory, timestamp, generation, journalVersion, userVersion);
+    }
+
+    Descriptor withIncrementedGeneration()
+    {
+        return new Descriptor(directory, timestamp, generation + 1, journalVersion, userVersion);
+    }
+
+    File fileFor(Component component)
+    {
+        return new File(directory, formatFileName(component));
+    }
+
+    File tmpFileFor(Component component)
+    {
+        return new File(directory, formatFileName(component) + '.' + TMP_SUFFIX);
+    }
+
+    static boolean isTmpFile(File file)
+    {
+        return TMP_FILE_PATTERN.matcher(file.name()).matches();
+    }
+
+    private String formatFileName(Component component)
+    {
+        return format(""%s%d%s%d%s%d%s%d.%s"",
+                      PREFIX, timestamp,
+                      SEPARATOR, generation,
+                      SEPARATOR, journalVersion,
+                      SEPARATOR, userVersion,
+                      component.extension);","[{'comment': '`String.format` is rather expensive, think it would be best to use `StringBuilder` in this case', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Descriptor.java,"@@ -0,0 +1,167 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.List;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.cassandra.io.util.File;
+
+import static java.lang.String.format;
+import static java.util.stream.Collectors.toList;
+
+/**
+ * Timestamp and version encoded in the file name, e.g.
+ * log-1637159888484-2-1-1.data
+ * log-1637159888484-2-1-1.indx
+ * log-1637159888484-2-1-1.meta
+ * log-1637159888484-2-1-1.sync
+ */
+final class Descriptor implements Comparable<Descriptor>
+{
+    private static final String SEPARATOR = ""-"";
+    private static final String PREFIX = ""log"" + SEPARATOR;
+    private static final String TMP_SUFFIX = ""tmp"";
+
+    private static final Pattern DATA_FILE_PATTERN =
+        Pattern.compile(   PREFIX + ""(\\d+)"" // timestamp
+                      + SEPARATOR + ""(\\d+)"" // generation
+                      + SEPARATOR + ""(\\d+)"" // journal version
+                      + SEPARATOR + ""(\\d+)"" // user version
+                      + ""\\.""     + Component.DATA.extension);
+
+    private static final Pattern TMP_FILE_PATTERN =
+        Pattern.compile(   PREFIX + ""\\d+""   // timestamp
+                      + SEPARATOR + ""\\d+""   // generation
+                      + SEPARATOR + ""\\d+""   // journal version
+                      + SEPARATOR + ""\\d+""   // user version
+                      + ""\\.""     + ""[a-z]+"" // component extension
+                      + ""\\.""     + TMP_SUFFIX);
+
+
+    static final int JOURNAL_VERSION_1 = 1;
+    static final int CURRENT_JOURNAL_VERSION = JOURNAL_VERSION_1;
+
+    final File directory;
+    final long timestamp;
+    final int generation;
+
+    /**
+     * Serialization version for journal components; bumped as journal
+     * implementation evolves over time.
+     */
+    final int journalVersion;
+
+    /**
+     * Serialization version for user content - specifically journal keys
+     * and journal values; bumped when user logic evolves.
+     */
+    final int userVersion;
+
+    private Descriptor(File directory, long timestamp, int generation, int journalVersion, int userVersion)
+    {
+        this.directory = directory;
+        this.timestamp = timestamp;
+        this.generation = generation;
+        this.journalVersion = journalVersion;
+        this.userVersion = userVersion;
+    }
+
+    static Descriptor create(File directory, long timestamp, int userVersion)
+    {
+        return new Descriptor(directory, timestamp, 1, CURRENT_JOURNAL_VERSION, userVersion);
+    }
+
+    static Descriptor fromName(File directory, String name)
+    {
+        Matcher matcher = DATA_FILE_PATTERN.matcher(name);
+        if (!matcher.matches())
+            throw new IllegalArgumentException(""Provided filename is not valid for a data segment file"");
+
+        long timestamp = Long.parseLong(matcher.group(1));
+        int generation = Integer.parseInt(matcher.group(2));
+        int journalVersion = Integer.parseInt(matcher.group(3));
+        int userVersion = Integer.parseInt(matcher.group(4));
+
+        return new Descriptor(directory, timestamp, generation, journalVersion, userVersion);
+    }
+
+    Descriptor withIncrementedGeneration()
+    {
+        return new Descriptor(directory, timestamp, generation + 1, journalVersion, userVersion);
+    }
+
+    File fileFor(Component component)
+    {
+        return new File(directory, formatFileName(component));
+    }
+
+    File tmpFileFor(Component component)
+    {
+        return new File(directory, formatFileName(component) + '.' + TMP_SUFFIX);
+    }
+
+    static boolean isTmpFile(File file)
+    {
+        return TMP_FILE_PATTERN.matcher(file.name()).matches();
+    }
+
+    private String formatFileName(Component component)
+    {
+        return format(""%s%d%s%d%s%d%s%d.%s"",
+                      PREFIX, timestamp,
+                      SEPARATOR, generation,
+                      SEPARATOR, journalVersion,
+                      SEPARATOR, userVersion,
+                      component.extension);
+    }
+
+    static List<Descriptor> list(File directory)
+    {
+        try
+        {
+            return Arrays.stream(directory.listNames((file, name) -> DATA_FILE_PATTERN.matcher(name).matches()))
+                         .map(name -> fromName(directory, name))
+                         .collect(toList());
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    @Override
+    public int compareTo(Descriptor other)","[{'comment': 'atm we only use a fixed `directory` but we could spread that cross different disks, so would be good to also compare `directory` as well.', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Params.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+public interface Params","[{'comment': 'is this meant to be used in `cassandra.yaml`?   If so why not just make this a class with the defaults from `DEFAULT`?\r\n\r\nhttps://issues.apache.org/jira/browse/CASSANDRA-18221 is in the works now, that patch will add configs to Accord and plumb that into C*.  One feature that will exist there is a new annotation of `@Embedded` which will let you have complex/nested types but they get ""embedded"" into the config yaml...  so in your case I think you want\r\n\r\n```\r\n// in Config.java\r\n@Embedded\r\npublic final Params accord_journal = new Params;\r\n```\r\n\r\nand the yaml would be\r\n\r\n```\r\naccord_journal_segment_size: 33554432\r\naccord_journal_failure_policy: STOP\r\n...\r\n```', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Params.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+public interface Params
+{
+    enum FlushMode { BATCH, GROUP, PERIODIC }
+
+    enum FailurePolicy { STOP, STOP_JOURNAL, IGNORE, DIE }
+
+    /**
+     * @return maximum segment size
+     */
+    int segmentSize();
+
+    /**
+     * @return this journal's {@link FailurePolicy}
+     */
+    FailurePolicy failurePolicy();
+
+    /**
+     * @return journal flush (sync) mode
+     */
+    FlushMode flushMode();
+
+    /**
+     * @return milliseconds between journal flushes
+     */
+    int flushPeriod();
+
+    /**
+     * @return milliseconds to block writes for while waiting for a slow disk flush to complete
+     *         when in {@link FlushMode#PERIODIC} mode
+     */
+    int periodicFlushLagBlock();
+
+    /**
+     * @return user provided version to use for key and value serialization
+     */
+    int userVersion();","[{'comment': ""my last comment was to expose this via yaml, but we wouldn't want this as an `int` there, we would want an `enum` (allow opt-into new versions)...  guess we could use `@JsonIgnore` to hid it...\r\n\r\nguess `Params` shouldn't be yaml, but likely `DEFAULT` would, where it takes an enum and maps that to this `int`"", 'commenter': 'dcapwell'}, {'comment': ""If this should always be `MessagingService`'s version for all users, then we can use `org.apache.cassandra.net.MessagingService.Version`...  I added this to get access to versions w/o loading `MessagingService` for tests, but could use in this context as well."", 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Params.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+public interface Params
+{
+    enum FlushMode { BATCH, GROUP, PERIODIC }
+
+    enum FailurePolicy { STOP, STOP_JOURNAL, IGNORE, DIE }
+
+    /**
+     * @return maximum segment size
+     */
+    int segmentSize();
+
+    /**
+     * @return this journal's {@link FailurePolicy}
+     */
+    FailurePolicy failurePolicy();
+
+    /**
+     * @return journal flush (sync) mode
+     */
+    FlushMode flushMode();
+
+    /**
+     * @return milliseconds between journal flushes
+     */
+    int flushPeriod();","[{'comment': 'rather than use `int` should we use a `Duration` (actually `DurationSpec`)?  Thinking about the yaml side where we would want `flush_period: 1s`', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Params.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+public interface Params
+{
+    enum FlushMode { BATCH, GROUP, PERIODIC }
+
+    enum FailurePolicy { STOP, STOP_JOURNAL, IGNORE, DIE }
+
+    /**
+     * @return maximum segment size
+     */
+    int segmentSize();
+
+    /**
+     * @return this journal's {@link FailurePolicy}
+     */
+    FailurePolicy failurePolicy();
+
+    /**
+     * @return journal flush (sync) mode
+     */
+    FlushMode flushMode();
+
+    /**
+     * @return milliseconds between journal flushes
+     */
+    int flushPeriod();
+
+    /**
+     * @return milliseconds to block writes for while waiting for a slow disk flush to complete
+     *         when in {@link FlushMode#PERIODIC} mode
+     */
+    int periodicFlushLagBlock();","[{'comment': 'same as above, should this be a `DurationSpec`?', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Journal.java,"@@ -0,0 +1,644 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.file.FileStore;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.Executor;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.BooleanSupplier;
+import java.util.zip.CRC32;
+
+import javax.annotation.Nonnull;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.codahale.metrics.Timer.Context;
+import org.apache.cassandra.concurrent.Interruptible.TerminateException;
+import org.apache.cassandra.concurrent.Interruptible;
+import org.apache.cassandra.concurrent.SequentialExecutorPlus;
+import org.apache.cassandra.io.util.DataInputBuffer;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.io.util.PathUtils;
+import org.apache.cassandra.journal.Segments.ReferencedSegments;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.Crc;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+import org.apache.cassandra.utils.concurrent.WaitQueue;
+
+import static java.lang.String.format;
+import static java.util.Comparator.comparing;
+import static org.apache.cassandra.concurrent.ExecutorFactory.Global.executorFactory;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.Daemon.NON_DAEMON;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.Interrupts.SYNCHRONIZED;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.SimulatorSafe.SAFE;
+import static org.apache.cassandra.concurrent.Interruptible.State.NORMAL;
+import static org.apache.cassandra.concurrent.Interruptible.State.SHUTTING_DOWN;
+import static org.apache.cassandra.utils.Clock.Global.currentTimeMillis;
+import static org.apache.cassandra.utils.concurrent.WaitQueue.newWaitQueue;
+
+/**
+ * A generic append-only journal with some special features:
+ * <p><ul>
+ * <li>Records can be looked up by key
+ * <li>Records can be tagged with multiple owner node ids
+ * <li>Records can be invalidated by their owner ids
+ * <li>Fully invalidated records get purged during segment compaction
+ * </ul><p>
+ *
+ * Type parameters:
+ * @param <V> the type of records stored in the journal
+ * @param <K> the type of keys used to address the records;
+              must be fixed-size and byte-order comparable
+ */
+public class Journal<K, V>
+{
+    private static final Logger logger = LoggerFactory.getLogger(Journal.class);
+
+    final String name;
+    final File directory;
+    final Params params;
+
+    final KeySupport<K> keySupport;
+    final ValueSerializer<K, V> valueSerializer;
+
+    final Metrics<K, V> metrics;
+    final Flusher<K, V> flusher;
+    //final Invalidator<K, V> invalidator;
+    //final Compactor<K, V> compactor;
+
+    volatile long replayLimit;
+    final AtomicLong nextSegmentId = new AtomicLong();
+
+    private volatile ActiveSegment<K> currentSegment = null;
+
+    // segment that is ready to be used; allocator thread fills this and blocks until consumed
+    private volatile ActiveSegment<K> availableSegment = null;
+
+    private final AtomicReference<Segments<K>> segments = new AtomicReference<>();
+
+    Interruptible allocator;
+    private final WaitQueue segmentPrepared = newWaitQueue();
+    private final WaitQueue allocatorThreadWaitQueue = newWaitQueue();
+    private final BooleanSupplier allocatorThreadWaitCondition = () -> (availableSegment == null);
+
+    SequentialExecutorPlus closer;
+    //private final Set<Descriptor> invalidations = Collections.newSetFromMap(new ConcurrentHashMap<>());
+
+    public Journal(String name,
+                   File directory,
+                   Params params,
+                   KeySupport<K> keySupport,
+                   ValueSerializer<K, V> valueSerializer)
+    {
+        this.name = name;
+        this.directory = directory;
+        this.params = params;
+
+        this.keySupport = keySupport;
+        this.valueSerializer = valueSerializer;
+
+        this.metrics = new Metrics<>(name);
+        this.flusher = new Flusher<>(this);
+        //this.invalidator = new Invalidator<>(this);
+        //this.compactor = new Compactor<>(this);
+    }
+
+    public void start()
+    {
+        metrics.register(flusher);
+
+        deleteTmpFiles();
+
+        List<Descriptor> descriptors = Descriptor.list(directory);
+        // find the largest existing timestamp
+        descriptors.sort(null);
+        long maxTimestamp = descriptors.isEmpty()
+                          ? Long.MIN_VALUE
+                          : descriptors.get(descriptors.size() - 1).timestamp;
+        nextSegmentId.set(replayLimit = Math.max(currentTimeMillis(), maxTimestamp + 1));
+
+        segments.set(Segments.ofStatic(StaticSegment.open(descriptors, keySupport)));
+        closer = executorFactory().sequential(name + ""-closer"");
+        allocator = executorFactory().infiniteLoop(name + ""-allocator"", new AllocateRunnable(), SAFE, NON_DAEMON, SYNCHRONIZED);
+        advanceSegment(null);
+        flusher.start();
+        //invalidator.start();
+        //compactor.start();
+    }
+
+    /**
+     * Cleans up unfinished component files from previous run (metadata and index)
+     */
+    private void deleteTmpFiles()
+    {
+        for (File tmpFile : directory.listUnchecked(Descriptor::isTmpFile))
+            tmpFile.delete();
+    }
+
+    public void shutdown()
+    {
+        allocator.shutdown();
+        //compactor.stop();
+        //invalidator.stop();
+        flusher.shutdown();
+        closer.shutdown();
+        closeAllSegments();
+        metrics.deregister();
+    }
+
+    /**
+     * Looks up a record by the provided id.
+     * <p/>
+     * Looking up an invalidated record may or may not return a record, depending on
+     * compaction progress.
+     * <p/>
+     * In case multiple copies of the record exist in the log (e.g. because of user retries),
+     * only the first found record will be consumed.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @param consumer function to consume the raw record (bytes and invalidation set) if found
+     * @return true if the record was found, false otherwise
+     */
+    public boolean read(K id, RecordConsumer<K> consumer)
+    {
+        try (ReferencedSegments<K> segments = selectAndReference(id))
+        {
+            for (Segment<K> segment : segments.all())
+                if (segment.read(id, consumer))
+                    return true;
+        }
+
+        return false;
+    }
+
+    /**
+     * Looks up a record by the provided id.
+     * <p/>
+     * Looking up an invalidated record may or may not return a record, depending on
+     * compaction progress.
+     * <p/>
+     * In case multiple copies of the record exist in the log (e.g. because of user retries),
+     * the first one found will be returned.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @return deserialized record if found, null otherwise
+     */
+    public V read(K id)
+    {
+        EntrySerializer.EntryHolder<K> holder = new EntrySerializer.EntryHolder<>();
+
+        try (ReferencedSegments<K> segments = selectAndReference(id))
+        {
+            for (Segment<K> segment : segments.all())
+            {
+                if (segment.read(id, holder))
+                {
+                    try
+                    {
+                        return valueSerializer.deserialize(holder.key, new DataInputBuffer(holder.value, false), segment.descriptor.userVersion);
+                    }
+                    catch (IOException e)
+                    {
+                        // can only throw if serializer is buggy
+                        throw new RuntimeException(e);
+                    }
+                }
+            }
+        }
+
+        return null;
+    }
+
+    /**
+     * Synchronously write a record to the journal.
+     * <p/>
+     * Blocks until the record has been deemed durable according to the journal flush mode.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @param record the record to store
+     * @param hosts hosts expected to invalidate the record
+     */
+    public void write(K id, V record, Set<Integer> hosts)
+    {
+        try (DataOutputBuffer dob = DataOutputBuffer.scratchBuffer.get())
+        {
+            valueSerializer.serialize(record, dob, MessagingService.current_version);","[{'comment': '```suggestion\r\n            valueSerializer.serialize(record, dob, params.userVersion());\r\n```', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Journal.java,"@@ -0,0 +1,644 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.file.FileStore;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.Executor;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.BooleanSupplier;
+import java.util.zip.CRC32;
+
+import javax.annotation.Nonnull;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.codahale.metrics.Timer.Context;
+import org.apache.cassandra.concurrent.Interruptible.TerminateException;
+import org.apache.cassandra.concurrent.Interruptible;
+import org.apache.cassandra.concurrent.SequentialExecutorPlus;
+import org.apache.cassandra.io.util.DataInputBuffer;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.io.util.PathUtils;
+import org.apache.cassandra.journal.Segments.ReferencedSegments;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.Crc;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+import org.apache.cassandra.utils.concurrent.WaitQueue;
+
+import static java.lang.String.format;
+import static java.util.Comparator.comparing;
+import static org.apache.cassandra.concurrent.ExecutorFactory.Global.executorFactory;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.Daemon.NON_DAEMON;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.Interrupts.SYNCHRONIZED;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.SimulatorSafe.SAFE;
+import static org.apache.cassandra.concurrent.Interruptible.State.NORMAL;
+import static org.apache.cassandra.concurrent.Interruptible.State.SHUTTING_DOWN;
+import static org.apache.cassandra.utils.Clock.Global.currentTimeMillis;
+import static org.apache.cassandra.utils.concurrent.WaitQueue.newWaitQueue;
+
+/**
+ * A generic append-only journal with some special features:
+ * <p><ul>
+ * <li>Records can be looked up by key
+ * <li>Records can be tagged with multiple owner node ids
+ * <li>Records can be invalidated by their owner ids
+ * <li>Fully invalidated records get purged during segment compaction
+ * </ul><p>
+ *
+ * Type parameters:
+ * @param <V> the type of records stored in the journal
+ * @param <K> the type of keys used to address the records;
+              must be fixed-size and byte-order comparable
+ */
+public class Journal<K, V>
+{
+    private static final Logger logger = LoggerFactory.getLogger(Journal.class);
+
+    final String name;
+    final File directory;
+    final Params params;
+
+    final KeySupport<K> keySupport;
+    final ValueSerializer<K, V> valueSerializer;
+
+    final Metrics<K, V> metrics;
+    final Flusher<K, V> flusher;
+    //final Invalidator<K, V> invalidator;
+    //final Compactor<K, V> compactor;
+
+    volatile long replayLimit;
+    final AtomicLong nextSegmentId = new AtomicLong();
+
+    private volatile ActiveSegment<K> currentSegment = null;
+
+    // segment that is ready to be used; allocator thread fills this and blocks until consumed
+    private volatile ActiveSegment<K> availableSegment = null;
+
+    private final AtomicReference<Segments<K>> segments = new AtomicReference<>();
+
+    Interruptible allocator;
+    private final WaitQueue segmentPrepared = newWaitQueue();
+    private final WaitQueue allocatorThreadWaitQueue = newWaitQueue();
+    private final BooleanSupplier allocatorThreadWaitCondition = () -> (availableSegment == null);
+
+    SequentialExecutorPlus closer;
+    //private final Set<Descriptor> invalidations = Collections.newSetFromMap(new ConcurrentHashMap<>());
+
+    public Journal(String name,
+                   File directory,
+                   Params params,
+                   KeySupport<K> keySupport,
+                   ValueSerializer<K, V> valueSerializer)
+    {
+        this.name = name;
+        this.directory = directory;
+        this.params = params;
+
+        this.keySupport = keySupport;
+        this.valueSerializer = valueSerializer;
+
+        this.metrics = new Metrics<>(name);
+        this.flusher = new Flusher<>(this);
+        //this.invalidator = new Invalidator<>(this);
+        //this.compactor = new Compactor<>(this);
+    }
+
+    public void start()
+    {
+        metrics.register(flusher);
+
+        deleteTmpFiles();
+
+        List<Descriptor> descriptors = Descriptor.list(directory);
+        // find the largest existing timestamp
+        descriptors.sort(null);
+        long maxTimestamp = descriptors.isEmpty()
+                          ? Long.MIN_VALUE
+                          : descriptors.get(descriptors.size() - 1).timestamp;
+        nextSegmentId.set(replayLimit = Math.max(currentTimeMillis(), maxTimestamp + 1));
+
+        segments.set(Segments.ofStatic(StaticSegment.open(descriptors, keySupport)));
+        closer = executorFactory().sequential(name + ""-closer"");
+        allocator = executorFactory().infiniteLoop(name + ""-allocator"", new AllocateRunnable(), SAFE, NON_DAEMON, SYNCHRONIZED);
+        advanceSegment(null);
+        flusher.start();
+        //invalidator.start();
+        //compactor.start();
+    }
+
+    /**
+     * Cleans up unfinished component files from previous run (metadata and index)
+     */
+    private void deleteTmpFiles()
+    {
+        for (File tmpFile : directory.listUnchecked(Descriptor::isTmpFile))
+            tmpFile.delete();
+    }
+
+    public void shutdown()
+    {
+        allocator.shutdown();
+        //compactor.stop();
+        //invalidator.stop();
+        flusher.shutdown();
+        closer.shutdown();
+        closeAllSegments();
+        metrics.deregister();
+    }
+
+    /**
+     * Looks up a record by the provided id.
+     * <p/>
+     * Looking up an invalidated record may or may not return a record, depending on
+     * compaction progress.
+     * <p/>
+     * In case multiple copies of the record exist in the log (e.g. because of user retries),
+     * only the first found record will be consumed.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @param consumer function to consume the raw record (bytes and invalidation set) if found
+     * @return true if the record was found, false otherwise
+     */
+    public boolean read(K id, RecordConsumer<K> consumer)
+    {
+        try (ReferencedSegments<K> segments = selectAndReference(id))
+        {
+            for (Segment<K> segment : segments.all())
+                if (segment.read(id, consumer))
+                    return true;
+        }
+
+        return false;
+    }
+
+    /**
+     * Looks up a record by the provided id.
+     * <p/>
+     * Looking up an invalidated record may or may not return a record, depending on
+     * compaction progress.
+     * <p/>
+     * In case multiple copies of the record exist in the log (e.g. because of user retries),
+     * the first one found will be returned.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @return deserialized record if found, null otherwise
+     */
+    public V read(K id)
+    {
+        EntrySerializer.EntryHolder<K> holder = new EntrySerializer.EntryHolder<>();
+
+        try (ReferencedSegments<K> segments = selectAndReference(id))
+        {
+            for (Segment<K> segment : segments.all())
+            {
+                if (segment.read(id, holder))
+                {
+                    try
+                    {
+                        return valueSerializer.deserialize(holder.key, new DataInputBuffer(holder.value, false), segment.descriptor.userVersion);
+                    }
+                    catch (IOException e)
+                    {
+                        // can only throw if serializer is buggy
+                        throw new RuntimeException(e);
+                    }
+                }
+            }
+        }
+
+        return null;
+    }
+
+    /**
+     * Synchronously write a record to the journal.
+     * <p/>
+     * Blocks until the record has been deemed durable according to the journal flush mode.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @param record the record to store
+     * @param hosts hosts expected to invalidate the record
+     */
+    public void write(K id, V record, Set<Integer> hosts)
+    {
+        try (DataOutputBuffer dob = DataOutputBuffer.scratchBuffer.get())
+        {
+            valueSerializer.serialize(record, dob, MessagingService.current_version);
+            ActiveSegment<K>.Allocation alloc = allocate(dob.getLength(), hosts);
+            alloc.write(id, dob.unsafeGetBufferAndFlip(), hosts);
+            flusher.waitForFlush(alloc);
+        }
+        catch (IOException e)
+        {
+            // exception during record serialization into the scratch buffer
+            throw new RuntimeException(e);
+        }
+    }
+
+    /**
+     * Asynchronously write a record to the journal. Writes to the journal in the calling thread,
+     * but doesn't wait for flush.
+     * <p/>
+     * Executes the supplied callback on the executor provided,
+     * once the record has been deemed durable according to the journal flush mode.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @param record the record to store
+     * @param hosts hosts expected to invalidate the record
+     */
+    public void asyncWrite(K id, V record, Set<Integer> hosts, @Nonnull Executor executor, @Nonnull Runnable onDurable)
+    {
+        try (DataOutputBuffer dob = DataOutputBuffer.scratchBuffer.get())
+        {
+            valueSerializer.serialize(record, dob, MessagingService.current_version);","[{'comment': '```suggestion\r\n            valueSerializer.serialize(record, dob, params.userVersion());\r\n```', 'commenter': 'dcapwell'}]"
2256,src/java/org/apache/cassandra/journal/Journal.java,"@@ -0,0 +1,644 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.journal;
+
+import java.io.IOException;
+import java.nio.file.FileStore;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.Executor;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.BooleanSupplier;
+import java.util.zip.CRC32;
+
+import javax.annotation.Nonnull;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.codahale.metrics.Timer.Context;
+import org.apache.cassandra.concurrent.Interruptible.TerminateException;
+import org.apache.cassandra.concurrent.Interruptible;
+import org.apache.cassandra.concurrent.SequentialExecutorPlus;
+import org.apache.cassandra.io.util.DataInputBuffer;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.io.util.PathUtils;
+import org.apache.cassandra.journal.Segments.ReferencedSegments;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.Crc;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+import org.apache.cassandra.utils.concurrent.WaitQueue;
+
+import static java.lang.String.format;
+import static java.util.Comparator.comparing;
+import static org.apache.cassandra.concurrent.ExecutorFactory.Global.executorFactory;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.Daemon.NON_DAEMON;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.Interrupts.SYNCHRONIZED;
+import static org.apache.cassandra.concurrent.InfiniteLoopExecutor.SimulatorSafe.SAFE;
+import static org.apache.cassandra.concurrent.Interruptible.State.NORMAL;
+import static org.apache.cassandra.concurrent.Interruptible.State.SHUTTING_DOWN;
+import static org.apache.cassandra.utils.Clock.Global.currentTimeMillis;
+import static org.apache.cassandra.utils.concurrent.WaitQueue.newWaitQueue;
+
+/**
+ * A generic append-only journal with some special features:
+ * <p><ul>
+ * <li>Records can be looked up by key
+ * <li>Records can be tagged with multiple owner node ids
+ * <li>Records can be invalidated by their owner ids
+ * <li>Fully invalidated records get purged during segment compaction
+ * </ul><p>
+ *
+ * Type parameters:
+ * @param <V> the type of records stored in the journal
+ * @param <K> the type of keys used to address the records;
+              must be fixed-size and byte-order comparable
+ */
+public class Journal<K, V>
+{
+    private static final Logger logger = LoggerFactory.getLogger(Journal.class);
+
+    final String name;
+    final File directory;
+    final Params params;
+
+    final KeySupport<K> keySupport;
+    final ValueSerializer<K, V> valueSerializer;
+
+    final Metrics<K, V> metrics;
+    final Flusher<K, V> flusher;
+    //final Invalidator<K, V> invalidator;
+    //final Compactor<K, V> compactor;
+
+    volatile long replayLimit;
+    final AtomicLong nextSegmentId = new AtomicLong();
+
+    private volatile ActiveSegment<K> currentSegment = null;
+
+    // segment that is ready to be used; allocator thread fills this and blocks until consumed
+    private volatile ActiveSegment<K> availableSegment = null;
+
+    private final AtomicReference<Segments<K>> segments = new AtomicReference<>();
+
+    Interruptible allocator;
+    private final WaitQueue segmentPrepared = newWaitQueue();
+    private final WaitQueue allocatorThreadWaitQueue = newWaitQueue();
+    private final BooleanSupplier allocatorThreadWaitCondition = () -> (availableSegment == null);
+
+    SequentialExecutorPlus closer;
+    //private final Set<Descriptor> invalidations = Collections.newSetFromMap(new ConcurrentHashMap<>());
+
+    public Journal(String name,
+                   File directory,
+                   Params params,
+                   KeySupport<K> keySupport,
+                   ValueSerializer<K, V> valueSerializer)
+    {
+        this.name = name;
+        this.directory = directory;
+        this.params = params;
+
+        this.keySupport = keySupport;
+        this.valueSerializer = valueSerializer;
+
+        this.metrics = new Metrics<>(name);
+        this.flusher = new Flusher<>(this);
+        //this.invalidator = new Invalidator<>(this);
+        //this.compactor = new Compactor<>(this);
+    }
+
+    public void start()
+    {
+        metrics.register(flusher);
+
+        deleteTmpFiles();
+
+        List<Descriptor> descriptors = Descriptor.list(directory);
+        // find the largest existing timestamp
+        descriptors.sort(null);
+        long maxTimestamp = descriptors.isEmpty()
+                          ? Long.MIN_VALUE
+                          : descriptors.get(descriptors.size() - 1).timestamp;
+        nextSegmentId.set(replayLimit = Math.max(currentTimeMillis(), maxTimestamp + 1));
+
+        segments.set(Segments.ofStatic(StaticSegment.open(descriptors, keySupport)));
+        closer = executorFactory().sequential(name + ""-closer"");
+        allocator = executorFactory().infiniteLoop(name + ""-allocator"", new AllocateRunnable(), SAFE, NON_DAEMON, SYNCHRONIZED);
+        advanceSegment(null);
+        flusher.start();
+        //invalidator.start();
+        //compactor.start();
+    }
+
+    /**
+     * Cleans up unfinished component files from previous run (metadata and index)
+     */
+    private void deleteTmpFiles()
+    {
+        for (File tmpFile : directory.listUnchecked(Descriptor::isTmpFile))
+            tmpFile.delete();
+    }
+
+    public void shutdown()
+    {
+        allocator.shutdown();
+        //compactor.stop();
+        //invalidator.stop();
+        flusher.shutdown();
+        closer.shutdown();
+        closeAllSegments();
+        metrics.deregister();
+    }
+
+    /**
+     * Looks up a record by the provided id.
+     * <p/>
+     * Looking up an invalidated record may or may not return a record, depending on
+     * compaction progress.
+     * <p/>
+     * In case multiple copies of the record exist in the log (e.g. because of user retries),
+     * only the first found record will be consumed.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @param consumer function to consume the raw record (bytes and invalidation set) if found
+     * @return true if the record was found, false otherwise
+     */
+    public boolean read(K id, RecordConsumer<K> consumer)
+    {
+        try (ReferencedSegments<K> segments = selectAndReference(id))
+        {
+            for (Segment<K> segment : segments.all())
+                if (segment.read(id, consumer))
+                    return true;
+        }
+
+        return false;
+    }
+
+    /**
+     * Looks up a record by the provided id.
+     * <p/>
+     * Looking up an invalidated record may or may not return a record, depending on
+     * compaction progress.
+     * <p/>
+     * In case multiple copies of the record exist in the log (e.g. because of user retries),
+     * the first one found will be returned.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @return deserialized record if found, null otherwise
+     */
+    public V read(K id)
+    {
+        EntrySerializer.EntryHolder<K> holder = new EntrySerializer.EntryHolder<>();
+
+        try (ReferencedSegments<K> segments = selectAndReference(id))
+        {
+            for (Segment<K> segment : segments.all())
+            {
+                if (segment.read(id, holder))
+                {
+                    try
+                    {
+                        return valueSerializer.deserialize(holder.key, new DataInputBuffer(holder.value, false), segment.descriptor.userVersion);
+                    }
+                    catch (IOException e)
+                    {
+                        // can only throw if serializer is buggy
+                        throw new RuntimeException(e);
+                    }
+                }
+            }
+        }
+
+        return null;
+    }
+
+    /**
+     * Synchronously write a record to the journal.
+     * <p/>
+     * Blocks until the record has been deemed durable according to the journal flush mode.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @param record the record to store
+     * @param hosts hosts expected to invalidate the record
+     */
+    public void write(K id, V record, Set<Integer> hosts)
+    {
+        try (DataOutputBuffer dob = DataOutputBuffer.scratchBuffer.get())
+        {
+            valueSerializer.serialize(record, dob, MessagingService.current_version);
+            ActiveSegment<K>.Allocation alloc = allocate(dob.getLength(), hosts);
+            alloc.write(id, dob.unsafeGetBufferAndFlip(), hosts);
+            flusher.waitForFlush(alloc);
+        }
+        catch (IOException e)
+        {
+            // exception during record serialization into the scratch buffer
+            throw new RuntimeException(e);
+        }
+    }
+
+    /**
+     * Asynchronously write a record to the journal. Writes to the journal in the calling thread,
+     * but doesn't wait for flush.
+     * <p/>
+     * Executes the supplied callback on the executor provided,
+     * once the record has been deemed durable according to the journal flush mode.
+     *
+     * @param id user-provided record id, expected to roughly correlate with time and go up
+     * @param record the record to store
+     * @param hosts hosts expected to invalidate the record
+     */
+    public void asyncWrite(K id, V record, Set<Integer> hosts, @Nonnull Executor executor, @Nonnull Runnable onDurable)","[{'comment': 'When I see `Runnable` for callback, I tend to ask what happens in the failure case... if I am reading this properly `org.apache.cassandra.journal.Flusher.FlushRunnable#run` sees the failure (so does `org.apache.cassandra.journal.Journal#asyncWrite`), but has no way to notify the blocked logic that the segment couldn\'t save; this can be a problem as the ""chain"" of operations going on in Accord never knows that the operation failed and won\'t ever succeed, so can\'t properly handle the error case on their side.\r\n\r\nWe should notify if the section of the segment we are waiting on failed for w/e reason and that durability maybe lost', 'commenter': 'dcapwell'}, {'comment': ""I also know this doesn't exist outside of Accord, but `AsyncChain` helps with this pattern... so could have the following signature\r\n\r\n```\r\npublic AsyncChain<Void> asyncWrite(K id, V record, Set<Integer> hosts)\r\n```\r\n\r\nThis allows the caller to control things with regard to the callback better, such as what executor is used (`addCallback(callback, executor)`) or what to do after the option complete.\r\n\r\nThis does not solve the concern that errors do not propagate to callbacks, that is still an issue, this just changes how callbacks are used"", 'commenter': 'dcapwell'}]"
2258,src/java/org/apache/cassandra/config/Config.java,"@@ -596,6 +596,7 @@ public MemtableOptions()
     public volatile boolean use_statements_enabled = true;
 
     public boolean accord_transactions_enabled = false;
+    public long accord_scheduler_delay_in_ms = 200L;","[{'comment': 'This could also be named `accord_progress_log_scheduler_delay_in_ms` but I felt it to verbose compared to the current one. I am open to suggestions on it.', 'commenter': 'kamalesh0406'}, {'comment': ""Shouldn't this be a DurationSpec rather than a long particularly if you expect very large values?"", 'commenter': 'Claudenw'}, {'comment': 'agree with this comment, `DurationSpec` (or one of the ones that limit the domain) is what we should do; `in_ms` should be avoided ', 'commenter': 'dcapwell'}]"
2258,src/java/org/apache/cassandra/service/accord/AccordService.java,"@@ -137,7 +139,9 @@ private AccordService()
         this.messageSink = new AccordMessageSink();
         this.configService = new AccordConfigurationService(localId);
         this.scheduler = new AccordScheduler();
+        this.config = new AccordConfig(DatabaseDescriptor.getAccordSchedulerDelayInMS());","[{'comment': ""this pattern isn't something we can maintain; every new config comes with a massive burden to update this constructor; IMO we should push this logic into the YAML layer"", 'commenter': 'dcapwell'}]"
2267,conf/cassandra.yaml,"@@ -1924,8 +1924,14 @@ drop_compact_storage_enabled: false
 # which is used in some serialization to denote the format type in a compact way (such as local key cache); and 'name'
 # which will be used to recognize the format type - in particular that name will be used in sstable file names and in
 # stream headers so the name has to be the same for the same format across all the nodes in the cluster.
+# The first entry in this list is the format that will be used for newly-created SSTables. The other formats given
+# will be used to read any SSTables present in the data directories or streamed.
 sstable_formats:
   - class_name: org.apache.cassandra.io.sstable.format.big.BigFormat
     parameters:
       id: 0
       name: big
+  - class_name: org.apache.cassandra.io.sstable.format.bti.BtiFormat
+    parameters:
+      id: 1
+      name: bti","[{'comment': ""I wasn't able to look at the original SSTable format API configuration work, but there are a few things that worry me a bit:\r\n\r\n1.) Hard to know without actually writing the code, but it feels like some things would have been a bit simpler if we avoided `ParameterizedClass` and just used a `EncryptionOptions`-eqsue configuration object to contain the structure and validation logic for the formats.\r\n\r\n2.) There are a couple things here that feel very dangerous to expose to an operator via local configuration, with both local and cluster-wide implications.\r\n\r\nThe first is having the `id` concept specified in config. If we start a node, write some SSTables, bring the node down, change the id of the primary format, then start the node again, I think we can break things like `KeyCacheSerializer#deserialize()`, which have expectations around previously written `id`s and how they map to formats. The ID concept feels like it should be statically and globally defined by the format itself. If you create a custom format, it should simply avoid conflicting w/ built-in `id`s/`name`s. (This may become a distributed problem around streaming too through a local configuration change on another node?)\r\n\r\nSecond, while specifying a primary format (to write new SSTables) is necessary, allowing that to be determined by physical order in the YAML should IMO be avoided. It isn't a catastrophic risk, but one where you could silently write the wrong format for an accidental ordering mistake. The selection of a primary format should be explicit/not be a mystery in the absence of inline comments.\r\n\r\nLast (and I feel like we had this discussion around the Memtable API a while back), having to specify anything about the **built-in** formats in YAML space feels unnecessary. We'd still need something like this to specify custom formats, etc.\r\n\r\nI'm not pushing for a [Lucene-style SPI approach](https://www.elastic.co/blog/what-is-an-apache-lucene-codec) or anything in particular, but we need to at least discuss the above."", 'commenter': 'maedhroz'}, {'comment': 'Agree with what @maedhroz said, and to extend on that.\r\n\r\n1) We figure out which format to use by using `org.apache.cassandra.io.sstable.format.SSTableFormat.Type#current`, which uses the first element of the list, which is not determinist as it is based off `Map` ordering (see `org.apache.cassandra.io.sstable.format.SSTableFormat.Type#readFactories`)\r\n1.1) we use a system property to override, but we should be using a config in cassandra.yml.  cassandra.yml is accessible via system properties if that is what a user wants, but also plugs in well with the rest of the system.\r\n1.2) if we do go the config route, we can make this a hot-prop using the existing config system\r\n2) streaming now requires globally consistent configs... this isn\'t realistic without transactional cluster metadata, so cases such as `org.apache.cassandra.db.streaming.CassandraStreamHeader.CassandraStreamHeaderSerializer#serialize` can have one instance writing ""abc"" and the other side failing as it doesn\'t know about ""abc"" (because it defined it as ""big"").\r\n\r\nI strongly feel we should redo this config layer and remove id/name from operator control as this isn\'t safe.  \r\n\r\nNow, if you want to add a custom format (I believe one of the goals of the pluggable work) then we need a ""safe"" way to discover, and I feel the only real safe way is discovery is via class existence (such as `ServiceLoader` from java).', 'commenter': 'dcapwell'}, {'comment': 'This seems introduce in CASSANDRA-17056,  aggree with ""Hard to know without actually writing the code"" ,  and I think at the very least we should add more detailed comments to explain the useage of ""sstable_formats"" .\r\nI think it is not enough for users to configure this options if they do not read the code, ""what does the id mean ? "" , ""Can I set the id to 100? "", ""what does name mean? ""', 'commenter': 'Maxwell-Guo'}, {'comment': 'Let us continue this discussion in CASSANDRA-18441.', 'commenter': 'blambov'}, {'comment': ""I'll fix the configuration in CASSANDRA-18441, I agree it is bad and was aimed too much to make everything configurable.\r\n\r\n(nit: @dcapwell that map is `ImmutableMap` which retains order, OTOH that's too bad that `DatabaseDescriptor` does not make it explicit and returns just a `Map`)"", 'commenter': 'jacek-lewandowski'}]"
2267,src/java/org/apache/cassandra/db/rows/LazilyInitializedUnfilteredRowIterator.java,"@@ -105,4 +105,4 @@ public void close()
         if (iterator != null)
             iterator.close();
     }
-}
+}","[{'comment': 'nit: necessary to remove? I have no idea if we have a style-guide thing for EoF newlines :D', 'commenter': 'maedhroz'}, {'comment': 'Reverted', 'commenter': 'blambov'}]"
2267,src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java,"@@ -337,6 +337,41 @@ private void seekToChunkStart()
         }
     }
 
+    // Page management using chunk boundaries
+
+    @Override
+    public int maxBytesInPage()
+    {
+        return buffer.capacity();
+    }
+
+    @Override
+    public void padToPageBoundary() throws IOException","[{'comment': 'nit:\r\n```suggestion\r\n    public void padToPageBoundary()\r\n```\r\n(I think we removed it in the SAI patch as well.)', 'commenter': 'maedhroz'}, {'comment': 'Method removed', 'commenter': 'blambov'}]"
2267,src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java,"@@ -337,6 +337,41 @@ private void seekToChunkStart()
         }
     }
 
+    // Page management using chunk boundaries
+
+    @Override
+    public int maxBytesInPage()
+    {
+        return buffer.capacity();
+    }
+
+    @Override
+    public void padToPageBoundary() throws IOException
+    {
+        if (buffer.position() == 0)
+            return;
+
+        int padLength = buffer.remaining();
+
+        // Flush as much as we have
+        doFlush(0);
+        // But pretend we had a whole chunk
+        bufferOffset += padLength;
+        lastFlushOffset += padLength;
+    }
+
+    @Override
+    public int bytesLeftInPage()
+    {
+        return buffer.remaining();
+    }
+
+    @Override
+    public long paddedPosition()
+    {
+        return position() + (buffer.position() == 0 ? 0 : buffer.remaining());
+    }
+","[{'comment': ""nit: Didn't find any test coverage for these new padding-related methods, but my search was probably too shallow..."", 'commenter': 'maedhroz'}, {'comment': 'I would be surprised if there are. The related functionality is not ported.\r\n\r\nRemoved the methods, they are currently unused.', 'commenter': 'blambov'}]"
2267,src/java/org/apache/cassandra/db/DeletionTime.java,"@@ -191,4 +201,4 @@ public long serializedSize(DeletionTime delTime)
                  + TypeSizes.sizeof(delTime.markedForDeleteAt());
         }
     }
-}
+}","[{'comment': 'seems no necessary to remove and add a new line.', 'commenter': 'Maxwell-Guo'}]"
2267,src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java,"@@ -337,6 +337,41 @@ private void seekToChunkStart()
         }
     }
 
+    // Page management using chunk boundaries","[{'comment': 'What does line 340 means ? a comment for method ""maxBytesInPage()"" ？ ', 'commenter': 'Maxwell-Guo'}, {'comment': 'What does line 340 means ? a comment for method ""maxBytesInPage()"" ？ ', 'commenter': 'Maxwell-Guo'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/BtiFormat.java,"@@ -0,0 +1,490 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Set;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.lifecycle.LifecycleTransaction;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.sstable.Component;
+import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.io.sstable.GaugeProvider;
+import org.apache.cassandra.io.sstable.IScrubber;
+import org.apache.cassandra.io.sstable.MetricsProviders;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.filter.BloomFilterMetrics;
+import org.apache.cassandra.io.sstable.format.AbstractSSTableFormat;
+import org.apache.cassandra.io.sstable.format.SSTableFormat;
+import org.apache.cassandra.io.sstable.format.SSTableReader;
+import org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder;
+import org.apache.cassandra.io.sstable.format.SSTableWriter;
+import org.apache.cassandra.io.sstable.format.SortedTableScrubber;
+import org.apache.cassandra.io.sstable.format.Version;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.schema.TableMetadataRef;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+import org.apache.cassandra.utils.OutputHandler;
+import org.apache.cassandra.utils.Pair;
+
+/**
+ * Bigtable format with trie indices
+ */
+public class BtiFormat extends AbstractSSTableFormat<BtiTableReader, BtiTableWriter>
+{
+    private final static Logger logger = LoggerFactory.getLogger(BtiFormat.class);
+
+    public static final BtiFormat instance = new BtiFormat();
+
+    public static final Version latestVersion = new BtiVersion(BtiVersion.current_version);
+    static final BtiTableReaderFactory readerFactory = new BtiTableReaderFactory();
+    static final BtiTableWriterFactory writerFactory = new BtiTableWriterFactory();
+
+    public static class Components extends AbstractSSTableFormat.Components
+    {
+        public static class Types extends AbstractSSTableFormat.Components.Types
+        {
+            public static final Component.Type PARTITION_INDEX = Component.Type.createSingleton(""PARTITION_INDEX"", ""Partitions.db"", BtiFormat.class);
+            public static final Component.Type ROW_INDEX = Component.Type.createSingleton(""ROW_INDEX"", ""Rows.db"", BtiFormat.class);
+        }
+
+        public final static Component PARTITION_INDEX = Types.PARTITION_INDEX.getSingleton();
+
+        public final static Component ROW_INDEX = Types.ROW_INDEX.getSingleton();
+
+        private final static Set<Component> STREAMING_COMPONENTS = ImmutableSet.of(DATA,
+                                                                                   PARTITION_INDEX,
+                                                                                   ROW_INDEX,
+                                                                                   STATS,
+                                                                                   COMPRESSION_INFO,
+                                                                                   FILTER,
+                                                                                   DIGEST,
+                                                                                   CRC);
+
+        private final static Set<Component> PRIMARY_COMPONENTS = ImmutableSet.of(DATA,
+                                                                                 PARTITION_INDEX);
+
+        private final static Set<Component> MUTABLE_COMPONENTS = ImmutableSet.of(STATS);
+
+        private static final Set<Component> UPLOAD_COMPONENTS = ImmutableSet.of(DATA,
+                                                                                PARTITION_INDEX,
+                                                                                ROW_INDEX,
+                                                                                COMPRESSION_INFO,
+                                                                                STATS);
+
+        private static final Set<Component> BATCH_COMPONENTS = ImmutableSet.of(DATA,
+                                                                               PARTITION_INDEX,
+                                                                               ROW_INDEX,
+                                                                               COMPRESSION_INFO,
+                                                                               FILTER,
+                                                                               STATS);
+
+        private final static Set<Component> ALL_COMPONENTS = ImmutableSet.of(DATA,
+                                                                             PARTITION_INDEX,
+                                                                             ROW_INDEX,
+                                                                             STATS,
+                                                                             COMPRESSION_INFO,
+                                                                             FILTER,
+                                                                             DIGEST,
+                                                                             CRC,
+                                                                             TOC);
+
+        private final static Set<Component> GENERATED_ON_LOAD_COMPONENTS = ImmutableSet.of(FILTER);
+    }
+
+
+    private BtiFormat()
+    {
+
+    }
+
+    public static BtiFormat getInstance()
+    {
+        return instance;
+    }
+
+    public static boolean isDefault()
+    {
+        return getInstance().getType() == Type.current();
+    }
+
+    @Override
+    public Version getLatestVersion()
+    {
+        return latestVersion;
+    }
+
+    @Override
+    public Version getVersion(String version)
+    {
+        return new BtiVersion(version);
+    }
+
+    @Override
+    public BtiTableWriterFactory getWriterFactory()
+    {
+        return writerFactory;
+    }
+
+    @Override
+    public BtiTableReaderFactory getReaderFactory()
+    {
+        return readerFactory;
+    }
+
+    @Override
+    public Set<Component> streamingComponents()
+    {
+        return Components.STREAMING_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> primaryComponents()
+    {
+        return Components.PRIMARY_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> batchComponents()
+    {
+        return Components.BATCH_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> uploadComponents()
+    {
+        return Components.UPLOAD_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> mutableComponents()
+    {
+        return Components.MUTABLE_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> allComponents()
+    {
+        return Components.ALL_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> generatedOnLoadComponents()
+    {
+        return Components.GENERATED_ON_LOAD_COMPONENTS;
+    }
+
+    @Override
+    public SSTableFormat.KeyCacheValueSerializer<BtiTableReader, TrieIndexEntry> getKeyCacheValueSerializer()
+    {
+        throw new AssertionError(""BTI sstables do not use key cache"");
+    }
+
+    @Override
+    public IScrubber getScrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, OutputHandler outputHandler, IScrubber.Options options)
+    {
+        Preconditions.checkArgument(cfs.metadata().equals(transaction.onlyOne().metadata()));
+        return new BtiTableScrubber(cfs, transaction, outputHandler, options);
+    }
+
+    @Override
+    public BtiTableReader cast(SSTableReader sstr)
+    {
+        return (BtiTableReader) sstr;
+    }
+
+    @Override
+    public BtiTableWriter cast(SSTableWriter sstw)
+    {
+        return (BtiTableWriter) sstw;
+    }
+
+    @Override
+    public MetricsProviders getFormatSpecificMetricsProviders()
+    {
+        return BtiTableSpecificMetricsProviders.instance;
+    }
+
+    @Override
+    public void deleteOrphanedComponents(Descriptor descriptor, Set<Component> components)
+    {
+        SortedTableScrubber.deleteOrphanedComponents(descriptor, components);
+    }
+
+    private void delete(Descriptor desc, List<Component> components)
+    {
+        logger.info(""Deleting sstable: {}"", desc);
+
+        if (components.remove(SSTableFormat.Components.DATA))
+            components.add(0, SSTableFormat.Components.DATA); // DATA component should be first
+
+        for (Component component : components)
+        {
+            logger.trace(""Deleting component {} of {}"", component, desc);
+            desc.fileFor(component).deleteIfExists();
+        }
+    }
+
+    @Override
+    public void delete(Descriptor desc)
+    {
+        try
+        {
+            delete(desc, Lists.newArrayList(Sets.intersection(allComponents(), desc.discoverComponents())));
+        }
+        catch (Throwable t)
+        {
+            JVMStabilityInspector.inspectThrowable(t);
+        }
+    }
+
+    static class BtiTableReaderFactory implements SSTableReaderFactory<BtiTableReader, BtiTableReader.Builder>
+    {
+        @Override
+        public SSTableReader.Builder<BtiTableReader, BtiTableReader.Builder> builder(Descriptor descriptor)
+        {
+            return new BtiTableReader.Builder(descriptor);
+        }
+
+        @Override
+        public SSTableReaderLoadingBuilder<BtiTableReader, BtiTableReader.Builder> loadingBuilder(Descriptor descriptor, TableMetadataRef tableMetadataRef, Set<Component> components)
+        {
+            return new BtiTableReaderLoadingBuilder(new SSTable.Builder<>(descriptor).setTableMetadataRef(tableMetadataRef)
+                                                                                     .setComponents(components));
+        }
+
+        @Override
+        public Pair<DecoratedKey, DecoratedKey> readKeyRange(Descriptor descriptor, IPartitioner partitioner) throws IOException
+        {
+            return PartitionIndex.readFirstAndLastKey(descriptor.fileFor(Components.PARTITION_INDEX), partitioner);
+        }
+
+        @Override
+        public Class<BtiTableReader> getReaderClass()
+        {
+            return BtiTableReader.class;
+        }
+    }
+
+    static class BtiTableWriterFactory implements SSTableWriterFactory<BtiTableWriter, BtiTableWriter.Builder>
+    {
+        @Override
+        public BtiTableWriter.Builder builder(Descriptor descriptor)
+        {
+            return new BtiTableWriter.Builder(descriptor);
+        }
+
+        @Override
+        public long estimateSize(SSTableWriter.SSTableSizeParameters parameters)
+        {
+            return (long) ((parameters.partitionCount() // index entries
+                            + parameters.partitionCount() // keys in data file
+                            + parameters.dataSize()) // data
+                           * 1.2); // bloom filter and row index overhead
+        }
+    }
+
+    // versions are denoted as [major][minor].  Minor versions must be forward-compatible:
+    // new fields are allowed in e.g. the metadata component, but fields can't be removed
+    // or have their size changed.
+    //
+    static class BtiVersion extends Version
+    {
+        public static final String current_version = ""da"";
+        public static final String earliest_supported_version = ""ca"";
+
+        // aa (DSE 6.0): trie index format","[{'comment': 'I think these descriptions for DSE can be removed ,although dse and apache cassandra may have some in common, but I think developers may do not really know the version or sstable version for DSE. \r\nsome descriptions for apache cassandra sstable may be more suitable。', 'commenter': 'Maxwell-Guo'}, {'comment': 'I think as this sstable format may be used in dse for long time ,so these comments are left, in my mind, we may need to add comments to identify this file format, as this is the first time we introduce this sstable file format to apache cassandra, so we may do not need to describle the differences betweetn dse versions for Trie-indexed sstable, only need to add some comments about the  reason this file format has now evolved to the current committed version.', 'commenter': 'Maxwell-Guo'}, {'comment': 'as these versions may be used int the construct function of BtiVersion, and as cassandra do not have these sstable versions as dse have ,so I think we need to modify the code here to adapt to the existing file format(version) of apache cassandra', 'commenter': 'Maxwell-Guo'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/BtiFormat.java,"@@ -0,0 +1,490 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Set;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.lifecycle.LifecycleTransaction;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.sstable.Component;
+import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.io.sstable.GaugeProvider;
+import org.apache.cassandra.io.sstable.IScrubber;
+import org.apache.cassandra.io.sstable.MetricsProviders;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.filter.BloomFilterMetrics;
+import org.apache.cassandra.io.sstable.format.AbstractSSTableFormat;
+import org.apache.cassandra.io.sstable.format.SSTableFormat;
+import org.apache.cassandra.io.sstable.format.SSTableReader;
+import org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder;
+import org.apache.cassandra.io.sstable.format.SSTableWriter;
+import org.apache.cassandra.io.sstable.format.SortedTableScrubber;
+import org.apache.cassandra.io.sstable.format.Version;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.schema.TableMetadataRef;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+import org.apache.cassandra.utils.OutputHandler;
+import org.apache.cassandra.utils.Pair;
+
+/**
+ * Bigtable format with trie indices
+ */
+public class BtiFormat extends AbstractSSTableFormat<BtiTableReader, BtiTableWriter>
+{
+    private final static Logger logger = LoggerFactory.getLogger(BtiFormat.class);
+
+    public static final BtiFormat instance = new BtiFormat();
+
+    public static final Version latestVersion = new BtiVersion(BtiVersion.current_version);
+    static final BtiTableReaderFactory readerFactory = new BtiTableReaderFactory();
+    static final BtiTableWriterFactory writerFactory = new BtiTableWriterFactory();
+
+    public static class Components extends AbstractSSTableFormat.Components
+    {
+        public static class Types extends AbstractSSTableFormat.Components.Types
+        {
+            public static final Component.Type PARTITION_INDEX = Component.Type.createSingleton(""PARTITION_INDEX"", ""Partitions.db"", BtiFormat.class);
+            public static final Component.Type ROW_INDEX = Component.Type.createSingleton(""ROW_INDEX"", ""Rows.db"", BtiFormat.class);
+        }
+
+        public final static Component PARTITION_INDEX = Types.PARTITION_INDEX.getSingleton();
+
+        public final static Component ROW_INDEX = Types.ROW_INDEX.getSingleton();
+
+        private final static Set<Component> STREAMING_COMPONENTS = ImmutableSet.of(DATA,
+                                                                                   PARTITION_INDEX,
+                                                                                   ROW_INDEX,
+                                                                                   STATS,
+                                                                                   COMPRESSION_INFO,
+                                                                                   FILTER,
+                                                                                   DIGEST,
+                                                                                   CRC);
+
+        private final static Set<Component> PRIMARY_COMPONENTS = ImmutableSet.of(DATA,
+                                                                                 PARTITION_INDEX);
+
+        private final static Set<Component> MUTABLE_COMPONENTS = ImmutableSet.of(STATS);
+
+        private static final Set<Component> UPLOAD_COMPONENTS = ImmutableSet.of(DATA,
+                                                                                PARTITION_INDEX,
+                                                                                ROW_INDEX,
+                                                                                COMPRESSION_INFO,
+                                                                                STATS);
+
+        private static final Set<Component> BATCH_COMPONENTS = ImmutableSet.of(DATA,
+                                                                               PARTITION_INDEX,
+                                                                               ROW_INDEX,
+                                                                               COMPRESSION_INFO,
+                                                                               FILTER,
+                                                                               STATS);
+
+        private final static Set<Component> ALL_COMPONENTS = ImmutableSet.of(DATA,
+                                                                             PARTITION_INDEX,
+                                                                             ROW_INDEX,
+                                                                             STATS,
+                                                                             COMPRESSION_INFO,
+                                                                             FILTER,
+                                                                             DIGEST,
+                                                                             CRC,
+                                                                             TOC);
+
+        private final static Set<Component> GENERATED_ON_LOAD_COMPONENTS = ImmutableSet.of(FILTER);
+    }
+
+
+    private BtiFormat()
+    {
+
+    }
+
+    public static BtiFormat getInstance()
+    {
+        return instance;
+    }
+
+    public static boolean isDefault()
+    {
+        return getInstance().getType() == Type.current();
+    }
+
+    @Override
+    public Version getLatestVersion()
+    {
+        return latestVersion;
+    }
+
+    @Override
+    public Version getVersion(String version)
+    {
+        return new BtiVersion(version);
+    }
+
+    @Override
+    public BtiTableWriterFactory getWriterFactory()
+    {
+        return writerFactory;
+    }
+
+    @Override
+    public BtiTableReaderFactory getReaderFactory()
+    {
+        return readerFactory;
+    }
+
+    @Override
+    public Set<Component> streamingComponents()
+    {
+        return Components.STREAMING_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> primaryComponents()
+    {
+        return Components.PRIMARY_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> batchComponents()
+    {
+        return Components.BATCH_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> uploadComponents()
+    {
+        return Components.UPLOAD_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> mutableComponents()
+    {
+        return Components.MUTABLE_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> allComponents()
+    {
+        return Components.ALL_COMPONENTS;
+    }
+
+    @Override
+    public Set<Component> generatedOnLoadComponents()
+    {
+        return Components.GENERATED_ON_LOAD_COMPONENTS;
+    }
+
+    @Override
+    public SSTableFormat.KeyCacheValueSerializer<BtiTableReader, TrieIndexEntry> getKeyCacheValueSerializer()
+    {
+        throw new AssertionError(""BTI sstables do not use key cache"");
+    }
+
+    @Override
+    public IScrubber getScrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, OutputHandler outputHandler, IScrubber.Options options)
+    {
+        Preconditions.checkArgument(cfs.metadata().equals(transaction.onlyOne().metadata()));
+        return new BtiTableScrubber(cfs, transaction, outputHandler, options);
+    }
+
+    @Override
+    public BtiTableReader cast(SSTableReader sstr)
+    {
+        return (BtiTableReader) sstr;
+    }
+
+    @Override
+    public BtiTableWriter cast(SSTableWriter sstw)
+    {
+        return (BtiTableWriter) sstw;
+    }
+
+    @Override
+    public MetricsProviders getFormatSpecificMetricsProviders()
+    {
+        return BtiTableSpecificMetricsProviders.instance;
+    }
+
+    @Override
+    public void deleteOrphanedComponents(Descriptor descriptor, Set<Component> components)
+    {
+        SortedTableScrubber.deleteOrphanedComponents(descriptor, components);
+    }
+
+    private void delete(Descriptor desc, List<Component> components)
+    {
+        logger.info(""Deleting sstable: {}"", desc);
+
+        if (components.remove(SSTableFormat.Components.DATA))
+            components.add(0, SSTableFormat.Components.DATA); // DATA component should be first
+
+        for (Component component : components)
+        {
+            logger.trace(""Deleting component {} of {}"", component, desc);
+            desc.fileFor(component).deleteIfExists();
+        }
+    }
+
+    @Override
+    public void delete(Descriptor desc)
+    {
+        try
+        {
+            delete(desc, Lists.newArrayList(Sets.intersection(allComponents(), desc.discoverComponents())));
+        }
+        catch (Throwable t)
+        {
+            JVMStabilityInspector.inspectThrowable(t);
+        }
+    }
+
+    static class BtiTableReaderFactory implements SSTableReaderFactory<BtiTableReader, BtiTableReader.Builder>
+    {
+        @Override
+        public SSTableReader.Builder<BtiTableReader, BtiTableReader.Builder> builder(Descriptor descriptor)
+        {
+            return new BtiTableReader.Builder(descriptor);
+        }
+
+        @Override
+        public SSTableReaderLoadingBuilder<BtiTableReader, BtiTableReader.Builder> loadingBuilder(Descriptor descriptor, TableMetadataRef tableMetadataRef, Set<Component> components)
+        {
+            return new BtiTableReaderLoadingBuilder(new SSTable.Builder<>(descriptor).setTableMetadataRef(tableMetadataRef)
+                                                                                     .setComponents(components));
+        }
+
+        @Override
+        public Pair<DecoratedKey, DecoratedKey> readKeyRange(Descriptor descriptor, IPartitioner partitioner) throws IOException
+        {
+            return PartitionIndex.readFirstAndLastKey(descriptor.fileFor(Components.PARTITION_INDEX), partitioner);
+        }
+
+        @Override
+        public Class<BtiTableReader> getReaderClass()
+        {
+            return BtiTableReader.class;
+        }
+    }
+
+    static class BtiTableWriterFactory implements SSTableWriterFactory<BtiTableWriter, BtiTableWriter.Builder>
+    {
+        @Override
+        public BtiTableWriter.Builder builder(Descriptor descriptor)
+        {
+            return new BtiTableWriter.Builder(descriptor);
+        }
+
+        @Override
+        public long estimateSize(SSTableWriter.SSTableSizeParameters parameters)
+        {
+            return (long) ((parameters.partitionCount() // index entries
+                            + parameters.partitionCount() // keys in data file
+                            + parameters.dataSize()) // data
+                           * 1.2); // bloom filter and row index overhead
+        }
+    }
+
+    // versions are denoted as [major][minor].  Minor versions must be forward-compatible:
+    // new fields are allowed in e.g. the metadata component, but fields can't be removed
+    // or have their size changed.
+    //
+    static class BtiVersion extends Version
+    {
+        public static final String current_version = ""da"";
+        public static final String earliest_supported_version = ""ca"";
+
+        // aa (DSE 6.0): trie index format
+        // ab (DSE pre-6.8): ILLEGAL - handled as 'b' (predates 'ba'). Pre-GA ""LABS"" releases of DSE 6.8 used this
+        //                   sstable version.
+        // ac (DSE 6.0.11, 6.7.6): corrected sstable min/max clustering (DB-3691/CASSANDRA-14861)
+        // ad (DSE 6.0.14, 6.7.11): added hostId of the node from which the sstable originated (DB-4629)
+        // b  (DSE early 6.8 ""LABS"") has some of 6.8 features but not all
+        // ba (DSE 6.8): encrypted indices and metadata
+        //               new BloomFilter serialization format
+        //               add incremental NodeSync information to metadata
+        //               improved min/max clustering representation
+        //               presence marker for partition level deletions
+        // bb (DSE 6.8.5): added hostId of the node from which the sstable originated (DB-4629)
+        // versions aa-bz are not supported in OSS
+        // ca (DSE-DB aka Stargazer based on OSS 4.0): all OSS fields + all DSE fields in DSE serialization format
+        // da - same as ca but in OSS serialization format
+        // NOTE: when adding a new version, please add that to LegacySSTableTest, too.
+
+        private final boolean isLatestVersion;
+
+        /**
+         * DB-2648/CASSANDRA-9067: DSE 6.8/OSS 4.0 bloom filter representation changed (bitset data is no longer stored
+         * as BIG_ENDIAN longs, which avoids some redundant bit twiddling).
+         */
+        private final int correspondingMessagingVersion;
+
+        private final boolean hasOldBfFormat;
+        private final boolean hasAccurateMinMax;
+        private final boolean hasImprovedMinMax;
+        private final boolean hasKeyRange;
+        private final boolean hasPartitionLevelDeletionsPresenceMarker;
+        private final boolean hasOriginatingHostId;
+
+        BtiVersion(String version)
+        {
+            super(instance, version = mapAb(version));
+
+            isLatestVersion = version.compareTo(current_version) == 0;
+            hasOldBfFormat = version.compareTo(""b"") < 0;
+            hasAccurateMinMax = version.compareTo(""ac"") >= 0;
+            hasOriginatingHostId = version.matches(""(a[d-z])|(b[b-z])"") || version.compareTo(""ca"") >= 0;
+            hasImprovedMinMax = version.compareTo(""ba"") >= 0;
+            hasKeyRange = version.compareTo(""da"") >= 0;
+            hasPartitionLevelDeletionsPresenceMarker = version.compareTo(""ba"") >= 0;
+            correspondingMessagingVersion = MessagingService.VERSION_40;
+        }
+
+        // this is for the ab version which was used in the LABS, and then has been renamed to ba","[{'comment': 'what does this comment LABS mean ?', 'commenter': 'Maxwell-Guo'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/BtiTableScrubber.java,"@@ -0,0 +1,303 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOError;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.db.compaction.CompactionInterruptedException;
+import org.apache.cassandra.db.lifecycle.LifecycleTransaction;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.db.rows.UnfilteredRowIterators;
+import org.apache.cassandra.io.sstable.IScrubber;
+import org.apache.cassandra.io.sstable.SSTableRewriter;
+import org.apache.cassandra.io.sstable.format.SortedTableScrubber;
+import org.apache.cassandra.io.sstable.format.bti.BtiFormat.Components;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.OutputHandler;
+import org.apache.cassandra.utils.Throwables;
+
+public class BtiTableScrubber extends SortedTableScrubber<BtiTableReader> implements IScrubber
+{
+    private final boolean isIndex;
+    private ScrubPartitionIterator indexIterator;
+
+    public BtiTableScrubber(ColumnFamilyStore cfs,
+                            LifecycleTransaction transaction,
+                            OutputHandler outputHandler,
+                            IScrubber.Options options)
+    {
+        super(cfs, transaction, outputHandler, options);
+
+        boolean hasIndexFile = sstable.getComponents().contains(Components.PARTITION_INDEX);
+        this.isIndex = cfs.isIndex();
+        if (!hasIndexFile)
+        {
+            // if there's any corruption in the -Data.db then partitions can't be skipped over. but it's worth a shot.
+            outputHandler.warn(""Missing index component"");
+        }
+
+        try
+        {
+            this.indexIterator = hasIndexFile
+                                 ? openIndexIterator()
+                                 : null;
+        }
+        catch (RuntimeException ex)
+        {
+            outputHandler.warn(""Detected corruption in the index file - cannot open index iterator"", ex);
+        }
+    }
+
+    private ScrubPartitionIterator openIndexIterator()
+    {
+        try
+        {
+            return sstable.scrubPartitionsIterator();
+        }
+        catch (IOException e)
+        {
+            outputHandler.warn(""Index is unreadable."");","[{'comment': 'I think the message is not detailed enough， and the exception stack can be added.', 'commenter': 'Maxwell-Guo'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/BtiTableVerifier.java,"@@ -0,0 +1,207 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import com.google.common.base.Throwables;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.compaction.CompactionInterruptedException;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.sstable.IVerifier;
+import org.apache.cassandra.io.sstable.KeyReader;
+import org.apache.cassandra.io.sstable.SSTableIdentityIterator;
+import org.apache.cassandra.io.sstable.format.SSTableReader;
+import org.apache.cassandra.io.sstable.format.SortedTableVerifier;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.OutputHandler;
+
+public class BtiTableVerifier extends SortedTableVerifier<BtiTableReader> implements IVerifier
+{
+    public BtiTableVerifier(ColumnFamilyStore cfs, BtiTableReader sstable, OutputHandler outputHandler, boolean isOffline, Options options)
+    {
+        super(cfs, sstable, outputHandler, isOffline, options);
+    }
+
+    public void verify()
+    {
+        verifySSTableVersion();
+
+        verifySSTableMetadata();
+
+        verifyIndex();
+
+        verifyBloomFilter();
+
+        if (options.checkOwnsTokens && !isOffline && !(cfs.getPartitioner() instanceof LocalPartitioner))
+        {
+            if (verifyOwnedRanges() == 0)
+                return;
+        }
+
+        if (options.quick)
+            return;
+
+        if (verifyDigest() && !options.extendedVerification)
+            return;
+
+        verifySSTable();
+
+        outputHandler.output(""Verify of %s succeeded. All %d rows read successfully"", sstable, goodRows);
+    }
+
+    private void verifySSTable()
+    {
+        long rowStart;","[{'comment': 'I think rowStart can be move to line 95 in the while loop ', 'commenter': 'Maxwell-Guo'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/big/BigTableScanner.java,"@@ -345,11 +339,15 @@ protected UnfilteredRowIterator computeNext()
 
                 /*
                  * For a given partition key, we want to avoid hitting the data
-                 * file unless we're explicitely asked to. This is important
+                 * file unless we're explicitly asked to. This is important
                  * for PartitionRangeReadCommand#checkCacheFilter.
                  */
                 return new LazilyInitializedUnfilteredRowIterator(currentKey)
                 {
+                    // Store currentEntry reference during object instantiation as later (during initialize) the
+                    // reference may point to a different entry.
+                    private final RowIndexEntry rowIndexEntry = currentEntry;","[{'comment': 'Was there are test failing somewhere around this, or is it just something we noticed offhand?', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/BtiTableVerifier.java,"@@ -0,0 +1,207 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import com.google.common.base.Throwables;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.compaction.CompactionInterruptedException;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.sstable.IVerifier;
+import org.apache.cassandra.io.sstable.KeyReader;
+import org.apache.cassandra.io.sstable.SSTableIdentityIterator;
+import org.apache.cassandra.io.sstable.format.SSTableReader;
+import org.apache.cassandra.io.sstable.format.SortedTableVerifier;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.OutputHandler;
+
+public class BtiTableVerifier extends SortedTableVerifier<BtiTableReader> implements IVerifier
+{
+    public BtiTableVerifier(ColumnFamilyStore cfs, BtiTableReader sstable, OutputHandler outputHandler, boolean isOffline, Options options)
+    {
+        super(cfs, sstable, outputHandler, isOffline, options);
+    }
+
+    public void verify()
+    {
+        verifySSTableVersion();
+
+        verifySSTableMetadata();
+
+        verifyIndex();
+
+        verifyBloomFilter();","[{'comment': 'nit: Line 60 to the end of the method is duplicated in `BigTableVerifier#verify()` FWIW', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/BtiTableVerifier.java,"@@ -0,0 +1,207 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import com.google.common.base.Throwables;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.compaction.CompactionInterruptedException;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.sstable.IVerifier;
+import org.apache.cassandra.io.sstable.KeyReader;
+import org.apache.cassandra.io.sstable.SSTableIdentityIterator;
+import org.apache.cassandra.io.sstable.format.SSTableReader;
+import org.apache.cassandra.io.sstable.format.SortedTableVerifier;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.OutputHandler;
+
+public class BtiTableVerifier extends SortedTableVerifier<BtiTableReader> implements IVerifier
+{
+    public BtiTableVerifier(ColumnFamilyStore cfs, BtiTableReader sstable, OutputHandler outputHandler, boolean isOffline, Options options)
+    {
+        super(cfs, sstable, outputHandler, isOffline, options);
+    }
+
+    public void verify()
+    {
+        verifySSTableVersion();
+
+        verifySSTableMetadata();
+
+        verifyIndex();
+
+        verifyBloomFilter();
+
+        if (options.checkOwnsTokens && !isOffline && !(cfs.getPartitioner() instanceof LocalPartitioner))
+        {
+            if (verifyOwnedRanges() == 0)
+                return;
+        }
+
+        if (options.quick)
+            return;
+
+        if (verifyDigest() && !options.extendedVerification)
+            return;
+
+        verifySSTable();
+
+        outputHandler.output(""Verify of %s succeeded. All %d rows read successfully"", sstable, goodRows);
+    }
+
+    private void verifySSTable()
+    {
+        long rowStart;
+        outputHandler.output(""Extended Verify requested, proceeding to inspect values"");
+
+        try (VerifyController verifyController = new VerifyController(cfs);
+             KeyReader indexIterator = sstable.keyReader())
+        {
+            if (indexIterator.dataPosition() != 0)
+                markAndThrow(new RuntimeException(""First row position from index != 0: "" + indexIterator.dataPosition()));
+
+            List<Range<Token>> ownedRanges = isOffline ? Collections.emptyList() : Range.normalize(tokenLookup.apply(cfs.metadata().keyspace));
+            RangeOwnHelper rangeOwnHelper = new RangeOwnHelper(ownedRanges);
+            DecoratedKey prevKey = null;
+
+            while (!dataFile.isEOF())
+            {
+
+                if (verifyInfo.isStopRequested())","[{'comment': 'nit: line 97 to 125 duplicated in `BigTableVerifier#verifySSTable()`', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/BtiTableVerifier.java,"@@ -0,0 +1,207 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+import java.util.List;
+import java.util.Objects;
+
+import com.google.common.base.Throwables;
+
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.compaction.CompactionInterruptedException;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.dht.LocalPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.sstable.IVerifier;
+import org.apache.cassandra.io.sstable.KeyReader;
+import org.apache.cassandra.io.sstable.SSTableIdentityIterator;
+import org.apache.cassandra.io.sstable.format.SSTableReader;
+import org.apache.cassandra.io.sstable.format.SortedTableVerifier;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.OutputHandler;
+
+public class BtiTableVerifier extends SortedTableVerifier<BtiTableReader> implements IVerifier
+{
+    public BtiTableVerifier(ColumnFamilyStore cfs, BtiTableReader sstable, OutputHandler outputHandler, boolean isOffline, Options options)
+    {
+        super(cfs, sstable, outputHandler, isOffline, options);
+    }
+
+    public void verify()
+    {
+        verifySSTableVersion();
+
+        verifySSTableMetadata();
+
+        verifyIndex();
+
+        verifyBloomFilter();
+
+        if (options.checkOwnsTokens && !isOffline && !(cfs.getPartitioner() instanceof LocalPartitioner))
+        {
+            if (verifyOwnedRanges() == 0)
+                return;
+        }
+
+        if (options.quick)
+            return;
+
+        if (verifyDigest() && !options.extendedVerification)
+            return;
+
+        verifySSTable();
+
+        outputHandler.output(""Verify of %s succeeded. All %d rows read successfully"", sstable, goodRows);
+    }
+
+    private void verifySSTable()
+    {
+        long rowStart;
+        outputHandler.output(""Extended Verify requested, proceeding to inspect values"");
+
+        try (VerifyController verifyController = new VerifyController(cfs);
+             KeyReader indexIterator = sstable.keyReader())
+        {
+            if (indexIterator.dataPosition() != 0)
+                markAndThrow(new RuntimeException(""First row position from index != 0: "" + indexIterator.dataPosition()));
+
+            List<Range<Token>> ownedRanges = isOffline ? Collections.emptyList() : Range.normalize(tokenLookup.apply(cfs.metadata().keyspace));
+            RangeOwnHelper rangeOwnHelper = new RangeOwnHelper(ownedRanges);
+            DecoratedKey prevKey = null;
+
+            while (!dataFile.isEOF())
+            {
+
+                if (verifyInfo.isStopRequested())
+                    throw new CompactionInterruptedException(verifyInfo.getCompactionInfo());
+
+                rowStart = dataFile.getFilePointer();
+                outputHandler.debug(""Reading row at %d"", rowStart);
+
+                DecoratedKey key = null;
+                try
+                {
+                    key = sstable.decorateKey(ByteBufferUtil.readWithShortLength(dataFile));
+                }
+                catch (Throwable th)
+                {
+                    throwIfFatal(th);
+                    // check for null key below
+                }
+
+                if (options.checkOwnsTokens && ownedRanges.size() > 0 && !(cfs.getPartitioner() instanceof LocalPartitioner))
+                {
+                    try
+                    {
+                        rangeOwnHelper.validate(key);
+                    }
+                    catch (Throwable t)
+                    {
+                        outputHandler.warn(t, ""Key %s in sstable %s not owned by local ranges %s"", key, sstable, ownedRanges);
+                        markAndThrow(t);
+                    }
+                }
+
+                ByteBuffer currentIndexKey = indexIterator.key();
+                long nextRowPositionFromIndex = 0;
+                try
+                {
+                    nextRowPositionFromIndex = indexIterator.advance()
+                                               ? indexIterator.dataPosition()
+                                               : dataFile.length();
+                }
+                catch (Throwable th)
+                {
+                    markAndThrow(th);
+                }
+
+                long dataStart = dataFile.getFilePointer();
+                long dataStartFromIndex = currentIndexKey == null
+                                          ? -1
+                                          : rowStart + 2 + currentIndexKey.remaining();
+
+                long dataSize = nextRowPositionFromIndex - dataStartFromIndex;
+                // avoid an NPE if key is null
+                String keyName = key == null ? ""(unreadable key)"" : ByteBufferUtil.bytesToHex(key.getKey());
+                outputHandler.debug(""row %s is %s"", keyName, FBUtilities.prettyPrintMemory(dataSize));
+
+                try
+                {
+                    if (key == null || dataSize > dataFile.length())
+                        markAndThrow(new RuntimeException(String.format(""key = %s, dataSize=%d, dataFile.length() = %d"", key, dataSize, dataFile.length())));
+
+                    //mimic the scrub read path, intentionally unused
+                    try (UnfilteredRowIterator ignored = SSTableIdentityIterator.create(sstable, dataFile, key)) {
+                        // no-op, just open and close
+                    }
+
+                    if ((prevKey != null && prevKey.compareTo(key) > 0) || !key.getKey().equals(currentIndexKey) || dataStart != dataStartFromIndex)
+                        markAndThrow(new RuntimeException(""Key out of order: previous = "" + prevKey + "" : current = "" + key));
+
+                    goodRows++;
+                    prevKey = key;
+
+
+                    outputHandler.debug(""Row %s at %s valid, moving to next row at %s "", goodRows, rowStart, nextRowPositionFromIndex);
+                    dataFile.seek(nextRowPositionFromIndex);
+                }
+                catch (Throwable th)
+                {
+                    markAndThrow(th);
+                }
+            }
+        }
+        catch (Throwable t)
+        {
+            throw Throwables.propagate(t);","[{'comment': 'nit: I think we removed deprecated usages of `propagate()` in CASSANDRA-14218, FWIW', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/SSTableIterator.java,"@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.Slice;
+import org.apache.cassandra.db.Slices;
+import org.apache.cassandra.db.filter.ColumnFilter;
+import org.apache.cassandra.io.sstable.AbstractRowIndexEntry;
+import org.apache.cassandra.io.sstable.AbstractSSTableIterator;
+import org.apache.cassandra.io.sstable.format.bti.RowIndexReader.IndexInfo;
+import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.io.util.FileHandle;
+
+/**
+ *  Unfiltered row iterator over a BTI SSTable.
+ */
+class SSTableIterator extends AbstractSSTableIterator<AbstractRowIndexEntry>
+{
+    /**
+     * The index of the slice being processed.
+     */
+    private int slice;
+
+    public SSTableIterator(BtiTableReader sstable,
+                           FileDataInput file,
+                           DecoratedKey key,
+                           AbstractRowIndexEntry indexEntry,
+                           Slices slices,
+                           ColumnFilter columns,
+                           FileHandle ifile)
+    {
+        super(sstable, file, key, indexEntry, slices, columns, ifile);
+    }
+
+    protected Reader createReaderInternal(AbstractRowIndexEntry indexEntry, FileDataInput file, boolean shouldCloseFile)
+    {
+        if (indexEntry.isIndexed())
+            return new ForwardIndexedReader(indexEntry, file, shouldCloseFile);
+        else
+            return new ForwardReader(file, shouldCloseFile);
+    }
+
+    protected int nextSliceIndex()
+    {
+        int next = slice;
+        slice++;
+        return next;
+    }
+
+    protected boolean hasMoreSlices()
+    {
+        return slice < slices.size();
+    }
+
+    public boolean isReverseOrder()
+    {
+        return false;
+    }
+
+    private class ForwardIndexedReader extends ForwardReader
+    {
+        private final RowIndexReader indexReader;
+        long basePosition;","[{'comment': 'nit: can be `final`', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/SSTableReversedIterator.java,"@@ -0,0 +1,293 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.util.NoSuchElementException;
+
+import com.carrotsearch.hppc.LongStack;
+import org.apache.cassandra.db.ClusteringBound;
+import org.apache.cassandra.db.ClusteringComparator;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.Slice;
+import org.apache.cassandra.db.Slices;
+import org.apache.cassandra.db.UnfilteredValidation;
+import org.apache.cassandra.db.filter.ColumnFilter;
+import org.apache.cassandra.db.rows.RangeTombstoneBoundMarker;
+import org.apache.cassandra.db.rows.RangeTombstoneMarker;
+import org.apache.cassandra.db.rows.Unfiltered;
+import org.apache.cassandra.io.sstable.AbstractRowIndexEntry;
+import org.apache.cassandra.io.sstable.AbstractSSTableIterator;
+import org.apache.cassandra.io.sstable.format.bti.RowIndexReader.IndexInfo;
+import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.io.util.FileHandle;
+
+/**
+ * Unfiltered row iterator over a BTI SSTable that returns rows in reverse order.
+ */
+class SSTableReversedIterator extends AbstractSSTableIterator<TrieIndexEntry>
+{
+    /**
+     * The index of the slice being processed.
+     */
+    private int slice;
+
+    public SSTableReversedIterator(BtiTableReader sstable,
+                                   FileDataInput file,
+                                   DecoratedKey key,
+                                   TrieIndexEntry indexEntry,
+                                   Slices slices,
+                                   ColumnFilter columns,
+                                   FileHandle ifile)
+    {
+        super(sstable, file, key, indexEntry, slices, columns, ifile);
+    }
+
+    protected Reader createReaderInternal(TrieIndexEntry indexEntry, FileDataInput file, boolean shouldCloseFile)
+    {
+        if (indexEntry.isIndexed())
+            return new ReverseIndexedReader(indexEntry, file, shouldCloseFile);
+        else
+            return new ReverseReader(file, shouldCloseFile);
+    }
+
+    public boolean isReverseOrder()
+    {
+        return true;
+    }
+
+    protected int nextSliceIndex()
+    {
+        int next = slice;
+        slice++;
+        return slices.size() - (next + 1);
+    }
+
+    protected boolean hasMoreSlices()
+    {
+        return slice < slices.size();
+    }
+
+    /**
+     * Reverse iteration is performed by going through an index block (or the whole partition if not indexed) forwards
+     * and storing the positions of each entry that falls within the slice in a stack. Reverse iteration then pops out
+     * positions and reads the entries.
+     * <p>
+     * Note: The earlier version of this was constructing an in-memory view of the block instead, which gives better
+     * performance on bigger queries and index blocks (due to not having to read disk again). With the lower
+     * granularity of the tries it makes better sense to store as little as possible as the beginning of the block
+     * should very rarely be in other page/chunk cache locations. This has the benefit of being able to answer small
+     * queries (esp. LIMIT 1) faster and with less GC churn.
+     */
+    private class ReverseReader extends AbstractReader
+    {
+        LongStack rowOffsets = new LongStack();","[{'comment': '```suggestion\r\n        final LongStack rowOffsets = new LongStack();\r\n```', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/SSTableReversedIterator.java,"@@ -0,0 +1,293 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.util.NoSuchElementException;
+
+import com.carrotsearch.hppc.LongStack;
+import org.apache.cassandra.db.ClusteringBound;
+import org.apache.cassandra.db.ClusteringComparator;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.Slice;
+import org.apache.cassandra.db.Slices;
+import org.apache.cassandra.db.UnfilteredValidation;
+import org.apache.cassandra.db.filter.ColumnFilter;
+import org.apache.cassandra.db.rows.RangeTombstoneBoundMarker;
+import org.apache.cassandra.db.rows.RangeTombstoneMarker;
+import org.apache.cassandra.db.rows.Unfiltered;
+import org.apache.cassandra.io.sstable.AbstractRowIndexEntry;
+import org.apache.cassandra.io.sstable.AbstractSSTableIterator;
+import org.apache.cassandra.io.sstable.format.bti.RowIndexReader.IndexInfo;
+import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.io.util.FileHandle;
+
+/**
+ * Unfiltered row iterator over a BTI SSTable that returns rows in reverse order.
+ */
+class SSTableReversedIterator extends AbstractSSTableIterator<TrieIndexEntry>
+{
+    /**
+     * The index of the slice being processed.
+     */
+    private int slice;
+
+    public SSTableReversedIterator(BtiTableReader sstable,
+                                   FileDataInput file,
+                                   DecoratedKey key,
+                                   TrieIndexEntry indexEntry,
+                                   Slices slices,
+                                   ColumnFilter columns,
+                                   FileHandle ifile)
+    {
+        super(sstable, file, key, indexEntry, slices, columns, ifile);
+    }
+
+    protected Reader createReaderInternal(TrieIndexEntry indexEntry, FileDataInput file, boolean shouldCloseFile)
+    {
+        if (indexEntry.isIndexed())
+            return new ReverseIndexedReader(indexEntry, file, shouldCloseFile);
+        else
+            return new ReverseReader(file, shouldCloseFile);
+    }
+
+    public boolean isReverseOrder()
+    {
+        return true;
+    }
+
+    protected int nextSliceIndex()
+    {
+        int next = slice;
+        slice++;
+        return slices.size() - (next + 1);
+    }
+
+    protected boolean hasMoreSlices()
+    {
+        return slice < slices.size();
+    }
+
+    /**
+     * Reverse iteration is performed by going through an index block (or the whole partition if not indexed) forwards
+     * and storing the positions of each entry that falls within the slice in a stack. Reverse iteration then pops out
+     * positions and reads the entries.
+     * <p>
+     * Note: The earlier version of this was constructing an in-memory view of the block instead, which gives better
+     * performance on bigger queries and index blocks (due to not having to read disk again). With the lower
+     * granularity of the tries it makes better sense to store as little as possible as the beginning of the block
+     * should very rarely be in other page/chunk cache locations. This has the benefit of being able to answer small
+     * queries (esp. LIMIT 1) faster and with less GC churn.
+     */
+    private class ReverseReader extends AbstractReader
+    {
+        LongStack rowOffsets = new LongStack();
+        RangeTombstoneMarker blockOpenMarker, blockCloseMarker;
+        Unfiltered next = null;
+        boolean foundLessThan;
+        long startPos = -1;
+
+        private ReverseReader(FileDataInput file, boolean shouldCloseFile)
+        {
+            super(file, shouldCloseFile);
+        }
+
+        public void setForSlice(Slice slice) throws IOException
+        {
+            // read full row and filter
+            if (startPos == -1)
+                startPos = file.getFilePointer();
+            else
+                seekToPosition(startPos);
+
+            fillOffsets(slice, true, true, Long.MAX_VALUE);
+        }
+
+        protected boolean hasNextInternal() throws IOException
+        {
+            if (next != null)
+                return true;
+            next = computeNext();
+            return next != null;
+        }
+
+        protected Unfiltered nextInternal() throws IOException
+        {
+            if (!hasNextInternal())
+                throw new NoSuchElementException();
+
+            Unfiltered toReturn = next;
+            next = null;
+            return toReturn;
+        }
+
+        private Unfiltered computeNext() throws IOException
+        {
+            Unfiltered toReturn;
+            do
+            {
+                if (blockCloseMarker != null)
+                {
+                    toReturn = blockCloseMarker;
+                    blockCloseMarker = null;
+                    return toReturn;
+                }
+                while (!rowOffsets.isEmpty())
+                {
+                    seekToPosition(rowOffsets.pop());
+                    boolean hasNext = deserializer.hasNext();
+                    assert hasNext;
+                    toReturn = deserializer.readNext();
+                    UnfilteredValidation.maybeValidateUnfiltered(toReturn, metadata(), key, sstable);
+                    // We may get empty row for the same reason expressed on UnfilteredSerializer.deserializeOne.
+                    if (!toReturn.isEmpty())
+                        return toReturn;
+                }
+            }
+            while (!foundLessThan && advanceIndexBlock());
+
+            // open marker to be output only as slice is finished
+            if (blockOpenMarker != null)
+            {
+                toReturn = blockOpenMarker;
+                blockOpenMarker = null;
+                return toReturn;
+            }
+            return null;
+        }
+
+        protected boolean advanceIndexBlock() throws IOException
+        {
+            return false;
+        }
+
+        void fillOffsets(Slice slice, boolean filterStart, boolean filterEnd, long stopPosition) throws IOException
+        {
+            filterStart &= !slice.start().equals(ClusteringBound.BOTTOM);
+            filterEnd &= !slice.end().equals(ClusteringBound.TOP);
+            long currentPosition = -1;
+
+            ClusteringBound start = slice.start();
+            currentPosition = file.getFilePointer();
+            foundLessThan = false;
+            // This is a copy of handlePreSliceData which also checks currentPosition < stopPosition.
+            // Not extracted to method as we need both marker and currentPosition.
+            if (filterStart)
+            {
+                while (currentPosition < stopPosition && deserializer.hasNext() && deserializer.compareNextTo(start) <= 0)
+                {
+                    if (deserializer.nextIsRow())
+                        deserializer.skipNext();
+                    else
+                        updateOpenMarker((RangeTombstoneMarker) deserializer.readNext());
+
+                    currentPosition = file.getFilePointer();
+                    foundLessThan = true;
+                }
+            }
+
+            // We've reached the beginning of our queried slice. If we have an open marker
+            // we should return that at the end of the slice to close the deletion.
+            if (openMarker != null)
+                blockOpenMarker = new RangeTombstoneBoundMarker(start, openMarker);
+
+
+            // Now deserialize everything until we reach our requested end (if we have one)
+            // See SSTableIterator.ForwardRead.computeNext() for why this is a strict inequality below: this is the same
+            // reasoning here.
+            while (currentPosition < stopPosition && deserializer.hasNext()
+                   && (!filterEnd || deserializer.compareNextTo(slice.end()) < 0))
+            {
+                rowOffsets.push(currentPosition);
+                if (deserializer.nextIsRow())
+                    deserializer.skipNext();
+                else
+                    updateOpenMarker((RangeTombstoneMarker) deserializer.readNext());
+
+                currentPosition = file.getFilePointer();
+            }
+
+            // If we have an open marker, we should output that first, unless end is not being filtered
+            // (i.e. it's either top (where a marker can't be open) or we placed that marker during previous block).
+            if (openMarker != null && filterEnd)
+            {
+                // If we have no end and still an openMarker, this means we're indexed and the marker is closed in a following block.
+                blockCloseMarker = new RangeTombstoneBoundMarker(slice.end(), openMarker);
+                openMarker = null;
+            }
+        }
+    }
+
+    private class ReverseIndexedReader extends ReverseReader
+    {
+        private RowIndexReverseIterator indexReader;
+        final TrieIndexEntry indexEntry;
+        long basePosition;","[{'comment': '```suggestion\r\n        final long basePosition;\r\n```', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/BtiTableReaderLoadingBuilder.java,"@@ -0,0 +1,242 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.compress.CompressionMetadata;
+import org.apache.cassandra.io.sstable.KeyReader;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.format.CompressionInfoComponent;
+import org.apache.cassandra.io.sstable.format.FilterComponent;
+import org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder;
+import org.apache.cassandra.io.sstable.format.StatsComponent;
+import org.apache.cassandra.io.sstable.format.bti.BtiFormat.Components;
+import org.apache.cassandra.io.sstable.metadata.MetadataType;
+import org.apache.cassandra.io.sstable.metadata.StatsMetadata;
+import org.apache.cassandra.io.sstable.metadata.ValidationMetadata;
+import org.apache.cassandra.io.util.FileHandle;
+import org.apache.cassandra.metrics.TableMetrics;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.FilterFactory;
+import org.apache.cassandra.utils.IFilter;
+import org.apache.cassandra.utils.Throwables;
+
+import static com.google.common.base.Preconditions.checkNotNull;
+
+public class BtiTableReaderLoadingBuilder extends SSTableReaderLoadingBuilder<BtiTableReader, BtiTableReader.Builder>
+{
+    private final static Logger logger = LoggerFactory.getLogger(BtiTableReaderLoadingBuilder.class);
+
+    private FileHandle.Builder dataFileBuilder;
+    private FileHandle.Builder partitionIndexFileBuilder;
+    private FileHandle.Builder rowIndexFileBuilder;
+
+    public BtiTableReaderLoadingBuilder(SSTable.Builder<?, ?> builder)
+    {
+        super(builder);
+    }
+
+    @Override
+    public KeyReader buildKeyReader(TableMetrics tableMetrics) throws IOException
+    {
+        StatsComponent statsComponent = StatsComponent.load(descriptor, MetadataType.STATS, MetadataType.HEADER, MetadataType.VALIDATION);
+        return createKeyReader(statsComponent.statsMetadata());
+    }
+
+    private KeyReader createKeyReader(StatsMetadata statsMetadata) throws IOException
+    {
+        checkNotNull(statsMetadata);
+
+        try (PartitionIndex index = PartitionIndex.load(partitionIndexFileBuilder(), tableMetadataRef.getLocal().partitioner, false);
+             CompressionMetadata compressionMetadata = CompressionInfoComponent.maybeLoad(descriptor, components);
+             FileHandle dFile = dataFileBuilder(statsMetadata).withCompressionMetadata(compressionMetadata).complete();
+             FileHandle riFile = rowIndexFileBuilder().complete())
+        {
+            return PartitionIterator.create(index,
+                                            tableMetadataRef.getLocal().partitioner,
+                                            riFile,
+                                            dFile);
+        }
+    }
+
+    @Override
+    protected void openComponents(BtiTableReader.Builder builder, SSTable.Owner owner, boolean validate, boolean online) throws IOException
+    {
+        try
+        {
+            StatsComponent statsComponent = StatsComponent.load(descriptor, MetadataType.STATS, MetadataType.VALIDATION, MetadataType.HEADER);
+            builder.setSerializationHeader(statsComponent.serializationHeader(builder.getTableMetadataRef().getLocal()));
+            assert !online || builder.getSerializationHeader() != null;
+
+            builder.setStatsMetadata(statsComponent.statsMetadata());
+            ValidationMetadata validationMetadata = statsComponent.validationMetadata();
+            validatePartitioner(builder.getTableMetadataRef().getLocal(), validationMetadata);
+
+            boolean filterNeeded = online;
+            if (filterNeeded)
+                builder.setFilter(loadFilter(validationMetadata));
+            boolean rebuildFilter = filterNeeded && builder.getFilter() == null;
+
+            if (descriptor.version.hasKeyRange())
+            {
+                IPartitioner partitioner = tableMetadataRef.getLocal().partitioner;
+                builder.setFirst(partitioner.decorateKey(builder.getStatsMetadata().firstKey));
+                builder.setLast(partitioner.decorateKey(builder.getStatsMetadata().lastKey));
+            }
+
+            if (builder.getComponents().contains(Components.PARTITION_INDEX) && builder.getComponents().contains(Components.ROW_INDEX) && rebuildFilter)
+            {
+                @SuppressWarnings(""resource"")","[{'comment': '```suggestion\r\n                @SuppressWarnings({ ""resource"", ""RedundantSuppression"" })\r\n```\r\nnit: We\'ve started doing this in a few places to remove noise for people using IDEA, FWIW', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/PartitionIndexBuilder.java,"@@ -0,0 +1,239 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.util.function.Consumer;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.io.tries.IncrementalTrieWriter;
+import org.apache.cassandra.io.tries.Walker;
+import org.apache.cassandra.io.util.FileHandle;
+import org.apache.cassandra.io.util.SequentialWriter;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.bytecomparable.ByteComparable;
+
+/**
+ * Partition index builder: stores index or data positions in an incrementally built, page aware on-disk trie.
+ *
+ * Not to be used outside of package. Public only for IndexRewriter tool.
+ */
+public class PartitionIndexBuilder implements AutoCloseable
+{
+    private final SequentialWriter writer;
+    private final IncrementalTrieWriter<PartitionIndex.Payload> trieWriter;
+    private final FileHandle.Builder fhBuilder;
+
+    // the last synced data file position
+    private long dataSyncPosition;
+    // the last synced row index file position
+    private long rowIndexSyncPosition;
+    // the last synced partition index file position
+    private long partitionIndexSyncPosition;
+
+    // Partial index can only be used after all three files have been synced to the required positions.
+    private long partialIndexDataEnd;
+    private long partialIndexRowEnd;
+    private long partialIndexPartitionEnd;
+    private IncrementalTrieWriter.PartialTail partialIndexTail;
+    private Consumer<PartitionIndex> partialIndexConsumer;
+    private DecoratedKey partialIndexLastKey;
+
+    private int lastDiffPoint;
+    private DecoratedKey firstKey;
+    private DecoratedKey lastKey;
+    private DecoratedKey lastWrittenKey;
+    private PartitionIndex.Payload lastPayload;
+
+    public PartitionIndexBuilder(SequentialWriter writer, FileHandle.Builder fhBuilder)
+    {
+        this.writer = writer;
+        this.trieWriter = IncrementalTrieWriter.open(PartitionIndex.TRIE_SERIALIZER, writer);
+        this.fhBuilder = fhBuilder;
+    }
+
+    /*
+     * Called when partition index has been flushed to the given position.
+     * If this makes all required positions for a partial view flushed, this will call the partialIndexConsumer.
+     */
+    public void markPartitionIndexSynced(long upToPosition)
+    {
+        partitionIndexSyncPosition = upToPosition;
+        refreshReadableBoundary();
+    }
+
+    /*
+     * Called when row index has been flushed to the given position.
+     * If this makes all required positions for a partial view flushed, this will call the partialIndexConsumer.
+     */
+    public void markRowIndexSynced(long upToPosition)
+    {
+        rowIndexSyncPosition = upToPosition;
+        refreshReadableBoundary();
+    }
+
+    /*
+     * Called when data file has been flushed to the given position.
+     * If this makes all required positions for a partial view flushed, this will call the partialIndexConsumer.
+     */
+    public void markDataSynced(long upToPosition)
+    {
+        dataSyncPosition = upToPosition;
+        refreshReadableBoundary();
+    }
+
+    private void refreshReadableBoundary()
+    {
+        if (partialIndexConsumer == null)
+            return;
+        if (dataSyncPosition < partialIndexDataEnd)
+            return;
+        if (rowIndexSyncPosition < partialIndexRowEnd)
+            return;
+        if (partitionIndexSyncPosition < partialIndexPartitionEnd)
+            return;
+
+        try (FileHandle fh = fhBuilder.withLengthOverride(writer.getLastFlushOffset()).complete())
+        {
+            @SuppressWarnings(""resource"")","[{'comment': '```suggestion\r\n            @SuppressWarnings({ ""resource"", ""RedundantSuppression"" })\r\n```', 'commenter': 'maedhroz'}]"
2267,src/java/org/apache/cassandra/io/sstable/format/bti/PartitionIterator.java,"@@ -0,0 +1,246 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable.format.bti;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.PartitionPosition;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.sstable.KeyReader;
+import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.io.util.FileHandle;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.Throwables;
+
+import static org.apache.cassandra.utils.FBUtilities.immutableListWithFilteredNulls;
+
+class PartitionIterator extends PartitionIndex.IndexPosIterator implements KeyReader
+{
+    private final PartitionIndex partitionIndex;
+    private final IPartitioner partitioner;
+    private final PartitionPosition limit;
+    private final int exclusiveLimit;
+    private final FileHandle dataFile;
+    private final FileHandle rowIndexFile;
+
+    private FileDataInput dataInput;
+    private FileDataInput indexInput;
+
+    private DecoratedKey currentKey;
+    private TrieIndexEntry currentEntry;
+    private DecoratedKey nextKey;
+    private TrieIndexEntry nextEntry;
+
+    @SuppressWarnings(""resource"")","[{'comment': '```suggestion\r\n    @SuppressWarnings({ ""resource"", ""RedundantSuppression"" })\r\n```', 'commenter': 'maedhroz'}]"
2276,test/unit/org/apache/cassandra/service/DiskFailurePolicyTest.java,"@@ -124,12 +135,29 @@ public void teardown()
     public void testPolicies()
     {
         DatabaseDescriptor.setDiskFailurePolicy(testPolicy);
-        JVMStabilityInspector.inspectThrowable(t);
-        Assert.assertEquals(expectJVMKilled, killerForTests.wasKilled());
-        Assert.assertEquals(expectJVMKilledQuiet, killerForTests.wasKilledQuietly());
-        if (!expectJVMKilled) {
+        try
+        {
+            JVMStabilityInspector.inspectThrowable(t);
+        }
+        catch (OutOfMemoryError e)","[{'comment': 'this is eventually thrown by `JVMStabilityInspector.inspectThrowable(t);` if you follow it deeply. We just need to make the difference between oom thrown as part of the test and oom thrown _while running a test_.', 'commenter': 'smiklosovic'}]"
2276,src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java,"@@ -96,6 +96,20 @@ public void handleFSError(FSError e)
         }
     }
 
+    private boolean isCausedByOutOfMemoryException(Throwable error)
+    {
+        for (Throwable t = error; t != null; t = t.getCause())
+        {
+            if (t instanceof OutOfMemoryError)
+                return true;
+            for (Throwable s : t.getSuppressed())
+                if (s instanceof OutOfMemoryError)
+                return true;","[{'comment': 'code format : the ""return true;"" should have a ""tab""  before ?\r\n`if (s instanceof OutOfMemoryError)\r\n                return true;`', 'commenter': 'Maxwell-Guo'}, {'comment': 'thanks', 'commenter': 'smiklosovic'}]"
2276,src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java,"@@ -81,7 +81,7 @@ public void handleFSError(FSError e)
 
                 // for both read and write errors mark the path as unwritable.
                 DisallowedDirectories.maybeMarkUnwritable(new File(e.path));
-                if (e instanceof FSReadError)
+                if (e instanceof FSReadError && !isCausedByOutOfMemoryException(e))","[{'comment': 'what about change the function to ""isNotCausedByOutOfMemeoryException"", so no ""!"" is needed before the function ,also the function logic should be rewrite ~~~~', 'commenter': 'Maxwell-Guo'}, {'comment': 'I rewrote the logic to be more generic.', 'commenter': 'smiklosovic'}]"
2276,src/java/org/apache/cassandra/utils/JVMStabilityInspector.java,"@@ -168,7 +168,7 @@ else if (t instanceof UnrecoverableIllegalStateException)
             inspectThrowable(t.getCause(), fn);
     }
 
-    private static final Set<String> FORCE_HEAP_OOM_IGNORE_SET = ImmutableSet.of(""Java heap space"", ""GC Overhead limit exceeded"");
+    private static final Set<String> FORCE_HEAP_OOM_IGNORE_SET = ImmutableSet.of(""Java heap space"", ""GC Overhead limit exceeded"", ""Java heap space test"");","[{'comment': 'I think ""Java heap space test"" may be not needed here, as FORCE_HEAP_OOM_IGNORE_SET is not used in the ut.\r\n', 'commenter': 'Maxwell-Guo'}, {'comment': 'No, this has to be there in order to skip the logic in the method where it is referenced.', 'commenter': 'smiklosovic'}, {'comment': ' what I mean is  such information(""Java heap space test"") will certainly not exist in the actual production environment, and the purpose of existence （""Java heap space test""） seems to be used only for ut testing. If it is purely used for testing, then why the information in ut does not directly use the existing exception message', 'commenter': 'Maxwell-Guo'}, {'comment': '@Maxwell-Guo  What if _test itself_ throws OOM exception with a message ""Java heap space""? How would you make the difference between legit OOM thrown during tests and OOM thrown as part of the test itself?', 'commenter': 'smiklosovic'}, {'comment': '@Maxwell-Guo If you have better solution, I am all ears.', 'commenter': 'smiklosovic'}, {'comment': '@Maxwell-Guo  I rewrote it so it is not there anymore.', 'commenter': 'smiklosovic'}, {'comment': 'what about using System.setProperty?\r\nat https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/JVMStabilityInspector.java#L182\r\n\r\n`        if (FORCE_HEAP_OOM_IGNORE_SET.contains(oom.getMessage()))\r\n            return;\r\n        if (System.getProperty(""oom.ut.test.message"") != null && System.getProperty(""oom.ut.test.message"").contains(oom.getMessage()))\r\n            return;\r\n        logger.error(""Force heap space OutOfMemoryError in the presence of"", oom);\r\n        // Start to produce heap space OOM forcibly.\r\n        List<long[]> ignored = new ArrayList<>();`\r\n\r\nand at https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/service/DiskFailurePolicyTest.java#L57\r\n\r\nwe can setting \r\n`  static\r\n    {\r\n        System.setProperty(""oom.ut.test.message"", ""Java heap space test"");\r\n    }\r\n`\r\nothers can be left as it is\r\n', 'commenter': 'Maxwell-Guo'}, {'comment': 'I think the current solution I just pushed is good enough. You probably posted the comment before checking it?', 'commenter': 'smiklosovic'}, {'comment': 'sorry @smiklosovic , the version when I commented was not the latest, so I have such a question. \r\n+1 on this, and thanks for this work.', 'commenter': 'Maxwell-Guo'}]"
2276,test/unit/org/apache/cassandra/service/DiskFailurePolicyTest.java,"@@ -79,22 +88,24 @@ public DiskFailurePolicyTest(DiskFailurePolicy testPolicy, boolean isStartUpInPr
     public static Collection<Object[]> generateData()
     {
         return Arrays.asList(new Object[][]{
-                             { Config.DiskFailurePolicy.die, true, new FSReadError(new IOException(), ""blah""), false, true, true},
-                             { DiskFailurePolicy.ignore, true, new FSReadError(new IOException(), ""blah""), true, false, false},
-                             { DiskFailurePolicy.stop, true, new FSReadError(new IOException(), ""blah""), false, true, true},
-                             { DiskFailurePolicy.stop_paranoid, true, new FSReadError(new IOException(), ""blah""), false, true, true},
-                             { Config.DiskFailurePolicy.die, true, new CorruptSSTableException(new IOException(), ""blah""), false, true, true},
-                             { DiskFailurePolicy.ignore, true, new CorruptSSTableException(new IOException(), ""blah""), true, false, false},
-                             { DiskFailurePolicy.stop, true, new CorruptSSTableException(new IOException(), ""blah""), false, true, true},
-                             { DiskFailurePolicy.stop_paranoid, true, new CorruptSSTableException(new IOException(), ""blah""), false, true, true},
-                             { Config.DiskFailurePolicy.die, false, new FSReadError(new IOException(), ""blah""), false, true, false},
-                             { DiskFailurePolicy.ignore, false, new FSReadError(new IOException(), ""blah""), true, false, false},
-                             { DiskFailurePolicy.stop, false, new FSReadError(new IOException(), ""blah""), false, false, false},
-                             { DiskFailurePolicy.stop_paranoid, false, new FSReadError(new IOException(), ""blah""), false, false, false},
-                             { Config.DiskFailurePolicy.die, false, new CorruptSSTableException(new IOException(), ""blah""), false, true, false},
-                             { DiskFailurePolicy.ignore, false, new CorruptSSTableException(new IOException(), ""blah""), true, false, false},
-                             { DiskFailurePolicy.stop, false, new CorruptSSTableException(new IOException(), ""blah""), true, false, false},
-                             { DiskFailurePolicy.stop_paranoid, false, new CorruptSSTableException(new IOException(), ""blah""), false, false, false}
+                             { die, true, new FSReadError(new IOException(), ""blah""), false, true, true},
+                             { ignore, true, new FSReadError(new IOException(), ""blah""), true, false, false},
+                             { stop, true, new FSReadError(new IOException(), ""blah""), false, true, true},
+                             { stop_paranoid, true, new FSReadError(new IOException(), ""blah""), false, true, true},
+                             { die, true, new CorruptSSTableException(new IOException(), ""blah""), false, true, true},
+                             { ignore, true, new CorruptSSTableException(new IOException(), ""blah""), true, false, false},
+                             { stop, true, new CorruptSSTableException(new IOException(), ""blah""), false, true, true},
+                             { stop_paranoid, true, new CorruptSSTableException(new IOException(), ""blah""), false, true, true},
+                             { die, false, new FSReadError(new IOException(), ""blah""), false, true, false},
+                             { ignore, false, new FSReadError(new IOException(), ""blah""), true, false, false},
+                             { stop, false, new FSReadError(new IOException(), ""blah""), false, false, false},
+                             { stop_paranoid, false, new FSReadError(new IOException(), ""blah""), false, false, false},
+                             { die, false, new CorruptSSTableException(new IOException(), ""blah""), false, true, false},
+                             { ignore, false, new CorruptSSTableException(new IOException(), ""blah""), true, false, false},
+                             { stop, false, new CorruptSSTableException(new IOException(), ""blah""), true, false, false},
+                             { stop_paranoid, false, new CorruptSSTableException(new IOException(), ""blah""), false, false, false},
+                             { best_effort, false, new FSReadError(new IOException(new OutOfMemoryError(""Java heap space test"")), ""best_effort_oom""), true, false, false},","[{'comment': 'as ""Java heap space test"" removed in FORCE_HEAP_OOM_IGNORE_SET , so I think some other like ""Java heap space"" can be used here. I don\'t know if this is the right suggestion.', 'commenter': 'Maxwell-Guo'}, {'comment': 'see above', 'commenter': 'smiklosovic'}]"
2295,src/java/org/apache/cassandra/config/Config.java,"@@ -359,9 +352,14 @@ public MemtableOptions()
 
     public String[] data_file_directories = new String[0];
 
-    public List<ParameterizedClass> sstable_formats = ImmutableList.of(new ParameterizedClass(BigFormat.class.getName(),// ""org.apache.cassandra.io.sstable.format.big.BigFormat"",
-                                                                                              ImmutableMap.of(SSTABLE_FORMAT_ID, ""0"",
-                                                                                                              SSTABLE_FORMAT_NAME, ""big"")));
+    public static class SSTableFormats
+    {
+        public String selected_format;
+        public String selected_version;
+        public Map<String, Map<String, String>> options = new HashMap<>();
+    }
+
+    public SSTableFormats sstable_formats = new SSTableFormats();","[{'comment': 'nit: can use `final` here', 'commenter': 'dcapwell'}, {'comment': ""This doesn't seem to match the yaml format talked about in JIRA?\r\n\r\n```\r\n  sstable:\r\n    format:\r\n      default: bti\r\n      bti:\r\n        row_index_granularity: 4KiB\r\n```"", 'commenter': 'dcapwell'}, {'comment': 'fixed', 'commenter': 'jacek-lewandowski'}]"
2295,src/java/org/apache/cassandra/config/Config.java,"@@ -359,9 +352,14 @@ public MemtableOptions()
 
     public String[] data_file_directories = new String[0];
 
-    public List<ParameterizedClass> sstable_formats = ImmutableList.of(new ParameterizedClass(BigFormat.class.getName(),// ""org.apache.cassandra.io.sstable.format.big.BigFormat"",
-                                                                                              ImmutableMap.of(SSTABLE_FORMAT_ID, ""0"",
-                                                                                                              SSTABLE_FORMAT_NAME, ""big"")));
+    public static class SSTableFormats
+    {
+        public String selected_format;
+        public String selected_version;","[{'comment': 'I honestly keep debating this personally... should this be in `options`?', 'commenter': 'dcapwell'}, {'comment': 'my thinking is, if we do want to later (not this patch) allow overriding via JMX or Settings table, how can you do thing safely?\r\n\r\nLets say we have the following cases\r\n\r\n1) keep same format, just change version (use case; major/minor upgrade wants to delay using latest until ""stable"")\r\n\r\n```\r\nsstable_formats.selected_version = next // this is ""safe""\r\n```\r\n\r\n2) switch from big to bti or some 3rd party format\r\n\r\n```\r\nsstable_formats.selected_version = bti_version // unsafe, breaks BIG\r\nsstable_formats.selected_format = bti\r\n```\r\n\r\nor\r\n\r\n```\r\nsstable_formats.selected_format = bti // unsafe as ""selected_version"" still points to BIG version\r\nsstable_formats.selected_version = bti_version\r\n```', 'commenter': 'dcapwell'}, {'comment': 'Where would we express the expected target format and version then? If we ever go for changing that at runtime, this should always be changed together. \r\n', 'commenter': 'jacek-lewandowski'}, {'comment': ""> this should always be changed together\r\n\r\nagree, but we would need locking or copy-on-write configs to support that, which we don't have in this patch (and not arguing we add).  This means a config change happens each time a field is modified, which could put us in a inconsistence state waiting for the operator to update the other fields.\r\n\r\nSo, by having the version in `options` this at least scopes things to where they make sense... if you are using `big` and want to test out `bti v42` you can update the configs for `bti.version=42`, this would have no runtime impact as its untouched.  But let's say you are using `bti v41` and want to switch to `v42`?  you then do `bit.version=42` and the next sstable uses `v42`.  By having the version in the options it makes the concurrency easier to reason about"", 'commenter': 'dcapwell'}]"
2295,src/java/org/apache/cassandra/config/YamlConfigurationLoader.java,"@@ -203,6 +202,8 @@ private static void verifyReplacements(Map<Class<?>, Map<String, Replacement>> r
         Yaml rawYaml = new Yaml(loaderOptions);
 
         Map<String, Object> rawConfig = rawYaml.load(new ByteArrayInputStream(configBytes));
+        if (rawConfig == null)","[{'comment': 'What case caused this?', 'commenter': 'dcapwell'}, {'comment': 'Just for testing - wanted to make it possible to pass an empty yaml to assert the defaults. I think this is a valid situation - whole yaml should be optional.', 'commenter': 'jacek-lewandowski'}]"
2295,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1349,68 +1359,97 @@ public static void applyPartitioner(Config conf)
         paritionerName = partitioner.getClass().getCanonicalName();
     }
 
-    @VisibleForTesting
-    public static Map<String, Supplier<SSTableFormat<?, ?>>> loadSSTableFormatFactories(List<ParameterizedClass> configuredFormats)
+    private static void validateSSTableFormatFactories(Iterable<SSTableFormat.Factory> factories)
     {
-        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> sstableFormatFactories = ImmutableMap.builderWithExpectedSize(configuredFormats.size());
-        Set<String> names = new HashSet<>(configuredFormats.size());
-        Set<Integer> ids = new HashSet<>(configuredFormats.size());
+        Map<String, SSTableFormat.Factory> factoryByName = new HashMap<>();
+        for (SSTableFormat.Factory factory : factories)
+        {
+            if (factory.name() == null)
+                throw new ConfigurationException(String.format(""SSTable format name in %s cannot be null"", factory.getClass().getName()));
+
+            if (!factory.name().matches(""^[a-z]+$""))
+                throw new ConfigurationException(String.format(""SSTable format name for %s must be non-empty, lower-case letters only string"", factory.getClass().getName()));
 
-        for (ParameterizedClass formatConfig : configuredFormats)
+            SSTableFormat.Factory prev = factoryByName.put(factory.name(), factory);
+            if (prev != null)
+                throw new ConfigurationException(String.format(""Multiple sstable format implementations with the same name %s: %s and %s"", factory.name(), factory.getClass().getName(), prev.getClass().getName()));
+        }
+    }
+
+    private static ImmutableMap<String, Supplier<SSTableFormat<?, ?>>> validateAndMatchSSTableFormatOptions(Iterable<SSTableFormat.Factory> factories, Map<String, Map<String, String>> options)
+    {
+        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> providersBuilder = ImmutableMap.builder();
+        for (SSTableFormat.Factory factory : factories)
         {
-            assert formatConfig.parameters != null;
-            Map<String, String> params = new HashMap<>(formatConfig.parameters);
-
-            String name = params.get(Config.SSTABLE_FORMAT_NAME);
-            if (name == null)
-                throw new ConfigurationException(""Missing 'name' parameter in sstable format configuration for "" + formatConfig.class_name);
-            if (!name.matches(""^[a-z]+$""))
-                throw new ConfigurationException(""'name' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be non-empty, lower-case letters only string"");
-            if (names.contains(name))
-                throw new ConfigurationException(""Name '"" + name + ""' of sstable format "" + formatConfig.class_name + "" is already defined for another sstable format"");
-            params.remove(Config.SSTABLE_FORMAT_NAME);
-
-            String idString = params.get(Config.SSTABLE_FORMAT_ID);
-            if (idString == null)
-                throw new ConfigurationException(""Missing 'id' parameter in sstable format configuration for "" + formatConfig.class_name);
-            int id;
-            try
-            {
-                id = Integer.parseInt(idString);
-            }
-            catch (RuntimeException ex)
-            {
-                throw new ConfigurationException(""'id' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be an integer"");
-            }
-            if (id < 0 || id > 127)
-                throw new ConfigurationException(""'id' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be within bounds [0..127] range"");
-            if (ids.contains(id))
-                throw new ConfigurationException(""ID '"" + id + ""' of sstable format "" + formatConfig.class_name + "" is already defined for another sstable format"");
-            params.remove(Config.SSTABLE_FORMAT_ID);
-
-            Supplier<SSTableFormat<?, ?>> factory = () -> {
-                Class<SSTableFormat<?, ?>> cls = FBUtilities.classForName(formatConfig.class_name, ""sstable format"");
-                if (!SSTableFormat.class.isAssignableFrom(cls))
-                    throw new ConfigurationException(String.format(""Class %s for sstable format %s does not implement %s"", formatConfig.class_name, name, SSTableFormat.class.getName()));
-
-                SSTableFormat<?, ?> sstableFormat = FBUtilities.instanceOrConstruct(cls.getName(), ""sstable format"");
-                sstableFormat.setup(id, name, params);
-                return sstableFormat;
-            };
-            sstableFormatFactories.put(name, factory);
-            names.add(name);
-            ids.add(id);
+            Map<String, String> formatOptions = options != null ? ImmutableMap.copyOf(options.getOrDefault(factory.name(), ImmutableMap.of())) : ImmutableMap.of();
+            providersBuilder.put(factory.name(), () -> factory.getInstance(formatOptions));
         }
+        ImmutableMap<String, Supplier<SSTableFormat<?, ?>>> providers = providersBuilder.build();
+        if (options != null)
+        {
+            Sets.SetView<String> unknownFormatNames = Sets.difference(options.keySet(), providers.keySet());
+            if (!unknownFormatNames.isEmpty())
+                throw new ConfigurationException(String.format(""Configuration contains options of unknown sstable formats: %s"", unknownFormatNames));
+        }
+        return providers;
+    }
 
-        return sstableFormatFactories.build();
+    private static Pair<SSTableFormat<?, ?>, Version> getAndValidateFormatAndVersion(Map<String, SSTableFormat<?, ?>> sstableFormats, String selectedFormatName, String selectedVersionStr)
+    {
+        SSTableFormat<?, ?> selectedFormat;
+        if (StringUtils.isBlank(selectedFormatName))
+            selectedFormatName = BigFormat.NAME;
+        selectedFormat = sstableFormats.get(selectedFormatName);
+        if (selectedFormat == null)
+            throw new ConfigurationException(String.format(""Selected sstable format '%s' is not available."", selectedFormatName));
+
+        Version selectedVersion;
+        if (StringUtils.isBlank(selectedVersionStr))
+            selectedVersion = selectedFormat.getLatestVersion();
+        else
+            selectedVersion = selectedFormat.getVersion(selectedVersionStr);
+        if (selectedVersion == null)
+            throw new ConfigurationException(String.format(""Selected sstable version '%s' is not available for the selected sstable format '%s'"", selectedVersionStr, selectedFormat.name()));
+        if (!selectedVersion.isCompatible())
+            throw new ConfigurationException(String.format(""Unsupported version '%s' for the selected sstable format '%s'"", selectedVersion, selectedFormat.name()));
+
+        return Pair.create(selectedFormat, selectedVersion);
     }
 
     private static void applySSTableFormats()
     {
-        if (sstableFormatFactories != null)
-            logger.warn(""Reinitializing SSTableFactories - this should happen only in tests"");
+        ServiceLoader<SSTableFormat.Factory> loader = ServiceLoader.load(SSTableFormat.Factory.class, DatabaseDescriptor.class.getClassLoader());
+        List<SSTableFormat.Factory> factories = Iterables.toList(loader);
+        if (factories.isEmpty())
+            applySSTableFormats(ImmutableList.of(new BigFormat.BigFormatFactory()), conf.sstable_formats);
+        else
+            applySSTableFormats(factories, conf.sstable_formats);","[{'comment': '```suggestion\r\n        if (factories.isEmpty())\r\n            factories = ImmutableList.of(new BigFormat.BigFormatFactory());\r\n        applySSTableFormats(factories, conf.sstable_formats);\r\n```', 'commenter': 'dcapwell'}]"
2295,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1349,68 +1359,97 @@ public static void applyPartitioner(Config conf)
         paritionerName = partitioner.getClass().getCanonicalName();
     }
 
-    @VisibleForTesting
-    public static Map<String, Supplier<SSTableFormat<?, ?>>> loadSSTableFormatFactories(List<ParameterizedClass> configuredFormats)
+    private static void validateSSTableFormatFactories(Iterable<SSTableFormat.Factory> factories)
     {
-        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> sstableFormatFactories = ImmutableMap.builderWithExpectedSize(configuredFormats.size());
-        Set<String> names = new HashSet<>(configuredFormats.size());
-        Set<Integer> ids = new HashSet<>(configuredFormats.size());
+        Map<String, SSTableFormat.Factory> factoryByName = new HashMap<>();
+        for (SSTableFormat.Factory factory : factories)
+        {
+            if (factory.name() == null)
+                throw new ConfigurationException(String.format(""SSTable format name in %s cannot be null"", factory.getClass().getName()));","[{'comment': '```suggestion\r\n                throw new ConfigurationException(String.format(""SSTable format name in %s cannot be null"", factory.getClass().getCanonicalName()));\r\n```', 'commenter': 'dcapwell'}]"
2295,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1349,68 +1359,97 @@ public static void applyPartitioner(Config conf)
         paritionerName = partitioner.getClass().getCanonicalName();
     }
 
-    @VisibleForTesting
-    public static Map<String, Supplier<SSTableFormat<?, ?>>> loadSSTableFormatFactories(List<ParameterizedClass> configuredFormats)
+    private static void validateSSTableFormatFactories(Iterable<SSTableFormat.Factory> factories)
     {
-        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> sstableFormatFactories = ImmutableMap.builderWithExpectedSize(configuredFormats.size());
-        Set<String> names = new HashSet<>(configuredFormats.size());
-        Set<Integer> ids = new HashSet<>(configuredFormats.size());
+        Map<String, SSTableFormat.Factory> factoryByName = new HashMap<>();
+        for (SSTableFormat.Factory factory : factories)
+        {
+            if (factory.name() == null)
+                throw new ConfigurationException(String.format(""SSTable format name in %s cannot be null"", factory.getClass().getName()));
+
+            if (!factory.name().matches(""^[a-z]+$""))
+                throw new ConfigurationException(String.format(""SSTable format name for %s must be non-empty, lower-case letters only string"", factory.getClass().getName()));","[{'comment': '```suggestion\r\n                throw new ConfigurationException(String.format(""SSTable format name for %s must be non-empty, lower-case letters only string"", factory.getClass().getCanonicalName()));\r\n```', 'commenter': 'dcapwell'}]"
2295,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1349,68 +1359,97 @@ public static void applyPartitioner(Config conf)
         paritionerName = partitioner.getClass().getCanonicalName();
     }
 
-    @VisibleForTesting
-    public static Map<String, Supplier<SSTableFormat<?, ?>>> loadSSTableFormatFactories(List<ParameterizedClass> configuredFormats)
+    private static void validateSSTableFormatFactories(Iterable<SSTableFormat.Factory> factories)
     {
-        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> sstableFormatFactories = ImmutableMap.builderWithExpectedSize(configuredFormats.size());
-        Set<String> names = new HashSet<>(configuredFormats.size());
-        Set<Integer> ids = new HashSet<>(configuredFormats.size());
+        Map<String, SSTableFormat.Factory> factoryByName = new HashMap<>();
+        for (SSTableFormat.Factory factory : factories)
+        {
+            if (factory.name() == null)
+                throw new ConfigurationException(String.format(""SSTable format name in %s cannot be null"", factory.getClass().getName()));
+
+            if (!factory.name().matches(""^[a-z]+$""))
+                throw new ConfigurationException(String.format(""SSTable format name for %s must be non-empty, lower-case letters only string"", factory.getClass().getName()));
 
-        for (ParameterizedClass formatConfig : configuredFormats)
+            SSTableFormat.Factory prev = factoryByName.put(factory.name(), factory);
+            if (prev != null)
+                throw new ConfigurationException(String.format(""Multiple sstable format implementations with the same name %s: %s and %s"", factory.name(), factory.getClass().getName(), prev.getClass().getName()));","[{'comment': '```suggestion\r\n                throw new ConfigurationException(String.format(""Multiple sstable format implementations with the same name %s: %s and %s"", factory.name(), factory.getClass().getCanonicalName(), prev.getClass().getCanonicalName()));\r\n```', 'commenter': 'dcapwell'}]"
2295,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1349,68 +1359,97 @@ public static void applyPartitioner(Config conf)
         paritionerName = partitioner.getClass().getCanonicalName();
     }
 
-    @VisibleForTesting
-    public static Map<String, Supplier<SSTableFormat<?, ?>>> loadSSTableFormatFactories(List<ParameterizedClass> configuredFormats)
+    private static void validateSSTableFormatFactories(Iterable<SSTableFormat.Factory> factories)
     {
-        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> sstableFormatFactories = ImmutableMap.builderWithExpectedSize(configuredFormats.size());
-        Set<String> names = new HashSet<>(configuredFormats.size());
-        Set<Integer> ids = new HashSet<>(configuredFormats.size());
+        Map<String, SSTableFormat.Factory> factoryByName = new HashMap<>();
+        for (SSTableFormat.Factory factory : factories)
+        {
+            if (factory.name() == null)
+                throw new ConfigurationException(String.format(""SSTable format name in %s cannot be null"", factory.getClass().getName()));
+
+            if (!factory.name().matches(""^[a-z]+$""))
+                throw new ConfigurationException(String.format(""SSTable format name for %s must be non-empty, lower-case letters only string"", factory.getClass().getName()));
 
-        for (ParameterizedClass formatConfig : configuredFormats)
+            SSTableFormat.Factory prev = factoryByName.put(factory.name(), factory);
+            if (prev != null)
+                throw new ConfigurationException(String.format(""Multiple sstable format implementations with the same name %s: %s and %s"", factory.name(), factory.getClass().getName(), prev.getClass().getName()));
+        }
+    }
+
+    private static ImmutableMap<String, Supplier<SSTableFormat<?, ?>>> validateAndMatchSSTableFormatOptions(Iterable<SSTableFormat.Factory> factories, Map<String, Map<String, String>> options)
+    {
+        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> providersBuilder = ImmutableMap.builder();
+        for (SSTableFormat.Factory factory : factories)
         {
-            assert formatConfig.parameters != null;
-            Map<String, String> params = new HashMap<>(formatConfig.parameters);
-
-            String name = params.get(Config.SSTABLE_FORMAT_NAME);
-            if (name == null)
-                throw new ConfigurationException(""Missing 'name' parameter in sstable format configuration for "" + formatConfig.class_name);
-            if (!name.matches(""^[a-z]+$""))
-                throw new ConfigurationException(""'name' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be non-empty, lower-case letters only string"");
-            if (names.contains(name))
-                throw new ConfigurationException(""Name '"" + name + ""' of sstable format "" + formatConfig.class_name + "" is already defined for another sstable format"");
-            params.remove(Config.SSTABLE_FORMAT_NAME);
-
-            String idString = params.get(Config.SSTABLE_FORMAT_ID);
-            if (idString == null)
-                throw new ConfigurationException(""Missing 'id' parameter in sstable format configuration for "" + formatConfig.class_name);
-            int id;
-            try
-            {
-                id = Integer.parseInt(idString);
-            }
-            catch (RuntimeException ex)
-            {
-                throw new ConfigurationException(""'id' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be an integer"");
-            }
-            if (id < 0 || id > 127)
-                throw new ConfigurationException(""'id' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be within bounds [0..127] range"");
-            if (ids.contains(id))
-                throw new ConfigurationException(""ID '"" + id + ""' of sstable format "" + formatConfig.class_name + "" is already defined for another sstable format"");
-            params.remove(Config.SSTABLE_FORMAT_ID);
-
-            Supplier<SSTableFormat<?, ?>> factory = () -> {
-                Class<SSTableFormat<?, ?>> cls = FBUtilities.classForName(formatConfig.class_name, ""sstable format"");
-                if (!SSTableFormat.class.isAssignableFrom(cls))
-                    throw new ConfigurationException(String.format(""Class %s for sstable format %s does not implement %s"", formatConfig.class_name, name, SSTableFormat.class.getName()));
-
-                SSTableFormat<?, ?> sstableFormat = FBUtilities.instanceOrConstruct(cls.getName(), ""sstable format"");
-                sstableFormat.setup(id, name, params);
-                return sstableFormat;
-            };
-            sstableFormatFactories.put(name, factory);
-            names.add(name);
-            ids.add(id);
+            Map<String, String> formatOptions = options != null ? ImmutableMap.copyOf(options.getOrDefault(factory.name(), ImmutableMap.of())) : ImmutableMap.of();","[{'comment': 'rather than checking `options` in the loop you can do the following outside the loop\r\n\r\n```\r\nif (options == null)\r\n  options = ImmutableMap.of();\r\n```', 'commenter': 'dcapwell'}, {'comment': 'fixed', 'commenter': 'jacek-lewandowski'}]"
2295,src/java/org/apache/cassandra/config/DatabaseDescriptor.java,"@@ -1349,68 +1359,97 @@ public static void applyPartitioner(Config conf)
         paritionerName = partitioner.getClass().getCanonicalName();
     }
 
-    @VisibleForTesting
-    public static Map<String, Supplier<SSTableFormat<?, ?>>> loadSSTableFormatFactories(List<ParameterizedClass> configuredFormats)
+    private static void validateSSTableFormatFactories(Iterable<SSTableFormat.Factory> factories)
     {
-        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> sstableFormatFactories = ImmutableMap.builderWithExpectedSize(configuredFormats.size());
-        Set<String> names = new HashSet<>(configuredFormats.size());
-        Set<Integer> ids = new HashSet<>(configuredFormats.size());
+        Map<String, SSTableFormat.Factory> factoryByName = new HashMap<>();
+        for (SSTableFormat.Factory factory : factories)
+        {
+            if (factory.name() == null)
+                throw new ConfigurationException(String.format(""SSTable format name in %s cannot be null"", factory.getClass().getName()));
+
+            if (!factory.name().matches(""^[a-z]+$""))
+                throw new ConfigurationException(String.format(""SSTable format name for %s must be non-empty, lower-case letters only string"", factory.getClass().getName()));
 
-        for (ParameterizedClass formatConfig : configuredFormats)
+            SSTableFormat.Factory prev = factoryByName.put(factory.name(), factory);
+            if (prev != null)
+                throw new ConfigurationException(String.format(""Multiple sstable format implementations with the same name %s: %s and %s"", factory.name(), factory.getClass().getName(), prev.getClass().getName()));
+        }
+    }
+
+    private static ImmutableMap<String, Supplier<SSTableFormat<?, ?>>> validateAndMatchSSTableFormatOptions(Iterable<SSTableFormat.Factory> factories, Map<String, Map<String, String>> options)
+    {
+        ImmutableMap.Builder<String, Supplier<SSTableFormat<?, ?>>> providersBuilder = ImmutableMap.builder();
+        for (SSTableFormat.Factory factory : factories)
         {
-            assert formatConfig.parameters != null;
-            Map<String, String> params = new HashMap<>(formatConfig.parameters);
-
-            String name = params.get(Config.SSTABLE_FORMAT_NAME);
-            if (name == null)
-                throw new ConfigurationException(""Missing 'name' parameter in sstable format configuration for "" + formatConfig.class_name);
-            if (!name.matches(""^[a-z]+$""))
-                throw new ConfigurationException(""'name' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be non-empty, lower-case letters only string"");
-            if (names.contains(name))
-                throw new ConfigurationException(""Name '"" + name + ""' of sstable format "" + formatConfig.class_name + "" is already defined for another sstable format"");
-            params.remove(Config.SSTABLE_FORMAT_NAME);
-
-            String idString = params.get(Config.SSTABLE_FORMAT_ID);
-            if (idString == null)
-                throw new ConfigurationException(""Missing 'id' parameter in sstable format configuration for "" + formatConfig.class_name);
-            int id;
-            try
-            {
-                id = Integer.parseInt(idString);
-            }
-            catch (RuntimeException ex)
-            {
-                throw new ConfigurationException(""'id' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be an integer"");
-            }
-            if (id < 0 || id > 127)
-                throw new ConfigurationException(""'id' parameter in sstable format configuration for "" + formatConfig.class_name + "" must be within bounds [0..127] range"");
-            if (ids.contains(id))
-                throw new ConfigurationException(""ID '"" + id + ""' of sstable format "" + formatConfig.class_name + "" is already defined for another sstable format"");
-            params.remove(Config.SSTABLE_FORMAT_ID);
-
-            Supplier<SSTableFormat<?, ?>> factory = () -> {
-                Class<SSTableFormat<?, ?>> cls = FBUtilities.classForName(formatConfig.class_name, ""sstable format"");
-                if (!SSTableFormat.class.isAssignableFrom(cls))
-                    throw new ConfigurationException(String.format(""Class %s for sstable format %s does not implement %s"", formatConfig.class_name, name, SSTableFormat.class.getName()));
-
-                SSTableFormat<?, ?> sstableFormat = FBUtilities.instanceOrConstruct(cls.getName(), ""sstable format"");
-                sstableFormat.setup(id, name, params);
-                return sstableFormat;
-            };
-            sstableFormatFactories.put(name, factory);
-            names.add(name);
-            ids.add(id);
+            Map<String, String> formatOptions = options != null ? ImmutableMap.copyOf(options.getOrDefault(factory.name(), ImmutableMap.of())) : ImmutableMap.of();
+            providersBuilder.put(factory.name(), () -> factory.getInstance(formatOptions));
         }
+        ImmutableMap<String, Supplier<SSTableFormat<?, ?>>> providers = providersBuilder.build();
+        if (options != null)
+        {
+            Sets.SetView<String> unknownFormatNames = Sets.difference(options.keySet(), providers.keySet());
+            if (!unknownFormatNames.isEmpty())
+                throw new ConfigurationException(String.format(""Configuration contains options of unknown sstable formats: %s"", unknownFormatNames));
+        }
+        return providers;
+    }
 
-        return sstableFormatFactories.build();
+    private static Pair<SSTableFormat<?, ?>, Version> getAndValidateFormatAndVersion(Map<String, SSTableFormat<?, ?>> sstableFormats, String selectedFormatName, String selectedVersionStr)
+    {
+        SSTableFormat<?, ?> selectedFormat;
+        if (StringUtils.isBlank(selectedFormatName))
+            selectedFormatName = BigFormat.NAME;","[{'comment': 'I prefer this to live in `Config` so defaults are more clear and not scattered all over the place', 'commenter': 'dcapwell'}, {'comment': 'fixed', 'commenter': 'jacek-lewandowski'}]"
2295,src/java/org/apache/cassandra/io/sstable/format/SSTableFormat.java,"@@ -113,99 +114,85 @@ IScrubber getScrubber(ColumnFamilyStore cfs,
 
     void deleteOrphanedComponents(Descriptor descriptor, Set<Component> components);
 
-    void setup(int id, String name, Map<String, String> options);
-
-    Type getType();
-
     /**
      * Deletes the existing components of the sstables represented by the provided descriptor.
      * The method is also responsible for cleaning up the in-memory resources occupied by the stuff related to that
      * sstables, such as row key cache entries.
      */
     void delete(Descriptor descriptor);
 
+    /**
+     * This class is not completely redundant","[{'comment': 'I do feel that this is redundant, its just another reference to the values in `org.apache.cassandra.config.DatabaseDescriptor#sstableFormats` and 100% depends on that...\r\n\r\n`current` -> `DD.getSelectedSSTableFormat`\r\n`getByName` -> `DD.sstableFormats.get(name)`\r\n`getByClass` -> \r\n\r\n```\r\nDatabaseDescriptor.getSSTableFormats().values().stream()\r\n                              .filter(f -> f.getClass().equals(formatClass))\r\n                              .findFirst()\r\n                              .orElseGet(() -> {\r\n                                  throw new NoSuchElementException(""Unknown sstable format class: "" + formatClass);\r\n                              });\r\n```', 'commenter': 'dcapwell'}, {'comment': 'Did I really write ""not""??? 🤦🏻\u200d♂️ I wanted to write it is redundant and that\'s why I marked it deprecated. That ""not"" is a typing mistake.\r\n\r\nThe only reason I haven\'t removed it is that the additional changeset would be quite large. So if we agree on that, we can do that.', 'commenter': 'jacek-lewandowski'}]"
2295,src/java/org/apache/cassandra/io/sstable/format/SSTableFormat.java,"@@ -54,9 +50,14 @@
  */
 public interface SSTableFormat<R extends SSTableReader, W extends SSTableWriter>
 {
-    int ordinal();
     String name();
 
+    @Deprecated
+    default Type getType()","[{'comment': 'only used for tests, and is redundant as `name` and `this.getClass()` is known by holding reference to this', 'commenter': 'dcapwell'}, {'comment': ""yes, that's why it is marked as deprecated and left only to not pollute the pull request."", 'commenter': 'jacek-lewandowski'}]"
2295,test/unit/org/apache/cassandra/config/DatabaseDescriptorRefTest.java,"@@ -198,10 +200,11 @@
     ""org.apache.cassandra.io.sstable.MetricsProviders"",
     ""org.apache.cassandra.io.sstable.SSTable"",
     ""org.apache.cassandra.io.sstable.SSTable$Builder"",
-    ""org.apache.cassandra.io.sstable.format.AbstractSSTableFormat"",
     ""org.apache.cassandra.io.sstable.format.SSTableFormat"",
     ""org.apache.cassandra.io.sstable.format.SSTableFormat$Components"",
     ""org.apache.cassandra.io.sstable.format.SSTableFormat$Components$Types"",
+    ""org.apache.cassandra.io.sstable.format.SSTableFormat$Factory"",
+    ""org.apache.cassandra.io.sstable.format.SSTableFormat$Factory"",","[{'comment': 'duplicate', 'commenter': 'dcapwell'}, {'comment': '🤦🏻\u200d♂️ ', 'commenter': 'jacek-lewandowski'}, {'comment': 'fixed', 'commenter': 'jacek-lewandowski'}]"
2295,test/unit/org/apache/cassandra/config/DatabaseDescriptorRefTest.java,"@@ -217,6 +220,8 @@
     ""org.apache.cassandra.io.sstable.format.SortedTableWriter$Builder"",
     ""org.apache.cassandra.io.sstable.format.Version"",
     ""org.apache.cassandra.io.sstable.format.big.BigFormat"",
+    ""org.apache.cassandra.io.sstable.format.big.BigFormat$BigFormatFactory"",
+    ""org.apache.cassandra.io.sstable.format.big.BigFormat$BigFormatFactory"",","[{'comment': 'duplicate', 'commenter': 'dcapwell'}, {'comment': 'fixed', 'commenter': 'jacek-lewandowski'}]"
2295,test/unit/org/apache/cassandra/schema/MockSchema.java,"@@ -170,7 +170,7 @@ public static SSTableReader sstable(int generation, int size, boolean keepRef, l
                                                sstableId(generation),
                                                format.getType());
 
-        if (format == BigFormat.getInstance())
+        if (format instanceof BigFormat)","[{'comment': 'I am ok with this, but the rest of the patch uses `org.apache.cassandra.io.sstable.format.big.BigFormat#is`, which does a `name` check', 'commenter': 'dcapwell'}, {'comment': 'fixed', 'commenter': 'jacek-lewandowski'}]"
2295,src/java/org/apache/cassandra/cache/AutoSavingCache.java,"@@ -196,12 +205,18 @@ public int loadSaved()
         // modern format, allows both key and value (so key cache load can be purely sequential)
         File dataPath = getCacheDataPath(CURRENT_VERSION);
         File crcPath = getCacheCrcPath(CURRENT_VERSION);
-        if (dataPath.exists() && crcPath.exists())
+        File metadataPath = getCacheMetadataPath(CURRENT_VERSION);
+        if (dataPath.exists() && crcPath.exists() && metadataPath.exists())","[{'comment': ""unrelated to this patch, sad that upgrading from 4.1 -> 5.0 ignores the cache as we don't support reading older versions."", 'commenter': 'dcapwell'}, {'comment': ""This is doable, but I don't know if complicating this code is worth restoring the cache from previous version 🤷🏻\u200d♂️ "", 'commenter': 'jacek-lewandowski'}]"
2333,src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java,"@@ -53,29 +60,81 @@ public class CompactionStats extends NodeToolCmd
     public void execute(NodeProbe probe)
     {
         PrintStream out = probe.output().out;
+        out.print(pendingTasksAndConcurrentCompactorsStats(probe));
+        out.print(compactionsCompletedStats(probe));
+        out.print(compactionThroughPutStats(probe));
+        out.println();
         CompactionManagerMBean cm = probe.getCompactionManagerProxy();
+        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughputBytes(), humanReadable, vtableOutput, out);
+    }
+
+    private static String pendingTasksAndConcurrentCompactorsStats(NodeProbe probe)
+    {
         Map<String, Map<String, Integer>> pendingTaskNumberByTable =
             (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
-        int numTotalPendingTask = 0;
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""%s concurrent compactors, %s pending tasks"", numConcurrentCompactorsConfigured(probe)
+        , numPendingTasks(pendingTaskNumberByTable)));
+        toPrint.append(LINE_SEPARATOR);
         for (Entry<String, Map<String, Integer>> ksEntry : pendingTaskNumberByTable.entrySet())
         {
+            String ksName = ksEntry.getKey();
             for (Entry<String, Integer> tableEntry : ksEntry.getValue().entrySet())
-                numTotalPendingTask += tableEntry.getValue();
+            {
+                toPrint.append(""- "" + ksName + '.' + tableEntry.getKey() + "": "" + tableEntry.getValue());
+                toPrint.append(LINE_SEPARATOR);
+            }
         }
-        out.println(""pending tasks: "" + numTotalPendingTask);
+
+        return toPrint.toString();
+    }
+
+    private static int numPendingTasks(Map<String, Map<String, Integer>> pendingTaskNumberByTable)
+    {
+        int numTotalPendingTasks = 0;
         for (Entry<String, Map<String, Integer>> ksEntry : pendingTaskNumberByTable.entrySet())
         {
-            String ksName = ksEntry.getKey();
             for (Entry<String, Integer> tableEntry : ksEntry.getValue().entrySet())
-            {
-                String tableName = tableEntry.getKey();
-                int pendingTaskCount = tableEntry.getValue();
-
-                out.println(""- "" + ksName + '.' + tableName + "": "" + pendingTaskCount);
-            }
+                numTotalPendingTasks += tableEntry.getValue();
         }
-        out.println();
-        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughputBytes(), humanReadable, vtableOutput, out);
+
+        return numTotalPendingTasks;
+    }
+
+    private static int numConcurrentCompactorsConfigured(NodeProbe probe)","[{'comment': ""We don't really need this method, we can call probe directly in the single usage there is of it."", 'commenter': 'driftx'}]"
2333,src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java,"@@ -53,29 +60,81 @@ public class CompactionStats extends NodeToolCmd
     public void execute(NodeProbe probe)
     {
         PrintStream out = probe.output().out;
+        out.print(pendingTasksAndConcurrentCompactorsStats(probe));
+        out.print(compactionsCompletedStats(probe));
+        out.print(compactionThroughPutStats(probe));
+        out.println();
         CompactionManagerMBean cm = probe.getCompactionManagerProxy();
+        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughputBytes(), humanReadable, vtableOutput, out);
+    }
+
+    private static String pendingTasksAndConcurrentCompactorsStats(NodeProbe probe)
+    {
         Map<String, Map<String, Integer>> pendingTaskNumberByTable =
             (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
-        int numTotalPendingTask = 0;
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""%s concurrent compactors, %s pending tasks"", numConcurrentCompactorsConfigured(probe)
+        , numPendingTasks(pendingTaskNumberByTable)));
+        toPrint.append(LINE_SEPARATOR);
         for (Entry<String, Map<String, Integer>> ksEntry : pendingTaskNumberByTable.entrySet())
         {
+            String ksName = ksEntry.getKey();
             for (Entry<String, Integer> tableEntry : ksEntry.getValue().entrySet())
-                numTotalPendingTask += tableEntry.getValue();
+            {
+                toPrint.append(""- "" + ksName + '.' + tableEntry.getKey() + "": "" + tableEntry.getValue());
+                toPrint.append(LINE_SEPARATOR);
+            }
         }
-        out.println(""pending tasks: "" + numTotalPendingTask);
+
+        return toPrint.toString();
+    }
+
+    private static int numPendingTasks(Map<String, Map<String, Integer>> pendingTaskNumberByTable)
+    {
+        int numTotalPendingTasks = 0;
         for (Entry<String, Map<String, Integer>> ksEntry : pendingTaskNumberByTable.entrySet())
         {
-            String ksName = ksEntry.getKey();
             for (Entry<String, Integer> tableEntry : ksEntry.getValue().entrySet())
-            {
-                String tableName = tableEntry.getKey();
-                int pendingTaskCount = tableEntry.getValue();
-
-                out.println(""- "" + ksName + '.' + tableName + "": "" + pendingTaskCount);
-            }
+                numTotalPendingTasks += tableEntry.getValue();
         }
-        out.println();
-        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughputBytes(), humanReadable, vtableOutput, out);
+
+        return numTotalPendingTasks;
+    }
+
+    private static int numConcurrentCompactorsConfigured(NodeProbe probe)
+    {
+        return probe.getConcurrentCompactors();
+    }
+
+    private static String compactionsCompletedStats(NodeProbe probe)
+    {
+        Long completedTasks = (Long)probe.getCompactionMetric(""CompletedTasks"");
+        CassandraMetricsRegistry.JmxMeterMBean totalCompactionsCompletedMetrics =
+            (CassandraMetricsRegistry.JmxMeterMBean)probe.getCompactionMetric(""TotalCompactionsCompleted"");
+        NumberFormat formatter = new DecimalFormat(""##.00"");
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""compactions completed: %s"", completedTasks));
+        toPrint.append(LINE_SEPARATOR);
+        toPrint.append(String.format(""\tminute rate:    %s/second"", formatter.format(totalCompactionsCompletedMetrics.getOneMinuteRate())));
+        toPrint.append(LINE_SEPARATOR);
+        toPrint.append(String.format(""\t5 minute rate:    %s/second"", formatter.format(totalCompactionsCompletedMetrics.getFiveMinuteRate())));
+        toPrint.append(LINE_SEPARATOR);
+        toPrint.append(String.format(""\t15 minute rate:    %s/second"", formatter.format(totalCompactionsCompletedMetrics.getFifteenMinuteRate())));
+        toPrint.append(LINE_SEPARATOR);
+        toPrint.append(String.format(""\tMean rate:    %s/second"", formatter.format(totalCompactionsCompletedMetrics.getMeanRate())));
+        toPrint.append(LINE_SEPARATOR);
+
+        return toPrint.toString();
+    }
+
+    private static String compactionThroughPutStats(NodeProbe probe)
+    {
+        Config conf = DatabaseDescriptor.loadConfig();","[{'comment': ""Nodetool should work against any node; we shouldn't import DD at all and instead get this information from the probe."", 'commenter': 'driftx'}, {'comment': 'NodeProbe.getCompactionThroughputMebibytesAsDouble too reads from DatabaseDescriptor(via StorageService bean). Not sure what should be the correct source of data here.', 'commenter': 'mghildiy'}, {'comment': 'Yes, but over JMX from the node connected, not the local value, so it will be correct.', 'commenter': 'driftx'}, {'comment': 'So issue here is that I have to get hold of both configured value and actual value. I was under improession that NodeProbe.getCompactionThroughputMebibytesAsDouble would give me actual value.', 'commenter': 'mghildiy'}, {'comment': ""There isn't a running actual throughtput value available, you'll need to expose compactionRateLimiter.getRate in CompactionManager to JMX."", 'commenter': 'driftx'}]"
2333,src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java,"@@ -53,29 +60,81 @@ public class CompactionStats extends NodeToolCmd
     public void execute(NodeProbe probe)
     {
         PrintStream out = probe.output().out;
+        out.print(pendingTasksAndConcurrentCompactorsStats(probe));
+        out.print(compactionsCompletedStats(probe));
+        out.print(compactionThroughPutStats(probe));
+        out.println();
         CompactionManagerMBean cm = probe.getCompactionManagerProxy();
+        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughputBytes(), humanReadable, vtableOutput, out);
+    }
+
+    private static String pendingTasksAndConcurrentCompactorsStats(NodeProbe probe)
+    {
         Map<String, Map<String, Integer>> pendingTaskNumberByTable =
             (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
-        int numTotalPendingTask = 0;
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""%s concurrent compactors, %s pending tasks"", numConcurrentCompactorsConfigured(probe)
+        , numPendingTasks(pendingTaskNumberByTable)));
+        toPrint.append(LINE_SEPARATOR);
         for (Entry<String, Map<String, Integer>> ksEntry : pendingTaskNumberByTable.entrySet())
         {
+            String ksName = ksEntry.getKey();
             for (Entry<String, Integer> tableEntry : ksEntry.getValue().entrySet())
-                numTotalPendingTask += tableEntry.getValue();
+            {
+                toPrint.append(""- "" + ksName + '.' + tableEntry.getKey() + "": "" + tableEntry.getValue());
+                toPrint.append(LINE_SEPARATOR);
+            }
         }
-        out.println(""pending tasks: "" + numTotalPendingTask);
+
+        return toPrint.toString();
+    }
+
+    private static int numPendingTasks(Map<String, Map<String, Integer>> pendingTaskNumberByTable)
+    {
+        int numTotalPendingTasks = 0;
         for (Entry<String, Map<String, Integer>> ksEntry : pendingTaskNumberByTable.entrySet())
         {
-            String ksName = ksEntry.getKey();
             for (Entry<String, Integer> tableEntry : ksEntry.getValue().entrySet())
-            {
-                String tableName = tableEntry.getKey();
-                int pendingTaskCount = tableEntry.getValue();
-
-                out.println(""- "" + ksName + '.' + tableName + "": "" + pendingTaskCount);
-            }
+                numTotalPendingTasks += tableEntry.getValue();
         }
-        out.println();
-        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughputBytes(), humanReadable, vtableOutput, out);
+
+        return numTotalPendingTasks;
+    }
+
+    private static int numConcurrentCompactorsConfigured(NodeProbe probe)
+    {
+        return probe.getConcurrentCompactors();
+    }
+
+    private static String compactionsCompletedStats(NodeProbe probe)
+    {
+        Long completedTasks = (Long)probe.getCompactionMetric(""CompletedTasks"");
+        CassandraMetricsRegistry.JmxMeterMBean totalCompactionsCompletedMetrics =
+            (CassandraMetricsRegistry.JmxMeterMBean)probe.getCompactionMetric(""TotalCompactionsCompleted"");
+        NumberFormat formatter = new DecimalFormat(""##.00"");
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""compactions completed: %s"", completedTasks));
+        toPrint.append(LINE_SEPARATOR);
+        toPrint.append(String.format(""\tminute rate:    %s/second"", formatter.format(totalCompactionsCompletedMetrics.getOneMinuteRate())));
+        toPrint.append(LINE_SEPARATOR);
+        toPrint.append(String.format(""\t5 minute rate:    %s/second"", formatter.format(totalCompactionsCompletedMetrics.getFiveMinuteRate())));
+        toPrint.append(LINE_SEPARATOR);
+        toPrint.append(String.format(""\t15 minute rate:    %s/second"", formatter.format(totalCompactionsCompletedMetrics.getFifteenMinuteRate())));
+        toPrint.append(LINE_SEPARATOR);
+        toPrint.append(String.format(""\tMean rate:    %s/second"", formatter.format(totalCompactionsCompletedMetrics.getMeanRate())));
+        toPrint.append(LINE_SEPARATOR);
+
+        return toPrint.toString();
+    }
+
+    private static String compactionThroughPutStats(NodeProbe probe)
+    {
+        Config conf = DatabaseDescriptor.loadConfig();
+        double compactionTPConfigured = conf.compaction_throughput.toMebibytesPerSecond();
+        double compactionTPActual = probe.getCompactionThroughputMebibytesAsDouble();
+        double percentage = (compactionTPActual / compactionTPConfigured) * 100;","[{'comment': 'Need the handle the case when the configured throughput is zero.', 'commenter': 'driftx'}]"
2333,src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java,"@@ -53,29 +58,82 @@ public class CompactionStats extends NodeToolCmd
     public void execute(NodeProbe probe)
     {
         PrintStream out = probe.output().out;
+        out.print(pendingTasksAndConcurrentCompactorsStats(probe));
+        out.print(compactionsCompletedStats(probe));
+        out.print(compactionThroughPutStats(probe));
+        out.println();
         CompactionManagerMBean cm = probe.getCompactionManagerProxy();
+        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughputBytes(), humanReadable, vtableOutput, out);
+    }
+
+    private static String pendingTasksAndConcurrentCompactorsStats(NodeProbe probe)
+    {
         Map<String, Map<String, Integer>> pendingTaskNumberByTable =
             (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
-        int numTotalPendingTask = 0;
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""%s concurrent compactors, %s pending tasks"", probe.getConcurrentCompactors()
+        , numPendingTasks(pendingTaskNumberByTable)));","[{'comment': 'I think there is a problem of code format\r\nline 75 should have some space before \',\' , so that In this way “，”  can be aligned with ""%$""', 'commenter': 'Maxwell-Guo'}, {'comment': 'yeah just align it like this\r\n\r\n    toPrint.append(String.format(""%s concurrent compactors, %s pending tasks"",\r\n                                 probe.getConcurrentCompactors(), \r\n                                 numPendingTasks(pendingTaskNumberByTable)));', 'commenter': 'smiklosovic'}]"
2333,src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java,"@@ -53,29 +58,82 @@ public class CompactionStats extends NodeToolCmd
     public void execute(NodeProbe probe)
     {
         PrintStream out = probe.output().out;
+        out.print(pendingTasksAndConcurrentCompactorsStats(probe));
+        out.print(compactionsCompletedStats(probe));
+        out.print(compactionThroughPutStats(probe));
+        out.println();
         CompactionManagerMBean cm = probe.getCompactionManagerProxy();
+        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughputBytes(), humanReadable, vtableOutput, out);
+    }
+
+    private static String pendingTasksAndConcurrentCompactorsStats(NodeProbe probe)
+    {
         Map<String, Map<String, Integer>> pendingTaskNumberByTable =
             (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
-        int numTotalPendingTask = 0;
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""%s concurrent compactors, %s pending tasks"", probe.getConcurrentCompactors()
+        , numPendingTasks(pendingTaskNumberByTable)));
+        toPrint.append(LINE_SEPARATOR);
         for (Entry<String, Map<String, Integer>> ksEntry : pendingTaskNumberByTable.entrySet())
         {
+            String ksName = ksEntry.getKey();
             for (Entry<String, Integer> tableEntry : ksEntry.getValue().entrySet())
-                numTotalPendingTask += tableEntry.getValue();
+            {
+                toPrint.append(""- "" + ksName + '.' + tableEntry.getKey() + "": "" + tableEntry.getValue());","[{'comment': 'Can we use String.format uniformly.\r\nAlthough its performance is not very good, and actually I prefer to use string concatenation.\r\nBut Cassandra seems to be all String.formt,So it seems that unity looks better', 'commenter': 'Maxwell-Guo'}, {'comment': 'yeah just be consistent, use String.format', 'commenter': 'smiklosovic'}]"
2333,test/unit/org/apache/cassandra/tools/nodetool/CompactionStatsTest.java,"@@ -130,6 +130,14 @@ compactionId, OperationType.COMPACTION, CQLTester.KEYSPACE, currentTable(), byte
                                                     CompactionInfo.Unit.BYTES, (double) bytesCompacted / bytesTotal * 100);
         Assertions.assertThat(stdout).containsPattern(expectedStatsPattern);
 
+        Assertions.assertThat(stdout).containsPattern(""[0-9]* concurrent compactors, [0-9]* pending tasks"");","[{'comment': 'I think to be on the safe side, you can add a test case that uses the invokeNodetool method for full-link testing.\r\nThough when using invokeNodetool from a client side , It is impossible to specifically determine the value of the test, but we can test and output some expected results, such as whether the output information has the print out field we need, and when there is no data, and when we write a certain amount of data then do a major, These print out data should met our expectations。\r\n@driftx WDYT?', 'commenter': 'Maxwell-Guo'}, {'comment': 'I think it is just fine as is.', 'commenter': 'smiklosovic'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -40,19 +54,105 @@
 import accord.local.PreLoadContext;
 import accord.local.SafeCommandStore;
 import accord.primitives.Deps;
+import accord.local.SaveStatus;
+import accord.primitives.AbstractKeys;
+import accord.primitives.AbstractRanges;
+import accord.primitives.PartialDeps;
+import accord.primitives.PartialTxn;
+import accord.primitives.Range;
+import accord.primitives.Ranges;
+import accord.primitives.Routable;
 import accord.primitives.RoutableKey;
+import accord.primitives.Routables;
+import accord.primitives.Seekable;
+import accord.primitives.Seekables;
+import accord.primitives.Timestamp;
 import accord.primitives.TxnId;
 import accord.utils.Invariants;
 import accord.utils.async.AsyncChain;
 import accord.utils.async.AsyncChains;
+import accord.utils.async.Observable;
+import org.apache.cassandra.cql3.UntypedResultSet;
 import org.apache.cassandra.service.accord.async.AsyncOperation;
 import org.apache.cassandra.utils.Clock;
+import org.apache.cassandra.utils.Interval;
+import org.apache.cassandra.utils.IntervalTree;
+import org.apache.cassandra.utils.concurrent.AsyncPromise;
 import org.apache.cassandra.utils.concurrent.UncheckedInterruptedException;
 
 import static org.apache.cassandra.concurrent.ExecutorFactory.Global.executorFactory;
 
 public class AccordCommandStore extends CommandStore
 {
+    private static final Logger logger = LoggerFactory.getLogger(AccordCommandStore.class);
+
+    private static final class RangeCommandSummary","[{'comment': 'I can always serialize this to a `ByteBuffer` like we do for `CommandsForKeys`, could even use the same logic I think... I just went this way to be quicker and easier to debug while I am testing', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -248,10 +407,113 @@ public void completeOperation(AccordSafeCommandStore store,
                                   Map<RoutableKey, AccordSafeCommandsForKey> commandsForKeys)
     {
         Invariants.checkState(current == store);
+        maybeUpdateRangeIndex(commands);
         current.complete();
         current = null;
     }
 
+
+    private static CommandTimeseriesHolder fromRangeSummary(Seekable keyOrRange, List<RangeCommandSummary> matches)
+    {
+        return new CommandTimeseriesHolder()
+        {
+            @Override
+            public CommandTimeseries<?> byId()
+            {
+                CommandTimeseries.Update<RangeCommandSummary> builder = new CommandTimeseries.Update<>(keyOrRange, RangeCommandSummaryLoader.INSTANCE);
+                for (RangeCommandSummary m : matches)
+                {
+                    if (m.status == SaveStatus.Invalidated)
+                        continue;
+                    builder.add(m.txnId, m);
+                }
+                return builder.build();
+            }
+
+            @Override
+            public CommandTimeseries<?> byExecuteAt()
+            {
+                CommandTimeseries.Update<RangeCommandSummary> builder = new CommandTimeseries.Update<>(null, RangeCommandSummaryLoader.INSTANCE);
+                for (RangeCommandSummary m : matches)
+                {
+                    if (m.status == SaveStatus.Invalidated)
+                        continue;
+                    builder.add(m.executeAt != null ? m.executeAt : m.txnId, m);
+                }
+                return builder.build();
+            }
+        };
+    }
+
+    <O> O mapReduceForRange(Routables<?, ?> keysOrRanges, Ranges slice, BiFunction<CommandTimeseriesHolder, O, O> map, O accumulate, O terminalValue)
+    {
+        switch (keysOrRanges.domain())
+        {
+            case Key:
+            {
+                AbstractKeys<Key, ?> keys = (AbstractKeys<Key, ?>) keysOrRanges;
+                for (Key key : keys)
+                {
+                    if (!slice.contains(key)) continue;
+
+                    accumulate = map.apply(fromRangeSummary(key, rangesToCommands.search(key)), accumulate);
+                    if (accumulate.equals(terminalValue))
+                        return accumulate;
+                }
+            }
+            break;
+            case Range:
+            {
+                AbstractRanges<?> ranges = (AbstractRanges<?>) keysOrRanges;
+                ranges = ranges.slice(slice, Routables.Slice.Minimal);
+                for (Range range : ranges)
+                {
+                    accumulate = map.apply(fromRangeSummary(range, rangesToCommands.search(Interval.create(range.start(), range.end()))), accumulate);
+                    if (accumulate.equals(terminalValue))
+                        return accumulate;
+                }
+            }
+            break;
+            default:
+                throw new AssertionError(""Unknown domain: "" + keysOrRanges.domain());
+        }
+        return accumulate;
+    }
+
+    private void maybeUpdateRangeIndex(Map<TxnId, AccordSafeCommand> commands)
+    {
+        for (Map.Entry<TxnId, AccordSafeCommand> e : commands.entrySet())
+        {
+            TxnId txnId = e.getKey();
+            if (txnId.domain() != Routable.Domain.Range)
+                continue;
+            Command current = e.getValue().current();
+            if (current.saveStatus() == SaveStatus.NotWitnessed)
+                continue; // don't know the range/dependencies, so can't cache
+            PartialTxn txn = current.partialTxn();
+            Seekables<?, ?> keys = txn.keys();
+            if (keys.domain() != Routable.Domain.Range)
+                throw new AssertionError(""Found a Range Transaction that had non-Range keys: "" + current);
+            if (keys.isEmpty())
+                throw new AssertionError(""Found a Range Transaction that has empty keys: "" + current);
+            PartialDeps deps = current.partialDeps();
+            List<TxnId> dependsOn = deps == null ? Collections.emptyList() : deps.txnIds();
+
+            RangeCommandSummary summary = new RangeCommandSummary(txnId, current.saveStatus(), current.executeAt(), dependsOn);
+            Ranges ranges = (Ranges) keys;
+            put(ranges, summary);
+        }
+    }
+
+    private void put(Ranges ranges, RangeCommandSummary summary)
+    {
+        IntervalTree.Builder<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> builder = rangesToCommands.unbuild();
+        //TODO double check this tree has same inclusive/exclusive semantics as this range...
+        for (Range range : ranges)
+            builder.add(new Interval<>(range.start(), range.end(), summary));","[{'comment': 'I believe this is not correct as this method gets called on state change, and `IntervalTree` will just add the update (so we see new and old); will look how to filter this', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -248,10 +407,113 @@ public void completeOperation(AccordSafeCommandStore store,
                                   Map<RoutableKey, AccordSafeCommandsForKey> commandsForKeys)
     {
         Invariants.checkState(current == store);
+        maybeUpdateRangeIndex(commands);
         current.complete();
         current = null;
     }
 
+
+    private static CommandTimeseriesHolder fromRangeSummary(Seekable keyOrRange, List<RangeCommandSummary> matches)
+    {
+        return new CommandTimeseriesHolder()
+        {
+            @Override
+            public CommandTimeseries<?> byId()
+            {
+                CommandTimeseries.Update<RangeCommandSummary> builder = new CommandTimeseries.Update<>(keyOrRange, RangeCommandSummaryLoader.INSTANCE);
+                for (RangeCommandSummary m : matches)
+                {
+                    if (m.status == SaveStatus.Invalidated)
+                        continue;
+                    builder.add(m.txnId, m);
+                }
+                return builder.build();
+            }
+
+            @Override
+            public CommandTimeseries<?> byExecuteAt()
+            {
+                CommandTimeseries.Update<RangeCommandSummary> builder = new CommandTimeseries.Update<>(null, RangeCommandSummaryLoader.INSTANCE);
+                for (RangeCommandSummary m : matches)
+                {
+                    if (m.status == SaveStatus.Invalidated)
+                        continue;
+                    builder.add(m.executeAt != null ? m.executeAt : m.txnId, m);
+                }
+                return builder.build();
+            }
+        };
+    }
+
+    <O> O mapReduceForRange(Routables<?, ?> keysOrRanges, Ranges slice, BiFunction<CommandTimeseriesHolder, O, O> map, O accumulate, O terminalValue)
+    {
+        switch (keysOrRanges.domain())
+        {
+            case Key:
+            {
+                AbstractKeys<Key, ?> keys = (AbstractKeys<Key, ?>) keysOrRanges;
+                for (Key key : keys)
+                {
+                    if (!slice.contains(key)) continue;
+
+                    accumulate = map.apply(fromRangeSummary(key, rangesToCommands.search(key)), accumulate);
+                    if (accumulate.equals(terminalValue))
+                        return accumulate;
+                }
+            }
+            break;
+            case Range:
+            {
+                AbstractRanges<?> ranges = (AbstractRanges<?>) keysOrRanges;
+                ranges = ranges.slice(slice, Routables.Slice.Minimal);
+                for (Range range : ranges)
+                {
+                    accumulate = map.apply(fromRangeSummary(range, rangesToCommands.search(Interval.create(range.start(), range.end()))), accumulate);
+                    if (accumulate.equals(terminalValue))
+                        return accumulate;
+                }
+            }
+            break;
+            default:
+                throw new AssertionError(""Unknown domain: "" + keysOrRanges.domain());
+        }
+        return accumulate;
+    }
+
+    private void maybeUpdateRangeIndex(Map<TxnId, AccordSafeCommand> commands)
+    {
+        for (Map.Entry<TxnId, AccordSafeCommand> e : commands.entrySet())
+        {
+            TxnId txnId = e.getKey();
+            if (txnId.domain() != Routable.Domain.Range)
+                continue;
+            Command current = e.getValue().current();
+            if (current.saveStatus() == SaveStatus.NotWitnessed)
+                continue; // don't know the range/dependencies, so can't cache
+            PartialTxn txn = current.partialTxn();
+            Seekables<?, ?> keys = txn.keys();
+            if (keys.domain() != Routable.Domain.Range)
+                throw new AssertionError(""Found a Range Transaction that had non-Range keys: "" + current);
+            if (keys.isEmpty())
+                throw new AssertionError(""Found a Range Transaction that has empty keys: "" + current);
+            PartialDeps deps = current.partialDeps();
+            List<TxnId> dependsOn = deps == null ? Collections.emptyList() : deps.txnIds();
+
+            RangeCommandSummary summary = new RangeCommandSummary(txnId, current.saveStatus(), current.executeAt(), dependsOn);
+            Ranges ranges = (Ranges) keys;
+            put(ranges, summary);
+        }
+    }
+
+    private void put(Ranges ranges, RangeCommandSummary summary)
+    {
+        IntervalTree.Builder<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> builder = rangesToCommands.unbuild();
+        //TODO double check this tree has same inclusive/exclusive semantics as this range...","[{'comment': 'this looks inclusive on both sides, and our range tend to be `(, ]`, so not a perfect match... will look to improve', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -156,12 +174,33 @@ ImmutableSortedMap<Timestamp, ByteBuffer> getValues(CommandsForKey cfk)
         }
     }
 
+    private enum TokenType
+    {
+        Murmur3((byte) 1),","[{'comment': 'TODO add more partitioner support... our tests only use murmur3 so did this just for place holder', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -156,12 +174,33 @@ ImmutableSortedMap<Timestamp, ByteBuffer> getValues(CommandsForKey cfk)
         }
     }
 
+    private enum TokenType
+    {
+        Murmur3((byte) 1),
+        ;
+
+        private final byte value;","[{'comment': ""my thinking is that a `murmur3` token can't compare with a `bytes order` token, so by pulling out the `type` as the bytes prefix, we know that only `murmur3` tokens will be seen during the query if the range is for a `murmur3` token range"", 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -537,6 +581,20 @@ public static Mutation getCommandMutation(AccordCommandStore commandStore, Accor
         }
     }
 
+    public static ByteBuffer serializeToken(Token token)
+    {
+        return serializeToken(token, ByteBufferAccessor.instance);
+    }
+
+    private static <V> V serializeToken(Token token, ValueAccessor<V> accessor)","[{'comment': 'I could have written this off `ByteBuffer`, but had to do a lot of `ValueAccessor` work recently so was more habit than anything', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -595,13 +653,257 @@ public static UntypedResultSet loadCommandRow(CommandStore commandStore, TxnId t
     {
         String cql = ""SELECT * FROM %s.%s "" +
                      ""WHERE store_id = ? "" +
+                     ""AND domain = ? "" +
                      ""AND txn_id=(?, ?, ?)"";
 
         return executeInternal(String.format(cql, ACCORD_KEYSPACE_NAME, COMMANDS),
                                commandStore.id(),
+                               txnId.domain().ordinal(),
                                txnId.msb, txnId.lsb, txnId.node.id);
     }
 
+    public static void findAllCommandsByDomain(int commandStore, Routable.Domain domain, Set<String> columns, Observable<UntypedResultSet.Row> callback)
+    {
+        WalkCommandsForDomain work = new WalkCommandsForDomain(commandStore, domain, columns, Stage.READ.executor(), callback);
+        work.schedule();
+    }
+
+    private static abstract class TableWalk implements Runnable, DebuggableTask
+    {
+        private final long creationTimeNanos = Clock.Global.nanoTime();
+        private final Executor executor;
+        private final Observable<UntypedResultSet.Row> callback;
+        private long startTimeNanos = -1;
+        private int numQueries = 0;
+        private UntypedResultSet.Row lastSeen = null;
+
+        private TableWalk(Executor executor, Observable<UntypedResultSet.Row> callback)
+        {
+            this.executor = executor;
+            this.callback = callback;
+        }
+
+        protected boolean shouldContinue(UntypedResultSet.Row lastRow)
+        {
+            return true;
+        }
+
+        protected abstract UntypedResultSet query(UntypedResultSet.Row lastSeen);
+
+        public final void schedule()
+        {
+            executor.execute(this);
+        }
+
+        @Override
+        public final void run()
+        {
+            try
+            {
+                if (startTimeNanos == -1)
+                    startTimeNanos = Clock.Global.nanoTime();
+                numQueries++;
+                UntypedResultSet result = query(lastSeen);
+                if (result.isEmpty())
+                {
+                    callback.onCompleted();
+                    return;
+                }
+                UntypedResultSet.Row lastRow = null;
+                for (UntypedResultSet.Row row : result)
+                {
+                    callback.onNext(row);
+                    lastRow = row;
+                }
+                if (shouldContinue(lastRow))
+                {
+                    lastSeen = lastRow;
+                    schedule();
+                }
+                else
+                {
+                    lastSeen = null;
+                }
+            }
+            catch (Throwable t)
+            {
+                callback.onError(t);
+            }
+        }
+
+        @Override
+        public long creationTimeNanos()
+        {
+            return creationTimeNanos;
+        }
+
+        @Override
+        public long startTimeNanos()
+        {
+            return startTimeNanos;
+        }
+
+        @Override
+        public String description()
+        {
+            return String.format(""Table Walker for %s; queries = %d"", getClass().getSimpleName(), numQueries);
+        }
+    }
+
+    private static String selection(TableMetadata metadata, Set<String> requiredColumns, Set<String> forIteration)
+    {
+        StringBuilder selection = new StringBuilder();
+        if (requiredColumns.isEmpty())
+            selection.append(""*"");
+        else
+        {
+            Sets.SetView<String> other = Sets.difference(requiredColumns, forIteration);
+            for (String name : other)
+            {
+                ColumnMetadata meta = metadata.getColumn(new ColumnIdentifier(name, true));
+                if (meta == null)
+                    throw new IllegalArgumentException(""Unknown column: "" + name);
+            }
+            List<String> names = new ArrayList<>(forIteration.size() + other.size());
+            names.addAll(forIteration);
+            names.addAll(other);
+            // this sort is to make sure the CQL is determanistic
+            Collections.sort(names);
+            for (int i = 0; i < names.size(); i++)
+            {
+                if (i > 0)
+                    selection.append("", "");
+                selection.append(names.get(i));
+            }
+        }
+        return selection.toString();
+    }
+
+    private static class WalkCommandsForDomain extends TableWalk
+    {
+        private static final Set<String> COLUMNS_FOR_ITERATION = ImmutableSet.of(""txn_id"", ""store_id"", ""domain"");
+        private final String cql;
+        private final int storeId, domain;
+
+        private WalkCommandsForDomain(int commandStore, Routable.Domain domain, Set<String> requiredColumns, Executor executor, Observable<UntypedResultSet.Row> callback)
+        {
+            super(executor, callback);
+            this.storeId = commandStore;
+            this.domain = domain.ordinal();
+            cql = String.format(""SELECT %s "" +
+                                ""FROM %s "" +
+                                ""WHERE store_id = ? "" +
+                                ""      AND domain = ? "" +
+                                ""      AND token(store_id, domain, txn_id) > token(?, ?, (?, ?, ?)) "" +","[{'comment': 'since we know this table has a token layout that matches the data (local partitioner) this helps make sure that the exec engine does what we want.', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/dht/Token.java,"@@ -41,6 +42,16 @@ public abstract class Token implements RingPosition<Token>, Serializable
         public abstract ByteBuffer toByteArray(Token token);
         public abstract Token fromByteArray(ByteBuffer bytes);
 
+        public byte[] toOrderedByteArray(Token token, ByteComparable.Version version)
+        {
+            return ByteSourceInverse.readBytes(asComparableBytes(token, version));
+        }
+
+        public Token fromOrderedByteArray(byte[] bytes, ByteComparable.Version version)","[{'comment': 'Happens to be unused, but fine.', 'commenter': 'aweisberg'}, {'comment': 'unused, but I like to make sure the to/from are both there... I know our style guide says not to add dead code... but I felt its still best to have to/from APIs go together', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -252,6 +341,47 @@ public void completeOperation(AccordSafeCommandStore store,
         current = null;
     }
 
+    <O> O mapReduceForRange(Routables<?, ?> keysOrRanges, Ranges slice, BiFunction<CommandTimeseriesHolder, O, O> map, O accumulate, O terminalValue)
+    {
+        keysOrRanges = keysOrRanges.slice(slice, Routables.Slice.Minimal);
+        switch (keysOrRanges.domain())
+        {
+            case Key:
+            {
+                AbstractKeys<Key, ?> keys = (AbstractKeys<Key, ?>) keysOrRanges;
+                for (CommandTimeseriesHolder summary : commandsForRanges.search(keys))
+                {
+                    accumulate = map.apply(summary, accumulate);
+                    if (accumulate.equals(terminalValue))
+                        return accumulate;
+                }
+            }
+            break;
+            case Range:
+            {
+                AbstractRanges<?> ranges = (AbstractRanges<?>) keysOrRanges;
+                for (Range range : ranges)
+                {
+                    CommandTimeseriesHolder summary = commandsForRanges.search(range);
+                    if (summary == null)
+                        continue;
+                    accumulate = map.apply(summary, accumulate);
+                    if (accumulate.equals(terminalValue))
+                        return accumulate;
+                }
+            }
+            break;
+            default:
+                throw new AssertionError(""Unknown domain: "" + keysOrRanges.domain());
+        }
+        return accumulate;
+    }
+
+    CommandsForRanges.Builder unbuild()","[{'comment': ""Pedantic, but this doesn't `unbuild` the `CommandStore` it unbuilds a `CommandsForRanges`."", 'commenter': 'aweisberg'}, {'comment': 'switched to `update` and did a refactor that was in the back of my head; split `Builder` into 2: `Builder` and `Updater`...  one with a `build` method (for init) and one with `apply` which is for mutations', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/gms/ApplicationState.java,"@@ -57,6 +57,9 @@
      **/
     SSTABLE_VERSIONS,
     DISK_USAGE,
+
+    // TODO (replace with TCM) : This is a temp solution to know what epoch is current for each instance in the cluster, used for topology ACK callbacks","[{'comment': 'should be deleted in https://issues.apache.org/jira/browse/CASSANDRA-18444', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordConfigurationService.java,"@@ -20,30 +20,53 @@
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.concurrent.CopyOnWriteArrayList;
 
-import com.google.common.base.Preconditions;
+import javax.annotation.concurrent.GuardedBy;
 
 import accord.api.ConfigurationService;
 import accord.local.Node;
 import accord.topology.Topology;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.gms.IEndpointStateChangeSubscriber;
+import org.apache.cassandra.gms.VersionedValue;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.service.StorageService;
 
 /**
  * Currently a stubbed out config service meant to be triggered from a dtest
  */
 public class AccordConfigurationService implements ConfigurationService
 {
     private final Node.Id localId;
-    private final List<Listener> listeners = new ArrayList<>();
+    private final List<Listener> listeners = new CopyOnWriteArrayList<>();
+    @GuardedBy(""this"")
     private final List<Topology> epochs = new ArrayList<>();
 
     public AccordConfigurationService(Node.Id localId)
     {
         this.localId = localId;
         epochs.add(Topology.EMPTY);
+
+        // TODO (replace with TCM): Need to know when an epoch is known on the peers, so use gossip until TCM is ready
+        Gossiper.instance.register(new IEndpointStateChangeSubscriber()","[{'comment': 'should be deleted in https://issues.apache.org/jira/browse/CASSANDRA-18444', 'commenter': 'dcapwell'}]"
2339,test/distributed/org/apache/cassandra/distributed/test/accord/NewSchemaTest.java,"@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test.accord;
+
+import java.nio.ByteBuffer;
+import java.util.List;
+
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.service.accord.AccordService;
+
+public class NewSchemaTest extends AccordTestBase
+{
+    private static final Logger logger = LoggerFactory.getLogger(NewSchemaTest.class);
+
+    @Override
+    protected Logger logger()
+    {
+        return logger;
+    }
+
+    @Test
+    public void test()","[{'comment': 'there were bugs when we would add new key spaces, so added this test to make sure this is better covered', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/CommandsForRanges.java,"@@ -0,0 +1,337 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.function.Function;
+import java.util.function.Predicate;
+import javax.annotation.Nullable;
+
+import com.google.common.collect.AbstractIterator;
+
+import accord.api.Key;
+import accord.api.RoutingKey;
+import accord.impl.CommandTimeseries;
+import accord.impl.CommandTimeseriesHolder;
+import accord.local.Command;
+import accord.local.SaveStatus;
+import accord.primitives.AbstractKeys;
+import accord.primitives.Range;
+import accord.primitives.Ranges;
+import accord.primitives.RoutableKey;
+import accord.primitives.Seekable;
+import accord.primitives.Timestamp;
+import accord.primitives.TxnId;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey.TokenKey;
+import org.apache.cassandra.service.accord.api.PartitionKey;
+import org.apache.cassandra.utils.Interval;
+import org.apache.cassandra.utils.IntervalTree;
+
+public class CommandsForRanges
+{
+    private static final class RangeCommandSummary
+    {
+        public final TxnId txnId;
+        public final SaveStatus status;
+        public final @Nullable Timestamp executeAt;
+        public final List<TxnId> deps;
+
+        RangeCommandSummary(TxnId txnId, SaveStatus status, @Nullable Timestamp executeAt, List<TxnId> deps)
+        {
+            this.txnId = txnId;
+            this.status = status;
+            this.executeAt = executeAt;
+            this.deps = deps;
+        }
+
+        @Override
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+            RangeCommandSummary that = (RangeCommandSummary) o;
+            return txnId.equals(that.txnId);
+        }
+
+        @Override
+        public int hashCode()
+        {
+            return Objects.hash(txnId);
+        }
+
+        @Override
+        public String toString()
+        {
+            return ""RangeCommandSummary{"" +
+                   ""txnId="" + txnId +
+                   "", status="" + status +
+                   '}';
+        }
+    }
+
+    private enum RangeCommandSummaryLoader implements CommandTimeseries.CommandLoader<RangeCommandSummary>
+    {
+        INSTANCE;
+
+        @Override
+        public RangeCommandSummary saveForCFK(Command command)
+        {
+            //TODO split write from read?
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public TxnId txnId(RangeCommandSummary data)
+        {
+            return data.txnId;
+        }
+
+        @Override
+        public Timestamp executeAt(RangeCommandSummary data)
+        {
+            return data.executeAt;
+        }
+
+        @Override
+        public SaveStatus saveStatus(RangeCommandSummary data)
+        {
+            return data.status;
+        }
+
+        @Override
+        public List<TxnId> depsIds(RangeCommandSummary data)
+        {
+            return data.deps;
+        }
+    }
+
+    public class Builder
+    {
+        private final IntervalTree.Builder<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> builder;
+
+        private Builder(IntervalTree.Builder<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> builder)
+        {
+            this.builder = builder;
+        }
+
+        public Builder put(Ranges ranges, TxnId txnId, SaveStatus status, Timestamp execteAt, List<TxnId> dependsOn)
+        {
+            remove(txnId);
+            return put(ranges, new RangeCommandSummary(txnId, status, execteAt, dependsOn));
+        }
+
+        private Builder put(Ranges ranges, RangeCommandSummary summary)
+        {
+            for (Range range : ranges)
+                put(range, summary);
+            return this;
+        }
+
+        private Builder put(Range range, RangeCommandSummary summary)
+        {
+            builder.add(Interval.create(normalize(range.start(), range.startInclusive(), true),
+                                        normalize(range.end(), range.endInclusive(), false),
+                                        summary));
+            return this;
+        }
+
+        private Builder remove(TxnId txnId)
+        {
+            return removeIf(data -> data.txnId.equals(txnId));
+        }
+
+        private Builder removeIf(Predicate<RangeCommandSummary> predicate)
+        {
+            return removeIf((i1, i2, data) -> predicate.test(data));
+        }
+
+        private Builder removeIf(IntervalTree.Builder.TriPredicate<RoutableKey, RoutableKey, RangeCommandSummary> predicate)
+        {
+            builder.removeIf(predicate);
+            return this;
+        }
+
+        public void apply()
+        {
+            rangesToCommands = builder.build();
+        }
+    }
+
+    private IntervalTree<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> rangesToCommands = IntervalTree.emptyTree();
+
+    public Iterable<CommandTimeseriesHolder> search(AbstractKeys<Key, ?> keys)
+    {
+        // group by the keyspace, as ranges are based off TokenKey, which is scoped to a range
+        Map<String, List<Key>> groupByKeyspace = new TreeMap<>();
+        for (Key key : keys)
+            groupByKeyspace.computeIfAbsent(((PartitionKey) key).keyspace(), ignore -> new ArrayList<>()).add(key);
+        return () -> new AbstractIterator<CommandTimeseriesHolder>()
+        {
+            Iterator<String> ksIt = groupByKeyspace.keySet().iterator();
+            Iterator<Map.Entry<Range, Set<RangeCommandSummary>>> rangeIt;
+
+            @Override
+            protected CommandTimeseriesHolder computeNext()
+            {
+                while (true)
+                {
+                    if (rangeIt != null && rangeIt.hasNext())
+                    {
+                        Map.Entry<Range, Set<RangeCommandSummary>> next = rangeIt.next();
+                        return result(next.getKey(), next.getValue());
+                    }
+                    rangeIt = null;
+                    if (!ksIt.hasNext())
+                    {
+                        ksIt = null;
+                        return endOfData();
+                    }
+                    String ks = ksIt.next();
+                    List<Key> keys = groupByKeyspace.get(ks);
+                    Map<Range, Set<RangeCommandSummary>> groupByRange = new TreeMap<>(Range::compare);","[{'comment': 'Seems like this could just be a list since keys should already be sorted?', 'commenter': 'aweisberg'}, {'comment': ""don't think so, so the keys are ordered, but some ranges maybe larger than others\r\n\r\n```\r\n[10, 20]\r\n[0, 10000]\r\n```\r\n\r\nso if you have the following keys\r\n\r\n```\r\n15 -> [10, 20], [0, 10000]\r\n35 -> [0, 10000]\r\n```\r\n\r\nhere we see `[0, 10000]` twice, so we would need search for it, hence why map made sense to me (lower search cost)"", 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/async/AsyncOperation.java,"@@ -57,7 +57,7 @@
     static class Context
     {
         final HashMap<TxnId, AccordSafeCommand> commands = new HashMap<>();
-        final HashMap<RoutableKey, AccordSafeCommandsForKey> commandsForKeys = new HashMap<>();
+        final TreeMap<RoutableKey, AccordSafeCommandsForKey> commandsForKeys = new TreeMap<>();","[{'comment': 'What does this need to be sorted for now?', 'commenter': 'aweisberg'}, {'comment': ""its for `org.apache.cassandra.service.accord.AccordSafeCommandStore#mapReduceForKey`\r\n\r\n```\r\nfor (RoutableKey key : commandsForKeys.keySet())\r\n                {\r\n                    //TODO (duplicate code): this is a repeat of Key... only change is checking contains in range\r\n                    if (!keysOrRanges.contains(key)) continue;\r\n                    SafeCommandsForKey forKey = commandsForKey(key);\r\n...\r\n```\r\n\r\nI thought it looked like callers of `mapReduce` expect ordering, but `map` logically shouldn't... so I might be able to remove; would just need to double check this ordering expectations."", 'commenter': 'dcapwell'}, {'comment': 'I actually think it should stay, so we have deterministic ordering (something needed by simulator)', 'commenter': 'dcapwell'}]"
2339,test/unit/org/apache/cassandra/cql3/PreparedStatementsTest.java,"@@ -772,8 +773,11 @@ private static List<String> columnNames(ResultSet rs)
         return rs.getColumnDefinitions().asList().stream().map(d -> d.getName()).collect(Collectors.toList());
     }
 
-    private static void updateTxnState()
+    private void updateTxnState()
     {
+        // this class keeps dropping tables, so the commands_for_keys cache points to stale data
+        AccordKeyspace.truncateTables();","[{'comment': 'This should be removed at some point since Accord should be able to handle that? Do we need a TODO to remember to remove this?', 'commenter': 'aweisberg'}, {'comment': 'I think it was fixed by the change to ignore unknown ranges... if I remove the tests still pass', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordCommandStore.java,"@@ -105,6 +191,9 @@ public boolean inStore()
     @Override
     protected void registerHistoricalTransactions(Deps deps)
     {
+        // TODO (impl) : Its not clear why this exists
+        // accord.coordinate.CoordinateSyncPoint.coordinate(accord.local.Node, accord.primitives.TxnId, accord.primitives.Seekables<?,?>)","[{'comment': ""As mentioned later it seems like for bootstrap we want to load in some historical information about what transactions existed? @belliottsmith  can you explain?\r\n\r\nI don't understand the cases where it is skipping keys either."", 'commenter': 'aweisberg'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -231,6 +276,7 @@ private static ColumnMetadata getColumn(TableMetadata metadata, String name)
               ""accord commands per key"",
               ""CREATE TABLE %s (""
               + ""store_id int, ""
+              + ""key_hash blob, "" // can't use ""token"" as this is restricted word in CQL","[{'comment': 'Nit, since we generally call them `Token` it might be clearer to call it `key_token`?', 'commenter': 'aweisberg'}, {'comment': 'I used `token` before, which is a reserved word... I could def use `key_token`', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordKeyspace.java,"@@ -595,13 +664,230 @@ public static UntypedResultSet loadCommandRow(CommandStore commandStore, TxnId t
     {
         String cql = ""SELECT * FROM %s.%s "" +
                      ""WHERE store_id = ? "" +
+                     ""AND domain = ? "" +
                      ""AND txn_id=(?, ?, ?)"";
 
         return executeInternal(String.format(cql, ACCORD_KEYSPACE_NAME, COMMANDS),
                                commandStore.id(),
+                               txnId.domain().ordinal(),
                                txnId.msb, txnId.lsb, txnId.node.id);
     }
 
+    public static void findAllCommandsByDomain(int commandStore, Routable.Domain domain, Set<String> columns, Observable<UntypedResultSet.Row> callback)
+    {
+        WalkCommandsForDomain work = new WalkCommandsForDomain(commandStore, domain, columns, Stage.READ.executor(), callback);
+        work.schedule();
+    }
+
+    private static abstract class TableWalk implements Runnable, DebuggableTask","[{'comment': 'I deleted most of my comments on this CQL stuff since it seems temporary anyways.', 'commenter': 'aweisberg'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordSafeCommandStore.java,"@@ -174,19 +187,34 @@ private <O> O mapReduceForKey(Routables<?, ?> keysOrRanges, Ranges slice, BiFunc
                     if (accumulate.equals(terminalValue))
                         return accumulate;
                 }
-                break;
+            }
+            break;
             case Range:
-                // TODO (required): implement
-                throw new UnsupportedOperationException();
+            {
+                // TODO (correctness): if a range is used, then the range must exist in the PreLoadContext, else the commandsForKeys won't be in-sync... can this be detected?","[{'comment': ""It can, it's a safe command store so it still has the preload context?"", 'commenter': 'aweisberg'}, {'comment': 'added defensive check', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/AccordVerbHandler.java,"@@ -43,6 +43,19 @@ public AccordVerbHandler(Node node)
     public void doVerb(Message<T> message) throws IOException
     {
         logger.debug(""Receiving {} from {}"", message.payload, message.from());
-        message.payload.process(node, EndpointMapping.getId(message.from()), message);
+        T request = message.payload;
+        Node.Id from = EndpointMapping.getId(message.from());","[{'comment': 'Move this into the innermost `if` since it is unused outside.', 'commenter': 'aweisberg'}, {'comment': '`from` is needed on line 59 as well, so if the outer if is false, then we do `request.process(node, from, message);`', 'commenter': 'dcapwell'}]"
2339,src/java/org/apache/cassandra/service/accord/CommandsForRanges.java,"@@ -0,0 +1,368 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.function.Function;
+import java.util.function.Predicate;
+import javax.annotation.Nullable;
+
+import com.google.common.collect.AbstractIterator;
+
+import accord.api.Key;
+import accord.api.RoutingKey;
+import accord.impl.CommandTimeseries;
+import accord.impl.CommandTimeseriesHolder;
+import accord.local.Command;
+import accord.local.SaveStatus;
+import accord.primitives.AbstractKeys;
+import accord.primitives.Range;
+import accord.primitives.Ranges;
+import accord.primitives.RoutableKey;
+import accord.primitives.Seekable;
+import accord.primitives.Timestamp;
+import accord.primitives.TxnId;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey;
+import org.apache.cassandra.service.accord.api.AccordRoutingKey.TokenKey;
+import org.apache.cassandra.service.accord.api.PartitionKey;
+import org.apache.cassandra.utils.Interval;
+import org.apache.cassandra.utils.IntervalTree;
+
+public class CommandsForRanges
+{
+    private static final class RangeCommandSummary
+    {
+        public final TxnId txnId;
+        public final SaveStatus status;
+        public final @Nullable Timestamp executeAt;
+        public final List<TxnId> deps;
+
+        RangeCommandSummary(TxnId txnId, SaveStatus status, @Nullable Timestamp executeAt, List<TxnId> deps)
+        {
+            this.txnId = txnId;
+            this.status = status;
+            this.executeAt = executeAt;
+            this.deps = deps;
+        }
+
+        @Override
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+            RangeCommandSummary that = (RangeCommandSummary) o;
+            return txnId.equals(that.txnId);
+        }
+
+        @Override
+        public int hashCode()
+        {
+            return Objects.hash(txnId);
+        }
+
+        @Override
+        public String toString()
+        {
+            return ""RangeCommandSummary{"" +
+                   ""txnId="" + txnId +
+                   "", status="" + status +
+                   '}';
+        }
+    }
+
+    private enum RangeCommandSummaryLoader implements CommandTimeseries.CommandLoader<RangeCommandSummary>
+    {
+        INSTANCE;
+
+        @Override
+        public RangeCommandSummary saveForCFK(Command command)
+        {
+            //TODO split write from read?
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public TxnId txnId(RangeCommandSummary data)
+        {
+            return data.txnId;
+        }
+
+        @Override
+        public Timestamp executeAt(RangeCommandSummary data)
+        {
+            return data.executeAt;
+        }
+
+        @Override
+        public SaveStatus saveStatus(RangeCommandSummary data)
+        {
+            return data.status;
+        }
+
+        @Override
+        public List<TxnId> depsIds(RangeCommandSummary data)
+        {
+            return data.deps;
+        }
+    }
+
+    public static abstract class AbstractBuilder<T extends AbstractBuilder<T>>
+    {
+        protected final IntervalTree.Builder<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> builder;
+
+        private AbstractBuilder(IntervalTree.Builder<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> builder)
+        {
+            this.builder = builder;
+        }
+
+        public T put(Ranges ranges, TxnId txnId, SaveStatus status, Timestamp execteAt, List<TxnId> dependsOn)
+        {
+            remove(txnId);
+            return put(ranges, new RangeCommandSummary(txnId, status, execteAt, dependsOn));
+        }
+
+        private T put(Ranges ranges, RangeCommandSummary summary)
+        {
+            for (Range range : ranges)
+                put(range, summary);
+            return (T) this;
+        }
+
+        private T put(Range range, RangeCommandSummary summary)
+        {
+            builder.add(Interval.create(normalize(range.start(), range.startInclusive(), true),
+                                        normalize(range.end(), range.endInclusive(), false),
+                                        summary));
+            return (T) this;
+        }
+
+        private T remove(TxnId txnId)
+        {
+            return removeIf(data -> data.txnId.equals(txnId));
+        }
+
+        private T removeIf(Predicate<RangeCommandSummary> predicate)
+        {
+            return removeIf((i1, i2, data) -> predicate.test(data));
+        }
+
+        private T removeIf(IntervalTree.Builder.TriPredicate<RoutableKey, RoutableKey, RangeCommandSummary> predicate)
+        {
+            builder.removeIf(predicate);
+            return (T) this;
+        }
+    }
+
+    public static class Builder extends AbstractBuilder<Builder>
+    {
+        public Builder()
+        {
+            super(new IntervalTree.Builder<>());
+        }
+
+        public CommandsForRanges build()
+        {
+            return new CommandsForRanges(builder.build());
+        }
+    }
+
+    public class Updater extends AbstractBuilder<Updater>
+    {
+        private Updater()
+        {
+            super(rangesToCommands.unbuild());
+        }
+
+        public void apply()
+        {
+            rangesToCommands = builder.build();
+        }
+    }
+
+    private IntervalTree<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> rangesToCommands;
+
+    public CommandsForRanges()
+    {
+        this(IntervalTree.emptyTree());
+    }
+
+    public CommandsForRanges(IntervalTree<RoutableKey, RangeCommandSummary, Interval<RoutableKey, RangeCommandSummary>> rangesToCommands)
+    {
+        this.rangesToCommands = rangesToCommands;
+    }
+
+    public Iterable<CommandTimeseriesHolder> search(AbstractKeys<Key, ?> keys)
+    {
+        // group by the keyspace, as ranges are based off TokenKey, which is scoped to a range
+        Map<String, List<Key>> groupByKeyspace = new TreeMap<>();
+        for (Key key : keys)
+            groupByKeyspace.computeIfAbsent(((PartitionKey) key).keyspace(), ignore -> new ArrayList<>()).add(key);
+        return () -> new AbstractIterator<CommandTimeseriesHolder>()
+        {
+            Iterator<String> ksIt = groupByKeyspace.keySet().iterator();
+            Iterator<Map.Entry<Range, Set<RangeCommandSummary>>> rangeIt;
+
+            @Override
+            protected CommandTimeseriesHolder computeNext()
+            {
+                while (true)
+                {
+                    if (rangeIt != null && rangeIt.hasNext())
+                    {
+                        Map.Entry<Range, Set<RangeCommandSummary>> next = rangeIt.next();
+                        return result(next.getKey(), next.getValue());
+                    }
+                    rangeIt = null;
+                    if (!ksIt.hasNext())
+                    {
+                        ksIt = null;
+                        return endOfData();
+                    }
+                    String ks = ksIt.next();
+                    List<Key> keys = groupByKeyspace.get(ks);
+                    Map<Range, Set<RangeCommandSummary>> groupByRange = new TreeMap<>(Range::compare);
+                    for (Key key : keys)
+                    {
+                        List<Interval<RoutableKey, RangeCommandSummary>> matches = rangesToCommands.matches(key);
+                        if (matches.isEmpty())
+                            continue;
+                        for (Interval<RoutableKey, RangeCommandSummary> interval : matches)
+                            groupByRange.computeIfAbsent(toRange(interval), ignore -> new HashSet<>()).add(interval.data);
+                    }
+                    rangeIt = groupByRange.entrySet().iterator();
+                }
+            }
+        };
+    }
+
+    private static Range toRange(Interval<RoutableKey, RangeCommandSummary> interval)
+    {
+        TokenKey start = (TokenKey) interval.min;
+        TokenKey end = (TokenKey) interval.max;
+        // TODO (correctness) : accord doesn't support wrap around, so decreaseSlightly may fail in some cases","[{'comment': 'Just leaving a comment here so I remember to check the outcome of this', 'commenter': 'aweisberg'}]"
2352,checkstyle_test.xml,"@@ -118,6 +118,13 @@
       <property name=""message"" value=""Use the CassandraRelevantProperties or CassandraRelevantEnv instead."" />
     </module>
 
+    <module name=""RegexpSinglelineJava"">
+      <property name=""id"" value=""clearValueSystemPropertyUsage""/>
+      <property name=""format"" value=""\.clearValue\(""/>
+      <property name=""ignoreComments"" value=""true""/>
+      <property name=""message"" value=""Please use withProperties approach instead."" />","[{'comment': '@bbotella please change the message to \r\n\r\n`Please use WithProperties in try-with-resources instead. See CASSANDRA-18453.`', 'commenter': 'smiklosovic'}]"
2352,test/distributed/org/apache/cassandra/distributed/test/MessageForwardingTest.java,"@@ -122,6 +122,7 @@ else if (traceEntry.activity.contains(""Enqueuing forwarded write to ""))
         finally
         {
             if (originalTraceTimeout == null)
+                // checkstyle: suppress below 'clearValueSystemPropertyUsage'","[{'comment': 'I think this case can also be handled by `WithProperties`, right? ', 'commenter': 'Mmuzaf'}]"
2352,test/distributed/org/apache/cassandra/distributed/action/GossipHelper.java,"@@ -473,6 +473,7 @@ public static void withProperty(CassandraRelevantProperties prop, boolean value,
         withProperty(prop, Boolean.toString(value), r);
     }
 
+    // checkstyle: suppress below 'clearValueSystemPropertyUsage'","[{'comment': 'The `checkstyle: suppress below ` uses `offCommentFormat` here [1], which means that all the checks below this comment are suppressed. This makes the checks wildly inaccurate, as a developer can still use `clearValue` and get no errors on the build. \r\n\r\nShould we use `checkstyle: suppress nearby` for all such places?\r\n\r\n[1] https://checkstyle.sourceforge.io/apidocs/com/puppycrawl/tools/checkstyle/filters/SuppressionCommentFilter.html', 'commenter': 'Mmuzaf'}, {'comment': 'sure, good idea.', 'commenter': 'smiklosovic'}]"
2352,test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java,"@@ -259,6 +259,7 @@ public void testInvalidPartitionPropertyOverride() throws Exception
         }
         finally
         {
+            // checkstyle: suppress below 'clearValueSystemPropertyUsage'","[{'comment': 'I think we can handle it instead, right?', 'commenter': 'Mmuzaf'}]"
2352,test/unit/org/apache/cassandra/net/ConnectionTest.java,"@@ -651,6 +651,7 @@ public void testPendingOutboundConnectionUpdatesMessageVersionOnReconnectAttempt
         {
             MessagingService.instance().versions.set(FBUtilities.getBroadcastAddressAndPort(),
                                                      current_version);
+            // checkstyle: suppress below 'clearValueSystemPropertyUsage'","[{'comment': 'The same as above, it looks like we can handle it :-)', 'commenter': 'Mmuzaf'}]"
2352,test/unit/org/apache/cassandra/db/commitlog/CommitLogTest.java,"@@ -894,12 +891,11 @@ public void handleMutation(Mutation m, int size, int entryLocation, CommitLogDes
         }
     }
 
-    private void assertReplay(int expectedReplayedMutations, Runnable systemPropertySetter) throws Throwable
+    // private void assertReplay(int expectedReplayedMutations, Runnable systemPropertySetter) throws Throwable","[{'comment': 'I think we can remove this comment :-)', 'commenter': 'Mmuzaf'}, {'comment': 'LOL yeah...!', 'commenter': 'bbotella'}]"
2352,test/distributed/org/apache/cassandra/distributed/test/FailingResponseDoesNotLogTest.java,"@@ -70,13 +71,13 @@ public static void beforeClassTopLevel() // need to make sure not to conflict wi
     @Test
     public void dispatcherErrorDoesNotLock() throws IOException
     {
-        CUSTOM_QUERY_HANDLER_CLASS.setString(AlwaysRejectErrorQueryHandler.class.getName());
-        try (Cluster cluster = Cluster.build(1)
-                                      .withConfig(c -> c.with(Feature.NATIVE_PROTOCOL, Feature.GOSSIP)
-                                                        .set(""client_error_reporting_exclusions"", ImmutableMap.of(""subnets"", Collections.singletonList(""127.0.0.1"")))
-                                      )
-                                      .start())
+        try (WithProperties properties = new WithProperties().set(CUSTOM_QUERY_HANDLER_CLASS, AlwaysRejectErrorQueryHandler.class.getName()))","[{'comment': 'Should cluster creation be included in try-with-resources?', 'commenter': 'jacek-lewandowski'}]"
2352,test/distributed/org/apache/cassandra/distributed/test/NativeMixedVersionTest.java,"@@ -41,15 +42,15 @@ public void v4ConnectionCleansUpThreadLocalState() throws IOException
     {
         // make sure to limit the netty thread pool to size 1, this will make the test determanistic as all work
         // will happen on the single thread.
-        IO_NETTY_EVENTLOOP_THREADS.setInt(1);
-        try (Cluster cluster = Cluster.build(1)
-                                      .withConfig(c ->
-                                                  c.with(Feature.values())
-                                                   .set(""read_thresholds_enabled"", true)
-                                                   .set(""local_read_size_warn_threshold"", ""1KiB"")
-                                      )
-                                      .start())
+        try (WithProperties properties = new WithProperties().set(IO_NETTY_EVENTLOOP_THREADS, 1))
         {
+            Cluster cluster = Cluster.build(1)","[{'comment': 'again.. leaves cluster unclosed', 'commenter': 'jacek-lewandowski'}]"
2352,test/distributed/org/apache/cassandra/distributed/test/PaxosRepair2Test.java,"@@ -333,14 +335,12 @@ private static void assertLowBoundPurged(Cluster cluster)
     @Test
     public void paxosAutoRepair() throws Throwable
     {
-        AUTO_REPAIR_FREQUENCY_SECONDS.setInt(1);
-        DISABLE_PAXOS_AUTO_REPAIRS.setBoolean(true);
-        try (Cluster cluster = init(Cluster.create(3, cfg -> cfg
-                                                             .set(""paxos_variant"", ""v2"")
-                                                             .set(""paxos_repair_enabled"", true)
-                                                             .set(""truncate_request_timeout_in_ms"", 1000L)))
-        )
+        try (WithProperties properties = new WithProperties().set(AUTO_REPAIR_FREQUENCY_SECONDS, 1).set(DISABLE_PAXOS_AUTO_REPAIRS, true))
         {
+            Cluster cluster = init(Cluster.create(3, cfg -> cfg","[{'comment': 'same', 'commenter': 'jacek-lewandowski'}]"
2352,test/distributed/org/apache/cassandra/distributed/test/MessageForwardingTest.java,"@@ -122,7 +122,7 @@ else if (traceEntry.activity.contains(""Enqueuing forwarded write to ""))
         finally
         {
             if (originalTraceTimeout == null)
-                WAIT_FOR_TRACING_EVENTS_TIMEOUT_SECS.clearValue();
+                WAIT_FOR_TRACING_EVENTS_TIMEOUT_SECS.clearValue(); // checkstyle: suppress nearby 'clearValueSystemPropertyUsage'","[{'comment': 'why not use `WithProperties` in this class as well?', 'commenter': 'jacek-lewandowski'}]"
2352,test/distributed/org/apache/cassandra/distributed/test/FailingResponseDoesNotLogTest.java,"@@ -70,13 +71,16 @@ public static void beforeClassTopLevel() // need to make sure not to conflict wi
     @Test
     public void dispatcherErrorDoesNotLock() throws IOException
     {
-        CUSTOM_QUERY_HANDLER_CLASS.setString(AlwaysRejectErrorQueryHandler.class.getName());
-        try (Cluster cluster = Cluster.build(1)
-                                      .withConfig(c -> c.with(Feature.NATIVE_PROTOCOL, Feature.GOSSIP)
-                                                        .set(""client_error_reporting_exclusions"", ImmutableMap.of(""subnets"", Collections.singletonList(""127.0.0.1"")))
-                                      )
-                                      .start())
+        try (
+            WithProperties properties = new WithProperties().set(CUSTOM_QUERY_HANDLER_CLASS, AlwaysRejectErrorQueryHandler.class.getName());","[{'comment': 'nit: formatting  - should it be in the same line as try?', 'commenter': 'jacek-lewandowski'}]"
2352,test/distributed/org/apache/cassandra/distributed/test/BootstrapBinaryDisabledTest.java,"@@ -62,7 +62,7 @@ public static void beforeClass() throws Throwable
     public static void afterClass()
     {
         if (originalResetBootstrapProgress == null)
-            RESET_BOOTSTRAP_PROGRESS.clearValue();
+            RESET_BOOTSTRAP_PROGRESS.clearValue(); // checkstyle: suppress nearby 'clearValueSystemPropertyUsage'","[{'comment': 'use `WithProperties` ?', 'commenter': 'jacek-lewandowski'}]"
2352,test/distributed/org/apache/cassandra/distributed/test/ring/BootstrapTest.java,"@@ -84,7 +84,7 @@ public void afterTest()
     {
         CassandraRelevantProperties.MIGRATION_DELAY.setLong(savedMigrationDelay);
         if (originalResetBootstrapProgress == null)
-            RESET_BOOTSTRAP_PROGRESS.clearValue();
+            RESET_BOOTSTRAP_PROGRESS.clearValue(); // checkstyle: suppress nearby 'clearValueSystemPropertyUsage'","[{'comment': 'Why not use `WithProperties`?', 'commenter': 'jacek-lewandowski'}]"
2352,test/unit/org/apache/cassandra/config/CassandraRelevantPropertiesTest.java,"@@ -88,92 +79,65 @@ public void testBoolean()
     @Test
     public void testBoolean_null()
     {
-        try
+        try (WithProperties properties = new WithProperties())
         {
             TEST_CASSANDRA_RELEVANT_PROPERTIES.getBoolean();","[{'comment': 'btw. there is missing assertion in this test case', 'commenter': 'jacek-lewandowski'}, {'comment': 'Will add it.', 'commenter': 'bbotella'}]"
2352,test/unit/org/apache/cassandra/service/AbstractFilesystemOwnershipCheckTest.java,"@@ -220,7 +220,7 @@ public void checkEnabledButClusterPropertyIsEmpty()
     public void checkEnabledButClusterPropertyIsUnset()
     {
         Assume.assumeFalse(options.getConfig(check_filesystem_ownership).containsKey(""ownership_token""));
-        CassandraRelevantProperties.FILE_SYSTEM_CHECK_OWNERSHIP_TOKEN.clearValue();
+        CassandraRelevantProperties.FILE_SYSTEM_CHECK_OWNERSHIP_TOKEN.clearValue(); // checkstyle: suppress nearby 'clearValueSystemPropertyUsage'","[{'comment': 'WithProperties should be used in this class as well', 'commenter': 'jacek-lewandowski'}]"
2374,src/java/org/apache/cassandra/service/StorageService.java,"@@ -5127,100 +5128,113 @@ private void startLeaving()
 
     public void decommission(boolean force) throws InterruptedException
     {
-        TokenMetadata metadata = tokenMetadata.cloneAfterAllLeft();
-        if (operationMode != Mode.LEAVING)
+        try
         {
-            if (!tokenMetadata.isMember(FBUtilities.getBroadcastAddressAndPort()))
-                throw new UnsupportedOperationException(""local node is not a member of the token ring yet"");
-            if (metadata.getAllEndpoints().size() < 2)
+            TokenMetadata metadata = tokenMetadata.cloneAfterAllLeft();
+            if (operationMode != Mode.LEAVING)
+            {
+                if (!tokenMetadata.isMember(FBUtilities.getBroadcastAddressAndPort()))
+                    throw new UnsupportedOperationException(""local node is not a member of the token ring yet"");
+                if (metadata.getAllEndpoints().size() < 2)
                     throw new UnsupportedOperationException(""no other normal nodes in the ring; decommission would be pointless"");
-            if (operationMode != Mode.NORMAL)
-                throw new UnsupportedOperationException(""Node in "" + operationMode + "" state; wait for status to become normal or restart"");
-        }
-        if (!isDecommissioning.compareAndSet(false, true))
-            throw new IllegalStateException(""Node is still decommissioning. Check nodetool netstats."");
+                if (operationMode != Mode.NORMAL)
+                    throw new UnsupportedOperationException(""Node in "" + operationMode + "" state; wait for status to become normal or restart"");
+            }
+            if (!isDecommissioning.compareAndSet(false, true))
+                throw new IllegalStateException(""Node is still decommissioning. Check nodetool netstats."");
 
-        if (logger.isDebugEnabled())
-            logger.debug(""DECOMMISSIONING"");
+            if (logger.isDebugEnabled())
+                logger.debug(""DECOMMISSIONING"");
 
-        try
-        {
-            PendingRangeCalculatorService.instance.blockUntilFinished();
+            try
+            {
+                PendingRangeCalculatorService.instance.blockUntilFinished();
 
-            String dc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
+                String dc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
 
-            if (operationMode != Mode.LEAVING) // If we're already decommissioning there is no point checking RF/pending ranges
-            {
-                int rf, numNodes;
-                for (String keyspaceName : Schema.instance.getNonLocalStrategyKeyspaces().names())
+                if (operationMode != Mode.LEAVING) // If we're already decommissioning there is no point checking RF/pending ranges
                 {
-                    if (!force)
+                    int rf, numNodes;
+                    for (String keyspaceName : Schema.instance.getNonLocalStrategyKeyspaces().names())
                     {
-                        Keyspace keyspace = Keyspace.open(keyspaceName);
-                        if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)
+                        if (!force)
                         {
-                            NetworkTopologyStrategy strategy = (NetworkTopologyStrategy) keyspace.getReplicationStrategy();
-                            rf = strategy.getReplicationFactor(dc).allReplicas;
-                            numNodes = metadata.getTopology().getDatacenterEndpoints().get(dc).size();
-                        }
-                        else
-                        {
-                            numNodes = metadata.getAllEndpoints().size();
-                            rf = keyspace.getReplicationStrategy().getReplicationFactor().allReplicas;
-                        }
+                            Keyspace keyspace = Keyspace.open(keyspaceName);
+                            if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)
+                            {
+                                NetworkTopologyStrategy strategy = (NetworkTopologyStrategy) keyspace.getReplicationStrategy();
+                                rf = strategy.getReplicationFactor(dc).allReplicas;
+                                numNodes = metadata.getTopology().getDatacenterEndpoints().get(dc).size();
+                            }
+                            else
+                            {
+                                numNodes = metadata.getAllEndpoints().size();
+                                rf = keyspace.getReplicationStrategy().getReplicationFactor().allReplicas;
+                            }
 
-                        if (numNodes <= rf)
-                            throw new UnsupportedOperationException(""Not enough live nodes to maintain replication factor in keyspace ""
-                                                                    + keyspaceName + "" (RF = "" + rf + "", N = "" + numNodes + "").""
-                                                                    + "" Perform a forceful decommission to ignore."");
+                            if (numNodes <= rf)
+                                throw new UnsupportedOperationException(""Not enough live nodes to maintain replication factor in keyspace ""
+                                                                        + keyspaceName + "" (RF = "" + rf + "", N = "" + numNodes + "").""
+                                                                        + "" Perform a forceful decommission to ignore."");
+                        }
+                        // TODO: do we care about fixing transient/full self-movements here? probably
+                        if (tokenMetadata.getPendingRanges(keyspaceName, FBUtilities.getBroadcastAddressAndPort()).size() > 0)
+                            throw new UnsupportedOperationException(""data is currently moving to this node; unable to leave the ring"");
                     }
-                    // TODO: do we care about fixing transient/full self-movements here? probably
-                    if (tokenMetadata.getPendingRanges(keyspaceName, FBUtilities.getBroadcastAddressAndPort()).size() > 0)
-                        throw new UnsupportedOperationException(""data is currently moving to this node; unable to leave the ring"");
                 }
-            }
 
-            startLeaving();
-            long timeout = Math.max(RING_DELAY_MILLIS, BatchlogManager.instance.getBatchlogTimeout());
-            setMode(Mode.LEAVING, ""sleeping "" + timeout + "" ms for batch processing and pending range setup"", true);
-            Thread.sleep(timeout);
+                startLeaving();
+                long timeout = Math.max(RING_DELAY_MILLIS, BatchlogManager.instance.getBatchlogTimeout());
+                setMode(Mode.LEAVING, ""sleeping "" + timeout + "" ms for batch processing and pending range setup"", true);
+                Thread.sleep(timeout);
 
-            Runnable finishLeaving = new Runnable()
-            {
-                public void run()
+                Runnable finishLeaving = new Runnable()
                 {
-                    shutdownClientServers();
-                    Gossiper.instance.stop();
-                    try
-                    {
-                        MessagingService.instance().shutdown();
-                    }
-                    catch (IOError ioe)
+                    public void run()
                     {
-                        logger.info(""failed to shutdown message service: {}"", ioe);
-                    }
+                        shutdownClientServers();
+                        Gossiper.instance.stop();
+                        try
+                        {
+                            MessagingService.instance().shutdown();
+                        }
+                        catch (IOError ioe)
+                        {
+                            logger.info(""failed to shutdown message service: {}"", ioe);
+                        }
 
-                    Stage.shutdownNow();
-                    SystemKeyspace.setBootstrapState(SystemKeyspace.BootstrapState.DECOMMISSIONED);
-                    setMode(Mode.DECOMMISSIONED, true);
-                    // let op be responsible for killing the process
-                }
-            };
-            unbootstrap(finishLeaving);
-        }
-        catch (InterruptedException e)
-        {
-            throw new UncheckedInterruptedException(e);
-        }
-        catch (ExecutionException e)
-        {
-            logger.error(""Error while decommissioning node "", e.getCause());
-            throw new RuntimeException(""Error while decommissioning node: "" + e.getCause().getMessage());
-        }
-        finally
-        {
-            isDecommissioning.set(false);
-        }
+                        Stage.shutdownNow();
+                        SystemKeyspace.setBootstrapState(SystemKeyspace.BootstrapState.DECOMMISSIONED);
+                        setMode(Mode.DECOMMISSIONED, true);
+                        // let op be responsible for killing the process
+                    }
+                };
+                unbootstrap(finishLeaving);
+            }
+            catch (InterruptedException e)
+            {
+                throw new UncheckedInterruptedException(e);
+            }
+            catch (ExecutionException e)
+            {
+                logger.error(""Error while decommissioning node "", e.getCause());
+                throw new RuntimeException(""Error while decommissioning node: "" + e.getCause().getMessage());
+            }
+            finally
+            {
+                isDecommissioning.set(false);
+            }
+        } catch (Exception e)","[{'comment': ""I think there is a problem for code format with cath '{'"", 'commenter': 'Maxwell-Guo'}, {'comment': 'Besides , can we use Throwable instand of Exception ？', 'commenter': 'Maxwell-Guo'}]"
2374,src/java/org/apache/cassandra/service/StorageService.java,"@@ -5236,6 +5250,21 @@ private void leaveRing()
         Uninterruptibles.sleepUninterruptibly(delay, MILLISECONDS);
     }
 
+    public boolean isDecommissionFailed()
+    {
+        if (operationMode == Mode.LEAVING && hasDecommissionFailed)","[{'comment': 'I think we can change this to `if (hasDecommissionFailed && operationMode == Mode.LEAVING)`\r\nif hasDecommissionFailed is false we will not need to do the comparison with operationMode\r\nEdit :\r\nor why we just return \r\nhasDecommissionFailed && operationMode == Mode.LEAVING\r\n? and not need for if () else \r\n', 'commenter': 'Maxwell-Guo'}]"
2374,src/java/org/apache/cassandra/service/StorageService.java,"@@ -433,6 +433,7 @@ public enum Mode { STARTING, NORMAL, JOINING, LEAVING, DECOMMISSIONED, MOVING, D
     private static final boolean allowSimultaneousMoves = CONSISTENT_SIMULTANEOUS_MOVES_ALLOW.getBoolean();
     private static final boolean joinRing = JOIN_RING.getBoolean();
     private boolean replacing;
+    private volatile boolean hasDecommissionFailed = false;","[{'comment': 'I think we can change the name to isDecommissionFailed as src/java/org/apache/cassandra/service/StorageServiceMBean.java  method isDecommissionFailed();\r\nEdit :\r\ncan we just AtomicBoolean ', 'commenter': 'Maxwell-Guo'}]"
2374,test/unit/org/apache/cassandra/service/StorageServiceServerTest.java,"@@ -658,4 +660,26 @@ public void isReplacingSameHostAddressAndHostIdTest() throws UnknownHostExceptio
             Assert.assertFalse(StorageService.instance.isReplacingSameHostAddressAndHostId(differentHostId));
         }
     }
-}
\ No newline at end of file
+
+    @Test
+    public void testIsDecommissionNotFailed()
+    {
+        assertFalse(StorageService.instance.isDecommissionFailed());
+        assertEquals(0, StorageMetrics.errorDecommissiong.getCount());
+    }
+
+    @Test","[{'comment': ""I think we can also add a new test case use ToolRunner.invokeNodetool see GossipInfoTest\r\nwe can use at frist set the node's mode to Un-normal and then use ToolRunner.invokeNodetool  to get the status of descommission.\r\n"", 'commenter': 'Maxwell-Guo'}]"
2374,src/java/org/apache/cassandra/tools/NodeTool.java,"@@ -163,6 +163,7 @@ public int execute(String... args)
                 InvalidatePermissionsCache.class,
                 InvalidateRolesCache.class,
                 InvalidateRowCache.class,
+                IsDecommissionFailed.class,","[{'comment': '@jaydeepkumar1984 where is this class? I do not see it implemented as part of this PR?', 'commenter': 'smiklosovic'}, {'comment': 'right, I think this patch may fail to compile. It is need for us to executed  test locally before post the pr.', 'commenter': 'Maxwell-Guo'}]"
2374,src/java/org/apache/cassandra/service/StorageService.java,"@@ -5127,100 +5128,113 @@ private void startLeaving()
 
     public void decommission(boolean force) throws InterruptedException
     {
-        TokenMetadata metadata = tokenMetadata.cloneAfterAllLeft();
-        if (operationMode != Mode.LEAVING)
+        try
         {
-            if (!tokenMetadata.isMember(FBUtilities.getBroadcastAddressAndPort()))
-                throw new UnsupportedOperationException(""local node is not a member of the token ring yet"");
-            if (metadata.getAllEndpoints().size() < 2)
+            TokenMetadata metadata = tokenMetadata.cloneAfterAllLeft();
+            if (operationMode != Mode.LEAVING)
+            {
+                if (!tokenMetadata.isMember(FBUtilities.getBroadcastAddressAndPort()))
+                    throw new UnsupportedOperationException(""local node is not a member of the token ring yet"");
+                if (metadata.getAllEndpoints().size() < 2)
                     throw new UnsupportedOperationException(""no other normal nodes in the ring; decommission would be pointless"");
-            if (operationMode != Mode.NORMAL)
-                throw new UnsupportedOperationException(""Node in "" + operationMode + "" state; wait for status to become normal or restart"");
-        }
-        if (!isDecommissioning.compareAndSet(false, true))
-            throw new IllegalStateException(""Node is still decommissioning. Check nodetool netstats."");
+                if (operationMode != Mode.NORMAL)
+                    throw new UnsupportedOperationException(""Node in "" + operationMode + "" state; wait for status to become normal or restart"");
+            }
+            if (!isDecommissioning.compareAndSet(false, true))
+                throw new IllegalStateException(""Node is still decommissioning. Check nodetool netstats."");
 
-        if (logger.isDebugEnabled())
-            logger.debug(""DECOMMISSIONING"");
+            if (logger.isDebugEnabled())
+                logger.debug(""DECOMMISSIONING"");
 
-        try
-        {
-            PendingRangeCalculatorService.instance.blockUntilFinished();
+            try
+            {
+                PendingRangeCalculatorService.instance.blockUntilFinished();
 
-            String dc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
+                String dc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
 
-            if (operationMode != Mode.LEAVING) // If we're already decommissioning there is no point checking RF/pending ranges
-            {
-                int rf, numNodes;
-                for (String keyspaceName : Schema.instance.getNonLocalStrategyKeyspaces().names())
+                if (operationMode != Mode.LEAVING) // If we're already decommissioning there is no point checking RF/pending ranges
                 {
-                    if (!force)
+                    int rf, numNodes;
+                    for (String keyspaceName : Schema.instance.getNonLocalStrategyKeyspaces().names())
                     {
-                        Keyspace keyspace = Keyspace.open(keyspaceName);
-                        if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)
+                        if (!force)
                         {
-                            NetworkTopologyStrategy strategy = (NetworkTopologyStrategy) keyspace.getReplicationStrategy();
-                            rf = strategy.getReplicationFactor(dc).allReplicas;
-                            numNodes = metadata.getTopology().getDatacenterEndpoints().get(dc).size();
-                        }
-                        else
-                        {
-                            numNodes = metadata.getAllEndpoints().size();
-                            rf = keyspace.getReplicationStrategy().getReplicationFactor().allReplicas;
-                        }
+                            Keyspace keyspace = Keyspace.open(keyspaceName);
+                            if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)
+                            {
+                                NetworkTopologyStrategy strategy = (NetworkTopologyStrategy) keyspace.getReplicationStrategy();
+                                rf = strategy.getReplicationFactor(dc).allReplicas;
+                                numNodes = metadata.getTopology().getDatacenterEndpoints().get(dc).size();
+                            }
+                            else
+                            {
+                                numNodes = metadata.getAllEndpoints().size();
+                                rf = keyspace.getReplicationStrategy().getReplicationFactor().allReplicas;
+                            }
 
-                        if (numNodes <= rf)
-                            throw new UnsupportedOperationException(""Not enough live nodes to maintain replication factor in keyspace ""
-                                                                    + keyspaceName + "" (RF = "" + rf + "", N = "" + numNodes + "").""
-                                                                    + "" Perform a forceful decommission to ignore."");
+                            if (numNodes <= rf)
+                                throw new UnsupportedOperationException(""Not enough live nodes to maintain replication factor in keyspace ""
+                                                                        + keyspaceName + "" (RF = "" + rf + "", N = "" + numNodes + "").""
+                                                                        + "" Perform a forceful decommission to ignore."");
+                        }
+                        // TODO: do we care about fixing transient/full self-movements here? probably
+                        if (tokenMetadata.getPendingRanges(keyspaceName, FBUtilities.getBroadcastAddressAndPort()).size() > 0)
+                            throw new UnsupportedOperationException(""data is currently moving to this node; unable to leave the ring"");
                     }
-                    // TODO: do we care about fixing transient/full self-movements here? probably
-                    if (tokenMetadata.getPendingRanges(keyspaceName, FBUtilities.getBroadcastAddressAndPort()).size() > 0)
-                        throw new UnsupportedOperationException(""data is currently moving to this node; unable to leave the ring"");
                 }
-            }
 
-            startLeaving();
-            long timeout = Math.max(RING_DELAY_MILLIS, BatchlogManager.instance.getBatchlogTimeout());
-            setMode(Mode.LEAVING, ""sleeping "" + timeout + "" ms for batch processing and pending range setup"", true);
-            Thread.sleep(timeout);
+                startLeaving();
+                long timeout = Math.max(RING_DELAY_MILLIS, BatchlogManager.instance.getBatchlogTimeout());
+                setMode(Mode.LEAVING, ""sleeping "" + timeout + "" ms for batch processing and pending range setup"", true);
+                Thread.sleep(timeout);","[{'comment': 'maybe it is good time to sleep uninterruptibly? Look into Uninterruptibles in google guava ', 'commenter': 'smiklosovic'}]"
2374,src/java/org/apache/cassandra/metrics/StorageMetrics.java,"@@ -45,6 +45,7 @@
     public static final Counter totalHintsInProgress  = Metrics.counter(factory.createMetricName(""TotalHintsInProgress""));
     public static final Counter totalHints = Metrics.counter(factory.createMetricName(""TotalHints""));
     public static final Counter repairExceptions = Metrics.counter(factory.createMetricName(""RepairExceptions""));
+    public static final Counter errorDecommissiong = Metrics.counter(factory.createMetricName(""ErrorDecommissioning""));","[{'comment': '@jaydeepkumar1984 Why do we need metrics for knowing if decommission failed? What do you plan to do if you determine that the decommission has failed?', 'commenter': 'smiklosovic'}]"
2374,src/java/org/apache/cassandra/service/StorageService.java,"@@ -5127,100 +5128,113 @@ private void startLeaving()
 
     public void decommission(boolean force) throws InterruptedException
     {
-        TokenMetadata metadata = tokenMetadata.cloneAfterAllLeft();
-        if (operationMode != Mode.LEAVING)
+        try
         {
-            if (!tokenMetadata.isMember(FBUtilities.getBroadcastAddressAndPort()))
-                throw new UnsupportedOperationException(""local node is not a member of the token ring yet"");
-            if (metadata.getAllEndpoints().size() < 2)
+            TokenMetadata metadata = tokenMetadata.cloneAfterAllLeft();
+            if (operationMode != Mode.LEAVING)
+            {
+                if (!tokenMetadata.isMember(FBUtilities.getBroadcastAddressAndPort()))
+                    throw new UnsupportedOperationException(""local node is not a member of the token ring yet"");
+                if (metadata.getAllEndpoints().size() < 2)
                     throw new UnsupportedOperationException(""no other normal nodes in the ring; decommission would be pointless"");
-            if (operationMode != Mode.NORMAL)
-                throw new UnsupportedOperationException(""Node in "" + operationMode + "" state; wait for status to become normal or restart"");
-        }
-        if (!isDecommissioning.compareAndSet(false, true))
-            throw new IllegalStateException(""Node is still decommissioning. Check nodetool netstats."");
+                if (operationMode != Mode.NORMAL)
+                    throw new UnsupportedOperationException(""Node in "" + operationMode + "" state; wait for status to become normal or restart"");
+            }
+            if (!isDecommissioning.compareAndSet(false, true))","[{'comment': ""I think we can not include some code into the try catch , like line 5143 \r\n` if (!isDecommissioning.compareAndSet(false, true))`\r\nfirst time you have execute a decommission, and the node is being doing decommission which may take a long time ,\r\nand after a while you may execute another decommission , you will got a false. But actually the node's decommission is be doing now and have not failed. \r\nyou will get a wrong status."", 'commenter': 'Maxwell-Guo'}]"
2375,src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java,"@@ -48,29 +53,82 @@ public class CompactionStats extends NodeToolCmd
     public void execute(NodeProbe probe)
     {
         PrintStream out = probe.output().out;
+        out.print(pendingTasksAndConcurrentCompactorsStats(probe));
+        out.print(compactionsCompletedStats(probe));
+        out.print(compactionThroughputStats(probe));
+        out.println();
         CompactionManagerMBean cm = probe.getCompactionManagerProxy();
+        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughput(), humanReadable, out);
+    }
+
+    private static String pendingTasksAndConcurrentCompactorsStats(NodeProbe probe)
+    {
         Map<String, Map<String, Integer>> pendingTaskNumberByTable =
-            (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
-        int numTotalPendingTask = 0;
+                (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""%s concurrent compactors, %s pending tasks"", probe.getConcurrentCompactors()
+                , numPendingTasks(pendingTaskNumberByTable)));","[{'comment': ""we should make ',' aligned with %s"", 'commenter': 'Maxwell-Guo'}]"
2375,src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java,"@@ -48,29 +53,82 @@ public class CompactionStats extends NodeToolCmd
     public void execute(NodeProbe probe)
     {
         PrintStream out = probe.output().out;
+        out.print(pendingTasksAndConcurrentCompactorsStats(probe));
+        out.print(compactionsCompletedStats(probe));
+        out.print(compactionThroughputStats(probe));
+        out.println();
         CompactionManagerMBean cm = probe.getCompactionManagerProxy();
+        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughput(), humanReadable, out);
+    }
+
+    private static String pendingTasksAndConcurrentCompactorsStats(NodeProbe probe)
+    {
         Map<String, Map<String, Integer>> pendingTaskNumberByTable =
-            (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
-        int numTotalPendingTask = 0;
+                (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
+        StringBuffer toPrint = new StringBuffer();
+        toPrint.append(String.format(""%s concurrent compactors, %s pending tasks"", probe.getConcurrentCompactors()
+                , numPendingTasks(pendingTaskNumberByTable)));
+        toPrint.append(LINE_SEPARATOR);
         for (Entry<String, Map<String, Integer>> ksEntry : pendingTaskNumberByTable.entrySet())
         {
+            String ksName = ksEntry.getKey();
             for (Entry<String, Integer> tableEntry : ksEntry.getValue().entrySet())
-                numTotalPendingTask += tableEntry.getValue();
+            {
+                toPrint.append(""- "" + ksName + '.' + tableEntry.getKey() + "": "" + tableEntry.getValue());","[{'comment': 'should we use String format?', 'commenter': 'Maxwell-Guo'}, {'comment': 'Looks like this was missed?', 'commenter': 'driftx'}, {'comment': 'This is not latest. Tablebuilder is used in latest changes. I pushed those changes few days back. I am not aure why they are not visible. Let me check.', 'commenter': 'mghildiy'}, {'comment': 'I see, the other commit is on top of this one, okay, that works.', 'commenter': 'driftx'}]"
2375,src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java,"@@ -48,37 +50,73 @@ public class CompactionStats extends NodeToolCmd
     public void execute(NodeProbe probe)
     {
         PrintStream out = probe.output().out;
+        TableBuilder tableBuilder = new TableBuilder();
+        pendingTasksAndConcurrentCompactorsStats(probe, tableBuilder);
+        compactionsCompletedStats(probe, tableBuilder);
+        compactionThroughputStats(probe, tableBuilder);
+        tableBuilder.printTo(out);
         CompactionManagerMBean cm = probe.getCompactionManagerProxy();
+        reportCompactionTable(cm.getCompactions(), probe.getCompactionThroughput(), humanReadable, out, tableBuilder);
+    }
+
+    private void pendingTasksAndConcurrentCompactorsStats(NodeProbe probe, TableBuilder tableBuilder)
+    {
         Map<String, Map<String, Integer>> pendingTaskNumberByTable =
-            (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");
-        int numTotalPendingTask = 0;
+                (Map<String, Map<String, Integer>>) probe.getCompactionMetric(""PendingTasksByTableName"");","[{'comment': ""Shouldn't the 4 spaces be removed at the beginning of line 65?"", 'commenter': 'Maxwell-Guo'}]"
2395,src/java/org/apache/cassandra/service/accord/AccordStateCache.java,"@@ -15,547 +15,442 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.cassandra.service.accord;
 
 import java.util.HashMap;
-import java.util.HashSet;
+import java.util.Iterator;
 import java.util.Map;
-import java.util.Objects;
-import java.util.Set;
+import java.util.function.BiFunction;
 import java.util.function.Function;
 import java.util.function.ToLongFunction;
 import java.util.stream.Stream;
 
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import accord.utils.Invariants;
-import accord.utils.async.AsyncChains;
-import accord.utils.async.AsyncResult;
-import org.apache.cassandra.utils.ObjectSizes;
+import accord.utils.IntrusiveLinkedList;
+import org.apache.cassandra.concurrent.ExecutorPlus;
+import org.apache.cassandra.service.accord.AccordCachingState.Status;
 
-import static org.apache.cassandra.service.accord.AccordLoadingState.LoadingState.FAILED;
-import static org.apache.cassandra.service.accord.AccordLoadingState.LoadingState.LOADED;
+import static accord.utils.Invariants.checkState;
+import static java.lang.String.format;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.EVICTED;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.FAILED_TO_LOAD;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.LOADING;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.SAVING;
 
 /**
  * Cache for AccordCommand and AccordCommandsForKey, available memory is shared between the two object types.
- *
+ * </p>
  * Supports dynamic object sizes. After each acquire/free cycle, the cacheable objects size is recomputed to
  * account for data added/removed during txn processing if it's modified flag is set
  */
-public class AccordStateCache
+public class AccordStateCache extends IntrusiveLinkedList<AccordCachingState<?,?>>
 {
     private static final Logger logger = LoggerFactory.getLogger(AccordStateCache.class);
 
-    public static class Node<K, V> extends AccordLoadingState<K, V>
-    {
-        static final long EMPTY_SIZE = ObjectSizes.measure(new AccordStateCache.Node(null));
-
-        private Node<?, ?> prev;
-        private Node<?, ?> next;
-        private int references = 0;
-        private long lastQueriedEstimatedSizeOnHeap = 0;
-
-        public Node(K key)
-        {
-            super(key);
-        }
-
-        public int referenceCount()
-        {
-            return references;
-        }
-
-        boolean isLoaded()
-        {
-            return state() == LOADED;
-        }
-
-        public boolean isComplete()
-        {
-            switch (state())
-            {
-                case PENDING:
-                case UNINITIALIZED:
-                    return false;
-                case FAILED:
-                case LOADED:
-                    return true;
-                default: throw new UnsupportedOperationException(""Unknown state: "" + state());
-            }
-        }
-
-        private boolean isInQueue()
-        {
-            return prev != null && next != null;
-        }
-
-        long estimatedSizeOnHeap(ToLongFunction<V> estimator)
-        {
-            long result = EMPTY_SIZE;
-            V v;
-            if (isLoaded() && (v = value()) != null)
-                result += estimator.applyAsLong(v);
-            lastQueriedEstimatedSizeOnHeap = result;
-            return result;
-        }
-
-        long estimatedSizeOnHeapDelta(ToLongFunction<V> estimator)
-        {
-            long prevSize = lastQueriedEstimatedSizeOnHeap;
-            return estimatedSizeOnHeap(estimator) - prevSize;
-        }
-
-        boolean shouldUpdateSize()
-        {
-            return isLoaded() && lastQueriedEstimatedSizeOnHeap == EMPTY_SIZE;
-        }
-
-        void maybeCleanupLoad()
-        {
-            state();
-        }
-
-        @Override
-        public String toString()
-        {
-            return ""Node{"" + state() +
-                   "", key="" + key() +
-                   "", references="" + references +
-                   ""}@"" + Integer.toHexString(System.identityHashCode(this));
-        }
-    }
-
     static class Stats
     {
         private long queries;
         private long hits;
         private long misses;
     }
 
-    private static class NamedMap<K, V> extends HashMap<K, V>
-    {
-        final String name;
+    private final Map<Object, AccordCachingState<?, ?>> cache = new HashMap<>();
+    private final HashMap<Class<?>, Instance<?, ?, ?>> instances = new HashMap<>();
 
-        public NamedMap(String name)
-        {
-            this.name = name;
-        }
-    }
-
-    private final Map<Object, Node<?, ?>> cache = new HashMap<>();
-    private final Set<Instance<?, ?, ?>> instances = new HashSet<>();
-
-    private final NamedMap<Object, AsyncResult<Void>> saveResults = new NamedMap<>(""saveResults"");
+    private final ExecutorPlus loadExecutor, saveExecutor;
 
     private int unreferenced = 0;
-    Node<?, ?> head;
-    Node<?, ?> tail;
     private long maxSizeInBytes;
     private long bytesCached = 0;
     private final Stats stats = new Stats();
 
-    public AccordStateCache(long maxSizeInBytes)
+    public AccordStateCache(ExecutorPlus loadExecutor, ExecutorPlus saveExecutor, long maxSizeInBytes)
     {
+        this.loadExecutor = loadExecutor;
+        this.saveExecutor = saveExecutor;
         this.maxSizeInBytes = maxSizeInBytes;
     }
 
     public void setMaxSize(long size)
     {
         maxSizeInBytes = size;
-        maybeEvict();
+        maybeEvictSomeNodes();
     }
 
     public long getMaxSize()
     {
         return maxSizeInBytes;
     }
 
-    @VisibleForTesting
-    public void clear()
-    {
-        head = tail = null;
-        cache.clear();
-        saveResults.clear();
-    }
-
-    @VisibleForTesting
-    public Map<Object, AsyncResult<Void>> saveResults()
-    {
-        return saveResults;
-    }
-
-    private void unlink(Node<?, ?> node)
+    private void unlink(AccordCachingState<?, ?> node)
     {
-        Node<?, ?> prev = node.prev;
-        Node<?, ?> next = node.next;
-
-        if (prev == null)
-        {
-            Preconditions.checkState(head == node, ""previous is null but the head isnt the provided node!"");
-            head = next;
-        }
-        else
-        {
-            prev.next = next;
-        }
-
-        if (next == null)
-        {
-            Preconditions.checkState(tail == node, ""next is null but the tail isnt the provided node!"");
-            tail = prev;
-        }
-        else
-        {
-            next.prev = prev;
-        }
-
-        node.prev = null;
-        node.next = null;
+        node.unlink();
         unreferenced--;
     }
 
-    private void push(Node<?, ?> node)
+    private void link(AccordCachingState<?, ?> node)
     {
-        if (head != null)
-        {
-            node.prev = null;
-            node.next = head;
-            head.prev = node;
-            head = node;
-        }
-        else
-        {
-            head = node;
-            tail = node;
-        }
+        addLast(node);
         unreferenced++;
     }
 
-    private <K, V> void updateSize(Node<K, V> node, ToLongFunction<V> estimator)
-    {
-        bytesCached += node.estimatedSizeOnHeapDelta(estimator);
-    }
-
-    // don't evict if there's an outstanding save result. If an item is evicted then reloaded
-    // before it's mutation is applied, out of date info will be loaded
-    private boolean canEvict(Node<?, ?> node)
+    @SuppressWarnings(""unchecked"")
+    private <K, V> void maybeUpdateSize(AccordCachingState<?, ?> node, ToLongFunction<?> estimator)
     {
-        Invariants.checkState(node.references == 0);
-        return node.state() == FAILED || !hasActiveAsyncResult(saveResults, node.key());
+        if (node.shouldUpdateSize())
+            bytesCached += ((AccordCachingState<K, V>) node).estimatedSizeOnHeapDelta((ToLongFunction<V>) estimator);
     }
 
-    private void maybeEvict()
+    /*
+     * Roughly respects LRU semantics when evicting. Might consider prioritising keeping MODIFIED nodes around
+     * for longer to maximise the chances of hitting system tables fewer times (or not at all).
+     */
+    private void maybeEvictSomeNodes()
     {
         if (bytesCached <= maxSizeInBytes)
             return;
 
-        Node<?, ?> current = tail;
-        while (current != null && bytesCached > maxSizeInBytes)
+        Iterator<AccordCachingState<?, ?>> iter = this.iterator();","[{'comment': 'Can we avoid allocating an iterator here? This is called all the time. Maybe we could add a peek method and manually iterate through the linked list?', 'commenter': 'bdeggleston'}, {'comment': 'Introduced iterator caching to `IntrusiveLinkedList`, like Agrona does in all of its single-threaded collections, here: https://github.com/apache/cassandra-accord/pull/51', 'commenter': 'iamaleksey'}, {'comment': 'This ended up being unnecessary (see comments on the linked cassandra-accord PR).', 'commenter': 'iamaleksey'}]"
2395,src/java/org/apache/cassandra/service/accord/AccordStateCache.java,"@@ -15,547 +15,442 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.cassandra.service.accord;
 
 import java.util.HashMap;
-import java.util.HashSet;
+import java.util.Iterator;
 import java.util.Map;
-import java.util.Objects;
-import java.util.Set;
+import java.util.function.BiFunction;
 import java.util.function.Function;
 import java.util.function.ToLongFunction;
 import java.util.stream.Stream;
 
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import accord.utils.Invariants;
-import accord.utils.async.AsyncChains;
-import accord.utils.async.AsyncResult;
-import org.apache.cassandra.utils.ObjectSizes;
+import accord.utils.IntrusiveLinkedList;
+import org.apache.cassandra.concurrent.ExecutorPlus;
+import org.apache.cassandra.service.accord.AccordCachingState.Status;
 
-import static org.apache.cassandra.service.accord.AccordLoadingState.LoadingState.FAILED;
-import static org.apache.cassandra.service.accord.AccordLoadingState.LoadingState.LOADED;
+import static accord.utils.Invariants.checkState;
+import static java.lang.String.format;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.EVICTED;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.FAILED_TO_LOAD;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.LOADING;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.SAVING;
 
 /**
  * Cache for AccordCommand and AccordCommandsForKey, available memory is shared between the two object types.
- *
+ * </p>
  * Supports dynamic object sizes. After each acquire/free cycle, the cacheable objects size is recomputed to
  * account for data added/removed during txn processing if it's modified flag is set
  */
-public class AccordStateCache
+public class AccordStateCache extends IntrusiveLinkedList<AccordCachingState<?,?>>
 {
     private static final Logger logger = LoggerFactory.getLogger(AccordStateCache.class);
 
-    public static class Node<K, V> extends AccordLoadingState<K, V>
-    {
-        static final long EMPTY_SIZE = ObjectSizes.measure(new AccordStateCache.Node(null));
-
-        private Node<?, ?> prev;
-        private Node<?, ?> next;
-        private int references = 0;
-        private long lastQueriedEstimatedSizeOnHeap = 0;
-
-        public Node(K key)
-        {
-            super(key);
-        }
-
-        public int referenceCount()
-        {
-            return references;
-        }
-
-        boolean isLoaded()
-        {
-            return state() == LOADED;
-        }
-
-        public boolean isComplete()
-        {
-            switch (state())
-            {
-                case PENDING:
-                case UNINITIALIZED:
-                    return false;
-                case FAILED:
-                case LOADED:
-                    return true;
-                default: throw new UnsupportedOperationException(""Unknown state: "" + state());
-            }
-        }
-
-        private boolean isInQueue()
-        {
-            return prev != null && next != null;
-        }
-
-        long estimatedSizeOnHeap(ToLongFunction<V> estimator)
-        {
-            long result = EMPTY_SIZE;
-            V v;
-            if (isLoaded() && (v = value()) != null)
-                result += estimator.applyAsLong(v);
-            lastQueriedEstimatedSizeOnHeap = result;
-            return result;
-        }
-
-        long estimatedSizeOnHeapDelta(ToLongFunction<V> estimator)
-        {
-            long prevSize = lastQueriedEstimatedSizeOnHeap;
-            return estimatedSizeOnHeap(estimator) - prevSize;
-        }
-
-        boolean shouldUpdateSize()
-        {
-            return isLoaded() && lastQueriedEstimatedSizeOnHeap == EMPTY_SIZE;
-        }
-
-        void maybeCleanupLoad()
-        {
-            state();
-        }
-
-        @Override
-        public String toString()
-        {
-            return ""Node{"" + state() +
-                   "", key="" + key() +
-                   "", references="" + references +
-                   ""}@"" + Integer.toHexString(System.identityHashCode(this));
-        }
-    }
-
     static class Stats
     {
         private long queries;
         private long hits;
         private long misses;
     }
 
-    private static class NamedMap<K, V> extends HashMap<K, V>
-    {
-        final String name;
+    private final Map<Object, AccordCachingState<?, ?>> cache = new HashMap<>();
+    private final HashMap<Class<?>, Instance<?, ?, ?>> instances = new HashMap<>();
 
-        public NamedMap(String name)
-        {
-            this.name = name;
-        }
-    }
-
-    private final Map<Object, Node<?, ?>> cache = new HashMap<>();
-    private final Set<Instance<?, ?, ?>> instances = new HashSet<>();
-
-    private final NamedMap<Object, AsyncResult<Void>> saveResults = new NamedMap<>(""saveResults"");
+    private final ExecutorPlus loadExecutor, saveExecutor;
 
     private int unreferenced = 0;
-    Node<?, ?> head;
-    Node<?, ?> tail;
     private long maxSizeInBytes;
     private long bytesCached = 0;
     private final Stats stats = new Stats();
 
-    public AccordStateCache(long maxSizeInBytes)
+    public AccordStateCache(ExecutorPlus loadExecutor, ExecutorPlus saveExecutor, long maxSizeInBytes)
     {
+        this.loadExecutor = loadExecutor;
+        this.saveExecutor = saveExecutor;
         this.maxSizeInBytes = maxSizeInBytes;
     }
 
     public void setMaxSize(long size)
     {
         maxSizeInBytes = size;
-        maybeEvict();
+        maybeEvictSomeNodes();
     }
 
     public long getMaxSize()
     {
         return maxSizeInBytes;
     }
 
-    @VisibleForTesting
-    public void clear()
-    {
-        head = tail = null;
-        cache.clear();
-        saveResults.clear();
-    }
-
-    @VisibleForTesting
-    public Map<Object, AsyncResult<Void>> saveResults()
-    {
-        return saveResults;
-    }
-
-    private void unlink(Node<?, ?> node)
+    private void unlink(AccordCachingState<?, ?> node)
     {
-        Node<?, ?> prev = node.prev;
-        Node<?, ?> next = node.next;
-
-        if (prev == null)
-        {
-            Preconditions.checkState(head == node, ""previous is null but the head isnt the provided node!"");
-            head = next;
-        }
-        else
-        {
-            prev.next = next;
-        }
-
-        if (next == null)
-        {
-            Preconditions.checkState(tail == node, ""next is null but the tail isnt the provided node!"");
-            tail = prev;
-        }
-        else
-        {
-            next.prev = prev;
-        }
-
-        node.prev = null;
-        node.next = null;
+        node.unlink();
         unreferenced--;
     }
 
-    private void push(Node<?, ?> node)
+    private void link(AccordCachingState<?, ?> node)
     {
-        if (head != null)
-        {
-            node.prev = null;
-            node.next = head;
-            head.prev = node;
-            head = node;
-        }
-        else
-        {
-            head = node;
-            tail = node;
-        }
+        addLast(node);
         unreferenced++;
     }
 
-    private <K, V> void updateSize(Node<K, V> node, ToLongFunction<V> estimator)
-    {
-        bytesCached += node.estimatedSizeOnHeapDelta(estimator);
-    }
-
-    // don't evict if there's an outstanding save result. If an item is evicted then reloaded
-    // before it's mutation is applied, out of date info will be loaded
-    private boolean canEvict(Node<?, ?> node)
+    @SuppressWarnings(""unchecked"")
+    private <K, V> void maybeUpdateSize(AccordCachingState<?, ?> node, ToLongFunction<?> estimator)
     {
-        Invariants.checkState(node.references == 0);
-        return node.state() == FAILED || !hasActiveAsyncResult(saveResults, node.key());
+        if (node.shouldUpdateSize())
+            bytesCached += ((AccordCachingState<K, V>) node).estimatedSizeOnHeapDelta((ToLongFunction<V>) estimator);
     }
 
-    private void maybeEvict()
+    /*
+     * Roughly respects LRU semantics when evicting. Might consider prioritising keeping MODIFIED nodes around
+     * for longer to maximise the chances of hitting system tables fewer times (or not at all).
+     */
+    private void maybeEvictSomeNodes()
     {
         if (bytesCached <= maxSizeInBytes)
             return;
 
-        Node<?, ?> current = tail;
-        while (current != null && bytesCached > maxSizeInBytes)
+        Iterator<AccordCachingState<?, ?>> iter = this.iterator();
+        while (iter.hasNext() && bytesCached > maxSizeInBytes)
         {
-            Node<?, ?> evict = current;
-            current = current.prev;
-
-            // TODO (expected, efficiency): can this be reworked so we're not skipping unevictable nodes everytime we try to evict?
-            if (!canEvict(evict))
-                continue;
+            AccordCachingState<?, ?> node = iter.next();
+            checkState(node.references == 0);
 
-            evict(evict, true);
+            /*
+             * TODO (expected, efficiency):
+             *    can this be reworked so we're not skipping unevictable nodes everytime we try to evict?
+             */
+            Status status = node.status(); // status() call completes (if completeable)
+            switch (status)
+            {
+                default: throw new IllegalStateException(""Unhandled status "" + status);
+                case LOADED:
+                    unlink(node);
+                    evict(node);
+                    break;
+                case MODIFIED:
+                    // schedule a save to disk, keep linked and in the cache map
+                    Instance<?, ?, ?> instance = instanceForNode(node);
+                    node.save(saveExecutor, instance.saveFunction);
+                    maybeUpdateSize(node, instance.heapEstimator);
+                    break;
+                case SAVING:
+                    // skip over until completes to LOADED or FAILED_TO_SAVE
+                    break;
+                case FAILED_TO_SAVE:
+                    // permanently unlink, but keep in the map; consider panicking instead when this happens","[{'comment': 'Can you add a TODO for this?', 'commenter': 'bdeggleston'}, {'comment': 'Sure, converted to a TODO.', 'commenter': 'iamaleksey'}]"
2395,src/java/org/apache/cassandra/service/accord/AccordStateCache.java,"@@ -15,547 +15,442 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.cassandra.service.accord;
 
 import java.util.HashMap;
-import java.util.HashSet;
+import java.util.Iterator;
 import java.util.Map;
-import java.util.Objects;
-import java.util.Set;
+import java.util.function.BiFunction;
 import java.util.function.Function;
 import java.util.function.ToLongFunction;
 import java.util.stream.Stream;
 
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import accord.utils.Invariants;
-import accord.utils.async.AsyncChains;
-import accord.utils.async.AsyncResult;
-import org.apache.cassandra.utils.ObjectSizes;
+import accord.utils.IntrusiveLinkedList;
+import org.apache.cassandra.concurrent.ExecutorPlus;
+import org.apache.cassandra.service.accord.AccordCachingState.Status;
 
-import static org.apache.cassandra.service.accord.AccordLoadingState.LoadingState.FAILED;
-import static org.apache.cassandra.service.accord.AccordLoadingState.LoadingState.LOADED;
+import static accord.utils.Invariants.checkState;
+import static java.lang.String.format;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.EVICTED;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.FAILED_TO_LOAD;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.LOADING;
+import static org.apache.cassandra.service.accord.AccordCachingState.Status.SAVING;
 
 /**
  * Cache for AccordCommand and AccordCommandsForKey, available memory is shared between the two object types.
- *
+ * </p>
  * Supports dynamic object sizes. After each acquire/free cycle, the cacheable objects size is recomputed to
  * account for data added/removed during txn processing if it's modified flag is set
  */
-public class AccordStateCache
+public class AccordStateCache extends IntrusiveLinkedList<AccordCachingState<?,?>>
 {
     private static final Logger logger = LoggerFactory.getLogger(AccordStateCache.class);
 
-    public static class Node<K, V> extends AccordLoadingState<K, V>
-    {
-        static final long EMPTY_SIZE = ObjectSizes.measure(new AccordStateCache.Node(null));
-
-        private Node<?, ?> prev;
-        private Node<?, ?> next;
-        private int references = 0;
-        private long lastQueriedEstimatedSizeOnHeap = 0;
-
-        public Node(K key)
-        {
-            super(key);
-        }
-
-        public int referenceCount()
-        {
-            return references;
-        }
-
-        boolean isLoaded()
-        {
-            return state() == LOADED;
-        }
-
-        public boolean isComplete()
-        {
-            switch (state())
-            {
-                case PENDING:
-                case UNINITIALIZED:
-                    return false;
-                case FAILED:
-                case LOADED:
-                    return true;
-                default: throw new UnsupportedOperationException(""Unknown state: "" + state());
-            }
-        }
-
-        private boolean isInQueue()
-        {
-            return prev != null && next != null;
-        }
-
-        long estimatedSizeOnHeap(ToLongFunction<V> estimator)
-        {
-            long result = EMPTY_SIZE;
-            V v;
-            if (isLoaded() && (v = value()) != null)
-                result += estimator.applyAsLong(v);
-            lastQueriedEstimatedSizeOnHeap = result;
-            return result;
-        }
-
-        long estimatedSizeOnHeapDelta(ToLongFunction<V> estimator)
-        {
-            long prevSize = lastQueriedEstimatedSizeOnHeap;
-            return estimatedSizeOnHeap(estimator) - prevSize;
-        }
-
-        boolean shouldUpdateSize()
-        {
-            return isLoaded() && lastQueriedEstimatedSizeOnHeap == EMPTY_SIZE;
-        }
-
-        void maybeCleanupLoad()
-        {
-            state();
-        }
-
-        @Override
-        public String toString()
-        {
-            return ""Node{"" + state() +
-                   "", key="" + key() +
-                   "", references="" + references +
-                   ""}@"" + Integer.toHexString(System.identityHashCode(this));
-        }
-    }
-
     static class Stats
     {
         private long queries;
         private long hits;
         private long misses;
     }
 
-    private static class NamedMap<K, V> extends HashMap<K, V>
-    {
-        final String name;
+    private final Map<Object, AccordCachingState<?, ?>> cache = new HashMap<>();
+    private final HashMap<Class<?>, Instance<?, ?, ?>> instances = new HashMap<>();
 
-        public NamedMap(String name)
-        {
-            this.name = name;
-        }
-    }
-
-    private final Map<Object, Node<?, ?>> cache = new HashMap<>();
-    private final Set<Instance<?, ?, ?>> instances = new HashSet<>();
-
-    private final NamedMap<Object, AsyncResult<Void>> saveResults = new NamedMap<>(""saveResults"");
+    private final ExecutorPlus loadExecutor, saveExecutor;
 
     private int unreferenced = 0;
-    Node<?, ?> head;
-    Node<?, ?> tail;
     private long maxSizeInBytes;
     private long bytesCached = 0;
     private final Stats stats = new Stats();
 
-    public AccordStateCache(long maxSizeInBytes)
+    public AccordStateCache(ExecutorPlus loadExecutor, ExecutorPlus saveExecutor, long maxSizeInBytes)
     {
+        this.loadExecutor = loadExecutor;
+        this.saveExecutor = saveExecutor;
         this.maxSizeInBytes = maxSizeInBytes;
     }
 
     public void setMaxSize(long size)
     {
         maxSizeInBytes = size;
-        maybeEvict();
+        maybeEvictSomeNodes();
     }
 
     public long getMaxSize()
     {
         return maxSizeInBytes;
     }
 
-    @VisibleForTesting
-    public void clear()
-    {
-        head = tail = null;
-        cache.clear();
-        saveResults.clear();
-    }
-
-    @VisibleForTesting
-    public Map<Object, AsyncResult<Void>> saveResults()
-    {
-        return saveResults;
-    }
-
-    private void unlink(Node<?, ?> node)
+    private void unlink(AccordCachingState<?, ?> node)
     {
-        Node<?, ?> prev = node.prev;
-        Node<?, ?> next = node.next;
-
-        if (prev == null)
-        {
-            Preconditions.checkState(head == node, ""previous is null but the head isnt the provided node!"");
-            head = next;
-        }
-        else
-        {
-            prev.next = next;
-        }
-
-        if (next == null)
-        {
-            Preconditions.checkState(tail == node, ""next is null but the tail isnt the provided node!"");
-            tail = prev;
-        }
-        else
-        {
-            next.prev = prev;
-        }
-
-        node.prev = null;
-        node.next = null;
+        node.unlink();
         unreferenced--;
     }
 
-    private void push(Node<?, ?> node)
+    private void link(AccordCachingState<?, ?> node)
     {
-        if (head != null)
-        {
-            node.prev = null;
-            node.next = head;
-            head.prev = node;
-            head = node;
-        }
-        else
-        {
-            head = node;
-            tail = node;
-        }
+        addLast(node);
         unreferenced++;
     }
 
-    private <K, V> void updateSize(Node<K, V> node, ToLongFunction<V> estimator)
-    {
-        bytesCached += node.estimatedSizeOnHeapDelta(estimator);
-    }
-
-    // don't evict if there's an outstanding save result. If an item is evicted then reloaded
-    // before it's mutation is applied, out of date info will be loaded
-    private boolean canEvict(Node<?, ?> node)
+    @SuppressWarnings(""unchecked"")
+    private <K, V> void maybeUpdateSize(AccordCachingState<?, ?> node, ToLongFunction<?> estimator)
     {
-        Invariants.checkState(node.references == 0);
-        return node.state() == FAILED || !hasActiveAsyncResult(saveResults, node.key());
+        if (node.shouldUpdateSize())
+            bytesCached += ((AccordCachingState<K, V>) node).estimatedSizeOnHeapDelta((ToLongFunction<V>) estimator);
     }
 
-    private void maybeEvict()
+    /*
+     * Roughly respects LRU semantics when evicting. Might consider prioritising keeping MODIFIED nodes around
+     * for longer to maximise the chances of hitting system tables fewer times (or not at all).
+     */
+    private void maybeEvictSomeNodes()
     {
         if (bytesCached <= maxSizeInBytes)
             return;
 
-        Node<?, ?> current = tail;
-        while (current != null && bytesCached > maxSizeInBytes)
+        Iterator<AccordCachingState<?, ?>> iter = this.iterator();
+        while (iter.hasNext() && bytesCached > maxSizeInBytes)
         {
-            Node<?, ?> evict = current;
-            current = current.prev;
-
-            // TODO (expected, efficiency): can this be reworked so we're not skipping unevictable nodes everytime we try to evict?
-            if (!canEvict(evict))
-                continue;
+            AccordCachingState<?, ?> node = iter.next();
+            checkState(node.references == 0);
 
-            evict(evict, true);
+            /*
+             * TODO (expected, efficiency):
+             *    can this be reworked so we're not skipping unevictable nodes everytime we try to evict?
+             */
+            Status status = node.status(); // status() call completes (if completeable)
+            switch (status)
+            {
+                default: throw new IllegalStateException(""Unhandled status "" + status);
+                case LOADED:
+                    unlink(node);
+                    evict(node);
+                    break;
+                case MODIFIED:
+                    // schedule a save to disk, keep linked and in the cache map
+                    Instance<?, ?, ?> instance = instanceForNode(node);
+                    node.save(saveExecutor, instance.saveFunction);
+                    maybeUpdateSize(node, instance.heapEstimator);
+                    break;
+                case SAVING:
+                    // skip over until completes to LOADED or FAILED_TO_SAVE","[{'comment': 'We should preserve the TODO about skipping unselectable nodes', 'commenter': 'bdeggleston'}, {'comment': ""I think it's sufficiently preserved, above, just above the switch statement? Unless you mean something else."", 'commenter': 'iamaleksey'}, {'comment': 'heh, oh right, missed it there :)', 'commenter': 'bdeggleston'}]"
2403,src/java/org/apache/cassandra/locator/Ec2SnitchIMDSv2.java,"@@ -17,53 +17,90 @@
  */
 
 package org.apache.cassandra.locator;
+
 import java.io.DataInputStream;
 import java.io.FilterInputStream;
 import java.io.IOException;
 import java.net.HttpURLConnection;
 import java.net.URL;
 import java.nio.charset.StandardCharsets;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
 
-import org.apache.cassandra.exceptions.ConfigurationException;
-import org.apache.cassandra.io.util.FileUtils;
-import org.apache.cassandra.utils.Clock.Global;
+import com.google.common.base.Supplier;
+import com.google.common.base.Suppliers;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.cassandra.io.util.FileUtils;
+
 /**
- * An implementation of the Ec2Snitch which uses Instance Meta Data Service v2 (IMDSv2) which requires you
+ * An implementation of the Ec2Snitch which uses Instance Metadata Service v2 (IMDSv2) which requires you
  * to get a session token first before calling the metaData service
  */
 public class Ec2SnitchIMDSv2 extends Ec2Snitch
 {
-    protected static final Logger logger = LoggerFactory.getLogger(Ec2SnitchIMDSv2.class);
-    private static final long REFRESH_TOKEN_TIME = 21600;
-    private static final String AWS_EC2_METADATA_HEADER_TTL = ""X-aws-ec2-metadata-token-ttl-seconds"";
-    private static final String TOKEN_TTL_SECONDS = String.valueOf(REFRESH_TOKEN_TIME);
-    private static final String AWS_EC2_METADATA_HEADER = ""X-aws-ec2-metadata-token"";
+    private static final Logger logger = LoggerFactory.getLogger(Ec2SnitchIMDSv2.class);
+    private static final int MAX_TOKEN_TIME_IN_SECONDS = 21600;
+    private static final String AWS_EC2_METADATA_TOKEN_HEADER = ""X-aws-ec2-metadata-token"";
+    private static final String AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER = ""X-aws-ec2-metadata-token-ttl-seconds"";
     private static final String TOKEN_ENDPOINT = ""http://169.254.169.254/latest/api/token"";
 
-    private String myToken;
-    private Long myLastTokenTime;
-
+    private Supplier<ApiCallResult> tokenSupplier;
+    private long tokenTTL;
 
-    public Ec2SnitchIMDSv2() throws IOException, ConfigurationException
+    public Ec2SnitchIMDSv2() throws IOException
     {
-        super();
+        super(new SnitchProperties());
     }
 
-    @Override
-    String awsApiCall(final String url) throws IOException, ConfigurationException
+    public Ec2SnitchIMDSv2(SnitchProperties snitchProperties) throws IOException
     {
-        // Populate the region and zone by introspection, fail if 404 on metadata
-        if (myToken == null || myLastTokenTime == null
-            || Global.currentTimeMillis() - myLastTokenTime > (REFRESH_TOKEN_TIME - 100))
+        super(snitchProperties);
+
+        String parsedTokenTTL = snitchProperties.get(AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER, Integer.toString(MAX_TOKEN_TIME_IN_SECONDS));
+
+        try
         {
-            getAndSetNewToken();
+            tokenTTL = Integer.parseInt(parsedTokenTTL);
+
+            if (tokenTTL > MAX_TOKEN_TIME_IN_SECONDS || tokenTTL < 1)
+            {
+                logger.info(String.format(""property %s was set to %s which is more than maximum allowed range of (0, 21600]: %s, defaulting to %s"",
+                                          AWS_EC2_METADATA_TOKEN_HEADER, tokenTTL, MAX_TOKEN_TIME_IN_SECONDS, MAX_TOKEN_TIME_IN_SECONDS));
+                tokenTTL = MAX_TOKEN_TIME_IN_SECONDS;
+            }
         }
-        final HttpURLConnection conn = (HttpURLConnection) new URL(url).openConnection();
-        conn.setRequestProperty(AWS_EC2_METADATA_HEADER, myToken);
-        return getContent(conn);
+        catch (NumberFormatException ex)
+        {
+            logger.error(String.format(""Unable to parse integer from %s, value to parse: %s. Defaulting to %s"",
+                                       AWS_EC2_METADATA_TOKEN_HEADER, parsedTokenTTL, MAX_TOKEN_TIME_IN_SECONDS));
+            tokenTTL = MAX_TOKEN_TIME_IN_SECONDS;
+        }
+
+        tokenSupplier = Suppliers.memoizeWithExpiration(this::getNewToken, Math.max(5, tokenTTL - 100), TimeUnit.SECONDS);","[{'comment': '5 is probably wrong, what I try to do here is that when we get a token, by the time we memorize it, some time already passed, no? So memorizing it with same timeout as TTL for that token does not make sense because it might happen that it would return invalid token. Being on the safe side of ""tokenTTL - 100s"" seems like a good idea (it was in the original PR too). I am not sure about the minimum there though ... what if TTL is ... 3? It would then hold invalid token for 2 seconds? (5-3) ... not sure what the logic should be here.', 'commenter': 'smiklosovic'}, {'comment': 'Well, it is memoized when it is retrieved. It is just a session token and a session can be expired externally as well. Therefore, we should simply recognize such a situation and simply retry', 'commenter': 'jacek-lewandowski'}]"
2403,.build/cassandra-build-deps-template.xml,"@@ -123,5 +123,9 @@
       <groupId>com.fasterxml.jackson.dataformat</groupId>
       <artifactId>jackson-dataformat-yaml</artifactId>
     </dependency>
+    <dependency>
+      <groupId>com.github.tomakehurst</groupId>
+      <artifactId>wiremock-jre8</artifactId>","[{'comment': 'should whether it is `jre8` or `jre11` depend on which Java version we compile it against?', 'commenter': 'jacek-lewandowski'}, {'comment': 'I run the tests with Java 11 and it just works. I think this is backward compatible. `jre11` probably can not be run with Java 8 but jre8 can be run with Java 11 just fine.', 'commenter': 'smiklosovic'}]"
2403,src/java/org/apache/cassandra/locator/CloudApiCallable.java,"@@ -0,0 +1,94 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.util.Map;
+
+import com.google.common.collect.ImmutableMap;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+
+public interface CloudApiCallable","[{'comment': 'why not make it an abstract class and have url in a field?', 'commenter': 'jacek-lewandowski'}, {'comment': 'also, maybe a different name? something more like `CloudMetadataServiceConnector` (with `I` prefix if you decide to keep it an interface)?', 'commenter': 'jacek-lewandowski'}]"
2403,src/java/org/apache/cassandra/locator/Ec2MetadataServiceConnector.java,"@@ -0,0 +1,234 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.utils.Clock;
+import org.apache.cassandra.utils.Pair;
+
+import static java.lang.String.format;
+
+public interface Ec2MetadataServiceConnector extends CloudApiCallable
+{
+    String EC2_METADATA_TYPE_PROPERTY = ""ec2_metadata_type"";
+    String DEFAULT_METADATA_SERVICE_URL = ""http://169.254.169.254"";
+
+    enum EC2MetadataType
+    {
+        V1(""v1""),
+        V2(""v2"");
+
+        private final String name;
+
+        EC2MetadataType(String name)
+        {
+            this.name = name;
+        }
+
+        @Override
+        public String toString()
+        {
+            return name;
+        }
+    }
+
+    static Ec2MetadataServiceConnector create(SnitchProperties props)
+    {
+        try
+        {
+            EC2MetadataType type = EC2MetadataType.valueOf(props.get(EC2_METADATA_TYPE_PROPERTY, EC2MetadataType.V2.toString()).toUpperCase());
+
+            switch (type)
+            {
+                case V1:
+                    return V1Connector.create(props);","[{'comment': 'well, provider could be included in enum, then we would just call `type.create(props)`', 'commenter': 'jacek-lewandowski'}, {'comment': 'great idea.', 'commenter': 'smiklosovic'}]"
2403,src/java/org/apache/cassandra/locator/Ec2MetadataServiceConnector.java,"@@ -0,0 +1,234 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.utils.Clock;
+import org.apache.cassandra.utils.Pair;
+
+import static java.lang.String.format;
+
+public interface Ec2MetadataServiceConnector extends CloudApiCallable
+{
+    String EC2_METADATA_TYPE_PROPERTY = ""ec2_metadata_type"";
+    String DEFAULT_METADATA_SERVICE_URL = ""http://169.254.169.254"";
+
+    enum EC2MetadataType
+    {
+        V1(""v1""),
+        V2(""v2"");
+
+        private final String name;
+
+        EC2MetadataType(String name)
+        {
+            this.name = name;
+        }
+
+        @Override
+        public String toString()
+        {
+            return name;
+        }
+    }
+
+    static Ec2MetadataServiceConnector create(SnitchProperties props)
+    {
+        try
+        {
+            EC2MetadataType type = EC2MetadataType.valueOf(props.get(EC2_METADATA_TYPE_PROPERTY, EC2MetadataType.V2.toString()).toUpperCase());
+
+            switch (type)
+            {
+                case V1:
+                    return V1Connector.create(props);
+                case V2:
+                    return V2Connector.create(props);
+                default:
+                    throw new IllegalArgumentException();
+            }
+        }
+        catch (IllegalArgumentException ex)
+        {
+            throw new ConfigurationException(format(""%s must be one of %s"", EC2_METADATA_TYPE_PROPERTY,
+                                                    Arrays.stream(EC2MetadataType.values())
+                                                          .map(EC2MetadataType::toString)
+                                                          .collect(Collectors.joining("", ""))));
+        }
+    }
+
+    class V1Connector implements Ec2MetadataServiceConnector
+    {
+        @VisibleForTesting
+        static final String EC2_METADATA_URL_PROPERTY = ""ec2_metadata_url"";
+
+        private final String metadataServiceUrl;
+
+        static V1Connector create(SnitchProperties props)
+        {
+            String url = props.get(EC2_METADATA_URL_PROPERTY, DEFAULT_METADATA_SERVICE_URL);
+            return new V1Connector(url);
+        }
+
+        public V1Connector(String metadataServiceUrl)
+        {
+            this.metadataServiceUrl = metadataServiceUrl;
+        }
+
+        @Override
+        public String getApiUrl()
+        {
+            return metadataServiceUrl;
+        }
+
+        @Override
+        public String toString()
+        {
+            return ""V1Connector{"" + EC2_METADATA_URL_PROPERTY + '=' + getApiUrl() + '}';","[{'comment': 'probably we should return full name, that is, including the information that it is EC2 connector', 'commenter': 'jacek-lewandowski'}]"
2403,src/java/org/apache/cassandra/locator/Ec2MetadataServiceConnector.java,"@@ -0,0 +1,234 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.utils.Clock;
+import org.apache.cassandra.utils.Pair;
+
+import static java.lang.String.format;
+
+public interface Ec2MetadataServiceConnector extends CloudApiCallable
+{
+    String EC2_METADATA_TYPE_PROPERTY = ""ec2_metadata_type"";
+    String DEFAULT_METADATA_SERVICE_URL = ""http://169.254.169.254"";
+
+    enum EC2MetadataType
+    {
+        V1(""v1""),
+        V2(""v2"");
+
+        private final String name;
+
+        EC2MetadataType(String name)
+        {
+            this.name = name;
+        }
+
+        @Override
+        public String toString()
+        {
+            return name;
+        }
+    }
+
+    static Ec2MetadataServiceConnector create(SnitchProperties props)
+    {
+        try
+        {
+            EC2MetadataType type = EC2MetadataType.valueOf(props.get(EC2_METADATA_TYPE_PROPERTY, EC2MetadataType.V2.toString()).toUpperCase());
+
+            switch (type)
+            {
+                case V1:
+                    return V1Connector.create(props);
+                case V2:
+                    return V2Connector.create(props);
+                default:
+                    throw new IllegalArgumentException();
+            }
+        }
+        catch (IllegalArgumentException ex)
+        {
+            throw new ConfigurationException(format(""%s must be one of %s"", EC2_METADATA_TYPE_PROPERTY,
+                                                    Arrays.stream(EC2MetadataType.values())
+                                                          .map(EC2MetadataType::toString)
+                                                          .collect(Collectors.joining("", ""))));
+        }
+    }
+
+    class V1Connector implements Ec2MetadataServiceConnector
+    {
+        @VisibleForTesting
+        static final String EC2_METADATA_URL_PROPERTY = ""ec2_metadata_url"";
+
+        private final String metadataServiceUrl;
+
+        static V1Connector create(SnitchProperties props)
+        {
+            String url = props.get(EC2_METADATA_URL_PROPERTY, DEFAULT_METADATA_SERVICE_URL);
+            return new V1Connector(url);
+        }
+
+        public V1Connector(String metadataServiceUrl)
+        {
+            this.metadataServiceUrl = metadataServiceUrl;
+        }
+
+        @Override
+        public String getApiUrl()
+        {
+            return metadataServiceUrl;
+        }
+
+        @Override
+        public String toString()
+        {
+            return ""V1Connector{"" + EC2_METADATA_URL_PROPERTY + '=' + getApiUrl() + '}';
+        }
+    }
+
+    class V2Connector extends V1Connector
+    {
+        private static final int MAX_TOKEN_TIME_IN_SECONDS = 21600;
+        private static final int MIN_TOKEN_TIME_IN_SECONDS = 30;
+
+        @VisibleForTesting
+        static int HTTP_REQUEST_RETRIES = 1;
+        @VisibleForTesting
+        static final String AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER_PROPERTY = ""X-aws-ec2-metadata-token-ttl-seconds"";
+
+        @VisibleForTesting
+        static final String AWS_EC2_METADATA_TOKEN_HEADER = ""X-aws-ec2-metadata-token"";
+        @VisibleForTesting
+        static final String TOKEN_QUERY = ""/latest/api/token"";
+
+        private Pair<String, Long> token;
+        private final Duration tokenTTL;
+
+        static V2Connector create(SnitchProperties props)
+        {
+            String url = props.get(EC2_METADATA_URL_PROPERTY, DEFAULT_METADATA_SERVICE_URL);
+
+            String tokenTTLString = props.get(AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER_PROPERTY,
+                                              Integer.toString(MAX_TOKEN_TIME_IN_SECONDS));
+
+            Duration tokenTTL;
+            try
+            {
+                tokenTTL = Duration.ofSeconds(Integer.parseInt(tokenTTLString));
+
+                if (tokenTTL.getSeconds() < MIN_TOKEN_TIME_IN_SECONDS || tokenTTL.getSeconds() > MAX_TOKEN_TIME_IN_SECONDS)
+                {
+                    throw new ConfigurationException(format(""property %s was set to %s which is not in allowed range of [%s..%s]"",
+                                                            AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER_PROPERTY,
+                                                            tokenTTL,
+                                                            MIN_TOKEN_TIME_IN_SECONDS,
+                                                            MAX_TOKEN_TIME_IN_SECONDS));
+                }
+            }
+            catch (NumberFormatException ex)
+            {
+                throw new ConfigurationException(format(""Unable to parse integer from %s, value to parse: %s"",
+                                                        AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER_PROPERTY, tokenTTLString));
+            }
+
+            return new V2Connector(url, tokenTTL);
+        }
+
+        public V2Connector(String metadataServiceUrl, Duration tokenTTL)
+        {
+            super(metadataServiceUrl);
+            this.tokenTTL = tokenTTL;
+        }
+
+        @Override
+        public String apiCall(String url, String query, String method, Map<String, String> extraHeaders, int expectedResponseCode) throws IOException
+        {
+            Map<String, String> headers = new HashMap<>(extraHeaders);
+            for (int retry = 0; retry <= HTTP_REQUEST_RETRIES; retry++)
+            {
+                String resolvedToken;
+                if (token != null && token.right > Clock.Global.currentTimeMillis())
+                    resolvedToken = token.left;
+                else
+                    resolvedToken = getToken();
+
+                try
+                {
+                    headers.put(AWS_EC2_METADATA_TOKEN_HEADER, resolvedToken);
+                    return super.apiCall(url, query, method, headers, expectedResponseCode);
+                }
+                catch (HttpException ex)
+                {
+                    if (retry == HTTP_REQUEST_RETRIES)
+                        throw ex;
+
+                    if (ex.responseCode == 401) // invalidate token if it is 401
+                        this.token = null;
+                }
+            }
+
+            throw new AssertionError();
+        }
+
+        @Override
+        public String toString()
+        {
+            return ""V2Connector{"" +","[{'comment': 'same suggestion as for v1', 'commenter': 'jacek-lewandowski'}]"
2403,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceConnector.java,"@@ -0,0 +1,94 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.HttpURLConnection;
+import java.net.URL;
+import java.util.Map;
+
+import com.google.common.collect.ImmutableMap;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+
+abstract class AbstractCloudMetadataServiceConnector
+{
+    protected String metadataServiceUrl;","[{'comment': 'Can it be final?', 'commenter': 'jacek-lewandowski'}, {'comment': '@jacek-lewandowski I would have to create a constructor in this abstract class to set it there because it complains that it might be uninitialized. ', 'commenter': 'smiklosovic'}, {'comment': 'I did it like I just said above.', 'commenter': 'smiklosovic'}]"
2403,src/java/org/apache/cassandra/locator/Ec2MetadataServiceConnector.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+import java.util.function.Function;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.utils.Clock;
+import org.apache.cassandra.utils.Pair;
+
+import static java.lang.String.format;
+import static java.util.Arrays.stream;
+import static java.util.stream.Collectors.joining;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2MetadataType.v2;
+
+abstract class Ec2MetadataServiceConnector extends AbstractCloudMetadataServiceConnector
+{
+    static final String EC2_METADATA_TYPE_PROPERTY = ""ec2_metadata_type"";
+    static final String EC2_METADATA_URL_PROPERTY = ""ec2_metadata_url"";
+    static final String DEFAULT_EC2_METADATA_URL = ""http://169.254.169.254"";
+
+    Ec2MetadataServiceConnector(SnitchProperties properties)
+    {
+        metadataServiceUrl = properties.get(EC2_METADATA_URL_PROPERTY, DEFAULT_EC2_METADATA_URL);
+    }
+
+    enum EC2MetadataType
+    {
+        v1(props -> Ec2MetadataServiceConnector.V1Connector.create(props)),","[{'comment': 'Uppercase for enum names?', 'commenter': 'jacek-lewandowski'}, {'comment': '@jacek-lewandowski \r\n\r\nI had it like that but I liked this more because I do not need to use ""toUpperCase()"" when I am parsing it from the file. In the file, it will be probably ""v1"" or ""v2"" intead of ""V1"" or ""V2"". Also in \'toString\' it would look strange to have it uppercased so I would have to lowercase it again. And all of this just to have an enum in uppercase ... ', 'commenter': 'smiklosovic'}]"
2403,src/java/org/apache/cassandra/locator/Ec2MetadataServiceConnector.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+import java.util.function.Function;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.utils.Clock;
+import org.apache.cassandra.utils.Pair;
+
+import static java.lang.String.format;
+import static java.util.Arrays.stream;
+import static java.util.stream.Collectors.joining;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2MetadataType.v2;
+
+abstract class Ec2MetadataServiceConnector extends AbstractCloudMetadataServiceConnector
+{
+    static final String EC2_METADATA_TYPE_PROPERTY = ""ec2_metadata_type"";
+    static final String EC2_METADATA_URL_PROPERTY = ""ec2_metadata_url"";
+    static final String DEFAULT_EC2_METADATA_URL = ""http://169.254.169.254"";
+
+    Ec2MetadataServiceConnector(SnitchProperties properties)
+    {
+        metadataServiceUrl = properties.get(EC2_METADATA_URL_PROPERTY, DEFAULT_EC2_METADATA_URL);
+    }
+
+    enum EC2MetadataType
+    {
+        v1(props -> Ec2MetadataServiceConnector.V1Connector.create(props)),
+        v2(props -> Ec2MetadataServiceConnector.V2Connector.create(props));
+
+        private final Function<SnitchProperties, Ec2MetadataServiceConnector> creator;","[{'comment': 'maybe name it ""provider"" ?', 'commenter': 'jacek-lewandowski'}]"
2403,src/java/org/apache/cassandra/locator/Ec2MetadataServiceConnector.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+import java.util.function.Function;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.utils.Clock;
+import org.apache.cassandra.utils.Pair;
+
+import static java.lang.String.format;
+import static java.util.Arrays.stream;
+import static java.util.stream.Collectors.joining;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2MetadataType.v2;
+
+abstract class Ec2MetadataServiceConnector extends AbstractCloudMetadataServiceConnector
+{
+    static final String EC2_METADATA_TYPE_PROPERTY = ""ec2_metadata_type"";
+    static final String EC2_METADATA_URL_PROPERTY = ""ec2_metadata_url"";
+    static final String DEFAULT_EC2_METADATA_URL = ""http://169.254.169.254"";
+
+    Ec2MetadataServiceConnector(SnitchProperties properties)
+    {
+        metadataServiceUrl = properties.get(EC2_METADATA_URL_PROPERTY, DEFAULT_EC2_METADATA_URL);
+    }
+
+    enum EC2MetadataType
+    {
+        v1(props -> Ec2MetadataServiceConnector.V1Connector.create(props)),
+        v2(props -> Ec2MetadataServiceConnector.V2Connector.create(props));
+
+        private final Function<SnitchProperties, Ec2MetadataServiceConnector> creator;
+
+        EC2MetadataType(Function<SnitchProperties, Ec2MetadataServiceConnector> creator)
+        {
+            this.creator = creator;
+        }
+
+        Ec2MetadataServiceConnector create(SnitchProperties properties)
+        {
+            return creator.apply(properties);
+        }
+    }
+
+    static Ec2MetadataServiceConnector create(SnitchProperties props)
+    {
+        try
+        {
+            return EC2MetadataType.valueOf(props.get(EC2_METADATA_TYPE_PROPERTY, v2.name())).create(props);
+        }
+        catch (IllegalArgumentException ex)
+        {
+            throw new ConfigurationException(format(""%s must be one of %s"", EC2_METADATA_TYPE_PROPERTY,
+                                                    stream(EC2MetadataType.values()).map(Enum::name).collect(joining("", ""))));
+        }
+    }
+
+    static class V1Connector extends Ec2MetadataServiceConnector
+    {
+        static V1Connector create(SnitchProperties props)
+        {
+            return new V1Connector(props);
+        }
+
+        V1Connector(SnitchProperties props)
+        {
+            super(props);
+        }
+
+        @Override
+        public String toString()
+        {
+            return V1Connector.class.getName() + '{' + EC2_METADATA_URL_PROPERTY + '=' + metadataServiceUrl + '}';","[{'comment': 'nit: maybe use `String.format`', 'commenter': 'jacek-lewandowski'}]"
2403,src/java/org/apache/cassandra/locator/Ec2MetadataServiceConnector.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+import java.util.function.Function;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.utils.Clock;
+import org.apache.cassandra.utils.Pair;
+
+import static java.lang.String.format;
+import static java.util.Arrays.stream;
+import static java.util.stream.Collectors.joining;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2MetadataType.v2;
+
+abstract class Ec2MetadataServiceConnector extends AbstractCloudMetadataServiceConnector
+{
+    static final String EC2_METADATA_TYPE_PROPERTY = ""ec2_metadata_type"";
+    static final String EC2_METADATA_URL_PROPERTY = ""ec2_metadata_url"";
+    static final String DEFAULT_EC2_METADATA_URL = ""http://169.254.169.254"";
+
+    Ec2MetadataServiceConnector(SnitchProperties properties)
+    {
+        metadataServiceUrl = properties.get(EC2_METADATA_URL_PROPERTY, DEFAULT_EC2_METADATA_URL);
+    }
+
+    enum EC2MetadataType
+    {
+        v1(props -> Ec2MetadataServiceConnector.V1Connector.create(props)),
+        v2(props -> Ec2MetadataServiceConnector.V2Connector.create(props));
+
+        private final Function<SnitchProperties, Ec2MetadataServiceConnector> creator;
+
+        EC2MetadataType(Function<SnitchProperties, Ec2MetadataServiceConnector> creator)
+        {
+            this.creator = creator;
+        }
+
+        Ec2MetadataServiceConnector create(SnitchProperties properties)
+        {
+            return creator.apply(properties);
+        }
+    }
+
+    static Ec2MetadataServiceConnector create(SnitchProperties props)
+    {
+        try
+        {
+            return EC2MetadataType.valueOf(props.get(EC2_METADATA_TYPE_PROPERTY, v2.name())).create(props);
+        }
+        catch (IllegalArgumentException ex)
+        {
+            throw new ConfigurationException(format(""%s must be one of %s"", EC2_METADATA_TYPE_PROPERTY,
+                                                    stream(EC2MetadataType.values()).map(Enum::name).collect(joining("", ""))));
+        }
+    }
+
+    static class V1Connector extends Ec2MetadataServiceConnector
+    {
+        static V1Connector create(SnitchProperties props)
+        {
+            return new V1Connector(props);
+        }
+
+        V1Connector(SnitchProperties props)
+        {
+            super(props);
+        }
+
+        @Override
+        public String toString()
+        {
+            return V1Connector.class.getName() + '{' + EC2_METADATA_URL_PROPERTY + '=' + metadataServiceUrl + '}';
+        }
+    }
+
+    static class V2Connector extends Ec2MetadataServiceConnector
+    {
+        @VisibleForTesting","[{'comment': 'nit: perhaps we do not need to annotate static final fields with `@VisibleForTesting`', 'commenter': 'jacek-lewandowski'}, {'comment': 'I will gladly get rid of them.', 'commenter': 'smiklosovic'}]"
2403,test/unit/org/apache/cassandra/locator/Ec2ConnectorTest.java,"@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.time.Duration;
+import java.util.Properties;
+
+import org.junit.Test;
+
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2MetadataType;
+import org.apache.cassandra.locator.Ec2MetadataServiceConnector.V1Connector;
+import org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector;
+
+import static java.lang.String.format;
+import static java.util.Arrays.stream;
+import static java.util.stream.Collectors.joining;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.DEFAULT_EC2_METADATA_URL;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2_METADATA_TYPE_PROPERTY;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER_PROPERTY;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.MAX_TOKEN_TIME_IN_SECONDS;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.MIN_TOKEN_TIME_IN_SECONDS;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class Ec2ConnectorTest
+{
+    @Test
+    public void testV1Configuration()
+    {
+        Properties p = new Properties();
+        p.setProperty(EC2_METADATA_TYPE_PROPERTY, EC2MetadataType.v1.name());
+        Ec2MetadataServiceConnector ec2Connector = Ec2MetadataServiceConnector.create(new SnitchProperties(p));
+
+        assertTrue(ec2Connector instanceof V1Connector);
+
+        assertEquals(DEFAULT_EC2_METADATA_URL, ec2Connector.metadataServiceUrl);
+    }
+
+    @Test
+    public void testV2Configuration()
+    {
+        Ec2MetadataServiceConnector ec2Connector = Ec2MetadataServiceConnector.create(new SnitchProperties(new Properties()));
+
+        // v2 connector by default
+        assertTrue(ec2Connector instanceof V2Connector);
+        assertEquals(DEFAULT_EC2_METADATA_URL, ec2Connector.metadataServiceUrl);
+        assertEquals(Duration.ofSeconds(MAX_TOKEN_TIME_IN_SECONDS), ((V2Connector) ec2Connector).tokenTTL);
+    }
+
+    @Test
+    public void testInvalidConfiguration()
+    {
+        try
+        {
+            Properties p = new Properties();
+            p.setProperty(EC2_METADATA_TYPE_PROPERTY, ""non-existent"");
+
+            Ec2MetadataServiceConnector.create(new SnitchProperties(p));
+            fail(""it should not be possible create a connector of type non-existent"");
+        }
+        catch (ConfigurationException ex)","[{'comment': 'Could you use AssertJ in tests? (see https://www.tabnine.com/code/java/methods/org.assertj.core.api.Assertions/assertThatExceptionOfType)', 'commenter': 'jacek-lewandowski'}, {'comment': 'rewritten', 'commenter': 'smiklosovic'}]"
2403,test/unit/org/apache/cassandra/locator/Ec2SnitchTest.java,"@@ -71,30 +124,60 @@ public static void setup() throws Exception
         StorageService.instance.initServer(0);
     }
 
-    private class TestEC2Snitch extends Ec2Snitch
+    private static class TestEc2Snitch extends Ec2Snitch
     {
-        public TestEC2Snitch() throws IOException, ConfigurationException
+        public TestEc2Snitch() throws IOException, ConfigurationException
         {
-            super();
+            this(new SnitchProperties());
         }
 
-        public TestEC2Snitch(SnitchProperties props) throws IOException, ConfigurationException
+        public TestEc2Snitch(SnitchProperties props) throws IOException, ConfigurationException","[{'comment': 'What do you think about refactoring this test a bit? We have separated a snitch from a connector so it would be enough to simply mock the connector and do not bother distinguishing between v1 and v2 at all. The whole test would be much simpler and easier to follow', 'commenter': 'jacek-lewandowski'}]"
2403,test/unit/org/apache/cassandra/locator/Ec2V2ConnectorMockingTest.java,"@@ -0,0 +1,174 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.Properties;
+
+import org.junit.Rule;
+import org.junit.Test;
+
+import com.github.tomakehurst.wiremock.client.MappingBuilder;
+import com.github.tomakehurst.wiremock.junit.WireMockRule;
+import org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector;
+
+import static com.github.tomakehurst.wiremock.client.WireMock.aResponse;
+import static com.github.tomakehurst.wiremock.client.WireMock.equalTo;
+import static com.github.tomakehurst.wiremock.client.WireMock.get;
+import static com.github.tomakehurst.wiremock.client.WireMock.put;
+import static com.github.tomakehurst.wiremock.client.WireMock.urlEqualTo;
+import static com.github.tomakehurst.wiremock.core.WireMockConfiguration.wireMockConfig;
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2MetadataType.v2;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2_METADATA_TYPE_PROPERTY;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V1Connector.EC2_METADATA_URL_PROPERTY;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.AWS_EC2_METADATA_TOKEN_HEADER;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER_PROPERTY;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.TOKEN_QUERY;
+import static org.apache.cassandra.locator.Ec2Snitch.ZONE_NAME_QUERY;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+
+public class Ec2V2ConnectorMockingTest
+{
+    private static final String token = ""thisismytoken"";
+    private static final String az = ""us-east-1a"";
+
+    @Rule
+    public WireMockRule v2Service = new WireMockRule(wireMockConfig().bindAddress(""127.0.0.1"").port(8080));
+
+    @Test
+    public void testV2Connector() throws Throwable
+    {
+        v2Service.stubFor(tokenRequest(100, 200, token));
+        v2Service.stubFor(azRequest(az, 200, token));
+
+        assertEquals(az, getConnector(100).apiCall(ZONE_NAME_QUERY));
+    }
+
+    @Test
+    public void testV2ConnectorWhenUnauthorized() throws Throwable
+    {
+        String unauthorizedBody = ""<?xml version=\""1.0\"" encoding=\""iso-8859-1\""?>\n"" +
+                                  ""<!DOCTYPE html PUBLIC \""-//W3C//DTD XHTML 1.0 Transitional//EN\""\n"" +
+                                  ""\t\""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"">\n"" +
+                                  ""<html xmlns=\""http://www.w3.org/1999/xhtml\"" xml:lang=\""en\"" lang=\""en\"">\n"" +
+                                  "" <head>\n"" +
+                                  ""  <title>401 - Unauthorized</title>\n"" +
+                                  "" </head>\n"" +
+                                  "" <body>\n"" +
+                                  ""  <h1>401 - Unauthorized</h1>\n"" +
+                                  "" </body>\n"" +
+                                  ""</html>\n"";
+
+        v2Service.stubFor(tokenRequest(100, 200, token));
+
+        v2Service.stubFor(get(urlEqualTo(ZONE_NAME_QUERY)).withHeader(AWS_EC2_METADATA_TOKEN_HEADER, equalTo(token))
+                                                          .willReturn(aResponse().withStatus(401)
+                                                                                 .withStatusMessage(""Unauthorized"")
+                                                                                 .withHeader(""Content-Type"", ""text/html"")
+                                                                                 .withHeader(""Content-Length"", String.valueOf(unauthorizedBody.getBytes(UTF_8).length))));
+
+        V2Connector.HTTP_REQUEST_RETRIES = 0;
+        Ec2MetadataServiceConnector v2Connector = getConnector(100);
+
+        try
+        {
+            v2Connector.apiCall(ZONE_NAME_QUERY);","[{'comment': 'nit: maybe AssertJ?', 'commenter': 'jacek-lewandowski'}, {'comment': 'sure', 'commenter': 'smiklosovic'}]"
2403,test/unit/org/apache/cassandra/locator/Ec2V2ConnectorMockingTest.java,"@@ -0,0 +1,174 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.Properties;
+
+import org.junit.Rule;
+import org.junit.Test;
+
+import com.github.tomakehurst.wiremock.client.MappingBuilder;
+import com.github.tomakehurst.wiremock.junit.WireMockRule;
+import org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector;
+
+import static com.github.tomakehurst.wiremock.client.WireMock.aResponse;
+import static com.github.tomakehurst.wiremock.client.WireMock.equalTo;
+import static com.github.tomakehurst.wiremock.client.WireMock.get;
+import static com.github.tomakehurst.wiremock.client.WireMock.put;
+import static com.github.tomakehurst.wiremock.client.WireMock.urlEqualTo;
+import static com.github.tomakehurst.wiremock.core.WireMockConfiguration.wireMockConfig;
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2MetadataType.v2;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.EC2_METADATA_TYPE_PROPERTY;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V1Connector.EC2_METADATA_URL_PROPERTY;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.AWS_EC2_METADATA_TOKEN_HEADER;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.AWS_EC2_METADATA_TOKEN_TTL_SECONDS_HEADER_PROPERTY;
+import static org.apache.cassandra.locator.Ec2MetadataServiceConnector.V2Connector.TOKEN_QUERY;
+import static org.apache.cassandra.locator.Ec2Snitch.ZONE_NAME_QUERY;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+
+public class Ec2V2ConnectorMockingTest","[{'comment': 'maybe you merge these tests to Ec2ConnectorTest?', 'commenter': 'jacek-lewandowski'}]"
2416,src/java/org/apache/cassandra/index/sai/disk/v1/segment/Segment.java,"@@ -87,14 +87,15 @@ public boolean intersects(AbstractBounds<PartitionPosition> keyRange)
         if (keyRange instanceof Range && ((Range<?>)keyRange).isWrapAround())
             return keyRange.contains(minKeyBound) || keyRange.contains(maxKeyBound);
 
-        int cmp = keyRange.right.getToken().compareTo(minKey);
+        int cmp = keyRange.right.compareTo(minKeyBound);
         // if right is minimum, it means right is the max token and bigger than maxKey.
         // if right bound is less than minKey, no intersection","[{'comment': '```suggestion\r\n        // if right bound is less than minKeyBound, no intersection\r\n```', 'commenter': 'maedhroz'}]"
2416,src/java/org/apache/cassandra/index/sai/disk/v1/segment/Segment.java,"@@ -87,14 +87,15 @@ public boolean intersects(AbstractBounds<PartitionPosition> keyRange)
         if (keyRange instanceof Range && ((Range<?>)keyRange).isWrapAround())
             return keyRange.contains(minKeyBound) || keyRange.contains(maxKeyBound);
 
-        int cmp = keyRange.right.getToken().compareTo(minKey);
+        int cmp = keyRange.right.compareTo(minKeyBound);
         // if right is minimum, it means right is the max token and bigger than maxKey.
         // if right bound is less than minKey, no intersection
         if (!keyRange.right.isMinimum() && (!keyRange.inclusiveRight() && cmp == 0 || cmp < 0))
             return false;
 
+        cmp = keyRange.left.compareTo(maxKeyBound);
         // if left bound is bigger than maxKey, no intersection
-        return keyRange.isStartInclusive() || keyRange.left.getToken().compareTo(maxKey) < 0;
+        return (keyRange.isStartInclusive() || cmp != 0) && cmp <= 0;","[{'comment': 'With the changes in this method, we no longer need the `minKey` or `maxKey` fields in this class, right?\r\n\r\nCC @mike-tr-adamson ', 'commenter': 'maedhroz'}, {'comment': 'removed `minKey` and `maxKey`', 'commenter': 'jasonstack'}]"
2416,src/java/org/apache/cassandra/index/sai/disk/v1/segment/Segment.java,"@@ -87,14 +87,15 @@ public boolean intersects(AbstractBounds<PartitionPosition> keyRange)
         if (keyRange instanceof Range && ((Range<?>)keyRange).isWrapAround())
             return keyRange.contains(minKeyBound) || keyRange.contains(maxKeyBound);
 
-        int cmp = keyRange.right.getToken().compareTo(minKey);
+        int cmp = keyRange.right.compareTo(minKeyBound);
         // if right is minimum, it means right is the max token and bigger than maxKey.
         // if right bound is less than minKey, no intersection
         if (!keyRange.right.isMinimum() && (!keyRange.inclusiveRight() && cmp == 0 || cmp < 0))
             return false;
 
+        cmp = keyRange.left.compareTo(maxKeyBound);
         // if left bound is bigger than maxKey, no intersection","[{'comment': '```suggestion\r\n        // if left bound is bigger than maxKeyBound, no intersection\r\n```', 'commenter': 'maedhroz'}]"
2420,test/distributed/org/apache/cassandra/distributed/test/sai/IndexStreamingTest.java,"@@ -0,0 +1,134 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.sai;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+
+import org.junit.Test;
+
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.Row;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.distributed.util.QueryResultUtil;
+import org.assertj.core.api.Assertions;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+public class IndexStreamingTest extends TestBaseImpl
+{
+    private static final ByteBuffer BLOB = ByteBuffer.wrap(new byte[1 << 16]);
+    // zero copy streaming sends all components, so the events will include non-Data files as well
+    private static final int NUM_COMPONENTS = 17;","[{'comment': ""nit: We can probably make this a little more self-documenting if we sum up `V1OnDiskFormat.PER_SSTABLE_COMPONENTS`, `V1OnDiskFormat.LITERAL_COMPONENTS`, and the expected core SSTable components (perhaps from `SSTableFormat#streamingComponents()` sans compression info, which I guess we don't have here?)"", 'commenter': 'maedhroz'}, {'comment': 'Good idea. Fixed', 'commenter': 'pkolaczk'}]"
2420,test/distributed/org/apache/cassandra/distributed/test/sai/IndexStreamingTest.java,"@@ -0,0 +1,134 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.sai;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+
+import org.junit.Test;
+
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.Row;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.distributed.util.QueryResultUtil;
+import org.assertj.core.api.Assertions;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+public class IndexStreamingTest extends TestBaseImpl
+{
+    private static final ByteBuffer BLOB = ByteBuffer.wrap(new byte[1 << 16]);
+    // zero copy streaming sends all components, so the events will include non-Data files as well
+    private static final int NUM_COMPONENTS = 17;
+
+    @Test
+    public void zeroCopy() throws IOException
+    {
+        test(true);
+    }
+
+    @Test
+    public void notZeroCopy() throws IOException
+    {
+        test(false);
+    }
+
+    private void test(boolean zeroCopyStreaming) throws IOException
+    {
+        try (Cluster cluster = init(Cluster.build(2)
+                                           .withConfig(c -> c.with(Feature.values())
+                                                             .set(""stream_entire_sstables"", zeroCopyStreaming).set(""streaming_slow_events_log_timeout"", ""0s""))
+                                           .start()))
+        {
+            // streaming sends events every 65k, so need to make sure that the files are larger than this to hit
+            // all cases of the vtable
+            cluster.schemaChange(withKeyspace(
+                ""CREATE TABLE %s.test (pk int PRIMARY KEY, v text, b blob) WITH compression = { 'enabled' : false };""
+            ));
+            cluster.schemaChange(withKeyspace(
+                ""CREATE CUSTOM INDEX ON %s.test(v) USING 'StorageAttachedIndex';""","[{'comment': ""Once numerics support merges, I guess we'll want to parameterize this test further to use those. The number of expected index components might not actually change though. CC @mike-tr-adamson \r\n\r\nI'm not sure if that means we should merge this first and adjust inside a rebased CASSANDRA-18067 or the other way around."", 'commenter': 'maedhroz'}]"
2420,src/java/org/apache/cassandra/io/sstable/SSTableZeroCopyWriter.java,"@@ -61,12 +58,8 @@ public SSTableZeroCopyWriter(Builder<?, ?> builder,
         lifecycleNewTracker.trackNew(this);
         this.componentWriters = new HashMap<>();
 
-        if (!descriptor.getFormat().streamingComponents().containsAll(components))
-            throw new AssertionError(format(""Unsupported streaming component detected %s"",
-                                            Sets.difference(ImmutableSet.copyOf(components), descriptor.getFormat().streamingComponents())));
-","[{'comment': '@pkolaczk @mike-tr-adamson I\'m not sure we want to just remove this validation, but checking SAI components raises some other questions. The simplest way around this for now might be...\r\n\r\n```\r\nSet<Component> unsupported = components.stream()\r\n                                       .filter(c -> !descriptor.getFormat().streamingComponents().contains(c))\r\n                                       .filter(c-> c.type != SSTableFormat.Components.Types.CUSTOM)\r\n                                       .collect(Collectors.toSet());\r\n\r\nif (!unsupported.isEmpty())\r\n    throw new AssertionError(format(""Unsupported streaming component detected %s"", unsupported));\r\n```\r\n\r\nHowever, it might also be worth looking at why we use `CUSTOM` for SAI components in the first place. Is now the time to simply create a new first-class type in `Types`?\r\n\r\n```\r\npublic static final Component.Type SAI = Component.Type.create(""SAI"", ""SAI\\\\+.*.db"", null);\r\n```\r\n\r\n(The regex could be stricter, but details...)\r\n\r\nIf we do this, we the `CUSTOM` in the check above becomes `SAI`, we replace `CUSTOM` w/ `SAI` in a few places in the SAI code, like `Version`, and things should just work, right?', 'commenter': 'maedhroz'}, {'comment': 'btw, I just did exactly this, and it works fine...at least `IndexStreamingTest` has no issues', 'commenter': 'maedhroz'}, {'comment': ""Why do we want to artificially restrict the set of components streamed in the place that can stream any component?\r\nIt's not like streaming supports only some kind of components and doesn't support some other types. It will copy anything we'll throw at it. \r\n"", 'commenter': 'pkolaczk'}, {'comment': 'As for this:\r\n```\r\npublic static final Component.Type SAI = Component.Type.create(""SAI"", ""SAI\\\\+.*.db"", null);\r\n```\r\n\r\nThis wouldn\'t be very consistent with what we use component types for now in the core - because now each core sstable component  has its own type. So why should we throw all different SAI component types into a single type? \r\n\r\nI think we need to take another look at how custom components are registered and simply allow modules to provide their own types. So SAI component types should be defined in SAI. We could also allow the component type to decide whether it should be streamed or not.\r\n', 'commenter': 'pkolaczk'}, {'comment': '> I think we need to take another look at how custom components are registered and simply allow modules to provide their own types. So SAI component types should be defined in SAI. We could also allow the component type to decide whether it should be streamed or not.\r\n\r\nI think this would work. +1', 'commenter': 'maedhroz'}]"
2420,src/java/org/apache/cassandra/db/streaming/ComponentManifest.java,"@@ -52,13 +53,17 @@ public ComponentManifest(Map<Component, Long> components)
     }
 
     @VisibleForTesting
-    public static ComponentManifest create(Descriptor descriptor)
+    public static ComponentManifest create(SSTable sstable)
     {
-        LinkedHashMap<Component, Long> components = new LinkedHashMap<>(descriptor.getFormat().streamingComponents().size());
+        Set<Component> streamingComponents = sstable.getComponents();
+        LinkedHashMap<Component, Long> components = new LinkedHashMap<>(streamingComponents.size());
 
-        for (Component component : descriptor.getFormat().streamingComponents())
+        for (Component component : streamingComponents)
         {
-            File file = descriptor.fileFor(component);
+            if (component == SSTableFormat.Components.TOC)","[{'comment': ""nit: I guess the spirit of the original code here was to delegate this as much as possible to the SSTable format. What if we tried to preserve that (and coupled it with the `SAI` component I suggested below) and did something like...\r\n\r\n```\r\nif (!sstable.descriptor.getFormat().streamingComponents().contains(component) && component.type != BigFormat.Components.Types.SAI)\r\n```\r\nI like the idea of using `sstable.getComponents()` to make sure we don't forget anything, but it seems like picking out the `TOC` alone is a little brittle. WDYT?"", 'commenter': 'maedhroz'}, {'comment': ""We could also hide this in `SSTable` with a `getStreamingComponents()` and leave this logic in the manifest almost unchanged? It already makes a copy of `components`, and this isn't really a hot path anyway, so filtering there w/ something like...\r\n\r\n```\r\npublic Set<Component> getStreamingComponents()\r\n{\r\n    return components.stream()\r\n                     .filter(c -> descriptor.getFormat().streamingComponents().contains(c) || c.type == Components.Types.SAI)\r\n                     .collect(Collectors.toSet());\r\n}\r\n```"", 'commenter': 'maedhroz'}, {'comment': 'First, I like the idea of having `getStreamingComponents` method on the sstable. This looks like a better place to tell which components are eligible for streaming. \r\n\r\nHowever, even with such method, we still have a ""responsibility leak"" from SAI here. \r\n\r\n```\r\nc.type == Components.Types.SAI\r\n```\r\n\r\nWhy should SSTable code know anything about SAI? Why should it assume all SAI components should be streamed?  IMHO this is something that should be decided by the SAI code (or whoever owns the component), not by sstable code.\r\n\r\nSAI already controls the type of the component, so I guess the ideal-world solution would be to have this information associated directly with the type so instead of special-casing SAI vs core components, we could write the condition like this:\r\n\r\n```\r\n return components.stream()\r\n                     .filter(c -> c.type.shouldBeStreamed)\r\n                     .collect(Collectors.toSet());\r\n```\r\n\r\nWDYT?', 'commenter': 'pkolaczk'}, {'comment': 'Yeah, at the end of the day, some things are just not part of the SSTable format, so if/when that is the case, having the component decide whether it\'s ""streamable"" doesn\'t seem like a bad idea. (Or we could do that for all components.)', 'commenter': 'maedhroz'}]"
2420,src/java/org/apache/cassandra/io/sstable/format/SSTableFormat.java,"@@ -59,7 +60,18 @@ public interface SSTableFormat<R extends SSTableReader, W extends SSTableWriter>
      */
     Set<Component> allComponents();
 
-    Set<Component> streamingComponents();
+    /**
+     * Returns the components that should be streamed to other nodes on repair / rebuild.
+     * This includes only the core SSTable components produced by this format.
+     * Custom components registered by e.g. secondary indexes are not included.
+     * Use {@link SSTableReader#getStreamingComponents()} for the list of all components including the custom ones.
+     */
+    default Set<Component> streamingComponents()","[{'comment': 'Once all `Component` instances have their ""streamability"" embedded, we don\'t need this method at all, right? i.e. `SSTableZeroCopyWriter` will no longer need special casing?', 'commenter': 'maedhroz'}, {'comment': ""Looks like we need it for testing. I'll think of a way to get rid of it completely."", 'commenter': 'pkolaczk'}]"
2420,src/java/org/apache/cassandra/io/sstable/format/SSTableFormat.java,"@@ -156,23 +168,23 @@ public static class Types
         {
             // the base data for an sstable: the remaining components can be regenerated
             // based on the data component
-            public static final Component.Type DATA = Component.Type.createSingleton(""DATA"", ""Data.db"", null);
+            public static final Component.Type DATA = Component.Type.createSingleton(""DATA"", ""Data.db"", true, null);
             // file to hold information about uncompressed data length, chunk offsets etc.
-            public static final Component.Type COMPRESSION_INFO = Component.Type.createSingleton(""COMPRESSION_INFO"", ""CompressionInfo.db"", null);
+            public static final Component.Type COMPRESSION_INFO = Component.Type.createSingleton(""COMPRESSION_INFO"", ""CompressionInfo.db"", true, null);
             // statistical metadata about the content of the sstable
-            public static final Component.Type STATS = Component.Type.createSingleton(""STATS"", ""Statistics.db"", null);
+            public static final Component.Type STATS = Component.Type.createSingleton(""STATS"", ""Statistics.db"", true, null);
             // serialized bloom filter for the row keys in the sstable
-            public static final Component.Type FILTER = Component.Type.createSingleton(""FILTER"", ""Filter.db"", null);
+            public static final Component.Type FILTER = Component.Type.createSingleton(""FILTER"", ""Filter.db"", true, null);
             // holds CRC32 checksum of the data file
-            public static final Component.Type DIGEST = Component.Type.createSingleton(""DIGEST"", ""Digest.crc32"", null);
+            public static final Component.Type DIGEST = Component.Type.createSingleton(""DIGEST"", ""Digest.crc32"", true, null);
             // holds the CRC32 for chunks in an uncompressed file.
-            public static final Component.Type CRC = Component.Type.createSingleton(""CRC"", ""CRC.db"", null);
+            public static final Component.Type CRC = Component.Type.createSingleton(""CRC"", ""CRC.db"", true, null);
             // table of contents, stores the list of all components for the sstable
-            public static final Component.Type TOC = Component.Type.createSingleton(""TOC"", ""TOC.txt"", null);
+            public static final Component.Type TOC = Component.Type.createSingleton(""TOC"", ""TOC.txt"", false, null);
             // built-in secondary index (may exist multiple per sstable)
-            public static final Component.Type SECONDARY_INDEX = Component.Type.create(""SECONDARY_INDEX"", ""SI_.*.db"", null);
+            public static final Component.Type SECONDARY_INDEX = Component.Type.create(""SECONDARY_INDEX"", ""SI_.*.db"", true, null);","[{'comment': 'nit: Are `SECONDARY_INDEX` components streamable? I thought we always rebuilt them at the receiver...', 'commenter': 'maedhroz'}, {'comment': 'Good catch. I forgot to change it. ', 'commenter': 'pkolaczk'}]"
2420,src/java/org/apache/cassandra/io/sstable/format/SSTableFormat.java,"@@ -156,23 +168,23 @@ public static class Types
         {
             // the base data for an sstable: the remaining components can be regenerated
             // based on the data component
-            public static final Component.Type DATA = Component.Type.createSingleton(""DATA"", ""Data.db"", null);
+            public static final Component.Type DATA = Component.Type.createSingleton(""DATA"", ""Data.db"", true, null);
             // file to hold information about uncompressed data length, chunk offsets etc.
-            public static final Component.Type COMPRESSION_INFO = Component.Type.createSingleton(""COMPRESSION_INFO"", ""CompressionInfo.db"", null);
+            public static final Component.Type COMPRESSION_INFO = Component.Type.createSingleton(""COMPRESSION_INFO"", ""CompressionInfo.db"", true, null);
             // statistical metadata about the content of the sstable
-            public static final Component.Type STATS = Component.Type.createSingleton(""STATS"", ""Statistics.db"", null);
+            public static final Component.Type STATS = Component.Type.createSingleton(""STATS"", ""Statistics.db"", true, null);
             // serialized bloom filter for the row keys in the sstable
-            public static final Component.Type FILTER = Component.Type.createSingleton(""FILTER"", ""Filter.db"", null);
+            public static final Component.Type FILTER = Component.Type.createSingleton(""FILTER"", ""Filter.db"", true, null);
             // holds CRC32 checksum of the data file
-            public static final Component.Type DIGEST = Component.Type.createSingleton(""DIGEST"", ""Digest.crc32"", null);
+            public static final Component.Type DIGEST = Component.Type.createSingleton(""DIGEST"", ""Digest.crc32"", true, null);
             // holds the CRC32 for chunks in an uncompressed file.
-            public static final Component.Type CRC = Component.Type.createSingleton(""CRC"", ""CRC.db"", null);
+            public static final Component.Type CRC = Component.Type.createSingleton(""CRC"", ""CRC.db"", true, null);
             // table of contents, stores the list of all components for the sstable
-            public static final Component.Type TOC = Component.Type.createSingleton(""TOC"", ""TOC.txt"", null);
+            public static final Component.Type TOC = Component.Type.createSingleton(""TOC"", ""TOC.txt"", false, null);
             // built-in secondary index (may exist multiple per sstable)
-            public static final Component.Type SECONDARY_INDEX = Component.Type.create(""SECONDARY_INDEX"", ""SI_.*.db"", null);
+            public static final Component.Type SECONDARY_INDEX = Component.Type.create(""SECONDARY_INDEX"", ""SI_.*.db"", true, null);
             // custom component, used by e.g. custom compaction strategy
-            public static final Component.Type CUSTOM = Component.Type.create(""CUSTOM"", null, null);
+            public static final Component.Type CUSTOM = Component.Type.create(""CUSTOM"", null, true, null);","[{'comment': 'I guess not all CUSTOM components should be streamable?\r\n\r\nIs the plan here to just create something like the other `Types` inner classes, but in SAI\'s `V1OnDiskFormat`? If we did that, we could specify ""streamability"" there in those types. For example:\r\n\r\n`Component.Type TERMS_DATA = Component.Type.create(""TermsData"", ""SAI\\\\+.*\\\\+TermsData.db"", true, null);`\r\n\r\nThis is really where all the stuff in `IndexComponent` probably should have gone in the first place.\r\n\r\nCC @mike-tr-adamson ', 'commenter': 'maedhroz'}, {'comment': '...or more completely, something like...\r\n\r\n```\r\npublic static class Components\r\n{\r\n    public static class Types extends AbstractSSTableFormat.Components.Types\r\n    {\r\n        public static final Component.Type TERMS_DATA = Component.Type.create(""TermsData"", ""SAI\\\\+.*\\\\+TermsData.db"", true, null);\r\n...\r\n    }\r\n\r\n    public final static Component TERMS_DATA = Types.TERMS_DATA.getSingleton();\r\n...\r\n}\r\n```', 'commenter': 'maedhroz'}, {'comment': ""Yes, that is the plan. I haven't finished that yet."", 'commenter': 'pkolaczk'}]"
2420,src/java/org/apache/cassandra/io/sstable/format/big/BigFormat.java,"@@ -82,9 +82,9 @@ public static class Components extends SSTableFormat.Components
         public static class Types extends SSTableFormat.Components.Types
         {
             // index of the row keys with pointers to their positions in the data file
-            public static final Component.Type PRIMARY_INDEX = Component.Type.createSingleton(""PRIMARY_INDEX"", ""Index.db"", BigFormat.class);
+            public static final Component.Type PRIMARY_INDEX = Component.Type.createSingleton(""PRIMARY_INDEX"", ""Index.db"", true, BigFormat.class);","[{'comment': ""We'll need to update [the doc on `SSTable_API.md`](https://github.com/apache/cassandra/blob/cep-7-sai/src/java/org/apache/cassandra/io/sstable/SSTable_API.md#components) to include the new `streamable` parameter."", 'commenter': 'adelapena'}]"
2420,src/java/org/apache/cassandra/io/sstable/Component.java,"@@ -60,31 +61,34 @@ public final static class Type
         /**
          * Creates a new non-singleton type and registers it a global type registry - see {@link #registerType(Type)}.
          *
-         * @param name        type name, must be unique for this and all parent formats
-         * @param repr        the regular expression to be used to recognize a name represents this type
-         * @param formatClass format class for which this type is defined for
+         * @param name         type name, must be unique for this and all parent formats
+         * @param repr         the regular expression to be used to recognize a name represents this type
+         * @param isStreamable whether components of this type should be streamed to other nodes
+         * @param formatClass  format class for which this type is defined for
          */
-        public static Type create(String name, String repr, Class<? extends SSTableFormat<?, ?>> formatClass)
+        public static Type create(String name, String repr, boolean isStreamable, Class<? extends SSTableFormat<?, ?>> formatClass)
         {
-            return new Type(name, repr, false, formatClass);
+            return new Type(name, repr, false, isStreamable, formatClass);","[{'comment': 'Nit: the parameter could be named `streamable` for consistency with the attribute and constructor names.', 'commenter': 'adelapena'}]"
2420,src/java/org/apache/cassandra/io/sstable/Component.java,"@@ -60,31 +61,34 @@ public final static class Type
         /**
          * Creates a new non-singleton type and registers it a global type registry - see {@link #registerType(Type)}.
          *
-         * @param name        type name, must be unique for this and all parent formats
-         * @param repr        the regular expression to be used to recognize a name represents this type
-         * @param formatClass format class for which this type is defined for
+         * @param name         type name, must be unique for this and all parent formats
+         * @param repr         the regular expression to be used to recognize a name represents this type
+         * @param isStreamable whether components of this type should be streamed to other nodes
+         * @param formatClass  format class for which this type is defined for
          */
-        public static Type create(String name, String repr, Class<? extends SSTableFormat<?, ?>> formatClass)
+        public static Type create(String name, String repr, boolean isStreamable, Class<? extends SSTableFormat<?, ?>> formatClass)
         {
-            return new Type(name, repr, false, formatClass);
+            return new Type(name, repr, false, isStreamable, formatClass);
         }
 
         /**
          * Creates a new singleton type and registers it in a global type registry - see {@link #registerType(Type)}.
          *
-         * @param name        type name, must be unique for this and all parent formats
-         * @param repr        the regular expression to be used to recognize a name represents this type
-         * @param formatClass format class for which this type is defined for
+         * @param name         type name, must be unique for this and all parent formats
+         * @param repr         the regular expression to be used to recognize a name represents this type
+         * @param isStreamable whether components of this type should be streamed to other nodes
+         * @param formatClass  format class for which this type is defined for
          */
-        public static Type createSingleton(String name, String repr, Class<? extends SSTableFormat<?, ?>> formatClass)
+        public static Type createSingleton(String name, String repr, boolean isStreamable, Class<? extends SSTableFormat<?, ?>> formatClass)","[{'comment': 'Nit: the parameter could be named `streamable` for consistency with the attribute and constructor names.', 'commenter': 'adelapena'}]"
2420,test/distributed/org/apache/cassandra/distributed/test/sai/IndexStreamingTest.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.sai;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.Row;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.distributed.util.QueryResultUtil;
+import org.apache.cassandra.index.sai.disk.v1.V1OnDiskFormat;
+import org.assertj.core.api.Assertions;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+public class IndexStreamingTest extends TestBaseImpl
+{
+    private static final ByteBuffer BLOB = ByteBuffer.wrap(new byte[1 << 16]);
+    private static final int NUM_COMPONENTS;
+
+    static
+    {
+        DatabaseDescriptor.clientInitialization();
+        NUM_COMPONENTS = sstableStreamingComponentsCount()
+                         + V1OnDiskFormat.PER_SSTABLE_COMPONENTS.size()
+                         + V1OnDiskFormat.LITERAL_COMPONENTS.size();
+    }
+
+    private static int sstableStreamingComponentsCount() {
+        return (int) DatabaseDescriptor.getSelectedSSTableFormat()
+                                       .allComponents()
+                                       .stream()
+                                       .filter(c -> c.type.streamable)
+                                       .count() - 1;  // -1 because we don't include the compression component
+    }
+
+    @Test
+    public void zeroCopy() throws IOException
+    {
+        test(true);
+    }
+
+    @Test
+    public void notZeroCopy() throws IOException
+    {
+        test(false);
+    }
+
+    private void test(boolean zeroCopyStreaming) throws IOException
+    {
+        try (Cluster cluster = init(Cluster.build(2)
+                                           .withConfig(c -> c.with(Feature.values())
+                                                             .set(""stream_entire_sstables"", zeroCopyStreaming).set(""streaming_slow_events_log_timeout"", ""0s""))","[{'comment': 'Nit: break the long line\r\n```suggestion\r\n                                                             .set(""stream_entire_sstables"", zeroCopyStreaming)\r\n                                                             .set(""streaming_slow_events_log_timeout"", ""0s""))\r\n```', 'commenter': 'adelapena'}]"
2420,test/distributed/org/apache/cassandra/distributed/test/sai/IndexStreamingTest.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.sai;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.Row;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.distributed.util.QueryResultUtil;
+import org.apache.cassandra.index.sai.disk.v1.V1OnDiskFormat;
+import org.assertj.core.api.Assertions;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+public class IndexStreamingTest extends TestBaseImpl
+{
+    private static final ByteBuffer BLOB = ByteBuffer.wrap(new byte[1 << 16]);
+    private static final int NUM_COMPONENTS;
+
+    static
+    {
+        DatabaseDescriptor.clientInitialization();
+        NUM_COMPONENTS = sstableStreamingComponentsCount()
+                         + V1OnDiskFormat.PER_SSTABLE_COMPONENTS.size()
+                         + V1OnDiskFormat.LITERAL_COMPONENTS.size();
+    }
+
+    private static int sstableStreamingComponentsCount() {
+        return (int) DatabaseDescriptor.getSelectedSSTableFormat()
+                                       .allComponents()
+                                       .stream()
+                                       .filter(c -> c.type.streamable)
+                                       .count() - 1;  // -1 because we don't include the compression component
+    }
+
+    @Test
+    public void zeroCopy() throws IOException
+    {
+        test(true);
+    }
+
+    @Test
+    public void notZeroCopy() throws IOException
+    {
+        test(false);
+    }
+
+    private void test(boolean zeroCopyStreaming) throws IOException
+    {
+        try (Cluster cluster = init(Cluster.build(2)
+                                           .withConfig(c -> c.with(Feature.values())
+                                                             .set(""stream_entire_sstables"", zeroCopyStreaming).set(""streaming_slow_events_log_timeout"", ""0s""))
+                                           .start()))
+        {
+            // streaming sends events every 65k, so need to make sure that the files are larger than this to hit
+            // all cases of the vtable","[{'comment': 'Maybe this comment would be better placed either right before the `INSERT` or when declaring the `BLOB` size.', 'commenter': 'adelapena'}]"
2420,test/distributed/org/apache/cassandra/distributed/test/sai/IndexStreamingTest.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.sai;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.Row;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.distributed.util.QueryResultUtil;
+import org.apache.cassandra.index.sai.disk.v1.V1OnDiskFormat;
+import org.assertj.core.api.Assertions;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+public class IndexStreamingTest extends TestBaseImpl
+{
+    private static final ByteBuffer BLOB = ByteBuffer.wrap(new byte[1 << 16]);
+    private static final int NUM_COMPONENTS;
+
+    static
+    {
+        DatabaseDescriptor.clientInitialization();
+        NUM_COMPONENTS = sstableStreamingComponentsCount()
+                         + V1OnDiskFormat.PER_SSTABLE_COMPONENTS.size()
+                         + V1OnDiskFormat.LITERAL_COMPONENTS.size();
+    }
+
+    private static int sstableStreamingComponentsCount() {
+        return (int) DatabaseDescriptor.getSelectedSSTableFormat()
+                                       .allComponents()
+                                       .stream()
+                                       .filter(c -> c.type.streamable)
+                                       .count() - 1;  // -1 because we don't include the compression component
+    }
+
+    @Test
+    public void zeroCopy() throws IOException
+    {
+        test(true);
+    }
+
+    @Test
+    public void notZeroCopy() throws IOException
+    {
+        test(false);
+    }
+
+    private void test(boolean zeroCopyStreaming) throws IOException
+    {
+        try (Cluster cluster = init(Cluster.build(2)
+                                           .withConfig(c -> c.with(Feature.values())
+                                                             .set(""stream_entire_sstables"", zeroCopyStreaming).set(""streaming_slow_events_log_timeout"", ""0s""))
+                                           .start()))
+        {
+            // streaming sends events every 65k, so need to make sure that the files are larger than this to hit
+            // all cases of the vtable
+            cluster.schemaChange(withKeyspace(
+                ""CREATE TABLE %s.test (pk int PRIMARY KEY, v text, b blob) WITH compression = { 'enabled' : false };""
+            ));
+            cluster.schemaChange(withKeyspace(
+                ""CREATE CUSTOM INDEX ON %s.test(v) USING 'StorageAttachedIndex';""
+            ));
+            cluster.stream().forEach(i ->
+                i.nodetoolResult(""disableautocompaction"", KEYSPACE).asserts().success()
+            );
+            IInvokableInstance first = cluster.get(1);
+            IInvokableInstance second = cluster.get(2);
+            long sstableCount = 10;
+            long expectedFiles = zeroCopyStreaming ? sstableCount * NUM_COMPONENTS : sstableCount;
+            for (int i = 0; i < sstableCount; i++)
+            {
+                first.executeInternal(withKeyspace(""insert into %s.test(pk, v, b) values (?, ?, ?)""), i, ""v"" + i, BLOB);
+                first.flush(KEYSPACE);
+            }
+
+            second.nodetoolResult(""rebuild"", ""--keyspace"", KEYSPACE).asserts().success();
+
+            SimpleQueryResult qr = first.executeInternalWithResult(""SELECT * FROM system_views.streaming"");
+            String txt = QueryResultUtil.expand(qr);
+            qr.reset();
+            assertThat(qr.toObjectArrays().length).describedAs(""Found rows\n%s"", txt).isEqualTo(1);
+            assertThat(qr.hasNext()).isTrue();
+            Row row = qr.next();
+            QueryResultUtil.assertThat(row)
+                           .isEqualTo(""peers"", Collections.singletonList(""/127.0.0.2:7012""))","[{'comment': '```suggestion\r\n                           .isEqualTo(""peers"", Collections.singletonList(second.broadcastAddress().toString()))\r\n```', 'commenter': 'adelapena'}]"
2420,test/distributed/org/apache/cassandra/distributed/test/sai/IndexStreamingTest.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.sai;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.Row;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.distributed.util.QueryResultUtil;
+import org.apache.cassandra.index.sai.disk.v1.V1OnDiskFormat;
+import org.assertj.core.api.Assertions;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+public class IndexStreamingTest extends TestBaseImpl
+{
+    private static final ByteBuffer BLOB = ByteBuffer.wrap(new byte[1 << 16]);
+    private static final int NUM_COMPONENTS;
+
+    static
+    {
+        DatabaseDescriptor.clientInitialization();
+        NUM_COMPONENTS = sstableStreamingComponentsCount()
+                         + V1OnDiskFormat.PER_SSTABLE_COMPONENTS.size()
+                         + V1OnDiskFormat.LITERAL_COMPONENTS.size();
+    }
+
+    private static int sstableStreamingComponentsCount() {
+        return (int) DatabaseDescriptor.getSelectedSSTableFormat()
+                                       .allComponents()
+                                       .stream()
+                                       .filter(c -> c.type.streamable)
+                                       .count() - 1;  // -1 because we don't include the compression component
+    }
+
+    @Test
+    public void zeroCopy() throws IOException
+    {
+        test(true);
+    }
+
+    @Test
+    public void notZeroCopy() throws IOException
+    {
+        test(false);
+    }
+
+    private void test(boolean zeroCopyStreaming) throws IOException
+    {
+        try (Cluster cluster = init(Cluster.build(2)
+                                           .withConfig(c -> c.with(Feature.values())
+                                                             .set(""stream_entire_sstables"", zeroCopyStreaming).set(""streaming_slow_events_log_timeout"", ""0s""))
+                                           .start()))
+        {
+            // streaming sends events every 65k, so need to make sure that the files are larger than this to hit
+            // all cases of the vtable
+            cluster.schemaChange(withKeyspace(
+                ""CREATE TABLE %s.test (pk int PRIMARY KEY, v text, b blob) WITH compression = { 'enabled' : false };""
+            ));
+            cluster.schemaChange(withKeyspace(
+                ""CREATE CUSTOM INDEX ON %s.test(v) USING 'StorageAttachedIndex';""
+            ));
+            cluster.stream().forEach(i ->
+                i.nodetoolResult(""disableautocompaction"", KEYSPACE).asserts().success()
+            );
+            IInvokableInstance first = cluster.get(1);
+            IInvokableInstance second = cluster.get(2);
+            long sstableCount = 10;
+            long expectedFiles = zeroCopyStreaming ? sstableCount * NUM_COMPONENTS : sstableCount;
+            for (int i = 0; i < sstableCount; i++)
+            {
+                first.executeInternal(withKeyspace(""insert into %s.test(pk, v, b) values (?, ?, ?)""), i, ""v"" + i, BLOB);
+                first.flush(KEYSPACE);
+            }
+
+            second.nodetoolResult(""rebuild"", ""--keyspace"", KEYSPACE).asserts().success();
+
+            SimpleQueryResult qr = first.executeInternalWithResult(""SELECT * FROM system_views.streaming"");
+            String txt = QueryResultUtil.expand(qr);
+            qr.reset();
+            assertThat(qr.toObjectArrays().length).describedAs(""Found rows\n%s"", txt).isEqualTo(1);
+            assertThat(qr.hasNext()).isTrue();
+            Row row = qr.next();
+            QueryResultUtil.assertThat(row)
+                           .isEqualTo(""peers"", Collections.singletonList(""/127.0.0.2:7012""))
+                           .isEqualTo(""follower"", true)
+                           .isEqualTo(""operation"", ""Rebuild"")
+                           .isEqualTo(""status"", ""success"")
+                           .isEqualTo(""progress_percentage"", 100.0F)
+                           .isEqualTo(""success_message"", null).isEqualTo(""failure_cause"", null)
+                           .isEqualTo(""files_sent"", expectedFiles)
+                           .columnsEqualTo(""files_sent"", ""files_to_send"")
+                           .columnsEqualTo(""bytes_sent"", ""bytes_to_send"")
+                           .isEqualTo(""files_received"", 0L)
+                           .columnsEqualTo(""files_received"", ""files_to_receive"", ""bytes_received"", ""bytes_to_receive"");
+            long totalBytes = row.getLong(""bytes_sent"");
+            assertThat(totalBytes).isGreaterThan(0);
+
+            qr = second.executeInternalWithResult(""SELECT * FROM system_views.streaming"");
+            txt = QueryResultUtil.expand(qr);
+            qr.reset();
+            assertThat(qr.toObjectArrays().length).describedAs(""Found rows\n%s"", txt).isEqualTo(1);
+            assertThat(qr.hasNext()).isTrue();
+
+            QueryResultUtil.assertThat(qr.next())
+                           .isEqualTo(""peers"", Collections.singletonList(""/127.0.0.1:7012""))","[{'comment': '```suggestion\r\n                           .isEqualTo(""peers"", Collections.singletonList(first.broadcastAddress().toString()))\r\n```', 'commenter': 'adelapena'}]"
2420,test/distributed/org/apache/cassandra/distributed/test/sai/IndexStreamingTest.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.sai;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.Row;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.distributed.util.QueryResultUtil;
+import org.apache.cassandra.index.sai.disk.v1.V1OnDiskFormat;
+import org.assertj.core.api.Assertions;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+public class IndexStreamingTest extends TestBaseImpl
+{
+    private static final ByteBuffer BLOB = ByteBuffer.wrap(new byte[1 << 16]);
+    private static final int NUM_COMPONENTS;
+
+    static
+    {
+        DatabaseDescriptor.clientInitialization();
+        NUM_COMPONENTS = sstableStreamingComponentsCount()
+                         + V1OnDiskFormat.PER_SSTABLE_COMPONENTS.size()
+                         + V1OnDiskFormat.LITERAL_COMPONENTS.size();
+    }
+
+    private static int sstableStreamingComponentsCount() {
+        return (int) DatabaseDescriptor.getSelectedSSTableFormat()
+                                       .allComponents()
+                                       .stream()
+                                       .filter(c -> c.type.streamable)
+                                       .count() - 1;  // -1 because we don't include the compression component
+    }
+
+    @Test
+    public void zeroCopy() throws IOException
+    {
+        test(true);
+    }
+
+    @Test
+    public void notZeroCopy() throws IOException
+    {
+        test(false);
+    }
+
+    private void test(boolean zeroCopyStreaming) throws IOException
+    {
+        try (Cluster cluster = init(Cluster.build(2)
+                                           .withConfig(c -> c.with(Feature.values())
+                                                             .set(""stream_entire_sstables"", zeroCopyStreaming).set(""streaming_slow_events_log_timeout"", ""0s""))
+                                           .start()))
+        {
+            // streaming sends events every 65k, so need to make sure that the files are larger than this to hit
+            // all cases of the vtable
+            cluster.schemaChange(withKeyspace(
+                ""CREATE TABLE %s.test (pk int PRIMARY KEY, v text, b blob) WITH compression = { 'enabled' : false };""
+            ));
+            cluster.schemaChange(withKeyspace(
+                ""CREATE CUSTOM INDEX ON %s.test(v) USING 'StorageAttachedIndex';""
+            ));
+            cluster.stream().forEach(i ->
+                i.nodetoolResult(""disableautocompaction"", KEYSPACE).asserts().success()
+            );
+            IInvokableInstance first = cluster.get(1);
+            IInvokableInstance second = cluster.get(2);
+            long sstableCount = 10;
+            long expectedFiles = zeroCopyStreaming ? sstableCount * NUM_COMPONENTS : sstableCount;
+            for (int i = 0; i < sstableCount; i++)
+            {
+                first.executeInternal(withKeyspace(""insert into %s.test(pk, v, b) values (?, ?, ?)""), i, ""v"" + i, BLOB);
+                first.flush(KEYSPACE);
+            }
+
+            second.nodetoolResult(""rebuild"", ""--keyspace"", KEYSPACE).asserts().success();
+
+            SimpleQueryResult qr = first.executeInternalWithResult(""SELECT * FROM system_views.streaming"");
+            String txt = QueryResultUtil.expand(qr);
+            qr.reset();
+            assertThat(qr.toObjectArrays().length).describedAs(""Found rows\n%s"", txt).isEqualTo(1);
+            assertThat(qr.hasNext()).isTrue();
+            Row row = qr.next();
+            QueryResultUtil.assertThat(row)
+                           .isEqualTo(""peers"", Collections.singletonList(""/127.0.0.2:7012""))
+                           .isEqualTo(""follower"", true)
+                           .isEqualTo(""operation"", ""Rebuild"")
+                           .isEqualTo(""status"", ""success"")
+                           .isEqualTo(""progress_percentage"", 100.0F)
+                           .isEqualTo(""success_message"", null).isEqualTo(""failure_cause"", null)
+                           .isEqualTo(""files_sent"", expectedFiles)
+                           .columnsEqualTo(""files_sent"", ""files_to_send"")
+                           .columnsEqualTo(""bytes_sent"", ""bytes_to_send"")
+                           .isEqualTo(""files_received"", 0L)
+                           .columnsEqualTo(""files_received"", ""files_to_receive"", ""bytes_received"", ""bytes_to_receive"");
+            long totalBytes = row.getLong(""bytes_sent"");
+            assertThat(totalBytes).isGreaterThan(0);
+
+            qr = second.executeInternalWithResult(""SELECT * FROM system_views.streaming"");
+            txt = QueryResultUtil.expand(qr);
+            qr.reset();
+            assertThat(qr.toObjectArrays().length).describedAs(""Found rows\n%s"", txt).isEqualTo(1);
+            assertThat(qr.hasNext()).isTrue();
+
+            QueryResultUtil.assertThat(qr.next())
+                           .isEqualTo(""peers"", Collections.singletonList(""/127.0.0.1:7012""))
+                           .isEqualTo(""follower"", false)
+                           .isEqualTo(""operation"", ""Rebuild"")
+                           .isEqualTo(""status"", ""success"")
+                           .isEqualTo(""progress_percentage"", 100.0F)
+                           .isEqualTo(""success_message"", null).isEqualTo(""failure_cause"", null)
+                           .columnsEqualTo(""files_to_receive"", ""files_received"").isEqualTo(""files_received"", expectedFiles)
+                           .columnsEqualTo(""bytes_to_receive"", ""bytes_received"").isEqualTo(""bytes_received"", totalBytes)
+                           .columnsEqualTo(""files_sent"", ""files_to_send"", ""bytes_sent"", ""bytes_to_send"").isEqualTo(""files_sent"", 0L);
+
+            // did we trigger slow event log?
+            cluster.forEach(i -> Assertions.assertThat(i.logs().grep(""Handling streaming events took longer than"").getResult()).describedAs(""Unable to find slow log for node%d"", i.config().num()).isNotEmpty());","[{'comment': 'Nit: break the long line\r\n```suggestion\r\n            cluster.forEach(i -> Assertions.assertThat(i.logs().grep(""Handling streaming events took longer than"").getResult())\r\n                                           .describedAs(""Unable to find slow log for node%d"", i.config().num())\r\n                                           .isNotEmpty());\r\n```', 'commenter': 'adelapena'}]"
2420,src/java/org/apache/cassandra/io/sstable/SSTableZeroCopyWriter.java,"@@ -195,14 +197,15 @@ public void close()
             writer.close();
     }
 
-    public void writeComponent(Component.Type type, DataInputPlus in, long size) throws ClosedChannelException
+    public void writeComponent(Component component, DataInputPlus in, long size) throws ClosedChannelException
     {
-        logger.info(""Writing component {} to {} length {}"", type, componentWriters.get(type).getPath(), prettyPrintMemory(size));
+        SequentialWriter writer = componentWriters.get(component.name);","[{'comment': ""Build fails here with `eclipse-warnings` saying `Potential resource leak: 'writer' may not be closed`"", 'commenter': 'adelapena'}]"
2420,src/java/org/apache/cassandra/io/sstable/SSTableZeroCopyWriter.java,"@@ -61,12 +61,14 @@ public SSTableZeroCopyWriter(Builder<?, ?> builder,
         lifecycleNewTracker.trackNew(this);
         this.componentWriters = new HashMap<>();
 
-        if (!descriptor.getFormat().streamingComponents().containsAll(components))
-            throw new AssertionError(format(""Unsupported streaming component detected %s"",
-                                            Sets.difference(ImmutableSet.copyOf(components), descriptor.getFormat().streamingComponents())));
+        Set<Component> unsupported = components.stream()
+                                               .filter(c -> !c.type.streamable)
+                                               .collect(Collectors.toSet());
+        if (!unsupported.isEmpty())
+            throw new AssertionError(format(""Unsupported streaming component detected %s"", unsupported));","[{'comment': '```suggestion\r\n            throw new AssertionError(format(""Unsupported streaming components detected: %s"", unsupported));\r\n```', 'commenter': 'maedhroz'}]"
2420,test/distributed/org/apache/cassandra/distributed/test/sai/IndexStreamingTest.java,"@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.distributed.test.sai;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collections;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.Feature;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.Row;
+import org.apache.cassandra.distributed.api.SimpleQueryResult;
+import org.apache.cassandra.distributed.test.TestBaseImpl;
+import org.apache.cassandra.distributed.util.QueryResultUtil;
+import org.apache.cassandra.index.sai.disk.v1.V1OnDiskFormat;
+import org.assertj.core.api.Assertions;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+public class IndexStreamingTest extends TestBaseImpl
+{
+    private static final ByteBuffer BLOB = ByteBuffer.wrap(new byte[1 << 16]);
+    private static final int NUM_COMPONENTS;
+
+    static
+    {
+        DatabaseDescriptor.clientInitialization();
+        NUM_COMPONENTS = sstableStreamingComponentsCount()
+                         + V1OnDiskFormat.PER_SSTABLE_COMPONENTS.size()
+                         + V1OnDiskFormat.LITERAL_COMPONENTS.size();
+    }
+
+    private static int sstableStreamingComponentsCount() {","[{'comment': 'nit: newline for `{`', 'commenter': 'maedhroz'}]"
2456,src/java/org/apache/cassandra/utils/SyncUtil.java,"@@ -149,18 +106,8 @@ public static void sync(FileDescriptor fd) throws SyncFailedException
                 throw new RuntimeException(e);
             }
 
-            int useCount = 1;
-            try
-            {
-                if (fdUseCountField != null)
-                    useCount = ((AtomicInteger)fdUseCountField.get(fd)).get();
-            }
-            catch (Exception e)
-            {
-                throw new RuntimeException(e);
-            }
-            if (closed || !fd.valid() || useCount < 0)
-                throw new SyncFailedException(""Closed "" + closed + "" valid "" + fd.valid() + "" useCount "" + useCount);
+            if (closed || !fd.valid())","[{'comment': 'This ""sync"" method is unused and can be removed entirely. Then, we can drop all the reflections from this class.', 'commenter': 'jacek-lewandowski'}]"
2458,src/java/org/apache/cassandra/locator/AlibabaCloudSnitch.java,"@@ -17,130 +17,47 @@
  */
 package org.apache.cassandra.locator;
 
-import java.io.DataInputStream;
-import java.io.FilterInputStream;
 import java.io.IOException;
-import java.net.HttpURLConnection;
-import java.net.MalformedURLException;
-import java.net.SocketTimeoutException;
-import java.net.URL;
-import java.nio.charset.StandardCharsets;
-import java.util.Map;
-import org.apache.cassandra.db.SystemKeyspace;
-import org.apache.cassandra.exceptions.ConfigurationException;
-import org.apache.cassandra.gms.ApplicationState;
-import org.apache.cassandra.gms.EndpointState;
-import org.apache.cassandra.gms.Gossiper;
-import org.apache.cassandra.io.util.FileUtils;
-import org.apache.cassandra.utils.FBUtilities;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.DefaultCloudMetadataServiceConnector;
+
+import static org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.METADATA_URL_PROPERTY;
 
 /**
- *  A snitch that assumes an ECS region is a DC and an ECS availability_zone
- *  is a rack. This information is available in the config for the node. the 
- *  format of the zone-id is like :cn-hangzhou-a where cn means china, hangzhou
- *  means the hangzhou region, a means the az id. We use cn-hangzhou as the dc,
- *  and f as the zone-id.
+ * A snitch that assumes an ECS region is a DC and an ECS availability_zone
+ * is a rack. This information is available in the config for the node. the
+ * format of the zone-id is like :cn-hangzhou-a where cn means china, hangzhou
+ * means the hangzhou region, a means the az id. We use cn-hangzhou as the dc,
+ * and f as the zone-id.","[{'comment': 'sorry I make a misstake at the first time push this pr, this should be ""a as the zone-id"" not f.', 'commenter': 'Maxwell-Guo'}]"
2458,src/java/org/apache/cassandra/locator/AlibabaCloudSnitch.java,"@@ -17,130 +17,47 @@
  */
 package org.apache.cassandra.locator;
 
-import java.io.DataInputStream;
-import java.io.FilterInputStream;
 import java.io.IOException;
-import java.net.HttpURLConnection;
-import java.net.MalformedURLException;
-import java.net.SocketTimeoutException;
-import java.net.URL;
-import java.nio.charset.StandardCharsets;
-import java.util.Map;
-import org.apache.cassandra.db.SystemKeyspace;
-import org.apache.cassandra.exceptions.ConfigurationException;
-import org.apache.cassandra.gms.ApplicationState;
-import org.apache.cassandra.gms.EndpointState;
-import org.apache.cassandra.gms.Gossiper;
-import org.apache.cassandra.io.util.FileUtils;
-import org.apache.cassandra.utils.FBUtilities;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.DefaultCloudMetadataServiceConnector;
+
+import static org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.METADATA_URL_PROPERTY;
 
 /**
- *  A snitch that assumes an ECS region is a DC and an ECS availability_zone
- *  is a rack. This information is available in the config for the node. the 
- *  format of the zone-id is like :cn-hangzhou-a where cn means china, hangzhou
- *  means the hangzhou region, a means the az id. We use cn-hangzhou as the dc,
- *  and f as the zone-id.
+ * A snitch that assumes an ECS region is a DC and an ECS availability_zone
+ * is a rack. This information is available in the config for the node. the
+ * format of the zone-id is like :cn-hangzhou-a where cn means china, hangzhou
+ * means the hangzhou region, a means the az id. We use cn-hangzhou as the dc,
+ * and f as the zone-id.
  */
-public class AlibabaCloudSnitch extends AbstractNetworkTopologySnitch
+public class AlibabaCloudSnitch extends AbstractCloudMetadataServiceSnitch
 {
-    protected static final Logger logger = LoggerFactory.getLogger(AlibabaCloudSnitch.class);
-    protected static final String ZONE_NAME_QUERY_URL = ""http://100.100.100.200/latest/meta-data/zone-id"";
-    private static final String DEFAULT_DC = ""UNKNOWN-DC"";
-    private static final String DEFAULT_RACK = ""UNKNOWN-RACK"";
-    private Map<InetAddressAndPort, Map<String, String>> savedEndpoints; 
-    protected String ecsZone;
-    protected String ecsRegion;
-    
-    private static final int HTTP_CONNECT_TIMEOUT = 30000;
-    
-    
-    public AlibabaCloudSnitch() throws MalformedURLException, IOException 
-    {
-        String response = alibabaApiCall(ZONE_NAME_QUERY_URL);
-        String[] splits = response.split(""/"");
-        String az = splits[splits.length - 1];
-
-        // Split ""us-central1-a"" or ""asia-east1-a"" into ""us-central1""/""a"" and ""asia-east1""/""a"".
-        splits = az.split(""-"");
-        ecsZone = splits[splits.length - 1];
-
-        int lastRegionIndex = az.lastIndexOf(""-"");
-        ecsRegion = az.substring(0, lastRegionIndex);
+    private static final String DEFAULT_METADATA_SERVICE_URL = ""http://100.100.100.200"";
+    private static final String ZONE_NAME_QUERY_URL = ""/latest/meta-data/zone-id"";
 
-        String datacenterSuffix = (new SnitchProperties()).get(""dc_suffix"", """");
-        ecsRegion = ecsRegion.concat(datacenterSuffix);
-        logger.info(""AlibabaSnitch using region: {}, zone: {}."", ecsRegion, ecsZone);
-    
-    }
-    
-    String alibabaApiCall(String url) throws ConfigurationException, IOException, SocketTimeoutException
+    public AlibabaCloudSnitch() throws IOException
     {
-        // Populate the region and zone by introspection, fail if 404 on metadata
-        HttpURLConnection conn = (HttpURLConnection) new URL(url).openConnection();
-        DataInputStream d = null;
-        try
-        {
-            conn.setConnectTimeout(HTTP_CONNECT_TIMEOUT);
-            conn.setRequestMethod(""GET"");
-            
-            int code = conn.getResponseCode();
-            if (code != HttpURLConnection.HTTP_OK)
-                throw new ConfigurationException(""AlibabaSnitch was unable to execute the API call. Not an ecs node? and the returun code is "" + code);
-
-            // Read the information. I wish I could say (String) conn.getContent() here...
-            int cl = conn.getContentLength();
-            byte[] b = new byte[cl];
-            d = new DataInputStream((FilterInputStream) conn.getContent());
-            d.readFully(b);
-            return new String(b, StandardCharsets.UTF_8);
-        }
-        catch (SocketTimeoutException e)
-        {
-            throw new SocketTimeoutException(""Timeout occurred reading a response from the Alibaba ECS metadata"");
-        }
-        finally
-        {
-            FileUtils.close(d);
-            conn.disconnect();
-        }
+        this(new SnitchProperties());
     }
-    
-    @Override
-    public String getRack(InetAddressAndPort endpoint)
+
+    public AlibabaCloudSnitch(SnitchProperties properties) throws IOException
     {
-        if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort()))
-            return ecsZone;
-        EndpointState state = Gossiper.instance.getEndpointStateForEndpoint(endpoint);
-        if (state == null || state.getApplicationState(ApplicationState.RACK) == null)
-        {
-            if (savedEndpoints == null)
-                savedEndpoints = SystemKeyspace.loadDcRackInfo();
-            if (savedEndpoints.containsKey(endpoint))
-                return savedEndpoints.get(endpoint).get(""rack"");
-            return DEFAULT_RACK;
-        }
-        return state.getApplicationState(ApplicationState.RACK).value;
-    
+        this(properties, new DefaultCloudMetadataServiceConnector(properties.get(METADATA_URL_PROPERTY,","[{'comment': 'what about merge METADATA_URL_PROPERTY and ZONE_NAME_QUERY_URL  together as before not just split them into two Strings and make them configurable. \r\nor make ZONE_NAME_QUERY_URL configurable through Snitchpropetry ? (In this way I may suggest changing the name to ZONE_NAME_QUERY_URL_PATH)', 'commenter': 'Maxwell-Guo'}, {'comment': 'See above comment about why it is split into two properties.\r\n\r\nOnce we implement ""DefaultCloudServiceSnitch"", we can do it like 1 address with whole endpoint.', 'commenter': 'smiklosovic'}]"
2458,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceConnector.java,"@@ -25,22 +25,39 @@
 import java.net.URL;
 import java.util.Map;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.ImmutableMap;
 
 import static java.nio.charset.StandardCharsets.UTF_8;
 
 abstract class AbstractCloudMetadataServiceConnector
 {
+    static final String METADATA_URL_PROPERTY = ""metadata_url"";
+
+    public static class DefaultCloudMetadataServiceConnector extends AbstractCloudMetadataServiceConnector
+    {
+        protected DefaultCloudMetadataServiceConnector(String metadataServiceUrl)","[{'comment': 'as said before , what about make this metaserviceUrl with the full url path ,not only the url address.\r\nOr make both zone path configurable which may be more flexible.', 'commenter': 'Maxwell-Guo'}, {'comment': 'It is metadata service address only because that is variable we configure in mocking tests. See `Ec2V2ConnectorMockingTest` in trunk.\r\n\r\nSo the idea is that we change just the service url but not the endpoints.', 'commenter': 'smiklosovic'}]"
2458,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceSnitch.java,"@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.SystemKeyspace;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.EndpointState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.utils.FBUtilities;
+
+abstract class AbstractCloudMetadataServiceSnitch extends AbstractNetworkTopologySnitch
+{
+    static final Logger logger = LoggerFactory.getLogger(AbstractCloudMetadataServiceSnitch.class);
+
+    static final String DEFAULT_DC = ""UNKNOWN-DC"";
+    static final String DEFAULT_RACK = ""UNKNOWN-RACK"";
+
+    protected final AbstractCloudMetadataServiceConnector connector;
+    protected final SnitchProperties snitchProperties;
+
+    protected String localRack;","[{'comment': 'So my preference is that those two fields are private final in the subclasses and in this class we only have abstract methods like `getLocalRack()` and `getLocalDC()` which are called from `getRack(endpoint)` and `getDatacenter(endpoint)`', 'commenter': 'jacek-lewandowski'}]"
2458,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceSnitch.java,"@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.SystemKeyspace;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.EndpointState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.utils.FBUtilities;
+
+abstract class AbstractCloudMetadataServiceSnitch extends AbstractNetworkTopologySnitch
+{
+    static final Logger logger = LoggerFactory.getLogger(AbstractCloudMetadataServiceSnitch.class);
+
+    static final String DEFAULT_DC = ""UNKNOWN-DC"";
+    static final String DEFAULT_RACK = ""UNKNOWN-RACK"";
+
+    protected final AbstractCloudMetadataServiceConnector connector;
+    protected final SnitchProperties snitchProperties;
+
+    protected String localRack;
+    protected String localDc;
+
+    private Map<InetAddressAndPort, Map<String, String>> savedEndpoints;
+
+    public AbstractCloudMetadataServiceSnitch(AbstractCloudMetadataServiceConnector connector, SnitchProperties snitchProperties)
+    {
+        this.connector = connector;
+        this.snitchProperties = snitchProperties;
+    }
+
+    @Override
+    public String getRack(InetAddressAndPort endpoint)
+    {
+        if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort()))
+            return localRack;
+        EndpointState state = Gossiper.instance.getEndpointStateForEndpoint(endpoint);
+        if (state == null || state.getApplicationState(ApplicationState.RACK) == null)
+        {
+            if (savedEndpoints == null)
+                savedEndpoints = SystemKeyspace.loadDcRackInfo();
+            if (savedEndpoints.containsKey(endpoint))
+                return savedEndpoints.get(endpoint).get(""rack"");
+            return DEFAULT_RACK;
+        }
+        return state.getApplicationState(ApplicationState.RACK).value;
+
+    }
+
+    @Override
+    public String getDatacenter(InetAddressAndPort endpoint)
+    {
+        if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort()))
+            return localDc;
+        EndpointState state = Gossiper.instance.getEndpointStateForEndpoint(endpoint);
+        if (state == null || state.getApplicationState(ApplicationState.DC) == null)
+        {
+            if (savedEndpoints == null)
+                savedEndpoints = SystemKeyspace.loadDcRackInfo();
+            if (savedEndpoints.containsKey(endpoint))
+                return savedEndpoints.get(endpoint).get(""data_center"");
+            return DEFAULT_DC;
+        }
+        return state.getApplicationState(ApplicationState.DC).value;
+    }
+
+    protected void parseResponse(String response)","[{'comment': 'I think that this method does not belong to this abstract implementation. There should be no details about the response interpretation. If this method is used in many places, perhaps it could go as static to something like `SnitchUtils`.', 'commenter': 'jacek-lewandowski'}]"
2458,src/java/org/apache/cassandra/locator/Ec2MetadataServiceConnector.java,"@@ -45,7 +45,7 @@ abstract class Ec2MetadataServiceConnector extends AbstractCloudMetadataServiceC
 
     Ec2MetadataServiceConnector(SnitchProperties properties)
     {
-        super(properties.get(EC2_METADATA_URL_PROPERTY, DEFAULT_EC2_METADATA_URL));
+        super(properties.get(EC2_METADATA_URL_PROPERTY, properties.get(METADATA_URL_PROPERTY, DEFAULT_EC2_METADATA_URL)));","[{'comment': 'why do we need this?', 'commenter': 'jacek-lewandowski'}, {'comment': 'Good question.\r\n\r\n`METADATA_URL_PROPERTY = ""metadata_url""`\r\n\r\nand \r\n\r\n`EC2_METADATA_URL_PROPERTY = ""ec2_metadata_url""`.\r\n\r\nThe latter was introduced when we were doing IDMSv2 for AWS but with this PR in place, I wanted to be backward compatible.\r\n\r\nAs `ec2_metadata_url` key is in 3.0 -> 4.1 but it would not be in trunk, then upgrading to trunk would fail because that key would not be there anymore. \r\n\r\nOn the other hand, if one deploys it to trunk for the first time, it can set the url like ""metadata_url"" and I like the idea that metadata_url property would be same for all cloud snitches.\r\n\r\n', 'commenter': 'smiklosovic'}, {'comment': 'perhaps we should not bother that much - who would set it to something else than a default value?', 'commenter': 'jacek-lewandowski'}, {'comment': 'yeah ... true, hm.', 'commenter': 'smiklosovic'}, {'comment': 'anyway it does not hurt if it is here.', 'commenter': 'smiklosovic'}]"
2458,src/java/org/apache/cassandra/locator/DefaultCloudMetadataServiceSnitch.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.DefaultCloudMetadataServiceConnector;
+
+import static org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.METADATA_URL_PROPERTY;
+
+/**
+ * Default cloud-based metadata service snitch which parses datacenter and rack of a node by
+ * executing GET query for URL configured in cassandra-rackdc.properties under
+ * key {@link AbstractCloudMetadataServiceConnector#METADATA_URL_PROPERTY}. It expects HTTP response code 200.
+ * It is not passing any headers to GET request.
+ * <p>
+ * The response will be parsed like following - if the response body is, for example ""us-central1-a"", then
+ * the datacenter will be ""us-central1"" and rack will be ""a"". There is value of ""dc_suffix"" key in
+ * cassandra-rackdc.properties appended to datacenter.
+ */
+public class DefaultCloudMetadataServiceSnitch extends AbstractCloudMetadataServiceSnitch","[{'comment': 'does this class need to be public?', 'commenter': 'jacek-lewandowski'}, {'comment': 'Yes, I believe it needs to be public. Keep in mind that we might reference this snitch in `cassandra.yaml` so we would be able to use this snitch for all cloud deployments when the response is parsed in a very opinionated way, for free, out of the box. \r\n\r\nIf this was not public, then it is not able to be constructed as making it package protected would throw:\r\n\r\n    org.apache.cassandra.exceptions.ConfigurationException: Default constructor for snitch class \r\n    \'org.apache.cassandra.locator.MySnitch\' is inaccessible.\r\n\tat org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:762)\r\n\tat org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:751)\r\n\tat org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:1491)\r\n\tat org.apache.cassandra.config.DatabaseDescriptor.applySnitch(DatabaseDescriptor.java:1338)\r\n\tat org.apache.cassandra.config.DatabaseDescriptor.applyAll(DatabaseDescriptor.java:435)\r\n\tat org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:244)\r\n\tat org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:228)\r\n\tat org.apache.cassandra.service.CassandraDaemon.applyConfig(CassandraDaemon.java:819)\r\n\tat org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:762)\r\n\tat org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:890)\r\n\r\nWith snitch like this (same holds for the DefaultCloudMetadataServiceSnitch)\r\n\r\n    class MySnitch extends AbstractNetworkTopologySnitch\r\n    {\r\n        @Override\r\n        public String getRack(InetAddressAndPort endpoint)\r\n        {\r\n            return ""r1"";\r\n        }\r\n\r\n        @Override\r\n        public String getDatacenter(InetAddressAndPort endpoint)\r\n        {\r\n            return ""dc1"";\r\n        }\r\n    }\r\n\r\nThis does not help either\r\n\r\n    class MySnitch extends AbstractNetworkTopologySnitch\r\n    {\r\n         public MySnitch() {}\r\n         ....\r\n    }\r\n\r\nWe can not make it final either because `AlibabaCloudSnitch` is extending it.', 'commenter': 'smiklosovic'}]"
2458,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceConnector.java,"@@ -25,22 +25,41 @@
 import java.net.URL;
 import java.util.Map;
 
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 
 import static java.nio.charset.StandardCharsets.UTF_8;
 
 abstract class AbstractCloudMetadataServiceConnector
 {
+    static final String METADATA_URL_PROPERTY = ""metadata_url"";","[{'comment': 'CassandraRelevantProperties?', 'commenter': 'jacek-lewandowski'}, {'comment': 'I dont think that is actually necessary. It is not like this property is a _system property_. This is just a key of a property in cassandra-rackdc.properties.', 'commenter': 'smiklosovic'}, {'comment': 'ah, yes, right', 'commenter': 'jacek-lewandowski'}]"
2458,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceConnector.java,"@@ -25,22 +25,41 @@
 import java.net.URL;
 import java.util.Map;
 
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 
 import static java.nio.charset.StandardCharsets.UTF_8;
 
 abstract class AbstractCloudMetadataServiceConnector
 {
+    static final String METADATA_URL_PROPERTY = ""metadata_url"";
+
+    public static class DefaultCloudMetadataServiceConnector extends AbstractCloudMetadataServiceConnector
+    {
+        protected DefaultCloudMetadataServiceConnector(String metadataServiceUrl)
+        {
+            super(metadataServiceUrl);
+        }
+    }
+
     protected final String metadataServiceUrl;
 
+    @VisibleForTesting // for mockito
+    protected String getMetadataServiceUrl()
+    {
+        return metadataServiceUrl;
+    }
+
     protected AbstractCloudMetadataServiceConnector(String metadataServiceUrl)
     {
+        Preconditions.checkNotNull(metadataServiceUrl, METADATA_URL_PROPERTY + "" for a connector can not be null"");","[{'comment': 'Since you do validation, maybe also validate if it is a valid URL?', 'commenter': 'jacek-lewandowski'}]"
2458,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceConnector.java,"@@ -25,22 +25,41 @@
 import java.net.URL;
 import java.util.Map;
 
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 
 import static java.nio.charset.StandardCharsets.UTF_8;
 
 abstract class AbstractCloudMetadataServiceConnector
 {
+    static final String METADATA_URL_PROPERTY = ""metadata_url"";
+
+    public static class DefaultCloudMetadataServiceConnector extends AbstractCloudMetadataServiceConnector
+    {
+        protected DefaultCloudMetadataServiceConnector(String metadataServiceUrl)
+        {
+            super(metadataServiceUrl);
+        }
+    }
+
     protected final String metadataServiceUrl;
 
+    @VisibleForTesting // for mockito
+    protected String getMetadataServiceUrl()
+    {
+        return metadataServiceUrl;
+    }
+
     protected AbstractCloudMetadataServiceConnector(String metadataServiceUrl)
     {
+        Preconditions.checkNotNull(metadataServiceUrl, METADATA_URL_PROPERTY + "" for a connector can not be null"");
         this.metadataServiceUrl = metadataServiceUrl;
     }
 
     public String apiCall(String query) throws IOException
     {
-        return apiCall(metadataServiceUrl, query, 200);
+        return apiCall(getMetadataServiceUrl(), query, 200);","[{'comment': 'why?', 'commenter': 'jacek-lewandowski'}, {'comment': 'This is here due to tests. I had to mock what that method would return, this can not be done on a field.', 'commenter': 'smiklosovic'}, {'comment': 'Nevertheless, I think this is not needed. I saw your mock and the reason why it fails with NPE is passing `anyString()` as url arg for `apiCall` - simply change it to `any()` and it will accept nulls.\r\n', 'commenter': 'jacek-lewandowski'}]"
2458,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceConnector.java,"@@ -61,6 +80,7 @@ public String apiCall(String url,
             conn = (HttpURLConnection) new URL(url + query).openConnection();
             extraHeaders.forEach(conn::setRequestProperty);
             conn.setRequestMethod(method);
+            conn.setConnectTimeout(30000);","[{'comment': 'Should we take the timeout from the properties with 30s as default?', 'commenter': 'jacek-lewandowski'}, {'comment': 'well it is tricky to pass anything to these connectors from the properties as currently it is accepting metadata service url only. We might either refactor bunch of stuff for such a triviality or we can just have a system property in `CassandraRelevantProperties`? It is hard to believe this would be something which would be tweaked by users regularly, maybe for power users as the ultimate way to weak it but otherwise .... ', 'commenter': 'smiklosovic'}, {'comment': ""since we have a generic metadata_url - why not just have a generic metadata_request_timeout in snitch properties? Though, I don't insist on this one 😃 "", 'commenter': 'jacek-lewandowski'}]"
2458,src/java/org/apache/cassandra/locator/DefaultCloudMetadataServiceSnitch.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.DefaultCloudMetadataServiceConnector;
+
+import static org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.METADATA_URL_PROPERTY;
+
+/**
+ * Default cloud-based metadata service snitch which parses datacenter and rack of a node by
+ * executing GET query for URL configured in cassandra-rackdc.properties under
+ * key {@link AbstractCloudMetadataServiceConnector#METADATA_URL_PROPERTY}. It expects HTTP response code 200.
+ * It is not passing any headers to GET request.
+ * <p>
+ * The response will be parsed like following - if the response body is, for example ""us-central1-a"", then
+ * the datacenter will be ""us-central1"" and rack will be ""a"". There is value of ""dc_suffix"" key in
+ * cassandra-rackdc.properties appended to datacenter.
+ */
+public class DefaultCloudMetadataServiceSnitch extends AbstractCloudMetadataServiceSnitch
+{
+    public DefaultCloudMetadataServiceSnitch() throws IOException
+    {
+        this(new SnitchProperties());
+    }
+
+    public DefaultCloudMetadataServiceSnitch(SnitchProperties properties) throws IOException","[{'comment': 'maybe let it accept the default url as argument so that the inheritors can set it directly?', 'commenter': 'jacek-lewandowski'}, {'comment': ""I don't think that is really necessary. \r\n\r\nI can do this though.\r\n\r\n    public DefaultCloudMetadataServiceSnitch(String url) throws IOException\r\n    {\r\n        this(new SnitchProperties(Pair.create(METADATA_URL_PROPERTY, url)));\r\n    }\r\n\r\nIf you look how the response is parsed etc, we also need dc suffix propagated and in case like I did it it would not be parsed from property file.\r\n\r\nI can also do this\r\n\r\n    public DefaultCloudMetadataServiceSnitch(SnitchProperties properties, String url) throws IOException\r\n    {\r\n        this(properties, new DefaultCloudMetadataServiceConnector(url));\r\n    }\r\n\r\nBut why would you do that if it is going to be taken from `properties` anyway as we need them?"", 'commenter': 'smiklosovic'}]"
2458,src/java/org/apache/cassandra/locator/DefaultCloudMetadataServiceSnitch.java,"@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.ImmutableMap;
+
+import org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.DefaultCloudMetadataServiceConnector;
+
+import static org.apache.cassandra.locator.AbstractCloudMetadataServiceConnector.METADATA_URL_PROPERTY;
+
+/**
+ * Default cloud-based metadata service snitch which parses datacenter and rack of a node by
+ * executing GET query for URL configured in cassandra-rackdc.properties under
+ * key {@link AbstractCloudMetadataServiceConnector#METADATA_URL_PROPERTY}. It expects HTTP response code 200.
+ * It is not passing any headers to GET request.
+ * <p>
+ * The response will be parsed like following - if the response body is, for example ""us-central1-a"", then
+ * the datacenter will be ""us-central1"" and rack will be ""a"". There is value of ""dc_suffix"" key in
+ * cassandra-rackdc.properties appended to datacenter.
+ */
+public class DefaultCloudMetadataServiceSnitch extends AbstractCloudMetadataServiceSnitch
+{","[{'comment': 'would you rename it into something like `GenericCloudMetadataServiceSnitch`?', 'commenter': 'jacek-lewandowski'}, {'comment': 'works for me.', 'commenter': 'smiklosovic'}]"
2458,src/java/org/apache/cassandra/locator/AbstractCloudMetadataServiceSnitch.java,"@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.SystemKeyspace;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.EndpointState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.Pair;
+
+import static java.lang.String.format;
+
+abstract class AbstractCloudMetadataServiceSnitch extends AbstractNetworkTopologySnitch
+{
+    static final Logger logger = LoggerFactory.getLogger(AbstractCloudMetadataServiceSnitch.class);
+
+    static final String DEFAULT_DC = ""UNKNOWN-DC"";
+    static final String DEFAULT_RACK = ""UNKNOWN-RACK"";
+
+    protected final AbstractCloudMetadataServiceConnector connector;
+    protected final SnitchProperties snitchProperties;
+
+    private final String localRack;
+    private final String localDc;
+
+    private Map<InetAddressAndPort, Map<String, String>> savedEndpoints;
+
+    public AbstractCloudMetadataServiceSnitch(AbstractCloudMetadataServiceConnector connector,
+                                              SnitchProperties snitchProperties,
+                                              Pair<String, String> dcAndRack)
+    {
+        this.connector = connector;
+        this.snitchProperties = snitchProperties;
+        this.localDc = dcAndRack.left;
+        this.localRack = dcAndRack.right;
+
+        logger.info(format(""%s using datacenter: %s, rack: %s, connector: %s, properties: %s"",
+                           getClass().getName(), getLocalDc(), getLocalRack(), connector, snitchProperties));
+    }
+
+    @Override
+    public String getLocalRack()
+    {
+        return localRack;
+    }
+
+    public String getLocalDc()","[{'comment': ""I don't know if it is intentional, but there is a `getLocalDatacenter()` method in the `IEndpointSnitch` interface with some default implementation"", 'commenter': 'jacek-lewandowski'}, {'comment': 'great catch.', 'commenter': 'smiklosovic'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/BufferedChecksumCrc32cIndexInput.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.index.sai.disk.io;
+
+import org.apache.lucene.store.BufferedChecksum;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+
+import java.io.IOException;
+import java.util.zip.CRC32C;
+import java.util.zip.Checksum;
+
+import static org.apache.cassandra.index.sai.disk.io.IndexFileUtils.CHECKSUM_CRC32C_FACTORY;
+
+/**
+ * This implementation of {@link ChecksumIndexInput} is based on {@link org.apache.lucene.store.BufferedChecksumIndexInput}
+ * but uses {@link CRC32C} checksum algorithm instead of {@code CRC32} for performance reasons.
+ *
+ * @see org.apache.cassandra.index.sai.disk.io.IndexFileUtils.ChecksummingWriter
+ */
+public class BufferedChecksumCrc32cIndexInput extends ChecksumIndexInput
+{
+    final IndexInput main;
+    final Checksum digest;
+
+    public BufferedChecksumCrc32cIndexInput(IndexInput main) {
+        super(""BufferedChecksumCrc32cIndexInput("" + main + ')');
+        this.main = main;
+        this.digest = new BufferedChecksum(CHECKSUM_CRC32C_FACTORY.get());
+    }
+
+    public byte readByte() throws IOException
+    {
+        byte b = this.main.readByte();
+        this.digest.update(b);
+        return b;
+    }
+
+    public void readBytes(byte[] b, int offset, int len) throws IOException {","[{'comment': ""nit: Check for `{` that need to move to newlines in this class. (Constructor, here, subsequent methods, etc.) I'm sure that's just Lucene formatting copied over..."", 'commenter': 'maedhroz'}, {'comment': 'Yes, it was copied ""as is"" with minimal changes. Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/BufferedChecksumCrc32cIndexInput.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.index.sai.disk.io;
+
+import org.apache.lucene.store.BufferedChecksum;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+
+import java.io.IOException;
+import java.util.zip.CRC32C;
+import java.util.zip.Checksum;
+
+import static org.apache.cassandra.index.sai.disk.io.IndexFileUtils.CHECKSUM_CRC32C_FACTORY;
+
+/**
+ * This implementation of {@link ChecksumIndexInput} is based on {@link org.apache.lucene.store.BufferedChecksumIndexInput}
+ * but uses {@link CRC32C} checksum algorithm instead of {@code CRC32} for performance reasons.
+ *
+ * @see org.apache.cassandra.index.sai.disk.io.IndexFileUtils.ChecksummingWriter
+ */
+public class BufferedChecksumCrc32cIndexInput extends ChecksumIndexInput
+{
+    final IndexInput main;","[{'comment': '```suggestion\r\n    final IndexInput delegate;\r\n```\r\n\r\nnit: ""delegate"" would be a more common name for this, something people will immediately understand as part of a forwarding/wrapper class', 'commenter': 'maedhroz'}, {'comment': 'Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/BufferedChecksumCrc32cIndexInput.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.index.sai.disk.io;
+
+import org.apache.lucene.store.BufferedChecksum;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+
+import java.io.IOException;
+import java.util.zip.CRC32C;
+import java.util.zip.Checksum;
+
+import static org.apache.cassandra.index.sai.disk.io.IndexFileUtils.CHECKSUM_CRC32C_FACTORY;
+
+/**
+ * This implementation of {@link ChecksumIndexInput} is based on {@link org.apache.lucene.store.BufferedChecksumIndexInput}
+ * but uses {@link CRC32C} checksum algorithm instead of {@code CRC32} for performance reasons.
+ *
+ * @see org.apache.cassandra.index.sai.disk.io.IndexFileUtils.ChecksummingWriter
+ */
+public class BufferedChecksumCrc32cIndexInput extends ChecksumIndexInput
+{
+    final IndexInput main;
+    final Checksum digest;
+
+    public BufferedChecksumCrc32cIndexInput(IndexInput main) {","[{'comment': '```suggestion\r\n    public BufferedChecksumCrc32cIndexInput(IndexInput delegate) {\r\n```', 'commenter': 'maedhroz'}, {'comment': 'Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/BufferedChecksumCrc32cIndexInput.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.index.sai.disk.io;
+
+import org.apache.lucene.store.BufferedChecksum;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+
+import java.io.IOException;
+import java.util.zip.CRC32C;
+import java.util.zip.Checksum;
+
+import static org.apache.cassandra.index.sai.disk.io.IndexFileUtils.CHECKSUM_CRC32C_FACTORY;
+
+/**
+ * This implementation of {@link ChecksumIndexInput} is based on {@link org.apache.lucene.store.BufferedChecksumIndexInput}
+ * but uses {@link CRC32C} checksum algorithm instead of {@code CRC32} for performance reasons.
+ *
+ * @see org.apache.cassandra.index.sai.disk.io.IndexFileUtils.ChecksummingWriter
+ */
+public class BufferedChecksumCrc32cIndexInput extends ChecksumIndexInput
+{
+    final IndexInput main;
+    final Checksum digest;
+
+    public BufferedChecksumCrc32cIndexInput(IndexInput main) {
+        super(""BufferedChecksumCrc32cIndexInput("" + main + ')');
+        this.main = main;
+        this.digest = new BufferedChecksum(CHECKSUM_CRC32C_FACTORY.get());","[{'comment': ""Why limit this class to a particular `Checksum` implementation? Here's what I would consider...\r\n\r\n1.) Rename the class `BufferedChecksumIndexInput`.\r\n2.) Pass the `Checksum` to the constructor along w/ the delegate.\r\n3.) Update any relevant comments to reflect this, etc.\r\n\r\nLucene should have done this w/ its own `BufferedChecksumIndexInput`, but they made it unnecessarily brittle."", 'commenter': 'maedhroz'}, {'comment': 'Yeah, I intentionally made it ""closed"". Since `BufferedChecksumCrc32cIndexInput` is often used directly through the code (especially in tests), I thought it was safer for a developer to use it ""as is"", so as not to miss the correct checksumming algo.\r\n\r\nI don\'t see any problem with the way you suggested, I\'ll fix it.', 'commenter': 'Mmuzaf'}, {'comment': 'Reasonable to assume the fact that we share a factory gets us ""to the bar"" in terms of safety.', 'commenter': 'maedhroz'}, {'comment': ""I've made the `BufferedChecksumIndexInput` package protected and moved a class instantiation to `IndexFileUtils.instance.getBufferedChecksumIndexInput`, so we now have everything related to CRC in one single place."", 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/v1/SAICodecUtils.java,"@@ -99,9 +100,26 @@ public static void validate(IndexInput input, long footerPointer) throws IOExcep
         input.seek(current);
     }
 
+    /**
+     * See the {@link org.apache.lucene.codecs.CodecUtil#checksumEntireFile(org.apache.lucene.store.IndexInput)}.
+     * @param input IndexInput to validate.
+     * @throws IOException if a corruption is detected.
+     */
     public static void validateChecksum(IndexInput input) throws IOException
     {
-        CodecUtil.checksumEntireFile(input);
+        IndexInput clone = input.clone();
+        clone.seek(0L);
+        ChecksumIndexInput in = new BufferedChecksumCrc32cIndexInput(clone);
+
+        assert in.getFilePointer() == 0L;","[{'comment': 'nit: I guess we might as well throw an error message in here, something like...\r\n\r\n```\r\nassert in.getFilePointer() == 0L : in.getFilePointer() + "" bytes already read from this input!"";\r\n```', 'commenter': 'maedhroz'}, {'comment': 'Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/IndexFileUtils.java,"@@ -44,6 +46,7 @@ public class IndexFileUtils
                                                                                              .build();
 
     public static final IndexFileUtils instance = new IndexFileUtils(DEFAULT_WRITER_OPTION);
+    public static final Supplier<Checksum> CHECKSUM_CRC32C_FACTORY = CRC32C::new;","[{'comment': 'nit: If we make the other changes suggested, this could probably just become `CHECKSUM_FACTORY`, as it might not always and forever be `CRC32C`. If we did change it, I guess there would be some versioning involved, but no need to think about that now.', 'commenter': 'maedhroz'}, {'comment': 'Renamed to `CHECKSUM_FACTORY`.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/v1/SAICodecUtils.java,"@@ -99,9 +100,26 @@ public static void validate(IndexInput input, long footerPointer) throws IOExcep
         input.seek(current);
     }
 
+    /**
+     * See the {@link org.apache.lucene.codecs.CodecUtil#checksumEntireFile(org.apache.lucene.store.IndexInput)}.
+     * @param input IndexInput to validate.
+     * @throws IOException if a corruption is detected.
+     */
     public static void validateChecksum(IndexInput input) throws IOException
     {
-        CodecUtil.checksumEntireFile(input);
+        IndexInput clone = input.clone();
+        clone.seek(0L);
+        ChecksumIndexInput in = new BufferedChecksumCrc32cIndexInput(clone);
+
+        assert in.getFilePointer() == 0L;
+
+        if (in.length() < (long) footerLength())
+            throw new CorruptIndexException(""misplaced codec footer (file truncated?): length="" + in.length() + "" but footerLength=="" + footerLength(), input);
+        else
+        {
+            in.seek(in.length() - (long) footerLength());
+            checkFooter(in);","[{'comment': 'So with the changes in this patch, we now have the methods in this class `readCRC()` and `writeCRC()` that both have imprecise names, comments, and `IllegalStateException` error messages. One simple solution is to leave the method names the same and add a ""C"" to the instances of ""CRC32"" and ""CRC-32"". The other is to just rename the methods `readChecksum()` and `writeChecksum()`, then remove the specific language. (ex. ""Illegal CRC-32 checksum: "" -> ""Illegal checksum: "")\r\n\r\nI\'m fine w/ either approach.', 'commenter': 'maedhroz'}, {'comment': 'For me both of the `readChecksum()` and `writeChecksum()` look more natural. Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/IndexFileUtils.java,"@@ -75,9 +79,20 @@ public IndexInput openBlockingInput(File file)
         return IndexInputReader.create(randomReader, fileHandle::close);
     }
 
+    public ChecksumIndexInput getBufferedChecksumIndexInput(IndexInput indexInput)
+    {
+        return new BufferedChecksumIndexInput(indexInput, CHECKSUM_FACTORY.get());
+    }
+
+    /**
+     * The SequentialWriter that calculates checksum of the data written to the file. This writer extends
+     * Cassandra's {@link SequentialWriter} to add the checksumming functionality and typically is used alongside","[{'comment': '```suggestion\r\n     * {@link SequentialWriter} to add the checksumming functionality and typically is used together\r\n```', 'commenter': 'maedhroz'}, {'comment': 'Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/IndexFileUtils.java,"@@ -75,9 +79,20 @@ public IndexInput openBlockingInput(File file)
         return IndexInputReader.create(randomReader, fileHandle::close);
     }
 
+    public ChecksumIndexInput getBufferedChecksumIndexInput(IndexInput indexInput)
+    {
+        return new BufferedChecksumIndexInput(indexInput, CHECKSUM_FACTORY.get());
+    }
+
+    /**
+     * The SequentialWriter that calculates checksum of the data written to the file. This writer extends
+     * Cassandra's {@link SequentialWriter} to add the checksumming functionality and typically is used alongside
+     * with {@link IndexOutputWriter}. This, in turn, is used in conjunction with the {@link BufferedChecksumIndexInput}","[{'comment': '```suggestion\r\n     * with {@link IndexOutputWriter}. This, in turn, is used in conjunction with {@link BufferedChecksumIndexInput}\r\n```', 'commenter': 'maedhroz'}, {'comment': 'Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/IndexFileUtils.java,"@@ -75,9 +79,20 @@ public IndexInput openBlockingInput(File file)
         return IndexInputReader.create(randomReader, fileHandle::close);
     }
 
+    public ChecksumIndexInput getBufferedChecksumIndexInput(IndexInput indexInput)
+    {
+        return new BufferedChecksumIndexInput(indexInput, CHECKSUM_FACTORY.get());
+    }
+
+    /**
+     * The SequentialWriter that calculates checksum of the data written to the file. This writer extends
+     * Cassandra's {@link SequentialWriter} to add the checksumming functionality and typically is used alongside
+     * with {@link IndexOutputWriter}. This, in turn, is used in conjunction with the {@link BufferedChecksumIndexInput}
+     * to verify the checksum of the data read from the file, so that they must share the same checksum algorithm.","[{'comment': '```suggestion\r\n     * to verify the checksum of the data read from the file, so they must share the same checksum algorithm.\r\n```', 'commenter': 'maedhroz'}, {'comment': 'Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/io/IndexFileUtils.java,"@@ -75,9 +79,20 @@ public IndexInput openBlockingInput(File file)
         return IndexInputReader.create(randomReader, fileHandle::close);
     }
 
+    public ChecksumIndexInput getBufferedChecksumIndexInput(IndexInput indexInput)","[{'comment': ""nit: Just make this `static` so we don't need to refer to the `instance`?"", 'commenter': 'maedhroz'}, {'comment': 'Fixed.', 'commenter': 'Mmuzaf'}]"
2782,src/java/org/apache/cassandra/index/sai/disk/v1/SAICodecUtils.java,"@@ -99,9 +100,26 @@ public static void validate(IndexInput input, long footerPointer) throws IOExcep
         input.seek(current);
     }
 
+    /**
+     * See the {@link org.apache.lucene.codecs.CodecUtil#checksumEntireFile(org.apache.lucene.store.IndexInput)}.","[{'comment': '```suggestion\r\n     * See {@link org.apache.lucene.codecs.CodecUtil#checksumEntireFile(org.apache.lucene.store.IndexInput)}.\r\n```', 'commenter': 'maedhroz'}, {'comment': 'Fixed.', 'commenter': 'Mmuzaf'}]"
2784,src/java/org/apache/cassandra/config/Config.java,"@@ -1129,6 +1123,8 @@ public enum PaxosOnLinearizabilityViolation
      */
     public ParameterizedClass default_compaction = null;
 
+    public volatile AccordSpec accord = new AccordSpec();","[{'comment': ""don't need `volatile` here, `final` is better... we only mutate the leaf fields, not the complex fields"", 'commenter': 'dcapwell'}]"
2784,conf/cassandra.yaml,"@@ -2113,3 +2110,15 @@ accord_transactions_enabled: false
 #   a stable cluster. If a node from a previous version was started by accident we won't any longer toggle behaviors as when UPGRADING.
 #
 storage_compatibility_mode: NONE
+
+accord:","[{'comment': 'need to comment this out; non of the fields are defined outside comments', 'commenter': 'dcapwell'}]"
2784,conf/cassandra.yaml,"@@ -2113,3 +2110,15 @@ accord_transactions_enabled: false
 #   a stable cluster. If a node from a previous version was started by accident we won't any longer toggle behaviors as when UPGRADING.
 #
 storage_compatibility_mode: NONE
+
+accord:
+  # Enables the execution of Accord (multi-key) transactions on this node.
+  # accord_transactions_enabled: false","[{'comment': 'think this is just `enabled`', 'commenter': 'dcapwell'}]"
2784,conf/cassandra.yaml,"@@ -2113,3 +2110,16 @@ accord_transactions_enabled: false
 #   a stable cluster. If a node from a previous version was started by accident we won't any longer toggle behaviors as when UPGRADING.
 #
 storage_compatibility_mode: NONE
+
+# accord:
+# Enables the execution of Accord (multi-key) transactions on this node.","[{'comment': ""spacing is off, and should we add a `#` in front so users can block uncomment and have this still work?  We don't really have many examples of nested properties with comments inside the nesting... so this could be the first time this could cause confusion I think."", 'commenter': 'dcapwell'}, {'comment': 'there is one: `repair` section but it is broken as well. \r\nwill fix the accord section', 'commenter': 'jacek-lewandowski'}]"
2784,src/java/org/apache/cassandra/config/AccordSpec.java,"@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.config;
+
+public class AccordSpec
+{
+    public volatile boolean enabled = false;
+
+    public volatile String journal_directory;
+
+    public volatile OptionaldPositiveInt shard_count = OptionaldPositiveInt.UNDEFINED;
+
+    public volatile DurationSpec.IntSecondsBound progress_log_schedule_delay = new DurationSpec.IntSecondsBound(1);","[{'comment': 'unless we plan to have JMX/VTable overrides of these properties, we should not add `volatile`... For example `enabled` can not change during the life of the JVM as we only add the accord tables on boot...  same for most of these configs, they are init only', 'commenter': 'dcapwell'}, {'comment': 'I was primarily thinking about tests', 'commenter': 'jacek-lewandowski'}]"
2784,src/java/org/apache/cassandra/config/Config.java,"@@ -1129,6 +1123,8 @@ public enum PaxosOnLinearizabilityViolation
      */
     public ParameterizedClass default_compaction = null;
 
+    public AccordSpec accord = new AccordSpec();","[{'comment': '`final`', 'commenter': 'dcapwell'}]"
2784,test/conf/cassandra-mtls.yaml,"@@ -87,3 +86,5 @@ authenticator:
   class_name : org.apache.cassandra.auth.MutualTlsAuthenticator
   parameters :
     validator_class_name: org.apache.cassandra.auth.SpiffeCertificateValidator
+accord:
+  journal_directory: build/test/cassandra/accord_journal","[{'comment': 'FYI, you can also do\r\n\r\n```\r\naccord.journal_directory: build/test/cassandra/accord_journal\r\n```\r\n\r\nNo need to change, was mostly saying as it might have been faster for you when you first changed these, so thought to mention', 'commenter': 'dcapwell'}, {'comment': 'yeah, but now we have all the Accord related stuff grouped together', 'commenter': 'jacek-lewandowski'}]"
2844,src/java/org/apache/cassandra/metrics/RepairMetrics.java,"@@ -18,17 +18,75 @@
 
 package org.apache.cassandra.metrics;
 
+import java.util.Collections;","[{'comment': 'This is from https://issues.apache.org/jira/browse/CASSANDRA-18952 and no changes are from this PR... can ignore this file', 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/SharedContext.java,"@@ -59,6 +59,16 @@ public interface SharedContext
     ScheduledExecutorPlus optionalTasks();
 
     MessageDelivery messaging();
+    default SharedContext withMessaging(MessageDelivery messaging)","[{'comment': 'This is from https://issues.apache.org/jira/browse/CASSANDRA-18952 and no changes are from this PR... can ignore this file', 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/RepairMessageVerbHandler.java,"@@ -286,27 +285,28 @@ public void doVerb(final Message<RepairMessage> message)
                     break;
 
                 case PREPARE_CONSISTENT_REQ:
-                    ctx.repair().consistent.local.handlePrepareMessage(message.from(), (PrepareConsistentRequest) message.payload);
+                    ctx.repair().consistent.local.handlePrepareMessage(message);
                     break;
 
                 case PREPARE_CONSISTENT_RSP:
-                    ctx.repair().consistent.coordinated.handlePrepareResponse((PrepareConsistentResponse) message.payload);
+                    ctx.repair().consistent.coordinated.handlePrepareResponse(message);
                     break;
 
                 case FINALIZE_PROPOSE_MSG:
-                    ctx.repair().consistent.local.handleFinalizeProposeMessage(message.from(), (FinalizePropose) message.payload);
+                    ctx.repair().consistent.local.handleFinalizeProposeMessage(message);
                     break;
 
                 case FINALIZE_PROMISE_MSG:
-                    ctx.repair().consistent.coordinated.handleFinalizePromiseMessage((FinalizePromise) message.payload);
+                    ctx.repair().consistent.coordinated.handleFinalizePromiseMessage(message);
                     break;
 
                 case FINALIZE_COMMIT_MSG:
-                    ctx.repair().consistent.local.handleFinalizeCommitMessage(message.from(), (FinalizeCommit) message.payload);
+                    ctx.repair().consistent.local.handleFinalizeCommitMessage(message);
                     break;
 
                 case FAILED_SESSION_MSG:
                     FailSession failure = (FailSession) message.payload;
+                    sendAck(message);","[{'comment': ""prob could nack if we don't know the session... ack vs nack really doesn't matter for IR as it doesn't wait for the result... the retries really are just for stability and not for the senders benefit."", 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/consistent/CoordinatorSession.java,"@@ -154,34 +160,37 @@ synchronized boolean hasFailed()
         return getState() == State.FAILED || Iterables.any(participantStates.values(), v -> v == State.FAILED);
     }
 
-    protected void sendMessage(InetAddressAndPort destination, Message<RepairMessage> message)
-    {
-        logger.trace(""Sending {} to {}"", message.payload, destination);
-
-        ctx.messaging().send(message, destination);
-    }
-
     public Future<Void> prepare()
     {
         Preconditions.checkArgument(allStates(State.PREPARING));
 
         logger.info(""Beginning prepare phase of incremental repair session {}"", sessionID);
-        Message<RepairMessage> message =
-            Message.out(Verb.PREPARE_CONSISTENT_REQ, new PrepareConsistentRequest(sessionID, coordinator, participants));
+
+        PrepareConsistentRequest request = new PrepareConsistentRequest(sessionID, coordinator, participants);
         for (final InetAddressAndPort participant : participants)
         {
-            sendMessage(participant, message);
+            sendMessageWithRetries(ctx,
+                                                 notDone(prepareFuture),
+                                                 request,
+                                                 Verb.PREPARE_CONSISTENT_REQ,
+                                                 participant);
         }
         return prepareFuture;
     }
 
-    public synchronized void handlePrepareResponse(InetAddressAndPort participant, boolean success)
+    public synchronized void handlePrepareResponse(Message<PrepareConsistentResponse> msg)
     {
+        InetAddressAndPort participant = msg.payload.participant;
+        boolean success = msg.payload.success;
         if (getState() == State.FAILED)
         {
             logger.trace(""Incremental repair {} has failed, ignoring prepare response from {}"", sessionID, participant);
+            sendFailureResponse(ctx, msg);
             return;
         }
+        sendAck(ctx, msg);
+        if (getParticipantState(participant) != State.PREPARING)","[{'comment': ""this also fixes a IR bug where we see the response *after* we get a fail session request...  in that case we fail and log a stack trace saying we can't go from `FAILURE -> PREPARED`"", 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/consistent/CoordinatorSession.java,"@@ -218,19 +227,29 @@ public synchronized Future<Void> finalizePropose()
     {
         Preconditions.checkArgument(allStates(State.REPAIRING));
         logger.info(""Proposing finalization of repair session {}"", sessionID);
-        Message<RepairMessage> message = Message.out(Verb.FINALIZE_PROPOSE_MSG, new FinalizePropose(sessionID));
+        FinalizePropose request = new FinalizePropose(sessionID);
         for (final InetAddressAndPort participant : participants)
         {
-            sendMessage(participant, message);
+            sendMessageWithRetries(ctx, notDone(finalizeProposeFuture), request, Verb.FINALIZE_PROPOSE_MSG, participant);
         }
         return finalizeProposeFuture;
     }
 
-    public synchronized void handleFinalizePromise(InetAddressAndPort participant, boolean success)
+    public synchronized void handleFinalizePromise(Message<FinalizePromise> message)
     {
+        InetAddressAndPort participant = message.payload.participant;
+        boolean success = message.payload.promised;
         if (getState() == State.FAILED)
         {
             logger.trace(""Incremental repair {} has failed, ignoring finalize promise from {}"", sessionID, participant);
+            sendFailureResponse(ctx, message);
+            return;
+        }
+        sendAck(ctx, message);
+        if (getParticipantState(participant) != State.REPAIRING)","[{'comment': ""this also fixes a IR bug where we see the response *after* we get a fail session request...  in that case we fail and log a stack trace saying we can't go from `FAILURE -> FINALIZE_PROMISED`"", 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/consistent/LocalSessions.java,"@@ -637,10 +642,13 @@ public LocalSession getSession(TimeUUID sessionID)
     }
 
     @VisibleForTesting
-    synchronized void putSessionUnsafe(LocalSession session)
+    synchronized boolean putSessionUnsafe(LocalSession session)
     {
+        if (sessions.containsKey(session.sessionID))","[{'comment': ""this actually makes it safe... but does have a big change in semantics... for retry logic we need ability to dedup, so blind overriding can't be done... I could always rename this to `putSession`...."", 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/messages/RepairMessage.java,"@@ -67,11 +86,36 @@ private enum ErrorHandling { NONE, TIMEOUT, RETRY }
         map.put(Verb.SYNC_REQ, timeoutVersion);
         map.put(Verb.VALIDATION_RSP, SUPPORTS_RETRY);
         map.put(Verb.SYNC_RSP, SUPPORTS_RETRY);
+        // IR messages
+        map.put(Verb.PREPARE_CONSISTENT_REQ, SUPPORTS_RETRY);
+        map.put(Verb.PREPARE_CONSISTENT_RSP, SUPPORTS_RETRY);
+        map.put(Verb.FINALIZE_PROPOSE_MSG, SUPPORTS_RETRY);
+        map.put(Verb.FINALIZE_PROMISE_MSG, SUPPORTS_RETRY);
+        map.put(Verb.FINALIZE_COMMIT_MSG, SUPPORTS_RETRY);
+        map.put(Verb.FAILED_SESSION_MSG, SUPPORTS_RETRY);
         VERB_TIMEOUT_VERSIONS = Collections.unmodifiableMap(map);
+
+        EnumSet<Verb> allowsRetry = EnumSet.noneOf(Verb.class);","[{'comment': 'this logic is from https://issues.apache.org/jira/browse/CASSANDRA-18952 but added IR messages as well', 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/messages/RepairMessage.java,"@@ -67,11 +86,36 @@ private enum ErrorHandling { NONE, TIMEOUT, RETRY }
         map.put(Verb.SYNC_REQ, timeoutVersion);
         map.put(Verb.VALIDATION_RSP, SUPPORTS_RETRY);
         map.put(Verb.SYNC_RSP, SUPPORTS_RETRY);
+        // IR messages
+        map.put(Verb.PREPARE_CONSISTENT_REQ, SUPPORTS_RETRY);
+        map.put(Verb.PREPARE_CONSISTENT_RSP, SUPPORTS_RETRY);
+        map.put(Verb.FINALIZE_PROPOSE_MSG, SUPPORTS_RETRY);
+        map.put(Verb.FINALIZE_PROMISE_MSG, SUPPORTS_RETRY);
+        map.put(Verb.FINALIZE_COMMIT_MSG, SUPPORTS_RETRY);
+        map.put(Verb.FAILED_SESSION_MSG, SUPPORTS_RETRY);
         VERB_TIMEOUT_VERSIONS = Collections.unmodifiableMap(map);
+
+        EnumSet<Verb> allowsRetry = EnumSet.noneOf(Verb.class);
+        allowsRetry.add(Verb.PREPARE_MSG);
+        allowsRetry.add(Verb.VALIDATION_REQ);
+        allowsRetry.add(Verb.VALIDATION_RSP);
+        allowsRetry.add(Verb.SYNC_REQ);
+        allowsRetry.add(Verb.SYNC_RSP);
+        allowsRetry.add(Verb.SNAPSHOT_MSG);
+        allowsRetry.add(Verb.CLEANUP_MSG);
+        // IR messages
+        allowsRetry.add(Verb.PREPARE_CONSISTENT_REQ);
+        allowsRetry.add(Verb.PREPARE_CONSISTENT_RSP);
+        allowsRetry.add(Verb.FINALIZE_PROPOSE_MSG);
+        allowsRetry.add(Verb.FINALIZE_PROMISE_MSG);
+        allowsRetry.add(Verb.FINALIZE_COMMIT_MSG);
+        allowsRetry.add(Verb.FAILED_SESSION_MSG);
+        ALLOWS_RETRY = Collections.unmodifiableSet(allowsRetry);
     }
-    private static final Set<Verb> SUPPORTS_RETRY_WITHOUT_VERSION_CHECK = Collections.unmodifiableSet(EnumSet.of(Verb.CLEANUP_MSG));
 
     private static final Logger logger = LoggerFactory.getLogger(RepairMessage.class);
+    private static final NoSpamLogger noSpam = NoSpamLogger.getLogger(logger, 1, TimeUnit.MINUTES);","[{'comment': 'from https://issues.apache.org/jira/browse/CASSANDRA-18952', 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/messages/RepairMessage.java,"@@ -121,27 +165,25 @@ public static <T> void sendMessageWithRetries(SharedContext ctx, RepairMessage r
 
     public static void sendMessageWithRetries(SharedContext ctx, RepairMessage request, Verb verb, InetAddressAndPort endpoint)
     {
-        sendMessageWithRetries(ctx, backoff(ctx, verb), always(), request, verb, endpoint, new RequestCallback<>()
-        {
-            @Override
-            public void onResponse(Message<Object> msg)
-            {
-            }
+        sendMessageWithRetries(ctx, backoff(ctx, verb), always(), request, verb, endpoint, NOOP_CALLBACK, 0);
+    }
 
-            @Override
-            public void onFailure(InetAddressAndPort from, RequestFailureReason failureReason)
-            {
-            }
-        }, 0);
+    public static void sendMessageWithRetries(SharedContext ctx, Supplier<Boolean> allowRetry, RepairMessage request, Verb verb, InetAddressAndPort endpoint)
+    {
+        sendMessageWithRetries(ctx, backoff(ctx, verb), allowRetry, request, verb, endpoint, NOOP_CALLBACK, 0);
     }
 
-    private static <T> void sendMessageWithRetries(SharedContext ctx, Backoff backoff, Supplier<Boolean> allowRetry, RepairMessage request, Verb verb, InetAddressAndPort endpoint, RequestCallback<T> finalCallback, int attempt)
+    @VisibleForTesting
+    static <T> void sendMessageWithRetries(SharedContext ctx, Backoff backoff, Supplier<Boolean> allowRetry, RepairMessage request, Verb verb, InetAddressAndPort endpoint, RequestCallback<T> finalCallback, int attempt)
     {
+        if (!ALLOWS_RETRY.contains(verb))","[{'comment': 'https://issues.apache.org/jira/browse/CASSANDRA-18952', 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/messages/RepairMessage.java,"@@ -165,13 +207,32 @@ public void onFailure(InetAddressAndPort from, RequestFailureReason failureReaso
                                                          backoff.computeWaitTime(attempt), backoff.unit());
                             return;
                         }
+                        maybeRecordRetry(failureReason);
                         finalCallback.onFailure(from, failureReason);
                         return;
                     default:
                         throw new AssertionError(""Unknown error handler: "" + allowed);
                 }
             }
 
+            private void maybeRecordRetry(@Nullable RequestFailureReason reason)","[{'comment': 'from https://issues.apache.org/jira/browse/CASSANDRA-18952', 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/utils/NoSpamLogger.java,"@@ -51,19 +51,18 @@ public enum Level
     }
 
     @VisibleForTesting
-    static interface Clock
+    public interface Clock","[{'comment': 'this was done in https://issues.apache.org/jira/browse/CASSANDRA-18952; simulation detects that global clock is used and rejects it, so we now use the simulated clock here, which means the no spam logic happens often (simulation time moves much faster than wall clock)', 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/repair/ConcurrentIrWithPreviewFuzzTest.java,"@@ -62,7 +64,7 @@ public void concurrentIrWithPreview()
                 // cause a delay in validation to have more failing previews
                 closeables.add(cluster.nodes.get(pickParticipant(rs, previewCoordinator, preview)).doValidation(next -> (cfs, validator) -> {
                     if (validator.desc.parentSessionId.equals(preview.state.id))
-                        cluster.unorderedScheduled.schedule(() -> next.accept(cfs, validator), 1, TimeUnit.HOURS);
+                        delayValidation(cluster, ir, next, cfs, validator);","[{'comment': 'this was a flakey issue that this patch discovered... with IR retries we can have IR take longer than 1h, so waiting by time is unsafe and leads to flakey builds... now we check IR state and keep retrying until IR completes', 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/repair/FailingRepairFuzzTest.java,"@@ -114,6 +114,7 @@ public void failingRepair()
                 }
 
                 cluster.processAll();
+                Assertions.assertThat(repair.state.isComplete()).describedAs(""Repair job did not complete, and no work is pending..."").isTrue();","[{'comment': 'added to produce a more useful error msg when repairs are not complete... These classes all assume that nothing left to process means the repair is complete, but it could also mean a bug in repair that will never get recovered... in that case the repair is not complete and the error message is hard to understand... so this new error message is trying to express this case', 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/repair/FuzzTestBase.java,"@@ -278,6 +282,12 @@ public void setupSchema()
         createSchema();
     }
 
+    protected void cleanupRepairTables()
+    {
+        for (String table : Arrays.asList(SystemKeyspace.REPAIRS))","[{'comment': ""IR keeps trying to load the system table, which can slow down the tests... so added this as a work around... it would be great if storage could be isolated and wasn't global state =("", 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/repair/FuzzTestBase.java,"@@ -319,25 +329,16 @@ static void enableMessageFaults(Cluster cluster)
             @Override
             public Set<Faults> apply(Cluster.Node node, Message<?> message)
             {
+                if (RepairMessage.ALLOWS_RETRY.contains(message.verb()))","[{'comment': 'this is from https://issues.apache.org/jira/browse/CASSANDRA-18952', 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/repair/FuzzTestBase.java,"@@ -319,25 +329,16 @@ static void enableMessageFaults(Cluster cluster)
             @Override
             public Set<Faults> apply(Cluster.Node node, Message<?> message)
             {
+                if (RepairMessage.ALLOWS_RETRY.contains(message.verb()))
+                {
+                    allowDrop.add(message.id());
+                    return Faults.DROPPED;
+                }
                 switch (message.verb())
                 {
-                    case PREPARE_MSG:
-                    case VALIDATION_REQ:
-                    case VALIDATION_RSP:
-                    case SYNC_REQ:
-                    case SYNC_RSP:
-                    case SNAPSHOT_MSG:
-                    case CLEANUP_MSG:
-                        allowDrop.add(message.id());
-                        return Faults.DROPPED;
                     // these messages are not resilent to ephemeral issues
-                    case PREPARE_CONSISTENT_REQ:
-                    case PREPARE_CONSISTENT_RSP:
-                    case FINALIZE_PROPOSE_MSG:
-                    case FINALIZE_PROMISE_MSG:
-                    case FINALIZE_COMMIT_MSG:
-                    case FAILED_SESSION_MSG:
-
+                    case STATUS_REQ:","[{'comment': ""these messages actually no-op as `StorageService` is not initialized... they actually cause annoying cases as they fail sessions, so didn't try to get working... just didn't want tests to fail if those messages happen"", 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/repair/FuzzTestBase.java,"@@ -799,6 +814,8 @@ private <REQ, RSP> void maybeEnqueue(Message<REQ> message, InetAddressAndPort to
                 CallbackContext cb;
                 if (callback != null)
                 {
+                    if (callbacks.containsKey(message.id()))","[{'comment': ""this is to better match `MessagingService`, this doesn't happen but better to be consistent"", 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/repair/FuzzTestBase.java,"@@ -850,7 +867,14 @@ private <REQ, RSP> void maybeEnqueue(Message<REQ> message, InetAddressAndPort to
                             if (ctx != null)
                             {
                                 assert ctx == cb;
-                                ctx.onFailure(to, RequestFailureReason.TIMEOUT);
+                                try
+                                {
+                                    ctx.onFailure(to, RequestFailureReason.TIMEOUT);
+                                }
+                                catch (Throwable t)
+                                {
+                                    failures.add(t);","[{'comment': ""when IR messages retry they would NPE as they use a null `desc`... this would get handled by the future and ignored, so tests would fail saying the repair didn't complete rather than the NPE... this fix makes sure we fail fast"", 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/repair/messages/RepairMessageTest.java,"@@ -0,0 +1,182 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.repair.messages;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import org.apache.cassandra.concurrent.ScheduledExecutorPlus;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.RequestFailureReason;
+import org.apache.cassandra.gms.IGossiper;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.metrics.RepairMetrics;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessageDelivery;
+import org.apache.cassandra.net.RequestCallback;
+import org.apache.cassandra.net.Verb;
+import org.apache.cassandra.repair.SharedContext;
+import org.apache.cassandra.utils.Backoff;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.TimeUUID;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+
+import static org.apache.cassandra.repair.messages.RepairMessage.always;
+import static org.apache.cassandra.repair.messages.RepairMessage.sendMessageWithRetries;
+import static org.apache.cassandra.test.asserts.ExtendedAssertions.assertThat;
+
+// Tests may use verb / message pairs that do not make sense... that is due to the fact that the message sending logic does not validate this and delegates such validation to messaging, which is mocked within the class...
+// By using messages with simpler state it makes the test easier to read, even though the verb -> message mapping is incorrect.
+public class RepairMessageTest","[{'comment': 'this is from https://issues.apache.org/jira/browse/CASSANDRA-18952', 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/test/asserts/ExtendedAssertions.java,"@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.test.asserts;
+
+import com.codahale.metrics.Counting;
+import org.assertj.core.api.AbstractObjectAssert;
+
+public class ExtendedAssertions","[{'comment': 'this is from https://issues.apache.org/jira/browse/CASSANDRA-18952', 'commenter': 'dcapwell'}]"
2844,test/unit/org/apache/cassandra/utils/NoSpamLoggerTest.java,"@@ -80,7 +80,7 @@ public boolean equals(Object o)
     @BeforeClass
     public static void setUpClass() throws Exception
     {
-        NoSpamLogger.CLOCK = () -> now;
+        NoSpamLogger.unsafeSetClock(() -> now);","[{'comment': 'this is from https://issues.apache.org/jira/browse/CASSANDRA-18952', 'commenter': 'dcapwell'}]"
2844,src/java/org/apache/cassandra/repair/messages/RepairMessage.java,"@@ -232,4 +298,15 @@ private static ErrorHandling errorHandlingSupported(SharedContext ctx, InetAddre
             return ErrorHandling.TIMEOUT;
         return ErrorHandling.NONE;
     }
+
+    public static void sendFailureResponse(SharedContext ctx, Message<?> respondTo)","[{'comment': ""I think we'll need to check the version of the node we're responding to. Otherwise I don't think they'll be expecting a response for this message"", 'commenter': 'bdeggleston'}, {'comment': 'thats fine, we drop in the messaging layer... these are *replies* which require we have Callbacks define for the message number, so the below block is what we hit in 4.x\r\n\r\n```\r\nRequestCallbacks.CallbackInfo callbackInfo = MessagingService.instance().callbacks.remove(message.id(), message.from());\r\n        if (callbackInfo == null)\r\n        {\r\n            String msg = ""Callback already removed for {} (from {})"";\r\n            logger.trace(msg, message.id(), message.from());\r\n            Tracing.trace(msg, message.id(), message.from());\r\n            return;\r\n        }\r\n```', 'commenter': 'dcapwell'}, {'comment': ""sure, but why reply if it's just going to be dropped? It's just a cleanliness thing. That said, no one's going to notice a few empty messages getting sent into the void, so I'm fine leaving as is if you'd prefer."", 'commenter': 'bdeggleston'}, {'comment': ""> but why reply if it's just going to be dropped?\r\n\r\nin order to know this we need to check gossip, which maybe stale.  So If we think its 4.x but its actually 5.x then we avoid sending the ack/nack and it will retry... \r\n\r\nNow, since we don't 100% support mixed version repairs, by not caring we make this simpler to reason about with the cost of empty dropped messages while in mixed mode (again, we don't support)... so this patch does not break mixed mode, but isn't 100% ideal in mixed mode..."", 'commenter': 'dcapwell'}]"
2899,src/java/org/apache/cassandra/service/paxos/PaxosPrepare.java,"@@ -1232,7 +1232,6 @@ public void serialize(Response response, DataOutputPlus out, int version) throws
         {
             if (response.isRejected())
             {
-                out.writeByte(0);","[{'comment': ""Could we just move the contents of `serializeRejection` back here? I don't think it needs to be in a separate method, especially when we're serializing the success response inline. @dcapwell this bug only exists in the cep-15 branch"", 'commenter': 'bdeggleston'}]"
